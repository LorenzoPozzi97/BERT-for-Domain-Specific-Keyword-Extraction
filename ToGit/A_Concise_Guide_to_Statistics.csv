,Relevance,Tags,Heading,Seg,Sentence
0,1,"['statistics', 'data', 'probability theory', 'probability', 'random', 'processes']",Chapter  Basics of Probability Theory,seg_1,"abstract statistics deals with the collection and interpretation of data. this chapter lays a foundation that allows to rigorously describe non-deterministic processes and to reason about non-deterministic quantities. the mathematical framework is given by probability theory, whose objects of interest are random quantities, their description and properties."
1,1,"['quantile', 'moment', 'distribution', 'probability']",Chapter  Basics of Probability Theory,seg_1,keywords probability · distribution · moment · quantile
2,1,"['sample', 'random', 'statistics', 'data', 'probability theory', 'set', 'probability', 'random sample', 'processes']", Probability and Events,seg_3,"in statistics, we are concerned with the collection, analysis, and interpretation of data, typically given as a random sample from a large set. we therefore need to lay a foundation in probability theory that allows us to formally represent nondeterministic processes and study their properties."
3,1,"['outcome', 'set', 'outcomes']", Probability and Events,seg_3,"a first example. let us consider the following situation: a dice is rolled leading to any of the numbers {1, . . . , 6} as a possible outcome. with two dice, the possible outcomes are described by the set"
4,0,[], Probability and Events,seg_3,"= {(i, j)|1 ≤ i, j ≤ 6} ,"
5,1,"['set', 'outcomes']", Probability and Events,seg_3,of size | | = 36. the set of outcomes that lead to a sum of at least 10 is then
6,0,[], Probability and Events,seg_3,"a = {(4, 6), (6, 4), (5, 5), (5, 6), (6, 5), (6, 6)} ⊂ ,"
7,1,"['set', 'probability', 'outcomes']", Probability and Events,seg_3,a set of size 6. a first definition of the probability that we will roll a sum of at least 10 is given by counting the number of outcomes that lead to a sum larger or equal
8,1,['outcomes'], Probability and Events,seg_3,10 and divide it by the number of all possible outcomes:
9,0,[], Probability and Events,seg_3,"|a| 6 p(a) = = , | | 36"
10,1,['outcomes'], Probability and Events,seg_3,with the intuitive interpretation that 6 out of 36 possible outcomes are of the desired type. this definition implicitly assumes that each of the 36 possible outcomes has the same chance of occurring.
11,1,"['intersection', 'sets', 'events', 'probability', 'event', 'outcomes']", Probability and Events,seg_3,"any collection of possible outcomes x ⊆ is called an event; the previous definition assigns a probability of p(x) = |x |/| | to such an event. events are sets and we can apply the usual operations on them: let a be as above the event of having a sum of at least 10. let us further denote by b the event that both dice show an even number; thus, b = {(2, 2), (2, 4), (2, 6), (4, 2), (4, 4), (4, 6), (6, 2), (6, 4), (6, 6)} and |b| = 9. the event c of rolling a sum of at least 10 and both dice even is then described by the intersection of the two events:"
12,0,[], Probability and Events,seg_3,"c = a ∩ b = {(4, 6), (6, 4), (6, 6)},"
13,1,['probability'], Probability and Events,seg_3,and has probability
14,0,[], Probability and Events,seg_3,3 p(c) = p(a ∩ b) = . 36
15,1,"['union', 'event']", Probability and Events,seg_3,"similarly, we can ask for the event of rolling a total of at least ten or both dice even. this event corresponds to the union of a and b, since any of the elements of a or b will do:"
16,0,[], Probability and Events,seg_3,"d := a ∪ b = {(5, 5), (5, 6), (6, 5), (2, 2), (2, 4), (2, 6),"
17,0,[], Probability and Events,seg_3,"(4, 2), (4, 4), (4, 6), (6, 2), (6, 4), (6, 6)} ."
18,1,"['complement', 'event', 'complement of an event', 'outcomes']", Probability and Events,seg_3,"the complement of an event corresponds to all possible outcomes that are not covered by the event itself. for example, the event of not rolling an even number simultaneously on both dice is given by the complement of b, which is"
19,0,[], Probability and Events,seg_3,"bc = \b = {(i, j) ∈ |(i, j) ∈ b},"
20,1,['probability'], Probability and Events,seg_3,with probability
21,0,[], Probability and Events,seg_3,9 27 p (bc ) = 1 − p(b) = 1 − = . 36 36
22,1,"['case', 'events', 'set', 'function', 'outcomes']", Probability and Events,seg_3,"the general case. let be the set of all possible outcomes of a particular experiment and denote by a, b ⊆ any pair of events. then, any function p with the properties"
23,0,[], Probability and Events,seg_3,"p( ) = 1, (1.1)"
24,0,[], Probability and Events,seg_3,"p(a) ≥ 0, (1.2)"
25,0,[], Probability and Events,seg_3,p(a ∪ b) = p(a) + p(b) if a ∩ b = ∅ (1.3)
26,1,"['disjoint', 'probabilities', 'experiment', 'sets', 'probability measure', 'events', 'set', 'probability', 'outcomes', 'disjoint sets']", Probability and Events,seg_3,"defines a probability measure or simply a probability that allows to compute the probability of events. the first requirement (1.1) ensures that is really the set of all possible outcomes, so any experiment will lead to an element of ; the probability that something happens should thus be one. the second requirement (1.2) is that probabilities are never negative (but the probability of events might be zero). finally, the third requirement (1.3) gives us the algebraic rule how the probability of combined events is computed; importantly, this rule only applies for disjoint sets. using the algebra of sets as above, we can immediately derive some additional facts:"
27,0,[], Probability and Events,seg_3,"p (ac ) = p( \a) = 1 − p(a),"
28,0,[], Probability and Events,seg_3,"p(∅) = 1 − p( ) = 0,"
29,0,[], Probability and Events,seg_3,a ⊆ b ⇒ p(a) ≤ p(b).
30,1,"['set ', 'probability measures', 'experiment', 'case', 'discrete', 'probability measure', 'events', 'set', 'probability', 'outcome', 'outcomes']", Probability and Events,seg_3,"importantly, there are multiple ways to define a valid probability measure for any given set , so these three requirements do not specify a unique such measure. for assigning a probability to discrete events like the ones discussed so far, it is sufficient to specify the probability p({ω}) for each possible outcome ω ∈ of the experiment. for example, a die is described by its outcomes = {1, 2, 3, 4, 5, 6}. one possible probability measure is p({ω}) = 1/6 for each of the six possible outcomes ω; it describes a fair die. another probability is p({1}) = p({3}) = p({5}) = 1/18, p({2}) = p({4}) = p({6}) = 2/18, in which case the probability to roll an even number is twice the probability to roll an odd one. both probability measures are valid, and the particular choice depends on the various assumptions that are made when modeling the die and its behavior."
31,1,"['data', 'probability measure', 'probability', 'probability measures']", Probability and Events,seg_3,"typically, probability measures are either derived from such assumptions or they are inferred from observed data. such inference will be covered in chap. 2. for more complex examples, it might not be straightforward to construct a probability measure that correctly captures all assumptions."
32,1,"['results', 'case', 'discrete', 'cases', 'set', 'probability measure', 'probability', 'continuous', 'outcomes']", Probability and Events,seg_3,"if the possible outcomes become a continuous set, describing a length, for example, it is no longer possible to simply assign a probability to each element of this set to define a probability measure. this case requires more sophisticated mathematical machinery and is not covered here. however, the following results are essentially the same for discrete and continuous cases."
33,1,"['probabilities', 'events', 'union', 'probability', 'event']", Probability and Events,seg_3,union of events. we still need to delve a little deeper into the computation of event probabilities. note that the probability for a union of events is only the sum of the two individual probabilities provided the two events do not overlap and there is no
34,1,"['case', 'events', 'union']", Probability and Events,seg_3,"outcome that belongs to both events. in the above example of events a (sum larger equal 10) and b (both dice even), this is clearly not the case. consequently, the size of their union d is smaller than the sum of the individual sizes of a and b."
35,1,"['intersection', 'sets', 'probability']", Probability and Events,seg_3,"for computing the probability of d from a and b, we can use an inclusionexclusion argument: the size of d is the size of a plus the size of b, minus the size of the intersection a ∩ b. this becomes clear if we draw a venn-diagram of the sets as in fig. 1.1. the elements in the intersection are counted twice and we therefore have to correct by subtracting it once. indeed,"
36,0,[], Probability and Events,seg_3,"|d| = |a| + |b| − |a ∩ b|,"
37,0,[], Probability and Events,seg_3,and thus
38,0,[], Probability and Events,seg_3,|a| |b| |a ∩ b| p(d) = + − = p(a) + p(b) − p(a ∩ b). | | | | | |
39,1,['case'], Probability and Events,seg_3,"note that if a ∩ b = ∅, we recover the case (1.3) given in the original definition."
40,1,['sets'], Probability and Events,seg_3,"the inclusion–exclusion calculation is also possible for more than two sets, but does get a little more involved: already for three sets, we now count some subsets twice and three times."
41,1,"['independent', 'statistics', 'information', 'independence', 'events', 'probability']", Probability and Events,seg_3,"independence. one of the most important concepts in probability and statistics is independence. two events x and y are independent if the knowledge that y already occurred does not influence the probability that x will occur. in other words, knowing that y happened gives us no information on whether x also happened or not, and viceversa."
42,1,['independence'], Probability and Events,seg_3,"as an example, let us again look at rolling two dice: with some justification, we may assume that the roll of the first die does not influence the roll of the second. in particular, it does not matter whether we roll both dice at once or roll the first and then roll the second. in this example, the independence of the two dice is a modeling assumption and other modeling assumptions are possible."
43,1,"['events', 'independent']", Probability and Events,seg_3,"formally, two events x and y are independent if"
44,0,[], Probability and Events,seg_3,"p(x ∩ y ) = p(x)p(y ),"
45,1,['probability'], Probability and Events,seg_3,which means that the probability that both x and y occur is the probability that x occurs times the probability that y occurs.
46,1,"['independent', 'event']", Probability and Events,seg_3,"in the dice example, the event e = {(i, j)|i ≥ 5} of rolling a 5 or 6 on the first die, and f = {(i, j)| j ≥ 5} of rolling a 5 or 6 on the second die, are independent. indeed,"
47,0,[], Probability and Events,seg_3,"4 p(e ∩ f) = p({(i, j)|i ≥ 5, j ≥ 5}) = p({(5, 5), (5, 6), (6, 5), (6, 6)}) = and"
48,1,"['independent', 'conditional probability', 'conditional', 'events', 'probability']", Probability and Events,seg_3,"if two events are not independent, the probability of x happening provided we already know that y happened is captured by the conditional probability of x given y, which we denote by p(x |y ). this probability is given by"
49,0,[], Probability and Events,seg_3,"p(x ∩ y ) p(x |y ) = , p(y )"
50,1,"['outcomes', 'probability']", Probability and Events,seg_3,"and might be easier to remember in its equivalent form p(x |y )p(y ) = p(x ∩ y ), which reads “the probability of x and y happening simultaneously is the probability that y happens times the probability that x happens if y happened”. for example, what is the probability to roll at least a total of 10, if we already know that both dice are even? there are |b| = 9 possible outcomes that lead to both dice even. from these, |a ∩ b| = 3 have a sum of 10 or greater, leading to the probability"
51,1,"['events', 'independent']", Probability and Events,seg_3,"if two events x and y are independent, we have"
52,0,[], Probability and Events,seg_3,"p(x |y ) = p(x),"
53,1,['probability'], Probability and Events,seg_3,"as we would expect. this also agrees with the above interpretation: the probability of x happening is the same, irrespective of knowing whether y happened or not."
54,1,"['states', 'law of total probability', 'total probability', 'probability']", Probability and Events,seg_3,the law of total probability states that
55,0,[], Probability and Events,seg_3,"p(x) = p(x ∩ y1) + · · · + p(x ∩ yn),"
56,1,"['outcome', 'outcomes', 'probability']", Probability and Events,seg_3,"where the yi form a partition of , i.e., cover all possible outcomes without covering one outcome twice. we can read this as “the probability that x happens is the probability that x and y1 happen simultaneously or x and y2 happen simultaneously etc.”. the example in fig. 1.2 gives an intuitive representation of the theorem."
57,1,"['probabilities', 'conditional probabilities', 'conditional', 'events', 'law of total probability', 'probability', 'total probability']", Probability and Events,seg_3,"together, conditional probabilities and the law of total probability are powerful tools for computing the probability of particular events."
58,1,"['without replacement', 'replacement', 'probability', 'event']", Probability and Events,seg_3,"example 1 let us consider an urn containing 2 white and 3 black balls, from which two balls are drawn, and are not put back in (this is known as drawing without replacement). let us denote by c the event that we draw two identical colors, no matter which one, and by w and b the event of the first ball being white, respectively black. provided we know the first ball is white, we can conclude that there are 1 white and 3 black balls left in the urn. the probability to draw another white ball thus became"
59,0,[], Probability and Events,seg_3,p(c |w ) = 1/4.
60,0,[], Probability and Events,seg_3,"similarly, if we know the first ball is black, we can conclude 2 white and 2 black balls being left and thus"
61,0,[], Probability and Events,seg_3,p(c |b) = 2/4.
62,1,"['law of total probability', 'total probability', 'probability']", Probability and Events,seg_3,"by the law of total probability, p(c) is the probability to draw another white ball if we have a white, times the probability to draw a white in the first place plus the same for black:"
63,0,[], Probability and Events,seg_3,p(c) = p(c ∩ w ) + p(c ∩ b) = p(c |w )p(w ) + p(c |b)p(b).
64,0,[], Probability and Events,seg_3,with p(w ) = 2/5 and p(b) = 3/5 this amounts to p(c) = 2/5.
65,1,"['conditional probabilities', 'probabilities', 'conditional']", Probability and Events,seg_3,bayes’ rule. bayes’ rule is an important tool for manipulating conditional probabilities. it allows us to “invert” conditional probabilities by
66,0,[], Probability and Events,seg_3,"p(y ) p(y |x) = p(x |y ) , (1.4) p(x)"
67,1,"['probabilities', 'conditional probabilities', 'prior probabilities', 'conditional', 'information', 'event', 'posterior']", Probability and Events,seg_3,"which becomes evident if we simply multiply by p(x) to arrive at p(y ∩ x) = p(x ∩ y ).the two probabilities for x and y are called the prior probabilities, as they describe the chance that either event happens without taking into account any information about the other event. the left-hand side of (1.4) is called the posterior. while algebraically simple, this rule is very helpful in computing conditional probabilities in a variety of situations."
68,1,"['tests', 'test', 'probability']", Probability and Events,seg_3,"example 2 let us consider the following situation: a patient is going to see a doctor for an annual checkup and in the battery of tests that are performed, the test for a particular disease d comes back positive. the doctor is concerned, as he read in the brochure that this particular test has a probability of 0.9 to correctly detect the disease if the patient actually has it, and also a probability of 0.9 to correctly detect if the patient does not have it. should the patient be concerned, too?"
69,1,"['information', 'events', 'outcome', 'test']", Probability and Events,seg_3,"let us denote by + and − the events of positive, respectively negative, outcome of the test. from the brochure information, we know that"
70,0,[], Probability and Events,seg_3,"p(+|d) = 0.9,"
71,0,[], Probability and Events,seg_3,p(−|dc ) = 0.9.
72,1,"['test', 'probability']", Probability and Events,seg_3,"but what we are really interested in is the probability p(d|+) that the patient actually has the disease, provided the test says so. we compute this probability via bayes’ rule as"
73,0,[], Probability and Events,seg_3,p(d) p(d|+) = p(+|d) . p(+)
74,1,"['results', 'test results', 'law of total probability', 'probability', 'total probability', 'test']", Probability and Events,seg_3,we would therefore need to know the probability p(d) that a patient has the disease in the first place (regardless of any test results) and the probability p(+) that the test will be positive (regardless of whether the patient is sick). the latter is easily computed using the law of total probability
75,0,[], Probability and Events,seg_3,"p(+) = p(+|d)p(d) + p(+|dc )p(dc ),"
76,0,[], Probability and Events,seg_3,and
77,0,[], Probability and Events,seg_3,p(+|dc ) = 1 − p(−|dc ).
78,1,"['case', 'data', 'information', 'probability']", Probability and Events,seg_3,"the only thing we need to figure out is p(d), the probability to be sick in the first place. we might imagine this as the probability that, randomly picking someone from the street, this person is actually sick. it is important to understand that this probability has to be provided from the outside, as it cannot be derived from the information available in the problem specification. in our case, such data might be available from public health institutions. let us assume that 1 in 100 people are sick, so p(d) = 0.01 and consequently"
79,0,[], Probability and Events,seg_3,"p(+) = 0.9 × 0.01 + (1 − 0.9) × (1 − 0.01) = 0.108,"
80,1,"['test', 'probability', 'information']", Probability and Events,seg_3,"that is, for about one out of ten people, the test will be positive, irrespective of their actual health. this is simply because few people have the disease, but in one out of ten, the test will be incorrect. we assembled all information needed to actually compute the relevant probability:"
81,0,[], Probability and Events,seg_3,0.01 p(d|+) = 0.9 × ≈ 0.08. 0.108
82,1,"['test', 'independent', 'case', 'probability']", Probability and Events,seg_3,"maybe surprisingly, the probability to have the disease if the test is positive is less that 10% and getting a second opinion is clearly indicated! this is one reason to perform a second independent test in such a case."
83,1,"['conditional probabilities', 'probabilities', 'prior probabilities', 'conditional']", Probability and Events,seg_3,"the key point of this example is to not get confused by the two conditional probabilities p(x |y ) and p(y |x) and mistakenly assume them to be equal or at least of comparable size. as shown, the two can be very different, depending on the prior probabilities for x and y."
84,1,"['bayesian', 'bayesian statistics', 'statistics', 'statistical hypothesis', 'probability', 'data', 'information', 'hypothesis testing', 'statistical', 'prior probabilities', 'statistical hypothesis testing', 'hypothesis', 'measurement', 'probabilities']", Probability and Events,seg_3,"implications of bayes’ rule. bayes’ rule has some more implications along these lines, which we will briefly describe in a very informal manner: as we will see in the chapter on statistical hypothesis testing, classical techniques only allow us to compute the probability of seeing certain data d (e.g., a measurement), provided a given hypothesis h is true; very informally, p(d|h). of course, we actually want to know the probability p(h |d) of the hypothesis being true, given the data. however, bayes’ rule shows that this probability can only be computed if we have information about the hypothesis being true irrespective of any data. again, this information about prior probabilities has to come from “outside” and cannot be inferred from the hypothesis or the data. typically, this information is provided by either additional assumptions or by looking into other data. the branch of bayesian statistics deals with the incorporation of such prior data and provides many alternative ways of inference and hypothesis testing. however, many of these methods are more elaborate and special care needs to be taken to correctly apply them, which is one reason why we do not cover them in this text."
85,1,"['functions', 'outcomes', 'random variables', 'random', 'variables', 'random variable', 'events', 'variable', 'set', 'probability', 'event', 'outcome']", Random Variables,seg_5,"while events and algebraic set operations form the basis for describing random experiments, we gain much more flexibility and widen the applications of probability by introducing random variables. technically, these are functions mapping an outcome ω ∈ to a number. for example, we can describe the two dice example simply by defining the number of eyes rolled with die i as the random variable xi . the event of having at least a 5 on the first die is then described intuitively by the statement x1 ≥ 5. similarly, we can formulate the event that the sum is larger than 10 by x1 + x2 ≥ 10, instead of listing all corresponding outcomes."
86,1,"['random variables', 'random', 'probabilities', 'variables', 'events', 'event']", Random Variables,seg_5,"once probabilities are assigned to events, they transfer to random variables simply by finding the corresponding event described by a statement on the random variables. formally, p(x ∈ x ) = p({ω|x (ω) ∈ x })."
87,1,"['random variable', 'variable', 'probability', 'random']", Random Variables,seg_5,"for the two dice example, let us compute the probability of rolling at least a 5 with the first die using the random variable x1:"
88,0,[], Random Variables,seg_5,"p(x1 ≥ 5) = p (x1 ∈ {5, 6}) = p ({ω ∈ |x1(ω) ∈ {5, 6}})"
89,0,[], Random Variables,seg_5,"= p({(i, j)|i ≥ 5, 1 ≤ j ≤ 6})"
90,0,[], Random Variables,seg_5,"12 = p({(i, j)|i = 5}) + p({(i, j)|i = 6}) = . 36"
91,1,"['random variables', 'random', 'variables', 'intersection', 'events', 'sets', 'joint', 'probability', 'joint probability']", Random Variables,seg_5,the joint probability of two random variables x and y simultaneously taking values in their respective sets is given by the intersection of the corresponding events:
92,0,[], Random Variables,seg_5,"p(x ∈ x , y ∈ y ) = p ({ω|x (ω) ∈ x } ∩ {ω|y (ω) ∈ y }) ."
93,1,"['distribution function', 'probability', 'random', 'function', 'cumulative distribution function', 'random variable', 'events', 'distributions', 'distribution', 'random variables', 'variables', 'variable', 'probability distribution', 'tables']", Random Variables,seg_5,"the advantage of working with random variables instead of events comes from the fact that random variables have a probability distribution that describes the probability that the random variable takes a value smaller or equal to a certain number. the cumulative distribution function (cdf) fx () of a random variable x is defined as fx (x) = p(x ≤ x). it always starts with a value of 0 at x = −∞ and monotonically increases to 1 for larger values of x. often, very different problems lead to the same distribution for the involved random variables, which is why some distributions get their own name and their properties can be found in tables."
94,1,"['random variables', 'random', 'variables', 'discrete', 'events', 'set', 'continuous']", Random Variables,seg_5,"similar to events, random variables also come in two distinct flavors: they either take values in a discrete (but maybe infinite) set of values, as in the dice example, or they take values from a continuous set, like the set of real numbers ir."
95,1,"['discrete random variable', 'cumulative distribution function', 'random variables', 'mass function', 'variables', 'probability mass function', 'distribution function', 'discrete', 'distribution', 'random variable', 'variable', 'summation', 'probability', 'random', 'function']", Random Variables,seg_5,discrete random variables. a discrete random variable a has a probability mass function (pmf) in addition to its cumulative distribution function. the pmf is given by pa(a) = p(a = a) and we can easily compute the cdf from it by summation:
96,0,[], Random Variables,seg_5,fa(k) = ∑a
97,0,[], Random Variables,seg_5,k=−∞ pa(a).
98,1,"['experiment', 'bernoulli distributed', 'distribution', 'bernoulli', 'probability', 'bernoulli distribution', 'parameter', 'outcome', 'tail']", Random Variables,seg_5,"example 3 let us consider the following experiment: a coin is flipped n times. the probability of any flip to show head is given by our first distribution with its own name: the bernoulli distribution, which assigns a probability of p to head and 1− p to tail. if xi is the outcome of the ith flip, with xi = 0 for tail and xi = 1 for head, this distribution is completely described by p, as p(xi = 0) = 1 − p, p(xi = 1) = p and p(xi = k) = 0 for any value k that is neither 0 nor 1. thus, knowing that xi is bernoulli distributed with parameter p completely specifies all we need to know. in short, this statement is written as x ∼ bernoulli(p), where “∼” is read as “distributed as”."
99,1,"['independent', 'distribution', 'probability', 'tails']", Random Variables,seg_5,"what is the probability that we have to wait until the wth flip to see head for the first time? this is the question for the distribution of a waiting time w. let us see: to see the first head at flip w, all preceding w − 1 flips are necessarily tails. assuming the flips to be independent, this probability is"
100,0,[], Random Variables,seg_5,"p(x1 = 0, . . . , xw−1 = 0) = p(x1 = 0) · · · p(xw−1 = 0) = (1 − p)w−1."
101,1,"['discrete random variable', 'mass function', 'geometric distribution', 'probability mass function', 'discrete', 'distribution', 'random variable', 'variable', 'probability', 'random', 'function', 'geometric', 'realization']", Random Variables,seg_5,"the probability to actually see head in the wth flip is p(xw = 1) = p. thus, p(w = w) = (1 − p)w−1 p, the pmf of a geometric distribution, denoted w ∼ geom(p). the particular value w is called a realization of the random variable w. this is an example of a discrete random variable that has infinitely many possible values with positive probability. the probability mass function of a geometric distribution is given in fig. 1.3 (left)."
102,1,"['tails', 'probability']", Random Variables,seg_5,what is the probability to see exactly h heads if we flip n times? this question is a little more tricky: the probability to see h heads in n flips is ph . the probability that the remaining n−h flips are all tails is (1− p)n−h . but there are a multitude of ways to
103,1,"['tails', 'binomial']", Random Variables,seg_5,"n arrange the h heads and n−h tails. to be exact, there are ( h ) := (n−nh!)!h! (a binomial"
104,0,['n'], Random Variables,seg_5,"coefficient, read “n choose h”) many ways to do so: n! := 1 × 2 × 3 × · · · × n is the number of ways to arrange n flips in different order. the h heads can be drawn in h!"
105,1,"['binomial coefficient', 'binomial', 'coefficient', 'tails']", Random Variables,seg_5,"different orders, which we do not distinguish and treat as equivalent. similarly, there are (n − h)! ways to arrange the tails are equivalent, leading to the stated coefficient. more generally, the binomial coefficient gives the number of different ways to draw h objects out of n, if we do not care for the order in which they are drawn. for n = 3,"
106,1,['set'], Random Variables,seg_5,"3 there are ( 2 ) = 3 ways to draw exactly two of them: from the set {a, b, c}, the 3"
107,1,"['sets', 'set']", Random Variables,seg_5,"ways are {a, b}, {b, c}, {a, c}. the first set contains two possible ways to draw: first the a, then the b, or vice-versa, and similarly for the other two sets."
108,1,"['probabilities', 'parameters', 'mass function', 'distribution', 'random variable', 'variable', 'binomial', 'random', 'function', 'binomial distribution']", Random Variables,seg_5,"for our problem, let h be the number of heads in n flips. this is a random variable taking values between 0 and n. it has a binomial distribution with two parameters n and p and probabilities given by the mass function"
109,0,['n'], Random Variables,seg_5,n p(h = h) = ( h ) (1 − p)n−h ph;
110,1,"['plot', 'mass function', 'probability mass function', 'binomial', 'probability', 'function']", Random Variables,seg_5,"denoted by h ∼ binom(n, p). a plot of a binomial probability mass function is given in fig. 1.3 (right)."
111,1,"['set', 'probability']", Random Variables,seg_5,"let us combine these two calculations of the waiting time and the number of heads by solving the following problem: again, a coin is flipped n times, the probability to see head is p. let again h be the number of heads in these n flips and let w be the waiting time for the first head to appear. for completeness, we set w = n + 1, if no head appears at all."
112,1,"['joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'joint probability']", Random Variables,seg_5,"what is the probability to simultaneously see h heads with the first head appearing at the wth flip? this questions asks for the joint probability distribution given by p(h = h, w = w)."
113,1,"['random variables', 'independent', 'variables', 'information', 'cases', 'probability', 'random']", Random Variables,seg_5,"the two random variables h, w are not independent: if they were, we would have p(h = h|w = w) = p(h = h). but if no head has appeared at all (so w = n + 1), then the probability to see any more than zero heads, given this information, is zero: p(h = 1|w = n +1) = 0, but p(h = 1) > 0. for working out the correct answer, we therefore need to take this dependency into account. it is always a good idea to check some boundary cases first: as we saw,"
114,0,['n'], Random Variables,seg_5,"p(h = 0, w = n +1) = p(h = 0|w = n +1)p(w = n +1) = 1×(1− p)n,"
115,1,"['tails', 'probability']", Random Variables,seg_5,"the probability to see n tails. further, p(h = 0, w = w) = 0 for any w ≤ n, as we cannot have seen the first head somewhere in the sequence if we saw none at all."
116,1,"['cases', 'tails']", Random Variables,seg_5,"what about the non-boundary cases? for (h = h, w = w), we can use the following argument: to see h heads in total, given the first one in the wth flip, we know that the first w − 1 flips are all tails and we need to place h − 1 heads in the remaining n − w positions (the first is already placed in position w):"
117,0,['n'], Random Variables,seg_5,"n − w p(h = h|w = w) = ( h − 1 ) (1 − p)(n−w)−(h−1) ph−1,"
118,1,"['geometric distribution', 'distribution', 'trials', 'binomial', 'probability', 'geometric']", Random Variables,seg_5,the binomial probability of having h −1 heads in n −w trials. the probability of first head at w ≤ n is the geometric distribution p(w = w) = (1− p)w−1 p. combining:
119,0,['n'], Random Variables,seg_5,"n − w p(h = h, w = w) = ( h − 1 ) (1 − p)n−h ph ."
120,1,"['distribution', 'conditional distribution', 'conditional']", Random Variables,seg_5,"the conditional distribution of waiting w flips, given we have h heads in total, is easily calculated as"
121,0,['n'], Random Variables,seg_5,( n h
122,0,[], Random Variables,seg_5,− −
123,0,[], Random Variables,seg_5,w
124,0,[], Random Variables,seg_5,"1 ) p(w = w|h = h) = ,"
125,0,['n'], Random Variables,seg_5,( n h )
126,1,"['independent', 'table', 'joint', 'probability', 'joint probability']", Random Variables,seg_5,"the number of ways to place h − 1 heads in n − w positions over the number of ways to place h heads in n positions. interestingly, this probability is independent of the probability p to see head. for n = 3 and p = 0.5, the full joint probability p(h = h, w = w) is given in table 1.1."
127,1,"['marginal', 'distribution', 'law of total probability', 'probability', 'total probability', 'marginal distribution']", Random Variables,seg_5,"we might also be interested in computing the waiting time distribution without referring to the number of heads. this marginal distribution can be derived by applying the law of total probability. for example,"
128,1,"['marginal', 'marginal probability', 'probability']", Random Variables,seg_5,is the marginal probability that we see the first head in the second flip.
129,1,"['independent', 'case', 'probability']", Random Variables,seg_5,"example 4 to contribute another example, let us consider the following problem, encountered in molecular biology: dna molecules carrying the inheritance information of an organism can be modeled as a sequence of nucleotides. there are four different such nucleotides: arginine (abbreviated a), cytosine (c), guanine (g), and tyrosine (t). a common problem is to determine how closely related two dna sequences are. to make things easier, let us assume both sequences have the same length n, that the nucleotides in any two positions in the sequence are independent, and that each nucleotide has a probability of 1/4 to occur in any position. similarity of the sequences can then be established by counting in how many positions the two sequences have the same nucleotide. each such case is called a match, the converse a mismatch, so the following two sequences have seven matches and three mismatches (underlined):"
130,0,[], Random Variables,seg_5,a c c g t t g g t a a c g g t t c g a a
131,1,"['distribution', 'cases']", Random Variables,seg_5,"if the two sequences have nothing in common, we would expect to see a match in about 1/4 of the cases, and the number of matches would follow a binom(n, p = 1/4) distribution. conversely, evolutionarily related dna sequences would show a much higher proportion of matches."
132,1,"['estimate', 'frequencies', 'data', 'test', 'hypothesis']", Random Variables,seg_5,"in subsequent chapters, we will estimate the nucleotide frequencies p from data and test the hypothesis that sequences are related by comparing the observed and expected number of matches"
133,1,"['probability mass function', 'set', 'probability', 'random', 'function', 'mass function', 'events', 'density function', 'densities', 'continuous random variables', 'probability density function', 'continuous', 'random variables', 'variables']", Random Variables,seg_5,"continuous random variables. we also need random variables that take values in a continuous set to describe, e.g., measured lengths or optical densities. similar to events, we cannot cover these in all mathematical rigor. a nontrivial mathematical argument shows that for such a continuous random variables x, a probability mass function cannot be defined properly, because p(x = x) = 0 for all x. instead, most of these variables have a probability density function (pdf) fx (x) with the properties"
134,0,[], Random Variables,seg_5,"fx (x) ≥ 0 , ∞"
135,0,[], Random Variables,seg_5,∫ fx (y)dy = 1.
136,1,"['density function', 'interval', 'probability', 'function']", Random Variables,seg_5,"the density is a function such that the probability of x to take a value in any interval [xl , xu] is given by the area under the density function on this interval, that is, p(xl ≤"
137,0,[], Random Variables,seg_5,x ≤ xu) = ∫x
138,1,"['cumulative distribution function', 'distribution function', 'distribution', 'probability', 'function']", Random Variables,seg_5,u fx (y)dy. this probability can also be written in terms of the cumulative distribution function
139,0,[], Random Variables,seg_5,fx (x) = p(x ≤ x) = ∫ fx (y)dy
140,1,"['normal distributions', 'statistics', 'continuous distributions', 'normal', 'exponential', 'continuous', 'distributions']", Random Variables,seg_5,"as the difference fx (xu) − fx (xl). important continuous distributions include the exponential (see ex. 5 below) and the normal distributions (covered in sect. 1.3). in sect. 1.4, we will discuss several more distributions that frequently arise in statistics, like the t-, the χ2- and the f-distributions, and also demonstrate various relations between them."
141,1,"['density function', 'exponential distribution', 'continuous', 'continuous random variable', 'distribution', 'random variable', 'variable', 'exponential', 'random', 'function', 'event']", Random Variables,seg_5,"example 5 as a first example for a continuous random variable, let us consider the exponential distribution. this distribution often describes the waiting time w for an event such as a radioactive decay and has density function"
142,0,[], Random Variables,seg_5,"fw (w; λ) = λ exp(−λw),"
143,1,"['rate', 'cumulative distribution function', 'distribution function', 'distribution', 'parameter', 'event', 'function', 'average']", Random Variables,seg_5,where the rate λ > 0 is the distribution’s only parameter and 1/λ describes the average waiting time for the next event. the cumulative distribution function is easily calculated as
144,0,[], Random Variables,seg_5,fw (w; λ) = 1 − exp(−λw).
145,1,"['density function', 'random variable', 'variable', 'probability', 'random', 'function']", Random Variables,seg_5,"figure 1.4 shows the density function for λ = 2, the area of the gray region gives the probability that a random variable w ∼ exp(2) takes a value between 0.5 and 2, which we calulate to be"
146,0,[], Random Variables,seg_5,p(0.5 ≤ w ≤ 2) = ∫ 2 × exp(−2 × w)dw = 0.3496.
147,1,"['continuous distribution', 'interval', 'uniform distribution', 'distribution', 'continuous']", Random Variables,seg_5,"example 6 another example for a continuous distribution is the uniform distribution, which has the same density for each point in a certain interval. if u ∼ unif([a, b]), the density is given by"
148,0,[], Random Variables,seg_5,"1 , if a ≤ u ≤ b, fu (u) = { 0b,−a else."
149,1,"['probabilities', 'probability']", Random Variables,seg_5,it is important to understand that the probability density values cannot be interpreted as probabilities. in particular
150,0,[], Random Variables,seg_5,fx (x) ≤ 1.
151,1,"['density function', 'interval', 'random variable', 'variable', 'probability', 'random', 'function']", Random Variables,seg_5,"as an easy counterexample, let us consider a uniform random variable u on an interval [a, b]. for the interval [a, b] = [0, 0.5], the density function is fu (u) = 2 for each value u inside the interval. moreover, by moving the right boundary b of the interval towards a, we can make fu (u) arbitrarily large. thus, it clearly cannot be interpreted as a probability. the integral of the density over any subinterval is of course still a probability."
152,1,"['density function', 'continuous distribution', 'probability density function', 'parameters', 'continuous', 'gaussian distribution', 'distribution', 'normal', 'probability', 'function', 'normal distribution']", The Normal Distribution,seg_7,"probably the best known continuous distribution is the normal distribution, sometimes also called the gaussian distribution, as it was first completely described by c. f. gauß. this distribution has two parameters, μ and σ 2. its probability density function is"
153,1,"['functions', 'normal distributions', 'density function', 'curve', 'parameters', 'symmetric', 'distribution', 'normal', 'function', 'distributions']", The Normal Distribution,seg_7,"the normal density function has the famous bell-shaped curve and is symmetric around μ; its “width” is determined by σ. figure 1.5 (left) shows the density of three normal distributions with parameters (μ, σ 2) = (0, 1), (0, 3), and (3, 3), respectively. the corresponding cumulative distribution functions are given in fig. 1.5 (right)."
154,1,"['functions', 'standard normal distribution', 'parameters', 'standard normal', 'statistics', 'probability theory', 'distribution', 'normal', 'probability', 'standard', 'normal distribution']", The Normal Distribution,seg_7,"the normal distribution is so important in probability theory and statistics, that its density and cumulative distribution functions even have their own letters reserved for them: φ(x;μ, σ 2) for the pdf and (x;μ, σ 2) for the cdf. if no parameters are given, the two functions refer to the standard normal distribution with μ = 0 and σ = 1."
155,1,"['normally distributed', 'distribution', 'random variable', 'variable', 'normal', 'random', 'normal random variable', 'normal distribution']", The Normal Distribution,seg_7,"one of the helpful properties of the normal distribution is that whenever we scale a normal random variable x by multiplying with a fixed a and then shift it by some fixed value b to the new random variable y = ax + b, this new random variable is also normally distributed: with x ∼ norm(μ, σ 2), we have that y ∼"
156,0,[], The Normal Distribution,seg_7,"norm(aμ + b, a2σ 2). in particular, for x ∼ norm(μx , σx"
157,0,[], The Normal Distribution,seg_7,"2 ),"
158,0,[], The Normal Distribution,seg_7,"x − μx z = ∼ norm(0, 1) σx"
159,1,"['standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", The Normal Distribution,seg_7,has a standard normal distribution.
160,1,"['contrast', 'random variables', 'parameters', 'independent', 'variables', 'distribution', 'random variable', 'variable', 'normal', 'random', 'normal random variable']", The Normal Distribution,seg_7,"moreover, the sum of random variables with the same, arbitrary distribution does in general not have the same distribution as the variables themselves. in contrast, adding independent normal random variables always leads to a new normal random variable. let x1 ∼ norm(μ1, σ12) and x2 ∼ norm(μ2, σ22) be two independent normal variables, potentially with different parameters. then,"
161,0,[], The Normal Distribution,seg_7,"x1 + x2 ∼ norm(μ1 + μ2, σ12 + σ22)."
162,1,"['statistics', 'estimators', 'statistical', 'test statistics', 'test', 'distributions']", Important Distributions and Their Relations,seg_9,"we will later consider many more distributions that occur in various statistical contexts. in this section, we briefly review some of these and show how they are related among each other. this will later allow us to more easily understand how many distributions of estimators and test statistics are derived."
163,1,"['statistics', 'distributions']", Important Distributions and Their Relations,seg_9,arguably the most frequently encountered distributions in statistics are those in the following list:
164,1,"['degrees of freedom', 'parameters', 'distribution', 'normal', 'parameter', 'normal distribution']", Important Distributions and Their Relations,seg_9,"• normal distribution, with parameters μ and σ 2, • student’s t-distribution, with parameter d, the degrees of freedom, • χ2-distribution, with parameter d, the degrees of freedom, • f-distribution, with parameters m, n, two degrees of freedom."
165,1,"['functions', 'quantile', 'random number', 'density functions', 'statistics', 'distribution', 'random']", Important Distributions and Their Relations,seg_9,"instead of listing their density functions and properties here, we refer to the fact that they are easily available in any statistics package. in r, their distribution functions are accessible via the functions pnorm, pt, pchisq, and pf, respectively, where p can be replaced by d,q,r to get the density and quantile functions, or a random number generator, respectively."
166,1,['distributions'], Important Distributions and Their Relations,seg_9,these distributions are related as follows.
167,0,[], Important Distributions and Their Relations,seg_9,"x−μ • x ∼ norm(μ, σ 2) ⇒ ∼ norm(0, 1)."
168,1,['independent'], Important Distributions and Their Relations,seg_9,"σ • z1, . . . , zn ∼ norm(0, 1) ⇒ ∑in=1 zi2 ∼ χ2(n), if the zi are independent."
169,0,[], Important Distributions and Their Relations,seg_9,"1 • x1 ∼ χ2(n), x2 ∼ χ2(m) ⇒ n1"
170,1,['independent'], Important Distributions and Their Relations,seg_9,"x1 ∼ f(n, m), if x1, x2 are independent. x2 m • z ∼ norm(0, 1), x ∼ χ2(n) ⇒ z ∼ t (n)."
171,1,['independent'], Important Distributions and Their Relations,seg_9,"x √ n1 • x ∼ t (m) ⇒ x2 ∼ f(1, m). • x1 ∼ χ2(n), x2 ∼ χ2(m) ⇒ x1 + x2 ∼ χ2(n +m), if x1, x2 are independent. • xn ∼ t (n) ⇒ limn→∞ xn ∼ norm(0, 1)."
172,1,"['random variables', 'random', 'parameters', 'variables', 'standard normal', 'regression analysis', 'distribution', 'normal', 'regression', 'variations', 'standard', 'normally distributed', 'standard normal distribution', 'normal distribution']", Important Distributions and Their Relations,seg_9,"as an example, let us consider normally distributed random variables xi and yi with parameters μx and μy , respectively, and σx = σy = 1. let us further define the new random variables vx = ∑in=1(xi − μx )2 and vy = ∑im=1(yi − μy )2. these will later be called variations, as they measure how “spread out” the values of x and y are. both vx and vy are sums of squares of random variables with standard normal distribution. thus, both vx and vy follow χ2-distributions with parameters n and m, respectively. further, their quotient vx/vy is used in regression analysis; we immediately see that it is a quotient of two χ2-variables and therefore has an f(n, m)-distribution if scaled appropriately."
173,1,"['degrees of freedom', 'standard', 'standard normal', 'random variable', 'variable', 'normal', 'standard normal random variable', 'random', 'normal random variable']", Important Distributions and Their Relations,seg_9,"of particular importance is student’s t-distribution, which is the quotient of a standard normal random variable and a scaled χ2-variable. for n − 1 degrees of freedom, the t (n − 1)-distribution has density"
174,0,['n'], Important Distributions and Their Relations,seg_9,( n+ 2 1) f (t; n) = n+1 .
175,0,[], Important Distributions and Their Relations,seg_9,√nπ n2 (
176,0,[], Important Distributions and Their Relations,seg_9,1 + tn
177,1,"['densities', 'degrees of freedom', 'standard normal', 'distribution', 'normal', 'standard', 'tails']", Important Distributions and Their Relations,seg_9,"the densities of this distribution, with n = 2 and n = 20 degrees of freedom, are given in fig. 1.6 together with the standard normal density. with increasing n, the t-distribution approaches the standard normal as claimed, but has substantially heavier tails for few degrees of freedom."
178,1,"['quantile', 'cumulative distribution function', 'interval', 'distribution function', 'quantiles', 'distribution', 'random variable', 'variable', 'probability', 'random', 'function']", Quantiles,seg_11,"while the cumulative distribution function fx (x) describes the probability of a random variable x to take a value below a certain value x, the quantiles describe the converse: the α-quantile is the value x such that fx (x) = α, i.e., the value for which the random variable has a probability of α to take that or a lower value. slight difficulties might arise if there is not an exact value x but a whole interval, but the interpretation remains the same. the quantile function is then (neglecting technicalities) given by fx−1(α), the inverse function of the cdf. thus, if p(x ≤ q) = α, then q is the α-quantile of x."
179,1,"['standard normal', 'distribution', 'symmetry', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Quantiles,seg_11,"the 0.05- and the 0.8-quantile of a standard normal distribution are given in fig. 1.7 (left and right, respectively) as vertical dashed lines. the α-quantile of the standard normal distribution is denoted by zα, thus z0.05 = −1(0.05) ≈ −1.645 and z0.8 = −1(0.8) ≈ 0.842. using the symmetry of the standard normal distribution, z1−α = −zα."
180,1,"['cumulative distribution function', 'distribution function', 'distribution', 'random variable', 'variable', 'expectation', 'random', 'function', 'variance']", Moments,seg_13,"while the distribution of a random variable is completely described by the cumulative distribution function or the density/mass function, it is often helpful to describe its main properties by just a few key numbers. of particular interest are the expectation and the variance."
181,1,"['expected value', 'expectation', 'mean']", Expectation,seg_15,"the expectation, expected value, or mean is the number"
182,0,[], Expectation,seg_15,e(x) := ∫ x f (x)dx
183,1,['continuous'], Expectation,seg_15,"for a continuous, and"
184,0,[], Expectation,seg_15,e(x) := kp(x = k) ∑ k=−∞
185,1,"['discrete random variable', 'random variables', 'variables', 'discrete', 'location', 'random variable', 'variable', 'expectation', 'random']", Expectation,seg_15,"for a discrete random variable x. the expectation describes the location of the distribution, which informally is the center value around which the possible values of x disperse; it is often denoted by the letter μ. the expectation behaves nicely when summing random variables or multiplying them with constants. for random variables x, y and a non-random number a we have:"
186,0,[], Expectation,seg_15,"e(x + y ) = e(x) + e(y ),"
187,0,[], Expectation,seg_15,"e(ax) = ae(x),"
188,1,['independent'], Expectation,seg_15,"e(xy ) = e(x)e(y ) if x,y are independent."
189,1,"['distribution', 'random variable', 'variable', 'expected value', 'random', 'variance']", Variance and Standard Deviation,seg_17,the variance of a random variable x describes how much its values disperse around the expected value and is a measure for the width of its distribution. it is defined as
190,0,[], Variance and Standard Deviation,seg_17,"var(x) := e((x − e(x))2),"
191,1,"['expected value', 'mean']", Variance and Standard Deviation,seg_17,the mean squared distance of values to the expected value and is often denoted by σ 2. a short calculation gives the alternative description
192,0,[], Variance and Standard Deviation,seg_17,var(x) = e(x2) − (e(x))2 .
193,1,"['linear', 'random variables', 'variables', 'random', 'variance']", Variance and Standard Deviation,seg_17,"the variance is not linear and we have the following relations for random variables x, y and non-random numbers a, b:"
194,1,['independent'], Variance and Standard Deviation,seg_17,"var(x + y ) = var(x) + var(y ) if x,y are independent,"
195,0,[], Variance and Standard Deviation,seg_17,"var(x + b) = var(x),"
196,0,[], Variance and Standard Deviation,seg_17,var(ax) = a2var(x).
197,1,"['deviation', 'random', 'factor', 'information', 'distribution', 'variable', 'random variable', 'scale of a distribution', 'standard', 'standard deviation', 'variance']", Variance and Standard Deviation,seg_17,"the square-root σ is called the standard deviation. while it does not contain any more information than the variance, it is often more convenient for applications, as it is easier to interpret and has the same physical units as the random variable itself. it is a measure for the scale of a distribution, as rescaling x by any factor a changes the standard deviation by the same factor."
198,1,"['probability', 'random', 'mean', 'standard', 'standard deviation', 'parameters', 'distribution', 'variance', 'deviation', 'random variables', 'probabilities', 'standard deviations', 'variables', 'deviations', 'normal', 'normal distribution']", Variance and Standard Deviation,seg_17,"expectation and variance completely specify a normal distribution, whose two parameters they are. for x ∼ norm(μ, σ 2), the following approximations are often useful: the probability of x taking a value x at most one standard deviation away from the mean, i.e., x ∈ [μ− σ,μ+ σ ], is roughly 70%. similarly, the probabilities of observing a value at most 2, respectively 3 standard deviations from the mean, are roughly 95% and 99%, respectively. note that these probabilities can be very different for non-normal random variables."
199,1,"['continuous distribution', 'function', 'gamma distribution', 'rate', 'symmetric', 'mean', 'parameter', 'event', 'functions', 'density function', 'gamma', 'density functions', 'distribution', 'expectation', 'moments', 'continuous', 'variance', 'normal', 'shape parameter', 'exponentially distributed', 'normal distribution', 'exponentially']", Variance and Standard Deviation,seg_17,"example 7 for introducing yet another continuous distribution on the go, let us consider the gamma distribution with shape parameter k and scale parameter θ. it has density function f (x; k, θ) = xk−1 exp(−x/θ)/θk (k), is only defined for x > 0, and describes the distribution of the sum of k exponentially distributed waiting times, each with rate parameter θ (thus the time to wait for the kth event). this density function is usually not symmetric. for k = 2 and θ = 2, the distribution has expectation μ = kθ = 4 and variance σ 2 = kθ2 = 8; its density is shown in fig. 1.8 (solid line). for comparison, a normal distribution with the same expectation and variance is plotted by a dashed line. as we can see, the density functions look very different, although both have the same mean and variance. for additionally capturing their different shapes, higher moments are needed (see sect. 1.6.5)."
200,1,"['model', 'probabilities', 'independence', 'probability', 'random']", Variance and Standard Deviation,seg_17,"example 8 let us consider the following model of a random dna sequence as introduced earlier: we assume independence among the nucleotides and in each position, the probabilities of having a particular nucleotide are pa, pc , pg , pt , respectively. we investigate two sequences of length n by comparing the nucleotides in the same position. assume that the sequences are completely random and unrelated. at any position, the probability of a match is then p := p2a + pc"
201,0,[], Variance and Standard Deviation,seg_17,"2 + pg 2 + pt 2 , as both"
202,1,['set'], Variance and Standard Deviation,seg_17,nucleotides have to be the same. let us set mi = 1 if the sequences match in position i and mi = 0 else.
203,0,[], Variance and Standard Deviation,seg_17,"to decide whether two given sequences are related, we compute the number of matching nucleotides and compare it to the number of matches we expect just by chance. if the observed number is much higher than the expected number, we claim"
204,0,[], Variance and Standard Deviation,seg_17,1 that the sequences are in fact related.
205,1,"['random sequences', 'distribution', 'expectation', 'binomial', 'random', 'binomial distribution']", Variance and Standard Deviation,seg_17,"the total number of matches in two random sequences of length n is given by m := m1 + · · · + mn and follows a binomial distribution: m ∼ binom(n, p). applying the linearity of the expectation and some algebra, we compute the expected number of matches:"
206,0,[], Variance and Standard Deviation,seg_17,e(m) = kp(m = k) ∑ k=−∞
207,0,[], Variance and Standard Deviation,seg_17,= ∑ k ( k ) pk(1 − p)n−k k=0
208,0,['n'], Variance and Standard Deviation,seg_17,n (n − 1)! = ∑ np pk−1(1 − p)(n−1)−(k−1) (k − 1)!((n − 1) − (k − 1))! k=1 n
209,0,['n'], Variance and Standard Deviation,seg_17,∑ k=1 ( n k −
210,0,[], Variance and Standard Deviation,seg_17,− 1
211,0,[], Variance and Standard Deviation,seg_17,1 )
212,0,[], Variance and Standard Deviation,seg_17,= np pk−1(1 − p)(n−1)−(k−1)
213,0,[], Variance and Standard Deviation,seg_17,"= np,"
214,1,"['probabilities', 'variable', 'probability']", Variance and Standard Deviation,seg_17,"where the last equality holds because we have the pmf of a binom (n −1, p) variable in the sum, which sums to one. the result also makes intuitive sense: the expected number of matches is the proportion p of matches times the number of nucleotides n. consequently, for sequences of length n = 100, with nucleotide probabilities all equal to 0.25, the probability of a match is p = 0.25 and we expect to see 25 matches just by chance if the sequences are unrelated."
215,1,"['deviation', 'information', 'expected value', 'variance']", Variance and Standard Deviation,seg_17,"how surprised are we if we observe 29 matches? would this give us reason to conclude that the sequences might in fact be related? to answer these questions, we would need to know how likely it is to see a deviation of 4 from the expected value. this information is captured by the variance, which we can calculate as"
216,0,[], Variance and Standard Deviation,seg_17,"var(m) = var(m1) + · · · + var(mn),"
217,1,"['variance', 'independent']", Variance and Standard Deviation,seg_17,"because we assumed that the nucleotides are independent among positions. using the definition of the variance,"
218,0,[], Variance and Standard Deviation,seg_17,"var(m1) = e((m1)2)−(e(m1))2 = (02 ×(1− p)+12 × p)− p2 = p(1− p),"
219,0,[], Variance and Standard Deviation,seg_17,and we immediately get
220,0,[], Variance and Standard Deviation,seg_17,var(m) = nvar(m1) = np(1 − p) = 18.75
221,1,"['deviation', 'range', 'standard', 'standard deviation']", Variance and Standard Deviation,seg_17,"and a standard deviation of 4.33 these values indicate that the deviation of the observed number of matches (=29) from the expected number of matches (=25) is within the range that we would expect to see in unrelated sequences, giving no evidence of the sequences being related. we will see in chap. 3 how these arguments can be used for a more rigorous analysis."
222,1,"['normalized', 'random variable', 'variable', 'expectation', 'random', 'variance']", ZScores,seg_19,"using the expectation and variance of any random variable x, we can also compute a normalized version z with expectation zero and variance one by"
223,0,[], ZScores,seg_19,x − e(x) z = . √var(x)
224,1,"['deviation', 'random', 'standard deviations', 'expected value', 'random variable', 'variable', 'deviations', 'associated', 'standard', 'standard deviation', 'realization']", ZScores,seg_19,"this random variable is sometimes called the z-score. for a given realization x of x, the associated value z of z tells us how many standard deviations σ the value x is away from its expected value. in essence, this rescales to units of one standard deviation."
225,1,"['distribution', 'normal', 'normal distribution']", ZScores,seg_19,"importantly, however, the distribution of z might not belong to the same family as the distribution of x. an important exception is the normal distribution, where z ∼ norm(0, 1) if x ∼ norm(μ, σ 2)."
226,1,"['random variables', 'variables', 'covariance', 'random']", Covariance and Independence,seg_21,"for two random variables x and y, we can compute the covariance"
227,0,['e'], Covariance and Independence,seg_21,"cov(x, y ) = e ((x − e(x))(y − e(y )) ,"
228,1,"['dependent', 'information', 'variable', 'variance']", Covariance and Independence,seg_21,"to measure how much the variable x varies together with the variable y (and viceversa). with this information, we can also calculate the variance of dependent variables by"
229,0,[], Covariance and Independence,seg_21,"var(x + y ) = var(x) + var(y ) + 2cov(x, y )."
230,1,"['covariance', 'independent', 'case']", Covariance and Independence,seg_21,"as a special case, cov(x, x) = var(x). for independent x and y, the covariance is zero. the converse, however, is not true, as the following counterexample demonstrates."
231,1,"['outcomes', 'probability']", Covariance and Independence,seg_21,"example 9 let us consider possible outcomes = {1, 2, 3, 4} and a probability"
232,1,"['random variables', 'variables', 'random']", Covariance and Independence,seg_21,2 1 measure given by p({1}) = p({2}) = 5 and p({3}) = p({4}) = 10 . let us further define the two random variables x and y by
233,0,[], Covariance and Independence,seg_21,ω 1 2 3 4 x 1 −1 2 −2 y −1 1 2 −2
234,1,"['random variables', 'variables', 'dependent', 'random', 'expectations']", Covariance and Independence,seg_21,these two random variables are completely dependent. a simple calculation gives the expectations:
235,0,[], Covariance and Independence,seg_21,4 2 2 1 1 e(x) = ∑ kp(x = k) = 1 × + (−1) × + 2 × + (−2) × 5 5 10 10 k=1
236,0,[], Covariance and Independence,seg_21,"= 0,"
237,0,[], Covariance and Independence,seg_21,e(y ) = 0.
238,1,"['variables', 'covariance']", Covariance and Independence,seg_21,"from this, we calculate the covariance of the two variables as"
239,0,[], Covariance and Independence,seg_21,"cov(x, y ) = e(xy ) − e(x)e(y )"
240,0,[], Covariance and Independence,seg_21,2 2 1 1 = (−1) × + (−1) × + 4 × + 4 × − 0 × 0 5 5 10 10 = 0.
241,1,"['random variables', 'variables', 'dependent', 'covariance', 'random']", Covariance and Independence,seg_21,"thus, although the covariance of the two random variables is zero, they are nevertheless completely dependent."
242,1,"['coefficient', 'correlation coefficient', 'correlation']", Covariance and Independence,seg_21,"another derived measure for the dependency is the correlation coefficient of x and y, given by"
243,0,[], Covariance and Independence,seg_21,"cov(x, y ) r = . √var(x)√var(y )"
244,1,"['correlation', 'variable', 'mean', 'dependence']", Covariance and Independence,seg_21,"the correlation is also often denoted ρ(x, y ) and takes values in [−1, 1], where r = ±1 indicates very strong dependence. even then, however, this does not mean that either x or y cause each other. as an example, the correlation to see a wet street and people carrying an umbrella is likely to be very strong. but carrying an umbrella clearly does not cause the street to be wet. in fact, both are likely caused simultaneously by rainfall, a third variable that was not accounted for. thus, correlation is not causation."
245,1,['moments'], General Moments Skewness and Kurtosis,seg_23,the kth (central) moments are given by
246,0,['e'], General Moments Skewness and Kurtosis,seg_23,e ((x)k) and e ((x − e(x))k) respectively;
247,1,"['variance', 'moment']", General Moments Skewness and Kurtosis,seg_23,the variance is recovered as the second central moment.
248,1,"['deviation', 'normalized', 'moment', 'symmetric', 'mean', 'standard deviation', 'standard', 'skewness']", General Moments Skewness and Kurtosis,seg_23,"the third central moment, normalized by the standard deviation, is called the skewness and describes how symmetric the values spread around the mean by"
249,0,[], General Moments Skewness and Kurtosis,seg_23,x − e(x) skew(x) = e (( √var(x) )3) .
250,1,"['symmetric', 'distribution', 'skewness', 'symmetric distribution']", General Moments Skewness and Kurtosis,seg_23,a negative skewness indicates that the distribution “leans” towards the left and a perfectly symmetric distribution has skewness zero.
251,1,"['density function', 'normalized', 'moment', 'kurtosis', 'function', 'tail']", General Moments Skewness and Kurtosis,seg_23,the (normalized) fourth central moment is called the kurtosis and describes how fast the density function approaches zero in the left and right tail by
252,0,[], General Moments Skewness and Kurtosis,seg_23,x − e(x) kurtosis(x) = e (( √var(x) )4) − 3.
253,1,"['kurtosis', 'distribution', 'deviations', 'normal', 'mean', 'variance', 'normal distribution']", General Moments Skewness and Kurtosis,seg_23,"a negative kurtosis indicates that the variance is mostly caused by many values moderately far away from the mean, whereas a positive kurtosis indicates that the variance is determined by few extreme deviations from the mean. the sole reason for subtracting 3 is to make the kurtosis equal to zero for a normal distribution."
254,1,"['density function', 'kurtosis', 'shape parameter', 'expectation', 'parameter', 'function', 'skewness', 'variance']", General Moments Skewness and Kurtosis,seg_23,"for the above gamma-distribution with shape parameter k = 2 and scale parameter θ = 2 (cf. fig. 1.8), we compute a skewness of 1.414 and a kurtosis of 3. together with the expectation and the variance, these numbers often already give a reasonable description of the shape of the density function."
255,1,"['random variables', 'variables', 'distribution', 'normal', 'random', 'limit', 'normal distribution']", Important Limit Theorems,seg_25,"the sum of many similar random variables is of particular interest in many applications. in this section, we will discuss two important limit theorems that allow us to compute its distribution in a variety of situations and explain the omnipresence of the normal distribution in applications."
256,1,"['sample', 'random variables', 'independent', 'variables', 'arithmetic mean', 'random sample', 'law of large numbers', 'distribution', 'random variable', 'variable', 'expectation', 'mean', 'random']", Important Limit Theorems,seg_25,"the first theorem gives the law of large numbers (lln). consider a random sample x1, . . . , xn, described by n random variables. we assume that each random variable has the same distribution, and all are independent from each other, a property called independent and identically distributed (iid). in particular, they all have the same expectation e(x1) = · · · = e(xn). the theorem then says that if we take their arithmetic mean x̄ , it approaches the expectation as we increase n:"
257,0,['n'], Important Limit Theorems,seg_25,x1 + · · · + xn x̄ = → e(x1) as n → ∞. n
258,1,"['arithmetic mean', 'statistics', 'distribution', 'expectation', 'mean']", Important Limit Theorems,seg_25,"this theorem thus gives one reason why expectation and arithmetic mean are so tightly linked in statistics. importantly, the theorem does not require the xi to have any particular distribution."
259,1,"['random variables', 'random', 'variables', 'distribution', 'random variable', 'variable', 'normal', 'expectation', 'central limit theorem', 'variance', 'limit', 'normal distribution']", Important Limit Theorems,seg_25,"the second theorem is the central limit theorem (clt), which gives the reason for the omnipresence of the normal distribution. in essence, it tells us that if we sum up iid random variables, the sum itself will eventually become a random variable with a normal distribution, no matter what was the distribution of the individual random variables. let us again assume iid random variables xi , having any distribution with expectation μ and variance σ 2. then,"
260,0,['n'], Important Limit Theorems,seg_25,"∑in=1 xi − nμ → norm(0, 1) as n → ∞, √nσ"
261,0,[], Important Limit Theorems,seg_25,"or, equivalently,"
262,0,['n'], Important Limit Theorems,seg_25,"x̄ − μ √n → norm(0, 1) as n → ∞. σ"
263,1,"['sample', 'descriptive statistics', 'random sample', 'statistics', 'data', 'information', 'distribution', 'mean', 'plotting', 'random', 'variance']", Visualizing Distributions,seg_27,"given a random sample drawn from a distribution, it is often helpful to visualize this data and some of its properties like the mean and variance. plotting the sample points together with a given theoretical distribution often provides enough information to decide whether the distribution “fits” the data or not, i.e., if the data might have been drawn from this distribution. such descriptive statistics are a large topic in themselves, and we will only present some examples that are helpful in later chapters."
264,1,"['sample', 'parameters', 'quantiles', 'data', 'distribution', 'set', 'normal', 'moments', 'function', 'normal distribution']", Summaries,seg_29,"a first impression of a given set of data is given by simply stating some of the key moments and quantiles of the data. in r, these are directly computed using the summary() function. for 50 sample points from a normal distribution with parameters μ = 10 and σ 2 = 6, we get the following output:"
265,1,"['mean', 'median']", Summaries,seg_29,min. 1st qu. median mean 3rd qu. max. 5.137 8.795 9.953 10.070 11.220 15.030
266,1,"['expected value', 'quartile', 'median']", Summaries,seg_29,"in this summary, the minimal and maximal values are given together with the expected value and the 0.25-quantile (1st quartile), the 0.5-quantile (called the median and the 2nd quartile), and the 0.75-quantile (3rd quartile)."
267,1,"['exponential distribution', 'distribution', 'samples', 'exponential']", Summaries,seg_29,"similarly, for 50 samples from an exponential distribution with parameterλ = 0.4:"
268,1,"['mean', 'median']", Summaries,seg_29,min. 1st qu. median mean 3rd qu. max. 0.06219 0.60800 1.29100 2.54000 3.14100 19.09000
269,1,"['sample', 'data', 'symmetry', 'normal', 'exponential', 'mean', 'skewness']", Summaries,seg_29,"these summaries already reflect the symmetry of the normal data around their mean and the skewness of the exponential, which has most of its sample points at small values, but also contains comparably few large values."
270,1,"['functions', 'data', 'distribution', 'samples', 'normal', 'plotting', 'normal distribution']", Plotting Empirical Distributions,seg_31,"let us assume that we gathered some data and assume it follows a normal distribution. plotting the empirical and the theoretical density or cumulative distribution functions then gives a first impression whether this might be true. for example, fig. 1.9 gives these functions for n = 50 normal samples xi ∼ norm(10, 6)."
271,1,"['sample', 'density function', 'estimated', 'cumulative distribution function', 'distribution function', 'empirical cumulative distribution function', 'data', 'distribution', 'function']", Plotting Empirical Distributions,seg_31,"the empirical density function of the data is estimated by summing “smeared out” versions of the sample points, such as by assuming a gaussian bell-curve over each point and summing up the individual values of all these curves. the empirical cumulative distribution function (ecdf) is computed by the function"
272,0,['n'], Plotting Empirical Distributions,seg_31,"hn(x) f̂n(x) = , n"
273,1,"['sample', 'step function', 'function']", Plotting Empirical Distributions,seg_31,"where hn(x) is the number of sample points smaller than x. this leads to a step function, which in the example quite closely follows the theoretical function."
274,1,"['functions', 'densities', 'distribution']", Plotting Empirical Distributions,seg_31,"in practice, the similarity of the empirical and theoretical densities or cumulative distribution functions is often very difficult to judge by eye."
275,1,"['functions', 'quantile', 'parameters', 'empirical quantiles', 'quantiles', 'data', 'distribution', 'plotting', 'distributions']", QuantileQuantile Plots,seg_33,"another way of comparing two distributions is by plotting their quantile functions. this is extremely helpful when plotting the theoretical quantiles of an assumed distribution against the empirical quantiles of some data. for this, all parameters of the theoretical distribution have to be specified."
276,1,"['data', 'distribution', 'samples', 'normal', 'parameter', 'normal distribution']", QuantileQuantile Plots,seg_33,"an important exception is the normal distribution, which can be compared to samples without knowing its parameter values. here is how this works: we sort the data x1, . . . , xn such that x(1) ≤ · · · ≤ x(n) and thus x(i) is the ith smallest value of"
277,1,"['distribution', 'data', 'dataset']", QuantileQuantile Plots,seg_33,"i the dataset, which is the best guess for the -quantile of the distribution. if the data"
278,1,"['parameters', 'quantiles', 'distribution', 'normal', 'normal distribution']", QuantileQuantile Plots,seg_33,"n actually stem from a normal distribution with some unknown parameters μ and σ 2, the quantiles relate by"
279,0,[], QuantileQuantile Plots,seg_33,"x(i) ≈ μ + σ × zi/n,"
280,1,"['plot', 'standard normal', 'intercept', 'distribution', 'normal', 'parameter', 'standard', 'standard normal distribution', 'normal distribution', 'slope']", QuantileQuantile Plots,seg_33,"where zi/n is the theoretical i/n-quantile of the standard normal distribution. regardless of the actual parameter values, this is the equation of a line with slope σ and intercept μ. if we therefore plot the points"
281,0,[], QuantileQuantile Plots,seg_33,"(x(i), zi/n) ,"
282,0,[], QuantileQuantile Plots,seg_33,"we expect to see a straight line, regardless of the values of μ and σ 2."
283,1,"['plot', 'sample', 'exponential distribution', 'empirical quantiles', 'quantiles', 'distribution', 'normal', 'exponential', 'tails', 'normal distribution']", QuantileQuantile Plots,seg_33,"a quantile-quantile plot for a normal sample is given in fig. 1.10 (left) together with the theoretical quantiles (solid line). for comparison, sample points from an exponential distribution are plotted together with the normal distribution quantiles in fig. 1.10 (right). as we can see, the agreement of the theoretical and the empirical quantiles is quite good for the normal sample, but it is poor for the exponential sample, especially in the tails."
284,1,"['functions', 'plots', 'qqnorm', 'distribution', 'normal', 'normal distribution']", QuantileQuantile Plots,seg_33,quantile plots can be generated inr using the functionqqplot(). the functions qqnorm() and qqline() allow comparison to the normal distribution.
285,1,"['range', 'sample', 'normalized', 'quantiles', 'data', 'samples', 'symmetry', 'exponential', 'mean', 'standard', 'standard deviation', 'skewness', 'error', 'distributions', 'parameters', 'median', 'error bar', 'expectation', 'plot', 'deviation', 'barplots', 'barplot', 'boxplot', 'normal']", Barplots and Boxplots,seg_35,"it is still very popular to give data in terms of a bar with the height corresponding to the expectation and an additional error bar on top to indicate the standard deviation. however, this only shows two key numbers of the whole data (expectation and standard deviation), and does not allow to see how the data actually distribute. a much more informative alternative to plot data is to use the boxplot. it shows several parameters simultaneously: a rectangle denotes the positions of the 0.25- and 0.75- quantiles, with a horizontal line in the box showing the median (0.5-quantile). thus, the middle 50% of the data are contained in that rectangle. on the top and bottom of the rectangle, the “whiskers” show the range of 1.5 times the distance between the 0.25- and 0.75-quantiles. sample points outside this range are plotted individually. the previous normal and exponential data are both normalized to mean 2 and standard deviation 1 and the resulting data are shown as a barplot (left) and boxplot (right) in fig. 1.11. in the barplot, no difference between the two samples can be noticed, while the different distributions of the data, the skewness of the exponential, and the symmetry of the normal are immediately recognized in the boxplot. barplots with"
286,1,"['functions', 'error bars', 'barplot', 'boxplot', 'boxplots', 'error']", Barplots and Boxplots,seg_35,"out error bars and boxplots can be generated in r using the functions barplot() and boxplot(), respectively."
287,1,"['combinations', 'probability measure', 'events', 'probability']", Summary,seg_37,"probability theory allows us to study the properties of non-deterministic quantities. by defining a probability measure, we can compute the probability of events and their combinations."
288,1,"['probability mass function', 'distribution function', 'discrete', 'probability', 'random', 'function', 'cumulative distribution function', 'mass function', 'random variable', 'density function', 'continuous random variables', 'distribution', 'continuous', 'random variables', 'variables', 'variable']", Summary,seg_37,"a random variable’s distribution is given by its cumulative distribution function and the probability mass function for discrete and the density function for continuous random variables. importantly, the density fx (x) is not a probability. for a distribution of a random variable x, we can compute the α-quantile as the value qα such that p(x ≤ qα) = α."
289,1,"['geometric distribution', 'discrete', 'discrete distributions', 'distribution', 'continuous distributions', 'normal', 'exponential', 'binomial', 'continuous', 'statistical', 'geometric', 'normal distribution', 'distributions']", Summary,seg_37,"important discrete distributions are the binomial and geometric distribution, important continuous distributions are the normal distribution, the exponential, and various statistical distributions, including the t-, f-, and χ2-distributions, which are all related."
290,1,"['kurtosis', 'location', 'distribution', 'tails', 'probability distribution', 'expectation', 'moments', 'probability', 'skewness', 'variance']", Summary,seg_37,"several interesting properties of a probability distribution are given by its moments, some of which are the expectation, describing the location, the variance, describing the scale and the skewness and kurtosis, describing the asymmetry and heaviness of the tails, respectively."
291,1,"['plot', 'sample', 'random samples', 'random sample', 'normally distributed', 'samples', 'random', 'distributions']", Summary,seg_37,"we can visualize empirical distributions of given random samples using various graphs, such as barand box-plots and the quantile-quantile plot. the latter also allows us to easily assess if a given random sample is normally distributed."
292,1,"['sample', 'confidence intervals', 'method', 'contaminations', 'estimate', 'estimation', 'bootstrap', 'random sample', 'estimators', 'distribution', 'intervals', 'random', 'confidence']",Chapter  Estimation,seg_39,"abstract estimation is the inference of properties of a distribution from an observed random sample. estimators can be derived by various approaches. to quantify the quality of a given estimate, confidence intervals can be computed; the bootstrap is a general purpose method for this. vulnerability of some estimators to sample contaminations leads to robust alternatives."
293,1,"['confidence interval', 'interval', 'bootstrap', 'confidence']",Chapter  Estimation,seg_39,keywords maximum-likelihood · confidence interval · bootstrap
294,1,"['independent', 'parameters', 'random samples', 'observation', 'distribution', 'samples', 'random', 'function', 'estimator']", Introduction,seg_41,"we assume that n independent and identically distributed random samples x1, . . . , xn are drawn, whose realizations form an observation x1, . . . , xn . our goal is to infer one or more parameters θ of the distribution of the xi . for this, we construct an estimator θ̂n by finding a function g, such that"
295,0,[], Introduction,seg_41,"θ̂n = g(x1, . . . , xn)"
296,1,"['confidence intervals', 'estimate', 'data', 'intervals', 'distribution', 'random', 'confidence']", Introduction,seg_41,"is a “good guess” of the true value θ. since θ̂n depends on the data, it is a random variable. finding its distribution allows us to compute confidence intervals that quantify how likely it is that the true value θ is close to the estimate θ̂n ."
297,1,"['random sequences', 'data', 'random variable', 'variable', 'probability', 'random']", Introduction,seg_41,"example 10 let us revisit the problem of sequence matching from example 8 (p. 19) we already know that the number of matches in two random sequences is a random variable m ∼ binom (n, p), but do not know the probability p, and want to infer it from given data. for this, let us assume we are given two sequences of length n each, and record the matches m1, . . . , mn, where again mi = 1 if position i is a match, and mi = 0 if it is a mismatch, as well as the total number of matches m = m1+· · ·+mn ."
298,1,"['observations', 'likelihood', 'likelihood function', 'parameter', 'probability', 'function']", Introduction,seg_41,"for any fixed value of p, we can compute the probability to see exactly the observed matches m1, . . . , mn . the main new idea is to consider this probability as a function of the parameter p for given observations. this function is known as the likelihood function"
299,0,[], Introduction,seg_41,"ln(p) = p(m1 = m1, . . . , mn = mn) = ∏ p(mi = mi ) = pm(1 − p)n−m;"
300,1,"['independent', 'likelihood', 'case', 'data', 'joint', 'likelihood function', 'probability', 'function', 'outcome', 'joint probability']", Introduction,seg_41,"note that we can only write the joint probability as a product because we assume the positions (and therefore the individual matches) to be independent. we then seek the value p̂n that maximizes this likelihood and gives the highest probability for the observed outcome. in this sense, it therefore “best” explains the observed data. maximizing the likelihood is straightforward in this case: we differentiate the likelihood function with respect to p and find its roots by solving the equation"
301,0,[], Introduction,seg_41,∂ln(p) = 0. ∂p
302,0,[], Introduction,seg_41,"taking the derivative of ln(p) requires repeated application of the product-rule. it is therefore more convenient to use the log-likelihood for the maximization, given by"
303,0,['n'], Introduction,seg_41,n(p) = log ln(p) = ∑ log p(mi = mi ) = m log(p) + (n − m) log(1 − p).
304,1,"['function', 'case']", Introduction,seg_41,"maximizing either ln(p) or n(p) yields the exact same result, as the logarithm is a strictly increasing function, but we can conveniently differentiate each summand individually in the log-likelihood. in our case,"
305,0,['n'], Introduction,seg_41,"∂ n(p) 1 1 0 = ∂p = m p + (n − m)(−1 − p ) ,"
306,0,[], Introduction,seg_41,which gives
307,0,['n'], Introduction,seg_41,m n − m m = ⇐⇒ p = . p 1 − p n
308,1,"['parameter', 'estimate']", Introduction,seg_41,"thus, the desired estimate of the parameter value p is p̂n = m/n, the proportion of matches in the sequence."
309,1,"['random', 'function', 'estimator', 'experiment', 'estimate', 'data', 'random variable', 'samples', 'parameter', 'model', 'outcome', 'contrast', 'independent', 'variable', 'realization']", Introduction,seg_41,"it is important to understand the fundamental difference between the parameter p and its estimate p̂n : the parameter p is a fixed number, relating to the model describing the experiment. it is independent of the particular outcome m of the experiment. in contrast, its estimate p̂n is a function of the data and takes different values for different samples. for studying general properties of this estimator, we will therefore consider p̂n as the random variable m/n rather than its realization m/n. it then has"
310,1,"['experiment', 'estimate', 'distribution', 'realization']", Introduction,seg_41,"a distribution and if we were to repeat the same experiment over and over, p would always be the same, but the estimate would yield a different realization of p̂n each time."
311,1,"['estimator', 'distribution', 'random variable', 'variable', 'expectation', 'moments', 'random', 'variance']", Introduction,seg_41,"because an estimator is a random variable, it is helpful to either compute its entire distribution or some of its moments. for our example, we can easily work out the expectation and the variance of our estimator:"
312,0,"['n', 'e']", Introduction,seg_41,"m 1 np e( p̂n) = e ( n ) = n e(m) = n = p,"
313,1,"['parameter', 'estimator', 'unbiased']", Introduction,seg_41,"which shows that the estimator is unbiased and thus—on average—yields the correct value for the parameter, and"
314,0,['n'], Introduction,seg_41,1 p(1 − p) var( p̂n) = var(m) = . n2 n
315,1,"['sample size', 'sample', 'estimate', 'confident', 'data', 'probability', 'parameter', 'variance', 'estimator']", Introduction,seg_41,"the variance of the estimator decreases with increasing sample size n, which is intuitively plausible: by using more data, we are more confident about the correct value of the parameter p and expect the estimator to get closer to the true value with high probability. we also get a lower variance of the estimate if the variance of the data is smaller."
316,1,"['density function', 'true parameter', 'estimate', 'estimates', 'distribution', 'deviations', 'normal', 'expectation', 'mean', 'parameter', 'function', 'variance', 'estimator', 'normal distribution']", Introduction,seg_41,"the estimator of a true parameter value p = 0.25 is studied in fig. 2.1 on 1000 pairs of unrelated sequences of length 100. on the left, the values of p̂n are given for each such pair. most estimates lie reasonably close to the true value, but there are also some larger deviations. on the right, the empirical density function of p̂n is given (solid line) together with a normal density with the same expectation and variance (dashed line). the values of the estimate closely follow the normal distribution and the mean nicely corresponds to the correct parameter value p."
317,1,"['data', 'parameter', 'function', 'estimator']", Constructing Estimators,seg_43,"to derive an estimator for a parameter θ, we need to construct the function g(x1, . . . , xn). there are multiple methods to do this and we will discuss the maximum-likelihood and the least-squares approach in more depth. both methods rely on finding the parameter value that “best” explains the observed data, but there definition of “best” is different and requires finding the minimum or maximum of a certain function. a third approach, the minimax principle, will be presented in a more general framework in sect. 2.5."
318,1,"['estimation', 'case', 'distribution', 'set', 'binomial', 'binomial distributions', 'distributions']", MaximumLikelihood,seg_45,"to apply maximum-likelihood estimation in the general case, we need to specify a family of distributions that is parametrized by θ such that each value of θ selects one particular distribution from this family. in the previous example, this family was the set of all binomial distributions with fixed n, where each value for p selects one particular member of this family."
319,1,"['density function', 'likelihood', 'likelihood function', 'parameter', 'function']", MaximumLikelihood,seg_45,"here, we consider the density function f (x; θ), describing the family of distributions, and aim at estimating the parameter θ. the likelihood function for this parameter is"
320,0,[], MaximumLikelihood,seg_45,"ln(θ) = ∏ f (xi ; θ), i=1"
321,1,"['sample', 'continuous', 'case', 'discrete', 'distribution', 'continuous distributions', 'joint', 'probability', 'joint probability', 'function', 'distributions']", MaximumLikelihood,seg_45,"which in the discrete case corresponds to the joint probability that the underlying distribution generates the observed sample x1, . . . , xn . for a family of continuous distributions, this product can no longer be directly interpreted as a probability, but the overall reasoning remains the same. the corresponding log-likelihood function is"
322,0,[], MaximumLikelihood,seg_45,n(θ) = log (ln(θ)) = ∑ log ( f (xi ; θ)) . i=1
323,1,"['functions', 'likelihood', 'estimator']", MaximumLikelihood,seg_45,the maximum-likelihood estimator (mle) θ̂n of θ then corresponds to the value that maximizes the likelihood functions:
324,0,[], MaximumLikelihood,seg_45,θ̂n := argmaxθ ln(θ) = argmaxθ n(θ).
325,1,"['parameters', 'estimate', 'normally distributed', 'distribution', 'normal', 'measurements', 'central limit theorem', 'limit', 'normal distribution']", MaximumLikelihood,seg_45,"example 11 let us suppose that we perform n measurements and have good reason to expect them to be normally distributed such that x1, . . . , xn ∼ norm (μ, σ 2). the normal distribution can often be justified with the central limit theorem. we want to estimate both parameters from the n observed values x1, . . . , xn using the"
326,1,"['parameters', 'likelihood', 'likelihood function', 'function']", MaximumLikelihood,seg_45,"maximum-likelihood approach. let us denote the parameters as θ = (μ, σ ) and start with setting up the likelihood function"
327,0,['n'], MaximumLikelihood,seg_45,n n 1 1 (xi − μ)2 1 ln(θ) = π ∏
328,0,[], MaximumLikelihood,seg_45,i=1 σ exp (
329,0,[], MaximumLikelihood,seg_45,− 2σ 2 ) ∝ σ−n exp (−2σ 2 ∑ i=1
330,0,[], MaximumLikelihood,seg_45,"(xi − μ)2) ,"
331,1,['factors'], MaximumLikelihood,seg_45,"where we ignored constant factors in the second equation, as they do not contribute to the maximization (the symbol ∝ means “proportional to”). abbreviating"
332,0,[], MaximumLikelihood,seg_45,"x̄ = n1 ∑ xi and s2 = n1 ∑(xi − x̄)2, we can eliminate the sum and simplify to"
333,0,[], MaximumLikelihood,seg_45,"ns2 n(x̄ − μ)2 −n ln(θ) ∝ σ exp ( −2σ 2 ) exp ( − 2σ 2 ) ,"
334,1,['function'], MaximumLikelihood,seg_45,from which we immediately derive the log-likelihood function
335,0,[], MaximumLikelihood,seg_45,ns2 n(x̄ − μ)2 n(θ) ∝ −n log(σ ) − − . 2σ 2 2σ 2
336,1,['function'], MaximumLikelihood,seg_45,"we maximize this function by taking the derivatives with respect to μ and σ, respectively. for deriving μ̂n, the equation reads"
337,0,['n'], MaximumLikelihood,seg_45,"∂ n(θ) n = − (−2x̄ + 2μ) , ∂μ 2σ 2"
338,1,['estimator'], MaximumLikelihood,seg_45,and finding the roots yields the estimator
339,0,[], MaximumLikelihood,seg_45,−2x̄ + 2μ = 0 ⇐⇒ x̄ = μ.
340,1,"['parameters', 'arithmetic mean', 'estimators', 'expectation', 'mean', 'estimator']", MaximumLikelihood,seg_45,"not surprising, the arithmetic mean is an estimator for the expectation. the maximumlikelihood estimators for the two parameters are then"
341,0,['n'], MaximumLikelihood,seg_45,"n n 1 1 μ̂n = x̄ = ∑ xi and σ̂n2 = s2 = ∑(xi − x̄)2, n n i=1 i=1"
342,0,[], MaximumLikelihood,seg_45,where the derivation of σ̂n follows the same ideas.
343,1,"['estimate', 'estimators', 'distribution', 'normal', 'estimator']", MaximumLikelihood,seg_45,"calculating the distribution of an estimator will become crucial for establishing the bounds of a particular estimate. while this calculation is often difficult for general estimators, maximum-likelihood estimators have the convenient property of being asymptotically normal. formally,"
344,0,['n'], MaximumLikelihood,seg_45,"θ̂n − θ → norm(0, 1) as n → ∞,"
345,0,[], MaximumLikelihood,seg_45,√v ar(θ̂n)
346,1,"['sample size', 'sample', 'distribution', 'normal', 'estimator', 'normal distribution']", MaximumLikelihood,seg_45,"which simply means that if the sample size gets large enough, any maximumlikelihood estimator has a normal distribution. in retrospect, this explains the surprisingly good fit of the empirical and normal density in our introductory example (see fig. 2.1 (right))."
347,1,"['estimate', 'likelihood', 'parameter', 'outcome', 'estimator']", LeastSquares,seg_47,"instead of maximizing the likelihood of the observed outcome, we can also construct an estimator by looking at the distance of the observed outcome and the outcome that we would expect with a particular parameter value. the value that minimizes this distance is then an estimate for the parameter."
348,1,"['observations', 'expected value', 'measurements', 'parameter']", LeastSquares,seg_47,"let us denote by h(θ) the expected value of observations for parameter value θ. with measurements xi , we then minimize the distance"
349,0,[], LeastSquares,seg_47,d(θ) = ∑(xi − h(θ))2. i=1
350,1,"['regression', 'data', 'estimate']", LeastSquares,seg_47,the least-squares estimate (lse) θ̂n is then the value that minimized this squared difference between observed and expected data. this is a very common approach in regression (chap. 4).
351,1,['expected value'], LeastSquares,seg_47,"example 12 let us consider the sequence matching again, this time from a leastsquares perspective, and compare the matches with their expected value. at each position i, the expected value of a match is e(mi ) = p, while the observed match mi is either zero or one. we minimize the sum of their squared differences:"
352,0,[], LeastSquares,seg_47,p̃n = argminp ∑ (mi − p)2 . i=1
353,0,[], LeastSquares,seg_47,minimization is again done by finding the roots of the derivative. a quick calculation reveals
354,0,['n'], LeastSquares,seg_47,n n n n ∂ ∂
355,0,[], LeastSquares,seg_47,∂p ∑
356,0,[], LeastSquares,seg_47,(mi − p)2 = ∂p ( ∑ i=1
357,0,[], LeastSquares,seg_47,mi2 − 2p ∑ i=1
358,0,[], LeastSquares,seg_47,mi + ∑ i=1
359,0,[], LeastSquares,seg_47,"p2) = 0 − 2m + 2np,"
360,0,[], LeastSquares,seg_47,"m which yields p̃n = . in this example, the least-squares and the maximum-likelihood"
361,1,"['estimator', 'case']", LeastSquares,seg_47,"n estimator are identical, but this is not always the case."
362,1,"['estimator', 'data']", Properties of Estimators,seg_49,"in principle, there is no reason why we should not define an estimator θ̂n = g(x1, . . . , xn)= 0, which completely ignores the data. it is a formally valid estimator, but quite useless in practice. the question therefore arises, how we can capture properties of an estimator and conclude that, for example the mle is more useful than the proposed “zero-estimator”?"
363,1,"['sample size', 'sample', 'true parameter', 'estimate', 'consistency', 'parameter', 'estimator']", Properties of Estimators,seg_49,"consistency. the first useful property of an estimator is consistency, which means that with increasing sample size, the estimate approaches the true parameter value:"
364,0,['n'], Properties of Estimators,seg_49,θ̂n → θ as n → ∞.
365,1,"['estimate', 'estimators', 'samples', 'normal', 'binomial', 'estimator']", Properties of Estimators,seg_49,"while all three estimators ( p̂n, x̄ , s2) in the binomial and normal examples are consistent, the above estimator θ̂n ≡ 0 is obviously not, because the estimate does not get any closer to the true value, no matter how many samples we take."
366,1,"['bias', 'estimator', 'estimate']", Properties of Estimators,seg_49,"unbiasedness. even if an estimator is consistent, it might still be that it systematically overor underestimates the true value and introduces a bias in the estimate. the bias is given by the difference of expected and true value"
367,0,[], Properties of Estimators,seg_49,"e(θ̂n) − θ,"
368,1,"['unbiased estimator', 'true parameter', 'sampling', 'parameter', 'average', 'estimator', 'unbiased']", Properties of Estimators,seg_49,"and an estimator is called unbiased if this difference is zero. if we were to repeat the same sampling procedure multiple times, an unbiased estimator would on average neither overnor underestimate the true parameter value."
369,1,"['bias', 'estimators']", Properties of Estimators,seg_49,the following example shows the bias in one of the estimators we constructed earlier.
370,1,"['parameters', 'estimates', 'expectation', 'normal', 'unbiased']", Properties of Estimators,seg_49,example 13 consider the estimates for the parameters μ and σ 2 of a normal distribution as given above. are they unbiased? let us start with x̄ ; its unbiasedness is easily established by exploiting the linearity of the expectation:
371,0,"['n', 'e']", Properties of Estimators,seg_49,n n 1 1 1 e(x̄) = e (n ∑ i=1
372,0,['n'], Properties of Estimators,seg_49,xi) = n ∑ i=1
373,0,['n'], Properties of Estimators,seg_49,e(xi ) = n nμ = μ.
374,0,[], Properties of Estimators,seg_49,the calculation for s2 is slightly more elaborate and we skip some details:
375,0,"['n', 'e']", Properties of Estimators,seg_49,n n 1 1 e(s2) = ∑ e ((xi − x̄)2) = ∑ e ((xi − μ)(x̄ − μ)) n n i=1 i=1
376,0,['n'], Properties of Estimators,seg_49,2 nσ 2 1 = σ 2 − n σ 2 + n2 = σ 2 (1 − n ) = σ 2.
377,1,"['bias', 'variance', 'biased']", Properties of Estimators,seg_49,"the mle for the variance is therefore biased and systematically underestimates the true variance. it is nevertheless consistent, as the bias is proportional to 1/n and decreases rapidly to zero for increasing n."
378,1,"['sample', 'degrees of freedom', 'estimate', 'estimation', 'expectation']", Properties of Estimators,seg_49,"the reason for this can presumably be best explained with the following argument: we use n sample points x1, . . . , xn for estimation and thus divide by n. however, we also use the estimate x̄ instead of the true expectation μ. the value of any sample point is completely determined if we know x̄ and the other remaining points. the degrees of freedom in the estimate are therefore n − 1 rather than n, as we already “used” one degree for estimating μ. indeed,"
379,0,['n'], Properties of Estimators,seg_49,n 1 s2 = σ̂n2 = ∑ (xi − x̄)2 n − 1 i=1
380,1,"['unbiased estimator', 'variance', 'estimator', 'unbiased']", Properties of Estimators,seg_49,"is an unbiased estimator for the variance, but not a maximum-likelihood estimator."
381,1,['estimators'], Properties of Estimators,seg_49,"properties of ml-estimators. conveniently, maximum-likelihood estimators automatically have many desired properties. they are"
382,1,"['sample size', 'sample', 'true parameter', 'parameter', 'function', 'unbiased']", Properties of Estimators,seg_49,"• consistent: they approach the true parameter value with increasing sample size, • equivariant: if r is a function, then r(θ̂n) is also the mle of r(θ), • not necessarily unbiased, so we need to take caution here,"
383,1,['normal'], Properties of Estimators,seg_49,"• asymptotically normal: θ̂n−θ → norm (0, 1) as n → ∞."
384,0,[], Properties of Estimators,seg_49,√var(θ̂n)
385,1,"['estimated', 'estimate', 'estimators', 'consistency', 'samples', 'data', 'information', 'estimator', 'limit']", Confidence Intervals,seg_51,"the discussed properties of estimators provide valuable information for comparing and choosing an estimator, but they say rather little about the quality of a particular estimate. for example, consistency guarantees that the estimated value will approach the true value in the limit, but does not give information on how close it is to the true value, given some data with a certain number of samples."
386,1,"['interval', 'estimate', 'probability', 'confidence', 'confidence interval']", Confidence Intervals,seg_51,"for quantifying the quality of a particular estimate, we can compute a confidence interval (ci) around the estimate θ̂n, such that this interval covers the true value θ with some high probability 1−α. the narrower this interval, the closer we are to the true value, with high probability."
387,1,"['estimator', 'interval']", Confidence Intervals,seg_51,"let us go through the main ideas first, before we look into two concrete examples. for an estimator θ̂n, the (1 − α)-confidence interval is the interval"
388,0,[], Confidence Intervals,seg_51,c = [
389,0,[], Confidence Intervals,seg_51,"θ̂n − l, θ̂n + u] ,"
390,0,[], Confidence Intervals,seg_51,for a lower value l and an upper value u such that
391,0,[], Confidence Intervals,seg_51,p(θ ∈ c) = 1 − α. (2.1)
392,1,"['standard', 'interval', 'estimate', 'standard error', 'data', 'location', 'distribution', 'probability', 'random', 'confidence', 'variance', 'estimator', 'error', 'confidence interval']", Confidence Intervals,seg_51,"this interval c is random, because the value of θ̂n depends on the data. the location of the interval is determined by θ̂n, and its width depends on the distribution of the estimator and in particular on the estimator’s variance var(θ̂n). if the estimator’s variance decreases, the confidence interval gets narrower. this allows us to conclude that the difference of true value and estimate gets smaller with decreasing variance, with high probability. we usually need to work with the square-root of the estimator’s variance, which we call the standard error:"
393,0,[], Confidence Intervals,seg_51,se(θ̂n) = √
394,0,[], Confidence Intervals,seg_51,var(θ̂n).
395,1,"['confidence interval', 'true parameter', 'random', 'interval', 'standard error', 'results', 'distribution', 'random variable', 'variable', 'mean', 'parameter', 'standard', 'confidence', 'estimator', 'error', 'unbiased']", Confidence Intervals,seg_51,"for computing the confidence interval, we start by normalizing the estimator by shifting it by (the unknown) true parameter θ and scaling by 1/se(θ̂n). provided the estimator is unbiased, this normalization simply shifts the estimator’s distribution by its mean and scales by the standard error, which results in a new random variable with mean zero and standard error one. equation 2.1 then becomes"
396,0,[], Confidence Intervals,seg_51,"θ̂n − θ −l u p ( se(θ̂n) ∈ [ se(θ̂n) , se(θ̂n)]) = 1 − α. (2.2)"
397,1,"['unbiased estimator', 'normalized', 'interval', 'quantiles', 'distribution', 'estimator', 'unbiased']", Confidence Intervals,seg_51,"solving (2.2) requires that we find the two quantiles qα/2, q1−α/2 of the distribution of the normalized estimator, with 1−α/2−α/2 = 1−α. from these quantiles, we work out the upper value u = q1−α/2se(θ̂n) and thus the interval bound θ̂n +q1−α/2se(θ̂n), and similar for the lower value l. for an unbiased estimator, the (1 − α)-confidence interval therefore takes the general form"
398,0,[], Confidence Intervals,seg_51,"c = [θ̂n + qα/2se(θ̂n), θ̂n + q1−α/2se(θ̂n)] ,"
399,1,"['estimate', 'symmetric', 'quantiles', 'distribution', 'mean', 'variance', 'estimator', 'symmetric distribution']", Confidence Intervals,seg_51,which simplifies by qα/2 = −q1−α/2 if the estimator additionally has a symmetric distribution around its mean. the two main remaining problems are then to establish the distribution of θ̂n to calculate the quantiles and to estimate its variance.
400,1,"['confidence interval', 'interval', 'unbiased', 'symmetric', 'quantiles', 'distribution', 'normal', 'confidence', 'estimator', 'normal distribution']", Confidence Intervals,seg_51,"if θ̂n is an unbiased maximum-likelihood estimator, we already know that the estimator has a normal distribution and the correct quantiles are zα/2 and z1−α/2. the shifted and scaled interval is then symmetric around zero and the confidence interval is immediately given by"
401,0,[], Confidence Intervals,seg_51,c = [
402,0,[], Confidence Intervals,seg_51,"θ̂n − z1−α/2se(θ̂n), θ̂n + z1−α/2se(θ̂n)] ."
403,1,"['confidence intervals', 'parameters', 'estimates', 'case', 'estimators', 'intervals', 'normal', 'confidence']", Confidence Intervals,seg_51,"before we deal with the more general case of estimators that are not ml, let us first look into two concrete examples and work out confidence intervals for the sequence matching problem and the estimates for the normal parameters."
404,1,"['interval', 'probability', 'estimate']", Confidence Intervals,seg_51,"example 14 we would like to compute an interval [ p̂n − l, p̂n + u] around the estimate p̂n of the matching probability, such that the interval contains the true value p with given probability 1 − α:"
405,0,[], Confidence Intervals,seg_51,"p(p ∈ [ p̂n − l, p̂n + u]) = p( p̂n − l ≤ p ≤ p̂n + u) = 1 − α."
406,1,"['normalized', 'standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'estimator', 'normal distribution']", Confidence Intervals,seg_51,"because p̂n is the maximum-likelihood estimator of p, its distribution approaches a normal distribution for large n. its normalized form has a standard normal distribution:"
407,0,[], Confidence Intervals,seg_51,"p − p̂n ∼ norm (0, 1). se( p̂n)"
408,1,['quantiles'], Confidence Intervals,seg_51,we can therefore immediately solve the following equation by using the corresponding quantiles zα for u and l
409,0,[], Confidence Intervals,seg_51,−l p − p̂n u p ≤ ≤ = 1 − α. ( se( p̂n) se( p̂n) se( p̂n))
410,1,"['distribution', 'symmetry', 'normal', 'normal distribution']", Confidence Intervals,seg_51,"by exploiting the symmetry of the normal distribution, we derive"
411,0,[], Confidence Intervals,seg_51,"u = z1−α/2 ⇐⇒ u = z1−α/2se( p̂n) and l = zα/2se( p̂n), se( p̂n)"
412,1,"['interval', 'standard error', 'standard', 'confidence', 'error', 'confidence interval']", Confidence Intervals,seg_51,"the standard error of p̂n is se( p̂n) = √p(1 − p)/n, leading to the requested confidence interval"
413,0,['n'], Confidence Intervals,seg_51,"p̂n(1 − p̂n) p̂n(1 − p̂n) c = [ p̂n + zα/2√ n , p̂n + z1−α/2√ n ] ,"
414,1,"['true parameter', 'parameter', 'estimate']", Confidence Intervals,seg_51,where we replaced the unknown true parameter value p by its estimate p̂n .
415,1,"['sample', 'confidence interval', 'interval', 'asymptotic', 'approximation', 'case', 'asymptotic normal distribution', 'intervals', 'distribution', 'true distribution', 'normal', 'parameter', 'confidence', 'estimator', 'normal distribution']", Confidence Intervals,seg_51,"an immediate caveat of the approximation of the true distribution of the estimator p̂n by its asymptotic normal distribution is that this confidence interval is only valid for large sample sizes n and parameter values not too close to zero or one. for small p, for example, the confidence interval would also consider the case that p̂n takes on a negative value, which is not possible. hence, the approximations for this confidence interval are not always valid and more sophisticated intervals exist."
416,1,"['estimator', 'unbiased', 'data', 'normally distributed', 'distribution', 'random variable', 'variable', 'normal', 'expectation', 'random', 'variance', 'normal distribution']", Confidence Intervals,seg_51,"example 15 let us consider the estimator for the expectation of normally distributed data, i.e., x̄ = n1 ∑in=1 xi with xi ∼ norm (μ, σ 2). being the ml-estimator, this random variable has a normal distribution. we already checked that it is unbiased, and we easily compute its variance var(x̄) as"
417,0,['n'], Confidence Intervals,seg_51,n n 1 1 1 σ 2 var(x̄) = var (n ∑ i=1
418,0,[], Confidence Intervals,seg_51,xi) = n2 ∑ i=1
419,0,['n'], Confidence Intervals,seg_51,"var(xi ) = n2 nσ 2 = n ,"
420,1,"['normalized', 'estimated', 'independent', 'distribution', 'mean', 'variance']", Confidence Intervals,seg_51,"where we could take the sum outside the variance because we assumed the xi to be independent. thus, the normalized distribution of the difference in true and estimated mean is"
421,0,[], Confidence Intervals,seg_51,"x̄ − μ ∼ norm (0, 1). σ/√n"
422,1,"['variance', 'unbiased', 'estimate']", Confidence Intervals,seg_51,"again, we do not know the true variance and need to estimate is using the unbiased"
423,0,[], Confidence Intervals,seg_51,estimator s2 = n−
424,1,"['normalized', 'random variable', 'variable', 'random']", Confidence Intervals,seg_51,"1 1 ∑in=1(xi − x̄)2, which leads to the normalized random variable"
425,0,[], Confidence Intervals,seg_51,"x̄ − μ,"
426,0,[], Confidence Intervals,seg_51,s/√n
427,1,"['estimated', 'standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'variance', 'normal distribution']", Confidence Intervals,seg_51,"which does not have a standard normal distribution. we can derive its correct distribution by looking at the estimated variance in more detail. in particular, let us consider the quotient of the true and estimated variance:"
428,0,['n'], Confidence Intervals,seg_51,s2 1 ∑in=1(xi − x̄)2 n xi − x̄ 2 (n − 1) = (n − 1) n−1 = . σ 2 σ 2 ∑ i=1 ( σ )
429,1,"['degrees of freedom', 'independent', 'estimate', 'variables', 'standard normal', 'distribution', 'variable', 'normal', 'standard', 'variance']", Confidence Intervals,seg_51,"each summand is the square of a standard normal variable and there are (n − 1) independent such variables. thus, from sect. 1.4, we know that the sum has a χ2-distribution with (n − 1) degrees of freedom. replacing the true variance by its estimate, we derive the distribution"
430,0,['n'], Confidence Intervals,seg_51,"x̄ − μ x̄ − μ σ/√n norm(0, 1) = × ∼ , s/√n σ/√n s/√n 1 2 √n−1χ (n − 1)"
431,1,"['degrees of freedom', 'interval']", Confidence Intervals,seg_51,which from sect. 1.4 we recognize as a t-distribution with (n−1) degrees of freedom. we therefore derive the correct (1 − α)-confidence interval
432,0,[], Confidence Intervals,seg_51,"s s c = [x̄ − t1−α/2(n − 1√n , x̄ + t1−α/2(n − 1)√n ]"
433,1,"['sample size', 'sample', 'interval', 'data', 'expected value', 'variance', 'estimator']", Confidence Intervals,seg_51,"for the estimator x̄ of the expected value. again, this interval gets narrower if we increase the sample size n or decrease the variance σ 2 of the data."
434,1,"['sample', 'estimated', 'confidence intervals', 'interval', 'intervals', 'distribution', 'samples', 'normal', 'mean', 'confidence', 'variance']", Confidence Intervals,seg_51,"as an example, let us repeatedly take 10 samples from a norm(5,16) distribution and compute the corresponding 0.9-confidence interval for the estimated mean x̄ . for each such computation, we derive a slightly different interval, both in terms of the center of the interval (due to the estimated mean x̄ ) and the length of the interval (due to the estimated variance of x̄ ). for 25 repetitions, the confidence intervals are plotted next to each other in fig. 2.2. some intervals, such as the 5th and the 24th, do not cover the true value. to demonstrate the effect of estimating the variance, we compute the correct t-based and the incorrect normal confidence intervals, both using the estimated variance, for the 5th sample (which is too far away from the true mean) as"
435,0,[], Confidence Intervals,seg_51,"ct = [1.396, 4.717] and cnorm = [0.631, 5.482]."
436,1,"['normal', 'quantiles', 'interval']", Confidence Intervals,seg_51,"the normal quantiles overestimate the width of the interval, such that the normal interval contains the true value, while the t-based does not."
437,1,"['sample', 'confidence intervals', 'method', 'estimate', 'asymptotic', 'interval', 'bootstrap', 'data', 'intervals', 'distribution', 'normal', 'confidence', 'variance', 'estimator', 'confidence interval']", The Bootstrap,seg_53,"for computing the confidence interval for a given estimate, we frequently encounter two problems: finding the variance of an estimator, and working out the distribution of an estimator that is not an mle. in addition, the theory leading to normal (or t-based) confidence intervals is based on the asymptotic distribution of the estimator, which might be quite different than the distribution for small sample sizes. a very popular way for solving these problems is by using the bootstrap method, which aims at estimating all necessary quantities directly from the data themselves. while mainly used for computing the estimator’s variance, the bootstrap method"
438,1,"['confidence intervals', 'estimators', 'distribution', 'intervals', 'moments', 'confidence']", The Bootstrap,seg_53,"also allows to compute higher moments, and even allows computation of confidence intervals for estimators with non-normal distribution."
439,1,"['sample', 'independent', 'sample mean', 'distribution', 'samples', 'expectation', 'mean', 'function']", The Bootstrap,seg_53,"let us suppose we take b independent samples y1, . . . , yb from a distribution. then, by the laws of large numbers, the sample mean approaches the true expectation for increasing b. the same argument still holds if we apply a function h on mean and expectation:"
440,1,"['variance', 'estimator']", The Bootstrap,seg_53,"for example, we recover the variance estimator by choosing h = (yi − ȳ )2."
441,1,"['cumulative distribution function', 'estimated', 'estimate', 'with replacement', 'approximation', 'distribution function', 'replacement', 'data', 'distribution', 'set', 'probability', 'function', 'variance', 'estimator']", The Bootstrap,seg_53,"the key idea on how this helps is the following: let us consider any estimator θ̂n and denote by f(x) = p(θ̂n ≤ x) its cumulative distribution function. for the beginning, we are interested in calculating var(θ̂n). this is comparatively easy if we know the distribution f of the estimator. if we do not, we can try to estimate this distribution by f̂, and subsequently estimate the variance using this estimated distribution as an approximation. let us assume we are given a set of data x1, . . . , xn . for estimating f̂, we re-sample new data from this given set, uniformly and with replacement, such that each xi has the same probability to be re-sampled, and can also be re-sampled several times. we repeat this re-sampling b times, where x j,1, . . . , x j,n is the jth new"
442,1,"['sample', 'estimated', 'estimates', 'estimator']", The Bootstrap,seg_53,"sample. from each such sample, we compute the estimator θ̂n, j , leading to a total of b estimates. each sample is prone to be different from the others, and so are the estimated values."
443,1,"['sample', 'extreme values', 'bootstrap', 'data', 'information', 'representative', 'samples', 'representative sample']", The Bootstrap,seg_53,"if the data are a representative sample, this re-sampling gets us all the information needed: there are only few sample points with extreme values. these therefore get re-sampled rarely and are only present in a few bootstrap samples. on the other hand, “typical” values are sampled often, possibly even multiple times, into one bootstrap"
444,1,"['combinations', 'estimation', 'data', 'sets', 'varying', 'estimator']", The Bootstrap,seg_53,"sample. the estimation is then performed more often on sets of “typical” data values than on more extreme, more unlikely combinations of values. this in turn gives a correct impression of how the estimator varies with varying data."
445,1,"['sample', 'data', 'samples', 'variance']", The Bootstrap,seg_53,"overall, there are nn different ways draw new samples of size n; this number is very large even for moderate n which ensures that we do get a different sample each time. the name “bootstrap” refers to the seemingly impossible task to lift ourselves out of the unknown variance problem by using the straps of our own boots, namely the data we have."
446,1,"['bootstrap procedure', 'variance', 'bootstrap', 'algorithm']", The Bootstrap,seg_53,the algorithm. we can write the general bootstrap procedure for estimating the variance in a more algorithmic form as
447,1,"['with replacement', 'replacement']", The Bootstrap,seg_53,"• draw x1, . . . , xn uniformly with replacement from {x1, . . . , xn}."
448,1,"['sample', 'bootstrap']", The Bootstrap,seg_53,"• compute θ̂n,i = g(x1, . . . , xn) from this bootstrap sample."
449,1,"['bootstrap', 'estimate', 'estimates', 'variance']", The Bootstrap,seg_53,"• repeat the two steps b times to get the estimates θ̂n,1, . . . , θ̂n,b. • compute the estimator’s bootstrap variance estimate"
450,0,[], The Bootstrap,seg_53,"b b var(θ̂n) ≈ vboot = 1 ⎛θ̂ − 1 θ̂ ⎞2 . b ∑ i=1 ⎝ n,i b ∑ j=1"
451,1,"['method', 'interval', 'bootstrap', 'statistics', 'bootstrapped', 'function', 'confidence', 'confidence interval']", The Bootstrap,seg_53,"in r, the package boot offers a function boot() that simplifies the computation of statistics by the bootstrap method. once the b bootstrapped values for θ̂n are computed, there are several ways to form a confidence interval."
452,1,"['confidence interval', 'interval', 'estimate', 'bootstrap', 'distribution', 'normal', 'confidence', 'variance', 'estimator', 'normal distribution']", The Bootstrap,seg_53,"the normal ci. if the estimator has a normal distribution, we may simply replace the variance var(θ̂n) by its bootstrap estimate vboot and form the usual normal (or t-based) confidence interval"
453,0,[], The Bootstrap,seg_53,cnorm = θ̂n ± z1−α/2√vboot.
454,1,"['quantile', 'empirical quantile', 'method', 'estimate', 'estimates', 'unbiased estimators', 'bootstrap', 'percentile', 'estimators', 'distribution', 'samples', 'estimator', 'unbiased']", The Bootstrap,seg_53,"the percentile ci. the second method relies on using the bootstrap samples of estimator values to compute the empirical quantile of its distribution and works for all unbiased estimators. for this, we sort the bootstrap estimates such that θ̂n,(1) ≤ · · · ≤ θ̂n,(b); again, the estimate with index (i) is the ith smallest one."
455,0,[], The Bootstrap,seg_53,"then for k = bα (suitably rounded to the next integer), θ̂α := θ̂n,(k) is the empirical"
456,1,"['percentile', 'interval', 'empirical percentile', 'distribution', 'confidence', 'confidence interval']", The Bootstrap,seg_53,α-quantile of the distribution of θ̂n and we form the empirical percentile confidence interval
457,0,[], The Bootstrap,seg_53,"cpercentile = θ̂ , θ̂ . [ α/2 1−α/2]"
458,1,"['sample', 'unbiased estimator', 'parameters', 'data', 'normally distributed', 'distribution', 'asymmetric', 'mean', 'skewness', 'variance', 'estimator', 'unbiased']", The Bootstrap,seg_53,"example 16 sometimes, the data are not normally distributed, but their logarithms are. the log-normal distribution with parameters μ and σ 2 is the distribution of x = exp(y ), with y ∼ norm (μ, σ 2). these parameters are the mean and variance of y, but not of x. the distribution of x is asymmetric and we want to quantify this asymmetry using the skewness measure from sect. 1.6.5. an unbiased estimator for the skewness θ of a sample is"
459,0,[], The Bootstrap,seg_53,n1 ∑in=1(xi − x̄)3 θ̂n = 3 . ( n1 ∑in=1(xi − x̄)2) 2
460,1,"['percentile', 'method', 'estimate', 'interval', 'bootstrap', 'distribution', 'normal', 'confidence', 'estimator', 'confidence interval']", The Bootstrap,seg_53,"instead of working out the distribution of this estimator, we apply the bootstrap method to derive the percentile and normal confidence interval for a given estimate."
461,1,"['confidence intervals', 'random', 'bootstrap', 'estimate', 'intervals', 'samples', 'skewness', 'confidence', 'parameters', 'random variables', 'variables', 'normal']", The Bootstrap,seg_53,"for illustration, we generate n = 500 samples x1, . . . , xn of log-normal random variables with parameters μ = 0, σ = 1. we then compute the skewness estimate θ̂n, followed by b = 100 bootstrap samples from the xi . normal and pivot confidence intervals are finally computed for α = 0.05."
462,1,"['interval', 'function', 'estimator', 'bootstrap', 'estimate', 'symmetric', 'results', 'samples', 'confidence', 'confidence interval', 'density function', 'percentile', 'skewed', 'distribution', 'estimated', 'normal']", The Bootstrap,seg_53,"the results are given in fig. 2.3, where the estimate θ̂n is indicated by the small circle, and the normal and percentile 0.95-cis are given by the solid and dashed lines, respectively. the solid black line gives the empirical density function of the estimator θ̂n estimated from the bootstrap samples. interestingly, the percentile confidence interval is not symmetric around the estimate, because of a skewed estimator distribution."
463,1,"['sample', 'contamination', 'measurement', 'contaminations', 'estimators', 'distribution']", Robust Estimation,seg_55,"there is one major problem with the estimators that we discussed so far: they all assume that each sample point is taken from the same underlying distribution, and there are thus no contaminations in the sample. a contamination can be an “outlier” that, by eye, can clearly be identified as an incorrect measurement, for example. many estimators are very sensitive to such contaminations."
464,1,"['outliers', 'sample', 'case', 'data', 'location', 'distribution', 'expected value', 'mean']", Robust Estimation,seg_55,"example 17 a sample of n = 20 points from a norm(5,4) is taken. in addition, the sample data is contaminated by only n′ = 2 outliers, leading to the empirical density given in fig. 2.4. estimating the mean of the sample distribution gives the location indicated by the solid vertical line; it is substantially shifted to the right from the correct expected value which would be somewhere near the maximum of the density in this case."
465,1,"['sample', 'robustness', 'contamination', 'contaminations', 'estimate', 'arithmetic mean', 'breakdown point', 'estimators', 'outlier', 'mean', 'variance', 'estimator']", Robust Estimation,seg_55,"to quantify how sensitive an estimator is to contaminations in the sample, the robustness of an estimator is measured by its breakdown point. it refers to the proportion of contaminations an estimator can handle before it gives arbitrarily large values. for the arithmetic mean, even one contamination has the capacity to make the estimate arbitrarily large, as even a single outlier very far away from the rest of the sample “pulls” the whole estimate away from the correct mean of the uncontaminated sample. its breakdown point is therefore zero. the same argument holds for the two estimators for the variance, which both also have a breakdown point of zero."
466,1,"['contaminations', 'empirical quantiles', 'statistics', 'location', 'estimators', 'sample', 'contamination', 'estimate', 'results', 'quantiles', 'data', 'order statistics', 'breakdown points']", Robust Estimation,seg_55,"while this might in practice not be as dramatic as theory suggests, simply because contaminations far away from the sample are often unlikely, it is nevertheless a reason to be uncomfortable, as it means that even a small amount of contamination can potentially yield very misleading results. for a small number of sample points, we might try a visual inspection to see if there are any unusual values in the sample, but this is clearly not a good strategy if we want to investigate large amounts of data. we will therefore investigate robust estimators with high breakdown points as alternatives for common estimators. here, we will discuss robust alternatives for estimating the location and scale. they all rely on order statistics of the sorted sample, again denoted x(1) ≤ · · · ≤ x(n), and typically estimate empirical quantiles."
467,1,"['sample', 'median', 'estimate', 'location', 'distribution', 'information', 'expectation', 'function', 'average']", Location Median and kTrimmed Mean,seg_57,"median. in addition to the expectation, the median is another measure for the location of a distribution. it corresponds to the 0.5-quantile q0.5 of a distribution such that p(x ≤ q0.5) = 0.5. we can estimate any α-quantile from the sorted sample simply by finding the correct index from k = nα. if k is an integer, we simply select the kth smallest value, i.e., q̂(α) = x(k). if k is not an integer, we compute the two nearest integer k′, k′′ and interpolate the corresponding values x(k′) and x(k′′). various ways for interpolation exist, many of which are implemented in thequantile () function in r. the estimate q̂(0.5) for the median is therefore simply the sample point in the middle (or the average of the two surrounding ones). as such, it does not use any information about the actual values of the sample points, but only uses information about the rank, i.e., their indices in the sorted sample."
468,1,['sample'], Location Median and kTrimmed Mean,seg_57,example 18 for n = 8 given sample points
469,0,[], Location Median and kTrimmed Mean,seg_57,"6.39, 0.887, 1.521, 8.635, 7.742, 7.462, 6.631, 5.511, the sorted values are"
470,0,[], Location Median and kTrimmed Mean,seg_57,"0.887, 1.521, 5.511, 6.39, 6.631, 7.462, 7.742, 8.635."
471,1,"['sample', 'estimated', 'median']", Location Median and kTrimmed Mean,seg_57,the median is estimated as q̂(0.5) = 21 × (6.39 + 6.631) = 6.5105 by interpolating between the two sample points with ranks 4 and 5.
472,1,"['mean', 'median']", Location Median and kTrimmed Mean,seg_57,"changing the largest value from 8.635 to 108.635 changes the mean substantially from μ̂ = 5.5974 to μ̂′ = 18.0974, but leaves the median unchanged at q̂ ′(0.5) = 6.5105. in r, the median is computed by median()."
473,1,"['sample', 'estimated', 'contaminations', 'median', 'normal']", Location Median and kTrimmed Mean,seg_57,"for the contaminated normal sample of example 17, the estimated median q̂0.5 = 3.821 is given by the dashed vertical line in fig. 2.4. it is reasonably close to the true median q0.5 = 5 and largely unaffected by the two contaminations."
474,1,"['robustness', 'breakdown point', 'median']", Location Median and kTrimmed Mean,seg_57,"let us look at the robustness of the median. we note that because it only considers ranks and not values, we can increase all points larger than the median arbitrarily without changing it. the same holds for decreasing smaller values and we conclude that the median has breakdown point 50%."
475,1,"['sample', 'median', 'skewed distributions', 'symmetric', 'skewed', 'location', 'distribution', 'expected value', 'breakdown point', 'expectation', 'data', 'mean', 'average', 'distributions']", Location Median and kTrimmed Mean,seg_57,"both the mean and the median are measures for the location of the distribution, trying to give a single number to describe where “most” of the values are. the mean gives the expectation or average of a sample, wheres the median indicates the point such that half of the data is smaller (resp. larger). if the distribution is symmetric, the two values are identical, but they differ for skewed distributions. because of its large breakdown point, the median is often the better choice for estimating the location. however, it cannot always be interpreted as the expected value of the distribution."
476,1,"['sample', 'contaminations', 'median', 'estimate', 'data', 'expectation', 'mean']", Location Median and kTrimmed Mean,seg_57,"trimmed means. if we still want to specifically estimate the expectation robustly, the k-trimmed mean is a good alternative to the median. it also uses the sorted values of the sample, but drops the k lowest and highest sample points of the data. the rationale is that contaminations are likely to be much smaller or much larger than the uncontaminated sample points. formally,"
477,0,['n'], Location Median and kTrimmed Mean,seg_57,n−k 1 μ̂(k) = ∑ x(i). n − 2k i=k+1
478,1,"['median', 'arithmetic mean', 'estimators', 'mean', 'estimator']", Location Median and kTrimmed Mean,seg_57,"for k = 0, we recover the ordinary arithmetic mean again, for k = n/2 (taken to the next suitable integer), we recover the median. the k-trimmed mean is thus a generalization of both estimators. the choice for k is somewhat arbitrary of course, and should always be stated if this estimator is used; common choices are k = 0.05 × n and k = 0.25 × n."
479,1,"['sample', 'mean']", Location Median and kTrimmed Mean,seg_57,"example 19 for the 8 sample points of the previous example and k = 1, the k-trimmed mean reads"
480,0,[], Location Median and kTrimmed Mean,seg_57,"1 μ̂(1) = (x(2) + · · · + x(7)) = 5.876, 8"
481,1,"['sample', 'estimate', 'arithmetic mean', 'data', 'mean']", Location Median and kTrimmed Mean,seg_57,"so the smallest and largest value are ignored and the ordinary arithmetic mean is computed from the remaining data. again, changing the largest sample value from 8.635 to 108.635 does not change the estimate, as this point is ignored in the computation. in r, the k-trimmed mean can be accessed by mean(. . . , trim=. . . )."
482,1,"['deviation', 'median', 'distribution', 'median absolute deviation', 'scale of a distribution', 'measuring']", Scale MAD and IQR,seg_59,similar considerations lead to two robust alternatives for measuring the scale of a distribution: the median absolute deviation (mad) and the inter-quartile-range (iqr).
483,1,"['deviation', 'median', 'variance']", Scale MAD and IQR,seg_59,"median absolute deviation. the mad follows the same ideas as the variance, but measures the median of the absolute distance to the median:"
484,1,['median'], Scale MAD and IQR,seg_59,mad = mediani (|xi − median j (x j )|).
485,1,"['boxplot', 'range', 'data']", Scale MAD and IQR,seg_59,"inter-quartile range. the iqr computes the difference between the 0.25- and 0.75-quantile, and is given by the rectangle in a boxplot (see sect. 1.8.4) that contains the medium 50% of the data:"
486,0,[], Scale MAD and IQR,seg_59,iqr = q 3 − q 1 . 4 4
487,1,"['functions', 'median', 'data', 'distribution', 'normal', 'variance', 'breakdown points', 'normal distribution']", Scale MAD and IQR,seg_59,"comparison to the variance. both the mad and the iqr give different measures for the scale compared to the variance. similar to the median, they both are more based on the ranks and not the absolute values of the particular data, and have high breakdown points. for the normal distribution, σ ≈ 1.48 × mad, so variance and mad are scaled versions of each other. inr, both mad and iqr are easily accessible via the functions mad() and iqr()."
488,1,"['sample', 'estimated', 'table', 'estimators', 'samples']", Scale MAD and IQR,seg_59,"example 20 for n = 20 samples contaminated with n′ = 2 “outliers” of example 17, the various estimators are summarized in the following table. the second column gives the values estimated on the contaminated sample, the third column gives the values computed on the uncontaminated subset of the sample."
489,0,[], Scale MAD and IQR,seg_59,estimator value true value x̄ = μ̂ 11.19 4.81
490,0,[], Scale MAD and IQR,seg_59,m̂ = q(0.5) 4.39 4.28
491,0,[], Scale MAD and IQR,seg_59,μ̂(α = 0.1) 5.11 4.62 s = σ̂ 22.12 1.95
492,0,[], Scale MAD and IQR,seg_59,iqr 2.46 1.52
493,0,[], Scale MAD and IQR,seg_59,mad 1.23 1.07
494,1,"['deviation', 'sample', 'contrast', 'estimates', 'mean', 'standard', 'standard deviation']", Scale MAD and IQR,seg_59,"as we would expect, the mean and the standard deviation give very different values on the contaminated and uncontaminated sample. in contrast, their robust counterparts all give estimates on the contaminated sample that are reasonably close to the uncontaminated values."
495,1,"['estimation', 'estimators', 'minimax estimation']", Minimax Estimation and Missing Observations,seg_61,"in addition to maximum-likelihood and least-squares, minimax estimation is a third principle to construct estimators."
496,1,"['minimax estimation', 'estimation', 'estimators']", Loss and Risk,seg_63,"before introducing minimax estimation, let us briefly look into a theoretical framework that allows us to compare the performance of various estimators and derive new principles for their construction."
497,1,"['functions', 'true parameter', 'estimate', 'loss', 'parameter']", Loss and Risk,seg_63,"the loss-function l (θ, θ̂ ) measures the distance from the true parameter value and its estimate. two popular choices for loss functions are the squared loss l (θ, θ̂ ) = (θ − θ̂ )2 and the absolute loss l (θ, θ̂ ) = |θ − θ̂ |."
498,1,"['sample', 'risk', 'loss', 'estimator']", Loss and Risk,seg_63,"the loss depends on the actual value of the estimator, and thus on the specific sample. to get a more general measure, we therefore look at the expected loss, known as the risk of the estimator"
499,0,[], Loss and Risk,seg_63,"r(θ, θ̂ ) = e(l (θ, θ̂ ))."
500,1,"['unbiased estimator', 'risk', 'loss function', 'loss', 'function', 'variance', 'estimator', 'unbiased']", Loss and Risk,seg_63,"for example, the risk of an unbiased estimator θ̂ with respect to the squared loss function is simply its variance:"
501,0,[], Loss and Risk,seg_63,"r(θ, θ̂ ) = e(l (θ, θ̂ )) = e((θ − θ̂ )2) = e((θ̂ − e(θ̂))2)."
502,1,"['risk', 'true parameter', 'parameter', 'average', 'estimator']", Loss and Risk,seg_63,"a small risk indicates that on average, for all possible true parameter values, the estimator is not too far off."
503,1,"['risk', 'loss', 'probability', 'estimator']", Loss and Risk,seg_63,example 21 let us calculate the risk for the maximum-likelihood estimator p̂n of the matching probability p in the sequence matching example with respect to squared loss of the mle p̂n = m/n:
504,0,"['n', 'e']", Loss and Risk,seg_63,"p(1 − p) r(p, p̂) = e(( p̂n − p)2) = var( p̂n) = . n"
505,1,"['risk', 'estimate']", Loss and Risk,seg_63,"as shown in fig. 2.5 (solid line), the risk is highest for p ≈ 1/2 and lowest for values near the boundary. intuitively, for p = 0, we will not observe any matches, and always estimate correctly."
506,1,"['risk', 'estimator', 'parameter', 'minimax estimator']", Minimax Estimators,seg_65,"for some applications, we might be interested in having a guarantee that the risk is not too high for any possible true value of the parameter. for this, we construct a minimax estimator θ̃ such that the maximal risk"
507,0,[], Minimax Estimators,seg_65,"max r(θ, θ̃ ) θ"
508,1,"['loss', 'average', 'estimator']", Minimax Estimators,seg_65,"is minimal. this means that while we allow the loss of this estimator to be larger for some values of θ, it will stay lower on average than any other estimator."
509,1,['estimator'], Minimax Estimators,seg_65,example 22 let us consider a different estimator for the matching example:
510,0,['n'], Minimax Estimators,seg_65,m + 1√n 2 p̃n = n + √n
511,1,"['loss', 'risk']", Minimax Estimators,seg_65,has squared loss risk
512,0,['n'], Minimax Estimators,seg_65,"n r(p, p̃n) = , 4(n + √n)2"
513,1,"['risk', 'parameter']", Minimax Estimators,seg_65,"which is constant for all parameter values p and is smaller than the maximal risk of the mle p̂n . indeed, maxp r(p, p̂n) = 1/4n but r(p, p̃n) < 1/4n for all p, as one easily verifies. as we see in fig. 2.5, however, the risk is not always lower than the mle-risk, but is lower for mid-range parameter values and higher in the extremes."
514,1,"['sample size', 'sample', 'probabilities', 'experiment', 'estimators', 'outcomes']", Minimax Estimators,seg_65,minimax estimators are also useful if some potential outcomes of an experiment have small probabilities and may not be observed due to a small sample size.
515,1,"['experiment', 'random', 'outcome']", Minimax Estimators,seg_65,"example 23 let us consider the following problem: the possible outcome of an experiment is one of s different types, such as a/c/g/t in the former random"
516,0,[], Minimax Estimators,seg_65,"sequence examples, with each sequence position being one of the four possible nucleotides."
517,1,"['sample', 'multinomial', 'experiment', 'multinomial distribution', 'observation', 'distribution', 'samples', 'binomial', 'probability', 'binomial distribution', 'experiments', 'outcomes']", Minimax Estimators,seg_65,"such experiments are described by a multinomial distribution, which is a generalization of the binomial distribution to more than two outcomes. the probability for an observation of category i is pi , and ∑is=1 pi = 1. a result of such an experiment is a vector (x1, . . . , xs) containing the number of samples of category i in xi , so that ∑is=1 xi = n is the total number of sample points.1"
518,1,"['mass function', 'probability mass function', 'probability', 'function']", Minimax Estimators,seg_65,"the probability mass function of (x1, . . . , xs) is given by"
519,0,['n'], Minimax Estimators,seg_65,"n! p(x1 = k1, . . . , xs = ks) = p1"
520,0,[], Minimax Estimators,seg_65,"k1 · · · psks , k1! · · · ks !"
521,1,"['binomial coefficient', 'estimated', 'observations', 'case', 'distribution', 'binomial', 'probability', 'binomial distribution', 'coefficient', 'number of observations', 'estimator']", Minimax Estimators,seg_65,"and the binomial distribution is recovered by setting s = 2, in which case p2 = 1− p1 and k2 = n −k1, leading to the binomial coefficient. the expected number of observations in the ith category is e(xi ) = npi . in the sequence example, we would thus expect to see npa a and npg g in a sequence of length n. the probability of category i can be estimated by the maximum-likelihood estimator"
522,0,['n'], Minimax Estimators,seg_65,"ki p̂n,i = . n"
523,1,"['sample size', 'sample', 'probabilities', 'observations', 'confidence']", Minimax Estimators,seg_65,a common rule-of-thumb suggests choosing a sample size n such that at least five observations are expected in each category for estimating the various probabilities with some confidence:
524,0,['n'], Minimax Estimators,seg_65,"5 n min(pn,i ) ≥ 5 ⇒ n ≥ . i mini (pn,i )"
525,1,['probabilities'], Minimax Estimators,seg_65,for the possible nucleotides with probabilities
526,0,[], Minimax Estimators,seg_65,"(pa, pc , pg , pt ) = (1/2, 1/4, 1/8, 1/8),"
527,1,"['samples', 'estimate']", Minimax Estimators,seg_65,we would thus need at least 5/0.125 = 40 samples to reliably estimate all probabilities.
528,1,"['sample size', 'sample', 'model', 'consequences', 'probabilities', 'estimate', 'control']", Minimax Estimators,seg_65,"in practice, we usually do not know these probabilities, of course, and sometimes have no control over the possible sample size n. imagine that we only have a sequence of 20 nucleotides. if it happens not to have any g, we consequently estimate p̂g = 0. this will have undesired consequences if we use these values for a model to describe the sequence matching probabilities, because the model would assume that g can never occur and will thus incorrectly predict the possible number of matchings."
529,1,"['number of observations', 'observations']", Minimax Estimators,seg_65,"one way of dealing with this problem is to introduce pseudo-counts by pretending that there is a certain number of observations in each category to begin with. for example, let us put a observations in each category before conducting the actual"
530,1,"['estimate', 'experiment', 'observations']", Minimax Estimators,seg_65,"experiment, and therefore see xi + a observations in category i after the experiment. then, we can use the estimate"
531,0,['n'], Minimax Estimators,seg_65,"xi + a p̃n,i = sa + n"
532,1,"['probabilities', 'data', 'estimate']", Minimax Estimators,seg_65,"for the categories’ probabilities, which is simply the mle for the modified data. with a > 0, each estimate is strictly larger than (but potentially very close to) zero."
533,1,"['independent', 'probabilities', 'estimates', 'estimation', 'data']", Minimax Estimators,seg_65,"let us assume we observed (xa, xc , xg , xt ) = (13, 6, 0, 1). how large should we choose a? if we choose it too large, it would spoil the whole estimation and assign almost identical probabilities everywhere, independent of the data. for example, with a = 1000 the estimates are"
534,0,[], Minimax Estimators,seg_65,"( p̃a, p̃c , p̃g , p̃t ) = (0.252, 0.25, 0.249, 0.249)."
535,1,['probabilities'], Minimax Estimators,seg_65,"if we choose a too small, it might not have an effect and we end up with non-zero, but extremely low probabilities. indeed, for a = 0.1,"
536,0,[], Minimax Estimators,seg_65,"( p̃a, p̃c , p̃g , p̃t ) = (0.642, 0.299, 0.005, 0.054)."
537,1,"['risk', 'parameters', 'multinomial', 'multinomial distribution', 'distribution', 'estimator', 'minimax estimator']", Minimax Estimators,seg_65,"we can calculate a reasonable compromise by selecting a such that we minimize the maximal risk of the corresponding estimator. for parameters of the multinomial distribution, this minimax estimator is achieved by choosing"
538,0,[], Minimax Estimators,seg_65,√n a = . s
539,1,['estimate'], Minimax Estimators,seg_65,"for the example, a = 1.118 and we estimate"
540,0,[], Minimax Estimators,seg_65,"( p̃a, p̃c , p̃g , p̃t ) = (0.577, 0.291, 0.046, 0.087),"
541,1,['data'], Minimax Estimators,seg_65,"which is fairly close to the correct values, taking into account that we do not have many data available."
542,1,"['estimator', 'binomial', 'case']", Minimax Estimators,seg_65,the seemingly ad-hoc estimator in sect. 2.5.2 for the binomial case was derived in this way.
543,1,"['estimate', 'samples', 'parameter', 'variance', 'estimator']", FisherInformation and CramérRao Bound,seg_67,"we conclude the chapter by a brief discussion of the idea of fisher-information, from which we can derive a theoretical lower bound for the variance of an estimator. this bound tells us how precise we can actually estimate a given parameter with a fixed number of samples."
544,1,['function'], FisherInformation and CramérRao Bound,seg_67,"recall the definition n(θ) = ∑i log( f (xi ; θ))of the log-likelihood function. the fisher-score is simply the derivative of this function with respect to the parameter(s),"
545,0,[], FisherInformation and CramérRao Bound,seg_67,"∂ n(θ) fisher-score = , ∂θ"
546,1,"['likelihood', 'likelihood function', 'parameter', 'function', 'estimator']", FisherInformation and CramérRao Bound,seg_67,"and we calculate a maximum-likelihood estimator by finding its roots. in addition, the fisher-information describes the curvature of the likelihood function around a parameter value θ. it is given by"
547,0,['n'], FisherInformation and CramérRao Bound,seg_67,n ∂ n(θ) ∂2 n(θ) in(θ) = ∑ var ( ∂θ ) = −ne ( ∂θ2 ) = ni (θ). i=1
548,1,"['estimated', 'likelihood', 'information', 'likelihood function', 'parameter', 'function']", FisherInformation and CramérRao Bound,seg_67,"loosely speaking, a large information indicates that the likelihood function will change noticeably when moving from θ to a nearby value θ ′; the parameter value can then be estimated more reliably. a small information indicates a shallow “valley” in the likelihood function, where substantially different parameter values lead to almost identical values of n(θ)."
549,1,['function'], FisherInformation and CramérRao Bound,seg_67,example 24 let us again consider the matching example with log-likelihood function n(p) = m log(p) + (n − m) log(1 − p) and
550,0,['n'], FisherInformation and CramérRao Bound,seg_67,∂ n(p) m n − m = − . ∂p p 1 − p
551,0,[], FisherInformation and CramérRao Bound,seg_67,the fisher-information is
552,0,['n'], FisherInformation and CramérRao Bound,seg_67,∂2 n(p) m n − m in(p) = −ne ( ∂p2 ) = −ne (− p2 − (1 − p)2 )
553,0,['n'], FisherInformation and CramérRao Bound,seg_67,n n n = e(m) + e(n − m) = . p2 (1 − p)2 p(1 − p)
554,1,"['sample', 'functions', 'parameters', 'estimate', 'likelihood', 'information', 'samples', 'likelihood function', 'parameter', 'function']", FisherInformation and CramérRao Bound,seg_67,"for sequences of length n = 20 nucleotides and m = 12 observed matches, the log-likelihood function and its fisher-information are given in fig. 2.6. as expected, the log-likelihood is highest at p = m/n. the fisher-information does not take into consideration the actual observed matches and shows that for parameters p in the mid-range, the information carried by a sample is much lower than for more extreme parameter values near zero or one. this tells us that true values near the boundaries are much easier to estimate, as they lead to more dramatic expected changes in the likelihood function. these properties of the likelihood and information functions become more pronounced if we increase the number of samples from n = 20 to n = 60."
555,1,"['sample size', 'sample', 'states', 'variance', 'estimator']", FisherInformation and CramérRao Bound,seg_67,cramér-rao bound. the main importance of the fisher-information is that it allows us to calculate the smallest possible variance that can be achieved with a given estimator and a given sample size. this cramér-rao bound states that
556,0,[], FisherInformation and CramérRao Bound,seg_67,"1 var(θ̂n) ≥ , in(θ)"
557,1,"['estimates', 'information', 'variance', 'estimator']", FisherInformation and CramérRao Bound,seg_67,"and we cannot decrease the variance of an estimator θ̂n below the reciprocal of its information. for getting estimates with lower variance and thus, for example,"
558,1,"['sample size', 'sample', 'confidence intervals', 'parameters', 'estimators', 'intervals', 'confidence', 'variance', 'estimator']", FisherInformation and CramérRao Bound,seg_67,"narrower confidence intervals, we either need to increase the sample size (because in(θ) ≈ ni (θ)) or choose another estimator. indeed, some estimators can be shown to have lowest variance among all other estimators for the same parameters."
559,1,"['sample', 'random sample', 'location', 'distribution', 'consistency', 'estimators', 'data', 'random', 'realization']", Summary,seg_69,"estimation allows us to infer the value of various properties of a distribution, such as its location, from data. we can construct corresponding estimators by the maximumlikelihood, the least-squares, and the minimax approach. estimators are random variables because their realization depends on a given random sample. their properties such as consistency and unbiasedness allow us to compare different estimators."
560,1,"['confidence intervals', 'empirical quantiles', 'probability', 'estimator', 'bootstrap', 'estimate', 'quantiles', 'data', 'intervals', 'standard', 'confidence', 'error', 'estimation', 'distribution', 'estimated', 'method', 'standard error']", Summary,seg_69,"for a concrete estimation, we can compute confidence intervals around the estimated value to quantify how good the estimate is. these intervals contain the true value with high probability. for their computation, we need to know the distribution of the estimator to find the corresponding quantiles, and its standard error to scale correctly. the bootstrap offers a practical method to establish confidence intervals by resampling the data and computing empirical quantiles from the corresponding estimated values."
561,1,"['sample', 'contaminations', 'median', 'breakdown point', 'estimators', 'data', 'sensitivity', 'estimator']", Summary,seg_69,"the breakdown point describes the sensitivity of an estimator to contaminations in the data. because many classical estimators have a very low breakdown point, we should usually try to use robust alternatives, such as the median. many robust estimators are based on the ranks of sample points rather than their values."
562,1,"['sample', 'probabilities', 'multinomial', 'estimation', 'observations', 'minimax estimation']", Summary,seg_69,missing observations and small sample sizes can cause major problems when estimating multinomial probabilities. using minimax estimation to calculate pseudocounts enables us to partly circumvent these problems.
563,1,"['levels', 'parameters', 'results', 'data', 'test results', 'probability', 'test', 'multiple testing', 'hypothesis']",Chapter  Hypothesis Testing,seg_71,"abstract testing provides the formal framework to reject or not reject a hypothesis on parameters, depending on whether it is supported by given data. test levels and p-values allow to quantify the chances of false rejections due to the randomness of the data. correct interpretation of test results is discussed in more detail and methods to adjust the probability of false rejections for multiple testing are presented."
564,1,"['hypotheses', 'multiple testing']",Chapter  Hypothesis Testing,seg_71,keywords hypotheses · p-value · multiple testing · fdr
565,1,"['sample', 'standard', 'treatment', 'results', 'random sample', 'control group', 'control', 'mean', 'random', 'average']", Introduction,seg_73,"a typical testing problem is the following: to decide if and which new treatment works better than a standard treatment, a group of patients is given the new treatment, while a control group is given the standard treatment. the time until recovery is recorded for each patient and a new treatment is considered better if the average recovery time is considerably shorter than that of the control group. the problem is to quantify what we mean by “considerably shorter”, as the average times are computed from a random sample and although the new treatment might be better, the controls patient might— just by chance—nevertheless recover in comparable time: by visual inspection of the results shown in fig. 3.1, it seems that treatments a and b work better than the control, but just by visual inspection, no clear decision can be made."
566,1,"['probabilities', 'treatment', 'statistical hypothesis', 'data', 'statistical hypothesis testing', 'statistical', 'hypotheses', 'hypothesis testing', 'statistical significance', 'significance', 'hypothesis']", Introduction,seg_73,"in this chapter, we investigate statistical hypothesis testing to formally describe the hypothesis to be tested (e.g., new treatment works better) and compare it with an alternative (e.g., new treatment works not better). hypothesis testing will allow us to capture that one of the hypotheses is correct but just by chance, the data suggests otherwise. quantifying these probabilities leads to the concept of statistical significance."
567,1,['random'], Introduction,seg_73,"example 25 let us first come back to the sequence matching example, where m ∼ binom(n, p0) is the number of matches of two random, unrelated sequences of fixed length n, and p0 = p2a + pc"
568,1,['probability'], Introduction,seg_73,2 + pg 2 + pt 2 is the probability of seeing a match in any given position.
569,1,"['true parameter', 'estimate', 'predicted', 'case', 'distribution', 'random variable', 'variable', 'probability', 'random', 'parameter', 'test', 'hypothesis']", Introduction,seg_73,"assume we observe m matches in a pair of such sequences and want to conclude whether or not these sequences are related. in example 8, we argued that if m is substantially larger than e(m) for unrelated sequences (thus using p0 as matching probability), we have good reason to claim that the sequences are related and the number of matches then follows a binom(n, p) distribution with a higher matching probability p > p0, leading also to a higher expected number of matches. another way of deciding whether the sequences are related is therefore to ask whether the true matching probability p and the theoretical probability p0 for the case of unrelated sequences are different. of course, we do not know the correct probability p, so we have to work with its estimate p̂n . to test our hypothesis, we therefore need to decide whether the observed matching probability deviates substantially from the predicted probability p0. because p̂n = m/n is a random variable, we can use its distribution to figure out how likely it is to observe a certain estimate p̂n = m/n under the assumption that the true parameter value is p0."
570,1,"['model', 'true parameter', 'parameter', 'hypothesis']", Introduction,seg_73,"for simplicity, let us assume that pa = pc = pg = pt = 1/4 and consequently p0 = 1/4 for unrelated sequences. let us further denote by h0 the hypothesis that the sequences are unrelated, so the true parameter p is the same as in the model for unrelated sequences:"
571,0,[], Introduction,seg_73,1 h0 : p = p0 = . 4
572,1,"['alternative hypothesis', 'data', 'hypotheses', 'null hypothesis', 'alternative hypotheses', 'hypothesis']", Introduction,seg_73,"we would like to calculate whether the observed data give evidence against this null hypothesis and in favor of an alternative hypothesis h1. several alternative hypotheses are possible, for example"
573,0,[], Introduction,seg_73,"• the simple alternative h1 : p = 0.35,"
574,0,[], Introduction,seg_73,"1 • the composite and one-sided alternative h1 : p > 4 ,"
575,0,[], Introduction,seg_73,1 • the composite and two-sided alternative h1 : p = 4 .
576,1,"['disjoint', 'sets', 'hypotheses', 'probability', 'parameter', 'alternative hypotheses']", Introduction,seg_73,"each of these alternatives claims that the probability for a match in the two sequences is a certain parameter p, and gives valid ranges for this parameter. while the simple alternative claims a specific value, the composite alternatives give whole ranges of feasible values. more general, we can also define two (disjoint) sets 0 and 1 such that the nulland alternative hypotheses are h0 : p ∈ 0 and h1 : p ∈ 1, respectively."
577,1,"['case', 'data', 'mean', 'probability', 'test', 'null hypothesis', 'hypothesis']", Introduction,seg_73,"intuitively, consider sequences of length n = 1000 with m = 100 observed matches. with p̂n = 1/10, this indicates that the matching probability is actually much smaller than p0 = 1/4. however, with the alternative h1 : p > 1/4, this data would not give evidence against h0. this does not mean that h0 is true (which clearly isn’t the case here), but that the alternative h1 cannot explain the data any better than h0. on the contrary, if we test h0 against the two-sided alternative h1 : p = 1/4, there is sufficient evidence that this alternative provides a better explanation for the data, so we would reject the null hypothesis."
578,1,"['quantitative', 'parameters', 'data', 'distribution', 'probability', 'random', 'null hypothesis', 'hypothesis']", Introduction,seg_73,"to make these considerations more quantitative and rigorous, let us use the onesided alternative h1 : p > 1/4, so we only need to check if we observe unusually many (and not perhaps unusually few) matches. consider the following argument: if h1 is true, we expect to see more matches than if h0 were true. however, the number of matches is random, and there is the possibility that even if h0 is true, the observed counts are high just by chance. knowing the distribution of m under h0 (i.e., with p = p0 = 1/4), we can compute the probability that m exceeds an observed number m of matches under h0; this is the probability that the distribution stated in h0 gives rise to data at least as extreme as observed. if this probability is very low, this indicates that the null hypothesis is unlikely to be true, as it is unlikely that the data have been generated with the stated parameters."
579,1,"['information', 'distribution', 'binomial', 'probability', 'parameter', 'binomial distribution', 'null hypothesis', 'hypothesis']", Introduction,seg_73,"we know that the number of matches has a binomial distribution. if the null hypothesis is true, we also know that the matching parameter p is equal to p0 = 1/4. with this information, we compute the probability to see at least the observed number of m matches, provided the null hypothesis is indeed correct, to be"
580,0,[], Introduction,seg_73,∑ i=
581,0,['n'], Introduction,seg_73,1 (n i ) 0
582,0,[], Introduction,seg_73,"p(m ≥ m|h0) = 1 − p(m < m|h0) = 1 − pi (1 − p0)n−i ,"
583,1,"['conditional probabilities', 'probabilities', 'conditional']", Introduction,seg_73,where we slightly abuse the notation for conditional probabilities for brevity.
584,1,['probability'], Introduction,seg_73,"for example 8, we considered sequences of length n = 100 and m = 29 observed matches. the probability to see at least this number of matches in unrelated sequences is p(m ≥ 29|h0) = 0.1495, which gives no evidence against h0 in favor of h1. on the contrary, the probability to observe at least m = 50 matches in unrelated sequences is p(m ≥ 50|h0) = 2.13e − 08, providing substantial evidence that h0 is actually false."
585,1,"['false positive', 'probabilities', 'errors', 'rejection region', 'hypothesis', 'probability', 'null hypothesis']", Introduction,seg_73,"the calculated probabilities p(m ≥ m|h0) are called p-values. to reject or not reject a given null hypothesis, we fix a probability α and compute a threshold c such that p(m ≥ c|h0) = α. we then reject h0 in favor of h1, if m > c and do not reject if m ≤ c, leading to the rejection region rα = (c,∞). the probability α describes the false positive errors, as it is the probability that we reject h0, although it is actually correct. for the example with sequence length n = 100 and a probability of α = 0.05, we calculate"
586,0,[], Introduction,seg_73,"0.0693, for c = 32,"
587,0,[], Introduction,seg_73,"p(m ≥ c|h0) = { 0.0446, for c = 33."
588,1,['case'], Introduction,seg_73,"we would therefore use the more conservative c = 33 as our threshold, reject h0 : p = 1/4 in favor of h1 : p > 1/4 in the case m = 50 > c, and not reject in the case m = 29 < c."
589,1,"['null hypothesis', 'hypothesis']", Introduction,seg_73,"importantly, not rejecting is not the same as accepting h0 because if we do not have evidence against h0, we cannot conclude that it is actually true. we should therefore never speak of “accepting” the null hypothesis, but only of “not rejecting”."
590,1,"['testing statistical', 'statistical hypotheses', 'statistical', 'hypotheses']", The General Procedure,seg_75,"the considerations taken in the introductory example can be divided in the following steps, providing the general setup for testing statistical hypotheses."
591,1,"['sample', 'independence']", The General Procedure,seg_75,"1. clarify assumptions on independence, sample sizes etc. this will guide the choices"
592,1,"['alternative hypothesis', 'null hypothesis', 'hypothesis']", The General Procedure,seg_75,in the next steps and is usually only done implicitly. 2. formulate the null hypothesis h0 and the alternative hypothesis h1.
593,1,"['test statistic', 'data', 'distribution', 'statistic', 'function', 'test']", The General Procedure,seg_75,"choose h0 such that we retain it unless there is strong evidence against it. choose h1 to be simple or composite and oneor two-sided depending on the problem. 3. choose a test statistic t = g(x1, . . . , xn) as a function of the data xi . 4. derive the null-distribution of t, i.e., its distribution p(t ≤ t |h0) under the"
594,1,"['data', 'test']", The General Procedure,seg_75,"assumption that h0 holds. this is usually the difficult part. of course, the choice of t above is already partially guided by the necessity to determine its null-distribution. for many practical purposes, steps 3 and 4 are done simply by looking up the appropriate test. 5. compute the test statistic’s value t = g(x1, . . . , xn) from the data x1, . . . , xn . 6. compute the p-value p(t ≥ t |h0)."
595,0,[], The General Procedure,seg_75,"the smaller the p-value, the less likely it is that the observed value t was generated from the null-distribution."
596,1,"['alternative hypothesis', 'false positive', 'information', 'hypothesis', 'probability', 'level', 'test', 'null hypothesis']", The General Procedure,seg_75,"ultimately, the p-value gives us the information to either reject or don’t reject the null hypothesis in favor of the alternative hypothesis. to arrive at such a decision from a given p-value, we decide on a level α for the test. this level gives the probability of getting a false positive, i.e., of falsely rejecting h0 although it is true, due to"
597,1,"['test statistic', 'distribution', 'expected value', 'statistic', 'hypotheses', 'rejection region', 'deviations', 'level', 'test', 'null hypothesis', 'hypothesis']", The General Procedure,seg_75,"uncommonly large deviations of the observed test statistic t from the expected value under h0. using this level, we then compute the rejection region rα of t such that we reject the null hypothesis if the observed value t is inside the rejection region rα , and do not reject if it is outside. this rejection region is computed from the distribution of t under h0 and the two hypotheses. if the null hypothesis is rejected at a level α, we say that the test is significant at level α."
598,1,"['statistical test', 'level', 'test', 'statistical']", The General Procedure,seg_75,"the p-value corresponds to the smallest level α such that the test would not yet reject. for example, if we observe a p-value of 0.034, the test would reject at the α = 0.05 level, but not at the α = 0.01 level. indeed, no r-implementation of a statistical test requires a test level; they all report the p-value, so we can decide what to do."
599,1,"['test statistic', 'distribution', 'statistic', 'hypothesis', 'probability', 'test', 'null hypothesis']", The General Procedure,seg_75,"absence of evidence is not evidence of absence. rejecting the null hypothesis hinges on the distribution of the test statistic assuming h0 is true. the p-value gives the probability that we see a value at least as extreme as the observed one under this distribution. it is not the probability that h0 is true. a very low p-value indicates that the test statistic is unlikely to take the observed value under h0 and therefore provides good reason to reject it. on the contrary, a high p-value does not give proof that h0 is actually true. in fact, it could be that the alternative just provides an even worse explanation, or the test has very low power to distinguish the two alternatives."
600,1,"['data', 'statistical', 'set', 'statistical test', 'level', 'confidence', 'test', 'null hypothesis', 'hypothesis']", The General Procedure,seg_75,"as a consequence, a statistical test should always be stated such that the null hypothesis defines the “status quo” and gets rejected if the desired result shows in the data. thus, testing for a difference between a default and non-default assumption, we would set h0 to state that there is no difference and reject this hypothesis in favor of the alternative that there is a difference. this way, we can quantify our level of confidence in the rejection by a low p-value. would we set the no difference scenario as alternative, we would aim at “proving” h0 with the data, which we can’t."
601,1,"['test statistic', 'data', 'statistic', 'probability', 'test']", The General Procedure,seg_75,"very informally, let t be our test statistic with value t for some specific data. then, we compute p(t ≥ t |h0), the probability to see this data if h0 is true, which is not p(h0|t ≥ t), the probability of h0 being true, given the data."
602,1,"['error', 'statistical hypothesis', 'data', 'errors', 'hypotheses', 'parameter', 'statistical', 'null hypothesis', 'hypothesis']", The General Procedure,seg_75,"stating the hypotheses. stating a null hypothesis h0 to reflect the default assumptions can become quite intricate for more involved problems. in particular, it might not be straightforward to formulate the hypothesis in terms of parameter regions or even to find the formal statistical hypothesis that correctly reflects our verbal hypothesis on the data. this lead some researchers to introduce a type-iii error (see sect. 3.5 for type-i/ii errors), which is often stated as “asking the wrong question and using the wrong h0”, or “correctly rejecting h0, but for the wrong reasons”."
603,1,"['case', 'rejection region', 'mean', 'rejection regions', 'test', 'null hypothesis', 'hypothesis']", The General Procedure,seg_75,"additionally, we have also to take some care in stating a useful alternative hypothesis. as we already saw, the choice of the alternative partly determines the rejection region. for the case of one mean μ, the decision for a twoor a one-sided alternative can usually be decided from the problem itself. however, imagine we were to test two means at the same time with null hypothesis h0 : μa > 0, μb > 0. a reasonable alternative is h1 : μa < 0, μb < 0, but it is very strict and we may want to relax it to h1 : (μa < 0 or μb < 0). depending on which alternative we select, this leads to different rejection regions."
604,1,"['estimates', 'location', 'sample', 'experiment', 'statistical tests', 'samples', 'mean', 'tests', 'statistical', 'model', 'distribution', 'variance', 'control', 'normal']", Testing the Mean of Normally Distributed Data,seg_77,"one of the most frequent applications of statistical testing involves questions about the location of a distribution. in particular, we are often interested whether the mean of a given sample deviates significantly from an assumed value or whether two samples have the same mean. a typical example is the comparison of the result of an experiment to a control. this control can either be an assumed model, predicting the mean, or another experiment. in this section, we will discuss statistical tests to answer these questions with decreasing amount of additional assumptions. we start with a pedagogical example where we assume the distribution to be normal and the variance to be known, before introducing the family of t-tests which allow us to use estimates for the variance."
605,1,"['quantitative', 'empirical quantiles', 'subjective', 'results', 'quantiles', 'data', 'tests', 'distribution', 'test', 'plot', 'deviation', 'test results', 'normality', 'normal', 'normal distribution']", Testing the Mean of Normally Distributed Data,seg_77,"these tests are not applicable for non-normal data and we should always check the data for normality before performing any of them. we can do this visually using a normal quantile-quantile plot as introduced in sect. 1.8.3. if the empirical quantiles do not deviate too much from the straight line expected from a normal distribution, it is usually safe to use a t-test. of course, “too much” deviation is pretty subjective and one should always provide the q-q-plot in addition to the test results. for a more quantitative check, we might use the kolmogorov-smirnov test for testing normality in the data (sect. 3.4.1) or the more specialized shapiro-wilks test (sect. 3.4.2)."
606,1,"['independent', 'treatment', 'case', 'data', 'samples', 'set', 'mean', 'measuring', 'paired']", Testing the Mean of Normally Distributed Data,seg_77,"while not directly testing the mean, the wilcoxon-test is often used for similar purposes on non-normally distributed data; it will be introduced in sect. 3.4.3. we also only consider the two-sample case with independent data. if the two samples happen to be paired, for example, by measuring the weight of the same set of people before and after a treatment and comparing the two values for each patient, a paired t-test needs to be used."
607,1,"['hypotheses', 'case']", Known Variance,seg_79,"let us assume that we measure the weights x1, . . . , xn of n people from a particular region. we claim that people are substantially heavier in this region than a particular weight μ0. thus, our two hypotheses are informally “h0 : weights are pretty much μ0” versus “h1 : weights are larger than μ0”. note that the case of weights lower than μ0 is not covered."
608,1,"['estimate', 'data', 'distribution', 'normal', 'variance', 'normal distribution']", Known Variance,seg_79,"clarifying assumptions. we assume that we have reason to believe (or actually checked) that the measured weights follow a normal distribution, so xi ∼ norm (μ, σ 2). for simplicity, we further assume that we know the variance σ 2 and do not need to estimate it from the data. clearly, this is often an unrealistic assumption, and we will discard it in sect. 3.3.2."
609,1,['hypotheses'], Known Variance,seg_79,formulating the hypotheses. the two hypotheses can be formally written as
610,0,[], Known Variance,seg_79,"h0 : μ = μ0 h1 : μ > μ0,"
611,0,[], Known Variance,seg_79,or equivalently
612,0,[], Known Variance,seg_79,h0 : μ − μ0 = 0
613,0,[], Known Variance,seg_79,h1 : μ − μ0 > 0.
614,1,"['set', 'data']", Known Variance,seg_79,"the alternative is composite, as it encompasses a set of possible values, and onesided, as it ignores the possibility that the data is actually smaller than the claimed value."
615,1,"['estimate', 'random number', 'test statistic', 'statistic', 'mean', 'random', 'test', 'estimator']", Known Variance,seg_79,"choosing the test statistic. since we do not know the true mean μ, we estimate it by x̄ = μ̂n , and thus compare a random number with the claimed mean μ0. this is an example where we use an estimator as the test statistic."
616,1,"['parameters', 'estimate', 'arithmetic mean', 'distribution', 'normal', 'mean', 'variance', 'normal distribution']", Known Variance,seg_79,"deriving the distribution under h0. as an mle, the arithmetic mean x̄ itself has a normal distribution with parameters e(x̄) = μ and var(x̄) = σ 2/n (this is where the known variance comes into play: the normal distribution is only correct if we do not need to estimate the variance)."
617,1,"['false positive', 'case', 'distribution', 'mean', 'probability', 'level', 'test', 'variance', 'distributions']", Known Variance,seg_79,"fixing a test level α, and thus the allowed probability for a false positive, only requires the distribution of x̄ under h0, which in this case is x̄ ∼ norm(μ0, σ 2/n). the composite alternative describes a whole family of distributions, namely all normal distributions with variance σ 2/n and any mean μ > μ0."
618,1,"['alternative hypothesis', 'sample', 'distribution', 'mean', 'hypothesis', 'null distribution', 'variance', 'distributions']", Known Variance,seg_79,"in fig. 3.2, the distributions of x̄ for n = 20 sample points and a variance of σ 2 = 40 are shown for a true mean of μ = 10 (solid line), of μ = 14 (dashed line), and of μ = 25 (dotted line). testing μ = μ0 = 10 thus leads to the null distribution depicted by a solid line, while the dashed and dotted lines describe two distributions out of infinitely many given in the alternative hypothesis."
619,1,"['arithmetic mean', 'rejection region', 'mean', 'level', 'test']", Known Variance,seg_79,"computing the rejection region. for an observed arithmetic mean x̄ , the p-value for the test is p(μ̂n ≥ x̄ |h0) = 1 − (x̄;μ0, σ 2/n). let us fix a test level of α = 0.05. the rejection region is easily calculated as (c,∞), where c is the value such that p(x̄ ≥ c|h0) = 0.05, which computes to"
620,0,[], Known Variance,seg_79,σ c = μ0 + z1−α . √n
621,1,['rejection region'], Known Variance,seg_79,"for the proposed α, we have z0.95 = 1.645, which for μ0 = 10 leads to the rejection region"
622,0,[], Known Variance,seg_79,"r0.95 = (12.326,∞),"
623,0,[], Known Variance,seg_79,"and we reject h0 : μ = μ0 = 10 if x̄ > 12.326. for smaller differences, we do not reject."
624,1,"['false positive', 'probabilities', 'false negative', 'probability', 'level', 'error']", Known Variance,seg_79,"the probability of a false positive for the level α = 0.05 is given in fig. 3.2 by the solid gray area. for a true value of μ = 14, this level leads to probability of 0.118 for a false negative (h0 not rejected although false), given by the dashed gray area, and a false positive probability of 0 for a true value of μ = 25. we will come back to these two error probabilities in sect. 3.5."
625,1,"['normalized', 'standard normal', 'distribution', 'normal', 'mean', 'standard', 'standard normal distribution', 'normal distribution']", Known Variance,seg_79,"equivalently, we can conclude that the normalized difference of observed and claimed mean has a standard normal distribution,"
626,0,[], Known Variance,seg_79,"x̄ − μ0 t := ∼ norm(0, 1), (3.1) σ √n1"
627,1,"['quantile', 'realization']", Known Variance,seg_79,"and we reject if its realization t exceeds the corresponding quantile, i.e., if"
628,0,[], Known Variance,seg_79,t > z1−α.
629,1,"['rejection region', 'test', 'intervals']", Known Variance,seg_79,"the two-sided alternative. if we test against the two-sided alternative h1 : μ = μ0, the rejection region consists of two intervals, which contain values that are considered too low and too high, respectively. very similar to before, the rejection region rα = (−∞, c1) ∪ (c2,+∞) is computed such that"
630,0,[], Known Variance,seg_79,p(x̄ ∈ rα|h0) = α
631,0,[], Known Variance,seg_79,"holds, which immediately leads to"
632,0,[], Known Variance,seg_79,"σ σ rα = (−∞, μ0 + zα/2 √n ) ∪ ("
633,0,[], Known Variance,seg_79,"μ0 + z1−α/2 √n ,+∞) ."
634,1,"['normalized', 'parameters', 'level']", Known Variance,seg_79,"we thus reject h0 if x̄ ∈ rα , or, equivalently, if |t | > z1−α/2, with t the normalized difference defined by (3.1). for the level α = 0.05 and parameters as before, the two critical thresholds are c1 = 7.228 and c2 = 12.772."
635,1,"['confidence intervals', 'interval', 'test statistic', 'intervals', 'rejection region', 'statistic', 'mean', 'level', 'confidence', 'test', 'estimator', 'null hypothesis', 'hypothesis']", Known Variance,seg_79,"rejection regions and confidence intervals. there is a striking similarity between the rejection region rα of the test statistic t and the (1 − α)-confidence interval of the estimator x̄ for the mean. indeed, if we use any estimator θ̂n directly as the test statistic, we reject the null hypothesis h0 : θ = θ0 at the level α if θ is not contained in the (1 − α)-confidence interval of θ̂n ."
636,1,"['tests', 'sample', 'sample mean', 'estimate', 'data', 'cases', 'samples', 'mean', 'variance', 'test', 'null hypothesis', 'hypothesis']", Unknown Variance tTests,seg_81,"clearly, the assumption that we do not know the mean of a sample—but exactly know the variance—is an oversimplification in most practical cases. we will therefore drop this assumption and additionally estimate the variance from the data, which leads to the t-distribution under the null hypothesis. the corresponding family of tests is then known as t-test. we describe these tests for three cases: (i) we test one sample mean for a specific value, but need to estimate the variance, (ii) we test if the means of two samples are equal, with both samples having the same, but unknown variance, (iii) we test if the means of two samples are equal, each sample having unknown variance."
637,1,"['data', 'normally distributed', 'cases', 'variances']", Unknown Variance tTests,seg_81,"we will again assume that the data x1, . . . , xn (and y1, . . . , ym for the twosample cases) are normally distributed with means μx and μy , and variances σx"
638,0,[], Unknown Variance tTests,seg_81,"2 and σy2, respectively."
639,1,"['sample', 'variance']", Unknown Variance tTests,seg_81,"one sample with unknown variance. let us consider n sample points from a norm(μx , σx"
640,1,"['distribution', 'variance', 'mean']", Unknown Variance tTests,seg_81,"2 ) distribution, where both the mean μx and the variance σx"
641,0,[], Unknown Variance tTests,seg_81,2 are
642,1,"['estimated', 'estimate']", Unknown Variance tTests,seg_81,unknown and need to be estimated. we estimate σx
643,0,[], Unknown Variance tTests,seg_81,2 as before by
644,0,['n'], Unknown Variance tTests,seg_81,n 1 sx
645,0,['n'], Unknown Variance tTests,seg_81,2 = σ̂x 2 = ∑(xi − x̄)2. n − 1 i=1
646,1,"['normalized', 'test', 'hypothesis']", Unknown Variance tTests,seg_81,"to test the hypothesis h0 : μx = μ0, we can again use the normalized difference"
647,0,[], Unknown Variance tTests,seg_81,x̄ − μ0 t = sx √n1
648,1,"['degrees of freedom', 'interval', 'statistic', 'estimator', 'normalized', 'estimate', 'data', 'mean', 'confidence', 'confidence interval', 'test statistic', 'distribution', 'test', 'variance', 'absolute value', 'estimated', 'normal', 'tails', 'normal distribution']", Unknown Variance tTests,seg_81,"between observed and expected mean as test statistic, this time replacing the variance by its estimate. the test statistic t does not have a normal distribution, but a t-distribution with n − 1 degrees of freedom. the reason for this is exactly the same as for computing the confidence interval for the mean’s estimator x̄ with estimated variance: the estimate of the mean from the data became more uncertain, leading to slightly heavier tails of the distribution. we consequently reject in favor of the twosided alternative h1 : μx = μ0 if the normalized difference of claimed and observed mean is too large and the absolute value of the test statistic therefore exceeds the corresponding t-quantile:"
649,0,[], Unknown Variance tTests,seg_81,"reject h0 ⇐⇒ |t | > t1−α/2(n − 1),"
650,0,[], Unknown Variance tTests,seg_81,and similarly for the one-sided alternatives.
651,1,"['estimated', 'random samples', 'distribution', 'samples', 'normal', 'random', 'test', 'variance', 'normal distribution']", Unknown Variance tTests,seg_81,"example 26 with 10 random samples from a norm(15, 40) distribution, the test h0 : μx = 10 against the two-sided alternative h1 : μx = 10 gives a p-value of 0.02262 for the t-test, but a too low p-value of 0.00302 when using the normal distribution with estimated variance."
652,1,"['estimators', 'normally distributed', 'standard normal distribution', 'estimator', 'sample', 'sample mean', 'experiment', 'data', 'samples', 'mean', 'standard', 'standard normal', 'distribution', 'test', 'variance', 'control', 'estimated', 'normal', 'normal distribution', 'variances']", Unknown Variance tTests,seg_81,"two samples with unknown and unequal variances. in practice, one is often interested in testing whether two samples differ significantly in their mean rather than testing one sample mean against a given value. as an example, we might have some data xi from an experiment and some data yi from a control, and ask whether there is any difference in the samples’ means. this is known as a two-sample test problem. formally, we like to test h0 : μx = μy against the alternative, e.g., h1 : μx = μy . moreover, the samples are allowed to be of different size. as before, our test statistic will be the difference of the two estimated means. because both estimators are maximum-likelihood and therefore have a normal distribution, their difference is also normally distributed. the only thing we need to figure out is the variance of this difference, so we can normalize correctly to get a standard normal distribution for the estimator. recalling the elementary properties of the variance, the variance of the difference is quickly established:"
653,0,[], Unknown Variance tTests,seg_81,"var(x̄ − ȳ ) = var(x̄) + var(−ȳ ) + 2cov(x̄ , ȳ ) = var(x̄) + var(ȳ ),"
654,1,"['samples', 'independent']", Unknown Variance tTests,seg_81,"because the x and y samples are independent and thus cov(x̄ , ȳ ) = 0. thus,"
655,0,[], Unknown Variance tTests,seg_81,sx
656,0,['n'], Unknown Variance tTests,seg_81,2 sy2 v̂ar(x̄ − ȳ ) = + n m
657,1,"['test statistic', 'statistic', 'variance', 'test', 'estimator']", Unknown Variance tTests,seg_81,"is an estimator for the variance of the difference, which immediately leads to the test statistic"
658,0,[], Unknown Variance tTests,seg_81,x̄ − ȳ t = .
659,0,[], Unknown Variance tTests,seg_81,√n1 sx
660,0,[], Unknown Variance tTests,seg_81,1 sy2
661,1,"['degrees of freedom', 'true parameter', 'test statistic', 'distribution', 'statistic', 'parameter', 'test']", Unknown Variance tTests,seg_81,"however, computing the correct distribution of this test statistic is quite tedious and yields a formula that contains the true parameter values, which are of course not available. in practice, this distribution is therefore approximated by a t-distribution with ν (or less) degrees of freedom, where ν is computed as"
662,0,['n'], Unknown Variance tTests,seg_81,( s n
663,1,"['parameters', 'test statistic', 'statistic', 'test']", Unknown Variance tTests,seg_81,"and we reject h0 : μx = μy if |t | > t1−α/2(ν) in favor of h1 : μx = μy . most implemented versions of this test, such as the functiont.test() inr, automatically decide on the correct test statistic and the relevant parameters."
664,1,"['estimates', 'samples', 'variances']", Unknown Variance tTests,seg_81,"two samples with unknown but equal variances. if the two variances are expected to be equal or at least very similar, we can replace the two individual estimates sx"
665,1,"['variance', 'pooled variance']", Unknown Variance tTests,seg_81,2 and sy2 by the pooled variance
666,0,['n'], Unknown Variance tTests,seg_81,(n − 1)s2 + (m − 1)s2 sx
667,0,['n'], Unknown Variance tTests,seg_81,2 y = x y . n + m − 2
668,1,"['deviation', 'sample', 'degrees of freedom', 'normalized', 'estimate', 'arithmetic mean', 'test statistic', 'statistic', 'mean', 'standard', 'standard deviation', 'test', 'variance']", Unknown Variance tTests,seg_81,"this estimate looks quite similar to the usual estimate of a variance: we “lost” two degrees of freedom, one for each arithmetic mean and consequently divide by the total degrees of freedom from the overall sample, m +n−2. the test statistic is again the difference of the two observed means, normalized by the standard deviation of this difference:"
669,0,[], Unknown Variance tTests,seg_81,x̄ − ȳ t = . + sxy √n1 m
670,1,"['degrees of freedom', 'test statistic', 'statistic', 'test']", Unknown Variance tTests,seg_81,this test statistic has a t-distribution with m + n − 2 degrees of freedom and we reject h0 : μx = μy if |t | > t1−α/2(m + n − 2) in favor of h1 : μx = μy .
671,1,"['sample', 'sample means', 'distribution', 'samples', 'sets', 'distributions', 'test', 'null hypothesis', 'hypothesis']", Unknown Variance tTests,seg_81,"example 27 as a very brief example, let us look at a t-test performed on two sets of samples with n = 10 samples from a norm(10, 40) distribution and m = 13 samples from a norm(12, 30) distribution. in practice, we would not know the correct distributions and want to test whether the two sample means differ significantly. for this, we perform a two-sided t-test with null hypothesis h0 : μx − μy = 0 versus the alternative h1 : μx − μy = 0. using the r implementation of this test, we get the following report:"
672,1,['sample'], Unknown Variance tTests,seg_81,welch two sample t-test
673,1,"['alternative hypothesis', 'sample', 'interval', 'estimates', 'confidence', 'mean', 'hypothesis', 'percent', 'confidence interval']", Unknown Variance tTests,seg_81,"data: x and y t = -2.0876, df = 15.988, p-value = 0.0532 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -9.96370876 0.07680932 sample estimates: mean of x mean of y 7.705368 12.648818"
674,1,"['degrees of freedom', 'interval', 'statistic', 'sample', 'data', 'confidence', 'confidence interval', 'test statistic', 'level', 'test', 'estimated', 'variances']", Unknown Variance tTests,seg_81,"in the first line, r informs us that it uses a variant of the t-test, known as the welch-test, here in its two-sample form. it then reports the data used, the values for the test statistic, the estimated degrees of freedom, and the p-value. we note that the degrees of freedom are not integer, but were calculated using an interpolation formula due to the different sample sizes and variances. after giving the alternative used, it reports the 95% confidence interval for the test statistic and finally the two estimated means. note that the confidence interval is for the difference in means and should contain 0, if the means are the same. the test gives a p-value of 0.0532, which is significant at the α = 0.1, but not the α = 0.05 level."
675,1,"['statistics', 'samples', 'hypotheses', 'test statistics', 'mean', 'tests', 'test']", Other Tests,seg_83,"apart from testing the mean of one or two samples, there are many more hypotheses that can be tested using suitable test statistics. in this section, we will introduce a small variety of such tests and briefly explain their major ideas."
676,1,"['sample', 'cumulative distribution function', 'distribution function', 'empirical cumulative distribution function', 'test statistic', 'distribution', 'samples', 'statistic', 'function', 'test']", Testing Equality of Distributions KolmogorovSmirnov,seg_85,the kolmogorov-smirnov (ks) test checks whether a sample follows a given distribution (one-sample version) or if two samples are equally distributed (two-sample version). the test statistic is the maximal difference (technically: the supremum of the difference) of the empirical cumulative distribution function and the given distribution function:
677,0,[], Testing Equality of Distributions KolmogorovSmirnov,seg_85,d := sup ∣∣∣ f̂n(x) − f(x)∣∣∣ . x
678,1,"['distribution', 'sample']", Testing Equality of Distributions KolmogorovSmirnov,seg_85,"this difference takes values between zero and one and is larger, the more the sample distribution deviates from the given one."
679,1,"['distribution function', 'distribution', 'parameter', 'function']", Testing Equality of Distributions KolmogorovSmirnov,seg_85,"the ks-test works for any distribution. however, the distribution function f has to be completely specified by explicitly stating all parameter values."
680,1,['tests'], Testing Equality of Distributions KolmogorovSmirnov,seg_85,the two-sample version tests the two empirical cdfs f̂ and ĝ:
681,0,[], Testing Equality of Distributions KolmogorovSmirnov,seg_85,d := sup ∣∣∣ f̂n(x) − ĝn(x)∣∣∣ . x
682,1,"['sample', 'exponentially', 'parameters', 'distribution', 'normal', 'hypothesis', 'exponentially distributed', 'null hypothesis', 'normal distribution', 'distributions']", Testing Equality of Distributions KolmogorovSmirnov,seg_85,"an example is given in fig. 3.3, where 20 exponentially distributed sample points are generated for λ = 3 and their ecdf compared to the cdf of a normal distribution with parameters μ = 0.5 and σ 2 = 0.1. the largest difference is d = 0.3983 found at x = 0.4213, leading to a p-value of p = 0.002248. this indicates strong evidence against the null hypothesis and we would conclude that the distributions are indeed different."
683,1,"['tests', 'estimated', 'method', 'estimates', 'data', 'statistical tests', 'distribution', 'statistical', 'normal', 'normally distributed', 'test', 'variance', 'normal distribution']", Testing for Normality ShapiroWilks,seg_87,"normality of the data is a very common assumption for statistical tests and procedures. the shapiro-wilks test provides a more specialized alternative to the kolmogorov-smirnov test for testing if the data is indeed normally distributed. this test compares the variance of the data, estimated as s2 = 1/(n − 1)∑i (xi − x̄)2, with the variance expected by a normal distribution. for computing this expected variance b2, the test uses the same method as a normal q-q-plot and estimates the"
684,1,"['test statistic', 'statistic', 'normal', 'test', 'variances']", Testing for Normality ShapiroWilks,seg_87,slope of the normal q-q-line (see sect. 1.8.3). the test statistic is then the quotient of the two variances
685,0,['n'], Testing for Normality ShapiroWilks,seg_87,"b2 w = , (n − 1)s2"
686,1,"['sample size', 'sample', 'data', 'normal', 'test', 'limit']", Testing for Normality ShapiroWilks,seg_87,"and we test h0 : w = 1, expected if the data is normal, against h1 : w = 1. this test generally performs better than the corresponding ks-test and should be preferred, especially for small sample sizes. a sample size of n ≥ 3 is necessary for the test to work, and an upper limit is given with n ≈ 3000."
687,1,"['sample', 'variances', 'data', 'location', 'samples', 'normally distributed', 'locations', 'test', 'distributions']", Testing Location Wilcoxon,seg_89,"for testing whether two distributions are equal, we can also rely on another non-parametric test known as the wilcoxon or mann-whitney-u test. while not testing the location itself but rather the whole distributions, this test can nevertheless often be used as a non-parametric alternative to the t-test family if the data is not normally distributed, provided the variances in the samples are of comparable size and the sample distributions are similar enough such that the differences are mainly caused by different locations."
688,1,"['sample', 'wilcoxon test', 'samples', 'joint', 'test', 'distributions']", Testing Location Wilcoxon,seg_89,"the main idea of the wilcoxon test is the following: let us again assume that x1, . . . , xn and y1, . . . , ym are two samples of size n and m, respectively, and let us again denote by x(i) the ith smallest sample point, which we say to have rank i. we now join the samples into a new sample z and denote by z(i) the ith smallest sample point in the joint sample, so i runs from 1 to n + m. provided the two samples have similar distributions, the ranks of the two individual samples within the joint sample should distribute equally and we expect to see a good “mixing” of the two samples in z."
689,1,"['sample', 'wilcoxon test', 'case', 'samples', 'joint', 'test']", Testing Location Wilcoxon,seg_89,"the wilcoxon test now compares the sum of the ranks of x and y in z to decide whether this is indeed the case. for this, let us denote by rx the sum of all ranks of elements of the x sample in the ordered joint sample z. in practice, we check whether z(i) originally belonged to the x and if so, we add i to the sum, until we reach the (m + n)th element of z. similary, let us denote by ry the sum of ranks of the y samples in z, established by the same procedure."
690,0,[], Testing Location Wilcoxon,seg_89,the sum of all ranks is then
691,0,['n'], Testing Location Wilcoxon,seg_89,"n+m (n + m)(n + m + 1) rx + ry = ∑ i = , 2 i=1"
692,1,"['sample', 'joint']", Testing Location Wilcoxon,seg_89,"n and the first sample of size n provides the fraction of the joint sample. further,"
693,0,[], Testing Location Wilcoxon,seg_89,n+m rx is bounded by
694,0,['n'], Testing Location Wilcoxon,seg_89,"n n(n + 1) n(n + 1) ∑ i = ≤ rx ≤ nm + , 2 2 i=1"
695,1,"['sample', 'joint', 'case']", Testing Location Wilcoxon,seg_89,"as one easily verifies: the lower bound gives the case that the xi form the first n elements of the joint sample, i.e., are all lower than the y j , whereas the upper bound gives the case that the xi are all greater than the y j ."
696,0,[], Testing Location Wilcoxon,seg_89,we then form the respective difference of the actual rank-sum from the upper bound:
697,0,[], Testing Location Wilcoxon,seg_89,"n(n + 1) m(m + 1) ux = nm + − rx and uy = nm + − ry , 2 2"
698,1,"['test statistic', 'statistic', 'test']", Testing Location Wilcoxon,seg_89,and use the minimum of these two differences as our test statistic
699,0,[], Testing Location Wilcoxon,seg_89,"u := min(ux , uy )."
700,1,"['sample', 'location', 'distribution', 'cases', 'normal', 'mean', 'variance', 'null hypothesis', 'normal distribution', 'hypothesis']", Testing Location Wilcoxon,seg_89,"for small n and m, the null-distribution p(u = u|h0) can be tabulated by exhaustively computing all cases and counting. for large n and m, the null-distribution is very close to a normal distribution and it suffices to compute its mean and variance. provided the null hypothesis h0 of equal location is true, we expect the ordered x sample to be uniformly dispersed in the ordered z sample and thus"
701,0,['n'], Testing Location Wilcoxon,seg_89,"n n(n + m + 1) e(rx ) = (rx + ry ) = , n + m 2"
702,1,"['variance', 'case']", Testing Location Wilcoxon,seg_89,in which case the variance of the rank-sum is
703,0,[], Testing Location Wilcoxon,seg_89,"nm(n + m + 1) var(rx ) = , 12"
704,1,"['expectation', 'variance']", Testing Location Wilcoxon,seg_89,and similarly for ry . the desired expectation and variance for u are then easily computed.
705,1,"['observations', 'normally distributed', 'paired', 'data', 'samples', 'false positives', 'paired observations', 'test', 'wilcoxon test', 'normality', 'variances']", Testing Location Wilcoxon,seg_89,"t-or wilcoxon test? the t-test is slightly more sensitive to false positives than the wilcoxon-test for normally distributed data, but this advantage decreases rapidly as the data deviates from normality. the wilcoxon-test performs much better on non-normal data and is available in as many variants (oneand two-sided, multidimensional, paired observations, etc.). the wilcoxon-test is also more conservative: if we reject using this test, we would also always reject using a t-test. we can therefore almost always use the wilcoxon-test, unless we are very sure that the data is normally distributed, the variances of the two samples are very different, or we explicitly want to test the mean(s)."
706,1,"['probabilities', 'multinomial', 'frequencies', 'data', 'test', 'null hypothesis', 'hypothesis']", Testing Multinomial Probabilities Pearsons χ,seg_91,"for multinomial data, like the nucleotide counts in a sequence of length n, we might want to test whether the probabilities for each category follow a particular distribution, given by the probabilities (p10, . . . , ps0). for example, one might want to test the hypothesis that the nucleotide frequencies in the sequence are all equal by testing the null hypothesis"
707,0,[], Testing Multinomial Probabilities Pearsons χ,seg_91,"h0 : (pa, pc , pg , pt ) = (p0a, pc"
708,0,[], Testing Multinomial Probabilities Pearsons χ,seg_91,"0 , pg 0 , pt 0 ) = (1/4, 1/4, 1/4, 1/4)"
709,0,[], Testing Multinomial Probabilities Pearsons χ,seg_91,against the alternative
710,0,[], Testing Multinomial Probabilities Pearsons χ,seg_91,"h1 : (pa, pc , pg , pt ) = (1/4, 1/4, 1/4, 1/4)."
711,1,"['normalized', 'contrast', 'probabilities', 'statistic', 'test', 'null hypothesis', 'hypothesis']", Testing Multinomial Probabilities Pearsons χ,seg_91,"in contrast to previous examples, we test all probabilities simultaneously. pearson’s χ2-test compares the expected number of counts in each category to the observed count xi . the expected count under the null hypothesis is e(xi ) = npi0. the χ2-test statistic is then given by the sum of normalized differences"
712,0,[], Testing Multinomial Probabilities Pearsons χ,seg_91,s (xi − npi0)2 t = ∑ 0 npi i=1
713,1,"['distribution', 'null hypothesis', 'hypothesis']", Testing Multinomial Probabilities Pearsons χ,seg_91,and has a χ2(s − 1) distribution under the null hypothesis.
714,1,"['prediction', 'categories', 'frequencies']", Testing Multinomial Probabilities Pearsons χ,seg_91,"example 28 recall that gregor mendel studied peas in order to find the laws of inheritance. more specifically, he cross-bred peas and observed counts for two features, namely round/wrinkled and yellow/green, leading to four categories which we abbreviate as ry,rg,wy,wg, respectively. the theory of inheritance leads to the prediction that the frequencies for these categories should be"
715,0,[], Testing Multinomial Probabilities Pearsons χ,seg_91,"(pr0y, pw 0 y, pr0g, pw 0 g) = (9/16, 3/16, 3/16, 1/16) ."
716,1,"['test statistic', 'statistic', 'test']", Testing Multinomial Probabilities Pearsons χ,seg_91,"in his original publication [1], mendel studied n = 556 peas and observed the counts x = (315, 101, 108, 32). this leads to a value of t = 0.47 for the test statistic. using"
717,1,"['degrees of freedom', 'null hypothesis', 'data', 'mean', 'hypothesis', 'level', 'test', 'critical value']", Testing Multinomial Probabilities Pearsons χ,seg_91,"the χ2-distribution with 3 degrees of freedom, the critical value for a test level of α = 0.05 is given by the 0.95-quantile χ2(3, 0.95) = 7.815, and we do not reject the null hypothesis. indeed, the p-value for this data is 0.95. as we discussed in sect. 3.2, however, this does not necessarily mean that we have evidence in favor of h0."
718,1,"['model', 'predicted', 'observations', 'data', 'number of observations', 'test']", Testing GoodnessofFit,seg_93,"the χ2-test can also be used as a goodness-of-fit test. this type of test is used for example to check whether some data is in agreement with given predicted values from a model. in the above example of mendel’s peas, the model of inheritance suggests a certain number of observations in each category, and the test compares this predicted numbers with the observed ones."
719,1,"['density function', 'model', 'parameters', 'results', 'case', 'data', 'normally distributed', 'distribution', 'function']", Testing GoodnessofFit,seg_93,"let us assume that a model predicts that the results follow a certain distribution with density function f (x; θ). the l parameter(s) θ are not known. as an example, we might predict that the data is normally distributed, in which case θ = (μ, σ 2) are the l = 2 unknown parameters."
720,1,"['sample', 'continuous distribution', 'parameters', 'interval', 'continuous', 'range', 'observations', 'data', 'intervals', 'distribution', 'probability', 'number of observations']", Testing GoodnessofFit,seg_93,"for sample data x1, . . . , xn , the probability to see each particular sample point xi is zero for a continuous distribution. the main idea is thus to discretize the result by splitting the range of possible values into s non-overlapping intervals and count the number of observations in each interval. with i j the jth such interval and some fixed parameters θ ,"
721,0,[], Testing GoodnessofFit,seg_93,p j (θ) = ∫ f (x; θ)dx
722,1,"['interval', 'predicted', 'observations', 'observation', 'parameter', 'probability']", Testing GoodnessofFit,seg_93,is the predicted probability to see an observation in this interval and we would therefore expect to see np j (θ) observations using this parameter value.
723,1,"['observations', 'statistics', 'test statistics', 'number of observations', 'test']", Testing GoodnessofFit,seg_93,we can now compare this expected number of observations with the observed number using the test statistics
724,0,['n'], Testing GoodnessofFit,seg_93,"s (n j − np j (θ))2 t = ∑ , np j (θ) i=1"
725,1,['degrees of freedom'], Testing GoodnessofFit,seg_93,which has a χ2-distribution with s − 1 − l degrees of freedom.
726,1,"['parameters', 'estimate', 'distribution', 'function', 'null hypothesis', 'hypothesis']", Testing GoodnessofFit,seg_93,"we are left with finding a general way to estimate the parameters θ . for reasons that cannot be properly explained here, the mle is not a good choice, as it does not necessarily lead to a known (let alone χ2) distribution of t under the null hypothesis. instead, we estimate the parameters such that they maximize the objective function"
727,0,[], Testing GoodnessofFit,seg_93,"q(θ) = ∏ p j (θ)n j , j=1"
728,1,"['statistics', 'test statistics', 'test']", Testing GoodnessofFit,seg_93,which then leads to the desired χ2-distribution of the test statistics.
729,1,"['model', 'data', 'distribution', 'hypothesis']", Testing GoodnessofFit,seg_93,"again, absence of evidence is not evidence of absence, and if the data does not give evidence to reject h0, we cannot conclude that the hypothesis is actually correct. this is a major drawback of goodness-of-fit testing: we can only use it to check if the model could potentially explain the data, but we cannot rule out that there are other models that might fit as good. as the alternative is two-sided and composite, we also cannot flip the hypothesis, as we would need to know the distribution of t under this two-sided composite hypothesis."
730,1,"['false positive', 'statistic', 'random sample', 'random', 'sample', 'data', 'hypothesis testing', 'random variable', 'mean', 'error', 'false negative', 'test statistic', 'test', 'variance', 'outcomes', 'hypothesis', 'table', 'hypothesis test', 'errors', 'variable', 'normal']", Sensitivity and Specificity,seg_95,"in any hypothesis testing procedure, there are two types of errors that occur due to the fact that the test statistic is a random variable. there is the possibility that although h0 is true, we observe many unusual values in the data just by chance, leading to an incorrect rejection of h0. this is called a false positive or a type-i error. on the other hand, there is also the possibility that h0 is false, but again the sample values suggest that it is correct, so we would incorrectly not reject h0, which is called a false negative or type-ii error. correctly rejecting or not rejecting h0 is called a true positive and true negative, respectively. an example for type-i and type-ii errors is already given in fig. 3.2, where we tested the mean of a normal random sample with known variance. the four possible outcomes of a hypothesis test are given in the following table:"
731,1,"['power of the test', 'test statistic', 'rejection region', 'statistic', 'probability', 'parameter', 'function', 'test', 'error']", Sensitivity and Specificity,seg_95,"the probability 1 − β to not make a type-ii error is often referred to as the power of the test; it is a function of the true value θ of the tested parameter. formally, the power function η(θ) is the probability that the test statistic takes a value in the rejection region, provided that h1 is true:"
732,0,[], Sensitivity and Specificity,seg_95,η(θ) := 1 − β(θ) = p(θ̂n ∈ rα|h1).
733,1,"['error', 'sensitivity', 'power of a test', 'probability', 'specificity', 'level', 'test', 'null hypothesis', 'hypothesis']", Sensitivity and Specificity,seg_95,"the power of a test is also called its sensitivity, as it gives the probability to correctly reject the null hypothesis if the alternative is true and therefore describes how sensitive the test is to the alternative. in addition, the probability 1 − α of not making a type-i error, and therefore correctly not rejecting h0 when it is true, is called the specificity of the test, as it describes how good the test can identify h0 among the alternatives. choosing a test level α therefore prescribes a fixed specificity for the test."
734,1,"['data', 'sensitivity', 'test', 'null hypothesis', 'hypothesis']", Sensitivity and Specificity,seg_95,"specificity and sensitivity are adversaries: making one of them very high usually reduces the other. take the following test as an example: the null hypothesis is always accepted, no matter how the data look like. clearly, this test is very specific: it always correctly detects the null hypothesis if it is true. on the other hand, it never correctly"
735,1,"['specificity', 'sensitivity', 'null hypothesis', 'hypothesis']", Sensitivity and Specificity,seg_95,"detects the alternative and thus has very low sensitivity. indeed the specificity is one, the maximal possible value, while the sensitivity is zero, the minimal possible value. always rejecting the null hypothesis gives the inverse picture."
736,1,"['sample', 'normal', 'mean', 'function', 'test']", Sensitivity and Specificity,seg_95,"example 29 in sect. 3.3.1, we tested whether the mean of a normal sample was equal to a given value. the power of this test is η(μ) = 0.882 for μ = 15 and η(μ) = 1 for μ = 25 (fig. 3.2). the power function is"
737,0,[], Sensitivity and Specificity,seg_95,"η(μ) := 1 − β(μ) = p(x̄ > c|h1),"
738,1,"['true parameter', 'estimated', 'test statistic', 'case', 'data', 'rejection region', 'statistic', 'parameter', 'test', 'null hypothesis', 'hypothesis']", Sensitivity and Specificity,seg_95,"where μ is the correct parameter value under h1. the power is lowest if the true parameter of the alternative is close to the assumed value of the null hypothesis and it is difficult to distinguish data produced under either hypothesis. the larger the difference, the higher the power. for a difference of true and estimated parameter values μ − μ0 > 5, the two-sided test reliably detects the alternative. the one-sided test has very low power for true values smaller than μ0, as it does not test against these values. almost none of these values would lead to a test statistic inside the rejection region, so the test will not reject h0. this is correct: the one-sided alternative does not provide a better explanation of the data than the null hypothesis in this case."
739,1,"['functions', 'level', 'function', 'test', 'critical value']", Sensitivity and Specificity,seg_95,"we compute the power function of the one-sided test as η(μ) = 1 − (c;μ, σ 2), where again c = μ + z1−ασ/√n is the critical value at the given test level. the power function for the two-sided alternative is η(μ) = 1 − ( (ch;μ, σ 2) − (cl;μ, σ 2)), where cl = μ + zα/2σ/√n and ch = μ + z1−α/2σ/√n are the low and high critical value, respectively. the resulting functions are given in fig. 3.4 (left) for the one-sided alternative h1 : μ > μ0 (solid line) and for the two-sided alternative h1 : μ = μ0 (dashed line)."
740,1,"['sample size', 'sample', 'power of the test', 'level', 'range', 'sensitivity', 'function', 'test']", Sensitivity and Specificity,seg_95,"for a larger sample size of n = 100, the power of the test improves, as shown by the dashed line in fig. 3.4 (right). the power function gets narrower, meaning that the test has better sensitivity closer to the value of h0, and can thus better distinguish h0 and h1 in this range. decreasing the test level to α = 0.001 (at sample size"
741,1,['function'], Sensitivity and Specificity,seg_95,"n = 20) makes the alternative more difficult to distinguish for values near μ0, but much better further away (dotted line). note that for μ = μ0, the power function always takes value η(μ0) = α."
742,1,"['multiple tests', 'wilcoxon tests', 'sample', 'experiment', 'results', 'data', 'samples', 'false positives', 'tests', 'level', 'control', 'test', 'measurement', 'test results', 'experiments']", Multiple Testing,seg_97,"so far, we always assumed that we are given one or two samples and perform one test on these samples. many measurement techniques, however, generate massive amounts of data on which thousands of tests are performed simultaneously. in biology, the microarray allows to measure thousands of genes from the same biological sample, and differences in gene expression are tested for each individual gene. the genome of the fruit-fly, for example, has around 14,000 genes, which are all measured on one array. assuming we test for differences in gene expression using wilcoxon tests at a level of α = 0.05, we would already expect to see 14,000 × 0.05 = 700 false positives, i.e., tests which claim that there is a significant difference between control and experiment, when in fact the difference is just by chance more extreme than expected. because each of these 700 test results would lead to time-consuming and expensive follow-up experiments, we need to think about techniques that would allow us to deal with multiple tests in a useful way."
743,1,"['hypotheses', 'false negatives', 'false positives', 'tests', 'realization']", Multiple Testing,seg_97,"general setting. let us assume that we perform k tests simultaneously. let v be the number of false positives, u the number of true positives, and s, t the number of true and false negatives, respectively, with k0 correct and k − k0 incorrect nullhypotheses. importantly, we do not know k0, but can of course observe the constant k and the realization r of r, the number of rejected hypotheses."
744,1,"['false positive', 'rate', 'levels', 'level', 'test', 'error', 'multiple testing']", Multiple Testing,seg_97,"there are two general strategies to cope with the problems posed by multiple testing: the family-wise error rate (fwer) tries to correct the individual test levels α such that an overall test level of α∗ is achieved. basically, these approaches try to calculate new test levels such that the chance of even one false positive is smaller than α∗ and thus"
745,0,[], Multiple Testing,seg_97,fwe = p(v > 0) ≤ 1 − (1 − α)k = α∗.
746,1,['tests'], Multiple Testing,seg_97,"one such approach is the bonferroni-correction, discussed below in sect. 3.6.1. this approach works well if the number of tests is comparatively small."
747,1,['tests'], Multiple Testing,seg_97,"more recently, benjamini and hochberg introduced the concept of a falsediscovery-rate (fdr), which works remarkably well even for a large number of tests. we will discuss the details in sect. 3.6.2. in essence, the fdr is the expected false discovery ratio of false and all positives:"
748,0,['e'], Multiple Testing,seg_97,v v fdr = e ( r ) = e ( s + v ) .
749,1,"['method', 'false positives']", Multiple Testing,seg_97,this method explicitly allows a certain number of false positives to occur and aims at controlling the fraction of false over all positives by recalibrating the individual p-values.
750,1,"['false positive', 'rate', 'data', 'control', 'probability', 'event', 'tests', 'error', 'hypothesis']", BonferroniCorrection,seg_99,"provided we are only interested in performing a small number of tests on the data, we can use the conservative bonferroni-correction. this correction implements a strategy to control the family-wise error rate. here is the idea: let fi be the event that hypothesis i is a false positive (it is rejected although it is actually true). the probability to have at least one such false positive in k tests is then"
751,0,[], BonferroniCorrection,seg_99,p(v > 0) = p ( i
752,0,[], BonferroniCorrection,seg_99,⋃ =1
753,0,[], BonferroniCorrection,seg_99,fi) ≤ ∑ i=1
754,0,[], BonferroniCorrection,seg_99,p(fi ) = ∑ i=1
755,0,[], BonferroniCorrection,seg_99,α = kα = α∗.
756,1,"['hypothesis', 'level', 'test', 'null hypothesis']", BonferroniCorrection,seg_99,"to guarantee an overall prescribed test level of α∗, we therefore perform each individual test at level α∗/k and reject the ith null hypothesis if its p-value pi is smaller, i.e., if"
757,0,[], BonferroniCorrection,seg_99,α∗ pi < . k
758,1,"['false positive', 'independent', 'results', 'case', 'cases', 'probability', 'level', 'tests', 'test', 'inequality']", BonferroniCorrection,seg_99,"the bonferroni-correction is very conservative as it tries to bring the overall probability of even one false positive down to α∗. additionally, the inequality compares the true probability of the fi to the case that the fi are all independent, and does not try to capture the dependencies. this yields the new individual test level of α∗/k, which in many cases is much lower than would be required. for the introductory microarray example, we have k = 14,000 individual tests. for an overall false positive probability of α∗ = 0.05, we thus need an individual test level of α∗/k = 0.05/14,000 ≈ 4e − 06, which is unrealistic to yield any meaningful results."
759,1,"['false positive', 'rate', 'levels', 'false discovery rate', 'control', 'probability', 'false positives', 'tests', 'test']", FalseDiscoveryRate FDR,seg_101,"instead of correcting the test levels such that the overall false positive probability is kept below a given threshold α∗, we can try to explicitly allow false positives to occur, but to control their expected fraction among all positives. this false discovery rate (fdr) is given by fdr = e (v/r). again, the number of tests k is known,"
760,1,"['false positives', 'random variable', 'variable', 'hypotheses', 'mean', 'random', 'realization', 'hypothesis']", FalseDiscoveryRate FDR,seg_101,"and the number of rejected hypotheses r is a random variable whose realization r can be observed. on the other hand, the number of false positives v is also a random variable, but its realization can not be observed. intuitively, if we would pick one of the rejected hypotheses at random, the fdr can be interpreted as the expected chance that this hypothesis was falsely rejected. for 100 rejected hypotheses, an fdr of 0.05 would also mean that we expect that 100 × 0.05 = 5 of these are false positives. to achieve a certain fdr in a concrete situation, we need to choose the number of rejections such that the prescribed fdr is reached; we can do this by appropriately adjusting the p-value for which to reject."
761,1,"['data', 'set', 'hypotheses', 'false positives', 'level', 'tests', 'test', 'realization', 'hypothesis']", FalseDiscoveryRate FDR,seg_101,"benjamini-hochberg procedure. we would like to have strategy that guarantees a proportion of false positives of at most q∗ among all rejected hypotheses. for this, we calculate at which p-value to reject a hypothesis such that the fdr stays below this desired threshold q∗ as follows: we expect e(v ) = αk false positives in k tests for a given test level α. for a given set of data, the number of null hypotheses rejected at this level is r(α), a realization of r. the benjamini-hochberg procedure computes this number r(α) such that the maximal number of hypotheses are rejected while still keeping the expected proportion of false positives below the given threshold q∗, thus"
762,0,['e'], FalseDiscoveryRate FDR,seg_101,v αk e ( r ) = r(α) ≤ q∗.
763,1,"['hypothesis test', 'hypothesis', 'test']", FalseDiscoveryRate FDR,seg_101,let again pi be the p-value of the ith hypothesis test and consider the ordered p-values p(1) ≤ · · · ≤ p(k). we then compute the largest index l such that
764,0,[], FalseDiscoveryRate FDR,seg_101,i p(i) ≤ q∗ k
765,0,[], FalseDiscoveryRate FDR,seg_101,for all (i) < l. the values qi = k
766,0,[], FalseDiscoveryRate FDR,seg_101,i q∗ are sometimes called the q-values. one can
767,1,['hypotheses'], FalseDiscoveryRate FDR,seg_101,show that if we reject those null hypotheses for which pi ≤ ql = k
768,0,[], FalseDiscoveryRate FDR,seg_101,"l q∗,"
769,0,[], FalseDiscoveryRate FDR,seg_101,"k0 fdr ≤ q∗ ≤ q∗, k"
770,1,['false positives'], FalseDiscoveryRate FDR,seg_101,which guarantees the desired proportion of false positives.
771,1,"['results', 'data', 'test results', 'level', 'tests', 'test']", FalseDiscoveryRate FDR,seg_101,"comparing p-values and q-values. despite some similarities, p-values and q-values have some fundamental differences. the p-value gives the smallest test level at which not to reject and thus needs to be correct and exact to assess the data. on the other hand, q∗ is a threshold for an expected ratio, so the actual ratio might be higher. it serves more as a “calling” tool that filters out uninteresting test results from a large number of performed tests. a proper analysis would then further investigate the remaining candidates, so it is usually not problematic if the desired q∗ is not exactly achieved in the actual study."
772,1,"['data', 'tests']", FalseDiscoveryRate FDR,seg_101,example 30 let us consider the scenario that k = 10 individual tests were performed on data and that their ordered p-values p(i) are
773,0,[], FalseDiscoveryRate FDR,seg_101,"0.00017, 0.003, 0.0071, 0.0107, 0.014, 0.32, 0.4, 0.54, 0.58, 0.98."
774,1,"['bonferroni correction', 'levels', 'method', 'hypotheses', 'false positives', 'level', 'test']", FalseDiscoveryRate FDR,seg_101,"for an overall test level of α∗ = 0.05, the bonferroni correction then requires individual test levels of α = α∗/k = 0.005 and we would reject the first 2 null hypotheses. on the other hand, we could decide to allow an expected fraction of q∗ = 0.05 false positives among all rejected hypotheses. using the benjaminihochberg method, we compute the corresponding q-values to be"
775,0,[], FalseDiscoveryRate FDR,seg_101,"0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.045, 0.05,"
776,1,['hypotheses'], FalseDiscoveryRate FDR,seg_101,and p(5) = 0.014 < 0.025 = q5 whereas p(6) = 0.32 > 0.03 = q6; we would consequently be able to reject the first 5 null hypotheses.
777,1,"['experiment', 'data', 'level', 'tests', 'test', 'null hypothesis', 'hypothesis']", Combining Results of Multiple Experiments,seg_103,"let us imagine that three groups independently did a particular experiment and each group performed the same test on their data. the reported p-values are 0.08, 0.06, and 0.07, respectively, none of them significant at the α = 0.05 level. however, we might argue that it seems unlikely that all three tests are so close to the 0.05-level just by chance and that combined, the three tests actually give significant evidence to reject the null hypothesis."
778,1,"['independent', 'test statistic', 'statistics', 'cases', 'test statistics', 'statistic', 'joint', 'hypothesis', 'joint probability', 'probability', 'tests', 'test', 'null hypothesis']", Combining Results of Multiple Experiments,seg_103,"we can invoke the following argument to justify this idea: let ti be the value of the test statistic ti of test i, leading to a p-value of pi , and let us again assume k independent tests. if the null hypothesis were true in all cases, we can ask for the joint probability of simultaneously observing the given values of the test statistics:"
779,0,[], Combining Results of Multiple Experiments,seg_103,"p(t1 > t1, . . . , tk > tk |h0) = ∏ p(ti > ti |h0), i=1"
780,1,['probability'], Combining Results of Multiple Experiments,seg_103,which corresponds to asking for the probability that the observed p-values occur under h0.
781,1,"['degrees of freedom', 'random variables', 'variables', 'uniformly distributed', 'distribution', 'random variable', 'variable', 'random', 'test', 'null hypothesis', 'hypothesis']", Combining Results of Multiple Experiments,seg_103,"let pi be the p-value of the ith test, treated as a random variable. it can be shown that the pi are uniformly distributed on [0,1] under h0 and thus each p-value is equally likely if the null hypothesis is correct. however, the product of uniform random variables is itself not uniform, so we need to calculate the corresponding distribution. a simple trick comes to the rescue: the logarithm of a uniform random variable u, scaled by (−2), has a χ2-distribution with two degrees of freedom:"
782,0,[], Combining Results of Multiple Experiments,seg_103,"−2 log(u ) ∼ χ2(2),"
783,1,['degrees of freedom'], Combining Results of Multiple Experiments,seg_103,"and we know from sect. 1.4 that the sum of χ2-variables still has a χ2-distribution, with corresponding degrees of freedom:"
784,0,[], Combining Results of Multiple Experiments,seg_103,q = −2 ∑ log(pi ) ∼ χ2(2k). i=1
785,1,"['experiment', 'experiments', 'test', 'null hypothesis', 'realization', 'hypothesis']", Combining Results of Multiple Experiments,seg_103,"this allows us to combine p-values from different experiments, provided the same test was used in each experiment. we compute the realization q of q from the observed p-values and reject the null hypothesis if q > χ12−α(2k)."
786,0,[], Combining Results of Multiple Experiments,seg_103,"example 31 for the p-values 0.08, 0.06, and 0.07, we compute the value"
787,0,[], Combining Results of Multiple Experiments,seg_103,q = −2 × (log(0.08) + log(0.06) + log(0.07)) = 15.9968
788,1,"['combination', 'test statistic', 'statistic', 'experiments', 'test', 'null hypothesis', 'hypothesis']", Combining Results of Multiple Experiments,seg_103,"for the test statistic q. with k = 3, this statistic has a χ2(6)-distribution, and the overall p-value is thus p = p(q > q|h0) ≈ 0.0138. while each individual result does not give evidence against the null hypothesis, the combination of the three experiments shows significant evidence that the null hypothesis is false."
789,1,"['alternative hypothesis', 'sample', 'test statistic', 'random sample', 'distribution', 'statistic', 'probability', 'random', 'level', 'test', 'null hypothesis', 'hypothesis']", Summary,seg_105,"to test a property of a distribution, we formulate two hypotheses—the null hypothesis h0 and the alternative hypothesis h1—,such that the null hypothesis represents the “status quo”. we then compute the value for a test statistic t from a random sample; the distribution of t under the null hypothesis allows us to compute the p-value—the probability of a false rejection. the smaller the p-value, the less likely h0 is true and we reject it if the p-value is below a prescribed test level α. however, we can not prove the correctness of a null hypothesis and the p-value is not the probability that h0 is correct."
790,1,"['test statistic', 'statistic', 'test']", Summary,seg_105,"before choosing a test statistic, it is often worthwhile to check whether robust alternatives are available, such as the wilcoxon-test to replace a classic t-test."
791,1,"['sample size', 'sample', 'probabilities', 'sensitivity', 'test statistic', 'statistic', 'hypotheses', 'specificity', 'test', 'null hypothesis', 'hypothesis']", Summary,seg_105,"the sensitivity and specificity of a test describe the probabilities to correctly reject or not reject the null hypothesis. they depend on the test statistic, the two hypotheses, but also on the sample size."
792,1,"['false positive', 'rate', 'hypotheses', 'probability', 'false positives', 'tests', 'multiple testing']", Summary,seg_105,"when preforming several tests simultaneously, we need to recalibrate the p-values to correct for multiple testing, which we can do either by the bonferroni-method, which controls the probability to get even one false positive, or using the false discovery rate approach, which allows a prescribed fraction of false positives among all rejected hypotheses."
793,1,"['linear', 'hypothesis tests', 'covariates', 'tests', 'model', 'parameters', 'regression', 'linear regression', 'response', 'hypothesis', 'regression analysis', 'variable', 'response variable']",Chapter  Regression,seg_109,abstract regression analysis describes the influence of covariates on a response variable. two regression models are discussed: linear regression on one or several covariates and analysis-of-variance. methods for estimating the respective model parameters and hypothesis tests for eliminating non-significant covariates are presented.
794,1,"['linear', 'model', 'regression', 'linear regression', 'anova']",Chapter  Regression,seg_109,keywords linear regression · model reduction · anova
795,1,"['regression function', 'covariates', 'regression', 'expected value', 'function', 'response']", Introduction,seg_111,"regression analysis aims at studying the influence of one or more covariates x on a response y. we only discuss parametric regression, where we know the functional relation between covariates and response, and try to identify the correct parameter(s) of this function. the regression function is then defined as the expected value conditioned on the values of the covariates:"
796,0,['e'], Introduction,seg_111,"r(x; θ) = e (y |x = x) = ∫ y f (y|x; θ)dy,"
797,1,"['parameters', 'regression function', 'covariates', 'conditional', 'data', 'regression', 'samples', 'conditional probability density', 'probability', 'conditional probability', 'function', 'average', 'response']", Introduction,seg_111,"where f (y|x; θ) is the conditional probability density of y |x with parameters θ. once we know the parameters, we can predict the average value for the response for any given values of the covariates by the regression function. for estimating the parameters from given data, we assume that we have n samples"
798,0,[], Introduction,seg_111,"(y1, x1,1, x1,2, . . . , x1,m), . . . , (yn, xn,1, xn,2, . . . , xn,m),"
799,1,"['covariate', 'model', 'contrast', 'measurement', 'regression analysis', 'covariates', 'regression', 'distribution', 'variable', 'response', 'response variable', 'error']", Introduction,seg_111,"where yi is the value of the response variable in the ith measurement and xi, j is the value of the jth covariate for that measurement. regression analysis assumes that the values of the covariates xi, j are known exactly. in contrast, the observed values of the response yi are subject to error and an error model is used to described their distribution around the true value. the assumed relation of covariates and response is then"
800,0,[], Introduction,seg_111,"y = r(x; θ) + ε,"
801,1,"['random error', 'error', 'random']", Introduction,seg_111,where ε is a random error term.
802,1,"['covariate', 'linear', 'model', 'regression model', 'regression', 'linear regression', 'response', 'linear regression model']", Introduction,seg_111,"example 32 let us consider the problem of finding the relation between the dry weight and height of a plant. we may claim that the dry weight increases linearly with the height of the plant. using the height as a covariate x and the dry weight as the response y, we can use the following linear regression model to describe this relation:"
803,0,[], Introduction,seg_111,y = β0 + β1x + ε.
804,1,"['parameter', 'error', 'data']", Introduction,seg_111,"here, the parameter β0 is the dry weight of a plant with height zero, and β1 describes how much more dry weight we get per increase in height. we implicitly assume that the height can be measured without error, but the measured weight spreads around the true value by an error ε. the first task is then to measure the weight and height of n plants, yielding the data (y1, x1) to (yn, xn). from this data, we would then try to find the correct parameter values for β0 and β1. once these are established, we can predict the dry weight from the height of any new plant."
805,1,"['ordinal', 'variables', 'covariates', 'discrete', 'regression', 'measurements', 'continuous', 'categories', 'response']", Classes of Regression Problems,seg_113,"depending on the type of covariates and response, we can distinguish several classes of regression problems. the two main types are metric variables, which are any kind of (continuous) numbers, and categorial variables, which describe membership in distinct classes. examples for the first type are measurements of length, (discrete) counts, and waiting times, for the second type categories such as male/female. categories can sometimes additionally have an order such as high > middle > low, and variables are then called ordinal."
806,1,"['covariate', 'model', 'linear', 'estimation', 'nonlinear regression', 'nonlinear', 'covariates', 'regression', 'hypothesis testing', 'linear regression', 'response', 'hypothesis']", Classes of Regression Problems,seg_113,"if both response and covariates are metric, we are in the setting usually called regression, and we can further distinguish linear from nonlinear regression, depending on the claimed functional relation r(). linear regression is the typical first example of regression methods and will be covered in detail in sect. 4.3 for a single covariate. if several covariates are involved, powerful methods can be applied to reduce the model and identify those covariates which have a significant influence on the response. the required methods for estimation and hypothesis testing are covered in sect. 4.4."
807,1,"['estimators', 'hypothesis tests', 'covariates', 'tests', 'model', 'parameters', 'response', 'anova', 'hypothesis', 'treatment', 'variable', 'combinations', 'response variable']", Classes of Regression Problems,seg_113,"regression of a metric response on categorial covariates requires analysisof-variance (anova). the impact of treatment (no drug / drug a / drug b) on the recovery time of a patient is an example. anova can be seen as an extension of t-tests to more than two means; we discuss anova in sect. 4.5. after presenting the estimators for the anova model parameters, we also develop hypothesis tests to investigate the influence of combinations of groups on the response variable."
808,1,"['covariate', 'linear', 'regression analysis', 'regression', 'linear regression']", Linear Regression One Covariate,seg_115,we start by discussing linear regression with one covariate. this is the workhorse in regression analysis; it is widely applicable in practice and many other methods can be derived as variants and extensions.
809,1,"['covariate', 'regression function', 'regression', 'function']", Problem Statement,seg_117,linear regression with one covariate assumes a regression function of the form
810,0,[], Problem Statement,seg_117,"y = β0 + β1 x + ε,"
811,1,"['functions', 'linear', 'covariate', 'parameters', 'regression functions', 'intercept', 'regression', 'linear regression', 'response', 'slope']", Problem Statement,seg_117,"where y is the metric response to the metric covariate x. the parameters (in linear regression traditionally called β) of this family of regression functions are the intercept β0 and the slope β1 of the (x, y)-line."
812,1,"['covariate', 'bias', 'homoscedacity', 'independent', 'random variable', 'variable', 'response', 'mean', 'random', 'variance', 'error']", Problem Statement,seg_117,"it is the error term ε that prevents us from observing the correct response directly, and makes y a random variable. we assume the error to have zero mean, so it does not introduce a bias in the analysis. we further assume that it has constant (but unknown) variance σ 2, independent of the value x of the covariate; this is called homoscedacity:"
813,0,[], Problem Statement,seg_117,e(ε|x = x) = 0
814,0,[], Problem Statement,seg_117,var(ε|x = x) = σ 2.
815,1,"['distribution', 'normal', 'heteroscedacity', 'variance', 'error', 'normal distribution']", Problem Statement,seg_117,"if the variance of the error does depend on the value x, we call this heteroscedacity. for some of the analyses, we do not need to assume a particular distribution of ε, but for more sophisticated analyses, we often assume that the error has a normal distribution. section 4.3.3 is devoted to methods for checking these assumptions on the error structure."
816,1,"['parameters', 'method', 'distribution', 'parameter', 'error']", Parameter Estimation,seg_119,"without assuming any particular error distribution, we cannot apply the maximumlikelihood principle and therefore rely on a least-squares method for estimating the two parameters β0 and β1. given the parameter values, the value"
817,0,[], Parameter Estimation,seg_119,ŷi = r(xi ) = β0 + β1xi
818,1,"['covariate', 'estimation', 'predicted', 'parameter', 'response']", Parameter Estimation,seg_119,is the predicted response to the value xi of the covariate. we can apply least-squares estimation to find those parameter values β̂0 and β̂1 that minimize the squared difference ε̂i2 = (yi − ŷi )2 between the measured and the predicted value:
819,0,[], Parameter Estimation,seg_119,"(β̂0, β̂1) = argminβ0,β1 ∑ ε̂i2 = argminβ0,β1 ∑ (yi − (β0 + β1xi ))2 . i=1 i=1"
820,1,"['sample', 'estimates', 'residuals', 'error']", Parameter Estimation,seg_119,the differences ε̂i2 are called the residuals and are themselves estimates of the error ε in each sample point.
821,1,['estimates'], Parameter Estimation,seg_119,this least-squares problem always has a unique solution and we even find explicit formulas for the estimates:
822,0,[], Parameter Estimation,seg_119,∑in=1(xi − x̄)(yi − ȳ) β̂1 = ∑in=1(xi − x̄)2
823,0,[], Parameter Estimation,seg_119,"β̂0 = ȳ − β̂1 x̄,"
824,1,"['variance', 'error', 'estimate']", Parameter Estimation,seg_119,from which we can also estimate the variance of the error by
825,0,['n'], Parameter Estimation,seg_119,"n 1 σ̂ 2 = ∑ ε̂i2, n − 2 i=1"
826,1,"['degrees of freedom', 'estimated']", Parameter Estimation,seg_119,using n − 2 degrees of freedom (one less per estimated β̂i ).
827,1,"['predicted', 'residuals', 'normally distributed', 'function', 'linear', 'standard', 'standard deviation', 'error', 'covariate', 'parameters', 'variance', 'response', 'response value', 'deviation', 'measurement', 'estimated', 'method']", Parameter Estimation,seg_119,"example 33 a typical example is shown in fig. 4.1, where 10 responses were measured from a linear function y = β0 + β1x + ε with true parameters β0 = 2 and β1 = 4 and normally distributed error with variance σ 2 = 40. using the leastsquares method, the parameters are estimated as β̂0 = −0.085 and β̂1 = 4.377. the estimated standard deviation is σ̂ 2 = 118.926. the solid line gives the predicted responses ŷi for any value x of the covariate. the residuals ε̂i are then the vertical differences between this line and the actual response value yi for a measurement, indicated by vertical dashed lines."
828,1,"['parameters', 'estimates', 'errors', 'estimators', 'normally distributed', 'normality', 'homoscedacity', 'error']", Parameter Estimation,seg_119,"mle under normality. if in addition to homoscedacity and unbiasedness, the error is normally distributed, we can also apply the maximum-likelihood approach to find the parameters β0, β1. interestingly this yields the exact same estimators and thus maximum-likelihood and least-squares estimates coincide under normality of errors."
829,1,"['estimated', 'parameters', 'asymptotic', 'unbiased', 'confidence intervals', 'asymptotic normal distribution', 'intervals', 'distribution', 'normal', 'confidence', 'normal distribution']", Parameter Estimation,seg_119,"the mles for the two parameters are consistent and unbiased. as mles, they also have an asymptotic normal distribution, so confidence intervals for the estimated parameters are given by"
830,0,[], Parameter Estimation,seg_119,"β̂i ± zα/2ŝe(β̂i ),"
831,1,"['errors', 'standard', 'standard errors']", Parameter Estimation,seg_119,where the estimators’ standard errors ŝe(β̂i ) = √
832,0,[], Parameter Estimation,seg_119,v̂ar(β̂i ) are
833,0,['n'], Parameter Estimation,seg_119,n ŝe(β̂0) = σ̂ √√√
834,0,[], Parameter Estimation,seg_119,√
835,0,[], Parameter Estimation,seg_119,1 ∑
836,0,['n'], Parameter Estimation,seg_119,"xi2, σ̂x√n n"
837,0,[], Parameter Estimation,seg_119,"σ̂ ŝe(β̂1) = , σ̂x√n"
838,0,[], Parameter Estimation,seg_119,and σ̂x
839,1,"['covariate', 'estimators', 'covariance matrix', 'covariance', 'variance', 'dispersion']", Parameter Estimation,seg_119,"2 = n− 1 1 ∑in=1(xi − x̄)2 is the dispersion of values taken for the covariate. we can even give the full covariance matrix, which contains the variance of the two estimators in the diagonal, and the covariance of them in the off-diagonals:"
840,0,['n'], Parameter Estimation,seg_119,σ̂ 2 n β̂ i=1 xi −x̄ v̂ar ( β̂
841,0,[], Parameter Estimation,seg_119,0 1 ) = σ̂x 2n ( n1 ∑−x̄
842,0,[], Parameter Estimation,seg_119,1 )
843,0,[], Parameter Estimation,seg_119,.
844,1,"['covariate', 'model', 'estimated', 'parameters', 'regression model', 'data', 'regression', 'intervals', 'response']", Parameter Estimation,seg_119,"prediction intervals. once the parameters are estimated from the data, we can apply this “fitted” regression model to predict the value y∗ of the response for other values x∗ of the covariate by"
845,0,[], Parameter Estimation,seg_119,ŷ∗ = β̂0 + β̂1x∗.
846,1,"['estimated', 'parameters', 'interval', 'prediction', 'estimation', 'data', 'random variable', 'variable', 'random', 'confidence', 'confidence interval']", Parameter Estimation,seg_119,"although for given parameters this gives a particular prediction, this prediction still depends on the estimated values β̂i . since these in turn depend on the original data used for the estimation, the prediction ŷ∗ is still a random variable and we should compute a confidence interval for ŷ∗ to quantify the confidence in the prediction."
847,1,"['estimates', 'error', 'interval']", Parameter Estimation,seg_119,"this interval depends on the error ε, but also on the error in the estimates β̂i and is given by"
848,0,[], Parameter Estimation,seg_119,"ŷ∗ ± zα/2ζ̂ ,"
849,1,"['deviation', 'estimated', 'standard', 'standard deviation', 'estimator']", Parameter Estimation,seg_119,"where the standard deviation ζ̂ is not the estimated standard deviation of the estimator, but is given by"
850,0,['n'], Parameter Estimation,seg_119,ζ̂ 2 = σ̂ 2 (n
851,0,[], Parameter Estimation,seg_119,∑
852,0,[], Parameter Estimation,seg_119,∑
853,0,[], Parameter Estimation,seg_119,1(
854,0,[], Parameter Estimation,seg_119,x
855,0,[], Parameter Estimation,seg_119,(
856,0,[], Parameter Estimation,seg_119,xi
857,0,[], Parameter Estimation,seg_119,−
858,0,[], Parameter Estimation,seg_119,−
859,0,[], Parameter Estimation,seg_119,x∗
860,0,[], Parameter Estimation,seg_119,x̄
861,0,[], Parameter Estimation,seg_119,)
862,0,[], Parameter Estimation,seg_119,)
863,0,[], Parameter Estimation,seg_119,2 2 + 1) .
864,1,"['sample', 'covariate', 'parameters', 'interval', 'estimation', 'data', 'outcome', 'confidence', 'variance', 'extrapolation', 'confidence interval']", Parameter Estimation,seg_119,"this confidence interval depends on the variance of the data, but additionally gets wider, the further away the new covariate value x∗ is from the original values used in the estimation of the parameters. for example, if values of the covariate in the original sample are all between x1 = 1 and xn = 10, an extrapolation by predicting the outcome for x∗ = 100 gives a much wider confidence interval than an interpolation at a value x∗ = 5."
865,1,"['covariate', 'linear', 'independent', 'parameters', 'residuals', 'data', 'regression', 'errors', 'normally distributed', 'linear regression', 'standard', 'variance', 'homoscedacity']", Checking Assumptions,seg_121,"application of standard linear regression techniques relies on two assumptions: homoscedacity of the data, i.e., constant variance independent of the covariate, and, for many conclusions, normally distributed errors. to check these assumptions, we can perform an a posteriori analysis of the resulting residuals yi − ŷi after estimating the parameters."
866,1,"['covariate', 'model', 'residuals', 'plotting', 'homoscedacity']", Checking Assumptions,seg_121,checking homoscedacity. homoscedacity can be assessed by plotting the residuals ε̂i of the fitted model against the covariate values xi . we then expect to see no particular pattern and points should scatter uniformly around zero. if the variance increases with the value of the covariate—one of the most common causes of heteroscedacity—a tilted ‘v’ pattern appears as the residuals get bigger with increasing xi .
867,1,"['sample', 'model', 'linear', 'covariate', 'regression function', 'residuals', 'regression', 'linear regression', 'function', 'variance', 'error']", Checking Assumptions,seg_121,"for 50 sample points using the same linear regression function as before, the residuals are shown in fig. 4.2 (left). changing the error model such that the variance increase linearly with the covariate yields the tilted ‘v’ pattern in fig. 4.2 (right)."
868,1,"['plots', 'residuals', 'sample', 'bias', 'mean', 'error', 'model', 'parameters', 'estimation', 'distribution', 'heteroscedacity', 'variance', 'homoscedacity', 'estimated', 'normality', 'normal', 'normal distribution']", Checking Assumptions,seg_121,"checking normality. assessing whether the error distribution is normal is straightforward once the model parameters are estimated. if the error distribution is normal, the resulting residuals should follow a normal distribution with mean μ = 0 and the estimated variance σ̂ 2. we can use a normal q–q-plot for visual inspection. for the example above with n = 50 sample points, the normal q–q-plots of the residuals are shown in fig. 4.3 for both homoscedacity (left) and heteroscedacity (right). in both plots, the residuals scatter around a mean of zero, and do not introduce a bias in the estimation, as expected."
869,1,"['functions', 'linear', 'data', 'regression', 'linear regression', 'function']", Linear Regression Using R,seg_123,"r already provides all necessary functions for easily fitting linear models. for given data vectors x and y, a linear regression is fitted using the function lm(), which requires an r formula as input. r formulas are a very convenient way of representing complex regression problems. an example is"
870,0,[], Linear Regression Using R,seg_123,"y ∼ x+ z,"
871,1,"['model', 'parameters', 'states', 'covariates', 'response']", Linear Regression Using R,seg_123,"which states that we want a fit of the response y on the two covariates x and z. thus, r fits the model y = β0 + β1 x + β2 z by estimating the three parameters βi . in r formulas, the arithmetic operators +,−, ∗, / as well as exponents have special interpretations, and should not be confused with their usual meaning."
872,1,"['covariate', 'linear', 'estimation', 'regression', 'linear regression', 'response']", Linear Regression Using R,seg_123,"parameter estimation. let us consider a linear regression problem with one covariate. the covariate vectorx contains the values of the covariate and the response vector y the corresponding values of the response. then, the r command"
873,0,[], Linear Regression Using R,seg_123,model <- lm(y ˜ x)
874,1,"['linear', 'model', 'linear model', 'parameters', 'information']", Linear Regression Using R,seg_123,estimates the various parameters and yields a data-structure model containing all the information about the resulting linear model.
875,1,"['plot', 'model', 'plots', 'residuals', 'normal']", Linear Regression Using R,seg_123,"inspection plots. the model can then be plotted using the commandplot(model) to successively generate four plots: the residuals, the normal q–q-plot, the residuals on another scale, and an influence plot (see sect. 4.4.4 for a discussion of different influence measures)."
876,1,"['plot', 'model', 'estimated', 'data', 'regression', 'plotting', 'estimated regression line', 'regression line']", Linear Regression Using R,seg_123,"model plot. we can also easily plot the model together with the data by first plotting the data by plot(y ˜ x), and then adding the estimated regression line with abline(model). this was done to generate fig. 4.1."
877,1,"['model', 'covariates', 'function']", Linear Regression Using R,seg_123,"prediction. predicting new values from a model is done using the predict() function, which expects a model data-structure and a data-frame with the new values of the covariates. for example"
878,0,[], Linear Regression Using R,seg_123,"predict(object=model, newdata=new.x, interval=’prediction’)"
879,0,[], Linear Regression Using R,seg_123,for two new values x = 2.4 and x = 90 yields the result
880,1,"['predicted', 'intervals']", Linear Regression Using R,seg_123,with the predicted values in the first column and the lower and upper value of the 0.95-prediction intervals in the second and third column.
881,1,"['linear', 'model', 'parameters', 'regression function', 'regression', 'linear regression', 'function']", On the Linear in Linear Regression,seg_125,"an important point to understand when using linear regression models is that we only need the regression function to be linear in the parameters. for example, the model"
882,0,[], On the Linear in Linear Regression,seg_125,y = β0 + β1 x2
883,1,"['covariate', 'linear', 'model', 'regression model', 'regression', 'linear regression', 'transformation', 'linear regression model']", On the Linear in Linear Regression,seg_125,"is a perfectly valid linear regression model, because we may simply replace the covariate x with a new covariate z = x2 to get the familiar equation. other models can be made linear by transformation. for example, the model"
884,0,[], On the Linear in Linear Regression,seg_125,y = β0 exp(β1 x)
885,1,"['linear', 'model', 'transformed', 'residuals', 'statistics', 'regression', 'normally distributed', 'normal', 'linear regression', 'error']", On the Linear in Linear Regression,seg_125,"is easily brought into a form suitable for linear regression by taking the logarithm. however, there is a caveat: if the original model has additive, normally distributed error, this error structure is also transformed by the logarithm and is no longer normal. this can cause major problems, as many of the statistics rely on normally distributed residuals."
886,1,"['covariate', 'linear', 'covariates', 'regression', 'linear regression', 'cloud of points', 'response', 'error']", Linear Regression Multiple Covariates,seg_127,"for more than one covariate, linear regression aims at fitting a hyperplane through a cloud of points. again, all values of covariates are assumed to be fixed and nonrandom, and only the value of the response is subject to error."
887,1,"['model', 'variables', 'function']", Linear Regression Multiple Covariates,seg_127,"example 34 let us reconsider our small introductory example of studying dry weight as a function of height in plants. instead of using only the height, we may also want to consider more variables that might have an influence on the dry weight. for example, we could study the influence of the amount of fertilizer on the weight. the model would then informally read"
888,1,['error'], Linear Regression Multiple Covariates,seg_127,dry weight = β0 + β1 height + β2 fertilizer amount + error.
889,1,"['model', 'regression analysis', 'covariates', 'data', 'regression']", Linear Regression Multiple Covariates,seg_127,"this allows us to ask more questions on the model, for example if the fertilizer has any influence at all. if not, we could delete it from the model, which would result in a new and more simple model that is still able to sufficiently describe the dry weight. regression analysis also allows us to explicitly take the influence of the covariates on each other into account. here, we might consider that more fertilizer also yields higher plants, which we need to take into consideration when trying to find the most simple model that sufficiently explains the data."
890,1,"['sample', 'model', 'covariate', 'covariates', 'data', 'regression', 'variable', 'response', 'response variable']", Problem Statement,seg_129,"we are studying a regression problem with m covariates x1, . . . , xm and one response variable y. the data are again n tuples of sample points, each with one value for each covariate and one value for the noisy measured response. the regression model is then of the form"
891,0,[], Problem Statement,seg_129,y = β0 + β1 x1 + β2 x2 + · · · + βm xm + ε
892,1,"['estimation', 'interaction', 'covariates', 'interaction terms']", Problem Statement,seg_129,and might additionally contain interaction terms to capture the dependency of covariates on each other. the resulting estimation problem of n equations in m covariates can then be written as
893,0,[], Problem Statement,seg_129,. . . . . .
894,0,[], Problem Statement,seg_129,⎛⎜
895,0,[], Problem Statement,seg_129,y ..
896,0,[], Problem Statement,seg_129,1 ⎞⎟ = ⎛⎜
897,0,[], Problem Statement,seg_129,1 ..
898,0,[], Problem Statement,seg_129,"x1.. ,1 · · · x1.. ,m ⎞⎟ · ⎛⎜"
899,0,[], Problem Statement,seg_129,β ..
900,0,[], Problem Statement,seg_129,0 ⎞⎟ + ⎛⎜
901,0,[], Problem Statement,seg_129,ε ..
902,0,[], Problem Statement,seg_129,"1 ⎞⎟ ,"
903,0,[], Problem Statement,seg_129,"⎝ yn ⎠ ⎝ 1 xn,1 · · · xn,m ⎠ ⎝βm ⎠ ⎝ εn ⎠"
904,1,"['design matrix', 'design', 'statistics', 'errors', 'normally distributed', 'mean', 'variance']", Problem Statement,seg_129,"using matrix-vector notation. the matrix x containing the xi, j is called the design matrix, where the first column is the “covariate” for the constant term β0. we again assume that each εi has mean zero and constant variance and that the errors are independent. for most of the statistics, we additionally require the errors to be normally distributed."
905,1,"['sum of squared', 'parameters', 'estimate']", Parameter Estimation,seg_131,"we can again estimate the parameters as β̂ = (β̂0, . . . , β̂m) by minimizing the sum of squared differences"
906,0,[], Parameter Estimation,seg_131,"β̂ = argminβ ∑ (yi − (β0 + β1xi,1 + · · · + βm xi,m))2. i=1"
907,1,['estimates'], Parameter Estimation,seg_131,"again, the estimates can be given in explicit form by"
908,0,[], Parameter Estimation,seg_131,"β̂ = (xt x)−1 xt y,"
909,1,"['covariate', 'design', 'design matrix', 'response']", Parameter Estimation,seg_131,where x is the above design matrix containing the covariate values and a 1 entry in the first column and y is the vector containing the n values of the response.
910,1,"['covariate', 'model', 'parameters', 'statistics', 'covariates', 'data', 'associated', 'mean', 'parameter', 'variation']", Hypothesis Testing and Model Reduction,seg_133,"a natural question to ask is whether we really need all the covariates to explain the measured responses, or if a smaller subset of covariates will already explain the data. this is the question for a minimal adequate model, which is the model with the smallest number of covariates that still explains the data reasonably well. in general, the data can always be better explained with more covariates, simply because we get more parameters to work with. however, reducing the model by eliminating one covariate and its associated parameter might lead to a new model that is almost as good as the larger model. for doing this properly, we need a way of quantifying what we mean by “almost as good”. we start with a central result on decomposing the overall variation that will then naturally lead to the required statistics."
911,1,"['regression', 'response', 'variation']", Hypothesis Testing and Model Reduction,seg_133,"decomposing the variation. without considering any regression, the total variation in the response is"
912,0,[], Hypothesis Testing and Model Reduction,seg_133,sstot = ∑ (yi − ȳ)2. i=1
913,1,"['factor', 'response', 'mean', 'dispersion']", Hypothesis Testing and Model Reduction,seg_133,it is proportional to var(y) with a factor of (n − 1) and is a measure of the overall dispersion of the response around its mean.
914,1,"['model', 'regression model', 'regression', 'variation']", Hypothesis Testing and Model Reduction,seg_133,"after fitting a regression model, this total variation can be decomposed into two components: the explained variation or regression sum-of-squares"
915,0,[], Hypothesis Testing and Model Reduction,seg_133,"ssreg = ∑ (ŷi − ȳ)2 , i=1"
916,1,"['regression', 'response', 'mean', 'variation', 'regression line', 'error']", Hypothesis Testing and Model Reduction,seg_133,"which measures how much of the total variation can be explained by the fact that the measured response is actually spread around the regression line and not simply around its mean, and the error sum-of-squares"
917,0,[], Hypothesis Testing and Model Reduction,seg_133,"sserr = ∑ (yi − ŷi )2 , i=1"
918,1,"['residual', 'estimation', 'predicted', 'dispersion', 'variation', 'response']", Hypothesis Testing and Model Reduction,seg_133,"which gives the unexplained variation still left, i.e., the dispersion of the measured response values around their predicted values. note that sserr is exactly the residual sum-of-squares ∑i ε̂i2 that is minimized for estimation."
919,1,"['model', 'regression model', 'regression', 'variation']", Hypothesis Testing and Model Reduction,seg_133,"using the regression model, the total variation can always be written as the sum of the explained and the unexplained variation:"
920,0,[], Hypothesis Testing and Model Reduction,seg_133,"sstot = ssreg + sserr,"
921,0,[], Hypothesis Testing and Model Reduction,seg_133,which is also demonstrated in fig. 4.4.
922,1,"['degrees of freedom', 'variance', 'variation']", Hypothesis Testing and Model Reduction,seg_133,"the variance can be recovered by dividing a variation by its corresponding degrees of freedom, which are sstot : n − 1, ssreg : m, and sserr : n − m − 1."
923,1,"['parameters', 'regression function', 'predicted', 'case', 'regression', 'set', 'mean', 'variations', 'variation', 'function', 'response']", Hypothesis Testing and Model Reduction,seg_133,"varying the parameters βi of the regression function does not change the measured response values, so the total variation sstot remains the same. however, we will alter the predicted response values and thus the contributions of the explained and unexplained variations to the total variation. a “good” set of parameters will have a large ssreg, so a lot of the total variation is explained by the fact that the response spreads around the regression function rather than its mean. the remaining unexplained variation caused by this spread around the regression function is minimized in this case."
924,1,"['deviation', 'model', 'predicted', 'case', 'covariates', 'response', 'variation', 'coefficient', 'coefficient of determination']", Hypothesis Testing and Model Reduction,seg_133,"we can exploit this decomposition of the total variation in two ways: first, it gives the coefficient of determination r2 = ssreg/sstot, which is the proportion of explained to total variation. consider using the model y = β0 + ε with no covariates, leading to the fit β̂0 = ȳ with constant predicted response. the coefficient of determination is zero in this case and the model cannot explain any of the deviation"
925,1,"['covariate', 'model', 'linear', 'measurement', 'coefficient', 'data', 'correlation', 'regression', 'linear regression', 'mean', 'correlation coefficient', 'measurement error', 'coefficient of determination', 'error']", Hypothesis Testing and Model Reduction,seg_133,"of measured yi from their mean. for a perfect linear fit with no measurement error, the coefficient is one, as the data is completely explained by the model. for a linear regression using a single covariate, the coefficient of determination is identical to the pearson’s correlation coefficient (see sect. 1.6.4), given by"
926,0,[], Hypothesis Testing and Model Reduction,seg_133,"cov(x, y ) r2 = . σxσy"
927,1,"['degrees of freedom', 'covariates', 'errors', 'normally distributed', 'coefficient', 'coefficient of determination']", Hypothesis Testing and Model Reduction,seg_133,"in addition to the coefficient of determination, we can also use the decomposition to compare models with a different number of covariates. for normally distributed errors, the various sum-of-squares all have χ2-distributions with appropriate degrees of freedom. thus, the quotient"
928,0,[], Hypothesis Testing and Model Reduction,seg_133,ssreg/m f = sserr/(n − m − 1)
929,1,"['regression', 'data', 'variation']", Hypothesis Testing and Model Reduction,seg_133,"of the explained versus the unexplained variation has an f(m, n−m−1)-distribution (see sect. 1.4). the larger this value becomes, the better the regression explains the data."
930,1,"['covariate', 'model', 'states', 'covariates', 'data', 'regression', 'statistical', 'mean', 'test', 'full model', 'hypothesis']", Hypothesis Testing and Model Reduction,seg_133,"do we need any covariate? now that we have worked out a way to quantify how good a regression explains the data, we can start asking statistical questions on the relevance of the various covariates for this explanation. the boldest question is to ask whether the regression actually explains the data at all. for this, we define the null model y = β0 + ε which in essence states that the data can be explained by the spread around its mean alone. we then compare this null model to the full model y = β0 + ∑ j β j x j + ε including all covariates. this comparison requires to test the hypothesis"
931,0,[], Hypothesis Testing and Model Reduction,seg_133,h0 : β1 = · · · = βm = 0
932,1,"['model', 'estimated', 'covariates', 'data', 'response']", Hypothesis Testing and Model Reduction,seg_133,"that it suffices to only consider β0 (i.e., the null model) estimated as β̂0 = ȳ to describe the data and the covariates thus have no relation to the response. with"
933,0,[], Hypothesis Testing and Model Reduction,seg_133,ssreg/m f = sserr/(n − m − 1)
934,1,"['quantile', 'test statistic', 'statistic', 'test', 'null hypothesis', 'hypothesis']", Hypothesis Testing and Model Reduction,seg_133,"as our test statistic, we reject this null hypothesis if f exceeds the corresponding quantile of the f-distribution: f > f1−α(m, n − m − 1)."
935,1,"['linear', 'model', 'hypothesis test', 'covariates', 'data', 'test', 'response', 'hypothesis']", Hypothesis Testing and Model Reduction,seg_133,"this hypothesis test can serve as a sanity check: if we do not find evidence to reject this hypothesis, the proposed model is not able to explain the data and none of the covariates has a significant (linear) relation to the response."
936,1,"['model', 'covariates', 'data', 'test', 'hypothesis']", Hypothesis Testing and Model Reduction,seg_133,"testing subsets of covariates. the same ideas allow us to check if a reduced model, using only a subset of the covariates, already provides a sufficient fit to the data. more specifically, we want to test the hypothesis"
937,0,[], Hypothesis Testing and Model Reduction,seg_133,h0 : βk+1 = · · · = βm = 0
938,1,"['data', 'covariates', 'method']", Hypothesis Testing and Model Reduction,seg_133,"that only the first k covariates are needed and the remaining m − k covariates do not provide any more explanation of the observed data. we only consider the first covariates simply to avoid clumsy notation; clearly, the exact same method works for any subset of covariates by simply re-ordering them."
939,1,"['model', 'variables', 'case', 'covariates', 'variation', 'full model']", Hypothesis Testing and Model Reduction,seg_133,"let us denote by ssreg(k) the explained variation of the reduced model, with only the first k covariates present, and similar notation for the remaining variables. the special case ssreg(m) ≡ ssreg then denotes the previously explained variation of the full model."
940,1,"['test statistic', 'statistic', 'test']", Hypothesis Testing and Model Reduction,seg_133,the test statistic
941,0,[], Hypothesis Testing and Model Reduction,seg_133,(ssreg(m) − ssreg(k)) /(m − k) fk = sserr/(n − m − 1)
942,1,"['covariate', 'model', 'covariates', 'data', 'distribution', 'level', 'null hypothesis', 'hypothesis']", Hypothesis Testing and Model Reduction,seg_133,"has a f(m − k, n − m − 1) distribution and we can reject the null hypothesis at the level α if fk > f1−α(m − k, n − m − 1). if we can reject h0, we conclude that the m − k covariates not contained in the reduced model do not significantly contribute to the explanation of the data. importantly, the term “significantly” is well-defined in this context. by setting k = 0, we re-derive the previous hypothesis that we do not need any covariate."
943,1,"['covariate', 'model', 'covariates', 'full model']", Hypothesis Testing and Model Reduction,seg_133,"model reduction. by exhaustively testing all possible reduced models, we can compute a minimal adequate model; it has the smallest number of covariates such that adding any of the other covariates does not significantly increase the model’s fit. for m covariates, there are 2m different models from subsets of the covariates. for larger models, one therefore retreats to iterative procedures for finding a minimal adequate model. the two simplest procedures either start with the full model and iteratively discard one covariate at a time, or start with the empty model and iteratively add one covariate. the order in which this is done may however impact the result, especially if the covariates influence each other."
944,1,"['plots', 'correlation', 'full model', 'linear', 'rate', 'experiment', 'results', 'covariates', 'data', 'parameter', 'linear regression model', 'covariate', 'model', 'regression model', 'linear model', 'factors', 'estimation', 'regression', 'linear regression', 'interaction terms', 'response', 'concentration', 'plot', 'independent', 'scatter plots', 'interaction', 'combinations']", Hypothesis Testing and Model Reduction,seg_133,"example 35 consider the following experiment: we want to investigate the influence of the concentration x1, x2, x3 of three different chemicals on the production rate y of a particular other chemical. we use a regression model that takes y as the response to various combinations of the three concentrations, each taken as a covariate. the linear regression model assumes that the response is the results of adding the three covariates with different scaling factors. we aim at finding the minimal adequate linear model to describe the response. we first try to get a visual impression of the data by investigating all pairwise scatter plots in fig. 4.5. in this plot, we observe a number of relations: first, the three covariates do not seem to interact with each other, i.e., they appear to be independent. second, the most pronounced influence on y seems to be from x3, and it appears to be slightly curved. moreover, there appears to be a correlation between response and x1, and only a weak response to x2. we therefore start with a full model that contains the three individual covariates together with their squares (to capture the potential curvature), and no interaction terms. the following r command will compute the parameter estimation for this model:"
945,0,[], Hypothesis Testing and Model Reduction,seg_133,"m <- lm(y ˜ x1+ x2+ x3+ i(x1ˆ2) + i(x2ˆ2) + i(x3ˆ2),data = d)"
946,1,"['function', 'model']", Hypothesis Testing and Model Reduction,seg_133,"the function i() tells r to interpret its content as an arithmetic expression, rather than an r formula. this command will fit the model"
947,0,[], Hypothesis Testing and Model Reduction,seg_133,y = β0 + β1 x1 + β2 x2 + β3 x3 + β4 x12 + β5 x22 + β6 x32.
948,1,['estimates'], Hypothesis Testing and Model Reduction,seg_133,the resulting estimates are shown using summary(m).
949,1,"['states', 'residuals', 'statistics', 'statistic', 'null hypothesis', 'full model', 'parameter', 'standard', 'standard deviation', 'covariate', 'model', 'test statistic', 'distribution', 'test', 'hypothesis', 'deviation', 'estimated']", Hypothesis Testing and Model Reduction,seg_133,"in this summary, r states the fitted model and a summary of the distribution of the residuals, and then returns the estimated value and the standard deviation for each parameter, as well as a test statistic and corresponding p-value on the significance of this parameter. these statistics are t-distributed, and test the null hypothesis h0 : βi = 0 that the corresponding covariate is not needed; recall from sect. 1.4 that f(1, m) = t2(m). in this full model, x3 is not significant and has a high p-value, making it a good candidate for model reduction, but its square is highly significant. overall, a very good fit was found, indicated by an r2-value of 0.9844."
950,1,"['function', 'model']", Hypothesis Testing and Model Reduction,seg_133,"from this first result, we start reducing the model by deleting x3 (but not its square) using the update() function, which takes the fitted model and an update to the original formula to compute a new model. the command"
951,0,[], Hypothesis Testing and Model Reduction,seg_133,"m2 <- update(m, ˜. − x3)"
952,1,['model'], Hypothesis Testing and Model Reduction,seg_133,"fits this new model, where the dot denotes the old formula, from which parts are discarded, resulting in"
953,1,"['model', 'anova', 'data', 'function', 'test', 'hypothesis']", Hypothesis Testing and Model Reduction,seg_133,"using the anova() function, we can compare the first and reduced model. this function will compute exactly the f-test described above and test the hypothesis h0 : β3 = 0 that x3 is not significant and can be discarded without significantly changing the ability of the model to explain the data. the call"
954,0,[], Hypothesis Testing and Model Reduction,seg_133,"anova(m, m2) yields the following result:"
955,1,"['variance', 'table']", Hypothesis Testing and Model Reduction,seg_133,analysis of variance table
956,1,"['covariate', 'model', 'covariates', 'data', 'full model']", Hypothesis Testing and Model Reduction,seg_133,"unsurprisingly, the covariate x3 can be discarded, with the same high p-value as before, and the reduced model explains the data as good as the full model. the next candidates for reduction are x2 and x22, so we delete these two covariates using m3 <- update(m2, ˜ . − x2− i(x2 ˆ2)), which yields"
957,1,"['covariate', 'model', 'case', 'intercept', 'covariates', 'response']", Hypothesis Testing and Model Reduction,seg_133,"continuing in the same fashion until all remaining covariates become significant, we find the minimal adequate model, which in this case claims that the resulting response can be explained by x1 and x32 plus the intercept. the covariate x2 was found to have no influence on the response. the minimal adequate model is therefore"
958,0,[], Hypothesis Testing and Model Reduction,seg_133,"y = β0 + β1 x1 + β6 x32 + ε,"
959,1,"['estimated', 'parameters', 'residuals', 'quantiles', 'errors', 'normal', 'tails', 'homoscedacity']", Hypothesis Testing and Model Reduction,seg_133,"with estimated parameters β̂0 = 1.3, β̂1 = 1.02, and β̂6 = 0.99, which are very close to their true values β0 = β1 =β6 = 1. to check the assumptions of homoscedacity and normal errors, we again give the residuals and the normal q–q-plot in fig. 4.6; both look fairly good, there is no sign of any structure in the residuals and the normal quantiles fit the residuals’ quantiles nicely, even in the tails."
960,1,['cases'], Hypothesis Testing and Model Reduction,seg_133,"practical procedure for finding minimal models. the procedure taken in the above example already uses the main ideas for more general cases. typically, we start with"
961,1,"['covariate', 'model', 'parameters', 'scatter plots', 'anova', 'plots', 'covariates', 'data', 'response', 'random', 'function', 'full model']", Hypothesis Testing and Model Reduction,seg_133,"some plots like the demonstrated pairwise scatter plots to see which covariates have an influence, discover structure in the data, and find “curvature” in the relationships. we continue with a first full model containing all relevant covariates that showed other than fully random influence on the response. we also additionally include all covariates showing curvature by taking their square, square-root, logarithm, or similar. looking at the summary, we iteratively delete the covariate with highest p-value until all parameters become significant. using the anova() function, we check whether the reduced model is significantly worse than the non-reduced model. the last steps are repeated until we find the minimal adequate model."
962,1,"['model', 'variables', 'interaction', 'successful', 'case', 'covariates', 'dependent', 'interaction terms']", Hypothesis Testing and Model Reduction,seg_133,"it is important to note that this procedure, while quite successful in practice, does not necessarily finds the best minimal adequate model, as the p-values of all covariates are changed when one of it is discarded, and the final model often depends on the order in which variables are discarded. this becomes more pronounced if the covariates are dependent among each other, in which case the initial model should contain appropriate interaction terms."
963,1,"['outliers', 'deviation', 'linear', 'model', 'regression model', 'data', 'regression', 'linear regression', 'standard', 'standard deviation']", Outliers,seg_135,"least-squares based linear regression methods are not very robust towards outliers, which are points that are far from where they are expected to be. to measure the extend to which a particular data point influences the overall regression, we can remove each point successively, then fit the regression model without this point, and compute the overall standard deviation of this model. points that change the overall standard deviation to a large extend then deserve special attention. another measure to quantify the influence of a particular point on the overall regression is the cook’s distance, computed for the jth point by"
964,0,[], Outliers,seg_135,"1 d j = ∑in=1 (ŷi − ŷi\ j )2 , m σ̂ 2"
965,1,"['predicted', 'plots', 'function', 'sample', 'linear', 'covariates', 'sets', 'error', 'model', 'linear model', 'plot']", Outliers,seg_135,"where again n is the number of sample points, m is the number of covariates, and σ̂ 2 is the mean-squared error of the model. the two sets of predicted values are ŷi for the model with all sample points and ŷi\ j for the model without the jth sample point. this distance is given in one of the plots when using the plot() function on a fitted linear model and can also be computed using the function cooks.distance()."
966,1,"['model', 'estimation', 'parameter', 'influence points', 'leverage points', 'leverage']", Outliers,seg_135,points that have a considerable influence on the parameter estimation are often called influence points or leverage points. they usually require special attention and one should always check how much the parameter values of the model change when ignoring such point.
967,1,"['deviation', 'linear', 'parameters', 'regression line', 'case', 'data', 'regression', 'outlier', 'intercept', 'linear regression', 'standard', 'standard deviation', 'cook’s distances', 'fitted line', 'slope']", Outliers,seg_135,"example 36 consider the situation in fig. 4.7, where a linear regression line is fitted to 16 data points, one of which is a clear outlier. the fitted line (solid) is “pulled” towards this outlier and both the intercept and the slope are considerably disturbed. in this case, the outlier is easily detected even by eye. removing it from the data yields the dashed regression line, which shows an excellent fit. this is also reflected in the standard deviation, which is around 15.7 for all points but the outlier, and drops to 1.22 if the outlier is removed before estimating the parameters. the cook’s distances give a similar picture. both measures are given for the 16 data points in fig. 4.8."
968,1,"['model', 'measurement', 'leverage point', 'outlier', 'cases', 'leverage points', 'homoscedacity', 'leverage']", Outliers,seg_135,"while quite intuitive, the term outlier already implies that the proposed underlying model is correct in the sense that all its assumptions, such as the linearity and homoscedacity, hold. if these assumptions are indeed correct, then leverage points can often be interpreted as incorrect measurement values. when values are recorded manually, a simple typo or a misplaced decimal point may already explain an unusual leverage point. in other cases, the measurement device might be incorrectly calibrated or failed to record a particular value, setting it to zero instead."
969,1,"['outliers', 'linear', 'model', 'measurement', 'covariate', 'regression model', 'nonlinear', 'data', 'regression', 'response', 'linear regression', 'leverage points', 'cook’s distances', 'leverage', 'linear regression model']", Outliers,seg_135,"on the other hand, the underlying model may already be incorrect. for example, we may be using a linear regression model on data generated by a nonlinear relation between covariate and response. then, leverage points with, e.g., large cook’s distances, might simply be caused by wrong model assumptions and are not outliers in the sense of an incorrect measurement."
970,1,"['covariate', 'model', 'data', 'function', 'response']", Outliers,seg_135,"example 37 let us consider the following situation: we are given response data to a covariate x. the true response is a cubic function of the covariate, and the correct model is"
971,0,[], Outliers,seg_135,y = β0 + β1 x3.
972,1,"['model', 'regression model', 'parameters', 'estimate', 'regression']", Outliers,seg_135,we now instead try to estimate the parameters of the regression model
973,0,[], Outliers,seg_135,y = β0 + β1 x
974,1,"['model', 'estimates', 'slope', 'data', 'parameter']", Outliers,seg_135,"using the data. this model neglects all the curvature, and we fit a straight line to the data. the resulting parameter estimates seem to look good; the slope parameter is significant and a good overall p-value for the model is achieved."
975,1,"['high leverage', 'leverage points', 'leverage']", Outliers,seg_135,"the overall fit is given by the solid line in fig. 4.9 (left), with an analysis of its leverage points in fig. 4.9 (right). however, the points with high leverage are"
976,1,"['covariate', 'model', 'dataset', 'residuals', 'measurements']", Outliers,seg_135,"clearly not “outliers” in the sense of incorrect measurements, but are artifacts from our attempt to fit a straight line to a curved dataset. a more detailed inspection of the residuals and the fit is indicated and higher powers of the covariate such as x2 and x3 need to be added to the model. fitting the correct model y ˜ i(xˆ3) yields the dashed line in fig. 4.9 (left), which shows an excellent fit."
977,1,"['outliers', 'statistics', 'outlier', 'function', 'high leverage', 'linear', 'data', 'model', 'linear model', 'parameters', 'estimation', 'regression', 'regression line', 'leverage', 'estimated', 'order statistics', 'breakdown points']", Robust Regression,seg_137,"least-squares estimation of regression parameters can be very sensitive to the presence of even a small number of outliers in the data. therefore, the question for more robust methods naturally arises and we will very briefly present two alternatives to least-squares estimation that are less sensitive to outliers and have high breakdown points: the rlm approach tries to robustly fit a linear model using so-called m-estimators, while the lqs approach relies on resistant regression by picking only “good” points for the estimation. we can not present the details of these two approaches in this book, but both are conveniently implemented in the mass package of r and can be called by rlm() and lqs(); these implementations can again be used using formulas, just like the lm() function for least-squares regression: rlm(y ˜ x) and lqs(y ˜ x) will do all the heavy lifting for us. they both encompass several different methods each, and work mainly by using order statistics for estimation and weighting schemes to detect and down-weight points with high leverage. for the two above examples of a true linear function and a cubic function, the fitted regression lines are shown in fig. 4.10. for the linear data, the outlier clearly does not influence the regression line anymore and a very good fit is achieved with both methods, basically by ignoring the outlier or giving it a very low weight in the estimation. the estimated models are almost identical for both estimations. using an incorrect model like the straight line for the cubic data, both methods will fail to"
978,1,"['outliers', 'estimated', 'estimates', 'data', 'cases']", Robust Regression,seg_137,"give a good explanation of the data, as shown in fig. 4.10 (right). the two estimated models are very different, however, due to the different way the methods ignore and down-weight what seem to be outliers. for comparison, the non-robust least-squares estimates are also given in both cases."
979,1,"['covariate', 'linear', 'case', 'covariates', 'regression', 'linear regression', 'response', 'anova']", AnalysisofVariance,seg_139,"using ideas very similar to linear regression of a metric response on metric covariates, we can also analyze the influence of categorial covariates on a metric response by anova. there is a very rich theory on anova, but we will only be concerned with the basic ideas and methods. in particular, we only discuss the case of one covariate."
980,1,"['covariate', 'levels', 'factor', 'anova', 'level', 'categories', 'response']", Problem Statement,seg_141,"we consider one categorial covariate x and a metric response y. in the context of anova, the covariate x is called a factor and its different possible categories are called factor levels. we are interested in the question whether the different factor levels have an influence on the response or not, and whether these influences are substantially different for the different factor levels. each factor level is claimed to give a particular response. again, the response is subject to noise, so the same factor level may lead to different response values, spread around a certain value. let us look into an example to make these considerations more concrete."
981,1,"['control', 'test', 'concentration']", Problem Statement,seg_141,"example 38 we want to test the effect of different growth media (that is, mixtures of different nutrients) on the growth of cells. there are three different media: a, b, and a control medium, and a, b are both tested in a low and a high concentration. thus,"
982,1,"['plot', 'covariate', 'levels', 'quantitative', 'factor', 'boxplot', 'response', 'level', 'experiments', 'concentration', 'anova']", Problem Statement,seg_141,"the covariate x is a factor with 5 levels, one for each possible medium. for each factor level (each medium), several experiments are conducted and the cell growth is recorded, which yields a metric response to the categorial covariate. a convenient way to visualize the influence of the factor levels on the response is the boxplot, which can easily be generated using boxplot(growth ˜ medium,data = d) and is shown in fig. 4.11. the first impression of this plot is that both a and b seem to have a considerable effect on the growth, but that there is no difference between them. the concentration of each medium also seems to have little to no effect. the main part of our discussion of anova will revolve around making these impressions quantitative."
983,1,"['model', 'levels', 'factor', 'observations', 'level', 'anova']", Problem Statement,seg_141,"formally, let us consider a factor x with k factor levels and assume that we have ni observations for the ith level. then, the anova model is"
984,0,[], Problem Statement,seg_141,"yi j = μi + εi j , i = 1 . . . k, j = 1 . . . ni ,"
985,1,"['measurement', 'levels', 'factor', 'distribution', 'normal', 'mean', 'associated', 'level', 'measurement error', 'variance', 'response', 'error', 'normal distribution']", Problem Statement,seg_141,"where yi j is the jth measured response for factor level i, μi is the true mean value for this level and εi j is the associated measurement error, which is again assumed to have mean zero and constant but unknown variance for all levels. often, we again assume the error to have a normal distribution as well."
986,1,"['level', 'factor', 'model', 'mean']", Problem Statement,seg_141,"we can derive an equivalent model by replacing the factor level means μi by an overall mean μ0 and the difference αi = μi − μ0 of each level mean to the overall mean, yielding"
987,0,[], Problem Statement,seg_141,yi j = μ0 + αi + εi j .
988,1,"['model', 'outcomes', 'levels', 'measurement', 'factor', 'mean', 'level', 'outcome', 'response', 'error', 'anova']", Problem Statement,seg_141,"in essence, the anova model claims that the undisturbed response at factor level i would take the value μi . due to the error perturbing the measurement, measured values yi j spread around the value μi . the anova is then performed to answer the question whether or not the factor level has an influence on the outcome and if so, which factor levels yield different outcomes. this in mainly a question whether the mean values of k levels differ significantly or not and in this sense, anova can be seen as an extension of t-test procedures to more than two groups."
989,1,"['levels', 'factor', 'estimators', 'sample', 'linear', 'estimate', 'data', 'model', 'parameters', 'regression', 'linear regression', 'level', 'variance', 'anova', 'estimated']", Parameter Estimation,seg_143,"in close resemblance to the linear regression, our first task is to estimate the various parameters of the anova model. let n = ∑i ni be the total number of sample points from all levels. the estimators for the overall and the individual factor level means are then the arithmetic means taken over the corresponding sample points. the differences in means are estimated as the difference of the estimators, and we can also estimate the variance in the data in the straightforward way."
990,1,['estimators'], Parameter Estimation,seg_143,we therefore derive the estimators
991,0,['n'], Parameter Estimation,seg_143,"k ni 1 μ̂0 = ȳ = ∑∑ yi j , n i=1 j=1"
992,0,[], Parameter Estimation,seg_143,"ni 1 μ̂i = ȳi = ∑ yi j , ni j=1"
993,0,[], Parameter Estimation,seg_143,"α̂i = μ̂0 − μ̂i ,"
994,0,['n'], Parameter Estimation,seg_143,k ni 1 σ̂ 2 = ∑∑ (yi j − ȳi )2. n − k i=1 j=1
995,1,"['degrees of freedom', 'levels', 'estimate', 'factor', 'variance']", Parameter Estimation,seg_143,"because we estimate the variance from k different factor levels, the correct degrees of freedom are n − k."
996,1,"['linear', 'factors', 'regression analysis', 'data', 'regression', 'hypotheses', 'linear regression', 'variation']", Hypothesis Testing,seg_145,"for testing the various hypotheses on the influence of the factors, we can apply the same general ideas as for the linear regression analysis. in particular, we can decompose the total variation sstot in the data, given by"
997,0,[], Hypothesis Testing,seg_145,sstot = ∑ ∑ (yi j − ȳ)2 i=1 j=1
998,1,"['model', 'factor', 'sum of squared', 'samples', 'mean', 'variation', 'level', 'anova']", Hypothesis Testing,seg_145,"into the variation explained by the anova model, and the remaining unexplained variation. in anova, the explained variation is the between-groups-variation, caused by the different means for each factor level. it is calculated as the sum of squared differences of group (i.e., factor level) means to the overall mean, scaled by the appropriate number of samples in each group."
999,0,[], Hypothesis Testing,seg_145,ssbetween = ∑ ni (ȳi − ȳ)2. i=1
1000,1,"['mean', 'data', 'variation']", Hypothesis Testing,seg_145,the remaining unexplained variation is the within-group-variation that measures how much the data from the same group disperses around the corresponding group mean.
1001,0,[], Hypothesis Testing,seg_145,sswithin = ∑ ∑ (yi j − ȳi )2 . i=1 j=1
1002,1,['variation'], Hypothesis Testing,seg_145,the total variation is then again composed of the sum of betweenand withingroup-variation:
1003,0,[], Hypothesis Testing,seg_145,sstot = ssbetween + sswithin.
1004,1,"['linear', 'model', 'estimated', 'parameters', 'independent', 'regression', 'linear regression', 'variation', 'anova']", Hypothesis Testing,seg_145,"as for the linear regression, the total variation is constant and independent of an anova model, but the contributions of the withinand the between-group-variations to the total variation depend on the estimated parameters of the anova model, allowing us to compare different models."
1005,1,"['model', 'levels', 'factor', 'data', 'variation', 'test', 'response', 'null hypothesis', 'hypothesis']", Hypothesis Testing,seg_145,"the global hypothesis. again, the boldest hypothesis we might want to test is the global null hypothesis that none of the factor levels has an influence on the response. if this hypothesis is not rejected, the model does not explain any of the variation in the data. the hypothesis is"
1006,0,[], Hypothesis Testing,seg_145,"h0 : μ1 = · · · = μk,"
1007,1,"['degrees of freedom', 'levels', 'factor', 'test statistic', 'errors', 'normally distributed', 'statistic', 'test', 'response']", Hypothesis Testing,seg_145,"which is also equivalent to h0 : α1 = · · · = αk = 0, i.e., no difference in the expected response to the factor levels. although we want to test the equality of group means, the test statistic is in fact based on comparing the between-group-variation ssbetween to the within-group-variation sswithin. dividing each by their corresponding degrees of freedom, this ratio has an f-distribution, provided that the errors εi j are normally distributed. the test statistic is thus"
1008,0,[], Hypothesis Testing,seg_145,"ssbetween/(k − 1) f = , sswithin/(n − k)"
1009,1,['level'], Hypothesis Testing,seg_145,"and with f ∼ f(k − 1, n − k), we reject h0 at level α, if f > f1−α(k − 1, n − k)."
1010,1,"['data', 'location', 'variation', 'response']", Hypothesis Testing,seg_145,"here is how this works: if the variation of response values in one group is similar to the overall variation, this indicates that these values can not be separated from the other groups. if, however, the variation in the group is much smaller than the overall variation, this means these values are more compact and have a distinct location within the overall data, making it easy to spot response values of this group. the two situations are depicted in fig. 4.12: if the response values are comparatively compact for each group, and the groups are different, the within-group-variations are small compared to the between-group-variations (left panel). conversely, similar withinand between-group-variations lead to many similar response values of different groups, making it difficult to distinguish them (right panel)."
1011,1,"['table', 'factor', 'states', 'data', 'hypothesis', 'level', 'test', 'null hypothesis', 'average']", Hypothesis Testing,seg_145,"example 39 let us test the global hypothesis for the example of cell growth in five different media. the global null hypothesis states that the medium does not cause any difference and thus the average cell growth is the same for all five conditions. we get the data as a table with two columns, the first containing the measured growth value (a number), the second the factor level (here, a simple name). to get an impression of how this data looks like, a selected subset of the rows are given below:"
1012,1,"['linear', 'model', 'factor', 'data', 'regression', 'linear regression', 'function', 'response', 'anova']", Hypothesis Testing,seg_145,"once the data is established, we can formalize the model using the same r formulas as for the linear regression and simply apply the function aov() instead of lm(). here, we compute the anova of the response growth with respect to the factor medium on the data d:"
1013,0,[], Hypothesis Testing,seg_145,"model <- aov(growth ˜ medium,data = d)"
1014,1,"['table', 'anova table', 'function', 'anova']", Hypothesis Testing,seg_145,the summary() function is used to give the anova table
1015,1,"['degrees of freedom', 'levels', 'factor', 'residuals', 'pooled variance', 'sum of squares', 'null hypothesis', 'data', 'mean', 'variance', 'test', 'response', 'hypothesis', 'hypothesis test', 'variation']", Hypothesis Testing,seg_145,"the column df contains the degrees of freedom of the factor and the residuals, the columns sum sq and mean sq contain the sum of squares and the mean sum of squares (variation). the value mean sq of the residuals is the pooled variance of the data, and the first row gives us the result of the global null hypothesis test. as indicated by the very low p-value, at least one of levels of the factor medium does significantly influence the response growth."
1016,1,"['linear', 'contrast', 'levels', 'factor', 'contrasts', 'hypotheses', 'level', 'test', 'response', 'hypothesis']", Hypothesis Testing,seg_145,"contrasts while the global hypothesis is a first check to decide whether the factor has any influence at all, we are often interested in a more detailed analysis to see which factor levels have influence and whether the influences of the different levels are significantly different. for this, we need to be able to test subsets of factor levels against each other and formally state hypotheses like “the first and second level of the factor differ from each other” and “the first and second level might not differ, but both differ from the third”. an elegant and powerful way to do this is by means of linear contrasts. a linear contrast is a weighted sum of the expected response to the factor levels"
1017,0,[], Hypothesis Testing,seg_145,"c = ∑ ciμi , i=1"
1018,1,"['linear', 'contrast', 'levels', 'factor', 'test', 'response', 'hypothesis']", Hypothesis Testing,seg_145,"where c = (c1, . . . , ck) are any numbers that sum to zero: c1 + · · · + ck = 0. let us assume that we want to test the hypothesis that the factor levels i and j yield the same response, which we can write as h0 : μi − μ j = 0. this hypothesis can be recast in terms of a linear contrast by setting the contrast values ci = +1, c j = −1 and cl = 0 for all other levels. the contrast then reads"
1019,0,[], Hypothesis Testing,seg_145,"c = μi − μ j ,"
1020,1,"['contrast', 'transformed', 'levels', 'estimate', 'factor', 'combinations', 'distribution', 'function', 'test', 'null hypothesis', 'hypothesis']", Hypothesis Testing,seg_145,"and we test the equivalent hypothesis h0 : c = 0. the problem is thus transformed from testing combinations of factor levels against each other to testing whether a function of the group means (the contrast) is zero. for performing this test, we need to find an estimate for the contrast c and to establish its distribution under the null hypothesis."
1021,1,"['contrast', 'estimators', 'estimator']", Hypothesis Testing,seg_145,"estimating the contrast is again straightforward: with the ci given, we can simply plug in the estimators for the group means to get the estimator for the contrast as"
1022,0,[], Hypothesis Testing,seg_145,c = ∑ ci ȳi . i=1
1023,1,"['contrast', 'interval', 'estimate', 'test statistic', 'rejection region', 'statistic', 'level', 'confidence', 'test', 'estimator', 'confidence interval']", Hypothesis Testing,seg_145,"we already saw that if we use an estimator as a test statistic, we can use the (1−α)- confidence interval of the estimator to get the corresponding rejection region of the test at level α: testing whether the contrast is zero at the test level α is equivalent to checking if the value zero is contained in the corresponding (1 − α)-confidence interval of the estimate. this ci is given by the usual form"
1024,0,[], Hypothesis Testing,seg_145,"ciα = c ± sα ŝe( c ),"
1025,1,"['standard error', 'estimator', 'error', 'standard']", Hypothesis Testing,seg_145,where the standard error of the estimator is given by
1026,0,[], Hypothesis Testing,seg_145,ni
1027,0,[], Hypothesis Testing,seg_145,ŝe( c ) = σ̂√√√√ ∑ i=1
1028,0,[], Hypothesis Testing,seg_145,"ci2 ,"
1029,1,"['tests', 'quantile', 'degrees of freedom', 'levels', 'contrast', 'contrasts', 'multiple testing']", Hypothesis Testing,seg_145,"as var( c ) = ∑i ci2var(ȳi ) and var(ȳi ) = σ̂ 2/ni . testing the difference of two levels i, j is equivalent to a t-test, which would give us the quantile sα = tα(ν) with corresponding degrees of freedom ν. however, using contrasts often means that we simultaneously perform several such pair-wise tests and thus need to correct for multiple testing. while the bonferroni-correction works for a small number of contrast, another option is to use scheffé’s-quantile"
1030,0,['n'], Hypothesis Testing,seg_145,"sα = √(k − 1)f1−α(k − 1, n − k)."
1031,1,"['level', 'contrast', 'test']", Hypothesis Testing,seg_145,the test at level α for a contrast c is then:
1032,0,[], Hypothesis Testing,seg_145,reject h0 : c = 0 if 0 ∈ ciα.
1033,1,"['contrast', 'case']", Hypothesis Testing,seg_145,setting up a contrast in the general case is very easy with the following rules:
1034,1,"['contrast', 'levels', 'factor', 'level']", Hypothesis Testing,seg_145,"• setting the contrast value ci to zero excludes the corresponding factor level. • factor levels with same sign for ci are lumped into one group. • factor levels with different signs are contrasted, i.e., their means are compared. • overall, the contrast values ci must sum to zero."
1035,1,['control'], Hypothesis Testing,seg_145,"let us look at a longer example, where we analyze the impact of the various media on the cell growth and try to figure out whether medium a and medium b differ from each other and from the control, and whether high versus low concentrations have an effect."
1036,1,"['model', 'level', 'factor', 'anova', 'data', 'function']", Hypothesis Testing,seg_145,"example 40 from the last example, we know that the anova model can explain the data with good p-value. in order to see the contribution of each factor level to this explanation, we can use the summary.lm() function."
1037,1,"['levels', 'factor', 'data', 'hypotheses', 'level', 'control', 'test']", Hypothesis Testing,seg_145,"the reported p-values of the four factor levels correspond to the difference of the factor level to the first factor level, i.e., the control. as expected from the first visual inspection of the data, all four differences of high a to the control, low a to the control and so forth are highly significant, indicating that using any medium yields better growth than using the control medium. however, the values do not allow us to see whether the concentrations have different effects, for example. we therefore want to test the following hypotheses:"
1038,1,['control'], Hypothesis Testing,seg_145,1. control vs. other media. the claim is that the effect of the other media differs
1039,1,"['contrast', 'levels', 'control']", Hypothesis Testing,seg_145,"from the control. we formulate this by giving a contrast of +4 to the control, and −1 to each of the four other levels, thus c = (4,−1,−1,−1,−1). 2. medium a and medium b differ. we can ignore the control here, and group the"
1040,0,[], Hypothesis Testing,seg_145,"two concentrations by choosing c = (0, 1, 1,−1,−1). 3. high and low concentrations of a have different effect: c = (0,−1, 1, 0, 0). 4. high and low concentrations of b have different effect: c = (0, 0, 0,−1, 1)."
1041,1,"['set', 'contrasts']", Hypothesis Testing,seg_145,these contrasts are simultaneously set up in r using the command
1042,0,[], Hypothesis Testing,seg_145,"contrasts(d$medium) <-cbind(c(4,−1,−1,−1,−1),c(0,1,1, + − 1,−1),c(0,−1,1,0,0),c(0,0,0,−1,1))"
1043,1,['contrast'], Hypothesis Testing,seg_145,which yields the contrast matrix
1044,1,"['factor', 'model', 'data']", Hypothesis Testing,seg_145,"this matrix is attached to the factor medium in the data frame d, and we can apply it by simply creating a new model with aov"
1045,0,[], Hypothesis Testing,seg_145,"model2 <- aov(growth ˜ medium,data = d)"
1046,0,[], Hypothesis Testing,seg_145,which yields the summary
1047,1,"['model', 'levels', 'factor', 'results', 'contrasts', 'data', 'level', 'control', 'response', 'hypothesis']", Hypothesis Testing,seg_145,"the first row gives again the global hypothesis that the factor levels give different response values. the second to fifth row give the results of the four contrasts attached to the data frame; only the difference of the control to the two other media is significant, whereas none of the comparisons between the non-control media shows a significant difference for α = 0.01. to simplify the model, we can therefore continue our analysis by lumping the four non-control factor levels into a single new factor level by"
1048,0,[], Hypothesis Testing,seg_145,"medlumped <- d$medium levels(medlumped)[2 : 5] <- ""medium"" d$medium <-medlumped"
1049,1,"['model', 'levels', 'parameters', 'estimate', 'factor', 'anova', 'function', 'full model']", Hypothesis Testing,seg_145,we again estimate the parameters for this new anova model with only two factor levels and compare it to the original full model using the anova() function. the
1050,1,"['level', 'model', 'explanatory', 'data']", Hypothesis Testing,seg_145,two models do not have significantly different explanatory power (at level α = 0.1) and we found the minimal model explaining the data.
1051,1,"['levels', 'factor', 'control', 'concentration']", Hypothesis Testing,seg_145,"the most simple explanation of the observed responses to the various factor levels is thus only the difference between the control and any medium and we found that there is no difference between the two media, nor between a medium in low or high concentration."
1052,1,"['error bars', 'estimated', 'levels', 'factor', 'results', 'plots', 'error bar', 'data', 'normally distributed', 'mean', 'level', 'plotting', 'response', 'error', 'anova']", Interpreting Error Bars,seg_147,"we conclude our investigation with a brief discussion of a common technique to find differences in the influences of factor levels on a response from plots of the data. in practice, the effect of a factor on a response is often not reported by giving the results of an anova, but rather by plotting bar-graphs of the response, with one bar per factor level. the height of each bar then corresponds to the estimated mean value of the group and an error bar is provided. sometimes, it is then argued that whenever two error bars overlap, the difference of the two groups is not significant, whereas if the bars do not overlap, the difference is significant. this, however, is only true for very particular choices of error bars under the strong additional assumption of normally distributed data in each group."
1053,1,"['plot', 'error bars', 'deviation', 'estimated', 'uncertainty', 'error bar', 'data', 'intervals', 'deviations', 'mean', 'standard', 'standard deviation', 'error', 'dispersion']", Interpreting Error Bars,seg_147,"standard deviations. the most common choice for error bars is to plot intervals of length corresponding to one standard deviation in each direction. apart from the problem that very different data can give the same bar-plot (recall sect. 1.8.4), knowing the mean and standard deviation does not allow inference of differences in the means. this is because the standard deviation represents the dispersion of values in the data, but not the uncertainty in the estimated means. this error bar is thus descriptive, but not inferential."
1054,1,"['error bars', 'estimated', 'confidence intervals', 'interval', 'estimate', 'intervals', 'mean', 'probability', 'confidence', 'error', 'confidence interval']", Interpreting Error Bars,seg_147,"confidence intervals. an option to derive inferential error bars is to compute the 95%-confidence intervals for each group mean and use their lengths as the lengths of the error bars. we know that the true value of a group mean is contained in the confidence interval of its estimate with probability 1 − α. if the confidence intervals of two estimated group means do not overlap, we can therefore safely infer that the two means are significantly different because the probability of them having the same"
1055,1,['method'], Interpreting Error Bars,seg_147,"value is lower than α. however, nothing can be said if two bars do overlap; thus, we can only infer different, but not equal group means using this method."
1056,1,"['deviation', 'estimated', 'levels', 'interval', 'factor', 'standard error', 'errors', 'mean', 'standard', 'level', 'standard deviation', 'confidence', 'standard errors', 'response', 'error', 'confidence interval']", Interpreting Error Bars,seg_147,"standard errors. alternatively, we can use the standard errors of the estimated group means. if we again denote by σ̂ the standard deviation of the response, taken over all factor levels, we already know that the estimated standard error for the mean ȳi of group i is ŝe(ȳi ) = σ̂ /√ni (the confidence interval then has length 2t1−α/2(ni )ŝe(ȳi )). let us recall that the difference μi − μ j of two group means is significant at the level α if"
1057,0,[], Interpreting Error Bars,seg_147,∣∣ ∣∣ ŝe(
1058,0,[], Interpreting Error Bars,seg_147,ȳ
1059,0,[], Interpreting Error Bars,seg_147,ȳ
1060,0,[], Interpreting Error Bars,seg_147,−
1061,0,[], Interpreting Error Bars,seg_147,−
1062,0,[], Interpreting Error Bars,seg_147,ȳ
1063,0,[], Interpreting Error Bars,seg_147,ȳ
1064,0,[], Interpreting Error Bars,seg_147,j j ) ∣∣
1065,0,[], Interpreting Error Bars,seg_147,∣∣
1066,0,['n'], Interpreting Error Bars,seg_147,> t1−α/2(ni + n j − 2).
1067,1,"['sample', 'standard errors', 'error bars', 'estimated', 'standard error', 'approximation', 'errors', 'standard', 'variance', 'error', 'variances']", Interpreting Error Bars,seg_147,"if the sample sizes ni and n j for the two groups are large enough, we know that t0.975(ni + n j − 2) ≈ 2 is a good approximation of the t-quantiles for α = 0.05. thus, the means are significantly different if they are more than twice the standard error of the estimated difference apart. the problem is that the standard error of the difference depends on the two groups that we compare and we can only give error bars that are valid for a particular pair of groups. however, this standard error is always smaller than the sum of the two standard errors of the individual means, because the variance of the difference is the sum of the variances:"
1068,0,['n'], Interpreting Error Bars,seg_147,σ̂ 2 σ̂ 2 ŝe(ȳi − ȳ j ) = √ ni + n j
1069,0,[], Interpreting Error Bars,seg_147,σ̂ 2 σ̂ 2 σ̂ σ̂ σ̂ σ̂ ≤ √ + + 2 √ √
1070,0,[], Interpreting Error Bars,seg_147,= √ (√
1071,0,['n'], Interpreting Error Bars,seg_147,+ )2 ni n j ni ni ni √n j
1072,0,[], Interpreting Error Bars,seg_147,= ŝe(ȳi ) + ŝe(ȳ j ).
1073,1,"['error bars', 'standard error', 'errors', 'standard', 'standard errors', 'error']", Interpreting Error Bars,seg_147,"therefore, we can use the individual standard errors for the means (one per group), but by doing so overestimate the standard error of the difference. this still allows us to infer that there is no significant difference (at the 5%-level) of any two group means if their corresponding error bars overlap. however, non-overlapping bars do not allow to infer a significant difference."
1074,1,"['sample size', 'sample', 'standard error', 'case', 'standard', 'error']", Interpreting Error Bars,seg_147,"least-significant difference (lsd). in the special case that each group has the same sample size, so n1 = · · · = nk, the standard error of the differences"
1075,0,['n'], Interpreting Error Bars,seg_147,σ̂ 2 σ̂ 2 σ̂ 2 σ̂ ŝe(ȳi − ȳ j ) = √ ni + n j = √
1076,0,[], Interpreting Error Bars,seg_147,2 ni
1077,0,[], Interpreting Error Bars,seg_147,= √2 √ni
1078,0,[], Interpreting Error Bars,seg_147,","
1079,0,[], Interpreting Error Bars,seg_147,"is the same for all pairs i,j of groups. from this, we derive the lsd as lsd ="
1080,1,"['error bar', 'error']", Interpreting Error Bars,seg_147,"σ̂ t1−α/2√2√ni . using a length of lsd/2 for each arm of the error bar, this allows us"
1081,0,[], Interpreting Error Bars,seg_147,to infer that there is no significant difference between the means of any two groups if the corresponding bars overlap and also that there is a significant difference between them if their corresponding bars do not overlap.
1082,1,"['linear', 'parameters', 'estimate', 'variables', 'covariates', 'regression', 'linear regression', 'response']", Summary,seg_149,"regression analysis allows us to study the influence of one or several covariates on a response. if all variables are metric, we can perform a linear regression and estimate parameters either by a least-squares or by a robust rlm or lqs approach."
1083,1,"['covariate', 'estimation', 'predicted', 'intervals']", Summary,seg_149,confidence intervals for predicted values also depend on the distance of the new covariate value to those used for estimation.
1084,1,"['model', 'covariates', 'variation']", Summary,seg_149,decomposing the total variation of the model into explained and unexplained variation allows us to use f-tests to discard covariates.
1085,1,"['linear', 'levels', 'factor', 'contrasts', 'combinations', 'test', 'response']", Summary,seg_149,anova lets us simultaneously compare several means by regressing a response on a factor with several levels. we test combinations of levels using f-tests on linear contrasts.
1086,1,"['error bars', 'levels', 'factor', 'normally distributed', 'samples', 'error']", Summary,seg_149,"for normally distributed samples, we can visually infer differences in factor levels from overlaps in appropriate error bars."
