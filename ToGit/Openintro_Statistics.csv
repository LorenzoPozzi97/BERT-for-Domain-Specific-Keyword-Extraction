,Relevance,Tags,Heading,Seg,Sentence
0,1,"['observations', 'statistics', 'case', 'data', 'sampling', 'experiments', 'statistical']",Chapter  Introduction to data,seg_1,"1.1 case study: using stents to prevent strokes 1.2 data basics 1.3 sampling principles and strategies 1.4 experiments scientists seek to answer questions using rigorous methods and careful observations. these observations – collected from the likes of field notes, surveys, and experiments – form the backbone of a statistical investigation and are called data. statistics is the study of how best to collect, analyze, and draw conclusions from data, and in this first chapter, we focus on both the properties of data and on the collection of data. for videos, slides, and other resources, please visit www.openintro.org/os"
1,1,"['evaluating', 'treatment', 'statistics']", Case study using stents to prevent strokes,seg_3,"section 1.1 introduces a classic challenge in statistics: evaluating the efficacy of a medical treatment. terms in this section, and indeed much of this chapter, will all be revisited later in the text. the plan for now is simply to get a sense of the role statistics can play in practice."
2,1,"['risk', 'experiment', 'events']", Case study using stents to prevent strokes,seg_3,in this section we will consider an experiment that studies effectiveness of stents in treating patients at risk of stroke. stents are devices put inside blood vessels that assist in patient recovery after cardiac events and reduce the risk of an additional heart attack or death. many doctors have hoped that there would be similar benefits for patients at risk of stroke. we start by writing the principal question the researchers hope to answer:
3,1,['risk'], Case study using stents to prevent strokes,seg_3,does the use of stents reduce the risk of stroke?
4,1,['experiment'], Case study using stents to prevent strokes,seg_3,the researchers who asked this question conducted an experiment with 451 at-risk patients. each volunteer patient was randomly assigned to one of two groups:
5,1,"['risk', 'factors', 'treatment group', 'treatment']", Case study using stents to prevent strokes,seg_3,"treatment group. patients in the treatment group received a stent and medical management. the medical management included medications, management of risk factors, and help in lifestyle modification."
6,1,"['control group', 'treatment', 'control', 'treatment group']", Case study using stents to prevent strokes,seg_3,"control group. patients in the control group received the same medical management as the treatment group, but they did not receive stents."
7,1,"['treatment group', 'treatment', 'control group', 'control']", Case study using stents to prevent strokes,seg_3,"researchers randomly assigned 224 patients to the treatment group and 227 to the control group. in this study, the control group provides a reference point against which we can measure the medical impact of stents in the treatment group."
8,1,"['results', 'outcomes']", Case study using stents to prevent strokes,seg_3,"researchers studied the effect of stents at two time points: 30 days after enrollment and 365 days after enrollment. the results of 5 patients are summarized in figure 1.1. patient outcomes are recorded as “stroke” or “no event”, representing whether or not the patient had a stroke at the end of a time period."
9,1,"['event ', 'treatment', 'control', 'event']", Case study using stents to prevent strokes,seg_3,patient group 0-30 days 0-365 days 1 treatment no event no event 2 treatment stroke stroke 3 treatment no event no event . . . . . . . . . 450 control no event no event 451 control no event no event
10,1,['results'], Case study using stents to prevent strokes,seg_3,figure 1.1: results for five patients from the stent study.
11,1,"['treatment group', 'table', 'treatment', 'intersection', 'data', 'statistical']", Case study using stents to prevent strokes,seg_3,"considering data from each patient individually would be a long, cumbersome path towards answering the original research question. instead, performing a statistical data analysis allows us to consider all of the data at once. figure 1.2 summarizes the raw data in a more helpful way. in this table, we can quickly see what happened over the entire study. for instance, to identify the number of patients in the treatment group who had a stroke within 30 days, we look on the left-side of the table at the intersection of the treatment and stroke: 33."
12,1,"['treatment', 'control', 'event']", Case study using stents to prevent strokes,seg_3,0-30 days 0-365 days stroke no event stroke no event treatment 33 191 45 179 control 13 214 28 199 total 46 405 73 378
13,1,"['descriptive statistics', 'statistics']", Case study using stents to prevent strokes,seg_3,figure 1.2: descriptive statistics for the stent study.
14,1,"['treatment', 'treatment group']", Case study using stents to prevent strokes,seg_3,"of the 224 patients in the treatment group, 45 had a stroke by the end of the first year. using these two numbers, compute the proportion of patients in the treatment group who had a stroke by the end of their first year. (please note: answers to all guided practice exercises are provided using footnotes.)1"
15,1,"['control groups', 'table', 'treatment', 'results', 'statistics', 'data', 'statistic', 'treatment and control groups', 'control']", Case study using stents to prevent strokes,seg_3,"we can compute summary statistics from the table. a summary statistic is a single number summarizing a large amount of data. for instance, the primary results of the study after 1 year could be described by two summary statistics: the proportion of people who had a stroke in the treatment and control groups."
16,1,['treatment'], Case study using stents to prevent strokes,seg_3,proportion who had a stroke in the treatment (stent) group: 45/224 = 0.20 = 20%.
17,1,"['control group', 'control']", Case study using stents to prevent strokes,seg_3,proportion who had a stroke in the control group: 28/227 = 0.12 = 12%.
18,1,"['rate', 'treatment group', 'treatment', 'statistics', 'data', 'statistical']", Case study using stents to prevent strokes,seg_3,"these two summary statistics are useful in looking for differences in the groups, and we are in for a surprise: an additional 8% of patients in the treatment group had a stroke! this is important for two reasons. first, it is contrary to what doctors expected, which was that stents would reduce the rate of strokes. second, it leads to a statistical question: do the data show a “real” difference between the groups?"
19,1,"['sample size', 'sample', 'data', 'variation', 'process']", Case study using stents to prevent strokes,seg_3,"this second question is subtle. suppose you flip a coin 100 times. while the chance a coin lands heads in any given coin flip is 50%, we probably won’t observe exactly 50 heads. this type of fluctuation is part of almost any type of data generating process. it is possible that the 8% difference in the stent study is due to this natural variation. however, the larger the difference we observe (for a particular sample size), the less believable it is that the difference is due to chance. so what we are really asking is the following: is the difference so large that we should reject the notion that it was due to chance?"
20,1,['statistical'], Case study using stents to prevent strokes,seg_3,"while we don’t yet have our statistical tools to fully address this question on our own, we can comprehend the conclusions of the published analysis: there was compelling evidence of harm by stents in this study of stroke patients."
21,1,"['results', 'representative']", Case study using stents to prevent strokes,seg_3,"be careful: do not generalize the results of this study to all patients and all stents. this study looked at patients with very specific characteristics who volunteered to be a part of this study and who may not be representative of all stroke patients. in addition, there are many types of stents and this study only considered the self-expanding wingspan stent (boston scientific). however, this study does leave us with an important lesson: we should keep our eyes open for surprises."
22,1,['treatment'], Case study using stents to prevent strokes,seg_3,"oluopf the t mr ige raait nempe ain nt , which10 33 43 appropriate area (s) used in occurred within 30 min and still persc isto en d t 2r 4o hl later. 2 44 46 the treatment of migraine atsince the most active site in contt roo llitnaglmigraine pain12 77 89 was the antero-internal part of the antitragus, the aim of tacks. this study was to verify the therapeutic value of this elective area (appropriate point) and to compare it with an area of the ear (representing the sciatic nerve) which is probably i m"
23,0,['n'], Case study using stents to prevent strokes,seg_3,n ients i inn gtrh
24,0,[], Case study using stents to prevent strokes,seg_3,"ouep tbr,ea thtem"
25,0,[], Case study using stents to prevent strokes,seg_3,loewnetr gbr raonu chpow
26,0,['e'], Case study using stents to prevent strokes,seg_3,f e three ap nta heiln
27,0,[], Case study using stents to prevent strokes,seg_3,eee 24 hours after receiving acupuncture? materials and meth(od d) s your findings so far migh
28,1,['treatment'], Case study using stents to prevent strokes,seg_3,"eta di ,ve treatment for migraines for all people"
29,0,[], Case study using stents to prevent strokes,seg_3,the study enrolled 94 few mah
30,0,[], Case study using stents to prevent strokes,seg_3,"leos, sdu iaff gneorsefdro asmmim graiignr eaintewso.foh r eo acw heev are"
31,0,"['o', 'n']", Case study using stents to prevent strokes,seg_3,". r this is not the only possible conclusion that can be drawn based without aura following theoinntey rnoau tiornafi lncd laisn sig ficsats ion foa f r. wih n aat ll ipsatio entse , o thteheear apcuopsusnicb tulre ewx asplaa lwnaa ystipoen rfor the observed difference between the headache disorders [5], whpoew rc"
32,0,[], Case study using stents to prevent strokes,seg_3,ereen sutbasg eqeuseno tlfy epxa am ti
33,0,[], Case study using stents to prevent strokes,seg_3,perf ierneceed2a4 cuphuonu cturrs ista . fttheeranraelc
34,1,['data'], Case study using stents to prevent strokes,seg_3,"yse isivoifng acupuncture in the two groups? at the women’s headache centre, department of gynaethe diaries collecting vas data was conducted by an cology and obstetrics of turin university. they were all impartial operator who did not know the group each patient included in the study1 du .2 ring asmiin"
35,1,['treatment'], Case study using stents to prevent strokes,seg_3,"gru ais neitai tts ackapn rodvida ednthiab t iot w ic as i, n. part i. researchers studying the effect of antibiotic treatment for acute i p"
36,0,['e'], Case study using stents to prevent strokes,seg_3,e -6 adults diagnosed with acute sinusitis to
37,0,[], Case study using stents to prevent strokes,seg_3,gible patients were ro
38,0,[], Case study using stents to prevent strokes,seg_3,ndobg linrdo lyup ass si:gnterdeta ottm heentticoar
39,0,[], Case study using stents to prevent strokes,seg_3,l c evoan lutar tio on
40,0,[], Case study using stents to prevent strokes,seg_3,l. os f ttu hedy difp fea rerntciec sip bea twnetesnrtehc eev
41,0,[], Case study using stents to prevent strokes,seg_3,s either a 10-day course of amoxicillin (an following two groupasn : tgirb
42,1,['range'], Case study using stents to prevent strokes,seg_3,"ou io p ta ic) (no=r 4a 6)p (a la vecre agbeoagseimio la btrain in ed a inpp t0e ,atr1a ,ntc2e , ta3nadndtats4tien. tht eh tweopglra oucpesbo consisted of symptomatic treatments 35.93 years, range 15s–u6c 0h"
43,0,[], Case study using stents to prevent strokes,seg_3,stludd iee dcw on
44,0,[], Case study using stents to prevent strokes,seg_3,"asge pesrt foarnmtes d, ue sitncg. anatanatlh yse is eon"
45,0,[], Case study using stents to prevent strokes,seg_3,"fdvao rifant cehe 10-day period, patients were asked if 3"
46,0,[], Case study using stents to prevent strokes,seg_3,cyh epx atip enetrw
47,0,[], Case study using stents to prevent strokes,seg_3,iean s cae skd edim
48,0,[], Case study using stents to prevent strokes,seg_3,tiprle esponses is summarized below.
49,1,"['t test', 'intensity', 'test']", Case study using stents to prevent strokes,seg_3,"informed consent to participation in the study. moreover, to evaluate the difference between group b migraine intensity was measured by means of a vas and group a, a t test for uns pae irlefd-rdeap taow"
50,0,[], Case study using stents to prevent strokes,seg_3,im ayspr po erv
51,1,"['level', 'variable']", Case study using stents to prevent strokes,seg_3,"- ement before applying nct (t0). formed for each level of the variable ‘‘tii mn e’’s .y inm thp et co asm e os f in group a, a specific algometer exerting a maximum proportions, a chi square tey st ewsas applied. all ann"
52,1,"['statistical', 'test']", Case study using stents to prevent strokes,seg_3,"alo yses total pressure of 250 g (sedatelec, france) was chosen to were performed using the statistical package for the social identify the tender points with pain–pressure test (ppt).grosu cip"
53,1,['mean'], Case study using stents to prevent strokes,seg_3,"ences t (srpesa s)tsm oftewnat re prog6 ra6m. all values given1i9 n the 85 every tender point located within the identified area by the followingc teo xn t at rr eo rl eported as6 ar5 ithmetic mean (±se1 m 6). 81 pilot study (fig. 1, area m) was tested with nct for 10 s total 131 35 166 starting from the auricle, that was ipsilateral, to the side of"
54,1,"['treatment', 'experienced', 'treatment group']", Case study using stents to prevent strokes,seg_3,ultsthe treatment group experienced improvement in symptoms? permanent needle ((b
55,0,[], Case study using stents to prevent strokes,seg_3,"sehdaattepleercc,enft ranecx e)pewraisenceod nlyim 89p parto iev ne tsm oue tn oftthienensty iremgp rotuo pm of s94in"
56,1,['experiment'], Case study using stents to prevent strokes,seg_3,"(43tih negrocuopntrol group? inserted after 1 min. on the contrary, if pain did not lessen a, 46 in group b) completed the experiment. four patients after 1 min, a furthe( rc te)ndien"
57,0,[], Case study using stents to prevent strokes,seg_3,"r pw oinh t iwch as g chrao llu enp ged ind thae higwh ite hdrrepwer frcoemntha egse tudoyf, p beact au ie senthseyex expperie ie ncnec deanimprovement in symptoms? same area and so on( .dw)hey"
58,0,[], Case study using stents to prevent strokes,seg_3,pau tirenfi tsnbd ecianmgesaw so
59,0,['n'], Case study using stents to prevent strokes,seg_3,arefa ofram n igh
60,0,[], Case study using stents to prevent strokes,seg_3,untbesau rag blgeeesxt acearbr ate ioanlod f p iff
61,0,[], Case study using stents to prevent strokes,seg_3,ainer inetn hece periin odepff ree cecdt in iv
62,1,['control'], Case study using stents to prevent strokes,seg_3,"ge thn eess of antibiotic and placebo treatments initial decrease in the pain in all the zones of the head last control at 24 h (two from group a and two from group affected, they were invitedfo toruism e apsrpo ecv ifi"
63,0,[], Case study using stents to prevent strokes,seg_3,in c g diasrym carpdttomsbo
64,0,[], Case study using stents to prevent strokes,seg_3,inwues reiteixsc.luh deo dw fre
65,0,[], Case study using stents to prevent strokes,seg_3,"omvetrh,e t sth atiis stiicsalnao natlytsh isesio"
66,1,['intensity'], Case study using stents to prevent strokes,seg_3,ncnely possible conclusion that can be drawn score the intensity of the pab ina w se
67,1,['intervals'], Case study using stents to prevent strokes,seg_3,thf ea yr r. eqw uesh tedat thi es ro emn oe vao lt oh fe tr hep no es es dli eb s.le oe nexpal ta ientation for the observed difference between intervals: after 10 min (t th
68,0,['e'], Case study using stents to prevent strokes,seg_3,"inge (ts2o ),f ap fta ertien frtosmign rot upha e dain d t"
69,0,[], Case study using stents to prevent strokes,seg_3,t giiovt eihceracn ond senp tlta oc theeb im opt
70,0,[], Case study using stents to prevent strokes,seg_3,"larne ta oftm theent groups that experience improvement 60 min (t3), after 120 minin (t4s)y , m"
71,1,['mean'], Case study using stents to prevent strokes,seg_3,"andpatfo tem r 2s4 hof(ts5i)n . usitiss?emi-permanent needles. in group a, the mean number of"
72,1,"['data matrix', 'data']", Data basics,seg_5,effective organization and description of data is a first step in most analyses. this section introduces the data matrix for organizing data as well as some terminology about different forms of data that will be used throughout this book.
73,1,"['variables', 'observations', 'data matrices', 'data']", Data basics,seg_5,"1.2.1 observations, variables, and data matrices"
74,1,"['observations', 'data', 'set', 'data set']", Data basics,seg_5,"figure 1.3 displays rows 1, 2, 3, and 50 of a data set for 50 randomly sampled loans offered through lending club, which is a peer-to-peer lending company. these observations will be referred to as the loan50 data set."
75,1,"['observational unit', 'rate', 'table', 'variables', 'case']", Data basics,seg_5,"each row in the table represents a single loan. the formal name for a row is a case or observational unit. the columns represent characteristics, called variables, for each of the loans. for example, the first row represents a loan of $7,500 with an interest rate of 7.34%, where the borrower is based in maryland (md) and has an income of $70,000."
76,0,[], Data basics,seg_5,"what is the grade of the first loan in figure 1.3? and what is the home ownership status of the borrower for that first loan? for these guided practice questions, you can check your answer in the footnote.4"
77,1,"['measurement', 'variables', 'data', 'variable']", Data basics,seg_5,"in practice, it is especially important to ask clarifying questions to ensure important aspects of the data are understood. for instance, it is always important to be sure we know what each variable means and the units of measurement. descriptions of the loan50 variables are given in figure 1.4."
78,1,"['data matrix', 'data']", Data basics,seg_5,figure 1.3: four rows from the loan50 data matrix.
79,1,"['variables', 'data', 'set', 'data set']", Data basics,seg_5,figure 1.4: variables and their descriptions for the loan50 data set.
80,1,"['observational unit', 'case', 'data', 'variable', 'data matrix']", Data basics,seg_5,"the data in figure 1.3 represent a data matrix, which is a convenient and common way to organize data, especially if collecting data in a spreadsheet. each row of a data matrix corresponds to a unique case (observational unit), and each column corresponds to a variable."
81,1,"['variables', 'data', 'cases', 'data matrix']", Data basics,seg_5,"when recording data, use a data matrix unless you have a very good reason to use a different structure. this structure allows new cases to be added as rows or new variables as new columns."
82,1,"['data matrix', 'data']", Data basics,seg_5,"the grades for assignments, quizzes, and exams in a course are often recorded in a gradebook that takes the form of a data matrix. how might you organize grade data using a data matrix?5"
83,1,"['rate', 'states', 'data', 'population']", Data basics,seg_5,"we consider data for 3,142 counties in the united states, which includes each county’s name, the state where it resides, its population in 2017, how its population changed from 2010 to 2017, poverty rate, and six additional characteristics. how might these data be organized in a data matrix?6"
84,1,"['variables', 'county', 'data', 'set', 'data matrix', 'data set']", Data basics,seg_5,"the data described in guided practice 1.4 represents the county data set, which is shown as a data matrix in figure 1.5. the variables are summarized in figure 1.6."
85,1,['variables'], Data basics,seg_5,1.2.2 types of variables
86,1,"['rate', 'median', 'variables', 'county', 'data', 'set', 'data set']", Data basics,seg_5,"examine the unemp rate, pop, state, and median edu variables in the county data set. each of these variables is inherently different from the other three, yet some share certain characteristics."
87,1,"['rate', 'range', 'variable', 'average', 'numerical']", Data basics,seg_5,"first consider unemp rate, which is said to be a numerical variable since it can take a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. on the other hand, we would not classify a variable reporting telephone area codes as numerical since the average, sum, and difference of area codes doesn’t have any clear meaning."
88,1,"['rate', 'discrete', 'variable', 'population', 'continuous', 'numerical']", Data basics,seg_5,"the pop variable is also numerical, although it seems to be a little different than unemp rate. this variable of the population count can only take whole non-negative numbers (0, 1, 2, ...). for this reason, the population variable is said to be discrete since it can only take numerical values with jumps. on the other hand, the unemployment rate variable is said to be continuous."
89,1,"['levels', 'categorical', 'variable', 'categorical variable', 'categories']", Data basics,seg_5,"the variable state can take up to 51 values after accounting for washington, dc: al, ak, ..., and wy. because the responses themselves are categories, state is called a categorical variable, and the possible values are called the variable’s levels."
90,1,"['ordinal', 'levels', 'median', 'categorical', 'county', 'nominal', 'variable', 'categorical variable', 'level']", Data basics,seg_5,"finally, consider the median edu variable, which describes the median education level of county residents and takes values below hs, hs diploma, some college, or bachelors in each county. this variable seems to be a hybrid: it is a categorical variable but the levels have a natural ordering. a variable with these properties is called an ordinal variable, while a regular categorical variable without this type of special ordering is called a nominal variable. to simplify analyses, any ordinal variable in this book will be treated as a nominal (unordered) categorical variable."
91,1,"['variables', 'numerical', 'categorical']", Data basics,seg_5,all variables categorical numerical
92,1,['variables'], Data basics,seg_5,figure 1.7: breakdown of variables into their respective types.
93,1,"['categorical', 'variables', 'statistics', 'discrete', 'continuous', 'numerical']", Data basics,seg_5,"data were collected about students in a statistics course. three variables were recorded for each student: number of siblings, student height, and whether the student had previously taken a statistics course. classify each of the variables as continuous numerical, discrete numerical, or categorical."
94,1,"['categorical', 'variables', 'statistics', 'discrete', 'variable', 'continuous', 'categories', 'numerical']", Data basics,seg_5,"the number of siblings and student height represent numerical variables. because the number of siblings is a count, it is discrete. height varies continuously, so it is a continuous numerical variable. the last variable classifies students into two categories – those who have and those who have not taken a statistics course – which makes this variable categorical."
95,1,"['experienced', 'experiment', 'evaluating', 'treatment', 'variable', 'control', 'numerical']", Data basics,seg_5,an experiment is evaluating the effectiveness of a new drug in treating migraines. a group variable is used to indicate the experiment group for each patient: treatment or control. the num migraines variable represents the number of migraines the patient experienced during a 3-month period. classify each variable as either numerical or categorical?7
96,1,['variables'], Data basics,seg_5,1.2.3 relationships between variables
97,1,['variables'], Data basics,seg_5,many analyses are motivated by a researcher looking for a relationship between two or more variables. a social scientist may like to answer some of the following questions:
98,1,"['average', 'county', 'percent']", Data basics,seg_5,"(1) if homeownership is lower than the national average in one county, will the percent of multi-unit"
99,1,"['average', 'county']", Data basics,seg_5,structures in that county tend to be above or below the national average?
100,1,"['average', 'county', 'population']", Data basics,seg_5,(2) does a higher than average increase in county population tend to correspond to counties with
101,1,['median'], Data basics,seg_5,higher or lower median household incomes?
102,1,"['level', 'predictor', 'median']", Data basics,seg_5,(3) how useful a predictor is median education level for the median household income for us
103,1,"['statistics', 'county', 'data', 'set', 'data set']", Data basics,seg_5,"to answer these questions, data must be collected, such as the county data set shown in figure 1.5. examining summary statistics could provide insights for each of the three questions about counties. additionally, graphs can be used to visually explore data."
104,1,"['plot', 'rate', 'variables', 'scatterplot', 'rates', 'data', 'county', 'set', 'data set', 'numerical', 'percent']", Data basics,seg_5,"scatterplots are one type of graph used to study the relationship between two numerical variables. figure 1.8 compares the variables homeownership and multi unit, which is the percent of units in multi-unit structures (e.g. apartments, condos). each point on the plot represents a single county. for instance, the highlighted dot corresponds to county 413 in the county data set: chattahoochee county, georgia, which has 39.4% of units in multi-unit structures and a homeownership rate of 31.3%. the scatterplot suggests a relationship between the two variables: counties with a higher rate of multi-units tend to have lower homeownership rates. we might brainstorm as to why this relationship exists and investigate each idea to determine which are the most reasonable explanations."
105,1,['percent'], Data basics,seg_5,80% etar 60% pihsren 40% woe ● mo 20% h 0% 0% 20% 40% 60% 80% 100% percent of units in multi−unit structures
106,1,"['rate', 'scatterplot', 'county', 'percent']", Data basics,seg_5,"figure 1.8: a scatterplot of homeownership versus the percent of units that are in multi-unit structures for us counties. the highlighted dot represents chattahoochee county, georgia, which has a multi-unit rate of 39.4% and a homeownership rate of 31.3%."
107,1,"['plot', 'variables', 'dependent', 'dependent variables', 'associated', 'rates']", Data basics,seg_5,"the multi-unit and homeownership rates are said to be associated because the plot shows a discernible pattern. when two variables show some connection with one another, they are called associated variables. associated variables can also be called dependent variables and vice-versa."
108,1,['median'], Data basics,seg_5,20% egn s a r 10% h a c e y n 7 oi t r a e l v 0% u o po ● p −10% $0 $20k $40k $60k $80k $100k $120k median household income
109,1,"['county', 'scatterplot', 'population', 'median']", Data basics,seg_5,"figure 1.9: a scatterplot showing pop change against median hh income. owsley county of kentucky, is highlighted, which lost 3.63% of its population from 2010 to 2017 and had median household income of $22,736."
110,1,"['variables', 'data', 'set', 'data set']", Data basics,seg_5,"examine the variables in the loan50 data set, which are described in figure 1.4 on page 12. create"
111,1,['variables'], Data basics,seg_5,8 two questions about possible relationships between variables in loan50 that are of interest to you.
112,1,"['median', 'variables', 'scatterplot', 'associated', 'population']", Data basics,seg_5,"this example examines the relationship between a county’s population change from 2010 to 2017 and median household income, which is visualized as a scatterplot in figure 1.9. are these variables associated?"
113,1,"['plot', 'median', 'variables', 'county', 'associated', 'population']", Data basics,seg_5,"the larger the median household income for a county, the higher the population growth observed for the county. while this trend isn’t true for every county, the trend in the plot is evident. since there is some relationship between the variables, they are associated."
114,1,"['association', 'median', 'variables', 'positive association', 'negatively associated', 'associated', 'population', 'rates']", Data basics,seg_5,"because there is a downward trend in figure 1.8 – counties with more units in multi-unit structures are associated with lower homeownership – these variables are said to be negatively associated. a positive association is shown in the relationship between the median hh income and pop change in figure 1.9, where counties with higher median household income tend to have higher rates of population growth."
115,1,"['variables', 'independent', 'associated']", Data basics,seg_5,"if two variables are not associated, then they are said to be independent. that is, two variables are independent if there is no evident relationship between the two."
116,1,"['variables', 'independent', 'associated']", Data basics,seg_5,a pair of variables are either related in some way (associated) or not (independent). no pair of variables is both associated and independent.
117,1,"['response variables', 'variables', 'explanatory', 'response']", Data basics,seg_5,1.2.4 explanatory and response variables
118,1,"['variables', 'county', 'data', 'variable', 'set', 'data set']", Data basics,seg_5,"when we ask questions about the relationship between two variables, we sometimes also want to determine if the change in one variable causes a change in the other. consider the following rephrasing of an earlier question about the county data set:"
119,1,"['county', 'population', 'median']", Data basics,seg_5,"if there is an increase in the median household income in a county, does this drive an increase in its population?"
120,1,"['median', 'variable', 'explanatory variable', 'explanatory', 'population']", Data basics,seg_5,"in this question, we are asking whether one variable affects another. if this is our underlying belief, then median household income is the explanatory variable and the population change is the"
121,1,"['variable', 'response', 'response variable']", Data basics,seg_5,9 response variable in the hypothesized relationship.
122,1,"['variable', 'explanatory variable', 'explanatory', 'response variable', 'response']", Data basics,seg_5,"when we suspect one variable might causally affect another, we label the first variable the explanatory variable and the second the response variable."
123,1,['response'], Data basics,seg_5,explanatory response
124,1,['variable'], Data basics,seg_5,variable variable
125,1,"['variables', 'cases', 'variable']", Data basics,seg_5,"for many pairs of variables, there is no hypothesized relationship, and these labels would not be applied to either variable in such cases."
126,1,"['experiment', 'variables', 'variable']", Data basics,seg_5,bear in mind that the act of labeling the variables in this way does nothing to guarantee that a causal relationship exists. a formal evaluation to check whether one variable causes a change in another requires an experiment.
127,1,"['observational studies', 'experiments']", Data basics,seg_5,1.2.5 introducing observational studies and experiments
128,1,"['observational studies', 'data', 'experiments', 'data collection']", Data basics,seg_5,there are two primary types of data collection: observational studies and experiments.
129,1,"['association', 'variables', 'observational study', 'observational studies', 'data', 'information', 'hypotheses']", Data basics,seg_5,"researchers perform an observational study when they collect data in a way that does not directly interfere with how the data arise. for instance, researchers may collect information via surveys, review medical or company records, or follow a cohort of many similar individuals to form hypotheses about why certain diseases might develop. in each of these situations, researchers merely observe the data that arise. in general, observational studies can provide evidence of a naturally occurring association between variables, but they cannot by themselves show a causal connection."
130,1,"['sample', 'experiment', 'treatment', 'case', 'trial', 'variable', 'explanatory variable', 'response', 'explanatory', 'randomized experiment', 'response variable']", Data basics,seg_5,"when researchers want to investigate the possibility of a causal connection, they conduct an experiment. usually there will be both an explanatory and a response variable. for instance, we may suspect administering a drug will reduce mortality in heart attack patients over the following year. to check if there really is a causal connection between the explanatory variable and the response, researchers will collect a sample of individuals and split them into groups. the individuals in each group are assigned a treatment. when individuals are randomly assigned to a group, the experiment is called a randomized experiment. for example, each heart attack patient in the drug trial could be randomly assigned, perhaps by flipping a coin, into one of two groups: the first group receives a placebo (fake treatment) and the second group receives the drug. see the case study in section 1.1 for another example of an experiment, though that study did not employ a placebo."
131,1,"['association', 'experiment', 'randomized experiment']", Data basics,seg_5,"in general, association does not imply causation, and causation can only be inferred from a randomized experiment."
132,1,"['variables', 'data', 'cases']", Sampling principles and strategies,seg_7,the first step in conducting research is to identify topics or questions that are to be investigated. a clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. it is also important to consider how data are collected so that they are reliable and help achieve the research goals.
133,1,"['samples', 'populations']", Sampling principles and strategies,seg_7,1.3.1 populations and samples
134,0,[], Sampling principles and strategies,seg_7,consider the following three research questions:
135,1,['average'], Sampling principles and strategies,seg_7,1. what is the average mercury content in swordfish in the atlantic ocean?
136,1,['average'], Sampling principles and strategies,seg_7,"2. over the last 5 years, what is the average time to complete a degree for duke undergrads?"
137,0,[], Sampling principles and strategies,seg_7,3. does a new drug reduce the number of deaths in patients with severe heart disease?
138,1,"['sample', 'estimate', 'case', 'data', 'cases', 'population', 'average']", Sampling principles and strategies,seg_7,"each research question refers to a target population. in the first question, the target population is all swordfish in the atlantic ocean, and each fish represents a case. often times, it is too expensive to collect data for every case in a population. instead, a sample is taken. a sample represents a subset of the cases and is often a small fraction of the population. for instance, 60 swordfish (or some other number) in the population might be selected, and this sample data may be used to provide an estimate of the population average and answer the research question."
139,1,['population'], Sampling principles and strategies,seg_7,"for the second and third questions above, identify the target population and what represents an"
140,1,['case'], Sampling principles and strategies,seg_7,18 individual case.
141,1,['anecdotal evidence'], Sampling principles and strategies,seg_7,1.3.2 anecdotal evidence
142,0,[], Sampling principles and strategies,seg_7,consider the following possible responses to the three research questions:
143,1,"['average', 'concentration']", Sampling principles and strategies,seg_7,"1. a man on the news got mercury poisoning from eating swordfish, so the average mercury concentration in swordfish must be dangerously high."
144,0,[], Sampling principles and strategies,seg_7,"2. i met two students who took more than 7 years to graduate from duke, so it must take longer to graduate at duke than at many other colleges."
145,0,[], Sampling principles and strategies,seg_7,"3. my friend’s dad had a heart attack and died after they gave him a new heart disease drug, so the drug must not work."
146,1,"['anecdotal evidence', 'data', 'cases', 'population', 'representative']", Sampling principles and strategies,seg_7,"each conclusion is based on data. however, there are two problems. first, the data only represent one or two cases. second, and more importantly, it is unclear whether these cases are actually representative of the population. data collected in this haphazard fashion are called anecdotal evidence."
147,1,"['cases', 'data']", Sampling principles and strategies,seg_7,"be careful of data collected in a haphazard fashion. such evidence may be true and verifiable, but it may only represent extraordinary cases."
148,0,[], Sampling principles and strategies,seg_7,"figure 1.10: in february 2010, some media pundits cited one large snow storm as valid evidence against global warming. as comedian jon stewart pointed out, “it’s one storm, in one region, of one country.”"
149,1,"['sample', 'cases', 'population']", Sampling principles and strategies,seg_7,"anecdotal evidence typically is composed of unusual cases that we recall based on their striking characteristics. for instance, we are more likely to remember the two people we met who took 7 years to graduate than the six others who graduated in four years. instead of looking at the most unusual cases, we should examine a sample of many cases that represent the population."
150,1,"['sampling', 'population']", Sampling principles and strategies,seg_7,1.3.3 sampling from a population
151,1,"['sample', 'random', 'estimate', 'samples', 'biases', 'population', 'random sample']", Sampling principles and strategies,seg_7,"we might try to estimate the time to graduation for duke undergraduates in the last 5 years by collecting a sample of students. all graduates in the last 5 years represent the population, and graduates who are selected for review are collectively called the sample. in general, we always seek to randomly select a sample from a population. the most basic type of random selection is equivalent to how raffles are conducted. for example, in selecting graduates, we could write each graduate’s name on a raffle ticket and draw 100 tickets. the selected names would represent a random sample of 100 graduates. we pick samples randomly to reduce the chance we introduce biases."
152,1,['sample'], Sampling principles and strategies,seg_7,all graduates sample
153,1,['sample'], Sampling principles and strategies,seg_7,"figure 1.11: in this graphic, five graduates are randomly selected from the population to be included in the sample."
154,1,"['representative', 'sample']", Sampling principles and strategies,seg_7,suppose we ask a student who happens to be majoring in nutrition to select several graduates for the study. what kind of students do you think she might collect? do you think her sample would be representative of all graduates?
155,1,"['sample', 'bias', 'risk', 'biased', 'samples', 'population']", Sampling principles and strategies,seg_7,"perhaps she would pick a disproportionate number of graduates from health-related fields. or perhaps her selection would be a good representation of the population. when selecting samples by hand, we run the risk of picking a biased sample, even if their bias isn’t intended."
156,1,['sample'], Sampling principles and strategies,seg_7,all graduates sample graduates from health−related fields
157,1,['sample'], Sampling principles and strategies,seg_7,"figure 1.12: asked to pick a sample of graduates, a nutrition major might inadvertently pick a disproportionate number of graduates from health-related majors."
158,1,"['sample', 'bias', 'simple random sample', 'random sample', 'case', 'skewed', 'sampling', 'cases', 'population', 'random']", Sampling principles and strategies,seg_7,"if someone was permitted to pick and choose exactly which graduates were included in the sample, it is entirely possible that the sample could be skewed to that person’s interests, which may be entirely unintentional. this introduces bias into a sample. sampling randomly helps resolve this problem. the most basic random sample is called a simple random sample, and which is equivalent to using a raffle to select cases. this means that each case in the population has an equal chance of being included and there is no implied connection between the cases in the sample."
159,1,"['sample', 'bias', 'rate', 'random', 'simple random sample', 'results', 'population', 'random sample', 'representative', 'skew']", Sampling principles and strategies,seg_7,"the act of taking a simple random sample helps minimize bias. however, bias can crop up in other ways. even when people are picked at random, e.g. for surveys, caution must be exercised if the non-response rate is high. for instance, if only 30% of the people randomly sampled for a survey actually respond, then it is unclear whether the results are representative of the entire population. this non-response bias can skew results."
160,1,['sample'], Sampling principles and strategies,seg_7,population of interest sample
161,0,[], Sampling principles and strategies,seg_7,population actually sampled
162,1,['population'], Sampling principles and strategies,seg_7,"figure 1.13: due to the possibility of non-response, surveys studies may only reach a certain group within the population. it is difficult, and often times impossible, to completely fix this problem."
163,1,"['convenience sample', 'sample']", Sampling principles and strategies,seg_7,"another common downfall is a convenience sample, where individuals who are easily accessible are more likely to be included in the sample. for instance, if a political survey is done by stopping people walking in the bronx, this will not represent all of new york city. it is often difficult to discern what sub-population a convenience sample represents."
164,0,[], Sampling principles and strategies,seg_7,"we can easily access ratings for products, sellers, and companies through websites. these ratings are based only on those people who go out of their way to provide a rating. if 50% of online reviews for a product are negative, do you think this means that 50% of buyers are dissatisfied with the product?19"
165,1,['observational studies'], Sampling principles and strategies,seg_7,1.3.4 observational studies
166,1,"['treatment', 'observational studies', 'county', 'data', 'hypotheses', 'experiments', 'observational data']", Sampling principles and strategies,seg_7,"data where no treatment has been explicitly applied (or explicitly withheld) is called observational data. for instance, the loan data and county data described in section 1.2 are both examples of observational data. making causal conclusions based on experiments is often reasonable. however, making the same causal conclusions based on observational data can be treacherous and is not recommended. thus, observational studies are generally only sufficient to show associations or form hypotheses that we later check using experiments."
167,1,"['observational study', 'mean']", Sampling principles and strategies,seg_7,"suppose an observational study tracked sunscreen use and skin cancer, and it was found that the more sunscreen someone used, the more likely the person was to have skin cancer. does this mean sunscreen causes skin cancer?20"
168,1,"['risk', 'association', 'information', 'variable']", Sampling principles and strategies,seg_7,"some previous research tells us that using sunscreen actually reduces skin cancer risk, so maybe there is another variable that can explain this hypothetical association between sunscreen usage and skin cancer. one important piece of information that is absent is sun exposure. if someone is out in the sun all day, she is more likely to use sunscreen and more likely to get skin cancer. exposure to the sun is unaccounted for in the simple investigation."
169,0,[], Sampling principles and strategies,seg_7,use sunscreen skin cancer
170,1,"['method', 'response variables', 'variables', 'observational studies', 'variable', 'confounding', 'confounding variables', 'response', 'explanatory', 'correlated']", Sampling principles and strategies,seg_7,"sun exposure is what is called a confounding variable,21 which is a variable that is correlated with both the explanatory and response variables. while one method to justify making causal conclusions from observational studies is to exhaust the search for confounding variables, there is no guarantee that all confounding variables can be examined or measured."
171,1,"['rate', 'percentage', 'association', 'variables', 'county', 'variable', 'negative association']", Sampling principles and strategies,seg_7,"figure 1.8 shows a negative association between the homeownership rate and the percentage of multiunit structures in a county. however, it is unreasonable to conclude that there is a causal relationship between the two variables. suggest a variable that might explain the negative relationship.22"
172,1,"['risk', 'variables', 'data', 'information', 'events', 'sets', 'prospective and retrospective studies', 'data sets', 'retrospective studies', 'prospective study']", Sampling principles and strategies,seg_7,"observational studies come in two forms: prospective and retrospective studies. a prospective study identifies individuals and collects information as events unfold. for instance, medical researchers may identify and follow a group of patients over many years to assess the possible influences of behavior on cancer risk. one example of such a study is the nurses’ health study, started in 1976 and expanded in 1989. this prospective study recruits registered nurses and then collects data from them using questionnaires. retrospective studies collect data after events have taken place, e.g. researchers may review past events in medical records. some data sets may contain both prospectivelyand retrospectively-collected variables."
173,1,['sampling'], Sampling principles and strategies,seg_7,1.3.5 four sampling methods
174,1,"['estimates', 'representations', 'associated', 'random', 'graphical', 'data', 'graphical representations', 'population', 'statistical', 'random sampling', 'observational data', 'errors', 'sampling']", Sampling principles and strategies,seg_7,"almost all statistical methods are based on the notion of implied randomness. if observational data are not collected in a random framework from a population, these statistical methods – the estimates and errors associated with the estimates – are not reliable. here we consider four random sampling techniques: simple, stratified, cluster, and multistage sampling. figures 1.14 and 1.15 provide graphical representations of these techniques."
175,0,[], Sampling principles and strategies,seg_7,stratum 2 stratum 4 stratum 6
176,1,"['simple random sampling', 'stratified sampling', 'sampling', 'cases', 'strata', 'random', 'random sampling']", Sampling principles and strategies,seg_7,"figure 1.14: examples of simple random and stratified sampling. in the top panel, simple random sampling was used to randomly select the 18 cases. in the bottom panel, stratified sampling was used: cases were grouped into strata, then simple random sampling was employed within each stratum."
177,1,"['sample', 'simple random sample', 'random sample', 'case', 'information', 'sampling', 'cases', 'population', 'random', 'random sampling']", Sampling principles and strategies,seg_7,"simple random sampling is probably the most intuitive form of random sampling. consider the salaries of major league baseball (mlb) players, where each player is a member of one of the league’s 30 teams. to take a simple random sample of 120 baseball players and their salaries, we could write the names of that season’s several hundreds of players onto slips of paper, drop the slips into a bucket, shake the bucket around until we are sure the names are all mixed up, then draw out slips until we have the sample of 120 players. in general, a sample is referred to as “simple random” if each case in the population has an equal chance of being included in the final sample and knowing that a case is included in a sample does not provide useful information about which other cases are included."
178,1,"['sample', 'simple random sampling', 'method', 'sampling', 'cases', 'strata', 'population', 'random', 'random sampling']", Sampling principles and strategies,seg_7,"stratified sampling is a divide-and-conquer sampling strategy. the population is divided into groups called strata. the strata are chosen so that similar cases are grouped together, then a second sampling method, usually simple random sampling, is employed within each stratum. in the baseball salary example, the teams could represent the strata, since some teams have a lot more money (up to 4 times as much!). then we might randomly sample 4 players from each team for a total of 120 players."
179,1,"['sample', 'stratified sampling', 'outcome of interest', 'simple random sample', 'random sample', 'stratified sample', 'data', 'sampling', 'cases', 'random', 'outcome']", Sampling principles and strategies,seg_7,stratified sampling is especially useful when the cases in each stratum are very similar with respect to the outcome of interest. the downside is that analyzing data from a stratified sample is a more complex task than analyzing data from a simple random sample. the analysis methods introduced in this book would need to be extended to analyze data collected using stratified sampling.
180,1,['cases'], Sampling principles and strategies,seg_7,why would it be good for cases within each stratum to be very similar?
181,1,"['estimate', 'estimates', 'cases', 'population']", Sampling principles and strategies,seg_7,"we might get a more stable estimate for the subpopulation in a stratum if the cases are very similar, leading to more precise estimates within each group. when we combine these estimates into a single estimate for the full population, that population estimate will tend to be more precise since each individual group estimate is itself more precise."
182,1,"['sample', 'random', 'observations', 'population', 'random sample', 'cluster sample']", Sampling principles and strategies,seg_7,"in a cluster sample, we break up the population into many groups, called clusters. then we sample a fixed number of clusters and include all observations from each of those clusters in the sample. a multistage sample is like a cluster sample, but rather than keeping all observations in each cluster, we collect a random sample within each selected cluster."
183,1,"['cluster or multistage sampling', 'stratified sampling', 'variability', 'data', 'sampling']", Sampling principles and strategies,seg_7,"sometimes cluster or multistage sampling can be more economical than the alternative sampling techniques. also, unlike stratified sampling, these approaches are most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves don’t look very different from one another. for example, if neighborhoods represented clusters, then cluster or multistage sampling work best when the neighborhoods are very diverse. a downside of these methods is that more advanced techniques are typically required to analyze the data, though the methods in this book can be extended to handle such data."
184,1,"['rate', 'method', 'sampling', 'test']", Sampling principles and strategies,seg_7,"suppose we are interested in estimating the malaria rate in a densely tropical portion of rural indonesia. we learn that there are 30 villages in that part of the indonesian jungle, each more or less similar to the next. our goal is to test 150 individuals for malaria. what sampling method should be employed?"
185,1,"['sample', 'stratified sampling', 'simple random sample', 'random sample', 'data', 'information', 'sampling', 'cluster sampling', 'random', 'cluster sample', 'data collection', 'strata']", Sampling principles and strategies,seg_7,"a simple random sample would likely draw individuals from all 30 villages, which could make data collection extremely expensive. stratified sampling would be a challenge since it is unclear how we would build strata of similar individuals. however, cluster sampling or multistage sampling seem like very good ideas. if we decided to use multistage sampling, we might randomly select half of the villages, then randomly select 10 people from each. this would probably reduce our data collection costs substantially in comparison to a simple random sample, and the cluster sample would still give us reliable information, even if we would need to analyze the data with slightly more advanced methods than we discuss in this book."
186,0,[], Sampling principles and strategies,seg_7,cluster 9 cluster 2 cluster 5 cluster 7 ●
187,0,[], Sampling principles and strategies,seg_7,cluster 1 cluster 9 cluster 2 cliunsdteexr 5
188,1,"['sample', 'observations', 'sampled cluster', 'case', 'data', 'sampling', 'cluster sampling', 'measuring']", Sampling principles and strategies,seg_7,"figure 1.15: examples of cluster and multistage sampling. in the top panel, cluster sampling was used: data were binned into nine clusters, three of these clusters were sampled, and all observations within these three cluster were included in the sample. in the bottom panel, multistage sampling was used, which differs from cluster sampling only in that we randomly select a subset of each cluster to be included in the sample rather than measuring every case in each sampled cluster."
189,1,['percent'], Sampling principles and strategies,seg_7,x 60 e ef il 50 0% 20% 40% 60% 80% 100% percent internet users
190,1,"['experiment', 'variables', 'treatment', 'randomization', 'cases', 'randomized experiments', 'randomized experiment', 'experiments']", Experiments,seg_9,"studies where the researchers assign treatments to cases are called experiments. when this assignment includes randomization, e.g. using a coin flip to decide which treatment a patient receives, it is called a randomized experiment. randomized experiments are fundamentally important when trying to show a causal connection between two variables."
191,1,"['design', 'experimental']", Experiments,seg_9,1.4.1 principles of experimental design
192,1,['experiments'], Experiments,seg_9,randomized experiments are generally built on four principles.
193,1,"['cases', 'control']", Experiments,seg_9,"controlling. researchers assign treatments to cases, and they do their best to control any other"
194,1,['control'], Experiments,seg_9,"differences in the groups.27 for example, when patients take a drug in pill form, some patients take the pill with only a sip of water while others may have it with an entire glass of water. to control for the effect of water consumption, a doctor may ask all patients to drink a 12 ounce glass of water with the pill."
195,1,"['variables', 'treatment', 'treatment groups']", Experiments,seg_9,randomization. researchers randomize patients into treatment groups to account for variables
196,1,"['bias', 'treatment', 'control group', 'treatment or control group', 'control']", Experiments,seg_9,"that cannot be controlled. for example, some patients may be more susceptible to a disease than others due to their dietary habits. randomizing patients into the treatment or control group helps even out such differences, and it also prevents accidental bias from entering the study."
197,1,"['cases', 'estimate']", Experiments,seg_9,"replication. the more cases researchers observe, the more accurately they can estimate the effect"
198,1,"['sample', 'replicate', 'variable', 'explanatory variable', 'response', 'explanatory']", Experiments,seg_9,"of the explanatory variable on the response. in a single study, we replicate by collecting a sufficiently large sample. additionally, a group of scientists may replicate an entire study to verify an earlier finding."
199,1,"['variables', 'treatment']", Experiments,seg_9,"blocking. researchers sometimes know or suspect that variables, other than the treatment, influ-"
200,1,"['treatment group', 'treatment', 'treatment groups', 'cases', 'control group', 'variable', 'response', 'control']", Experiments,seg_9,"ence the response. under these circumstances, they may first group individuals based on this variable into blocks and then randomize cases within each block to the treatment groups. this strategy is often referred to as blocking. for instance, if we are looking at the effect of a drug on heart attacks, we might first split patients in the study into low-risk and high-risk blocks, then randomly assign half the patients from each block to the control group and the other half to the treatment group, as shown in figure 1.16. this strategy ensures each treatment group has an equal number of low-risk and high-risk patients."
201,1,"['experimental', 'design', 'data', 'experiments', 'statistical']", Experiments,seg_9,"it is important to incorporate the first three experimental design principles into any study, and this book describes applicable methods for analyzing data from such experiments. blocking is a slightly more advanced technique, and statistical methods in this book may be extended to analyze data collected using blocking."
202,1,"['bias', 'experiments']", Experiments,seg_9,1.4.2 reducing bias in human experiments
203,1,"['bias', 'data', 'cases', 'standard', 'experiments', 'data collection', 'unbiased']", Experiments,seg_9,"randomized experiments are the gold standard for data collection, but they do not ensure an unbiased perspective into the cause and effect relationship in all cases. human studies are perfect examples where bias can unintentionally arise. here we reconsider a study where a new drug was used to treat heart attack patients. in particular, researchers wanted to know if the drug reduced deaths in patients."
204,1,"['treatment group', 'experiment', 'treatment', 'control group', 'randomized experiment', 'control']", Experiments,seg_9,"these researchers designed a randomized experiment because they wanted to draw causal conclusions about the drug’s effect. study volunteers28 were randomly placed into two study groups. one group, the treatment group, received the drug. the other group, called the control group, did not receive any drug treatment."
205,1,"['risk', 'treatment group', 'treatment', 'randomization', 'treatment groups', 'variable', 'categories']", Experiments,seg_9,"figure 1.16: blocking using a variable depicting patient risk. patients are first divided into low-risk and high-risk blocks, then each block is evenly separated into the treatment groups using randomization. this strategy ensures an equal representation of patients in each treatment group from both the low-risk and high-risk categories."
206,1,"['treatment', 'risk', 'treatment group']", Experiments,seg_9,"put yourself in the place of a person in the study. if you are in the treatment group, you are given a fancy new drug that you anticipate will help you. on the other hand, a person in the other group doesn’t receive the drug and sits idly, hoping her participation doesn’t increase her risk of death. these perspectives suggest there are actually two effects: the one of interest is the effectiveness of the drug, and the second is an emotional effect that is difficult to quantify."
207,1,"['bias', 'treatment', 'results', 'control group', 'control']", Experiments,seg_9,"researchers aren’t usually interested in the emotional effect, which might bias the study. to circumvent this problem, researchers do not want patients to know which group they are in. when researchers keep the patients uninformed about their treatment, the study is said to be blind. but there is one problem: if a patient doesn’t receive a treatment, she will know she is in the control group. the solution to this problem is to give fake treatments to patients in the control group. a fake treatment is called a placebo, and an effective placebo is the key to making a study truly blind. a classic example of a placebo is a sugar pill that is made to look like the actual treatment pill. often times, a placebo results in a slight but real improvement in patients. this effect has been dubbed the placebo effect."
208,1,"['bias', 'treatment']", Experiments,seg_9,"the patients are not the only ones who should be blinded: doctors and researchers can accidentally bias a study. when a doctor knows a patient has been given the real treatment, she might inadvertently give that patient more attention or care than a patient that she knows is on the placebo. to guard against this bias, which again has been found to have a measurable effect in some instances, most modern studies employ a double-blind setup where doctors or researchers who interact with patients are, just like the patients, unaware of who is or is not receiving the"
209,1,['treatment'], Experiments,seg_9,29 treatment.
210,1,['experiment'], Experiments,seg_9,look back to the study in section 1.1 where researchers were testing whether stents were effective at reducing strokes in at-risk patients. is this an experiment? was the study blinded? was it double-blinded?30
211,0,[], Experiments,seg_9,"for the study in section 1.1, could the researchers have employed a placebo? if so, what would that placebo have looked like?31"
212,1,"['risk', 'experiment', 'treatment', 'control group', 'control']", Experiments,seg_9,"you may have many questions about the ethics of sham surgeries to create a placebo after reading guided practice 1.17. these questions may have even arisen in your mind when in the general experiment context, where a possibly helpful treatment was withheld from individuals in the control group; the main difference is that a sham surgery tends to create additional risk, while withholding a treatment only maintains a person’s risk."
213,1,"['treatment', 'experiments', 'risk']", Experiments,seg_9,"there are always multiple viewpoints of experiments and placebos, and rarely is it obvious which is ethically “correct”. for instance, is it ethical to use a sham surgery when it creates a risk to the patient? however, if we don’t use sham surgeries, we may promote the use of a costly treatment that has no real effect; if this happens, money and other resources will be diverted away from other treatments that are known to be helpful. ultimately, this is a difficult situation where we cannot perfectly protect both the patients who have volunteered for the study and the patients who may benefit (or not) from the treatment in the future."
214,0,[], Experiments,seg_9,percent with bachelor's degree
215,1,"['categorical data', 'categorical', 'statistics', 'case', 'data', 'statistical', 'numerical']",Chapter  Summarizing data,seg_11,"2.1 examining numerical data 2.2 considering categorical data 2.3 case study: malaria vaccine this chapter focuses on the mechanics and construction of summary statistics and graphs. we use statistical software for generating the summaries and graphs presented in this chapter and book. however, since this might be your first exposure to these concepts, we take our time in this chapter to detail how to create them. mastery of the content presented in this chapter will be crucial for understanding the methods and techniques introduced in rest of the book. for videos, slides, and other resources, please visit www.openintro.org/os"
216,1,"['categorical', 'variables', 'data', 'variable', 'set', 'data set', 'categorical variables', 'numerical']", Examining numerical data,seg_13,"in this section we will explore techniques for summarizing numerical variables. for example, consider the loan amount variable from the loan50 data set, which represents the loan size for all 50 loans in the data set. this variable is numerical since we can sensibly discuss the numerical difference of the size of two loans. on the other hand, area codes and zip codes are not numerical, but rather they are categorical variables."
217,1,"['variables', 'county', 'data', 'set', 'sets', 'data set', 'data sets']", Examining numerical data,seg_13,"throughout this section and the next, we will apply these methods using the loan50 and county data sets, which were introduced in section 1.2. if you’d like to review the variables from either data set, see figures 1.3 and 1.5."
218,1,"['paired data', 'data', 'scatterplots', 'paired']", Examining numerical data,seg_13,2.1.1 scatterplots for paired data
219,1,"['rate', 'variables', 'scatterplot', 'case', 'data', 'county', 'cases', 'set', 'data set', 'numerical']", Examining numerical data,seg_13,"a scatterplot provides a case-by-case view of data for two numerical variables. in figure 1.8 on page 16, a scatterplot was used to examine the homeownership rate against the fraction of housing units that were part of multi-unit properties (e.g. apartments) in the county data set. another scatterplot is shown in figure 2.1, comparing the total income of a borrower (total income) and the amount they borrowed (loan amount) for the loan50 data set. in any scatterplot, each point represents a single case. since there are 50 cases in loan50, there are 50 points in figure 2.1."
220,1,"['scatterplot', 'data', 'set', 'data set']", Examining numerical data,seg_13,figure 2.1: a scatterplot of total income versus loan amount for the loan50 data set.
221,0,[], Examining numerical data,seg_13,"looking at figure 2.1, we see that there are many borrowers with an income below $100,000 on the left side of the graph, while there are a handful of borrowers with income above $250,000."
222,1,"['plot', 'rate', 'median', 'variables']", Examining numerical data,seg_13,"figure 2.2 shows a plot of median household income against the poverty rate for 3,142 counties. what can be said about the relationship between these variables?"
223,1,"['nonlinear', 'scatterplots']", Examining numerical data,seg_13,"the relationship is evidently nonlinear, as highlighted by the dashed line. this is different from previous scatterplots we’ve seen, which show relationships that do not show much, if any, curvature in the trend."
224,1,"['rate', 'percent']", Examining numerical data,seg_13,$120k e m o $100k cn i d l $80k o h es $60k u oh n $40k a i d e $20k m $0 0% 10% 20% 30% 40% 50% poverty rate (percent)
225,1,"['model', 'rate', 'median', 'scatterplot', 'county', 'data', 'set', 'data set', 'statistical model', 'statistical']", Examining numerical data,seg_13,figure 2.2: a scatterplot of the median household income against the poverty rate for the county data set. a statistical model has also been fit to the data and is shown as a dashed line.
226,1,"['data', 'scatterplots']", Examining numerical data,seg_13,"what do scatterplots reveal about the data, and how are they useful?1"
227,1,"['variables', 'scatterplot', 'association']", Examining numerical data,seg_13,describe two variables that would have a horseshoe-shaped association in a scatterplot (∩ or _).2
228,1,"['dot plots', 'plots', 'mean']", Examining numerical data,seg_13,2.1.2 dot plots and the mean
229,1,"['plot', 'rate', 'variables', 'scatterplot', 'cases', 'variable', 'dot plot']", Examining numerical data,seg_13,"sometimes two variables are one too many: only one variable may be of interest. in these cases, a dot plot provides the most basic of displays. a dot plot is a one-variable scatterplot; an example using the interest rate of 50 loans is shown in figure 2.3. a stacked version of this dot plot is shown in figure 2.4."
230,1,['rate'], Examining numerical data,seg_13,5% 10% 15% 20% 25% interest rate
231,1,"['plot', 'rate', 'data', 'set', 'data set', 'mean', 'dot plot']", Examining numerical data,seg_13,figure 2.3: a dot plot of interest rate for the loan50 data set. the distribution’s mean is shown as a red triangle.
232,1,"['rate', 'percent']", Examining numerical data,seg_13,"5% 10% 15% 20% 25% interest rate, rounded to nearest percent"
233,1,"['plot', 'rate', 'rates', 'data', 'set', 'data set', 'mean', 'dot plot', 'percent']", Examining numerical data,seg_13,"figure 2.4: a stacked dot plot of interest rate for the loan50 data set. the rates have been rounded to the nearest percent in this plot, and the distribution’s mean is shown as a red triangle."
234,1,"['rate', 'observations', 'data', 'distribution', 'mean', 'number of observations', 'average', 'rates']", Examining numerical data,seg_13,"the mean, often called the average, is a common way to measure the center of a distribution of data. to compute the mean interest rate, we add up all the interest rates and divide by the number of observations:"
235,1,"['sample', 'rate', 'sample mean', 'distribution', 'variable', 'mean', 'average']", Examining numerical data,seg_13,"the sample mean is often labeled x̄. the letter x is being used as a generic placeholder for the variable of interest, interest rate, and the bar over the x communicates we’re looking at the average interest rate, which for these 50 loans was 11.57%. it is useful to think of the mean as the balancing point of the distribution, and it’s shown as a triangle in figures 2.3 and 2.4."
236,1,"['sample', 'sample mean', 'observations', 'mean', 'number of observations']", Examining numerical data,seg_13,the sample mean can be computed as the sum of the observed values divided by the number of observations:
237,0,['n'], Examining numerical data,seg_13,"where x1, x2, . . . , xn represent the n observed values."
238,1,['mean'], Examining numerical data,seg_13,examine the equation for the mean. what does x1 correspond to? and x2? can you infer a general meaning to what xi might represent?3
239,1,['sample'], Examining numerical data,seg_13,what was n in this sample of loans?4
240,1,"['sample', 'sample mean', 'estimate', 'observations', 'population mean', 'data', 'variable', 'set', 'data set', 'mean', 'population', 'average']", Examining numerical data,seg_13,"the loan50 data set represents a sample from a larger population of loans made through lending club. we could compute a mean for this population in the same way as the sample mean. however, the population mean has a special label: µ. the symbol µ is the greek letter mu and represents the average of all observations in the population. sometimes a subscript, such as x, is used to represent which variable the population mean refers to, e.g. µx. often times it is too expensive to measure the population mean precisely, so we often estimate µ using the sample mean, x̄."
241,1,"['sample', 'rate', 'estimated', 'estimate', 'data', 'set', 'data set', 'mean', 'population', 'average']", Examining numerical data,seg_13,"the average interest rate across all loans in the population can be estimated using the sample data. based on the sample of 50 loans, what would be a reasonable estimate of µx, the mean interest rate for all loans in the full data set?"
242,1,"['sample', 'rate', 'sample mean', 'estimate', 'mean', 'population', 'average']", Examining numerical data,seg_13,"the sample mean, 11.57%, provides a rough estimate of µx. while it’s not perfect, this is our single best guess of the average interest rate of all the loans in the population under study."
243,1,"['sample', 'sample mean', 'estimates', 'samples', 'mean', 'point estimates']", Examining numerical data,seg_13,"in chapter 5 and beyond, we will develop tools to characterize the accuracy of point estimates like the sample mean. as you might have guessed, point estimates based on larger samples tend to be more accurate than those based on smaller samples."
244,1,['mean'], Examining numerical data,seg_13,the mean is useful because it allows us to rescale or standardize a metric into something more easily interpretable and comparable. provide 2 examples where the mean is useful for making comparisons.
245,1,"['control group', 'set', 'standard', 'control', 'trial']", Examining numerical data,seg_13,"1. we would like to understand if a new drug is more effective at treating asthma attacks than the standard drug. a trial of 1500 adults is set up, where 500 receive the new drug, and 1000 receive a standard drug in the control group:"
246,1,['standard'], Examining numerical data,seg_13,new drug standard drug number of patients 500 1000 total asthma attacks 200 300
247,1,['average'], Examining numerical data,seg_13,"comparing the raw counts of 200 to 300 asthma attacks would make it appear that the new drug is better, but this is an artifact of the imbalanced group sizes. instead, we should look at the average number of asthma attacks per patient in each group:"
248,1,['standard'], Examining numerical data,seg_13,new drug: 200/500 = 0.4 standard drug: 300/1000 = 0.3
249,1,"['treatment', 'average', 'treatment group', 'standard']", Examining numerical data,seg_13,the standard drug has a lower average number of asthma attacks per patient than the average in the treatment group.
250,1,"['evaluating', 'statistic', 'average']", Examining numerical data,seg_13,"2. emilio opened a food truck last year where he sells burritos, and his business has stabilized over the last 3 months. over that 3 month period, he has made $11,000 while working 625 hours. emilio’s average hourly earnings provides a useful statistic for evaluating whether his venture is, at least from a financial perspective, worth it:"
251,1,"['average', 'standard']", Examining numerical data,seg_13,"by knowing his average hourly wage, emilio now has put his earnings into a standard unit that is easier to compare with many other jobs that he might consider."
252,1,"['county', 'data', 'set', 'data set', 'mean', 'average']", Examining numerical data,seg_13,"suppose we want to compute the average income per person in the us. to do so, we might first think to take the mean of the per capita incomes across the 3,142 counties in the county data set. what would be a better approach?"
253,1,"['county', 'data', 'variable', 'set', 'data set', 'mean', 'average']", Examining numerical data,seg_13,"the county data set is special in that each county actually represents many individual people. if we were to simply average across the income variable, we would be treating counties with 5,000 and 5,000,000 residents equally in the calculations. instead, we should compute the total income for each county, add up all the counties’ totals, and then divide by the number of people in all the counties. if we completed these steps with the county data, we would find that the per capita income for the us is $30,861. had we computed the simple mean of per capita income across counties, the result would have been just $26,093!"
254,1,"['weighted mean', 'weighted means', 'mean', 'information']", Examining numerical data,seg_13,"this example used what is called a weighted mean. for more information on this topic, check out the following online supplement regarding weighted means openintro.org/d?file=stat wtd mean."
255,1,['histograms'], Examining numerical data,seg_13,2.1.3 histograms and shape
256,1,"['histogram', 'observations', 'plots', 'observation', 'set', 'rates', 'data', 'samples', 'sets', 'data sets', 'data set', 'bin', 'plot', 'table', 'dot plot']", Examining numerical data,seg_13,"dot plots show the exact value for each observation. this is useful for small data sets, but they can become hard to read with larger samples. rather than showing the value of each observation, we prefer to think of the value as belonging to a bin. for example, in the loan50 data set, we created a table of counts for the number of loans with interest rates between 5.0% and 7.5%, then the number of loans with rates between 7.5% and 10.0%, and so on. observations that fall on the boundary of a bin (e.g. 10.00%) are allocated to the lower bin. this tabulation is shown in figure 2.5. these binned counts are plotted as bars in figure 2.6 into what is called a histogram, which resembles a more heavily binned version of the stacked dot plot shown in figure 2.4."
257,1,"['rate', 'data']", Examining numerical data,seg_13,figure 2.5: counts for the binned interest rate data.
258,1,['rate'], Examining numerical data,seg_13,15 y 10 cneuqerf 5 0 5% 10% 15% 20% 25% interest rate
259,1,"['rate', 'histogram', 'skewed', 'distribution']", Examining numerical data,seg_13,figure 2.6: a histogram of interest rate. this distribution is strongly skewed to the right.
260,1,"['rate', 'data', 'set', 'data set', 'data density', 'rates']", Examining numerical data,seg_13,"histograms provide a view of the data density. higher bars represent where the data are relatively more common. for instance, there are many more loans with rates between 5% and 10% than loans with rates between 20% and 25% in the data set. the bars make it easy to see how the density of the data changes relative to the interest rate."
261,1,"['data', 'distribution', 'longer right tail', 'tail', 'rates']", Examining numerical data,seg_13,"histograms are especially convenient for understanding the shape of the data distribution. figure 2.6 suggests that most loans have rates under 15%, while only a handful of loans have rates above 20%. when data trail off to the right in this way and has a longer right tail, the shape is said to be right skewed.5"
262,1,"['symmetric', 'skewed', 'data', 'distribution', 'sets', 'long left tail', 'tail', 'data sets']", Examining numerical data,seg_13,"data sets with the reverse characteristic – a long, thinner tail to the left – are said to be left skewed. we also say that such a distribution has a long left tail. data sets that show roughly equal trailing off in both directions are called symmetric."
263,1,"['long right tail', 'skewed', 'data', 'distribution', 'long left tail', 'tail', 'long tail']", Examining numerical data,seg_13,"when data trail off in one direction, the distribution has a long tail. if a distribution has a long left tail, it is left skewed. if a distribution has a long right tail, it is right skewed."
264,1,"['histogram', 'dot plots', 'plots', 'data', 'skew']", Examining numerical data,seg_13,take a look at the dot plots in figures 2.3 and 2.4. can you see the skew in the data? is it easier to see the skew in this histogram or the dot plots?6
265,1,"['dot plots', 'plots', 'mean']", Examining numerical data,seg_13,"besides the mean (since it was labeled), what can you see in the dot plots that you cannot see in the histogram?7"
266,1,"['histogram', 'symmetric', 'skewed', 'distribution', 'histograms']", Examining numerical data,seg_13,"in addition to looking at whether a distribution is skewed or symmetric, histograms can be used to identify modes. a mode is represented by a prominent peak in the distribution. there is only one prominent peak in the histogram of loan amount."
267,1,"['observations', 'data', 'sets', 'set', 'data set', 'data sets']", Examining numerical data,seg_13,"a definition of mode sometimes taught in math classes is the value with the most occurrences in the data set. however, for many real-world data sets, it is common to have no observations with the same value in a data set, making this definition impractical in data analysis."
268,1,"['bins', 'unimodal', 'observations', 'multimodal', 'distribution', 'histograms', 'bimodal', 'distributions']", Examining numerical data,seg_13,"figure 2.7 shows histograms that have one, two, or three prominent peaks. such distributions are called unimodal, bimodal, and multimodal, respectively. any distribution with more than 2 prominent peaks is called multimodal. notice that there was one prominent peak in the unimodal distribution with a second less prominent peak that was not counted since it only differs from its neighboring bins by a few observations."
269,1,"['plot', 'unimodal', 'multimodal', 'bimodal', 'distributions']", Examining numerical data,seg_13,"figure 2.7: counting only prominent peaks, the distributions are (left to right) unimodal, bimodal, and multimodal. note that we’ve said the left plot is unimodal intentionally. this is because we are counting prominent peaks, not just any peak."
270,1,"['rate', 'unimodal', 'multimodal', 'distribution', 'bimodal']", Examining numerical data,seg_13,"figure 2.6 reveals only one prominent mode in the interest rate. is the distribution unimodal, bimodal, or multimodal?"
271,0,[], Examining numerical data,seg_13,"unimodal. remember that uni stands for 1 (think unicycles). similarly, bi stands for 2 (think bicycles). we’re hoping a multicycle will be invented to complete this analogy."
272,1,"['data', 'measurements']", Examining numerical data,seg_13,height measurements of young students and adult teachers at a k-3 elementary school were taken. how many modes would you expect in this height data set?8
273,1,"['distribution', 'data']", Examining numerical data,seg_13,"looking for modes isn’t about finding a clear and correct answer about the number of modes in a distribution, which is why prominent is not rigorously defined in this book. the most important part of this examination is to better understand your data."
274,1,"['deviation', 'standard deviation', 'variance', 'standard']", Examining numerical data,seg_13,2.1.4 variance and standard deviation
275,1,"['deviation', 'method', 'variability', 'data', 'observation', 'set', 'data set', 'mean', 'standard', 'standard deviation', 'variance']", Examining numerical data,seg_13,"the mean was introduced as a method to describe the center of a data set, and variability in the data is also important. here, we introduce two measures of variability: the variance and the standard deviation. both of these are very useful in data analysis, even though their formulas are a bit tedious to calculate by hand. the standard deviation is the easier of the two to comprehend, and it roughly describes how far away the typical observation is from the mean."
276,1,"['deviation', 'rate', 'observations', 'observation', 'deviations', 'variable', 'mean']", Examining numerical data,seg_13,"we call the distance of an observation from its mean its deviation. below are the deviations for the 1st, 2nd, 3rd, and 50th observations in the interest rate variable:"
277,1,"['sample', 'deviations', 'variance', 'sample variance', 'average']", Examining numerical data,seg_13,"if we square these deviations and then take an average, the result is equal to the sample variance, denoted by s2:"
278,1,"['statistic', 'variance']", Examining numerical data,seg_13,"we divide by n − 1, rather than dividing by n, when computing a sample’s variance; there’s some mathematical nuance here, but the end result is that doing this makes this statistic slightly more reliable and useful."
279,1,['deviations'], Examining numerical data,seg_13,"notice that squaring the deviations does two things. first, it makes large values relatively much larger, seen by comparing (−0.67)2, (−1.65)2, (14.73)2, and (−5.49)2. second, it gets rid of any negative signs."
280,1,"['deviation', 'standard', 'standard deviation', 'variance']", Examining numerical data,seg_13,the standard deviation is defined as the square root of the variance:
281,1,"['deviation', 'observations', 'standard', 'standard deviation', 'variance']", Examining numerical data,seg_13,"while often omitted, a subscript of x may be added to the variance and standard deviation, i.e. s2x and sx, if it is useful as a reminder that these are the variance and standard deviation of the observations represented by x1, x2, ..., xn."
282,1,"['deviation', 'data', 'mean', 'variance', 'standard', 'standard deviation', 'average']", Examining numerical data,seg_13,the variance is the average squared distance from the mean. the standard deviation is the square root of the variance. the standard deviation is useful when considering how far the data are distributed from the mean.
283,1,"['deviation', 'standard deviations', 'observations', 'data', 'deviations', 'mean', 'standard', 'standard deviation', 'percentages']", Examining numerical data,seg_13,"the standard deviation represents the typical deviation of observations from the mean. usually about 70% of the data will be within one standard deviation of the mean and about 95% will be within two standard deviations. however, as seen in figures 2.8 and 2.9, these percentages are not strict rules."
284,1,"['deviation', 'standard', 'mean', 'population', 'standard deviation', 'variance']", Examining numerical data,seg_13,"like the mean, the population values for variance and standard deviation have special symbols: σ2 for the variance and σ for the standard deviation. the symbol σ is the greek letter sigma."
285,1,['rate'], Examining numerical data,seg_13,"6.5% 11.6% 16.7% 21.8% 26.9% interest rate, x = 11.57%, sx = 5.05%"
286,1,"['deviation', 'rate', 'standard deviations', 'data', 'deviations', 'variable', 'mean', 'standard', 'standard deviation', 'rates']", Examining numerical data,seg_13,"figure 2.8: for the interest rate variable, 34 of the 50 loans (68%) had interest rates within 1 standard deviation of the mean, and 48 of the 50 loans (96%) had rates within 2 standard deviations. usually about 70% of the data are within 1 standard deviation of the mean and 95% within 2 standard deviations, though this is far from a hard rule."
287,1,"['deviation', 'mean', 'population', 'standard', 'standard deviation', 'distributions']", Examining numerical data,seg_13,figure 2.9: three very different population distributions with the same mean µ = 0 and standard deviation σ = 1.
288,1,"['distribution', 'symmetric', 'skewed']", Examining numerical data,seg_13,"on page 45, the concept of shape of a distribution was introduced. a good description of the shape of a distribution should include modality and whether the distribution is symmetric or skewed to one side. using figure 2.9 as an example, explain why such a description is important.9"
289,1,"['rate', 'histogram', 'variability', 'distribution', 'variable', 'cases']", Examining numerical data,seg_13,"describe the distribution of the interest rate variable using the histogram in figure 2.6. the description should incorporate the center, variability, and shape of the distribution, and it should also be placed in context. also note any especially unusual cases."
290,1,"['deviation', 'sample', 'unimodal', 'skewed', 'distribution', 'mean', 'standard', 'standard deviation', 'rates']", Examining numerical data,seg_13,"the distribution of interest rates is unimodal and skewed to the high end. many of the rates fall near the mean at 11.57%, and most fall within one standard deviation (5.05%) of the mean. there are a few exceptionally large interest rates in the sample that are above 20%."
291,1,"['deviation', 'sample', 'sample mean', 'estimate', 'uncertainty', 'statistic', 'sample statistic', 'associated', 'mean', 'standard', 'standard deviation', 'variance']", Examining numerical data,seg_13,"in practice, the variance and standard deviation are sometimes used as a means to an end, where the “end” is being able to accurately estimate the uncertainty associated with a sample statistic. for example, in chapter 5 the standard deviation is used in calculations that help us understand how much a sample mean varies from one sample to the next."
292,1,"['quartiles', 'median', 'plots', 'box plots']", Examining numerical data,seg_13,"2.1.5 box plots, quartiles, and the median"
293,1,"['plot', 'rate', 'observations', 'statistics', 'box plot', 'data', 'variable', 'set', 'data set', 'plotting', 'dot plot']", Examining numerical data,seg_13,a box plot summarizes a data set using five statistics while also plotting unusual observations. figure 2.10 provides a vertical dot plot alongside a box plot of the interest rate variable from the loan50 data set.
294,1,"['outliers', 'whisker', 'median', 'quartile', 'third quartile', 'first quartile']", Examining numerical data,seg_13,suspected outliers 25% max whisker reach upper whisker 20% e tar t es 15% q3 (third quartile) r etni median 10% q1 (first quartile) lower whisker 5%
295,1,"['plot', 'box plot', 'dot plot', 'rates']", Examining numerical data,seg_13,"figure 2.10: a vertical dot plot, where points have been horizontally stacked, next to a labeled box plot for the interest rates of the 50 loans."
296,1,"['plot', 'percentile', 'median', 'observations', 'case', 'box plot', 'data', 'observation', 'set', 'data set', 'number of observations', 'average']", Examining numerical data,seg_13,"the first step in building a box plot is drawing a dark line denoting the median, which splits the data in half. figure 2.10 shows 50% of the data falling below the median and other 50% falling above the median. there are 50 loans in the data set (an even number) so the data are perfectly split into two groups of 25. we take the median in this case to be the average of the two observations closest to the 50th percentile, which happen to be the same value in this data set: (9.93%+9.93%)/2 = 9.93%. when there are an odd number of observations, there will be exactly one observation that splits the data into two halves, and in such a case that observation is the median (no average needed)."
297,1,"['median', 'observations', 'data', 'observation', 'number of observations', 'average']", Examining numerical data,seg_13,"if the data are ordered from smallest to largest, the median is the observation right in the middle. if there are an even number of observations, there will be two values in the middle, and the median is taken as their average."
298,1,"['range', 'first quartile', 'data', 'quartile', 'third quartile', 'standard', 'standard deviation', 'interquartile range', 'percentile', 'plot', 'deviation', 'variability', 'box plot', 'variable']", Examining numerical data,seg_13,"the second step in building a box plot is drawing a rectangle to represent the middle 50% of the data. the total length of the box, shown vertically in figure 2.10, is called the interquartile range (iqr, for short). it, like the standard deviation, is a measure of variability in data. the more variable the data, the larger the standard deviation and iqr tend to be. the two boundaries of the box are called the first quartile (the 25th percentile, i.e. 25% of the data fall below this value) and the third quartile (the 75th percentile), and these are often labeled q1 and q3, respectively."
299,1,"['plot', 'box plot']", Examining numerical data,seg_13,the iqr is the length of the box in a box plot. it is computed as
300,1,['percentiles'], Examining numerical data,seg_13,where q1 and q3 are the 25th and 75th percentiles.
301,1,"['percent', 'data', 'median']", Examining numerical data,seg_13,what percent of the data fall between q1 and the median? what percent is between the median and q3?10
302,1,"['plot', 'whisker', 'whiskers', 'data', 'box plot', 'limit']", Examining numerical data,seg_13,"extending out from the box, the whiskers attempt to capture the data outside of the box. however, their reach is never allowed to be more than 1.5× iqr. they capture everything within this reach. in figure 2.10, the upper whisker does not extend to the last two points, which is beyond q3 + 1.5× iqr, and so it extends only to the last point below this limit. the lower whisker stops at the lowest value, 5.31%, since there is no additional data to reach; the lower whisker’s limit is not shown in the figure because the plot does not extend down to q1−1.5× iqr. in a sense, the box is like the body of the box plot and the whiskers are like its arms trying to reach the rest of the data."
303,1,"['outliers', 'observations', 'whiskers', 'data', 'case', 'observation', 'rates']", Examining numerical data,seg_13,"any observation lying beyond the whiskers is labeled with a dot. the purpose of labeling these points – instead of extending the whiskers to the minimum and maximum observed values – is to help identify any observations that appear to be unusually distant from the rest of the data. unusually distant observations are called outliers. in this case, it would be reasonable to classify the interest rates of 24.85% and 26.30% as outliers since they are numerically distant from most of the data."
304,1,"['outlier', 'data', 'observation']", Examining numerical data,seg_13,an outlier is an observation that appears extreme relative to the rest of the data.
305,1,"['outliers', 'data']", Examining numerical data,seg_13,"examining data for outliers serves many useful purposes, including"
306,1,"['distribution', 'skew']", Examining numerical data,seg_13,1. identifying strong skew in the distribution.
307,1,"['errors', 'data collection', 'data']", Examining numerical data,seg_13,2. identifying possible data collection or data entry errors.
308,1,['data'], Examining numerical data,seg_13,3. providing insight into interesting properties of the data.
309,1,"['rate', 'estimate', 'data', 'set', 'data set']", Examining numerical data,seg_13,"using figure 2.10, estimate the following values for interest rate in the loan50 data set: (a) q1, (b) q3, and (c) iqr.11"
310,1,"['robust statistics', 'statistics']", Examining numerical data,seg_13,2.1.6 robust statistics
311,1,"['sample', 'rate', 'statistics', 'data', 'observation', 'set', 'sample statistics', 'data set']", Examining numerical data,seg_13,"how are the sample statistics of the interest rate data set affected by the observation, 26.3%? what would have happened if this loan had instead been only 15%? what would happen to these summary statistics if the observation at 26.3% had been even larger, say 35%? these scenarios are plotted alongside the original data in figure 2.11, and sample statistics are computed under each scenario in figure 2.12."
312,1,['rate'], Examining numerical data,seg_13,5% 10% 15% 20% 25% 30% 35% interest rate
313,1,"['rate', 'dot plots', 'plots', 'data', 'sets', 'data sets']", Examining numerical data,seg_13,figure 2.11: dot plots of the original interest rate data and two modified data sets.
314,1,"['rate', 'data', 'median']", Examining numerical data,seg_13,robust not robust scenario median iqr x̄ s original interest rate data 9.93% 5.76% 11.57% 5.05% move 26.3% → 15% 9.93% 5.76% 11.34% 4.61% move 26.3% → 35% 9.93% 5.76% 11.74% 5.68%
315,1,"['deviation', 'rate', 'median', 'observations', 'variable', 'mean', 'standard', 'standard deviation']", Examining numerical data,seg_13,"figure 2.12: a comparison of how the median, iqr, mean (x̄), and standard deviation (s) change had an extreme observations from the interest rate variable been different."
316,1,"['deviation', 'median', 'observations', 'mean', 'standard', 'standard deviation']", Examining numerical data,seg_13,"(a) which is more affected by extreme observations, the mean or median? figure 2.12 may be helpful. (b) is the standard deviation or iqr more affected by extreme observations?12"
317,1,"['deviation', 'robust statistics', 'median', 'observations', 'statistics', 'extreme value', 'mean', 'standard', 'standard deviation']", Examining numerical data,seg_13,"the median and iqr are called robust statistics because extreme observations have little effect on their values: moving the most extreme value generally has little influence on these statistics. on the other hand, the mean and standard deviation are more heavily influenced by changes in extreme observations, which can be important in some situations."
318,1,"['case', 'median']", Examining numerical data,seg_13,the median and iqr did not change under the three scenarios in figure 2.12. why might this be the case?
319,1,"['estimates', 'median', 'data', 'sets', 'data sets']", Examining numerical data,seg_13,"the median and iqr are only sensitive to numbers near q1, the median, and q3. since values in these regions are stable in the three data sets, the median and iqr estimates are also stable."
320,1,"['skewed', 'data', 'distribution', 'set', 'data set', 'mean', 'tail']", Examining numerical data,seg_13,"the distribution of loan amounts in the loan50 data set is right skewed, with a few large loans lingering out into the right tail. if you were wanting to understand the typical loan size, should you be more interested in the mean or median?13"
321,1,"['transforming', 'data']", Examining numerical data,seg_13,2.1.7 transforming data (special topic)
322,1,"['model', 'data', 'skewed', 'transform']", Examining numerical data,seg_13,"when data are very strongly skewed, we sometimes transform them so they are easier to model."
323,1,['population'], Examining numerical data,seg_13,3000 1000 2500 y 2000 y c c n n e e 1500 u u 500 q q e e fr 1000 fr 500 0 0 0m 2m 4m 6m 8m 10m 2 3 4 5 6 7 log10(population) population (m = millions)
324,1,"['plot', 'histogram', 'populations', 'county']", Examining numerical data,seg_13,"figure 2.13: (a) a histogram of the populations of all us counties. (b) a histogram of log10-transformed county populations. for this plot, the x-value corresponds to the power of 10, e.g. “4” on the x-axis corresponds to 104 = 10,000."
325,1,"['plot', 'histogram', 'populations', 'county', 'skew']", Examining numerical data,seg_13,"consider the histogram of county populations shown in figure 2.13(a), which shows extreme skew. what isn’t useful about this plot?"
326,1,"['bin', 'skew', 'data']", Examining numerical data,seg_13,"nearly all of the data fall into the left-most bin, and the extreme skew obscures many of the potentially interesting details in the data."
327,1,"['outliers', 'histogram', 'set', 'function', 'transformation', 'skew', 'symmetric', 'results', 'data', 'standard', 'populations', 'statistical', 'skewed', 'transformations', 'data set', 'skewed data', 'plot', 'county']", Examining numerical data,seg_13,"there are some standard transformations that may be useful for strongly right skewed data where much of the data is positive but clustered near zero. a transformation is a rescaling of the data using a function. for instance, a plot of the logarithm (base 10) of county populations results in the new histogram in figure 2.13(b). this data is symmetric, and any potential outliers appear much less extreme than in the original data set. by reigning in the outliers and extreme skew, transformations like this often make it easier to build statistical models against the data."
328,1,"['association', 'variables', 'positive association', 'scatterplot', 'skewed', 'data', 'regression', 'variable', 'population', 'transformation']", Examining numerical data,seg_13,"transformations can also be applied to one or both variables in a scatterplot. a scatterplot of the population change from 2010 to 2017 against the population in 2010 is shown in figure 2.14(a). in this first scatterplot, it’s hard to decipher any interesting patterns because the population variable is so strongly skewed. however, if we apply a log10 transformation to the population variable, as shown in figure 2.14(b), a positive association between the variables is revealed. in fact, we may be interested in fitting a trend line to the data when we explore methods around fitting regression lines in chapter 8."
329,1,"['scatterplot', 'nonlinear', 'data', 'observation', 'transforming', 'skew']", Examining numerical data,seg_13,"transformations other than the logarithm can be useful, too. for instance, the square root 1 (√original observation) and inverse ( original observation ) are commonly used by data scientists. common goals in transforming data are to see the data structure differently, reduce skew, assist in modeling, or straighten a nonlinear relationship in a scatterplot."
330,1,['population'], Examining numerical data,seg_13,−20% −20% 0m 2m 4m 6m 8m 10m 2 3 4 5 6 7 population before change (m = millions) log10(population before change)
331,1,"['scatterplot', 'data', 'population']", Examining numerical data,seg_13,figure 2.14: (a) scatterplot of population change against the population before the change. (b) a scatterplot of the same data but where the population size has been log-transformed.
332,1,['data'], Examining numerical data,seg_13,2.1.8 mapping data (special topic)
333,1,"['plots', 'set', 'numerical', 'rate', 'data', 'intensity map', 'median', 'dot plots', 'data set', 'box plots', 'percent', 'plot', 'intensity', 'variables', 'county', 'variable', 'intensity maps', 'hypotheses']", Examining numerical data,seg_13,"the county data set offers many numerical variables that we could plot using dot plots, scatterplots, or box plots, but these miss the true nature of the data. rather, when we encounter geographic data, we should create an intensity map, where colors are used to show higher and lower values of a variable. figures 2.15 and 2.16 shows intensity maps for poverty rate in percent (poverty), unemployment rate (unemployment rate), homeownership rate in percent (homeownership), and median household income (median hh income). the color key indicates which colors correspond to which values. the intensity maps are not generally very helpful for getting precise values in any given county, but they are very helpful for seeing geographic trends and generating interesting research questions or hypotheses."
334,1,"['intensity', 'rate', 'intensity maps']", Examining numerical data,seg_13,what interesting features are evident in the poverty and unemployment rate intensity maps?
335,1,"['rates', 'locations']", Examining numerical data,seg_13,"poverty rates are evidently higher in a few locations. notably, the deep south shows higher poverty rates, as does much of arizona and new mexico. high poverty rates are evident in the mississippi flood plains a little north of new orleans and also in a large section of kentucky."
336,1,"['rate', 'variables', 'observation', 'rates']", Examining numerical data,seg_13,"the unemployment rate follows similar trends, and we can see correspondence between the two variables. in fact, it makes sense for higher rates of unemployment to be closely related to poverty rates. one observation that stand out when comparing the two maps: the poverty rate is much higher than the unemployment rate, meaning while many people may be working, they are not making enough to break out of poverty."
337,1,"['intensity', 'intensity map', 'median']", Examining numerical data,seg_13,what interesting features are evident in the median hh income intensity map in figure 2.16(b)?14
338,0,[], Examining numerical data,seg_13,etar tn em 4% yolpmenu
339,1,"['rate', 'intensity', 'intensity map', 'percent']", Examining numerical data,seg_13,figure 2.15: (a) intensity map of poverty rate (percent). (b) map of the unemployment rate (percent).
340,0,[], Examining numerical data,seg_13,emocni dloh $47 esuoh naidem
341,1,"['intensity', 'rate', 'intensity map', 'median', 'percent']", Examining numerical data,seg_13,figure 2.16: (a) intensity map of homeownership rate (percent). (b) intensity map of median household income ($1000s).
342,0,[], Examining numerical data,seg_13,0 200 400 600 gestation (days)
343,0,[], Examining numerical data,seg_13,to have an unusually low or high aqi? explain your reasoning. 0 10 20 30 40 50 60 daily aqi
344,1,['mean'], Examining numerical data,seg_13,10 20 30 40 mean work travel (in min)
345,1,"['categorical data', 'sample', 'categorical', 'data', 'information', 'tables', 'set', 'data set']", Considering categorical data,seg_15,"in this section, we will introduce tables and other basic tools for categorical data that are used throughout this book. the loan50 data set represents a sample from a larger loan data set called loans. this larger data set contains information on 10,000 loans made through lending club. we will examine the relationship between homeownership, which for the loans data can take a value of rent, mortgage (owns but has a mortgage), or own, and app type, which indicates whether the loan application was made with a partner or whether it was an individual application."
346,1,"['contingency tables', 'plots', 'tables', 'bar plots']", Considering categorical data,seg_15,2.2.1 contingency tables and bar plots
347,1,"['column totals', 'contingency table', 'set', 'data', 'row totals', 'percentages', 'combination', 'categorical', 'row and column totals', 'data set', 'categories', 'categorical variables', 'outcomes', 'table', 'variables', 'variable']", Considering categorical data,seg_15,"figure 2.17 summarizes two variables: app type and homeownership. a table that summarizes data for two categorical variables in this way is called a contingency table. each value in the table represents the number of times a particular combination of variable outcomes occurred. for example, the value 3496 corresponds to the number of loans in the data set where the borrower rents their home and the application type was by an individual. row and column totals are also included. the row totals provide the total counts across each row (e.g. 3496 + 3839 + 1170 = 8505), and column totals are total counts down each column. we can also create a table that shows only the overall percentages or proportions for each combination of categories, or we can create a table for a single variable, such as the one shown in figure 2.18 for the homeownership variable."
348,1,['joint'], Considering categorical data,seg_15,homeownership rent mortgage own total individual 3496 3839 1170 8505 app type joint 362 950 183 1495
349,1,"['contingency table', 'table']", Considering categorical data,seg_15,figure 2.17: a contingency table for app type and homeownership.
350,0,[], Considering categorical data,seg_15,homeownership count rent 3858 mortgage 4789 own 1353 total 10000
351,1,"['variable', 'frequencies', 'table']", Considering categorical data,seg_15,figure 2.18: a table summarizing the frequencies of each value for the homeownership variable.
352,1,"['plot', 'bar plot', 'categorical', 'observations', 'variable', 'categorical variable', 'level']", Considering categorical data,seg_15,"a bar plot is a common way to display a single categorical variable. the left panel of figure 2.19 shows a bar plot for the homeownership variable. in the right panel, the counts are converted into proportions, showing the proportion of observations that are in each level (e.g. 3858/10000 = 0.3858 for rent)."
353,1,"['bar plots', 'plots']", Considering categorical data,seg_15,"figure 2.19: two bar plots of number. the left panel shows the counts, and the right panel shows the proportions in each group."
354,0,[], Considering categorical data,seg_15,2.2.2 row and column proportions
355,1,"['table', 'contingency table', 'intersection', 'row totals', 'variable', 'row total']", Considering categorical data,seg_15,"sometimes it is useful to understand the fractional breakdown of one variable in another, and we can modify our contingency table to provide such a view. figure 2.20 shows the row proportions for figure 2.17, which are computed as the counts divided by their row totals. the value 3496 at the intersection of individual and rent is replaced by 3496/8505 = 0.411, i.e. 3496 divided by its row total, 8505. so what does 0.411 represent? it corresponds to the proportion of individual applicants who rent."
356,1,['joint'], Considering categorical data,seg_15,rent mortgage own total individual 0.411 0.451 0.138 1.000 joint 0.242 0.635 0.122 1.000 total 0.386 0.479 0.135 1.000
357,1,"['table', 'variables', 'contingency table', 'joint', 'row total', 'error']", Considering categorical data,seg_15,figure 2.20: a contingency table with row proportions for the app type and homeownership variables. the row total is off by 0.001 for the joint row due to a rounding error.
358,1,"['rate', 'levels', 'column total', 'table', 'variables', 'contingency table', 'associated', 'vary', 'rates']", Considering categorical data,seg_15,"a contingency table of the column proportions is computed in a similar way, where each column proportion is computed as the count divided by the corresponding column total. figure 2.21 shows such a table, and here the value 0.906 indicates that 90.6% of renters applied as individuals for the loan. this rate is higher compared to loans from people with mortgages (80.2%) or who own their home (86.5%). because these rates vary between the three levels of homeownership (rent, mortgage, own), this provides evidence that the app type and homeownership variables are associated."
359,1,['joint'], Considering categorical data,seg_15,rent mortgage own total individual 0.906 0.802 0.865 0.851 joint 0.094 0.198 0.135 0.150 total 1.000 1.000 1.000 1.000
360,1,"['table', 'variables', 'contingency table', 'error']", Considering categorical data,seg_15,figure 2.21: a contingency table with column proportions for the app type and homeownership variables. the total for the last column is off by 0.001 due to a rounding error.
361,1,"['joint', 'association']", Considering categorical data,seg_15,"we could also have checked for an association between app type and homeownership in figure 2.20 using row proportions. when comparing these row proportions, we would look down columns to see if the fraction of loans where the borrower rents, has a mortgage, or owns varied across the individual to joint application types."
362,1,"['joint', 'intersection']", Considering categorical data,seg_15,(a) what does 0.122 at the intersection of joint and own represent in figure 2.20? (b) what does 0.135 represent in the figure 2.21?19
363,1,"['table', 'variables', 'contingency table', 'statistics', 'data', 'set', 'data set']", Considering categorical data,seg_15,"data scientists use statistics to filter spam from incoming email messages. by noting specific characteristics of an email, a data scientist may be able to classify some emails as spam or not spam with high accuracy. one such characteristic is whether the email contains no numbers, small numbers, or big numbers. another characteristic is the email format, which indicates whether or not an email has any html content, such as bolded text. we’ll focus on email format and spam status using the email data set, and these variables are summarized in a contingency table in figure 2.22. which would be more helpful to someone hoping to classify email as spam or regular email for this table: row or column proportions?"
364,1,['data'], Considering categorical data,seg_15,a data scientist would be interested in how the proportion of spam changes within each email format. this corresponds to column proportions: the proportion of spam in plain text emails and the proportion of spam in html emails.
365,1,"['confidence', 'information']", Considering categorical data,seg_15,"if we generate the column proportions, we can see that a higher fraction of plain text emails are spam (209/1195 = 17.5%) than compared to html emails (158/2726 = 5.8%). this information on its own is insufficient to classify an email as spam or not spam, as over 80% of plain text emails are not spam. yet, when we carefully combine this information with many other characteristics, we stand a reasonable chance of being able to classify some emails as spam or not spam with confidence."
366,1,"['contingency table', 'table']", Considering categorical data,seg_15,figure 2.22: a contingency table for spam and format.
367,1,['table'], Considering categorical data,seg_15,"example 2.25 points out that row and column proportions are not equivalent. before settling on one form for a table, it is important to consider each to ensure that the most useful table is constructed. however, sometimes it simply isn’t clear which, if either, is more useful."
368,1,['tables'], Considering categorical data,seg_15,look back to tables 2.20 and 2.21. are there any obvious scenarios where one might be more useful than the other?
369,1,"['variables', 'frequencies', 'relative frequencies', 'variable', 'explanatory variable', 'explanatory']", Considering categorical data,seg_15,"none that we thought were obvious! what is distinct about app type and homeownership vs the email example is that these two variables don’t have a clear explanatory-response variable relationship that we might hypothesize (see section 1.2.4 for these terms). usually it is most useful to “condition” on the explanatory variable. for instance, in the email example, the email format was seen as a possible explanatory variable of whether the message was spam, so we would find it more interesting to compute the relative frequencies (proportions) for each email format."
370,1,"['plot', 'bar plot', 'variables']", Considering categorical data,seg_15,2.2.3 using a bar plot with two variables
371,1,"['categorical', 'variables', 'plots', 'information', 'tables', 'bar plots', 'categorical variables']", Considering categorical data,seg_15,contingency tables using row or column proportions are especially useful for examining how two categorical variables are related. stacked bar plots provide a way to visualize the information in these tables.
372,1,"['plot', 'graphical', 'bar plot', 'levels', 'table', 'contingency table', 'information', 'variable']", Considering categorical data,seg_15,"a stacked bar plot is a graphical display of contingency table information. for example, a stacked bar plot representing figure 2.21 is shown in figure 2.23(a), where we have first created a bar plot using the homeownership variable and then divided each group by the levels of app type."
373,1,"['plot', 'bar plot']", Considering categorical data,seg_15,"one related visualization to the stacked bar plot is the side-by-side bar plot, where an example is shown in figure 2.23(b)."
374,1,"['plot', 'bar plot', 'table', 'variables', 'contingency table', 'standardized', 'joint', 'associated', 'level', 'vary']", Considering categorical data,seg_15,"for the last type of bar plot we introduce, the column proportions for the app type and homeownership contingency table have been translated into a standardized stacked bar plot in figure 2.23(c). this type of visualization is helpful in understanding the fraction of individual or joint loan applications for borrowers in each level of homeownership. additionally, since the proportions of joint and individual vary across the groups, we can conclude that the two variables are associated."
375,1,['joint'], Considering categorical data,seg_15,joint joint individual individual 4000 4000 y 3000 y 3000 c c n n e e u u q q e e r 2000 r 2000 f f
376,0,[], Considering categorical data,seg_15,1000 1000 0 0 rent mortgage own rent mortgage own
377,1,['joint'], Considering categorical data,seg_15,1.0 0.8 n 0.6 o it ropor 0.4 p joint 0.2 individual 0.0 rent mortgage own
378,1,"['plot', 'bar plot', 'standardized']", Considering categorical data,seg_15,"figure 2.23: (a) stacked bar plot for homeownership, where the counts have been further broken down by app type. (b) side-by-side bar plot for homeownership and app type. (c) standardized version of the stacked bar plot."
379,1,"['plot', 'bar plot', 'plots', 'standardized', 'bar plots']", Considering categorical data,seg_15,"examine the three bar plots in figure 2.23. when is the stacked, side-by-side, or standardized stacked bar plot the most useful?"
380,1,"['plot', 'bar plot', 'variable', 'explanatory variable', 'response', 'explanatory']", Considering categorical data,seg_15,"the stacked bar plot is most useful when it’s reasonable to assign one variable as the explanatory variable and the other variable as the response, since we are effectively grouping by one variable first and then breaking it down by the others."
381,1,"['plot', 'association', 'variables', 'plots', 'cases', 'variable', 'bar plots', 'response', 'explanatory', 'response variable', 'combinations']", Considering categorical data,seg_15,"side-by-side bar plots are more agnostic in their display about which variable, if any, represents the explanatory and which the response variable. it is also easy to discern the number of cases in of the six different group combinations. however, one downside is that it tends to require more horizontal space; the narrowness of figure 2.23(b) makes the plot feel a bit cramped. additionally, when two groups are of very different sizes, as we see in the own group relative to either of the other two groups, it is difficult to discern if there is an association between the variables."
382,1,"['plot', 'bar plot', 'association', 'observations', 'standardized', 'cases', 'variable']", Considering categorical data,seg_15,"the standardized stacked bar plot is helpful if the primary variable in the stacked bar plot is relatively imbalanced, e.g. the own category has only a third of the observations in the mortgage category, making the simple stacked bar plot less useful for checking for an association. the major downside of the standardized version is that we lose all sense of how many cases each of the bars represents."
383,1,"['plots', 'mosaic plots']", Considering categorical data,seg_15,2.2.4 mosaic plots
384,1,"['plot', 'bar plot', 'contingency tables', 'mosaic plot', 'standardized', 'variable', 'tables']", Considering categorical data,seg_15,a mosaic plot is a visualization technique suitable for contingency tables that resembles a standardized stacked bar plot with the benefit that we still see the relative group sizes of the primary variable as well.
385,1,"['plot', 'plots', 'mosaic plot', 'cases', 'variable', 'mosaic plots', 'level', 'categories']", Considering categorical data,seg_15,"to get started in creating our first mosaic plot, we’ll break a square into columns for each category of the homeownership variable, with the result shown in figure 2.24(a). each column represents a level of homeownership, and the column widths correspond to the proportion of loans in each of those categories. for instance, there are fewer loans where the borrower is an owner than where the borrower has a mortgage. in general, mosaic plots use box areas to represent the number of cases in each category."
386,1,"['plot', 'mosaic plot']", Considering categorical data,seg_15,figure 2.24: (a) the one-variable mosaic plot for homeownership. (b) two-variable mosaic plot for both homeownership and app type.
387,1,"['plot', 'variables', 'mosaic plot', 'variable', 'joint', 'jointly', 'associated']", Considering categorical data,seg_15,"to create a completed mosaic plot, the single-variable mosaic plot is further divided into pieces in figure 2.24(b) using the app type variable. each column is split proportional to the number of loans from individual and joint borrowers. for example, the second column represents loans where the borrower has a mortgage, and it was divided into individual loans (upper) and joint loans (lower). as another example, the bottom segment of the third column represents loans where the borrower owns their home and applied jointly, while the upper segment of this column represents borrowers who are homeowners and filed individually. we can again use this plot to see that the homeownership and app type variables are associated, since some columns are divided in different"
388,1,"['plot', 'bar plot', 'association', 'standardized', 'locations']", Considering categorical data,seg_15,"vertical locations than others, which was the same technique used for checking an association in the standardized stacked bar plot."
389,1,"['plot', 'variables', 'plots', 'mosaic plot', 'variable', 'explanatory variable', 'bar plots', 'response', 'explanatory', 'level']", Considering categorical data,seg_15,"in figure 2.24, we chose to first split by the homeowner status of the borrower. however, we could have instead first split by the application type, as in figure 2.25. like with the bar plots, it’s common to use the explanatory variable to represent the first split in a mosaic plot, and then for the response to break up each level of the explanatory variable, if these labels are reasonable to attach to the variables under consideration."
390,1,"['plot', 'mosaic plot', 'variable', 'joint']", Considering categorical data,seg_15,figure 2.25: mosaic plot where loans are grouped by the homeownership variable after they’ve been divided into the individual and joint application types.
391,1,"['chart', 'pie chart']", Considering categorical data,seg_15,2.2.5 the only pie chart you will see in this book
392,1,"['plot', 'chart', 'pie chart', 'bar plot', 'plots', 'information', 'cases', 'set', 'bar plots', 'pie charts', 'charts']", Considering categorical data,seg_15,"a pie chart is shown in figure 2.26 alongside a bar plot representing the same information. pie charts can be useful for giving a high-level overview to show how a set of cases break down. however, it is also difficult to decipher details in a pie chart. for example, it takes a couple seconds longer to recognize that there are more loans where the borrower has a mortgage than rent when looking at the pie chart, while this detail is very obvious in the bar plot. while pie charts can be useful, we prefer bar plots for their ease in comparing groups."
393,0,[], Considering categorical data,seg_15,4000 rent 3000 yc neuq re 2000 f own 1000 mortgage 0 rent mortgage own homeownership
394,1,"['plot', 'chart', 'pie chart', 'bar plot']", Considering categorical data,seg_15,figure 2.26: a pie chart and bar plot of homeownership.
395,1,"['numerical', 'data']", Considering categorical data,seg_15,2.2.6 comparing numerical data across groups
396,1,"['plot', 'hollow histograms', 'plots', 'data', 'box plots', 'histograms', 'numerical']", Considering categorical data,seg_15,some of the more interesting investigations can be considered by examining numerical data across groups. the methods required here aren’t really new: all that’s required is to make a numerical plot for each group in the same graph. here two convenient methods are introduced: side-by-side box plots and hollow histograms.
397,1,"['median', 'county', 'data', 'set', 'data set', 'population', 'observational data']", Considering categorical data,seg_15,"we will take a look again at the county data set and compare the median household income for counties that gained population from 2010 to 2017 versus counties that had no gain. while we might like to make a causal connection here, remember that these are observational data and so such an interpretation would be, at best, half-baked."
398,1,"['sample', 'random', 'median', 'data', 'loss', 'population', 'random sample']", Considering categorical data,seg_15,"there were 1,454 counties where the population increased from 2010 to 2017, and there were 1,672 counties with no gain (all but one were a loss). a random sample of 100 counties from the first group and 50 from the second group are shown in figure 2.27 to give a better sense of some of the raw median income data."
399,0,[], Considering categorical data,seg_15,"median income for 150 counties, in $1000s"
400,1,['population'], Considering categorical data,seg_15,population gain no population gain 38.2 43.6 42.2 61.5 51.1 45.7 48.3 60.3 50.7 44.6 51.8 40.7 48.1 56.4 41.9 39.3 40.4 40.3 40.6 63.3 52.1 60.3 49.8 51.7 57 47.2 45.9 51.1 34.1 45.5 52.8 49.1 51 42.3 41.5 46.1 80.8 46.3 82.2 43.6 39.7 49.4 44.9 51.7 46.4 75.2 40.6 46.3 62.4 44.1 51.3 29.1 51.8 50.5 51.9 34.7 54 42.9 52.2 45.1 27 30.9 34.9 61 51.4 56.5 62 46 46.4 40.7 51.8 61.1 53.8 57.6 69.2 48.4 40.5 48.6 43.4 34.7 45.7 53.1 54.6 55 46.4 39.9 56.7 33.1 21 37 63 49.1 57.2 44.1 50 38.9 52 31.9 45.7 46.6 46.5 38.9 50.9 56 34.6 56.3 38.7 45.7 74.2 63 49.6 53.7 77.5 60 56.2 43 21.7 63.2 47.6 55.9 39.1 57.8 42.6 44.5 34.5 48.9 50.4 49 45.6 39 38.8 37.1 50.9 42.1 43.2 57.2 44.7 71.7 35.3 100.2 35.4 41.3 33.6 42.6 55.5 38.6 52.7 63 43.4 56.5
401,1,"['sample', 'random', 'median', 'table', 'population', 'random sample']", Considering categorical data,seg_15,"figure 2.27: in this table, median household income (in $1000s) from a random sample of 100 counties that had population gains are shown on the left. median incomes from a random sample of 50 counties that had no population gain are shown on the right."
402,1,"['population', 'median']", Considering categorical data,seg_15,$20k gain no gain $20k $40k $60k $80k $100k change in population median household income
403,1,"['plot', 'hollow histograms', 'box plot', 'loss', 'population', 'histograms']", Considering categorical data,seg_15,"figure 2.28: side-by-side box plot (left panel) and hollow histograms (right panel) for med hh income, where the counties are split by whether there was a population gain or loss."
404,1,"['plot', 'plots', 'box plot', 'box plots', 'plotting']", Considering categorical data,seg_15,"the side-by-side box plot is a traditional tool for comparing across groups. an example is shown in the left panel of figure 2.28, where there are two box plots, one for each group, placed into one plotting window and drawn on the same scale."
405,1,"['plot', 'hollow histograms', 'method', 'data', 'histograms', 'plotting', 'numerical']", Considering categorical data,seg_15,"another useful plotting method uses hollow histograms to compare numerical data across groups. these are just the outlines of histograms of each group put on the same plot, as shown in the right panel of figure 2.28."
406,1,"['plots', 'variability']", Considering categorical data,seg_15,use the plots in figure 2.28 to compare the incomes for counties across the two groups. what do you notice about the approximate center of each group? what do you notice about the variability between groups? is the shape relatively consistent between groups? how many prominent modes are there for each group?20
407,1,['plot'], Considering categorical data,seg_15,what components of each plot in figure 2.28 do you find most useful?21
408,0,[], Considering categorical data,seg_15,prematurity trauma cardiovascular neuromuscular respiratory respiratory genetic/metabolic
409,1,"['frequency', 'relative frequency']", Considering categorical data,seg_15,genetic/metabolic immunocompromised cardiovascular gastrointestinal 0.00 0.10 0.20 0.30 prematurity relative frequency
410,0,[], Considering categorical data,seg_15,conservative moderate liberal support not support not sure
411,0,[], Considering categorical data,seg_15,democrat republican indep / other raise taxes on the rich raise taxes on the poor not sure
412,0,[], Case study malaria vaccine,seg_17,"suppose your professor splits the students in class into two groups: students on the left and students on the right. if p̂l and p̂r represent the proportion of students who own an apple product on the left and right, respectively, would you be surprised if p̂l did not exactly equal p̂r?"
413,0,[], Case study malaria vaccine,seg_17,"while the proportions would probably be close to each other, it would be unusual for them to be exactly the same. we would probably observe a small difference due to chance."
414,0,[], Case study malaria vaccine,seg_17,"if we don’t think the side of the room a person sits on in class is related to whether the person owns an apple product, what assumption are we making about the relationship between these two variables?25"
415,1,"['data', 'variability']", Case study malaria vaccine,seg_17,2.3.1 variability within data
416,1,"['experimental', 'experiment', 'treatment', 'results', 'control group', 'control']", Case study malaria vaccine,seg_17,"we consider a study on a new malaria vaccine called pfspz. in this study, volunteer patients were randomized into one of two experiment groups: 14 patients received an experimental vaccine or 6 patients received a placebo vaccine. nineteen weeks later, all 20 patients were exposed to a drug-sensitive malaria virus strain; the motivation of using a drug-sensitive strain of virus here is for ethical considerations, allowing any infections to be treated effectively. the results are summarized in figure 2.29, where 9 of the 14 treatment patients remained free of signs of infection while all of the 6 patients in the control group patients showed some baseline signs of infection."
417,1,['treatment'], Case study malaria vaccine,seg_17,outcome infection no infection total vaccine 5 9 14 treatment placebo 6 0 6
418,1,"['results', 'experiment']", Case study malaria vaccine,seg_17,figure 2.29: summary results for the malaria vaccine experiment.
419,1,"['experiment', 'observational study']", Case study malaria vaccine,seg_17,is this an observational study or an experiment? what implications does the study type have on what can be inferred from the results?26
420,1,['sample'], Case study malaria vaccine,seg_17,"in this study, a smaller proportion of patients who received the vaccine showed signs of an infection (35.7% versus 100%). however, the sample is very small, and it is unclear whether the difference provides convincing evidence that the vaccine is effective."
421,1,"['data', 'rates']", Case study malaria vaccine,seg_17,"data scientists are sometimes called upon to evaluate the strength of evidence. when looking at the rates of infection for patients in the two groups in this study, what comes to mind as we try to determine whether the data show convincing evidence of a real difference?"
422,1,"['sample', 'independent', 'treatment group', 'treatment', 'data', 'control group', 'samples', 'random', 'control', 'rates']", Case study malaria vaccine,seg_17,"the observed infection rates (35.7% for the treatment group versus 100% for the control group) suggest the vaccine may be effective. however, we cannot be sure if the observed difference represents the vaccine’s efficacy or is just from random chance. generally there is a little bit of fluctuation in sample data, and we wouldn’t expect the sample proportions to be exactly equal, even if the truth was that the infection rates were independent of getting the vaccine. additionally, with such small samples, perhaps it’s common to observe such large differences when we randomly split a group due to chance alone!"
423,1,"['sample size', 'sample', 'random noise', 'variables', 'data', 'random', 'outcomes', 'rates']", Case study malaria vaccine,seg_17,"example 2.33 is a reminder that the observed outcomes in the data sample may not perfectly reflect the true relationships between variables since there is random noise. while the observed difference in rates of infection is large, the sample size for the study is small, making it unclear if this observed difference represents efficacy of the vaccine or whether it is simply due to chance. we label these two competing claims, h0 and ha, which are spoken as “h-nought” and “h-a”:"
424,1,"['model', 'independent', 'variables', 'treatment', 'independence', 'outcome']", Case study malaria vaccine,seg_17,h0: independence model. the variables treatment and outcome are independent. they have
425,0,[], Case study malaria vaccine,seg_17,"no relationship, and the observed difference between the proportion of patients who developed an infection in the two groups, 64.3%, was due to chance."
426,1,"['model', 'independent', 'variables', 'rates']", Case study malaria vaccine,seg_17,ha: alternative model. the variables are not independent. the difference in infection rates of
427,1,['rate'], Case study malaria vaccine,seg_17,"64.3% was not due to chance, and vaccine affected the rate of infection."
428,1,"['rate', 'model', 'independence', 'mean', 'rates']", Case study malaria vaccine,seg_17,"what would it mean if the independence model, which says the vaccine had no influence on the rate of infection, is true? it would mean 11 patients were going to develop an infection no matter which group they were randomized into, and 9 patients would not develop an infection no matter which group they were randomized into. that is, if the vaccine did not affect the rate of infection, the difference in the infection rates was due to chance alone in how the patients were randomized."
429,1,"['model', 'rates']", Case study malaria vaccine,seg_17,"now consider the alternative model: infection rates were influenced by whether a patient received the vaccine or not. if this was true, and especially if this influence was substantial, we would expect to see some difference in the infection rates of patients in the groups."
430,1,"['model', 'case', 'data', 'independence']", Case study malaria vaccine,seg_17,"we choose between these two competing claims by assessing if the data conflict so much with h0 that the independence model cannot be deemed reasonable. if this is the case, and the data support ha, then we will reject the notion of independence and conclude the vaccine was effective."
431,1,['simulations'], Case study malaria vaccine,seg_17,"we’re going to implement simulations, where we will pretend we know that the malaria vaccine being tested does not work. ultimately, we want to understand if the large difference we observed is common in these simulations. if it is common, then maybe the difference we observed was purely due to chance. if it is very uncommon, then the possibility that the vaccine was helpful seems more plausible."
432,1,"['independent', 'simulation', 'randomization']", Case study malaria vaccine,seg_17,"figure 2.29 shows that 11 patients developed infections and 9 did not. for our simulation, we will suppose the infections were independent of the vaccine and we were able to rewind back to when the researchers randomized the patients in the study. if we happened to randomize the patients differently, we may get a different result in this hypothetical world where the vaccine doesn’t influence the infection. let’s complete another randomization using a simulation."
433,1,"['control groups', 'simulation', 'treatment', 'results', 'treatment and control groups', 'control']", Case study malaria vaccine,seg_17,"in this simulation, we take 20 notecards to represent the 20 patients, where we write down “infection” on 11 cards and “no infection” on 9 cards. in this hypothetical world, we believe each patient that got an infection was going to get it regardless of which group they were in, so let’s see what happens if we randomly assign the patients to the treatment and control groups again. we thoroughly shuffle the notecards and deal 14 into a vaccine pile and 6 into a placebo pile. finally, we tabulate the results, which are shown in figure 2.30."
434,1,"['treatment', 'simulated']", Case study malaria vaccine,seg_17,outcome infection no infection total treatment vaccine 7 7 14 (simulated) placebo 4 2 6 total 11 9 20
435,1,"['results', 'simulation', 'rates']", Case study malaria vaccine,seg_17,"figure 2.30: simulation results, where any difference in infection rates is purely due to chance."
436,1,"['simulated', 'rates']", Case study malaria vaccine,seg_17,what is the difference in infection rates between the two simulated groups in figure 2.30? how does this compare to the observed 64.3% difference in the actual data?27
437,1,['independence'], Case study malaria vaccine,seg_17,2.3.3 checking for independence
438,1,"['model', 'simulation', 'efficient', 'independence']", Case study malaria vaccine,seg_17,"we computed one possible difference under the independence model in guided practice 2.34, which represents one difference due to chance. while in this first simulation, we physically dealt out notecards to represent the patients, it is more efficient to perform this simulation using a computer. repeating the simulation on a computer, we get another difference due to chance:"
439,1,"['plot', 'rate', 'simulation', 'treatment', 'distribution', 'simulations', 'simulated', 'control', 'rates']", Case study malaria vaccine,seg_17,"and so on until we repeat the simulation enough times that we have a good idea of what represents the distribution of differences from chance alone. figure 2.31 shows a stacked plot of the differences found from 100 simulations, where each dot represents a simulated difference between the infection rates (control rate minus treatment rate)."
440,1,"['sample', 'model', 'condition', 'case', 'distribution', 'independence', 'simulated', 'random']", Case study malaria vaccine,seg_17,"note that the distribution of these simulated differences is centered around 0. we simulated these differences assuming that the independence model was true, and under this condition, we expect the difference to be near zero with some random fluctuation, where near is pretty generous in this case since the sample sizes are so small in this study."
441,0,[], Case study malaria vaccine,seg_17,"how often would you observe a difference of at least 64.3% (0.643) according to figure 2.31? often, sometimes, rarely, or never?"
442,1,"['probability', 'event']", Case study malaria vaccine,seg_17,it appears that a difference of at least 64.3% due to chance alone would only happen about 2% of the time according to figure 2.31. such a low probability indicates a rare event.
443,1,['rates'], Case study malaria vaccine,seg_17,−0.6 −0.4 −0.2 0.0 0.2 0.4 0.6 0.8 difference in infection rates
444,1,"['plot', 'model', 'independence', 'simulations', 'dot plot']", Case study malaria vaccine,seg_17,"figure 2.31: a stacked dot plot of differences from 100 simulations produced under the independence model, h0, where in these simulations infections are unaffected by the vaccine. two of the 100 simulations had a difference of at least 64.3%, the difference observed in the study."
445,1,"['results', 'event']", Case study malaria vaccine,seg_17,the difference of 64.3% being a rare event suggests two possible interpretations of the results of the study:
446,1,"['rate', 'model', 'independence']", Case study malaria vaccine,seg_17,"h0 independence model. the vaccine has no effect on infection rate, and we just happened to"
447,0,[], Case study malaria vaccine,seg_17,observe a difference that would only occur on a rare occasion.
448,1,"['rate', 'model']", Case study malaria vaccine,seg_17,"ha alternative model. the vaccine has an effect on infection rate, and the difference we"
449,0,[], Case study malaria vaccine,seg_17,"observed was actually due to the vaccine being effective at combatting malaria, which explains the large difference of 64.3%."
450,1,"['model', 'results', 'case', 'data', 'independence', 'simulations']", Case study malaria vaccine,seg_17,"based on the simulations, we have two options. (1) we conclude that the study results do not provide strong evidence against the independence model. that is, we do not have sufficiently strong evidence to conclude the vaccine had an effect in this clinical setting. (2) we conclude the evidence is sufficiently strong to reject h0 and assert that the vaccine was useful. when we conduct formal studies, usually we reject the notion that we just happened to observe a rare event.28 so in this case, we reject the independence model in favor of the alternative. that is, we are concluding the data provide strong evidence that the vaccine provides some protection against malaria in this clinical setting."
451,1,"['model', 'statistical inference', 'evaluating', 'statistics', 'data', 'errors', 'model selection', 'events', 'probability', 'control', 'statistical']", Case study malaria vaccine,seg_17,"one field of statistics, statistical inference, is built on evaluating whether such differences are due to chance. in statistical inference, data scientists evaluate which model is most reasonable given the data. errors do occur, just like rare events, and we might choose the wrong model. while we do not always choose correctly, statistical inference gives us tools to control and evaluate how often these errors occur. in chapter 5, we give a formal introduction to the problem of model selection. we spend the next two chapters building a foundation of probability and theory necessary to make that discussion rigorous."
452,1,['events'], Case study malaria vaccine,seg_17,simulated rosiglitazone cardiovascular events
453,1,['treatment'], Case study malaria vaccine,seg_17,control treatment alive 1500
454,1,"['treatment', 'control']", Case study malaria vaccine,seg_17,0 control treatment
455,0,[], Case study malaria vaccine,seg_17,simulated differences in proportions
456,0,[], Case study malaria vaccine,seg_17,ra 0 f 0 20 40 60 80 100 120 infant mortality (per 1000 live births)
457,0,[], Case study malaria vaccine,seg_17,best actor 50 40
458,1,"['statistics', 'conditional', 'probability', 'random', 'population', 'distributions', 'continuous', 'random variables', 'variables', 'sampling', 'conditional probability', 'continuous distributions']",Chapter  Probability,seg_19,"3.1 defining probability 3.2 conditional probability 3.3 sampling from a small population 3.4 random variables 3.5 continuous distributions probability forms the foundation of statistics, and you’re probably already aware of many of the ideas presented in this chapter. however, formalization of probability concepts is likely new for most readers. while this chapter provides a theoretical foundation for the ideas in later chapters and provides a path to a deeper understanding, mastery of the concepts introduced in this chapter is not required for applying the methods introduced in the rest of this book. for videos, slides, and other resources, please visit www.openintro.org/os"
459,1,"['set', 'probability']", Defining probability,seg_21,"statistics is based on probability, and while probability is not required for the applied techniques in this book, it may help you gain a deeper understanding of the methods and set a better foundation for future courses."
460,0,[], Defining probability,seg_21,3.1.1 introductory examples
461,0,[], Defining probability,seg_21,"before we get into technical ideas, let’s walk through some basic examples that may feel more familiar."
462,0,[], Defining probability,seg_21,"a “die”, the singular of dice, is a cube with six faces numbered 1, 2, 3, 4, 5, and 6. what is the chance of getting 1 when rolling a die?"
463,1,['outcomes'], Defining probability,seg_21,"if the die is fair, then the chance of a 1 is as good as the chance of any other number. since there are six outcomes, the chance must be 1-in-6 or, equivalently, 1/6."
464,0,[], Defining probability,seg_21,what is the chance of getting a 1 or 2 in the next roll?
465,1,['outcomes'], Defining probability,seg_21,"1 and 2 constitute two of the six equally likely possible outcomes, so the chance of getting one of these two outcomes must be 2/6 = 1/3."
466,1,['outcome'], Defining probability,seg_21,100%. the outcome must be one of these numbers.
467,0,[], Defining probability,seg_21,what is the chance of not rolling a 2?
468,1,"['outcomes', 'probability']", Defining probability,seg_21,"alternatively, we could have noticed that not rolling a 2 is the same as getting a 1, 3, 4, 5, or 6, which makes up five of the six equally likely outcomes and has probability 5/6."
469,0,[], Defining probability,seg_21,"consider rolling two dice. if 1/6 of the time the first die is a 1 and 1/6 of those times the second die is a 1, what is the chance of getting two 1s?"
470,1,['probability'], Defining probability,seg_21,3.1.2 probability
471,1,"['random', 'probability', 'random process', 'outcome', 'process']", Defining probability,seg_21,we use probability to build tools to describe and understand apparent randomness. we often frame probability in terms of a random process giving rise to an outcome.
472,1,"['random', 'random process', 'outcome', 'process']", Defining probability,seg_21,rolling a die or flipping a coin is a seemingly random process and each gives rise to an outcome.
473,1,"['random', 'probability', 'random process', 'outcome', 'process']", Defining probability,seg_21,the probability of an outcome is the proportion of times the outcome would occur if we observed the random process an infinite number of times.
474,1,['percentage'], Defining probability,seg_21,"probability is defined as a proportion, and it always takes values between 0 and 1 (inclusively). it may also be displayed as a percentage between 0% and 100%."
475,1,"['convergence', 'law of large numbers', 'probability', 'outcomes']", Defining probability,seg_21,"probability can be illustrated by rolling a die many times. let p̂n be the proportion of outcomes that are 1 after the first n rolls. as the number of rolls increases, p̂n will converge to the probability of rolling a 1, p = 1/6. figure 3.1 shows this convergence for 100,000 die rolls. the tendency of p̂n to stabilize around p is described by the law of large numbers."
476,1,"['simulation', 'probability']", Defining probability,seg_21,figure 3.1: the fraction of die rolls that are 1 at each stage in a simulation. the proportion tends to get closer to the probability 1/6 ≈ 0.167 as the number of rolls increases.
477,1,"['observations', 'probability', 'outcome']", Defining probability,seg_21,"as more observations are collected, the proportion p̂n of occurrences with a particular outcome converges to the probability p of that outcome."
478,1,"['deviations', 'law of large numbers', 'probability']", Defining probability,seg_21,"occasionally the proportion will veer off from the probability and appear to defy the law of large numbers, as p̂n does many times in figure 3.1. however, these deviations become smaller as the number of rolls increases."
479,1,['probability'], Defining probability,seg_21,above we write p as the probability of rolling a 1. we can also write this probability as
480,1,['process'], Defining probability,seg_21,"as we become more comfortable with this notation, we will abbreviate it further. for instance, if it is clear that the process is “rolling a die”, we could abbreviate p (rolling a 1) as p (1)."
481,1,"['random', 'random process', 'process', 'outcomes', 'processes']", Defining probability,seg_21,"random processes include rolling a die and flipping a coin. (a) think of another random process. (b) describe all the possible outcomes of that process. for instance, rolling a die is a random process with possible outcomes 1, 2, ..., 6.1"
482,1,"['random processes', 'random', 'random process', 'process', 'processes']", Defining probability,seg_21,"what we think of as random processes are not necessarily random, but they may just be too difficult to understand exactly. the fourth example in the footnote solution to guided practice 3.6 suggests a roommate’s behavior is a random process. however, even if a roommate’s behavior is not truly random, modeling her behavior as a random process can still be useful."
483,1,"['disjoint', 'mutually exclusive', 'outcomes']", Defining probability,seg_21,3.1.3 disjoint or mutually exclusive outcomes
484,1,"['disjoint', 'mutually exclusive', 'outcome', 'outcomes']", Defining probability,seg_21,"two outcomes are called disjoint or mutually exclusive if they cannot both happen. for instance, if we roll a die, the outcomes 1 and 2 are disjoint since they cannot both occur. on the other hand, the outcomes 1 and “rolling an odd number” are not disjoint since both occur if the outcome of the roll is a 1. the terms disjoint and mutually exclusive are equivalent and interchangeable."
485,1,"['disjoint', 'probabilities', 'probability', 'outcomes']", Defining probability,seg_21,"calculating the probability of disjoint outcomes is easy. when rolling a die, the outcomes 1 and 2 are disjoint, and we compute the probability that one of these outcomes will occur by adding their separate probabilities:"
486,1,"['disjoint', 'probabilities', 'probability', 'outcomes']", Defining probability,seg_21,"what about the probability of rolling a 1, 2, 3, 4, 5, or 6? here again, all of the outcomes are disjoint so we add the probabilities:"
487,1,"['disjoint', 'outcomes', 'addition rule']", Defining probability,seg_21,the addition rule guarantees the accuracy of this approach when the outcomes are disjoint.
488,1,"['disjoint', 'outcomes', 'probability']", Defining probability,seg_21,"if a1 and a2 represent two disjoint outcomes, then the probability that one of them occurs is given by"
489,1,"['disjoint', 'outcomes', 'probability']", Defining probability,seg_21,"if there are many disjoint outcomes a1, ..., ak, then the probability that one of these outcomes will occur is"
490,1,"['disjoint', 'addition rule', 'probability', 'outcomes']", Defining probability,seg_21,"we are interested in the probability of rolling a 1, 4, or 5. (a) explain why the outcomes 1, 4, and 5 are disjoint. (b) apply the addition rule for disjoint outcomes to determine p (1 or 4 or 5).2"
491,1,"['data', 'variable', 'set', 'data set']", Defining probability,seg_21,"in the loans data set in chapter 2, the homeownership variable described whether the borrower rents, has a mortgage, or owns her property. of the 10,000 borrowers, 3858 rented, 4789 had a mortgage, and 1353 owned their home.3"
492,1,"['disjoint', 'outcomes']", Defining probability,seg_21,"(a) are the outcomes rent, mortgage, and own disjoint?"
493,0,[], Defining probability,seg_21,(b) determine the proportion of loans with value mortgage and own separately.
494,1,"['disjoint', 'addition rule', 'probability', 'outcomes']", Defining probability,seg_21,(c) use the addition rule for disjoint outcomes to compute the probability a randomly selected
495,1,"['set', 'data set', 'data']", Defining probability,seg_21,loan from the data set is for someone who has a mortgage or owns her home.
496,1,"['disjoint', 'results', 'disjoint events', 'sets', 'events', 'set', 'event', 'outcomes']", Defining probability,seg_21,"data scientists rarely work with individual outcomes and instead consider sets or collections of outcomes. let a represent the event where a die roll results in 1 or 2 and b represent the event that the die roll is a 4 or a 6. we write a as the set of outcomes {1, 2} and b = {4, 6}. these sets are commonly called events. because a and b have no elements in common, they are disjoint events. a and b are represented in figure 3.2."
497,1,"['disjoint', 'events', 'outcomes']", Defining probability,seg_21,"figure 3.2: three events, a, b, and d, consist of outcomes from rolling a die. a and b are disjoint since they do not have any outcomes in common."
498,1,"['disjoint', 'probabilities', 'addition rule', 'disjoint events', 'events', 'probability', 'outcomes']", Defining probability,seg_21,the addition rule applies to both disjoint outcomes and disjoint events. the probability that one of the disjoint events a or b occurs is the sum of the separate probabilities:
499,1,"['addition rule', 'probability of event', 'probability', 'event']", Defining probability,seg_21,"(a) verify the probability of event a, p (a), is 1/3 using the addition rule. (b) do the same for event b.4"
500,1,"['disjoint', 'events', 'event', 'outcomes']", Defining probability,seg_21,"(a) using figure 3.2 as a reference, what outcomes are represented by event d? (b) are events b and d disjoint? (c) are events a and d disjoint?5"
501,1,"['disjoint', 'event']", Defining probability,seg_21,"in guided practice 3.10, you confirmed b and d from figure 3.2 are disjoint. compute the probability that event b or event d occurs.6"
502,1,"['disjoint', 'events', 'probabilities']", Defining probability,seg_21,3.1.4 probabilities when events are not disjoint
503,1,"['events', 'disjoint']", Defining probability,seg_21,"let’s consider calculations for two events that are not disjoint in the context of a regular deck of 52 cards, represented in figure 3.3. if you are unfamiliar with the cards in a regular deck, please"
504,0,[], Defining probability,seg_21,7 see the footnote.
505,1,['representations'], Defining probability,seg_21,figure 3.3: representations of the 52 unique cards in a deck.
506,1,['probability'], Defining probability,seg_21,(a) what is the probability that a randomly selected card is a diamond? (b) what is the probability that a randomly selected card is a face card?8
507,1,"['random processes', 'probabilities', 'variables', 'intersection', 'venn diagram', 'venn', 'random', 'outcomes', 'processes']", Defining probability,seg_21,"venn diagrams are useful when outcomes can be categorized as “in” or “out” for two or three variables, attributes, or random processes. the venn diagram in figure 3.4 uses a circle to represent diamonds and another to represent face cards. if a card is both a diamond and a face card, it falls into the intersection of the circles. if it is a diamond but not a face card, it will be in part of the left circle that is not in the right circle (and so on). the total number of cards that are diamonds is given by the total number of cards in the diamonds circle: 10 + 3 = 13. the probabilities are also shown (e.g. 10/52 = 0.1923)."
508,0,[], Defining probability,seg_21,there are also 30 cards that are 10 3 9 neither diamonds
509,1,"['venn diagram', 'venn']", Defining probability,seg_21,figure 3.4: a venn diagram for diamonds and face cards.
510,1,"['disjoint', 'probabilities', 'addition rule', 'disjoint events', 'events', 'venn diagram', 'venn', 'categories', 'event']", Defining probability,seg_21,"let a represent the event that a randomly selected card is a diamond and b represent the event that it is a face card. how do we compute p (a or b)? events a and b are not disjoint – the cards j♦, q♦, and k♦ fall into both categories – so we cannot use the addition rule for disjoint events. instead we use the venn diagram. we start by adding the probabilities of the two events:"
511,1,"['events', 'probability']", Defining probability,seg_21,"however, the three cards that are in both events were counted twice, once in each probability. we must correct this double counting:"
512,1,"['general addition rule', 'addition rule']", Defining probability,seg_21,this equation is an example of the general addition rule.
513,1,"['events', 'disjoint', 'probability']", Defining probability,seg_21,"if a and b are any two events, disjoint or not, then the probability that at least one of them will occur is"
514,1,"['events', 'probability']", Defining probability,seg_21,where p (a and b) is the probability that both events occur.
515,1,"['statistics', 'mean']", Defining probability,seg_21,"when we write “or” in statistics, we mean “and/or” unless we explicitly state otherwise. thus, a or b occurs means a, b, or both a and b occur."
516,1,"['disjoint', 'general addition rule', 'addition rule', 'disjoint events', 'events']", Defining probability,seg_21,"(a) if a and b are disjoint, describe why this implies p (a and b) = 0. (b) using part (a), verify that the general addition rule simplifies to the simpler addition rule for disjoint events if a and"
517,1,['disjoint'], Defining probability,seg_21,9 b are disjoint.
518,1,"['data', 'set', 'data set', 'joint']", Defining probability,seg_21,"in the loans data set describing 10,000 loans, 1495 loans were from joint applications (e.g. a couple applied together), 4789 applicants had a mortgage, and 950 had both of these characteristics. create"
519,1,"['venn diagram', 'venn']", Defining probability,seg_21,10 a venn diagram for this setup.
520,1,"['data', 'set', 'joint', 'venn diagram', 'data set', 'venn', 'probability']", Defining probability,seg_21,(a) use your venn diagram from guided practice 3.14 to determine the probability a randomly drawn loan from the loans data set is from a joint application where the couple had a mortgage. (b) what is the probability that the loan had either of these attributes?11
521,1,"['probability distributions', 'probability', 'distributions']", Defining probability,seg_21,3.1.5 probability distributions
522,1,"['disjoint', 'table', 'distribution', 'probability distribution', 'probability', 'associated', 'outcomes']", Defining probability,seg_21,a probability distribution is a table of all disjoint outcomes and their associated probabilities. figure 3.5 shows the probability distribution for the sum of two dice.
523,1,"['distribution', 'probability distribution', 'probability']", Defining probability,seg_21,figure 3.5: probability distribution for the sum of two dice.
524,1,"['probabilities', 'distribution', 'probability distribution', 'probability', 'outcomes']", Defining probability,seg_21,a probability distribution is a list of the possible outcomes with corresponding probabilities that satisfies three rules:
525,1,"['disjoint', 'outcomes']", Defining probability,seg_21,1. the outcomes listed must be disjoint.
526,1,['probability'], Defining probability,seg_21,2. each probability must be between 0 and 1.
527,1,['probabilities'], Defining probability,seg_21,3. the probabilities must total 1.
528,1,"['states', 'distributions']", Defining probability,seg_21,figure 3.6 suggests three distributions for household income in the united states. only one is correct. which one must it be? what is wrong with the other two?12
529,1,['range'], Defining probability,seg_21,income range $0-25k $25k-50k $50k-100k $100k+ (a) 0.18 0.39 0.33 0.16 (b) 0.38 -0.27 0.52 0.37 (c) 0.28 0.27 0.29 0.16
530,1,['distributions'], Defining probability,seg_21,figure 3.6: proposed distributions of us household incomes (guided practice 3.16).
531,1,"['plot', 'bar plot', 'probability distributions', 'data', 'distribution', 'probability distribution', 'probability', 'plotting', 'distributions']", Defining probability,seg_21,"chapter 1 emphasized the importance of plotting data to provide quick summaries. probability distributions can also be summarized in a bar plot. for instance, the distribution of us household incomes is shown in figure 3.7 as a bar plot. the probability distribution for the sum of two dice is shown in figure 3.5 and plotted in figure 3.8."
532,1,"['distribution', 'probability distribution', 'probability']", Defining probability,seg_21,figure 3.7: the probability distribution of us household income.
533,1,"['distribution', 'probability distribution', 'probability']", Defining probability,seg_21,figure 3.8: the probability distribution of the sum of two dice.
534,1,"['plot', 'histogram', 'outcomes', 'bar plot', 'probabilities', 'plots', 'case', 'discrete', 'locations', 'bar plots', 'plotting', 'numerical']", Defining probability,seg_21,"in these bar plots, the bar heights represent the probabilities of outcomes. if the outcomes are numerical and discrete, it is usually (visually) convenient to make a bar plot that resembles a histogram, as in the case of the sum of two dice. another example of plotting the bars at their respective locations is shown in figure 3.18 on page 115."
535,1,"['complement of an event', 'complement', 'event']", Defining probability,seg_21,3.1.6 complement of an event
536,1,"['sample', 'set', 'event', 'sample space', 'outcomes']", Defining probability,seg_21,"rolling a die produces a value in the set {1, 2, 3, 4, 5, 6}. this set of all possible outcomes is called the sample space (s) for rolling a die. we often use the sample space to examine the scenario where an event does not occur."
537,1,"['sample', 'set', 'event', 'outcome', 'sample space', 'outcomes']", Defining probability,seg_21,"let d = {2, 3} represent the event that the outcome of a die roll is 2 or 3. then the complement of d represents all outcomes in our sample space that are not in d, which is denoted by dc = {1, 4, 5, 6}. that is, dc is the set of all possible outcomes not already included in d. figure 3.9 shows the relationship between d, dc, and the sample space s."
538,1,"['sample', 'complement', 'set', 'event', 'sample space', 'outcomes']", Defining probability,seg_21,"figure 3.9: event d = {2, 3} and its complement, dc = {1, 4, 5, 6}. s represents the sample space, which is the set of all possible outcomes."
539,1,"['disjoint', 'outcome', 'complement', 'event', 'complement of an event']", Defining probability,seg_21,"a complement of an event a is constructed to have two very important properties: (i) every possible outcome not in a is in ac, and (ii) a and ac are disjoint. property (i) implies"
540,1,"['disjoint', 'addition rule', 'disjoint events', 'events', 'outcome']", Defining probability,seg_21,"that is, if the outcome is not in a, it must be represented in ac. we use the addition rule for disjoint events to apply property (ii):"
541,1,"['probability of an event', 'complement', 'probability', 'event']", Defining probability,seg_21,combining the last two equations yields a very useful relationship between the probability of an event and its complement.
542,1,"['complement', 'event', 'outcomes']", Defining probability,seg_21,"the complement of event a is denoted ac, and ac represents all outcomes not in a. a and ac are mathematically related:"
543,1,['complement'], Defining probability,seg_21,"in simple examples, computinga orac is feasible in a few steps. however, using the complement can save a lot of time as problems grow in complexity."
544,1,['event'], Defining probability,seg_21,let a represent the event where we roll two dice and their total is less than 12. (a) what does the event ac represent? (b) determine p (ac) from figure 3.5 on page 87. (c) determine p (a).15
545,1,['probabilities'], Defining probability,seg_21,find the following probabilities for rolling two dice:16
546,1,"['probability', 'probability of the event', 'event']", Defining probability,seg_21,"(b) the sum is at least 4. that is, determine the probability of the event b = {4, 5, ..., 12}."
547,1,"['probability', 'probability of the event', 'event']", Defining probability,seg_21,"(c) the sum is no more than 10. that is, determine the probability of the event d = {2, 3, ..., 10}."
548,1,['independence'], Defining probability,seg_21,3.1.7 independence
549,1,"['random processes', 'random', 'independent', 'variables', 'observations', 'information', 'outcome', 'processes']", Defining probability,seg_21,"just as variables and observations can be independent, random processes can be independent, too. two processes are independent if knowing the outcome of one provides no useful information about the outcome of the other. for instance, flipping a coin and rolling a die are two independent processes – knowing the coin was heads does not help determine the outcome of a die roll. on the other hand, stock prices usually move up or down together, so they are not independent."
550,1,"['independent', 'information', 'probability', 'outcome', 'processes']", Defining probability,seg_21,"example 3.5 provides a basic example of two independent processes: rolling two dice. we want to determine the probability that both will be 1. suppose one of the dice is red and the other white. if the outcome of the red die is a 1, it provides no information about the outcome of the white die. we first encountered this same question in example 3.5 (page 81), where we calculated the probability using the following reasoning: 1/6 of the time the red die is a 1, and 1/6 of those times the white die"
551,1,"['independent', 'probabilities', 'outcomes', 'processes']", Defining probability,seg_21,"will also be 1. this is illustrated in figure 3.10. because the rolls are independent, the probabilities of the corresponding outcomes can be multiplied to get the final answer: (1/6)× (1/6) = 1/36. this can be generalized to many independent processes."
552,0,[], Defining probability,seg_21,all rolls 1/6th of the first rolls are a 1. 1/6th of those times where the first roll is a 1 the second roll is also a 1.
553,1,"['independent', 'probability']", Defining probability,seg_21,what if there was also a blue die independent of the other two? what is the probability of rolling the three dice and getting all 1s?
554,0,[], Defining probability,seg_21,"the same logic applies from example 3.5. if 1/36 of the time the white and red dice are both 1, then 1/6 of those times the blue die will also be 1, so multiply:"
555,1,"['independent', 'multiplication rule', 'processes']", Defining probability,seg_21,example 3.21 illustrates what is called the multiplication rule for independent processes.
556,1,"['independent', 'probabilities', 'events', 'probability', 'processes']", Defining probability,seg_21,"if a and b represent events from two different and independent processes, then the probability that both a and b occur can be calculated as the product of their separate probabilities:"
557,1,"['independent', 'events', 'probability', 'processes']", Defining probability,seg_21,"similarly, if there are k events a1, ..., ak from k independent processes, then the probability they all occur is"
558,1,"['sample size', 'sample', 'random', 'independent', 'probability', 'population']", Defining probability,seg_21,"about 9% of people are left-handed. suppose 2 people are selected at random from the u.s. population. because the sample size of 2 is very small relative to the population, it is reasonable to assume these two people are independent. (a) what is the probability that both are left-handed? (b) what is the probability that both are right-handed?17"
559,0,[], Defining probability,seg_21,suppose 5 people are selected at random.18
560,1,['probability'], Defining probability,seg_21,(a) what is the probability that all are right-handed?
561,1,['probability'], Defining probability,seg_21,(b) what is the probability that all are left-handed?
562,1,['probability'], Defining probability,seg_21,(c) what is the probability that not all of the people are right-handed?
563,1,"['multiplication rule', 'independent', 'variables', 'information']", Defining probability,seg_21,"suppose the variables handedness and sex are independent, i.e. knowing someone’s sex provides no useful information about their handedness and vice-versa. then we can compute whether a randomly selected person is right-handed and female19 using the multiplication rule:"
564,0,[], Defining probability,seg_21,three people are selected at random.20
565,1,['probability'], Defining probability,seg_21,(a) what is the probability that the first person is male and right-handed?
566,1,['probability'], Defining probability,seg_21,(b) what is the probability that the first two people are male and right-handed?.
567,1,['probability'], Defining probability,seg_21,(c) what is the probability that the third person is female and left-handed?
568,1,['probability'], Defining probability,seg_21,(d) what is the probability that the first two people are male and right-handed and the third
569,0,[], Defining probability,seg_21,person is female and left-handed?
570,1,"['independent', 'information', 'events independent', 'events', 'outcome']", Defining probability,seg_21,"sometimes we wonder if one outcome provides useful information about another outcome. the question we are asking is, are the occurrences of the two events independent? we say that two events a and b are independent if they satisfy p (a and b) = p (a)× p (b)."
571,1,"['independent', 'event']", Defining probability,seg_21,"if we shuffle up a deck of cards and draw one, is the event that the card is a heart independent of the event that the card is an ace?"
572,1,['probability'], Defining probability,seg_21,the probability the card is a heart is 1/4 and the probability that it is an ace is 1/13. the probability the card is the ace of hearts is 1/52. we check whether p (a and b) = p (a)× p (b) is satisfied:
573,1,"['independent', 'events', 'independent events', 'event']", Defining probability,seg_21,"because the equation holds, the event that the card is a heart and the event that the card is an ace are independent events."
574,1,"['risk', 'probabilities', 'conditional probabilities', 'variables', 'conditional', 'information']", Conditional probability,seg_23,there can be rich relationships between two or more variables that are useful to understand. for example a car insurance company will consider information about a person’s driving history to assess the risk that they will be responsible for an accident. these types of relationships are the realm of conditional probabilities.
575,1,"['contingency table', 'probabilities', 'table']", Conditional probability,seg_23,3.2.1 exploring probabilities with a contingency table
576,1,"['sample', 'machine learning', 'results', 'data', 'variable', 'set', 'data set', 'test']", Conditional probability,seg_23,"the photo classify data set represents a classifier a sample of 1822 photos from a photo sharing website. data scientists have been working to improve a classifier for whether the photo is about fashion or not, and these 1822 photos represent a test for their classifier. each photo gets two classifications: the first is called mach learn and gives a classification from a machine learning (ml) system of either pred fashion or pred not. each of these 1822 photos have also been classified carefully by a team of people, which we take to be the source of truth; this variable is called truth and takes values fashion and not. figure 3.11 summarizes the results."
577,0,[], Conditional probability,seg_23,truth fashion not total pred fashion 197 22 219 mach learn pred not 112 1491 1603
578,1,"['table', 'contingency table', 'data', 'set', 'data set']", Conditional probability,seg_23,figure 3.11: contingency table summarizing the photo classify data set.
579,0,[], Conditional probability,seg_23,fashion photos 0.06
580,0,[], Conditional probability,seg_23,ml predicts fashion neither: 0.82
581,1,"['data', 'set', 'data set', 'venn diagram', 'venn']", Conditional probability,seg_23,figure 3.12: a venn diagram using boxes for the photo classify data set.
582,0,[], Conditional probability,seg_23,"if a photo is actually about fashion, what is the chance the ml classifier correctly identified the photo as being about fashion?"
583,1,"['data', 'probability', 'algorithm', 'estimate']", Conditional probability,seg_23,"we can estimate this probability using the data. of the 309 fashion photos, the ml algorithm correctly classified 197 of the photos:"
584,0,[], Conditional probability,seg_23,197 p (mach learn is pred fashion given truth is fashion) = = 0.638 309
585,1,"['sample', 'predicted', 'data', 'algorithm', 'set', 'data set', 'probability']", Conditional probability,seg_23,we sample a photo from the data set and learn the ml algorithm predicted this photo was not about fashion. what is the probability that it was incorrect and the photo is about fashion?
586,1,"['set', 'data set', 'data']", Conditional probability,seg_23,"if the ml classifier suggests a photo is not about fashion, then it comes from the second row in the data set. of these 1603 photos, 112 were actually about fashion:"
587,0,[], Conditional probability,seg_23,112 p (truth is fashion given mach learn is pred not) = = 0.070 1603
588,1,"['joint probabilities', 'probabilities', 'marginal', 'marginal and joint probabilities', 'joint']", Conditional probability,seg_23,3.2.2 marginal and joint probabilities
589,1,"['sample', 'column totals', 'probabilities', 'variables', 'row and column totals', 'marginal', 'data', 'variable', 'set', 'data set', 'probability', 'marginal probabilities', 'marginal probability']", Conditional probability,seg_23,"figure 3.11 includes row and column totals for each variable separately in the photo classify data set. these totals represent marginal probabilities for the sample, which are the probabilities based on a single variable without regard to any other variables. for instance, a probability based solely on the mach learn variable is a marginal probability:"
590,1,"['variables', 'joint', 'probability', 'outcomes', 'processes', 'joint probability']", Conditional probability,seg_23,a probability of outcomes for two or more variables or processes is called a joint probability:
591,0,[], Conditional probability,seg_23,197 p (mach learn is pred fashion and truth is fashion) = = 0.11 1822
592,1,"['joint', 'probability', 'joint probability']", Conditional probability,seg_23,"it is common to substitute a comma for “and” in a joint probability, although using either the word “and” or a comma is acceptable:"
593,0,[], Conditional probability,seg_23,"p (mach learn is pred fashion, truth is fashion)"
594,0,[], Conditional probability,seg_23,means the same thing as
595,0,[], Conditional probability,seg_23,p (mach learn is pred fashion and truth is fashion)
596,1,"['outcomes', 'variables', 'marginal', 'variable', 'joint', 'probability', 'marginal probability', 'processes', 'joint probability']", Conditional probability,seg_23,"if a probability is based on a single variable, it is a marginal probability. the probability of outcomes for two or more variables or processes is called a joint probability."
597,1,"['sample', 'joint probabilities', 'probabilities', 'table', 'variables', 'table proportions', 'joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'joint probability']", Conditional probability,seg_23,"we use table proportions to summarize joint probabilities for the photo classify sample. these proportions are computed by dividing each count in figure 3.11 by the table’s total, 1822, to obtain the proportions in figure 3.13. the joint probability distribution of the mach learn and truth variables is shown in figure 3.14."
598,1,"['table', 'probability table', 'data', 'set', 'data set', 'probability']", Conditional probability,seg_23,figure 3.13: probability table summarizing the photo classify data set.
599,1,"['outcome', 'probability']", Conditional probability,seg_23,joint outcome probability mach learn is pred fashion and truth is fashion 0.1081 mach learn is pred fashion and truth is not 0.0121 mach learn is pred not and truth is fashion 0.0615 mach learn is pred not and truth is not 0.8183 total 1.0000
600,1,"['joint probability distribution', 'data', 'distribution', 'probability distribution', 'set', 'joint', 'data set', 'probability', 'joint probability']", Conditional probability,seg_23,figure 3.14: joint probability distribution for the photo classify data set.
601,1,"['disjoint', 'probabilities', 'distribution', 'events', 'probability distribution', 'probability']", Conditional probability,seg_23,"verify figure 3.14 represents a probability distribution: events are disjoint, all probabilities are non-negative, and the probabilities sum to 1.25"
602,1,"['joint probabilities', 'probabilities', 'marginal', 'data', 'cases', 'set', 'joint', 'data set', 'probability', 'marginal probabilities', 'outcomes']", Conditional probability,seg_23,"we can compute marginal probabilities using joint probabilities in simple cases. for example, the probability a randomly selected photo from the data set is about fashion is found by summing the outcomes where truth takes value fashion:"
603,0,[], Conditional probability,seg_23,p (truth is fashion) = p (mach learn is pred fashion and truth is fashion)
604,0,[], Conditional probability,seg_23,+ p (mach learn is pred not and truth is fashion)
605,1,"['probability', 'conditional', 'conditional probability']", Conditional probability,seg_23,3.2.3 defining conditional probability
606,1,"['estimation', 'information', 'variable', 'probability']", Conditional probability,seg_23,"the ml classifier predicts whether a photo is about fashion, even if it is not perfect. we would like to better understand how to use information from a variable like mach learn to improve our probability estimation of a second variable, which in this example is truth."
607,1,"['estimate', 'machine learning', 'predicted', 'data', 'cases', 'set', 'data set', 'probability', 'random', 'limit']", Conditional probability,seg_23,"the probability that a random photo from the data set is about fashion is about 0.17. if we knew the machine learning classifier predicted the photo was about fashion, could we get a better estimate of the probability the photo is actually about fashion? absolutely. to do so, we limit our view to only those 219 cases where the ml classifier predicted that the photo was about fashion and look at the fraction where the photo was actually about fashion:"
608,0,[], Conditional probability,seg_23,197 p (truth is fashion given mach learn is pred fashion) = = 0.900 219
609,1,"['condition', 'prediction', 'conditional', 'probability', 'conditional probability']", Conditional probability,seg_23,we call this a conditional probability because we computed the probability under a condition: the ml classifier prediction said the photo was about fashion.
610,1,"['condition', 'outcome of interest', 'conditional probability', 'conditional', 'information', 'probability', 'event', 'outcome']", Conditional probability,seg_23,"there are two parts to a conditional probability, the outcome of interest and the condition. it is useful to think of the condition as information we know to be true, and this information usually can be described as a known outcome or event. we generally separate the text inside our probability notation into the outcome of interest and the condition with a vertical bar:"
611,0,[], Conditional probability,seg_23,p (truth is fashion given mach learn is pred fashion)
612,0,[], Conditional probability,seg_23,197 = p (truth is fashion | mach learn is pred fashion) = = 0.900 219
613,1,"['condition', 'predicted', 'probability', 'algorithm']", Conditional probability,seg_23,"in the last equation, we computed the probability a photo was about fashion based on the condition that the ml algorithm predicted it was about fashion as a fraction:"
614,0,[], Conditional probability,seg_23,p (truth is fashion | mach learn is pred fashion)
615,1,['cases'], Conditional probability,seg_23,# cases where truth is fashion and mach learn is pred fashion
616,1,['cases'], Conditional probability,seg_23,= # cases where mach learn is pred fashion
617,1,"['condition', 'outcome of interest', 'cases', 'outcome']", Conditional probability,seg_23,"we considered only those cases that met the condition, mach learn is pred fashion, and then we computed the ratio of those cases that satisfied our outcome of interest, photo was actually about fashion."
618,1,"['joint probabilities', 'probabilities', 'conditional probabilities', 'marginal', 'rates', 'data', 'conditional', 'marginal and joint probabilities', 'joint', 'percentages']", Conditional probability,seg_23,"frequently, marginal and joint probabilities are provided instead of count data. for example, disease rates are commonly listed in percentages rather than in a count format. we would like to be able to compute conditional probabilities even when no counts are available, and we use the last equation as a template to understand this technique."
619,1,"['sample', 'condition', 'outcome of interest', 'predicted', 'data', 'algorithm', 'conditional', 'cases', 'information', 'probability', 'conditional probability', 'outcome']", Conditional probability,seg_23,"we considered only those cases that satisfied the condition, where the ml algorithm predicted fashion. of these cases, the conditional probability was the fraction representing the outcome of interest, that the photo was about fashion. suppose we were provided only the information in figure 3.13, i.e. only probability data. then if we took a sample of 1000 photos, we would anticipate about 12.0% or 0.120 × 1000 = 120 would be predicted to be about fashion (mach learn is pred fashion). similarly, we would expect about 10.8% or 0.108 × 1000 = 108 to meet both the information criteria and represent our outcome of interest. then the conditional probability can be computed as"
620,0,[], Conditional probability,seg_23,p (truth is fashion | mach learn is pred fashion)
621,0,[], Conditional probability,seg_23,# (truth is fashion and mach learn is pred fashion)
622,0,[], Conditional probability,seg_23,= # (mach learn is pred fashion)
623,1,['probabilities'], Conditional probability,seg_23,"here we are examining exactly the fraction of two probabilities, 0.108 and 0.120, which we can write as"
624,0,[], Conditional probability,seg_23,p (truth is fashion and mach learn is pred fashion) and p (mach learn is pred fashion).
625,1,"['probabilities', 'conditional', 'probability', 'conditional probability']", Conditional probability,seg_23,the fraction of these probabilities is an example of the general formula for conditional probability.
626,1,"['condition', 'conditional', 'probability', 'conditional probability', 'outcome']", Conditional probability,seg_23,the conditional probability of outcome a given condition b is computed as the following:
627,1,"['condition', 'prediction', 'algorithm', 'conditional', 'probability', 'conditional probability']", Conditional probability,seg_23,"(a) write out the following statement in conditional probability notation: “the probability that the ml prediction was correct, if the photo was about fashion”. here the condition is now based on the photo’s truth status, not the ml algorithm."
628,1,"['probability', 'table']", Conditional probability,seg_23,26 (b) determine the probability from part (a). table 3.13 on page 96 may be helpful.
629,1,"['probability', 'algorithm']", Conditional probability,seg_23,(a) determine the probability that the algorithm is incorrect if it is known the photo is about fashion.
630,0,[], Conditional probability,seg_23,"(b) using the answers from part (a) and guided practice 3.29(b), compute"
631,0,[], Conditional probability,seg_23,p (mach learn is pred fashion | truth is fashion)
632,0,[], Conditional probability,seg_23,+ p (mach learn is pred not | truth is fashion)
633,0,[], Conditional probability,seg_23,(c) provide an intuitive argument to explain why the sum in (b) is 1.27
634,1,"['sample', 'likelihood', 'data', 'set', 'data set']", Conditional probability,seg_23,"the smallpox data set provides a sample of 6,224 individuals from the year 1721 who were exposed to smallpox in boston. doctors at the time believed that inoculation, which involves exposing a person to the disease in a controlled form, could reduce the likelihood of death."
635,1,"['outcomes', 'levels', 'variables', 'case', 'data', 'variable', 'tables']", Conditional probability,seg_23,"each case represents one person with two variables: inoculated and result. the variable inoculated takes two levels: yes or no, indicating whether the person was inoculated or not. the variable result has outcomes lived or died. these data are summarized in tables 3.15 and 3.16."
636,0,[], Conditional probability,seg_23,inoculated yes no total lived 238 5136 5374 result died 6 844 850
637,1,"['table', 'contingency table', 'data', 'set', 'data set']", Conditional probability,seg_23,figure 3.15: contingency table for the smallpox data set.
638,1,"['table proportions', 'data', 'table']", Conditional probability,seg_23,"figure 3.16: table proportions for the smallpox data, computed by dividing each count by the table total, 6224."
639,1,['probability'], Conditional probability,seg_23,"write out, in formal notation, the probability a randomly selected person who was not inoculated died from smallpox, and find this probability.28"
640,1,['probability'], Conditional probability,seg_23,determine the probability that an inoculated person died from smallpox. how does this result compare with the result of guided practice 3.31?29
641,1,"['experiment', 'variables', 'data', 'confounding', 'confounding variables']", Conditional probability,seg_23,the people of boston self-selected whether or not to be inoculated. (a) is this study observational or was this an experiment? (b) can we infer any causal connection using these data? (c) what are some potential confounding variables that might influence whether someone lived or died and also affect whether that person was inoculated?30
642,1,"['multiplication rule', 'general multiplication rule']", Conditional probability,seg_23,3.2.5 general multiplication rule
643,1,"['multiplication rule', 'independent', 'events', 'general multiplication rule', 'processes']", Conditional probability,seg_23,section 3.1.7 introduced the multiplication rule for independent processes. here we provide the general multiplication rule for events that might not be independent.
644,1,"['events', 'outcomes']", Conditional probability,seg_23,"if a and b represent two outcomes or events, then"
645,1,"['outcome of interest', 'outcome', 'condition']", Conditional probability,seg_23,it is useful to think of a as the outcome of interest and b as the condition.
646,1,"['multiplication rule', 'conditional', 'probability', 'conditional probability', 'general multiplication rule']", Conditional probability,seg_23,this general multiplication rule is simply a rearrangement of the conditional probability equation.
647,1,"['data', 'information', 'set', 'data set', 'probability']", Conditional probability,seg_23,"consider the smallpox data set. suppose we are given only two pieces of information: 96.08% of residents were not inoculated, and 85.88% of the residents who were not inoculated ended up surviving. how could we compute the probability that a resident was not inoculated and lived?"
648,1,"['multiplication rule', 'general multiplication rule']", Conditional probability,seg_23,we will compute our answer using the general multiplication rule and then verify it using figure 3.16. we want to determine
649,0,[], Conditional probability,seg_23,p (result = lived and inoculated = no)
650,0,[], Conditional probability,seg_23,and we are given that
651,0,[], Conditional probability,seg_23,"among the 96.08% of people who were not inoculated, 85.88% survived:"
652,1,"['multiplication rule', 'intersection', 'probability', 'general multiplication rule', 'error']", Conditional probability,seg_23,this is equivalent to the general multiplication rule. we can confirm this probability in figure 3.16 at the intersection of no and lived (with a small rounding error).
653,1,['probability'], Conditional probability,seg_23,use p (inoculated = yes) = 0.0392 and p (result = lived | inoculated = yes) = 0.9754 to determine the probability that a person was both inoculated and lived.31
654,0,[], Conditional probability,seg_23,"if 97.54% of the inoculated people lived, what proportion of inoculated people must have died?32"
655,1,"['disjoint', 'outcomes', 'variable', 'event', 'process']", Conditional probability,seg_23,"let a1, ..., ak represent all the disjoint outcomes for a variable or process. then if b is an event, possibly for another variable or process, we have:"
656,1,"['information', 'complement', 'event']", Conditional probability,seg_23,the rule for complements also holds when an event and its complement are conditioned on the same information:
657,1,"['risk', 'probabilities']", Conditional probability,seg_23,"based on the probabilities computed above, does it appear that inoculation is effective at reducing the risk of death from smallpox?33"
658,1,"['conditional', 'independence', 'probability', 'conditional probability']", Conditional probability,seg_23,3.2.6 independence considerations in conditional probability
659,1,"['independent', 'probabilities', 'conditional probabilities', 'conditional', 'information', 'events', 'outcome']", Conditional probability,seg_23,"if two events are independent, then knowing the outcome of one should provide no information about the other. we can show this is mathematically true using conditional probabilities."
660,1,['outcomes'], Conditional probability,seg_23,let x and y represent the outcomes of rolling two dice.34
661,1,['probability'], Conditional probability,seg_23,"(a) what is the probability that the first die, x, is 1?"
662,1,['probability'], Conditional probability,seg_23,(b) what is the probability that both x and y are 1?
663,1,"['probability', 'conditional', 'conditional probability']", Conditional probability,seg_23,(c) use the formula for conditional probability to compute p (y = 1 | x = 1).
664,1,"['multiplication rule', 'information', 'independence', 'processes']", Conditional probability,seg_23,we can show in guided practice 3.38(c) that the conditioning information has no influence by using the multiplication rule for independence processes:
665,1,"['outcomes', 'table']", Conditional probability,seg_23,ron is watching a roulette table in a casino and notices that the last five outcomes were black. he figures that the chances of getting black six times in a row is very small (about 1/64) and puts his paycheck on red. what is wrong with his reasoning?35
666,1,['tree diagrams'], Conditional probability,seg_23,3.2.7 tree diagrams
667,1,"['probabilities', 'data', 'process', 'outcomes', 'processes']", Conditional probability,seg_23,tree diagrams are a tool to organize outcomes and probabilities around the structure of the data. they are most useful when two or more processes occur in a sequence and each process is conditioned on its predecessors.
668,1,"['tree diagram', 'data', 'population', 'rates']", Conditional probability,seg_23,"the smallpox data fit this description. we see the population as split by inoculation: yes and no. following this split, survival rates were observed for each group. this structure is reflected in the tree diagram shown in figure 3.17. the first branch for inoculation is said to be the primary branch while the other branches are secondary."
669,1,"['tree diagram', 'data', 'set', 'data set']", Conditional probability,seg_23,figure 3.17: a tree diagram of the smallpox data set.
670,1,"['multiplication rule', 'joint probabilities', 'probabilities', 'conditional probabilities', 'marginal', 'tree diagram', 'data', 'conditional', 'information', 'joint', 'probability', 'marginal probabilities', 'general multiplication rule']", Conditional probability,seg_23,"tree diagrams are annotated with marginal and conditional probabilities, as shown in figure 3.17. this tree diagram splits the smallpox data by inoculation into the yes and no groups with respective marginal probabilities 0.0392 and 0.9608. the secondary branches are conditioned on the first, so we assign conditional probabilities to these branches. for example, the top branch in figure 3.17 is the probability that result = lived conditioned on the information that inoculated = yes. we may (and usually do) construct joint probabilities at the end of each branch in our tree by multiplying the numbers we come across as we move from left to right. these joint probabilities are computed using the general multiplication rule:"
671,0,[], Conditional probability,seg_23,p (inoculated = yes and result = lived)
672,0,[], Conditional probability,seg_23,= p (inoculated = yes)× p (result = lived|inoculated = yes)
673,1,"['statistics', 'probability']", Conditional probability,seg_23,"consider the midterm and final for a statistics class. suppose 13% of students earned an a on the midterm. of those students who earned an a on the midterm, 47% received an a on the final, and 11% of the students who earned lower than an a on the midterm received an a on the final. you randomly pick up a final exam and notice the student received an a. what is the probability that this student earned an a on the midterm?"
674,1,"['probabilities', 'conditional', 'probability', 'conditional probability']", Conditional probability,seg_23,"the end-goal is to find p (midterm = a|final = a). to calculate this conditional probability, we need the following probabilities:"
675,1,"['probabilities', 'tree diagram', 'information']", Conditional probability,seg_23,"however, this information is not provided, and it is not obvious how to calculate these probabilities. since we aren’t sure how to proceed, it is useful to organize the information into a tree diagram:"
676,1,"['probabilities', 'conditional probabilities', 'variables', 'marginal', 'tree diagram', 'conditional', 'case', 'marginal probabilities']", Conditional probability,seg_23,"when constructing a tree diagram, variables provided with marginal probabilities are often used to create the tree’s primary branches; in this case, the marginal probabilities are provided for midterm grades. the final grades, which correspond to the conditional probabilities provided, will be shown on the secondary branches."
677,1,"['probabilities', 'tree diagram']", Conditional probability,seg_23,"with the tree diagram constructed, we may compute the required probabilities:"
678,1,"['joint probabilities', 'probabilities', 'marginal', 'joint', 'probability', 'marginal probability']", Conditional probability,seg_23,"the marginal probability, p (final = a), was calculated by adding up all the joint probabilities on the right side of the tree that correspond to final = a. we may now finally take the ratio of the two probabilities:"
679,1,['probability'], Conditional probability,seg_23,the probability the student also earned an a on the midterm is about 0.39.
680,1,"['statistics', 'tree diagram', 'information', 'probability', 'tree diagrams']", Conditional probability,seg_23,"after an introductory statistics course, 78% of students can successfully construct tree diagrams. of those who can construct tree diagrams, 97% passed, while only 57% of those students who could not construct tree diagrams passed. (a) organize this information into a tree diagram. (b) what is the probability that a randomly selected student passed? (c) compute the probability a student is"
681,1,['tree diagram'], Conditional probability,seg_23,36 able to construct a tree diagram if it is known that she passed.
682,0,[], Conditional probability,seg_23,3.2.8 bayes’ theorem
683,1,"['probability', 'conditional', 'conditional probability']", Conditional probability,seg_23,"in many instances, we are given a conditional probability of the form"
684,1,['variable'], Conditional probability,seg_23,p (statement about variable 1 | statement about variable 2)
685,1,"['probability', 'conditional', 'conditional probability']", Conditional probability,seg_23,but we would really like to know the inverted conditional probability:
686,1,['variable'], Conditional probability,seg_23,p (statement about variable 2 | statement about variable 1)
687,1,"['tree diagram', 'conditional', 'cases', 'probability', 'conditional probability']", Conditional probability,seg_23,"tree diagrams can be used to find the second conditional probability when given the first. however, sometimes it is not possible to draw the scenario in a tree diagram. in these cases, we can apply a very useful and general formula: bayes’ theorem."
688,1,"['conditional probabilities', 'probabilities', 'tree diagram', 'conditional']", Conditional probability,seg_23,we first take a critical look at an example of inverting conditional probabilities where we still apply a tree diagram.
689,0,[], Conditional probability,seg_23,able to construct pass class
690,1,"['false positive', 'false negative', 'probability', 'random', 'test']", Conditional probability,seg_23,"in canada, about 0.35% of women over 40 will develop breast cancer in any given year. a common screening test for cancer is the mammogram, but this test is not perfect. in about 11% of patients with breast cancer, the test gives a false negative: it indicates a woman does not have breast cancer when she does have breast cancer. similarly, the test gives a false positive in 7% of patients who do not have breast cancer: it indicates these patients have breast cancer when they actually do not. if we tested a random woman over 40 for breast cancer using a mammogram and the test came back positive – that is, the test suggested the patient has cancer – what is the probability that the patient actually has breast cancer?"
691,1,"['test', 'probability', 'information']", Conditional probability,seg_23,"notice that we are given sufficient information to quickly compute the probability of testing positive if a woman has breast cancer (1.00 − 0.11 = 0.89). however, we seek the inverted probability of cancer given a positive test result. (watch out for the non-intuitive medical language: a positive test result suggests the possible presence of cancer in a mammogram screening.) this inverted probability may be broken into two pieces:"
692,0,[], Conditional probability,seg_23,p (has bc and mammogram+) p (has bc | mammogram+) = p (mammogram+)
693,1,"['tree diagram', 'probabilities']", Conditional probability,seg_23,where “has bc” is an abbreviation for the patient having breast cancer and “mammogram+” means the mammogram screening was positive. we can construct a tree diagram for these probabilities:
694,1,['probability'], Conditional probability,seg_23,the probability the patient has breast cancer and the mammogram is positive is
695,1,"['test', 'probability']", Conditional probability,seg_23,the probability of a positive test result is the sum of the two corresponding scenarios:
696,1,['probability'], Conditional probability,seg_23,"then if the mammogram screening is positive for a patient, the probability the patient has breast cancer is"
697,0,[], Conditional probability,seg_23,p (has bc and mammogram+) p (has bc | mammogram+) = p (mammogram+)
698,0,[], Conditional probability,seg_23,"that is, even if a patient has a positive mammogram screening, there is still only a 4% chance that she has breast cancer."
699,1,"['tests', 'test', 'condition']", Conditional probability,seg_23,"example 3.42 highlights why doctors often run more tests regardless of a first positive test result. when a medical condition is rare, a single positive test isn’t generally definitive."
700,1,['tree diagram'], Conditional probability,seg_23,"consider again the last equation of example 3.42. using the tree diagram, we can see that the numerator (the top of the fraction) is equal to the following product:"
701,1,"['probabilities', 'probability']", Conditional probability,seg_23,the denominator – the probability the screening was positive – is equal to the sum of probabilities for each positive screening scenario:
702,1,"['probabilities', 'marginal', 'tree diagram', 'conditional', 'probability', 'conditional probability', 'marginal probability']", Conditional probability,seg_23,"in the example, each of the probabilities on the right side was broken down into a product of a conditional probability and marginal probability using the tree diagram."
703,1,"['conditional', 'probability', 'conditional probability']", Conditional probability,seg_23,we can see an application of bayes’ theorem by substituting the resulting probability expressions into the numerator and denominator of the original conditional probability.
704,1,"['conditional', 'variable', 'probability', 'conditional probability']", Conditional probability,seg_23,consider the following conditional probability for variable 1 and variable 2:
705,1,"['outcome', 'variable']", Conditional probability,seg_23,p (outcome a1 of variable 1 | outcome b of variable 2)
706,1,"['states', 'conditional', 'probability', 'conditional probability']", Conditional probability,seg_23,bayes’ theorem states that this conditional probability can be identified as the following fraction:
707,1,"['variable', 'outcomes']", Conditional probability,seg_23,"where a2, a3, ..., and ak represent all other possible outcomes of the first variable."
708,1,"['probabilities', 'marginal', 'probability', 'tree diagrams', 'marginal probability']", Conditional probability,seg_23,"bayes’ theorem is a generalization of what we have done using tree diagrams. the numerator identifies the probability of getting both a1 and b. the denominator is the marginal probability of getting b. this bottom component of the fraction appears long and complicated since we have to add up probabilities from all of the different ways to get b. we always completed this step when using tree diagrams. however, we usually did it in a separate step so it didn’t seem as complex. to apply bayes’ theorem correctly, there are two preparatory steps:"
709,1,"['probabilities', 'marginal', 'variable', 'marginal probabilities', 'outcome']", Conditional probability,seg_23,"(1) first identify the marginal probabilities of each possible outcome of the first variable: p (a1),"
710,1,"['outcome', 'probability']", Conditional probability,seg_23,"(2) then identify the probability of the outcome b, conditioned on each possible scenario for the"
711,1,['variable'], Conditional probability,seg_23,"first variable: p (b|a1), p (b|a2), ..., p (b|ak)."
712,1,"['probabilities', 'tree diagram']", Conditional probability,seg_23,"once each of these probabilities are identified, they can be applied directly within the formula. bayes’ theorem tends to be a good option when there are so many scenarios that drawing a tree diagram would be complex."
713,1,"['tree diagram', 'events', 'probability', 'event']", Conditional probability,seg_23,"jose visits campus every thursday evening. however, some days the parking garage is full, often due to college events. there are academic events on 35% of evenings, sporting events on 20% of evenings, and no events on 45% of evenings. when there is an academic event, the garage fills up about 25% of the time, and it fills up 70% of evenings with sporting events. on evenings when there are no events, it only fills up about 5% of the time. if jose comes to campus and finds the garage full, what is the probability that there is a sporting event? use a tree diagram to solve this problem.37"
714,0,[], Conditional probability,seg_23,"here we solve the same problem presented in guided practice 3.43, except this time we use bayes’ theorem."
715,1,"['probabilities', 'condition', 'outcome of interest', 'event', 'outcome']", Conditional probability,seg_23,"the outcome of interest is whether there is a sporting event (call this a1), and the condition is that the lot is full (b). let a2 represent an academic event and a3 represent there being no event on campus. then the given probabilities can be written as"
716,1,"['probability', 'condition', 'event']", Conditional probability,seg_23,bayes’ theorem can be used to compute the probability of a sporting event (a1) under the condition that the parking lot is full (b):
717,1,"['information', 'probability', 'event']", Conditional probability,seg_23,"based on the information that the garage is full, there is a 56% probability that a sporting event is being held on campus that evening."
718,1,"['probability', 'information']", Conditional probability,seg_23,use the information in the previous exercise and example to verify the probability that there is an
719,1,['event'], Conditional probability,seg_23,38 academic event conditioned on the parking lot being full is 0.35.
720,1,"['event ', 'information', 'probability', 'event']", Conditional probability,seg_23,"in guided practice 3.43 and 3.45, you found that if the parking lot is full, the probability there is a sporting event is 0.56 and the probability there is an academic event is 0.35. using this information, compute p (no event | the lot is full).39"
721,1,"['bayesian', 'bayesian statistics', 'statistics', 'information', 'event']", Conditional probability,seg_23,"the last several exercises offered a way to update our belief about whether there is a sporting event, academic event, or no event going on at the school based on the information that the parking lot was full. this strategy of updating beliefs using bayes’ theorem is actually the foundation of an entire section of statistics called bayesian statistics. while bayesian statistics is very important and useful, we will not have time to cover much more of it in this book."
722,1,"['sample size', 'sample', 'observations', 'without replacement', 'replacement', 'sampling', 'cases', 'population']", Sampling from a small population,seg_25,"when we sample observations from a population, usually we’re only sampling a small fraction of the possible individuals or cases. however, sometimes our sample size is large enough or the population is small enough that we sample more than 10% of a population44 without replacement (meaning we do not have a chance of sampling the same cases twice). sampling such a notable fraction of a population can be important for how we analyze the sample."
723,1,['random'], Sampling from a small population,seg_25,"professors sometimes select a student at random to answer a question. if each student has an equal chance of being selected and there are 15 people in your class, what is the chance that she will pick you for the next question?"
724,1,['probability'], Sampling from a small population,seg_25,"if there are 15 people to ask and none are skipping class, then the probability is 1/15, or about 0.067."
725,1,['probability'], Sampling from a small population,seg_25,"if the professor asks 3 questions, what is the probability that you will not be selected? assume that she will not pick the same person twice in a given lecture."
726,1,['probability'], Sampling from a small population,seg_25,"for the first question, she will pick someone else with probability 14/15. when she asks the second question, she only has 14 people who have not yet been asked. thus, if you were not picked on the first question, the probability you are again not picked is 13/14. similarly, the probability you are again not picked on the third question is 12/13, and the probability of not being picked for any of the three questions is"
727,0,[], Sampling from a small population,seg_25,p (not picked in 3 questions)
728,1,['probabilities'], Sampling from a small population,seg_25,what rule permitted us to multiply the probabilities in example 3.48?45
729,1,['probability'], Sampling from a small population,seg_25,"suppose the professor randomly picks without regard to who she already selected, i.e. students can be picked more than once. what is the probability that you will not be picked for any of the three questions?"
730,1,"['multiplication rule', 'independent', 'probability', 'processes']", Sampling from a small population,seg_25,"each pick is independent, and the probability of not being picked for any individual question is 14/15. thus, we can use the multiplication rule for independent processes."
731,0,[], Sampling from a small population,seg_25,p (not picked in 3 questions)
732,0,[], Sampling from a small population,seg_25,"you have a slightly higher chance of not being picked compared to when she picked a new person for each question. however, you now may be picked more than once."
733,1,['probability'], Sampling from a small population,seg_25,"under the setup of example 3.50, what is the probability of being picked to answer all three questions?46"
734,1,"['sample', 'with replacement', 'observations', 'without replacement', 'replacement', 'independence', 'population', 'probability', 'event']", Sampling from a small population,seg_25,"if we sample from a small population without replacement, we no longer have independence between our observations. in example 3.48, the probability of not being picked for the second question was conditioned on the event that you were not picked for the first question. in example 3.50, the professor sampled her students with replacement: she repeatedly sampled the entire class without regard to who she already picked."
735,1,"['without replacement', 'replacement', 'probability']", Sampling from a small population,seg_25,"your department is holding a raffle. they sell 30 tickets and offer seven prizes. (a) they place the tickets in a hat and draw one for each prize. the tickets are sampled without replacement, i.e. the selected tickets are not placed back in the hat. what is the probability of winning a prize if you buy one ticket? (b) what if the tickets are sampled with replacement?47"
736,1,"['sampling', 'method']", Sampling from a small population,seg_25,compare your answers in guided practice 3.52. how much influence does the sampling method have on your chances of winning a prize?48
737,1,"['sample size', 'sample', 'independent', 'with replacement', 'observations', 'results', 'without replacement', 'replacement', 'sampling', 'probability', 'population']", Sampling from a small population,seg_25,"had we repeated guided practice 3.52 with 300 tickets instead of 30, we would have found something interesting: the results would be nearly identical. the probability would be 0.0233 without replacement and 0.0231 with replacement. when the sample size is only a small fraction of the population (under 10%), observations are nearly independent even when sampling without replacement."
738,1,"['model', 'outcomes', 'random variable', 'variable', 'statistical', 'random', 'process']", Random variables,seg_27,it’s often useful to model a process using what’s called a random variable. such a model allows us to apply a mathematical framework and statistical principles for better understanding and predicting outcomes in the real world.
739,1,"['statistics', 'percentages']", Random variables,seg_27,"two books are assigned for a statistics class: a textbook and its corresponding study guide. the university bookstore determined 20% of enrolled students do not buy either book, 55% buy the textbook only, and 25% buy both books, and these percentages are relatively constant from one term to another. if there are 100 students enrolled, how many books should the bookstore expect to sell to this class?"
740,0,[], Random variables,seg_27,"around 20 students will not buy either book (0 books total), about 55 will buy one book (55 books total), and approximately 25 will buy two books (totaling 50 books for these 25 students). the bookstore should expect to sell about 105 books for this class."
741,0,[], Random variables,seg_27,would you be surprised if the bookstore sold slightly more or less than 105 books?49
742,0,[], Random variables,seg_27,the textbook costs $137 and the study guide $33. how much revenue should the bookstore expect from this class of 100 students?
743,0,[], Random variables,seg_27,"about 55 students will just buy a textbook, providing revenue of"
744,0,[], Random variables,seg_27,the roughly 25 students who buy both the textbook and the study guide would pay a total of
745,1,"['sampling', 'variability']", Random variables,seg_27,"thus, the bookstore should expect to generate about $7, 535 + $4, 250 = $11, 785 from these 100 students for this one class. however, there might be some sampling variability so the actual amount may differ by a little bit."
746,1,"['distribution', 'probability distribution', 'average', 'probability']", Random variables,seg_27,figure 3.18: probability distribution for the bookstore’s revenue from one student. the triangle represents the average revenue per student.
747,1,['average'], Random variables,seg_27,what is the average revenue per student for this course?
748,0,[], Random variables,seg_27,"the expected total revenue is $11,785, and there are 100 students. therefore the expected revenue per student is $11, 785/100 = $117.85."
749,1,['expectation'], Random variables,seg_27,3.4.1 expectation
750,1,"['statistics', 'random variable', 'variable', 'random', 'outcome', 'process', 'numerical']", Random variables,seg_27,"we call a variable or process with a numerical outcome a random variable, and we usually represent this random variable with a capital letter such as x, y , or z. the amount of money a single student will spend on her statistics books is a random variable, and we represent it by x."
751,1,"['random', 'variable', 'random process', 'outcome', 'process', 'numerical']", Random variables,seg_27,a random process or variable with a numerical outcome.
752,1,"['probabilities', 'case', 'distribution', 'outcomes']", Random variables,seg_27,"the possible outcomes of x are labeled with a corresponding lower case letter x and subscripts. for example, we write x1 = $0, x2 = $137, and x3 = $170, which occur with probabilities 0.20, 0.55, and 0.25. the distribution of x is summarized in figure 3.18 and figure 3.19."
753,1,"['distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random']", Random variables,seg_27,"figure 3.19: the probability distribution for the random variable x, representing the bookstore’s revenue from a single student."
754,1,"['expected value', 'random variable', 'variable', 'probability', 'random', 'outcome', 'average']", Random variables,seg_27,"we computed the average outcome of x as $117.85 in example 3.57. we call this average the expected value of x, denoted by e(x). the expected value of a random variable is computed by adding each outcome weighted by its probability:"
755,1,"['probabilities', 'expected value', 'probability', 'outcome', 'outcomes']", Random variables,seg_27,"if x takes outcomes x1, ..., xk with probabilities p (x = x1), ..., p (x = xk), the expected value of x is the sum of each outcome multiplied by its corresponding probability:"
756,0,[], Random variables,seg_27,the greek letter µ may be used in place of the notation e(x).
757,1,"['distribution', 'probability distribution', 'mean', 'probability']", Random variables,seg_27,figure 3.20: a weight system representing the probability distribution for x. the string holds the distribution at the mean to keep the system balanced.
758,1,"['distribution', 'continuous distribution', 'mean', 'continuous']", Random variables,seg_27,figure 3.21: a continuous distribution can also be balanced at its mean.
759,1,"['expected value', 'random variable', 'variable', 'random', 'outcome', 'average']", Random variables,seg_27,"the expected value for a random variable represents the average outcome. for example, e(x) = 117.85 represents the average amount the bookstore expects to make from a single student, which we could also write as µ = 117.85."
760,1,"['continuous random variable', 'expected value', 'random variable', 'variable', 'random', 'continuous']", Random variables,seg_27,"it is also possible to compute the expected value of a continuous random variable (see sec50 tion 3.5). however, it requires a little calculus and we save it for a later class."
761,1,"['probability distributions', 'distribution', 'probability distribution', 'expectation', 'continuous probability distribution', 'mean', 'probability', 'continuous probability distributions', 'continuous', 'outcome', 'distributions']", Random variables,seg_27,"in physics, the expectation holds the same meaning as the center of gravity. the distribution can be represented by a series of weights at each outcome, and the mean represents the balancing point. this is represented in figures 3.18 and 3.20. the idea of a center of gravity also expands to continuous probability distributions. figure 3.21 shows a continuous probability distribution balanced atop a wedge placed at the mean."
762,1,"['random variables', 'variability', 'variables', 'random']", Random variables,seg_27,3.4.2 variability in random variables
763,1,['variability'], Random variables,seg_27,"suppose you ran the university bookstore. besides how much revenue you expect to generate, you might also want to know the volatility (variability) in your revenue."
764,1,"['sum of squared', 'case', 'set', 'random', 'data', 'random variable', 'mean', 'standard', 'standard deviation', 'expectation', 'data set', 'variance', 'deviation', 'probabilities', 'method', 'variability', 'deviations', 'variable', 'average']", Random variables,seg_27,"the variance and standard deviation can be used to describe the variability of a random variable. section 2.1.4 introduced a method for finding the variance and standard deviation for a data set. we first computed deviations from the mean (xi−µ), squared those deviations, and took an average to get the variance. in the case of a random variable, we again compute squared deviations. however, we take their sum weighted by their corresponding probabilities, just like we did for the expectation. this weighted sum of squared deviations equals the variance, and we calculate the standard deviation by taking the square root of the variance, just as we did in section 2.1.4."
765,1,"['probabilities', 'expected value', 'variance', 'outcomes']", Random variables,seg_27,"if x takes outcomes x1, ..., xk with probabilities p (x = x1), ..., p (x = xk) and expected value µ = e(x), then the variance of x, denoted by v ar(x) or the symbol σ2, is"
766,1,"['deviation', 'standard', 'standard deviation', 'variance']", Random variables,seg_27,"the standard deviation of x, labeled σ, is the square root of the variance."
767,1,"['deviation', 'statistics', 'expected value', 'standard', 'standard deviation', 'variance']", Random variables,seg_27,"compute the expected value, variance, and standard deviation of x, the revenue of a single statistics student for the bookstore."
768,1,"['outcome', 'results', 'table']", Random variables,seg_27,"it is useful to construct a table that holds computations for each outcome separately, then add up the results."
769,1,"['expected value', 'variance', 'table']", Random variables,seg_27,"thus, the expected value is µ = 117.85, which we computed earlier. the variance can be constructed by extending this table:"
770,1,"['deviation', 'standard', 'standard deviation', 'variance']", Random variables,seg_27,"the variance of x is σ2 = 3659.3, which means the standard deviation is σ = √3659.3 = $60.49."
771,0,[], Random variables,seg_27,"the bookstore also offers a chemistry textbook for $159 and a book supplement for $41. from past experience, they know about 25% of chemistry students just buy the textbook while 60% buy both the textbook and supplement.51"
772,0,[], Random variables,seg_27,(a) what proportion of students don’t buy either book? assume no students buy the supplement
773,0,[], Random variables,seg_27,without the textbook.
774,1,"['distribution', 'probability distribution', 'probability']", Random variables,seg_27,(b) let y represent the revenue from a single student. write out the probability distribution of
775,1,"['table', 'probability', 'associated', 'outcome']", Random variables,seg_27,"y , i.e. a table for each outcome and its associated probability."
776,0,[], Random variables,seg_27,(c) compute the expected revenue from a single chemistry student.
777,1,"['deviation', 'standard', 'variability', 'associated', 'standard deviation']", Random variables,seg_27,(d) find the standard deviation to describe the variability associated with the revenue from a
778,1,"['linear', 'random variables', 'combinations', 'variables', 'linear combinations', 'random']", Random variables,seg_27,3.4.3 linear combinations of random variables
779,1,"['combination', 'variables', 'loss', 'variable']", Random variables,seg_27,"so far, we have thought of each variable as being a complete story in and of itself. sometimes it is more appropriate to use a combination of variables. for instance, the amount of time a person spends commuting to work each week can be broken down into several daily commutes. similarly, the total gain or loss in a stock portfolio is the sum of the gains and losses in its components."
780,0,[], Random variables,seg_27,"john travels to work five days a week. we will use x1 to represent his travel time on monday, x2 to represent his travel time on tuesday, and so on. write an equation using x1, ..., x5 that represents his travel time for the week, denoted by w ."
781,0,[], Random variables,seg_27,his total weekly travel time is the sum of the five daily values:
782,0,[], Random variables,seg_27,breaking the weekly travel time w into pieces provides a framework for understanding each source of randomness and is useful for modeling w .
783,1,['average'], Random variables,seg_27,it takes john an average of 18 minutes each day to commute to work. what would you expect his average commute time to be for the week?
784,1,"['expected value', 'average']", Random variables,seg_27,"we were told that the average (i.e. expected value) of the commute time is 18 minutes per day: e(xi) = 18. to get the expected time for the sum of the five days, we can add up the expected time for each individual day:"
785,1,"['random variables', 'variables', 'random variable', 'variable', 'expectation', 'random']", Random variables,seg_27,"the expectation of the total time is equal to the sum of the expected individual times. more generally, the expectation of a sum of random variables is always the sum of the expectation for each random variable."
786,0,[], Random variables,seg_27,"elena is selling a tv at a cash auction and also intends to buy a toaster oven in the auction. if x represents the profit for selling the tv and y represents the cost of the toaster oven, write an equation that represents the net change in elena’s cash.52"
787,0,[], Random variables,seg_27,"based on past auctions, elena figures she should expect to make about $175 on the tv and pay about $23 for the toaster oven. in total, how much should she expect to make or spend?53"
788,0,[], Random variables,seg_27,would you be surprised if john’s weekly commute wasn’t exactly 90 minutes or if elena didn’t make
789,0,[], Random variables,seg_27,54 exactly $152? explain.
790,1,"['linear', 'random variables', 'combinations', 'variables', 'linear combinations', 'random', 'average']", Random variables,seg_27,"two important concepts concerning combinations of random variables have so far been introduced. first, a final value can sometimes be described as the sum of its parts in an equation. second, intuition suggests that putting the individual average values into this equation gives the average value we would expect in total. this second point needs clarification – it is guaranteed to be true in what are called linear combinations of random variables."
791,1,"['linear', 'random variables', 'linear combination', 'combination', 'variables', 'random']", Random variables,seg_27,a linear combination of two random variables x and y is a fancy phrase to describe a combination
792,1,"['random variables', 'variables', 'random variable', 'variable', 'random', 'coefficient']", Random variables,seg_27,"where a and b are some fixed and known numbers. for john’s commute time, there were five random variables – one for each work day – and each random variable could be written as having a fixed coefficient of 1:"
793,1,"['loss', 'random variable', 'variable', 'random', 'coefficient']", Random variables,seg_27,"for elena’s net gain or loss, the x random variable had a coefficient of +1 and the y random variable had a coefficient of -1."
794,1,"['linear', 'random variables', 'linear combination', 'combination', 'combinations', 'variables', 'nonlinear', 'cases', 'random variable', 'variable', 'mean', 'random', 'average']", Random variables,seg_27,"when considering the average of a linear combination of random variables, it is safe to plug in the mean of each random variable and then compute the final result. for a few examples of nonlinear combinations of random variables – cases where we cannot simply plug in the means – see"
795,0,[], Random variables,seg_27,55 the footnote.
796,1,"['linear', 'random variables', 'linear combination', 'combination', 'variables', 'random']", Random variables,seg_27,"if x and y are random variables, then a linear combination of the random variables is given by"
797,1,"['linear', 'random variables', 'linear combination', 'combination', 'variables', 'random variable', 'variable', 'random', 'average']", Random variables,seg_27,"where a and b are some fixed numbers. to compute the average value of a linear combination of random variables, plug in the average of each individual random variable and compute the result:"
798,1,"['expected value', 'mean']", Random variables,seg_27,"recall that the expected value is the same as the mean, e.g. e(x) = µx ."
799,0,[], Random variables,seg_27,"leonard has invested $6000 in caterpillar inc (stock ticker: cat) and $2000 in exxon mobil corp (xom). if x represents the change in caterpillar’s stock next month and y represents the change in exxon mobil’s stock next month, write an equation that describes how much money will be made or lost in leonard’s stocks for the month."
800,0,[], Random variables,seg_27,"for simplicity, we will suppose x and y are not in percents but are in decimal form (e.g. if caterpillar’s stock increases 1%, then x = 0.01; or if it loses 1%, then x = −0.01). then we can write an equation for leonard’s gain as"
801,1,['loss'], Random variables,seg_27,"if we plug in the change in the stock value for x and y , this equation gives the change in value of leonard’s stock portfolio for the month. a positive value represents a gain, and a negative value represents a loss."
802,0,[], Random variables,seg_27,"caterpillar stock has recently been rising at 2.0% and exxon mobil’s at 0.2% per month, respectively."
803,0,[], Random variables,seg_27,56 compute the expected change in leonard’s stock portfolio for next month.
804,1,['loss'], Random variables,seg_27,"you should have found that leonard expects a positive gain in guided practice 3.66. however, would you be surprised if he actually had a loss this month?57"
805,1,"['linear', 'random variables', 'variability', 'variables', 'combinations', 'linear combinations', 'random']", Random variables,seg_27,3.4.4 variability in linear combinations of random variables
806,1,"['linear', 'random variables', 'linear combination', 'quantitative', 'combination', 'variables', 'uncertainty', 'data', 'loss', 'associated', 'random', 'outcome', 'average', 'vary']", Random variables,seg_27,"quantifying the average outcome from a linear combination of random variables is helpful, but it is also important to have some sense of the uncertainty associated with the total outcome of that combination of random variables. the expected net gain or loss of leonard’s stock portfolio was considered in guided practice 3.66. however, there was no quantitative discussion of the volatility of this portfolio. for instance, while the average monthly gain might be about $124 according to the data, that gain is not guaranteed. figure 3.22 shows the monthly changes in a portfolio like leonard’s during a three year period. the gains and losses vary widely, and quantifying these fluctuations is important when investing in stocks."
807,0,[], Random variables,seg_27,−1000 −500 0 500 1000 monthly returns over 3 years
808,0,[], Random variables,seg_27,"figure 3.22: the change in a portfolio like leonard’s for 36 months, where $6000 is in caterpillar’s stock and $2000 is in exxon mobil’s."
809,1,"['deviation', 'independent', 'uncertainty', 'cases', 'associated', 'standard', 'standard deviation', 'variance', 'variances']", Random variables,seg_27,"just as we have done in many previous cases, we use the variance and standard deviation to describe the uncertainty associated with leonard’s monthly returns. to do so, the variances of each stock’s monthly return will be useful, and these are shown in figure 3.23. the stocks’ returns are nearly independent."
810,1,"['linear', 'random variables', 'linear combination', 'method', 'coefficients', 'variables', 'combination', 'uncertainty', 'probability theory', 'probability', 'random', 'variance', 'variances']", Random variables,seg_27,here we use an equation from probability theory to describe the uncertainty of leonard’s monthly returns; we leave the proof of this method to a dedicated probability course. the variance of a linear combination of random variables can be computed by plugging in the variances of the individual random variables and squaring the coefficients of the random variables:
811,1,"['random variables', 'independent', 'variables', 'random', 'variance']", Random variables,seg_27,"it is important to note that this equality assumes the random variables are independent; if independence doesn’t hold, then a modification to this equation would be required that we leave as a topic for a future course to cover. this equation can be used to compute the variance of leonard’s monthly return:"
812,1,"['deviation', 'variance', 'standard', 'standard deviation', 'average']", Random variables,seg_27,"the standard deviation is computed as the square root of the variance: √213, 600 = $463. while an average monthly return of $124 on an $8000 investment is nothing to scoff at, the monthly returns are so volatile that leonard should not expect this income to be very stable."
813,1,"['deviation', 'standard', 'standard deviation', 'variance']", Random variables,seg_27,mean (x̄) standard deviation (s) variance (s2) cat 0.0204 0.0757 0.0057 xom 0.0025 0.0455 0.0021
814,1,"['deviation', 'sample', 'estimated', 'statistics', 'data', 'mean', 'sample statistics', 'standard', 'standard deviation', 'variance']", Random variables,seg_27,"figure 3.23: the mean, standard deviation, and variance of the cat and xom stocks. these statistics were estimated from historical stock data, so notation used for sample statistics has been used."
815,1,"['linear', 'random variables', 'linear combination', 'combination', 'variables', 'random', 'variance', 'variances']", Random variables,seg_27,"the variance of a linear combination of random variables may be computed by squaring the constants, substituting in the variances for the random variables, and computing the result:"
816,1,"['deviation', 'linear', 'random variables', 'random', 'linear combination', 'independent', 'variables', 'combination', 'standard', 'standard deviation', 'variance']", Random variables,seg_27,this equation is valid as long as the random variables are independent of each other. the standard deviation of the linear combination may be found by taking the square root of the variance.
817,1,"['deviation', 'uncertainty', 'standard', 'standard deviation']", Random variables,seg_27,suppose john’s daily commute has a standard deviation of 4 minutes. what is the uncertainty in his total commute time for the week?
818,0,[], Random variables,seg_27,the expression for john’s commute time was
819,1,"['coefficient', 'variance']", Random variables,seg_27,"each coefficient is 1, and the variance of each day’s time is 42 = 16. thus, the variance of the total weekly commute time is"
820,1,"['deviation', 'deviation ']", Random variables,seg_27,standard deviation = √variance = √80 = 8.94
821,1,"['deviation', 'standard deviation', 'standard']", Random variables,seg_27,the standard deviation for john’s weekly work commute time is about 9 minutes.
822,1,['independent'], Random variables,seg_27,the computation in example 3.68 relied on an important assumption: the commute time for each day is independent of the time on other days of that week. do you think this is valid? explain.58
823,1,"['deviation', 'independent', 'standard deviations', 'variability', 'deviations', 'associated', 'standard', 'standard deviation']", Random variables,seg_27,consider elena’s two auctions from guided practice 3.62 on page 120. suppose these auctions are approximately independent and the variability in auction prices associated with the tv and toaster oven can be described using standard deviations of $25 and $8. compute the standard deviation of elena’s net gain.59
824,1,"['linear', 'linear combination', 'combination', 'variability', 'expected value', 'coefficient', 'coefficients']", Random variables,seg_27,"consider again guided practice 3.70. the negative coefficient for y in the linear combination was eliminated when we squared the coefficients. this generally holds true: negatives in a linear combination will have no impact on the variability computed for a linear combination, but they do impact the expected value computations."
825,1,"['discrete', 'cases', 'variable', 'continuous', 'outcome', 'numerical']", Continuous distributions,seg_29,"so far in this chapter we’ve discussed cases where the outcome of a variable is discrete. in this section, we consider a context where the outcome is a continuous numerical variable."
826,1,"['bins', 'hollow histograms', 'data', 'histograms']", Continuous distributions,seg_29,figure 3.24 shows a few different hollow histograms for the heights of us adults. how does changing the number of bins allow you to make different interpretations of the data?
827,1,"['sample', 'bin', 'bins', 'mean']", Continuous distributions,seg_29,"adding more bins provides greater detail. this sample is extremely large, which is why much smaller bins still work well. usually we do not use so many bins with smaller sample sizes since small counts per bin mean the bin heights are very volatile."
828,0,[], Continuous distributions,seg_29,height (cm) height (cm) ycneuqerf
829,1,"['bin', 'hollow histograms', 'bin widths', 'histograms', 'varying']", Continuous distributions,seg_29,figure 3.24: four hollow histograms of us adults heights with varying bin widths.
830,1,['sample'], Continuous distributions,seg_29,what proportion of the sample is between 180 cm and 185 cm tall (about 5’11” to 6’1”)?
831,1,"['sample size', 'sample', 'bins', 'estimate', 'range', 'probability']", Continuous distributions,seg_29,"we can add up the heights of the bins in the range 180 cm and 185 and divide by the sample size. for instance, this can be done with the two shaded bins shown in figure 3.25. the two bins in this region have counts of 195,307 and 156,239 people, resulting in the following estimate of the probability:"
832,1,['range'], Continuous distributions,seg_29,this fraction is the same as the proportion of the histogram’s area that falls in the range 180 to 185 cm.
833,1,"['bin', 'histogram']", Continuous distributions,seg_29,figure 3.25: a histogram with bin sizes of 2.5 cm. the shaded region represents individuals with heights between 180 and 185 cm.
834,1,"['continuous distributions', 'continuous', 'histograms', 'distributions']", Continuous distributions,seg_29,3.5.1 from histograms to continuous distributions
835,1,"['bins', 'curve', 'plot', 'histogram', 'hollow histogram', 'variable', 'population', 'continuous', 'numerical']", Continuous distributions,seg_29,"examine the transition from a boxy hollow histogram in the top-left of figure 3.24 to the much smoother plot in the lower-right. in this last plot, the bins are so slim that the hollow histogram is starting to resemble a smooth curve. this suggests the population height as a continuous numerical variable might best be explained by a curve that represents the outline of extremely slim bins."
836,1,"['sample', 'density function', 'curve', 'probability density function', 'histogram', 'distribution', 'probability', 'function']", Continuous distributions,seg_29,"this smooth curve represents a probability density function (also called a density or distribution), and such a curve is shown in figure 3.26 overlaid on a histogram of the sample. a density has a special property: the total area under the density’s curve is 1."
837,1,"['distribution', 'probability distribution', 'continuous probability distribution', 'probability', 'continuous']", Continuous distributions,seg_29,figure 3.26: the continuous probability distribution of heights for us adults.
838,1,"['probabilities', 'continuous distributions', 'continuous', 'distributions']", Continuous distributions,seg_29,3.5.2 probabilities from continuous distributions
839,0,[], Continuous distributions,seg_29,we computed the proportion of individuals with heights 180 to 185 cm in example 3.72 as a fraction:
840,0,[], Continuous distributions,seg_29,number of people between 180 and 185
841,1,"['sample size', 'sample']", Continuous distributions,seg_29,total sample size
842,1,"['curve', 'probability']", Continuous distributions,seg_29,"we found the number of people with heights between 180 and 185 cm by determining the fraction of the histogram’s area in this region. similarly, we can use the area in the shaded region under the curve to find a probability (with the help of a computer):"
843,0,[], Continuous distributions,seg_29,p (height between 180 and 185) = area between 180 and 185 = 0.1157
844,1,"['estimate', 'probability']", Continuous distributions,seg_29,the probability that a randomly selected person is between 180 and 185 cm is 0.1157. this is very close to the estimate from example 3.72: 0.1172.
845,1,"['plot', 'population']", Continuous distributions,seg_29,figure 3.27: density for heights in the us adult population with the area between 180 and 185 cm shaded. compare this plot with figure 3.25.
846,1,['probability'], Continuous distributions,seg_29,three us adults are randomly selected. the probability a single adult is between 180 and 185 cm
847,1,['probability'], Continuous distributions,seg_29,(a) what is the probability that all three are between 180 and 185 cm tall?
848,1,['probability'], Continuous distributions,seg_29,(b) what is the probability that none are between 180 and 185 cm?
849,1,['probability'], Continuous distributions,seg_29,what is the probability that a randomly selected person is exactly 180 cm? assume you can measure perfectly.
850,1,['probability'], Continuous distributions,seg_29,"this probability is zero. a person might be close to 180 cm, but not exactly 180 cm tall. this also makes sense with the definition of probability as area; there is no area captured between 180 cm and 180 cm."
851,1,['random'], Continuous distributions,seg_29,suppose a person’s height is rounded to the nearest centimeter. is there a chance that a random person’s measured height will be 180 cm?61
852,1,"['poisson', 'statistical inference', 'geometric distribution', 'negative binomial distribution', 'data', 'distribution', 'negative binomial', 'normal', 'binomial', 'poisson distribution', 'binomial distribution', 'statistical', 'geometric', 'normal distribution', 'distributions']",Chapter  Distributions of random variables,seg_31,"4.1 normal distribution 4.2 geometric distribution 4.3 binomial distribution 4.4 negative binomial distribution 4.5 poisson distribution in this chapter, we discuss statistical distributions that frequently arise in the context of data analysis or statistical inference. we start with the normal distribution in the first section, which is used frequently in later chapters of this book. the remaining sections will occasionally be referenced but may be considered optional for the content in this book. for videos, slides, and other resources, please visit www.openintro.org/os"
853,1,"['curve', 'variables', 'unimodal', 'symmetric', 'statistics', 'distribution', 'normal', 'scores', 'normal distribution', 'distributions']", Normal distribution,seg_33,"among all the distributions we see in practice, one is overwhelmingly the most common. the symmetric, unimodal, bell curve is ubiquitous throughout statistics. indeed it is so common, that people often know it as the normal curve or normal distribution,1 shown in figure 4.1. variables such as sat scores and heights of us adult males closely follow the normal distribution."
854,1,"['curve', 'normal']", Normal distribution,seg_33,figure 4.1: a normal curve.
855,1,"['variables', 'statistics', 'data', 'distribution', 'normal', 'normal distribution']", Normal distribution,seg_33,"many variables are nearly normal, but none are exactly normal. thus the normal distribution, while not perfect for any single problem, is very useful for a variety of problems. we will use it in data exploration and to solve important problems in statistics."
856,1,"['distribution', 'model', 'normal', 'normal distribution']", Normal distribution,seg_33,4.1.1 normal distribution model
857,1,"['deviation', 'model', 'normal distributions', 'curve', 'parameters', 'unimodal', 'symmetric', 'distribution', 'adjusted', 'normal', 'mean', 'standard', 'standard deviation', 'normal distribution', 'distributions']", Normal distribution,seg_33,"the normal distribution always describes a symmetric, unimodal, bell-shaped curve. however, these curves can look different depending on the details of the model. specifically, the normal distribution model can be adjusted using two parameters: mean and standard deviation. as you can probably guess, changing the mean shifts the bell curve to the left or right, while changing the standard deviation stretches or constricts the curve. figure 4.2 shows the normal distribution with mean 0 and standard deviation 1 in the left panel and the normal distributions with mean 19 and standard deviation 4 in the right panel. figure 4.3 shows these distributions on the same axis."
858,1,"['distribution', 'normal', 'normal distribution']", Normal distribution,seg_33,"figure 4.2: both curves represent the normal distribution. however, they differ in their center and spread."
859,1,"['deviation', 'distribution', 'normal', 'mean', 'standard', 'standard deviation', 'normal distribution', 'distributions']", Normal distribution,seg_33,"if a normal distribution has mean µ and standard deviation σ, we may write the distribution as n(µ, σ). the two distributions in figure 4.3 may be written as"
860,1,"['deviation', 'standard normal distribution', 'parameters', 'standard normal', 'distribution', 'normal', 'mean', 'standard', 'standard deviation', 'normal distribution']", Normal distribution,seg_33,"because the mean and standard deviation describe a normal distribution exactly, they are called the distribution’s parameters. the normal distribution with mean µ = 0 and standard deviation σ = 1 is called the standard normal distribution."
861,1,"['normal distributions', 'normal', 'distributions']", Normal distribution,seg_33,figure 4.3: the normal distributions shown in figure 4.2 but plotted together and on the same scale.
862,1,"['deviation', 'distribution', 'normal', 'mean', 'standard', 'standard deviation', 'normal distribution']", Normal distribution,seg_33,"write down the short-hand for a normal distribution with2 (a) mean 5 and standard deviation 3, (b) mean -100 and standard deviation 10, and (c) mean 2 and standard deviation 9."
863,0,[], Normal distribution,seg_33,4.1.2 standardizing with z-scores
864,1,"['standardized', 'data']", Normal distribution,seg_33,"we often want to put data onto a standardized scale, which can make comparisons more reasonable."
865,1,"['deviation', 'distribution', 'normal', 'mean', 'scores', 'standard', 'standard deviation']", Normal distribution,seg_33,table 4.4 shows the mean and standard deviation for total scores on the sat and act. the distribution of sat and act scores are both nearly normal. suppose ann scored 1300 on her sat and tom scored 24 on his act. who performed better?
866,1,"['deviation', 'standard deviations', 'deviations', 'mean', 'standard', 'standard deviation', 'average']", Normal distribution,seg_33,"we use the standard deviation as a guide. ann is 1 standard deviation above average on the sat: 1100 + 200 = 1300. tom is 0.5 standard deviations above the mean on the act: 21 + 0.5× 6 = 24. in figure 4.5, we can see that ann tends to do better with respect to everyone else than tom did, so her score was better."
867,1,['mean'], Normal distribution,seg_33,sat act mean 1100 21 sd 200 6
868,1,"['deviation', 'mean', 'standard', 'standard deviation']", Normal distribution,seg_33,figure 4.4: mean and standard deviation for the sat and act.
869,1,"['deviation', 'method', 'standard deviations', 'observations', 'standardization', 'observation', 'distribution', 'deviations', 'normal', 'mean', 'standard', 'standard deviation']", Normal distribution,seg_33,"example 4.2 used a standardization technique called a z-score, a method most commonly employed for nearly normal observations but that may be used with any distribution. the z-score of an observation is defined as the number of standard deviations it falls above or below the mean. if the observation is one standard deviation above the mean, its z-score is 1. if it is 1.5 standard deviations below the mean, then its z-score is -1.5. if x is an observation from a distribution n(µ, σ), we define the z-score mathematically as"
870,0,[], Normal distribution,seg_33,"using µsat = 1100, σsat = 200, and xann = 1300, we find ann’s z-score:"
871,1,"['scores', 'distributions']", Normal distribution,seg_33,figure 4.5: ann’s and tom’s scores shown against the sat and act distributions.
872,1,"['deviation', 'standard deviations', 'observation', 'distribution', 'deviations', 'mean', 'standard', 'standard deviation']", Normal distribution,seg_33,the z-score of an observation is the number of standard deviations it falls above or below the mean. we compute the z-score for an observation x that follows a distribution with mean µ and standard deviation σ using
873,1,"['deviation', 'mean', 'standard', 'standard deviation']", Normal distribution,seg_33,"3 use tom’s act score, 24, along with the act mean and standard deviation to find his z-score."
874,1,"['mean', 'observation']", Normal distribution,seg_33,"observations above the mean always have positive z-scores, while those below the mean always have negative z-scores. if an observation is equal to the mean, such as an sat score of 1100, then the z-score is 0."
875,1,"['random', 'standard deviations', 'deviations', 'random variable', 'variable', 'mean', 'standard']", Normal distribution,seg_33,"let x represent a random variable from n(µ = 3, σ = 2), and suppose we observe x = 5.19. (a) find the z-score of x. (b) use the z-score to determine how many standard deviations above or below the mean x falls.4"
876,1,"['distribution', 'normal', 'mean', 'standard', 'normal distribution']", Normal distribution,seg_33,head lengths of brushtail possums follow a normal distribution with mean 92.6 mm and standard
877,1,['deviation'], Normal distribution,seg_33,5 deviation 3.6 mm. compute the z-scores for possums with head lengths of 95.4 mm and 85.8 mm.
878,1,"['absolute value', 'symmetric', 'observations', 'observation', 'distribution']", Normal distribution,seg_33,we can use z-scores to roughly identify which observations are more unusual than others. an observation x1 is said to be more unusual than another observation x2 if the absolute value of its zscore is larger than the absolute value of the other observation’s z-score: |z1| > |z2|. this technique is especially insightful when a distribution is symmetric.
879,1,['observations'], Normal distribution,seg_33,which of the observations in guided practice 4.5 is more unusual?6
880,1,"['tail areas', 'tail']", Normal distribution,seg_33,4.1.3 finding tail areas
881,1,"['curve', 'percentage', 'percentile', 'statistics', 'cases', 'scores', 'tail areas', 'tail', 'distributions']", Normal distribution,seg_33,"it’s very useful in statistics to be able to identify tail areas of distributions. for instance, what fraction of people have an sat score below ann’s score of 1300? this is the same as the percentile ann is at, which is the percentage of cases that have lower scores than ann. we can visualize such a tail area like the curve and shading shown in figure 4.6."
882,0,[], Normal distribution,seg_33,figure 4.6: the area to the left of z represents the fraction of people who scored lower than ann.
883,0,[], Normal distribution,seg_33,"there are many techniques for doing this, and we’ll discuss three of the options."
884,1,"['deviation', 'statistical', 'tail', 'mean', 'deviation ', 'standard', 'standard deviation', 'test']", Normal distribution,seg_33,"1. the most common approach in practice is to use statistical software. for example, in the program r, we could find the area shown in figure 4.6 using the following command, which takes in the z-score and returns the lower tail area: .....> pnorm(1) .....[1] 0.8413447 according to this calculation, the region shaded that is below 1300 represents the proportion 0.841 (84.1%) of sat test takers who had z-scores below z = 1. more generally, we can also specify the cutoff explicitly if we also note the mean and standard deviation: .....> pnorm(1300, mean = 1100, sd = 200) .....[1] 0.8413447"
885,0,[], Normal distribution,seg_33,"there are many other software options, such as python or sas; even spreadsheet programs such as excel and google sheets support these calculations."
886,1,"['distribution', 'normal', 'tail areas', 'tail', 'normal distribution']", Normal distribution,seg_33,"2. a common strategy in classrooms is to use a graphing calculator, such as a ti or casio calculator. these calculators require a series of button presses that are less concisely described. you can find instructions on using these calculators for finding tail areas of a normal distribution in the openintro video library:"
887,0,[], Normal distribution,seg_33,www.openintro.org/videos
888,1,"['table', 'probability table', 'probability', 'tail areas', 'tail']", Normal distribution,seg_33,3. the last option for finding tail areas is to use what’s called a probability table; these are occasionally used in classrooms but rarely in practice. appendix c.1 contains such a table and a guide for how to use it.
889,1,"['statistics', 'distribution', 'test statistics', 'normal', 'test', 'normal distribution']", Normal distribution,seg_33,"we will solve normal distribution problems in this section by always first finding the z-score. the reason is that we will encounter close parallels called test statistics beginning in chapter 5; these are, in many instances, an equivalent of a z-score."
890,1,"['normal', 'probability']", Normal distribution,seg_33,4.1.4 normal probability examples
891,1,"['model', 'normal', 'scores']", Normal distribution,seg_33,"cumulative sat scores are approximated well by a normal model, n(µ = 1100, σ = 200)."
892,1,"['scores', 'probability']", Normal distribution,seg_33,"shannon is a randomly selected sat taker, and nothing is known about shannon’s sat aptitude. what is the probability shannon scores at least 1190 on her sats?"
893,1,"['distribution', 'normal', 'scores', 'tail', 'normal distribution']", Normal distribution,seg_33,"first, always draw and label a picture of the normal distribution. (drawings need not be exact to be useful.) we are interested in the chance she scores above 1190, so we shade this upper tail:"
894,1,"['curve', 'standard deviations', 'deviations', 'mean', 'standard']", Normal distribution,seg_33,"the picture shows the mean and the values at 2 standard deviations above and below the mean. the simplest way to find the shaded area under the curve makes use of the z-score of the cutoff value. with µ = 1100, σ = 200, and the cutoff value x = 1190, the z-score is computed as"
895,1,"['tail', 'statistical', 'method']", Normal distribution,seg_33,"using statistical software (or another preferred method), we can area left of z = 0.45 as 0.6736. to find the area above z = 0.45, we compute one minus the area of the lower tail:"
896,1,"['scores', 'probability']", Normal distribution,seg_33,the probability shannon scores at least 1190 on the sat is 0.3264.
897,1,"['curve', 'estimate', 'normal', 'probability']", Normal distribution,seg_33,"for any normal probability situation, always always always draw and label the normal curve and shade the area of interest first. the picture will provide an estimate of the probability. after drawing a figure to represent the situation, identify the z-score for the value of interest."
898,1,"['curve', 'normal', 'scores', 'probability']", Normal distribution,seg_33,"if the probability of shannon scoring at least 1190 is 0.3264, then what is the probability she scores less than 1190? draw the normal curve representing this exercise, shading the lower region instead of the upper one.7"
899,1,['percentile'], Normal distribution,seg_33,edward earned a 1030 on his sat. what is his percentile?
900,1,"['percentile', 'scores']", Normal distribution,seg_33,"first, a picture is needed. edward’s percentile is the proportion of people who do not get as high as a 1030. these are the scores to the left of 1030."
901,1,"['deviation', 'tail', 'mean', 'standard', 'standard deviation']", Normal distribution,seg_33,"identifying the mean µ = 1100, the standard deviation σ = 200, and the cutoff for the tail area x = 1030 makes it easy to compute the z-score:"
902,1,"['percentile', 'tail', 'statistical']", Normal distribution,seg_33,"using statistical software, we get a tail area of 0.3632. edward is at the 36th percentile."
903,1,['results'], Normal distribution,seg_33,use the results of example 4.9 to compute the proportion of sat takers who did better than edward.
904,0,[], Normal distribution,seg_33,8 also draw a new picture.
905,0,[], Normal distribution,seg_33,"many software programs return the area to the left when given a z-score. if you would like the area to the right, first find the area to the left and then subtract this amount from one."
906,1,"['percentile', 'percent']", Normal distribution,seg_33,stuart earned an sat score of 1500. draw a picture for each part. (a) what is his percentile? (b) what percent of sat takers did better than stuart?9
907,1,"['deviation', 'sample', 'normal', 'mean', 'standard', 'standard deviation']", Normal distribution,seg_33,"based on a sample of 100 men, the heights of male adults in the us is nearly normal with mean 70.0” and standard deviation 3.3”."
908,1,['percentile'], Normal distribution,seg_33,"mike is 5’7” and jose is 6’4”, and they both live in the us. (a) what is mike’s height percentile? (b) what is jose’s height percentile?"
909,0,[], Normal distribution,seg_33,10 also draw one picture for each part.
910,1,"['tail', 'percentile', 'observation']", Normal distribution,seg_33,the last several problems have focused on finding the percentile (or upper tail) for a particular observation. what if you would like to know the observation corresponding to a particular percentile?
911,1,['percentile'], Normal distribution,seg_33,erik’s height is at the 40th percentile. how tall is he?
912,0,[], Normal distribution,seg_33,"as always, first draw the picture."
913,1,"['tail probability', 'percentile', 'case', 'observation', 'probability', 'associated', 'tail']", Normal distribution,seg_33,"in this case, the lower tail probability is known (0.40), which can be shaded on the diagram. we want to find the observation that corresponds to this value. as a first step in this direction, we determine the z-score associated with the 40th percentile. using software, we can obtain the corresponding z-score of about -0.25."
914,1,"['parameters', 'set', 'population']", Normal distribution,seg_33,"knowing zerik = −0.25 and the population parameters µ = 70 and σ = 3.3 inches, the z-score formula can be set up to determine erik’s unknown height, labeled xerik :"
915,0,[], Normal distribution,seg_33,"solving for xerik yields a height of 69.18 inches. that is, erik is about 5’9”."
916,1,['percentile'], Normal distribution,seg_33,what is the adult male height at the 82nd percentile?
917,0,[], Normal distribution,seg_33,"again, we draw the figure first."
918,1,"['deviation', 'percentile', 'mean', 'standard', 'standard deviation']", Normal distribution,seg_33,"next, we want to find the z-score at the 82nd percentile, which will be a positive value and can be found using software as z = 0.92. finally, the height x is found using the z-score formula with the known mean µ, standard deviation σ, and z-score z = 0.92:"
919,1,['percentile'], Normal distribution,seg_33,this yields 73.04 inches or about 6’1” as the height at the 82nd percentile.
920,1,"['percentile', 'scores']", Normal distribution,seg_33,"11 the sat scores follow n(1100, 200). (a) what is the 95th percentile for sat scores? (b) what is the 97.5th percentile for sat scores?"
921,1,['probability'], Normal distribution,seg_33,"adult male heights follow n(70.0”, 3.3”).12 (a) what is the probability that a randomly selected male adult is at least 6’2” (74 inches)? (b) what is the probability that a male adult is shorter than 5’9” (69 inches)?"
922,1,"['probability', 'random']", Normal distribution,seg_33,what is the probability that a random adult male is between 5’9” and 6’2”?
923,1,['tail'], Normal distribution,seg_33,"these heights correspond to 69 inches and 74 inches. first, draw the figure. the area of interest is no longer an upper or lower tail."
924,1,"['curve', 'tails']", Normal distribution,seg_33,"the total area under the curve is 1. if we find the area of the two tails that are not shaded (from guided practice 4.16, these areas are 0.3821 and 0.1131), then we can find the middle area:"
925,1,['probability'], Normal distribution,seg_33,"that is, the probability of being between 5’9” and 6’2” is 0.5048."
926,1,"['scores', 'percent']", Normal distribution,seg_33,"sat scores follow n(1100, 200). what percent of sat takers get between 1100 and 1400?13"
927,1,['percent'], Normal distribution,seg_33,"adult male heights follow n(70.0”, 3.3”). what percent of adult males are between 5’5” and 5’7”?14"
928,1,"['standard deviations', 'estimate', 'range', 'distribution', 'deviations', 'normal', 'mean', 'probability', 'standard', 'normal distribution']", Normal distribution,seg_33,"here, we present a useful rule of thumb for the probability of falling within 1, 2, and 3 standard deviations of the mean in the normal distribution. this will be useful in a wide range of practical settings, especially when trying to make a quick estimate without a calculator or z-table."
929,1,"['probabilities', 'standard deviations', 'distribution', 'deviations', 'normal', 'mean', 'standard', 'normal distribution']", Normal distribution,seg_33,"figure 4.7: probabilities for falling within 1, 2, and 3 standard deviations of the mean in a normal distribution."
930,1,"['standard deviations', 'table', 'observations', 'probability table', 'distribution', 'deviations', 'normal', 'mean', 'probability', 'standard', 'normal distribution']", Normal distribution,seg_33,"use software, a calculator, or a probability table to confirm that about 68%, 95%, and 99.7% of observations fall within 1, 2, and 3, standard deviations of the mean in the normal distribution, respectively. for instance, first find the area that falls between z = −1 and z = 1, which should have an area of about 0.68. similarly there should be an area of about 0.95 between z = −2 and z = 2.15"
931,1,"['random', 'standard deviations', 'data', 'deviations', 'random variable', 'variable', 'normal', 'mean', 'probability', 'standard', 'normal random variable']", Normal distribution,seg_33,"it is possible for a normal random variable to fall 4, 5, or even more standard deviations from the mean. however, these occurrences are very rare if the data are nearly normal. the probability of being further than 4 standard deviations from the mean is about 1-in-15,000. for 5 and 6 standard deviations, it is about 1-in-2 million and 1-in-500 million, respectively."
932,1,"['deviation', 'model', 'normal', 'mean', 'scores', 'standard', 'standard deviation', 'test', 'percent']", Normal distribution,seg_33,sat scores closely follow the normal model with mean µ = 1100 and standard deviation σ = 200.16 (a) about what percent of test takers score 700 to 1500? (b) what percent score between 1100 and 1500?
933,1,"['geometric distribution', 'distribution', 'bernoulli', 'probability', 'bernoulli distribution', 'geometric', 'trial']", Geometric distribution,seg_35,"how long should we expect to flip a coin until it turns up heads? or how many times should we expect to roll a die until we get a 1? these questions can be answered using the geometric distribution. we first formalize each trial – such as a single coin flip or die toss – using the bernoulli distribution, and then we combine these with our tools from probability (chapter 3) to construct the geometric distribution."
934,1,"['distribution', 'bernoulli', 'bernoulli distribution']", Geometric distribution,seg_35,4.2.1 bernoulli distribution
935,1,['states'], Geometric distribution,seg_35,"many health insurance plans in the united states have a deductible, where the insured individual is responsible for costs up to the deductible, and then the costs above the deductible are shared between the individual and insurance company for the remainder of the year."
936,1,"['probability of a success', 'failure', 'success', 'probability', 'trial']", Geometric distribution,seg_35,"suppose a health insurance company found that 70% of the people they insure stay below their deductible in any given year. each of these people can be thought of as a trial. we label a person a success if her healthcare costs do not exceed the deductible. we label a person a failure if she does exceed her deductible in the year. because 70% of the individuals will not hit their deductible, we denote the probability of a success as p = 0.7. the probability of a failure is sometimes denoted with q = 1− p, which would be 0.3 for the insurance example."
937,1,"['outcomes', 'failure', 'trial', 'bernoulli', 'random variable', 'variable', 'success', 'random', 'outcome', 'bernoulli random variable']", Geometric distribution,seg_35,"when an individual trial only has two possible outcomes, often labeled as success or failure, it is called a bernoulli random variable. we chose to label a person who does not hit her deductible as a “success” and all others as “failures”. however, we could just as easily have reversed these labels. the mathematical framework we will build does not depend on which outcome is labeled a success and which a failure, as long as we are consistent."
938,1,"['random variables', 'variables', 'failure', 'data', 'trials', 'success', 'random']", Geometric distribution,seg_35,"bernoulli random variables are often denoted as 1 for a success and 0 for a failure. in addition to being convenient in entering data, it is also mathematically handy. suppose we observe ten trials:"
939,1,"['sample', 'sample mean', 'observations', 'mean']", Geometric distribution,seg_35,"then the sample proportion, p̂, is the sample mean of these observations:"
940,1,"['trials', 'successes']", Geometric distribution,seg_35,# of successes 1 + 1 + 1 + 0 + 1 + 0 + 0 + 1 + 1 + 0 p̂ = = = 0.6 # of trials 10
941,1,"['deviation', 'outcomes', 'random variables', 'random', 'variables', 'bernoulli', 'random variable', 'variable', 'mean', 'standard', 'standard deviation', 'numerical', 'bernoulli random variable']", Geometric distribution,seg_35,"this mathematical inquiry of bernoulli random variables can be extended even further. because 0 and 1 are numerical outcomes, we can define the mean and standard deviation of a bernoulli random variable. (see exercises 4.15 and 4.16.)"
942,1,"['deviation', 'random', 'bernoulli', 'variable', 'random variable', 'success', 'probability of success', 'mean', 'probability', 'standard', 'standard deviation', 'bernoulli random variable']", Geometric distribution,seg_35,"if x is a random variable that takes value 1 with probability of success p and 0 with probability 1− p, then x is a bernoulli random variable with mean and standard deviation"
943,1,"['random process', 'outcomes', 'failures', 'successes', 'failure', 'bernoulli', 'random variable', 'variable', 'success', 'random', 'process', 'numerical', 'bernoulli random variable']", Geometric distribution,seg_35,"in general, it is useful to think about a bernoulli random variable as a random process with only two outcomes: a success or failure. then we build our mathematical framework using the numerical labels 1 and 0 for successes and failures, respectively."
944,1,"['distribution', 'geometric distribution', 'geometric']", Geometric distribution,seg_35,4.2.2 geometric distribution
945,1,"['geometric distribution', 'distribution', 'trials', 'success', 'geometric']", Geometric distribution,seg_35,the geometric distribution is used to describe how many trials it takes to observe a success. let’s first look at an example.
946,1,"['case', 'cases', 'success', 'probability', 'random']", Geometric distribution,seg_35,"suppose we are working at the insurance company and need to find a case where the person did not exceed her (or his) deductible as a case study. if the probability a person will not exceed her deductible is 0.7 and we are drawing people at random, what are the chances that the first person will not have exceeded her deductible, i.e. be a success? the second person? the third? what about we pull n− 1 cases before we find the first success, i.e. the first success is the nth person? (if the first success is the fifth person, then we say n = 5.)"
947,1,['probability'], Geometric distribution,seg_35,the probability of stopping after the first person is just the chance the first person will not hit her (or his) deductible: 0.7. the probability the second person is the first to hit her deductible is
948,0,[], Geometric distribution,seg_35,p (second person is the first to hit deductible)
949,1,"['case', 'probability']", Geometric distribution,seg_35,"likewise, the probability it will be the third case is (0.3)(0.3)(0.7) = 0.063."
950,1,"['success', 'probability', 'failures']", Geometric distribution,seg_35,"if the first success is on the nth person, then there are n − 1 failures and finally 1 success, which corresponds to the probability (0.3)n−1(0.7). this is the same as (1− 0.7)n−1(0.7)."
951,1,"['case', 'independence', 'probability', 'random', 'geometric', 'success', 'geometric distribution', 'distribution', 'random variables', 'independent', 'variables', 'bernoulli', 'probability of success']", Geometric distribution,seg_35,"example 4.22 illustrates what the geometric distribution, which describes the waiting time until a success for independent and identically distributed (iid) bernoulli random variables. in this case, the independence aspect just means the individuals in the example don’t affect each other, and identical means they each have the same probability of success."
952,1,"['geometric distribution', 'distribution', 'geometric', 'exponentially']", Geometric distribution,seg_35,"the geometric distribution from example 4.22 is shown in figure 4.8. in general, the probabilities for a geometric distribution decrease exponentially fast."
953,1,"['trials', 'success']", Geometric distribution,seg_35,0.6 ytili 0.4 baborp 0.2 0.0 1 2 3 4 5 6 7 8 number of trials until a success for p = 0.7
954,1,"['geometric distribution', 'distribution', 'success', 'probability of success', 'probability', 'geometric']", Geometric distribution,seg_35,figure 4.8: the geometric distribution when the probability of success is p = 0.7.
955,1,"['deviation', 'distribution', 'trials', 'success', 'mean', 'standard', 'standard deviation', 'variance']", Geometric distribution,seg_35,"while this text will not derive the formulas for the mean (expected) number of trials needed to find the first success or the standard deviation or variance of this distribution, we present general formulas for each."
956,1,"['probability of a success', 'failure', 'success', 'probability', 'trial']", Geometric distribution,seg_35,"if the probability of a success in one trial is p and the probability of a failure is 1− p, then the probability of finding the first success in the nth trial is given by"
957,1,"['deviation', 'expected value', 'mean', 'standard', 'standard deviation', 'variance']", Geometric distribution,seg_35,"the mean (i.e. expected value), variance, and standard deviation of this wait time are given by"
958,1,"['expected value', 'mean']", Geometric distribution,seg_35,it is no accident that we use the symbol µ for both the mean and expected value. the mean and the expected value are one and the same.
959,1,"['probability of a success', 'geometric distribution', 'distribution', 'trials', 'success', 'probability', 'average', 'geometric']", Geometric distribution,seg_35,"it takes, on average, 1/p trials to get a success under the geometric distribution. this mathematical result is consistent with what we would expect intuitively. if the probability of a success is high (e.g. 0.8), then we don’t usually wait very long for a success: 1/0.8 = 1.25 trials on average. if the probability of a success is low (e.g. 0.1), then we would expect to view many trials before we see a success: 1/0.1 = 10 trials."
960,1,"['cases', 'case', 'probability']", Geometric distribution,seg_35,"the probability that a particular case would not exceed their deductible is said to be 0.7. if we were to examine cases until we found one that where the person did not hit her deductible, how many cases should we expect to check?17"
961,1,"['cases', 'success']", Geometric distribution,seg_35,what is the chance that we would find the first success within the first 3 cases?
962,1,"['sample', 'disjoint', 'independent', 'results', 'case', 'success', 'probability', 'population', 'outcomes']", Geometric distribution,seg_35,"this is the chance it is the first (n = 1), second (n = 2), or third (n = 3) case is the first success, which are three disjoint outcomes. because the individuals in the sample are randomly sampled from a large population, they are independent. we compute the probability of each case and add the separate results:"
963,1,"['successful', 'case', 'cases', 'probability']", Geometric distribution,seg_35,there is a probability of 0.973 that we would find a successful case within 3 cases.
964,0,[], Geometric distribution,seg_35,determine a more clever way to solve example 4.24. show that you get the same result.18
965,1,"['deviation', 'standard', 'standard deviation']", Geometric distribution,seg_35,"suppose a car insurer has determined that 88% of its drivers will not exceed their deductible in a given year. if someone at the company were to randomly draw driver files until they found one that had not exceeded their deductible, what is the expected number of drivers the insurance employee must check? what is the standard deviation of the number of driver files that must be drawn?"
966,1,"['deviation', 'success', 'probability', 'standard', 'standard deviation']", Geometric distribution,seg_35,"in this example, a success is again when someone will not exceed the insurance deductible, which has probability p = 0.88. the expected number of people to be checked is 1/p = 1/0.88 = 1.14 and the standard deviation is √(1− p)/p"
967,1,"['model', 'results', 'normal', 'experiments']", Geometric distribution,seg_35,"using the results from example 4.26, µ = 1.14 and σ = 0.39, would it be appropriate to use the normal model to find what proportion of experiments would end in 3 or fewer trials?19"
968,1,"['model', 'multiplication rule', 'independent', 'dependent', 'trial', 'independence', 'success', 'trials', 'probability', 'geometric', 'processes']", Geometric distribution,seg_35,"the independence assumption is crucial to the geometric distribution’s accurate description of a scenario. mathematically, we can see that to construct the probability of the success on the nth trial, we had to use the multiplication rule for independent processes. it is no simple task to generalize the geometric model for dependent trials."
969,1,"['geometric distribution', 'successes', 'distribution', 'trials', 'success', 'binomial', 'binomial distribution', 'geometric']", Binomial distribution,seg_37,"the binomial distribution is used to describe the number of successes in a fixed number of trials. this is different from the geometric distribution, which described the number of trials we must wait before we observe a success."
970,1,"['distribution', 'binomial distribution', 'binomial']", Binomial distribution,seg_37,4.3.1 the binomial distribution
971,0,[], Binomial distribution,seg_37,let’s again imagine ourselves back at the insurance agency where 70% of individuals do not exceed their deductible.
972,1,"['sample', 'random sample', 'random']", Binomial distribution,seg_37,"suppose the insurance agency is considering a random sample of four individuals they insure. what is the chance exactly one of them will exceed the deductible and the other three will not? let’s call the four people ariana (a), brittany (b), carlton (c), and damian (d) for convenience."
973,0,[], Binomial distribution,seg_37,let’s consider a scenario where one person exceeds the deductible:
974,1,"['cases', 'total probability', 'probability']", Binomial distribution,seg_37,"but there are three other scenarios: brittany, carlton, or damian could have been the one to exceed the deductible. in each of these cases, the probability is again (0.7)3(0.3)1. these four scenarios exhaust all the possible ways that exactly one of these four people could have exceeded the deductible, so the total probability is 4× (0.7)3(0.3)1 = 0.412."
975,1,['probability'], Binomial distribution,seg_37,verify that the scenario where brittany is the only one exceed the deductible has probability (0.7)3(0.3)1. 20
976,1,"['independent', 'probabilities', 'successes', 'distribution', 'bernoulli', 'success', 'trials', 'bernoulli trials', 'binomial', 'probability', 'binomial distribution', 'associated', 'probability of a success']", Binomial distribution,seg_37,"the scenario outlined in example 4.28 is an example of a binomial distribution scenario. the binomial distribution describes the probability of having exactly k successes in n independent bernoulli trials with probability of a success p (in example 4.28, n = 4, k = 3, p = 0.7). we would like to determine the probabilities associated with the binomial distribution more generally, i.e. we want a formula where we can use n, k, and p to obtain the probability. to do this, we reexamine each part of example 4.28."
977,1,['probability'], Binomial distribution,seg_37,"there were four individuals who could have been the one to exceed the deductible, and each of these four scenarios had the same probability. thus, we could identify the final probability as"
978,0,[], Binomial distribution,seg_37,[# of scenarios]× p (single scenario)
979,1,"['successes', 'trials', 'probability']", Binomial distribution,seg_37,the first component of this equation is the number of ways to arrange the k = 3 successes among the n = 4 trials. the second component is the probability of any of the four (equally probable) scenarios.
980,1,"['multiplication rule', 'independent', 'failures', 'successes', 'case', 'trials', 'events', 'independent events']", Binomial distribution,seg_37,"consider p (single scenario) under the general case of k successes and n − k failures in the n trials. in any such scenario, we apply the multiplication rule for independent events:"
981,0,[], Binomial distribution,seg_37,this is our general formula for p (single scenario).
982,1,"['failures', 'successes', 'trials']", Binomial distribution,seg_37,"secondly, we introduce a general formula for the number of ways to choose k successes in n trials, i.e. arrange k successes and n− k failures:"
983,0,['n'], Binomial distribution,seg_37,the quantity (n
984,1,['factorial'], Binomial distribution,seg_37,k) is read n choose k.21 the exclamation point notation (e.g. k!) denotes a factorial expression.
985,1,"['trials', 'successes']", Binomial distribution,seg_37,"using the formula, we can compute the number of ways to choose k = 3 successes in n = 4 trials:"
986,0,[], Binomial distribution,seg_37,this result is exactly what we found by carefully thinking of each possible scenario in example 4.28.
987,1,"['n choose k', 'binomial', 'probability']", Binomial distribution,seg_37,substituting n choose k for the number of scenarios and pk(1 − p)n−k for the single scenario probability yields the general binomial formula.
988,1,"['independent', 'successes', 'trials', 'success', 'probability', 'trial']", Binomial distribution,seg_37,suppose the probability of a single trial being a success is p. then the probability of observing exactly k successes in n independent trials is given by
989,1,"['deviation', 'successes', 'mean', 'standard', 'standard deviation', 'variance']", Binomial distribution,seg_37,"the mean, variance, and standard deviation of the number of observed successes are"
990,1,"['independent', 'failure', 'trial', 'trials', 'success', 'probability', 'outcome', 'probability of a success']", Binomial distribution,seg_37,"(1) the trials are independent. (2) the number of trials, n, is fixed. (3) each trial outcome can be classified as a success or failure. (4) the probability of a success, p, is the same for each trial."
991,1,['probability'], Binomial distribution,seg_37,"what is the probability that 3 of 8 randomly selected individuals will have exceeded the insurance deductible, i.e. that 5 of 8 will not exceed the deductible? recall that 70% of individuals will not exceed the deductible."
992,1,"['sample', 'model', 'independent', 'condition', 'failure', 'trial', 'trials', 'success', 'binomial', 'probability', 'random', 'outcome', 'probability of a success']", Binomial distribution,seg_37,"we would like to apply the binomial model, so we check the conditions. the number of trials is fixed (n = 8) (condition 2) and each trial outcome can be classified as a success or failure (condition 3). because the sample is random, the trials are independent (condition 1) and the probability of a success is the same for each trial (condition 4)."
993,1,"['outcome of interest', 'successes', 'trials', 'success', 'probability', 'outcome', 'probability of a success']", Binomial distribution,seg_37,"in the outcome of interest, there are k = 5 successes in n = 8 trials (recall that a success is an individual who does not exceed the deductible, and the probability of a success is p = 0.7. so the probability that 5 of 8 will not exceed the deductible and 3 will exceed the deductible is given by"
994,1,['factorial'], Binomial distribution,seg_37,dealing with the factorial part:
995,1,['probability'], Binomial distribution,seg_37,"using (0.7)5(0.3)3 ≈ 0.00454, the final probability is about 56× 0.00454 ≈ 0.254."
996,1,"['model', 'results', 'binomial', 'probability']", Binomial distribution,seg_37,"the first step in using the binomial model is to check that the model is appropriate. the second step is to identify n, p, and k. as the last stage use software or the formulas to determine the probability, then interpret the results."
997,1,"['binomial coefficient', 'binomial', 'coefficient']", Binomial distribution,seg_37,"if you must do calculations by hand, it’s often useful to cancel out as many terms as possible in the top and bottom of the binomial coefficient."
998,1,"['deviation', 'standard', 'case', 'cases', 'standard deviation']", Binomial distribution,seg_37,"if we randomly sampled 40 case files from the insurance agency discussed earlier, how many of the cases would you expect to not have exceeded the deductible in a given year? what is the standard deviation of the number that would not have exceeded the deductible?22"
999,1,"['model', 'condition', 'binomial', 'probability', 'random']", Binomial distribution,seg_37,"the probability that a random smoker will develop a severe lung condition in his or her lifetime is about 0.3. if you have 4 friends who smoke, are the conditions for the binomial model satisfied?23"
1000,1,"['sample', 'model', 'random', 'binomial', 'probability', 'random sample', 'population']", Binomial distribution,seg_37,suppose these four friends do not know each other and we can treat them as if they were a random sample from the population. is the binomial model appropriate? what is the probability that24
1001,1,['condition'], Binomial distribution,seg_37,(a) none of them will develop a severe lung condition?
1002,1,['condition'], Binomial distribution,seg_37,(b) one will develop a severe lung condition?
1003,1,['condition'], Binomial distribution,seg_37,(c) that no more than one will develop a severe lung condition?
1004,1,"['probability', 'condition']", Binomial distribution,seg_37,what is the probability that at least 2 of your 4 smoking friends will develop a severe lung condition in their lifetimes?25
1005,1,"['sample', 'random sample', 'random']", Binomial distribution,seg_37,suppose you have 7 friends who are smokers and they can be treated as a random sample of smok-
1006,1,"['mean', 'condition']", Binomial distribution,seg_37,"(a) how many would you expect to develop a severe lung condition, i.e. what is the mean?"
1007,1,"['probability', 'condition']", Binomial distribution,seg_37,(b) what is the probability that at most 2 of your 7 friends will develop a severe lung condition.
1008,1,"['n choose k', 'binomial', 'probability']", Binomial distribution,seg_37,"next we consider the first term in the binomial probability, n choose k under some special scenarios."
1009,1,"['failures', 'successes', 'failure', 'trials', 'success']", Binomial distribution,seg_37,how many ways can you arrange one success and n−1 failures in n trials? how many ways can you arrange n− 1 successes and one failure in n trials?28
1010,1,"['normal approximation', 'approximation', 'distribution', 'normal', 'binomial', 'binomial distribution']", Binomial distribution,seg_37,4.3.2 normal approximation to the binomial distribution
1011,1,"['sample size', 'sample', 'probabilities', 'estimate', 'range', 'observations', 'distribution', 'cases', 'normal', 'binomial', 'normal distribution']", Binomial distribution,seg_37,"the binomial formula is cumbersome when the sample size (n) is large, particularly when we consider a range of observations. in some cases we may use the normal distribution as an easier and faster way to estimate binomial probabilities."
1012,1,"['sample', 'rate', 'probability', 'population']", Binomial distribution,seg_37,"approximately 15% of the us population smokes cigarettes. a local government believed their community had a lower smoker rate and commissioned a survey of 400 randomly selected individuals. the survey found that only 42 of the 400 participants smoke cigarettes. if the true proportion of smokers in the community was really 15%, what is the probability of observing 42 or fewer smokers in a sample of 400 people?"
1013,1,"['model', 'binomial']", Binomial distribution,seg_37,we leave the usual verification that the four conditions for the binomial model are valid as an exercise.
1014,1,"['sample', 'probabilities', 'probability']", Binomial distribution,seg_37,"the question posed is equivalent to asking, what is the probability of observing k = 0, 1, 2, ..., or 42 smokers in a sample of n = 400 when p = 0.15? we can compute these 43 different probabilities and add them together to find the answer:"
1015,1,"['sample', 'probability']", Binomial distribution,seg_37,"if the true proportion of smokers in the community is p = 0.15, then the probability of observing 42 or fewer smokers in a sample of n = 400 is 0.0054."
1016,1,"['model', 'probabilities', 'method', 'range', 'distribution', 'normal', 'binomial', 'binomial distribution']", Binomial distribution,seg_37,"the computations in example 4.38 are tedious and long. in general, we should avoid such work if an alternative method exists that is faster, easier, and still accurate. recall that calculating probabilities of a range of values is much easier in the normal model. we might wonder, is it reasonable to use the normal model in place of the binomial distribution? surprisingly, yes, if certain conditions are met."
1017,1,"['sample size', 'sample', 'model', 'hollow histograms', 'histogram', 'hollow histogram', 'distribution', 'samples', 'success', 'binomial', 'probability', 'simulated', 'binomial distribution', 'histograms', 'probability of a success', 'distributions']", Binomial distribution,seg_37,"here we consider the binomial model when the probability of a success is p = 0.10. figure 4.9 shows four hollow histograms for simulated samples from the binomial distribution using four different sample sizes: n = 10, 30, 100, 300. what happens to the shape of the distributions as the sample size increases? what distribution does the last hollow histogram resemble?29"
1018,1,"['probability', 'binomial distribution', 'sample', 'success', 'mean', 'standard', 'standard deviation', 'sample size', 'parameters', 'distribution', 'binomial', 'deviation', 'normal', 'probability of success', 'normal distribution']", Binomial distribution,seg_37,the binomial distribution with probability of success p is nearly normal when the sample size n is sufficiently large that np and n(1 − p) are both at least 10. the approximate normal distribution has parameters corresponding to the mean and standard deviation of the binomial distribution:
1019,1,"['normal approximation', 'range', 'approximation', 'successes', 'distribution', 'normal', 'normal distribution']", Binomial distribution,seg_37,"the normal approximation may be used when computing the range of many possible successes. for instance, we may apply the normal distribution to the setting of example 4.38."
1020,1,"['sample', 'model', 'hollow histograms', 'plots', 'samples', 'binomial', 'histograms']", Binomial distribution,seg_37,"figure 4.9: hollow histograms of samples from the binomial model when p = 0.10. the sample sizes for the four plots are n = 10, 30, 100, and 300, respectively."
1021,1,"['sample', 'normal approximation', 'estimate', 'approximation', 'normal', 'probability']", Binomial distribution,seg_37,"how can we use the normal approximation to estimate the probability of observing 42 or fewer smokers in a sample of 400, if the true proportion of smokers is p = 0.15?"
1022,1,"['model', 'binomial']", Binomial distribution,seg_37,showing that the binomial model is reasonable was a suggested exercise in example 4.38. we also verify that both np and n(1− p) are at least 10:
1023,1,"['deviation', 'model', 'normal approximation', 'approximation', 'distribution', 'normal', 'mean', 'binomial', 'standard', 'binomial distribution', 'standard deviation']", Binomial distribution,seg_37,"with these conditions checked, we may use the normal approximation in place of the binomial distribution using the mean and standard deviation from the binomial model:"
1024,1,"['model', 'probability']", Binomial distribution,seg_37,we want to find the probability of observing 42 or fewer smokers using this model.
1025,1,"['model', 'normal', 'probability', 'estimate']", Binomial distribution,seg_37,"use the normal model n(µ = 60, σ = 7.14) to estimate the probability of observing 42 or fewer"
1026,0,[], Binomial distribution,seg_37,30 smokers. your answer should be approximately equal to the solution of example 4.38: 0.0054.
1027,1,"['normal approximation', 'approximation', 'intervals', 'normal']", Binomial distribution,seg_37,4.3.3 the normal approximation breaks down on small intervals
1028,1,"['normal approximation', 'range', 'approximation', 'distribution', 'normal', 'binomial', 'probability', 'binomial distribution']", Binomial distribution,seg_37,"the normal approximation to the binomial distribution tends to perform poorly when estimating the probability of a small range of counts, even when the conditions are met."
1029,1,"['sample', 'normal approximation', 'range', 'approximation', 'normal', 'binomial', 'probability']", Binomial distribution,seg_37,"suppose we wanted to compute the probability of observing 49, 50, or 51 smokers in 400 when p = 0.15. with such a large sample, we might be tempted to apply the normal approximation and use the range 49 to 51. however, we would find that the binomial solution and the normal approximation notably differ:"
1030,1,['normal'], Binomial distribution,seg_37,binomial: 0.0649 normal: 0.0421
1031,1,"['normal approximation', 'interval', 'approximation', 'distribution', 'normal', 'binomial', 'probability', 'normal distribution']", Binomial distribution,seg_37,"we can identify the cause of this discrepancy using figure 4.10, which shows the areas representing the binomial probability (outlined) and normal approximation (shaded). notice that the width of the area under the normal distribution is 0.5 units too slim on both sides of the interval."
1032,1,"['curve', 'normal', 'binomial', 'probability']", Binomial distribution,seg_37,figure 4.10: a normal curve with the area between 49 and 51 shaded. the outlined area represents the exact binomial probability.
1033,1,"['normal approximation', 'approximation', 'intervals', 'distribution', 'normal', 'binomial', 'binomial distribution']", Binomial distribution,seg_37,"the normal approximation to the binomial distribution for intervals of values is usually improved if cutoff values are modified slightly. the cutoff values for the lower end of a shaded region should be reduced by 0.5, and the cutoff value for the upper end should be increased by 0.5."
1034,1,"['normal approximation', 'interval', 'estimate', 'range', 'approximation', 'observations', 'distribution', 'normal', 'tail', 'normal distribution']", Binomial distribution,seg_37,"the tip to add extra area when applying the normal approximation is most often useful when examining a range of observations. in the example above, the revised normal distribution estimate is 0.0633, much closer to the exact value of 0.0649. while it is possible to also apply this correction when computing a tail area, the benefit of the modification usually disappears since the total interval is typically quite wide."
1035,1,"['geometric distribution', 'negative binomial distribution', 'distribution', 'negative binomial', 'success', 'binomial', 'probability', 'binomial distribution', 'geometric', 'trial']", Negative binomial distribution,seg_39,the geometric distribution describes the probability of observing the first success on the nth trial. the negative binomial distribution is more general: it describes the probability of observing the kth success on the nth trial.
1036,1,"['successful', 'probability']", Negative binomial distribution,seg_39,"each day a high school football coach tells his star kicker, brian, that he can go home after he successfully kicks four 35 yard field goals. suppose we say each kick has a probability p of being successful. if p is small – e.g. close to 0.1 – would we expect brian to need many attempts before he successfully kicks his fourth field goal?"
1037,1,"['successes', 'success', 'probability', 'probability of a success']", Negative binomial distribution,seg_39,"we are waiting for the fourth success (k = 4). if the probability of a success (p) is small, then the number of attempts (n) will probably be large. this means that brian is more likely to need many attempts before he gets k = 4 successes. to put this another way, the probability of n being small is low."
1038,1,"['case', 'distribution', 'negative binomial', 'binomial', 'binomial distribution']", Negative binomial distribution,seg_39,"to identify a negative binomial case, we check 4 conditions. the first three are common to the binomial distribution."
1039,1,"['independent', 'failure', 'trial', 'trials', 'success', 'probability', 'outcome', 'probability of a success']", Negative binomial distribution,seg_39,(1) the trials are independent. (2) each trial outcome can be classified as a success or failure. (3) the probability of a success (p) is the same for each trial. (4) the last trial must be a success.
1040,1,['probability'], Negative binomial distribution,seg_39,suppose brian is very diligent in his attempts and he makes each 35 yard field goal with probability p = 0.8. take a guess at how many attempts he would need before making his fourth kick.34
1041,0,[], Negative binomial distribution,seg_39,"in yesterday’s practice, it took brian only 6 tries to get his fourth field goal. write out each of the possible sequence of kicks."
1042,1,"['failures', 'successful', 'successes', 'success']", Negative binomial distribution,seg_39,"because it took brian six tries to get the fourth success, we know the last kick must have been a success. that leaves three successful kicks and two unsuccessful kicks (we label these as failures) that make up the first five attempts. there are ten possible sequences of these first five kicks, which are shown in figure 4.11. if brian achieved his fourth success (k = 4) on his sixth attempt (n = 6), then his order of successes and failures must be one of these ten possible sequences."
1043,1,"['failures', 'successes', 'success', 'probability', 'probability of a success']", Negative binomial distribution,seg_39,"each sequence in figure 4.11 has exactly two failures and four successes with the last attempt always being a success. if the probability of a success is p = 0.8, find the probability of the first sequence.35"
1044,1,['successful'], Negative binomial distribution,seg_39,figure 4.11: the ten possible sequences when the fourth successful kick is on the sixth attempt.
1045,1,"['successful', 'probability']", Negative binomial distribution,seg_39,"if the probability brian kicks a 35 yard field goal is p = 0.8, what is the probability it takes brian exactly six tries to get his fourth successful kick? we can write this as"
1046,1,"['disjoint', 'probability']", Negative binomial distribution,seg_39,where the sequences are from figure 4.11. we can break down this last probability into the sum of ten disjoint possibilities:
1047,1,"['total probability', 'probability']", Negative binomial distribution,seg_39,"the probability of the first sequence was identified in guided practice 4.45 as 0.0164, and each of the other sequences have the same probability. since each of the ten sequence has the same probability, the total probability is ten times that of any individual sequence."
1048,1,"['negative binomial', 'binomial', 'probability']", Negative binomial distribution,seg_39,the way to compute this negative binomial probability is similar to how the binomial problems were solved in section 4.3. the probability is broken into two pieces:
1049,0,[], Negative binomial distribution,seg_39,p (it takes brian six tries to make four field goals)
1050,0,[], Negative binomial distribution,seg_39,= [number of possible sequences]× p (single sequence)
1051,0,[], Negative binomial distribution,seg_39,"each part is examined separately, then we multiply to get the final result."
1052,1,"['failures', 'successes', 'case', 'probability']", Negative binomial distribution,seg_39,we first identify the probability of a single sequence. one particular case is to first observe all the failures (n− k of them) followed by the k successes:
1053,0,[], Negative binomial distribution,seg_39,p (single sequence)
1054,1,"['successes', 'failures']", Negative binomial distribution,seg_39,= p (n− k failures and then k successes)
1055,1,"['n choose k', 'observations', 'successes', 'case', 'observation', 'trials', 'success', 'coefficient']", Negative binomial distribution,seg_39,"we must also identify the number of sequences for the general case. above, ten sequences were identified where the fourth success came on the sixth attempt. these sequences were identified by fixing the last observation as a success and looking for all the ways to arrange the other observations. in other words, how many ways could we arrange k− 1 successes in n− 1 trials? this can be found using the n choose k coefficient but for n− 1 and k − 1 instead:"
1056,1,"['failures', 'successes', 'factorial', 'trials', 'factorial notation']", Negative binomial distribution,seg_39,"this is the number of different ways we can order k − 1 successes and n− k failures in n− 1 trials. if the factorial notation (the exclamation point) is unfamiliar, see page 150."
1057,1,"['independent', 'negative binomial distribution', 'distribution', 'negative binomial', 'success', 'trials', 'binomial', 'probability', 'binomial distribution', 'trial']", Negative binomial distribution,seg_39,"the negative binomial distribution describes the probability of observing the kth success on the nth trial, where all trials are independent:"
1058,1,"['success', 'trial']", Negative binomial distribution,seg_39,n− 1 p (the kth success on the nth trial) = (k − 1)
1059,1,"['success', 'trial', 'probability']", Negative binomial distribution,seg_39,the value p represents the probability that an individual trial is a success.
1060,1,"['successful', 'negative binomial distribution', 'distribution', 'negative binomial', 'binomial', 'probability', 'binomial distribution']", Negative binomial distribution,seg_39,show using the formula for the negative binomial distribution that the probability brian kicks his fourth successful field goal on the sixth attempt is 0.164.
1061,1,"['success', 'successes', 'probability']", Negative binomial distribution,seg_39,"the probability of a single success is p = 0.8, the number of successes is k = 4, and the number of necessary attempts under this scenario is n = 6."
1062,1,"['independent', 'negative binomial distribution', 'distribution', 'negative binomial', 'binomial', 'binomial distribution']", Negative binomial distribution,seg_39,the negative binomial distribution requires that each kick attempt by brian is independent. do you think it is reasonable to suggest that each of brian’s kick attempts are independent?36
1063,1,"['independent', 'probability']", Negative binomial distribution,seg_39,assume brian’s kick attempts are independent. what is the probability that brian will kick his fourth field goal within 5 attempts?37
1064,1,"['successes', 'case', 'observation', 'trials', 'negative binomial', 'success', 'binomial']", Negative binomial distribution,seg_39,"in the binomial case, we typically have a fixed number of trials and instead consider the number of successes. in the negative binomial case, we examine how many trials it takes to observe a fixed number of successes and require that the last observation be a success."
1065,1,"['case', 'negative binomial', 'binomial']", Negative binomial distribution,seg_39,"on 70% of days, a hospital admits at least one heart attack patient. on 30% of the days, no heart attack patients are admitted. identify each case below as a binomial or negative binomial case, and compute the probability.38"
1066,1,['probability'], Negative binomial distribution,seg_39,(a) what is the probability the hospital will admit a heart attack patient on exactly three days this
1067,1,['probability'], Negative binomial distribution,seg_39,(b) what is the probability the second day with a heart attack patient will be the fourth day of the
1068,1,['probability'], Negative binomial distribution,seg_39,(c) what is the probability the fifth day of next month will be the first day with a heart attack
1069,1,"['distribution', 'histogram', 'average']", Poisson distribution,seg_41,"there are about 8 million individuals in new york city. how many individuals might we expect to be hospitalized for acute myocardial infarction (ami), i.e. a heart attack, each day? according to historical records, the average number is about 4.4 individuals. however, we would also like to know the approximate distribution of counts. what would a histogram of the number of ami occurrences each day look like if we recorded the daily counts over an entire year?"
1070,1,"['deviation', 'sample', 'histogram', 'sample mean', 'unimodal', 'sample standard deviation', 'skewed', 'data', 'mean', 'standard', 'standard deviation', 'average']", Poisson distribution,seg_41,"39 a histogram of the number of occurrences of ami on 365 days for nyc is shown in figure 4.12. the sample mean (4.38) is similar to the historical average of 4.4. the sample standard deviation is about 2, and the histogram indicates that about 70% of the data fall between 2.4 and 6.4. the distribution’s shape is unimodal and skewed to the right."
1071,1,['events'], Poisson distribution,seg_41,80 y 60 cneu 40 qer f 20 0 0 5 10 ami events (by day)
1072,1,['histogram'], Poisson distribution,seg_41,figure 4.12: a histogram of the number of occurrences of ami on 365 separate days in nyc.
1073,1,"['poisson', 'distribution', 'events', 'population', 'poisson distribution']", Poisson distribution,seg_41,"the poisson distribution is often useful for estimating the number of events in a large population over a unit of time. for instance, consider each of the following events:"
1074,0,[], Poisson distribution,seg_41,"• having a heart attack,"
1075,0,[], Poisson distribution,seg_41,"• getting married, and"
1076,0,[], Poisson distribution,seg_41,• getting struck by lightning.
1077,1,"['poisson', 'independent', 'distribution', 'events', 'population', 'poisson distribution']", Poisson distribution,seg_41,"the poisson distribution helps us describe the number of such events that will occur in a day for a fixed population if the individuals within the population are independent. the poisson distribution could also be used over another unit of time, such as an hour or a week."
1078,1,"['poisson', 'rate', 'histogram', 'distribution', 'events', 'population', 'probability', 'poisson distribution', 'parameter', 'average']", Poisson distribution,seg_41,"the histogram in figure 4.12 approximates a poisson distribution with rate equal to 4.4. the rate for a poisson distribution is the average number of occurrences in a mostly-fixed population per unit of time. in example 4.50, the time unit is a day, the population is all new york city residents, and the historical rate is 4.4. the parameter in the poisson distribution is the rate – or how many events we expect to observe – and it is typically denoted by λ (the greek letter lambda) or µ. using the rate, we can describe the probability of observing exactly k events in a single unit of time."
1079,1,"['poisson', 'rate', 'events']", Poisson distribution,seg_41,suppose we are watching for events and the number of observed events follows a poisson distribution with rate λ. then
1080,1,['events'], Poisson distribution,seg_41,λke−λ p (observe k events) = k!
1081,1,"['deviation', 'distribution', 'mean', 'standard', 'standard deviation']", Poisson distribution,seg_41,"where k may take a value 0, 1, 2, and so on, and k! represents k-factorial, as described on page 150. the letter e ≈ 2.718 is the base of the natural logarithm. the mean and standard deviation of this distribution are λ and √λ, respectively."
1082,1,"['poisson', 'model', 'distribution', 'set', 'poisson distribution']", Poisson distribution,seg_41,"we will leave a rigorous set of conditions for the poisson distribution to a later course. however, we offer a few simple guidelines that can be used for an initial evaluation of whether the poisson model would be appropriate."
1083,1,"['poisson', 'random', 'distribution', 'random variable', 'events', 'variable', 'population', 'poisson distribution']", Poisson distribution,seg_41,"a random variable may follow a poisson distribution if we are looking for the number of events, the population that generates such events is large, and the events occur independently of each other."
1084,1,"['poisson', 'model', 'linear', 'independent', 'rate', 'generalized linear models', 'distribution', 'variable', 'events', 'poisson distribution', 'rates']", Poisson distribution,seg_41,"even when events are not really independent – for instance, saturdays and sundays are especially popular for weddings – a poisson model may sometimes still be reasonable if we allow it to have a different rate for different times. in the wedding example, the rate would be modeled as higher on weekends than on weekdays. the idea of modeling rates for a poisson distribution against a second variable such as the day of week forms the foundation of some more advanced methods that fall in the realm of generalized linear models. in chapters 8 and 9, we will discuss a foundation of linear models."
1085,1,"['confidence intervals', 'interval', 'estimates', 'range', 'statistics', 'sample', 'estimate', 'uncertainty', 'intervals', 'hypothesis testing', 'parameter', 'population', 'confidence', 'statistical', 'confidence interval', 'statistical inference', 'point estimates', 'hypothesis', 'variability', 'sampling']",Chapter  Foundations for inference,seg_43,"5.1 point estimates and sampling variability 5.2 confidence intervals for a proportion 5.3 hypothesis testing for a proportion statistical inference is primarily concerned with understanding and quantifying the uncertainty of parameter estimates. while the equations and details change depending on the setting, the foundations for inference are the same throughout all of statistics. we start with a familiar topic: the idea of using a sample proportion to estimate a population proportion. next, we create what’s called a confidence interval, which is a range of plausible values where we may find the true population value. finally, we introduce the hypothesis testing framework, which allows us to formally evaluate claims about the population, such as whether a survey provides strong evidence that a candidate has the support of a majority of the voting population. for videos, slides, and other resources, please visit www.openintro.org/os"
1086,1,"['population', 'estimate']", Point estimates and sampling variability,seg_45,"companies such as pew research frequently conduct polls as a way to understand the state of public opinion or knowledge on many topics, including politics, scientific understanding, brand recognition, and more. the ultimate goal in taking a poll is generally to use the responses to estimate the opinion or knowledge of the broader population."
1087,1,"['estimates', 'point estimates', 'error']", Point estimates and sampling variability,seg_45,5.1.1 point estimates and error
1088,1,"['sample', 'bias', 'estimate', 'sampling', 'parameter of interest', 'point estimate', 'population', 'parameter', 'response', 'error', 'sampling error']", Point estimates and sampling variability,seg_45,"suppose a poll suggested the us president’s approval rating is 45%. we would consider 45% to be a point estimate of the approval rating we might see if we collected responses from the entire population. this entire-population response proportion is generally referred to as the parameter of interest. when the parameter is a proportion, it is often denoted by p, and we often refer to the sample proportion as p̂ (pronounced p-hat1). unless we collect responses from every individual in the population, p remains unknown, and we use p̂ as our estimate of p. the difference we observe from the poll versus the parameter is called the error in the estimate. generally, the error consists of two aspects: sampling error and bias."
1089,1,"['sample size', 'sample', 'estimate', 'sample’s size', 'uncertainty', 'statistics', 'sampling', 'vary', 'sampling uncertainty', 'error', 'sampling error']", Point estimates and sampling variability,seg_45,"sampling error, sometimes called sampling uncertainty, describes how much an estimate will tend to vary from one sample to the next. for instance, the estimate from one sample might be 1% too low while in another it may be 3% too high. much of statistics, including much of this book, is focused on understanding and quantifying sampling error, and we will find it useful to consider a sample’s size to help us quantify this error; the sample size is often represented by the letter n."
1090,1,"['bias', 'estimate', 'biased', 'data', 'population', 'level', 'data collection']", Point estimates and sampling variability,seg_45,"bias describes a systematic tendency to overor under-estimate the true population value. for example, if we were taking a student poll asking about support for a new college stadium, we’d probably get a biased estimate of the stadium’s level of student support by wording the question as, do you support your school by supporting funding for the new stadium? we try to minimize bias through thoughtful data collection procedures, which were discussed in chapter 1 and are the topic of many other books."
1091,1,"['estimate', 'point estimate', 'variability']", Point estimates and sampling variability,seg_45,5.1.2 understanding the variability of a point estimate
1092,1,"['sample', 'estimate', 'parameter']", Point estimates and sampling variability,seg_45,"suppose the proportion of american adults who support the expansion of solar energy is p = 0.88, which is our parameter of interest.2 if we were to take a poll of 1000 american adults on this topic, the estimate would not be perfect, but how close might we expect the sample proportion in the poll would be to 88%? we want to understand, how does the sample proportion p̂ behave when"
1093,1,"['sample', 'random', 'simulation', 'simple random sample', 'population', 'random sample']", Point estimates and sampling variability,seg_45,"3 the true population proportion is 0.88. let’s find out! we can simulate responses we would get from a simple random sample of 1000 american adults, which is only possible because we know the actual support for expanding solar energy is 0.88. here’s how we might go about constructing such a simulation:"
1094,0,[], Point estimates and sampling variability,seg_45,"1. there were about 250 million american adults in 2018. on 250 million pieces of paper, write “support” on 88% of them and “not” on the other 12%."
1095,1,['sample'], Point estimates and sampling variability,seg_45,2. mix up the pieces of paper and pull out 1000 pieces to represent our sample of 1000 american adults.
1096,1,['sample'], Point estimates and sampling variability,seg_45,3. compute the fraction of the sample that say “support”.
1097,1,['simulation'], Point estimates and sampling variability,seg_45,"any volunteers to conduct this simulation? probably not. running this simulation with 250 million pieces of paper would be time-consuming and very costly, but we can simulate it using computer"
1098,1,"['sample', 'simulation', 'estimate', 'case', 'point estimate', 'population', 'error']", Point estimates and sampling variability,seg_45,"code; we’ve written a short program in figure 5.1 in case you are curious what the computer code looks like. in this simulation, the sample gave a point estimate of p̂1 = 0.894. we know the population proportion for the simulation was p = 0.88, so we know the estimate had an error of 0.894− 0.88 = +0.014."
1099,1,['set'], Point estimates and sampling variability,seg_45,"# 1. create a set of 250 million entries, where 88% of them are ""support"" # and 12% are ""not"". pop size <- 250000000 possible entries <- c(rep(""support"", 0.88 * pop size), rep(""not"", 0.12 * pop size))"
1100,1,"['sample', 'without replacement', 'replacement']", Point estimates and sampling variability,seg_45,"# 2. sample 1000 entries without replacement. sampled entries <- sample(possible entries, size = 1000)"
1101,1,"['sample size', 'sample']", Point estimates and sampling variability,seg_45,"# 3. compute p-hat: count the number that are ""support"", then divide by # the sample size. sum(sampled entries == ""support"") / 1000"
1102,1,['simulation'], Point estimates and sampling variability,seg_45,"figure 5.1: for those curious, this is code for a single p̂ simulation using the statistical software called r. each line that starts with # is a code comment, which is used to describe in regular language what the code is doing. we’ve provided software labs in r at openintro.org/stat/labs for anyone interested in learning more."
1103,1,"['sample', 'histogram', 'simulation', 'estimate', 'estimates', 'results', 'sampling', 'distribution', 'simulations', 'sampling distribution', 'error']", Point estimates and sampling variability,seg_45,"one simulation isn’t enough to get a great sense of the distribution of estimates we might expect in the simulation, so we should run more simulations. in a second simulation, we get p̂2 = 0.885, which has an error of +0.005. in another, p̂3 = 0.878 for an error of -0.002. and in another, an estimate of p̂4 = 0.859 with an error of -0.021. with the help of a computer, we’ve run the simulation 10,000 times and created a histogram of the results from all 10,000 simulations in figure 5.2. this distribution of sample proportions is called a sampling distribution. we can characterize this sampling distribution as follows:"
1104,1,"['distribution', 'parameter']", Point estimates and sampling variability,seg_45,"center. the center of the distribution is x̄p̂ = 0.880, which is the same as the parameter. notice"
1105,1,"['sample', 'bias', 'simulation', 'simple random sample', 'random sample', 'sampling', 'population', 'random']", Point estimates and sampling variability,seg_45,"that the simulation mimicked a simple random sample of the population, which is a straightforward sampling strategy that helps avoid sampling bias."
1106,1,"['deviation', 'distribution', 'standard', 'standard deviation']", Point estimates and sampling variability,seg_45,spread. the standard deviation of the distribution is sp̂ = 0.010. when we’re talking about a
1107,1,"['deviation', 'sample', 'variability', 'estimate', 'standard error', 'distribution', 'associated', 'point estimate', 'standard', 'standard deviation', 'error']", Point estimates and sampling variability,seg_45,"sampling distribution or the variability of a point estimate, we typically use the term standard error rather than standard deviation, and the notation sep̂ is used for the standard error associated with the sample proportion."
1108,1,"['distribution', 'symmetric', 'normal', 'normal distribution']", Point estimates and sampling variability,seg_45,"shape. the distribution is symmetric and bell-shaped, and it resembles a normal distribution."
1109,1,"['sample size', 'sample', 'histogram', 'estimate', 'observation', 'distribution', 'normal', 'population', 'normal distribution']", Point estimates and sampling variability,seg_45,"these findings are encouraging! when the population proportion is p = 0.88 and the sample size is n = 1000, the sample proportion p̂ tends to give a pretty good estimate of the population proportion. we also have the interesting observation that the histogram resembles a normal distribution."
1110,1,"['estimate', 'estimates', 'sampling', 'distribution', 'point estimate', 'point estimates', 'sampling distribution']", Point estimates and sampling variability,seg_45,"in real-world applications, we never actually observe the sampling distribution, yet it is useful to always think of a point estimate as coming from such a hypothetical distribution. understanding the sampling distribution will help us characterize and make sense of the point estimates that we do observe."
1111,1,"['sample size', 'sample', 'standard error', 'standard', 'error']", Point estimates and sampling variability,seg_45,"if we used a much smaller sample size of n = 50, would you guess that the standard error for p̂ would be larger or smaller than when we used n = 1000?"
1112,1,"['error', 'data']", Point estimates and sampling variability,seg_45,"intuitively, it seems like more data is better than less data, and generally that is correct! the typical error when p = 0.88 and n = 50 would be larger than the error we would expect when n = 1000."
1113,1,"['sample', 'point estimate', 'estimate']", Point estimates and sampling variability,seg_45,example 5.1 highlights an important property we will see again and again: a bigger sample tends to provide a more precise point estimate than a smaller sample.
1114,1,['sample'], Point estimates and sampling variability,seg_45,750 y 500 cne uqerf 250 0 0.84 0.86 0.88 0.90 0.92 sample proportions
1115,1,"['sample size', 'sample', 'histogram', 'population']", Point estimates and sampling variability,seg_45,"figure 5.2: a histogram of 10,000 sample proportions, where each sample is taken from a population where the population proportion is 0.88 and the sample size is n = 1000."
1116,1,"['limit', 'central limit theorem']", Point estimates and sampling variability,seg_45,5.1.3 central limit theorem
1117,1,"['distribution', 'normal', 'central limit theorem', 'limit', 'normal distribution']", Point estimates and sampling variability,seg_45,the distribution in figure 5.2 looks an awful lot like a normal distribution. that is no anomaly; it is the result of a general principle called the central limit theorem.
1118,1,"['sample size', 'sample', 'independent', 'standard error', 'observations', 'distribution', 'normal', 'mean', 'standard', 'error', 'normal distribution']", Point estimates and sampling variability,seg_45,"when observations are independent and the sample size is sufficiently large, the sample proportion p̂ will tend to follow a normal distribution with the following mean and standard error:"
1119,1,"['sample size', 'sample', 'condition', 'central limit theorem', 'limit']", Point estimates and sampling variability,seg_45,"in order for the central limit theorem to hold, the sample size is typically considered sufficiently large when np ≥ 10 and n(1− p) ≥ 10, which is called the success-failure condition."
1120,1,"['sample size', 'sample', 'independent', 'observations', 'statistics', 'central limit theorem', 'limit']", Point estimates and sampling variability,seg_45,"the central limit theorem is incredibly important, and it provides a foundation for much of statistics. as we begin applying the central limit theorem, be mindful of the two technical conditions: the observations must be independent, and the sample size must be sufficiently large such that np ≥ 10 and n(1− p) ≥ 10."
1121,1,"['error', 'estimated', 'standard error', 'data', 'sampling', 'distribution', 'central limit theorem', 'normal', 'simulated', 'mean', 'standard', 'sampling distribution', 'limit']", Point estimates and sampling variability,seg_45,earlier we estimated the mean and standard error of p̂ using simulated data when p = 0.88 and n = 1000. confirm that the central limit theorem applies and the sampling distribution is approximately normal.
1122,1,"['sample', 'observations']", Point estimates and sampling variability,seg_45,"independence. there are n = 1000 observations for each sample proportion p̂, and each of those"
1123,1,"['sample', 'independent', 'random', 'simple random sample', 'observations', 'random sample']", Point estimates and sampling variability,seg_45,observations are independent draws. the most common way for observations to be considered independent is if they are from a simple random sample.
1124,1,"['sample size', 'sample', 'condition']", Point estimates and sampling variability,seg_45,success-failure condition. we can confirm the sample size is sufficiently large by checking the
1125,1,['condition'], Point estimates and sampling variability,seg_45,success-failure condition and confirming the two calculated values are greater than 10:
1126,1,"['model', 'distribution', 'independence', 'normal', 'central limit theorem', 'limit', 'normal distribution']", Point estimates and sampling variability,seg_45,"the independence and success-failure conditions are both satisfied, so the central limit theorem applies, and it’s reasonable to model p̂ using a normal distribution."
1127,1,"['independent', 'experiment', 'treatment', 'treatment groups', 'random']", Point estimates and sampling variability,seg_45,subjects in an experiment are considered independent if they undergo random assignment to the treatment groups.
1128,1,"['sample', 'independent', 'random', 'simple random sample', 'observations', 'random sample']", Point estimates and sampling variability,seg_45,"if the observations are from a simple random sample, then they are independent."
1129,1,"['sample', 'random', 'case', 'independence', 'random process', 'process', 'error']", Point estimates and sampling variability,seg_45,"if a sample is from a seemingly random process, e.g. an occasional error on an assembly line, checking independence is more difficult. in this case, use your best judgement."
1130,1,"['sample', 'condition', 'sampling', 'samples', 'population', 'error', 'sampling error']", Point estimates and sampling variability,seg_45,"an additional condition that is sometimes added for samples from a population is that they are no larger than 10% of the population. when the sample exceeds 10% of the population size, the methods we discuss tend to overestimate the sampling error slightly versus what we would get using more advanced methods.4 this is very rarely an issue, and when it is an issue, our methods tend to be conservative, so we consider this additional check as optional."
1131,1,"['error', 'standard error', 'central limit theorem', 'mean', 'standard', 'limit']", Point estimates and sampling variability,seg_45,"compute the theoretical mean and standard error of p̂ when p = 0.88 and n = 1000, according to the central limit theorem."
1132,1,"['mean', 'population']", Point estimates and sampling variability,seg_45,the mean of the p̂’s is simply the population proportion: µp̂ = 0.88.
1133,1,"['standard error', 'error', 'standard']", Point estimates and sampling variability,seg_45,the calculation of the standard error of p̂ uses the following formula:
1134,1,"['distribution', 'sample', 'population']", Point estimates and sampling variability,seg_45,"estimate how frequently the sample proportion p̂ should be within 0.02 (2%) of the population value, p = 0.88. based on examples 5.2 and 5.3, we know that the distribution is approximately n(µp̂ = 0.88, sep̂ = 0.010)."
1135,1,"['distribution', 'normal', 'normal distribution']", Point estimates and sampling variability,seg_45,"after so much practice in section 4.1, this normal distribution example will hopefully feel familiar! we would like to understand the fraction of p̂’s between 0.86 and 0.90:"
1136,0,[], Point estimates and sampling variability,seg_45,"with µp̂ = 0.88 and sep̂ = 0.010, we can compute the z-score for both the left and right cutoffs:"
1137,1,"['table', 'case', 'sampling', 'distribution', 'statistical', 'population', 'tail areas', 'tail', 'tails', 'sampling distribution']", Point estimates and sampling variability,seg_45,"we can use either statistical software, a graphing calculator, or a table to find the areas to the tails, and in any case we will find that they are each 0.0228. the total tail areas are 2× 0.0228 = 0.0456, which leaves the shaded area of 0.9544. that is, about 95.44% of the sampling distribution in figure 5.2 is within ±0.02 of the population proportion, p = 0.88."
1138,1,"['sample', 'estimate']", Point estimates and sampling variability,seg_45,in example 5.1 we discussed how a smaller sample would tend to produce a less reliable estimate.
1139,0,['n'], Point estimates and sampling variability,seg_45,p(1−p) 5 explain how this intuition is reflected in the formula for sep̂ = √ n .
1140,1,"['limit', 'central limit theorem']", Point estimates and sampling variability,seg_45,5.1.4 applying the central limit theorem to a real-world setting
1141,1,"['sample', 'distribution', 'central limit theorem', 'normal', 'population', 'limit', 'normal distribution']", Point estimates and sampling variability,seg_45,we do not actually know the population proportion unless we conduct an expensive poll of all individuals in the population. our earlier value of p = 0.88 was based on a pew research conducted a poll of 1000 american adults that found p̂ = 0.887 of them favored expanding solar energy. the researchers might have wondered: does the sample proportion from the poll approximately follow a normal distribution? we can check the conditions from the central limit theorem:
1142,1,"['sample', 'random', 'simple random sample', 'random sample']", Point estimates and sampling variability,seg_45,"independence. the poll is a simple random sample of american adults, which means that the"
1143,1,['independent'], Point estimates and sampling variability,seg_45,observations are independent.
1144,1,"['condition', 'population']", Point estimates and sampling variability,seg_45,"success-failure condition. to check this condition, we need the population proportion, p, to"
1145,1,"['sample', 'condition', 'cases']", Point estimates and sampling variability,seg_45,"check if both np and n(1− p) are greater than 10. however, we do not actually know p, which is exactly why the pollsters would take a sample! in cases like these, we often use p̂ as our next best way to check the success-failure condition:"
1146,1,"['sample', 'case']", Point estimates and sampling variability,seg_45,"the sample proportion p̂ acts as a reasonable substitute for p during this check, and each value in this case is well above the minimum of 10."
1147,1,"['sample', 'standard error', 'approximation', 'substitution approximation', 'standard', 'error']", Point estimates and sampling variability,seg_45,this substitution approximation of using p̂ in place of p is also useful when computing the standard error of the sample proportion:
1148,1,"['sample', 'standard error', 'case', 'standard', 'error']", Point estimates and sampling variability,seg_45,"this substitution technique is sometimes referred to as the “plug-in principle”. in this case, sep̂ didn’t change enough to be detected using only 3 decimal places versus when we completed the calculation with 0.88 earlier. the computed standard error tends to be reasonably stable even when observing slightly different proportions in one sample or another."
1149,1,"['limit', 'central limit theorem']", Point estimates and sampling variability,seg_45,5.1.5 more details regarding the central limit theorem
1150,1,"['limit', 'central limit theorem']", Point estimates and sampling variability,seg_45,we’ve applied the central limit theorem in numerous examples so far this chapter:
1151,1,"['sample size', 'sample', 'independent', 'observations', 'distribution', 'normal', 'normal distribution']", Point estimates and sampling variability,seg_45,"when observations are independent and the sample size is sufficiently large, the distribution of p̂ resembles a normal distribution with"
1152,1,"['sample size', 'sample']", Point estimates and sampling variability,seg_45,the sample size is considered sufficiently large when np ≥ 10 and n(1− p) ≥ 10.
1153,1,"['condition', 'central limit theorem', 'limit']", Point estimates and sampling variability,seg_45,"in this section, we’ll explore the success-failure condition and seek to better understand the central limit theorem."
1154,1,"['sample', 'samples']", Point estimates and sampling variability,seg_45,"an interesting question to answer is, what happens when np < 10 or n(1− p) < 10? as we did in section 5.1.2, we can simulate drawing samples of different sizes where, say, the true proportion is p = 0.25. here’s a sample of size 10:"
1155,1,['sample'], Point estimates and sampling variability,seg_45,"2 in this sample, we observe a sample proportion of yeses of p̂ = = 0.2. we can simulate many such"
1156,1,"['variability', 'sampling', 'distribution', 'normal', 'mean', 'sampling distribution', 'normal distribution', 'distributions']", Point estimates and sampling variability,seg_45,"10 proportions to understand the sampling distribution of p̂ when n = 10 and p = 0.25, which we’ve plotted in figure 5.3 alongside a normal distribution with the same mean and variability. these distributions have a number of important differences."
1157,1,['sample'], Point estimates and sampling variability,seg_45,0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 −0.2 0.0 0.2 0.4 0.6 sample proportions
1158,1,"['sample size', 'sample', 'deviation', 'distribution', 'simulations', 'normal', 'mean', 'population', 'standard', 'standard deviation', 'normal distribution']", Point estimates and sampling variability,seg_45,figure 5.3: left: simulations of p̂ when the sample size is n = 10 and the population proportion is p = 0.25. right: a normal distribution with the same mean (0.25) and standard deviation (0.137).
1159,1,"['symmetric', 'normal']", Point estimates and sampling variability,seg_45,"unimodal? smooth? symmetric? normal: n(0.25, 0.14) yes yes yes n = 10, p = 0.25 yes no no"
1160,1,['condition'], Point estimates and sampling variability,seg_45,notice that the success-failure condition was not satisfied when n = 10 and p = 0.25:
1161,1,"['condition', 'sampling', 'distribution', 'normal', 'sampling distribution', 'normal distribution']", Point estimates and sampling variability,seg_45,"this single sampling distribution does not show that the success-failure condition is the perfect guideline, but we have found that the guideline did correctly identify that a normal distribution might not be appropriate."
1162,1,['simulations'], Point estimates and sampling variability,seg_45,"we can complete several additional simulations, shown in figures 5.4 and 5.5, and we can see some trends:"
1163,1,"['distribution', 'discrete', 'continuous']", Point estimates and sampling variability,seg_45,"1. when either np or n(1− p) is small, the distribution is more discrete, i.e. not continuous."
1164,1,"['distribution', 'skew']", Point estimates and sampling variability,seg_45,"2. when np or n(1− p) is smaller than 10, the skew in the distribution is more noteworthy."
1165,1,"['sample size', 'sample', 'variability', 'plots', 'distribution', 'normal']", Point estimates and sampling variability,seg_45,"3. the larger both np and n(1−p), the more normal the distribution. this may be a little harder to see for the larger sample size in these plots as the variability also becomes much smaller."
1166,1,"['distribution', 'normal', 'discreteness', 'normal distribution']", Point estimates and sampling variability,seg_45,"4. when np and n(1 − p) are both very large, the distribution’s discreteness is hardly evident, and the distribution looks much more like a normal distribution."
1167,1,"['sampling', 'sampling distributions', 'distributions']", Point estimates and sampling variability,seg_45,"figure 5.4: sampling distributions for several scenarios of p and n. rows: p = 0.10, p = 0.20, p = 0.50, p = 0.80, and p = 0.90. columns: n = 10 and n = 25."
1168,1,"['sampling', 'sampling distributions', 'distributions']", Point estimates and sampling variability,seg_45,"figure 5.5: sampling distributions for several scenarios of p and n. rows: p = 0.10, p = 0.20, p = 0.50, p = 0.80, and p = 0.90. columns: n = 50, n = 100, and n = 250."
1169,1,"['error', 'moment', 'standard error', 'mean', 'standard', 'discreteness', 'skew', 'distributions']", Point estimates and sampling variability,seg_45,"so far we’ve only focused on the skew and discreteness of the distributions. we haven’t considered how the mean and standard error of the distributions change. take a moment to look back at the graphs, and pay attention to three things:"
1170,1,"['sample', 'independent', 'simulation', 'data', 'sampling', 'distribution', 'parameter', 'population', 'sampling distribution', 'unbiased']", Point estimates and sampling variability,seg_45,"1. the centers of the distribution are always at the population proportion, p, that was used to generate the simulation. because the sampling distribution of p̂ is always centered at the population parameter p, it means the sample proportion p̂ is unbiased when the data are independent and drawn from such a population."
1171,1,"['sample size', 'sample', 'variability', 'estimate', 'sampling', 'distribution', 'population', 'sampling distribution']", Point estimates and sampling variability,seg_45,"2. for a particular population proportion p, the variability in the sampling distribution decreases as the sample size n becomes larger. this will likely align with your intuition: an estimate based on a larger sample size will tend to be more accurate."
1172,1,"['sample size', 'sample', 'variability', 'standard error', 'standard', 'error']", Point estimates and sampling variability,seg_45,"3. for a particular sample size, the variability will be largest when p = 0.5. the differences may be a little subtle, so take a close look. this reflects the role of the proportion p in the standard p(1−p) error formula: se = √ n . the standard error is largest when p = 0.5."
1173,1,"['condition', 'discrete', 'distribution', 'normal', 'standard']", Point estimates and sampling variability,seg_45,"at no point will the distribution of p̂ look perfectly normal, since p̂ will always be take discrete values (x/n). it is always a matter of degree, and we will use the standard success-failure condition with minimums of 10 for np and n(1− p) as our guideline within this book."
1174,1,['statistics'], Point estimates and sampling variability,seg_45,5.1.6 extending the framework for other statistics
1175,1,"['sample', 'random', 'sample mean', 'estimate', 'population mean', 'statistics', 'statistic', 'sample statistic', 'mean', 'point estimate', 'random sample', 'population', 'parameter', 'average']", Point estimates and sampling variability,seg_45,"the strategy of using a sample statistic to estimate a parameter is quite common, and it’s a strategy that we can apply to other statistics besides a proportion. for instance, if we want to estimate the average salary for graduates from a particular college, we could survey a random sample of recent graduates; in that example, we’d be using a sample mean x̄ to estimate the population mean µ for all graduates. as another example, if we want to estimate the difference in product prices for two websites, we might take a random sample of products available on both sites, check the prices on each, and use then compute the average difference; this strategy certainly would give us some idea of the actual difference through a point estimate."
1176,0,[], Point estimates and sampling variability,seg_45,"while this chapter emphases a single proportion context, we’ll encounter many different contexts throughout this book where these methods will be applied. the principles and general ideas are the same, even if the details change a little. we’ve also sprinkled some other contexts into the exercises to help you start thinking about how the ideas generalize."
1177,1,"['sample', 'estimate', 'range', 'standard error', 'associated', 'population', 'point estimate', 'standard', 'error']", Confidence intervals for a proportion,seg_47,"the sample proportion p̂ provides a single plausible value for the population proportion p. however, the sample proportion isn’t perfect and will have some standard error associated with it. when stating an estimate for the population proportion, it is better practice to provide a plausible range of values instead of supplying just the point estimate."
1178,1,"['parameter', 'population']", Confidence intervals for a proportion,seg_47,5.2.1 capturing the population parameter
1179,1,"['interval', 'estimate', 'range', 'point estimate', 'population', 'parameter', 'confidence', 'confidence interval']", Confidence intervals for a proportion,seg_47,"using only a point estimate is like fishing in a murky lake with a spear. we can throw a spear where we saw a fish, but we will probably miss. on the other hand, if we toss a net in that area, we have a good chance of catching the fish. a confidence interval is like fishing with a net, and it represents a range of plausible values where we are likely to find the population parameter."
1180,1,"['interval', 'estimate', 'range', 'point estimate', 'population', 'parameter', 'confidence', 'confidence interval']", Confidence intervals for a proportion,seg_47,"if we report a point estimate p̂, we probably will not hit the exact population proportion. on the other hand, if we report a range of plausible values, representing a confidence interval, we have a good shot at capturing the parameter."
1181,1,"['interval', 'population']", Confidence intervals for a proportion,seg_47,"if we want to be very certain we capture the population proportion in an interval, should we use a wider interval or a smaller interval?6"
1182,1,"['confidence', 'interval', 'confidence interval']", Confidence intervals for a proportion,seg_47,5.2.2 constructing a 95% confidence interval
1183,1,"['sample', 'interval', 'estimate', 'standard error', 'population', 'point estimate', 'standard', 'confidence', 'error', 'confidence interval']", Confidence intervals for a proportion,seg_47,"our sample proportion p̂ is the most plausible value of the population proportion, so it makes sense to build a confidence interval around this point estimate. the standard error provides a guide for how large we should make the confidence interval."
1184,1,"['interval', 'confident', 'sample', 'estimate', 'data', 'mean', 'standard', 'population', 'standard deviation', 'confidence', 'standard errors', 'limit', 'confidence interval', 'error', 'distribution', 'deviation', 'standard deviations', 'standard error', 'errors', 'deviations', 'normal', 'point estimate', 'normal distribution']", Confidence intervals for a proportion,seg_47,"the standard error represents the standard deviation of the point estimate, and when the central limit theorem conditions are satisfied, the point estimate closely follows a normal distribution. in a normal distribution, 95% of the data is within 1.96 standard deviations of the mean. using this principle, we can construct a confidence interval that extends 1.96 standard errors from the sample proportion to be 95% confident that the interval captures the population proportion:"
1185,1,['estimate'], Confidence intervals for a proportion,seg_47,point estimate ± 1.96× se
1186,1,"['confidence intervals', 'simulation', 'interval', 'intervals', 'samples', 'mean', 'parameter', 'population', 'confidence', 'process', 'confidence interval']", Confidence intervals for a proportion,seg_47,"but what does “95% confident” mean? suppose we took many samples and built a 95% confidence interval from each. then about 95% of those intervals would contain the parameter, p. figure 5.6 shows the process of creating 25 intervals from 25 samples from the simulation in section 5.1.2, where 24 of the resulting confidence intervals contain the simulation’s population proportion of p = 0.88, and one interval does not."
1187,1,"['confidence intervals', 'interval', 'estimates', 'intervals', 'population', 'point estimates', 'confidence']", Confidence intervals for a proportion,seg_47,"figure 5.6: twenty-five point estimates and confidence intervals from the simulations in section 5.1.2. these intervals are shown relative to the population proportion p = 0.88. only 1 of these 25 intervals did not capture the population proportion, and this interval has been bolded."
1188,1,"['simulation', 'interval', 'population']", Confidence intervals for a proportion,seg_47,"in figure 5.6, one interval does not contain p = 0.88. does this imply that the population proportion used in the simulation could not have been p = 0.88?"
1189,1,"['interval', 'estimates', 'range', 'observations', 'data', 'mean', 'parameter', 'standard', 'confidence', 'standard errors', 'confidence interval', 'parameter of interest', 'point estimates', 'standard deviations', 'errors', 'deviations']", Confidence intervals for a proportion,seg_47,"just as some observations naturally occur more than 1.96 standard deviations from the mean, some point estimates will be more than 1.96 standard errors from the parameter of interest. a confidence interval only provides a plausible range of values. while we might say other values are implausible based on the data, this does not mean they are impossible."
1190,1,"['confidence interval', 'interval', 'estimate', 'distribution', 'normal', 'point estimate', 'central limit theorem', 'confidence', 'limit', 'normal distribution']", Confidence intervals for a proportion,seg_47,"when the distribution of a point estimate qualifies for the central limit theorem and therefore closely follows a normal distribution, we can construct a 95% confidence interval as"
1191,0,[], Confidence intervals for a proportion,seg_47,point estimate± 1.96× se
1192,1,"['sample', 'random', 'interval', 'population', 'random sample', 'confidence', 'confidence interval']", Confidence intervals for a proportion,seg_47,in section 5.1 we learned about a pew research poll where 88.7% of a random sample of 1000 american adults supported expanding the role of solar power. compute and interpret a 95% confidence interval for the population proportion.
1193,1,"['confidence interval', 'interval', 'estimate', 'standard error', 'distribution', 'normal', 'point estimate', 'standard', 'confidence', 'error', 'normal distribution']", Confidence intervals for a proportion,seg_47,"we earlier confirmed that p̂ follows a normal distribution and has a standard error of sep̂ = 0.010. to compute the 95% confidence interval, plug the point estimate p̂ = 0.887 and standard error into the 95% confidence interval formula:"
1194,1,"['percentage', 'interval', 'confident', 'confidence', 'confidence interval']", Confidence intervals for a proportion,seg_47,we are 95% confident that the actual proportion of american adults who support expanding solar power is between 86.7% and 90.7%. (it’s common to round to the nearest percentage point or nearest tenth of a percentage point when reporting a confidence interval.)
1195,1,"['level', 'confidence', 'confidence level']", Confidence intervals for a proportion,seg_47,5.2.3 changing the confidence level
1196,1,"['confidence intervals', 'interval', 'intervals', 'level', 'confidence', 'confidence level']", Confidence intervals for a proportion,seg_47,"suppose we want to consider confidence intervals where the confidence level is higher than 95%, such as a confidence level of 99%. think back to the analogy about trying to catch a fish: if we want to be more sure that we will catch the fish, we should use a wider net. to create a 99% confidence level, we must also widen our 95% interval. on the other hand, if we want an interval with lower confidence, such as 90%, we could use a slightly narrower interval than our original 95% interval."
1197,1,"['confidence interval', 'levels', 'interval', 'estimate', 'intervals', 'distribution', 'normal', 'point estimate', 'confidence', 'normal distribution']", Confidence intervals for a proportion,seg_47,the 95% confidence interval structure provides guidance in how to make intervals with different confidence levels. the general 95% confidence interval for a point estimate that follows a normal distribution is
1198,1,['estimate'], Confidence intervals for a proportion,seg_47,point estimate ± 1.96× se
1199,1,"['standard errors', 'interval', 'estimate', 'standard error', 'data', 'errors', 'point estimate', 'standard', 'level', 'parameter', 'confidence', 'confidence level', 'error']", Confidence intervals for a proportion,seg_47,"there are three components to this interval: the point estimate, “1.96”, and the standard error. the choice of 1.96 × se was based on capturing 95% of the data since the estimate is within 1.96 standard errors of the parameter about 95% of the time. the choice of 1.96 corresponds to a 95% confidence level."
1200,1,"['random', 'standard deviations', 'normally distributed', 'deviations', 'random variable', 'variable', 'probability', 'standard']", Confidence intervals for a proportion,seg_47,"if x is a normally distributed random variable, what is the probability of the value x being within 2.58 standard deviations of the mean?7"
1201,1,"['random', 'interval', 'standard deviations', 'deviations', 'random variable', 'variable', 'normal', 'mean', 'standard', 'normal random variable', 'confidence', 'confidence interval']", Confidence intervals for a proportion,seg_47,"guided practice 5.9 highlights that 99% of the time a normal random variable will be within 2.58 standard deviations of the mean. to create a 99% confidence interval, change 1.96 in the 95% confidence interval formula to be 2.58. that is, the formula for a 99% confidence interval is"
1202,1,['estimate'], Confidence intervals for a proportion,seg_47,point estimate ± 2.58× se
1203,1,"['standard deviations', 'deviations', 'mean', 'standard']", Confidence intervals for a proportion,seg_47,"99%, extends −2.58 to 2.58 95%, extends −1.96 to 1.96 −3 −2 −1 0 1 2 3 standard deviations from the mean"
1204,1,"['distribution', 'normal', 'confidence', 'level', 'tail', 'confidence level', 'normal distribution']", Confidence intervals for a proportion,seg_47,"figure 5.7: the area between -z? and z? increases as z? becomes larger. if the confidence level is 99%, we choose z? such that 99% of a normal normal distribution is between -z? and z?, which corresponds to 0.5% in the lower tail and 0.5% in the upper tail: z? = 2.58."
1205,1,"['levels', 'estimates', 'associated', 'estimate', 'cases', 'confidence', 'distributions', 'model', 'distribution', 'point estimates', 'sampling distribution', 'sampling', 'normal', 'point estimate', 'normal distribution']", Confidence intervals for a proportion,seg_47,"this approach – using the z-scores in the normal model to compute confidence levels – is appropriate when a point estimate such as p̂ is associated with a normal distribution. for some other point estimates, a normal model is not a good fit; in these cases, we’ll use alternative distributions that better represent the sampling distribution."
1206,1,"['model', 'interval', 'estimate', 'standard error', 'normal', 'population', 'point estimate', 'standard', 'parameter', 'confidence', 'error', 'confidence interval']", Confidence intervals for a proportion,seg_47,"if a point estimate closely follows a normal model with standard error se, then a confidence interval for the population parameter is"
1207,1,['estimate'], Confidence intervals for a proportion,seg_47,point estimate ± z? × se
1208,1,"['level', 'confidence', 'confidence level']", Confidence intervals for a proportion,seg_47,where z? corresponds to the confidence level selected.
1209,1,"['standard normal', 'distribution', 'normal', 'standard', 'level', 'standard normal distribution', 'confidence', 'confidence level', 'normal distribution']", Confidence intervals for a proportion,seg_47,"figure 5.7 provides a picture of how to identify z? based on a confidence level. we select z? so that the area between -z? and z? in the standard normal distribution, n(0, 1), corresponds to the confidence level."
1210,1,"['interval', 'margin of error', 'confidence', 'error', 'confidence interval']", Confidence intervals for a proportion,seg_47,"in a confidence interval, z? × se is called the margin of error."
1211,1,"['interval', 'data', 'normality', 'confidence', 'confidence interval']", Confidence intervals for a proportion,seg_47,use the data in example 5.8 to create a 90% confidence interval for the proportion of american adults that support expanding the use of solar power. we have already verified conditions for normality.
1212,1,"['confidence interval', 'interval', 'table', 'standard normal', 'probability table', 'distribution', 'normal', 'confidence', 'probability', 'standard', 'standard normal distribution', 'tail', 'statistical', 'normal distribution']", Confidence intervals for a proportion,seg_47,"we first find z? such that 90% of the distribution falls between -z? and z? in the standard normal distribution, n(µ = 0, σ = 1). we can do this using a graphing calculator, statistical software, or a probability table by looking for an upper tail of 5% (the other 5% is in the lower tail): z? = 1.65. the 90% confidence interval can then be computed as"
1213,1,['confident'], Confidence intervals for a proportion,seg_47,"that is, we are 90% confident that 87.1% to 90.4% of american adults supported the expansion of solar power in 2018."
1214,1,"['confidence', 'interval', 'confidence interval']", Confidence intervals for a proportion,seg_47,"once you’ve determined a one-proportion confidence interval would be helpful for an application, there are four steps to constructing the interval:"
1215,1,"['level', 'confidence', 'confidence level']", Confidence intervals for a proportion,seg_47,"prepare. identify p̂ and n, and determine what confidence level you wish to use."
1216,1,"['confidence intervals', 'condition', 'intervals', 'normal', 'confidence']", Confidence intervals for a proportion,seg_47,"check. verify the conditions to ensure p̂ is nearly normal. for one-proportion confidence intervals, use p̂ in place of p to check the success-failure condition."
1217,1,['interval'], Confidence intervals for a proportion,seg_47,"calculate. if the conditions hold, compute se using p̂, find z?, and construct the interval."
1218,1,"['confidence', 'interval', 'confidence interval']", Confidence intervals for a proportion,seg_47,conclude. interpret the confidence interval in the context of the problem.
1219,1,['case'], Confidence intervals for a proportion,seg_47,5.2.4 more case studies
1220,0,[], Confidence intervals for a proportion,seg_47,"in new york city on october 23rd, 2014, a doctor who had recently been treating ebola patients in guinea went to the hospital with a slight fever and was subsequently diagnosed with ebola. soon thereafter, an nbc 4 new york/the wall street journal/marist poll found that 82% of new yorkers favored a “mandatory 21-day quarantine for anyone who has come in contact with an ebola patient”. this poll included responses of 1,042 new york adults between oct 26th and 28th, 2014."
1221,1,"['model', 'estimate', 'case', 'distribution', 'normal', 'point estimate', 'normal distribution']", Confidence intervals for a proportion,seg_47,"what is the point estimate in this case, and is it reasonable to use a normal distribution to model that point estimate?"
1222,1,"['independence', 'random sample', 'random', 'sample', 'estimate', 'simple random sample', 'condition', 'distribution', 'sampling distribution', 'sampling', 'normal', 'point estimate', 'normal distribution']", Confidence intervals for a proportion,seg_47,"the point estimate, based on a sample of size n = 1042, is p̂ = 0.82. to check whether p̂ can be reasonably modeled using a normal distribution, we check independence (the poll is based on a simple random sample) and the success-failure condition (1042× p̂ ≈ 854 and 1042× (1− p̂) ≈ 188, both easily greater than 10). with the conditions met, we are assured that the sampling distribution of p̂ can be reasonably modeled using a normal distribution."
1223,1,"['standard error', 'error', 'standard']", Confidence intervals for a proportion,seg_47,estimate the standard error of p̂ = 0.82 from the ebola survey.
1224,1,"['standard error', 'approximation', 'substitution approximation', 'standard', 'error']", Confidence intervals for a proportion,seg_47,we’ll use the substitution approximation of p ≈ p̂ = 0.82 to compute the standard error:
1225,1,"['confidence', 'interval', 'confidence interval']", Confidence intervals for a proportion,seg_47,"construct a 95% confidence interval for p, the proportion of new york adults who supported a quarantine for anyone who has come into contact with an ebola patient."
1226,1,"['interval', 'estimate', 'standard error', 'point estimate', 'standard', 'level', 'confidence', 'confidence level', 'error', 'confidence interval']", Confidence intervals for a proportion,seg_47,"using the standard error se = 0.012 from example 5.12, the point estimate 0.82, and z? = 1.96 for a 95% confidence level, the confidence interval is"
1227,1,['estimate'], Confidence intervals for a proportion,seg_47,"point estimate ± z? × se → 0.82 ± 1.96× 0.012 → (0.796, 0.844)"
1228,1,['confident'], Confidence intervals for a proportion,seg_47,we are 95% confident that the proportion of new york adults in october 2014 who supported a quarantine for anyone who had come into contact with an ebola patient was between 0.796 and 0.844.
1229,1,"['confidence', 'interval', 'confidence interval']", Confidence intervals for a proportion,seg_47,answer the following two questions about the confidence interval from example 5.13:8
1230,1,"['mean', 'confident']", Confidence intervals for a proportion,seg_47,(a) what does 95% confident mean in this context?
1231,1,"['confidence', 'interval', 'confidence interval']", Confidence intervals for a proportion,seg_47,(b) do you think the confidence interval is still valid for the opinions of new yorkers today?
1232,0,[], Confidence intervals for a proportion,seg_47,"in the pew research poll about solar energy, they also inquired about other forms of energy, and"
1233,0,[], Confidence intervals for a proportion,seg_47,9 84.8% of the 1000 respondents supported expanding the use of wind turbines.
1234,1,['model'], Confidence intervals for a proportion,seg_47,(a) is it reasonable to model the proportion of us adults who support expanding wind turbines
1235,1,"['distribution', 'normal', 'normal distribution']", Confidence intervals for a proportion,seg_47,using a normal distribution?
1236,1,"['level', 'confidence', 'interval', 'confidence interval']", Confidence intervals for a proportion,seg_47,(b) create a 99% confidence interval for the level of american support for expanding the use of
1237,0,[], Confidence intervals for a proportion,seg_47,wind turbines for power generation.
1238,1,"['confidence intervals', 'parameters', 'interval', 'estimate', 'population mean', 'intervals', 'cases', 'margin of error', 'mean', 'point estimate', 'population', 'confidence', 'error', 'confidence interval']", Confidence intervals for a proportion,seg_47,"we can also construct confidence intervals for other parameters, such as a population mean. in these cases, a confidence interval would be computed in a similar way to that of a single proportion: a point estimate plus/minus some margin of error. we’ll dive into these details in later chapters."
1239,1,"['confidence intervals', 'intervals', 'confidence']", Confidence intervals for a proportion,seg_47,5.2.5 interpreting confidence intervals
1240,1,"['confidence intervals', 'data', 'intervals', 'confidence']", Confidence intervals for a proportion,seg_47,"in each of the examples, we described the confidence intervals by putting them into the context of the data and also using somewhat formal language:"
1241,1,['confident'], Confidence intervals for a proportion,seg_47,solar. we are 90% confident that 87.1% to 90.4% of american adults support the expansion of
1242,0,[], Confidence intervals for a proportion,seg_47,solar power in 2018.
1243,1,['confident'], Confidence intervals for a proportion,seg_47,ebola. we are 95% confident that the proportion of new york adults in october 2014 who sup-
1244,0,[], Confidence intervals for a proportion,seg_47,ported a quarantine for anyone who had come into contact with an ebola patient was between 0.796 and 0.844.
1245,1,['confident'], Confidence intervals for a proportion,seg_47,wind turbine. we are 99% confident the proportion of americans adults that support expanding
1246,1,"['parameter', 'population']", Confidence intervals for a proportion,seg_47,"first, notice that the statements are always about the population parameter, which considers all american adults for the energy polls or all new york adults for the quarantine poll."
1247,1,"['interval', 'probability', 'population', 'level', 'parameter', 'confidence', 'confidence level', 'error']", Confidence intervals for a proportion,seg_47,"we also avoided another common mistake: incorrect language might try to describe the confidence interval as capturing the population parameter with a certain probability. making a probability interpretation is a common error: while it might be useful to think of it as a probability, the confidence level only quantifies how plausible it is that the parameter is in the given interval."
1248,1,"['point estimates', 'confidence intervals', 'parameters', 'interval', 'estimates', 'range', 'observations', 'intervals', 'population', 'parameter', 'confidence', 'confidence interval']", Confidence intervals for a proportion,seg_47,another important consideration of confidence intervals is that they are only about the population parameter. a confidence interval says nothing about individual observations or point estimates. confidence intervals only provide a plausible range for population parameters.
1249,1,"['bias', 'error', 'data', 'sampling', 'set', 'data set', 'parameter', 'population', 'data collection', 'sampling error']", Confidence intervals for a proportion,seg_47,"lastly, keep in mind the methods we discussed only apply to sampling error, not to bias. if a data set is collected in a way that will tend to systematically under-estimate (or over-estimate) the population parameter, the techniques we have discussed will not address that problem. instead, we rely on careful data collection procedures to help protect against bias in the examples we have considered, which is a common practice employed by data scientists to combat bias."
1250,1,"['confidence', 'interval', 'confident', 'confidence interval']", Confidence intervals for a proportion,seg_47,"consider the 90% confidence interval for the solar energy survey: 87.1% to 90.4%. if we ran the survey again, can we say that we’re 90% confident that the new survey’s proportion will be between 87.1% and 90.4%?10"
1251,0,[], Hypothesis testing for a proportion,seg_49,"the following question comes from a book written by hans rosling, anna rosling rönnlund, and ola rosling called factfulness:"
1252,0,[], Hypothesis testing for a proportion,seg_49,how many of the world’s 1 year old children today have been vaccinated against some disease:
1253,0,[], Hypothesis testing for a proportion,seg_49,"write down what your answer (or guess), and when you’re ready, find the answer in the footnote.13"
1254,1,"['hypothesis tests', 'tests', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"in this section, we’ll be exploring how people with a 4-year college degree perform on this and other world health questions as we learn about hypothesis tests, which are a framework used to rigorously evaluate competing ideas and claims."
1255,1,"['hypothesis testing', 'hypothesis']", Hypothesis testing for a proportion,seg_49,5.3.1 hypothesis testing framework
1256,0,[], Hypothesis testing for a proportion,seg_49,"we’re interested in understanding how much people know about world health and development. if we take a multiple choice world health question, then we might like to understand if"
1257,1,['random'], Hypothesis testing for a proportion,seg_49,h0: people never learn these particular topics and their responses are simply equivalent to random
1258,1,['random'], Hypothesis testing for a proportion,seg_49,"ha: people have knowledge that helps them do better than random guessing, or perhaps, they have"
1259,1,['random'], Hypothesis testing for a proportion,seg_49,false knowledge that leads them to actually do worse than random guessing.
1260,1,"['alternative hypothesis', 'data', 'hypotheses', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"these competing ideas are called hypotheses. we callh0 the null hypothesis andha the alternative hypothesis. when there is a subscript 0 like in h0, data scientists pronounce it as “nought” (e.g. h0 is pronounced “h-nought”)."
1261,1,"['alternative hypothesis', 'range', 'parameter', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,the null hypothesis (h0) often represents a skeptical perspective or a claim to be tested. the alternative hypothesis (ha) represents an alternative claim under consideration and is often represented by a range of possible parameter values.
1262,1,"['alternative hypothesis', 'data', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"our job as data scientists is to play the role of a skeptic: before we buy into the alternative hypothesis, we need to see strong supporting evidence."
1263,1,"['null hypothesis', 'hypothesis', 'random']", Hypothesis testing for a proportion,seg_49,"the null hypothesis often represents a skeptical position or a perspective of “no difference”. in our first example, we’ll consider whether the typical person does any different than random guessing on roslings’ question about infant vaccinations."
1264,1,"['alternative hypothesis', 'statistics', 'case', 'information', 'mean', 'random', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"the alternative hypothesis generally represents a new or stronger perspective. in the case of the question about infant vaccinations, it would certainly be interesting to learn whether people do better than random guessing, since that would mean that the typical person knows something about world health statistics. it would also be very interesting if we learned that people do worse than random guessing, which would suggest people believe incorrect information about world health."
1265,1,"['hypothesis testing', 'set', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"the hypothesis testing framework is a very general tool, and we often use it without a second thought. if a person makes a somewhat unbelievable claim, we are initially skeptical. however, if there is sufficient evidence that supports the claim, we set aside our skepticism and reject the null hypothesis in favor of the alternative. the hallmarks of hypothesis testing are also found in the us court system."
1266,1,"['set', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"a us court considers two possible claims about a defendant: she is either innocent or guilty. if we set these claims up in a hypothesis framework, which would be the null hypothesis and which the alternative?14"
1267,1,"['alternative hypothesis', 'case', 'hypothesis testing', 'mean', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"jurors examine the evidence to see whether it convincingly shows a defendant is guilty. even if the jurors leave unconvinced of guilt beyond a reasonable doubt, this does not mean they believe the defendant is innocent. this is also the case with hypothesis testing: even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as true. failing to find strong evidence for the alternative hypothesis is not equivalent to accepting the null hypothesis."
1268,1,"['alternative hypothesis', 'hypotheses', 'random', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"when considering roslings’ question about infant vaccination, the null hypothesis represents the notion that the people we will be considering – college-educated adults – are as accurate as random guessing. that is, the proportion p of respondents who pick the correct answer, that 80% of 1 year olds have been vaccinated against some disease, is about 33.3% (or 1-in-3 if wanting to be perfectly precise). the alternative hypothesis is that this proportion is something other than 33.3%. while it’s helpful to write these hypotheses in words, it can be useful to write them using mathematical notation:"
1269,1,"['null value', 'case', 'parameter', 'population', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"in this hypothesis setup, we want to make a conclusion about the population parameter p. the value we are comparing the parameter to is called the null value, which in this case is 0.333. it’s common to label the null value with the same symbol as the parameter but with a subscript ‘0’. that is, in this case, the null value is p0 = 0.333 (pronounced “p-nought equals 0.333”)."
1270,1,"['null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"it may seem impossible that the proportion of people who get the correct answer is exactly 33.3%. if we don’t believe the null hypothesis, should we simply reject it?"
1271,1,"['hypothesis testing', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"no. while we may not buy into the notion that the proportion is exactly 33.3%, the hypothesis testing framework requires that there be strong evidence before we reject the null hypothesis and conclude something more interesting."
1272,1,"['data', 'random']", Hypothesis testing for a proportion,seg_49,"after all, even if we don’t believe the proportion is exactly 33.3%, that doesn’t really tell us anything useful! we would still be stuck with the original question: do people do better or worse than random guessing on roslings’ question? without data that strongly points in one direction or the other, it is both uninteresting and pointless to reject h0."
1273,1,"['evaluating', 'null and alternative hypotheses', 'hypothesis testing', 'hypotheses', 'alternative hypotheses', 'hypothesis']", Hypothesis testing for a proportion,seg_49,another example of a real-world hypothesis testing situation is evaluating whether a new drug is better or worse than an existing drug at treating a particular disease. what should we use for the null and alternative hypotheses in this case?15
1274,1,"['confidence intervals', 'intervals', 'hypotheses', 'confidence']", Hypothesis testing for a proportion,seg_49,5.3.2 testing hypotheses using confidence intervals
1275,1,"['evaluating', 'hypothesis test', 'data', 'set', 'data set', 'test', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"we will use the rosling responses data set to evaluate the hypothesis test evaluating whether college-educated adults who get the question about infant vaccination correct is different from 33.3%. this data set summarizes the answers of 50 college-educated adults. of these 50 adults, 24% of respondents got the question correct that 80% of 1 year olds have been vaccinated against some disease."
1276,1,['data'], Hypothesis testing for a proportion,seg_49,"up until now, our discussion has been philosophical. however, now that we have data, we might ask ourselves: does the data provide strong evidence that the proportion of all college-educated adults who would answer this question correctly is different than 33.3%?"
1277,1,"['deviation', 'sample', 'data', 'population']", Hypothesis testing for a proportion,seg_49,"we learned in section 5.1 that there is fluctuation from one sample to another, and it is unlikely that our sample proportion, p̂, will exactly equal p, but we want to make a conclusion about p. we have a nagging concern: is this deviation of 24% from 33.3% simply due to chance, or does the data provide strong evidence that the population proportion is different from 33.3%?"
1278,1,"['confidence intervals', 'method', 'variability', 'estimate', 'uncertainty', 'hypothesis test', 'intervals', 'confidence', 'test', 'measuring', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"in section 5.2, we learned how to quantify the uncertainty in our estimate using confidence intervals. the same method for measuring variability can be useful for the hypothesis test."
1279,1,"['sample', 'interval', 'data', 'confidence', 'confidence interval']", Hypothesis testing for a proportion,seg_49,"check whether it is reasonable to construct a confidence interval for p using the sample data, and if so, construct a 95% confidence interval."
1280,1,"['sample', 'condition', 'simple random sample', 'random sample', 'data', 'independence', 'normal', 'random']", Hypothesis testing for a proportion,seg_49,"the conditions are met for p̂ to be approximately normal: the data come from a simple random sample (satisfies independence), and np̂ = 12 and n(1− p̂) = 38 are both at least 10 (success-failure condition)."
1281,1,"['error', 'interval', 'estimate', 'standard error', 'point estimate', 'standard', 'level', 'confidence', 'confidence level', 'critical value', 'confidence interval']", Hypothesis testing for a proportion,seg_49,"to construct the confidence interval, we will need to identify the point estimate (p̂ = 0.24), the critical value for the 95% confidence level (z? = 1.96), and the standard error of p̂ (sep̂ = √p̂(1− p̂)/n = 0.060). with those pieces, the confidence interval for p can be constructed:"
1282,1,['confident'], Hypothesis testing for a proportion,seg_49,we are 95% confident that the proportion of all college-educated adults to correctly answer this particular question about infant vaccination is between 12.2% and 35.8%.
1283,1,"['interval', 'range', 'hypothesis test', 'null value', 'data', 'random', 'confidence', 'test', 'null hypothesis', 'confidence interval', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"because the null value in the hypothesis test is p0 = 0.333, which falls within the range of plausible values from the confidence interval, we cannot say the null value is implausible.16 that is, the data do not provide sufficient evidence to reject the notion that the performance of collegeeducated adults was different than random guessing, and we do not reject the null hypothesis, h0."
1284,0,[], Hypothesis testing for a proportion,seg_49,explain why we cannot conclude that college-educated adults simply guessed on the infant vaccination question.
1285,1,"['sample', 'mean', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"while we failed to reject h0, that does not necessarily mean the null hypothesis is true. perhaps there was an actual difference, but we were not able to detect it with the relatively small sample of 50."
1286,1,"['statistical', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"in many statistical explanations, we use double negatives. for instance, we might say that the null hypothesis is not implausible or we failed to reject the null hypothesis. double negatives are used to communicate that while we are not rejecting a position, we are also not saying it is correct."
1287,0,[], Hypothesis testing for a proportion,seg_49,let’s move onto a second question posed by the roslings:
1288,0,[], Hypothesis testing for a proportion,seg_49,"there are 2 billion children in the world today aged 0-15 years old, how many children will there be in year 2100 according to the united nations?"
1289,1,"['hypotheses', 'random']", Hypothesis testing for a proportion,seg_49,"set up appropriate hypotheses to evaluate whether college-educated adults are better than random guessing on this question. also, see if you can guess the correct answer before checking the answer in the footnote!17"
1290,1,"['sample', 'model', 'distribution', 'normal', 'confidence', 'normal distribution']", Hypothesis testing for a proportion,seg_49,"this time we took a larger sample of 228 college-educated adults, 34 (14.9%) selected the correct answer to the question in guided practice 5.22: 2 billion. can we model the sample proportion using a normal distribution and construct a confidence interval?18"
1291,1,"['interval', 'confidence', 'hypotheses', 'confidence interval']", Hypothesis testing for a proportion,seg_49,"compute a 95% confidence interval for the fraction of college-educated adults who answered the children-in-2100 question correctly, and evaluate the hypotheses in guided practice 5.22."
1292,1,"['standard error', 'error', 'standard']", Hypothesis testing for a proportion,seg_49,"to compute the standard error, we’ll again use p̂ in place of p for the calculation:"
1293,1,"['interval', 'distribution', 'normal distribution', 'normal', 'confidence', 'confidence interval']", Hypothesis testing for a proportion,seg_49,"in guided practice 5.23, we found that p̂ can be modeled using a normal distribution, which ensures a 95% confidence interval may be accurately constructed as"
1294,1,"['interval', 'null value', 'data', 'statistically significant', 'population', 'random', 'confidence', 'null hypothesis', 'confidence interval', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"because the null value, p0 = 0.333, is not in the confidence interval, a population proportion of 0.333 is implausible and we reject the null hypothesis. that is, the data provide statistically significant evidence that the actual proportion of college adults who get the children-in-2100 question correct is different from random guessing. because the entire 95% confidence interval is below 0.333, we can conclude college-educated adults do worse than random guessing on this question."
1295,1,"['interval', 'level', 'confidence', 'confidence level', 'confidence interval']", Hypothesis testing for a proportion,seg_49,"one subtle consideration is that we used a 95% confidence interval. what if we had used a 99% confidence level? or even a 99.9% confidence level? it’s possible to come to a different conclusion if using a different confidence level. therefore, when we make a conclusion based on confidence interval, we should also be sure it is clear what confidence level we used."
1296,1,['random'], Hypothesis testing for a proportion,seg_49,"the worse-than-random performance on this last question is not a fluke: there are many such world health questions where people do worse than random guessing. in general, the answers suggest that people tend to be more pessimistic about progress than reality suggests. this topic is discussed in much greater detail in the roslings’ book, factfulness."
1297,1,['errors'], Hypothesis testing for a proportion,seg_49,5.3.3 decision errors
1298,1,"['tests', 'hypothesis tests', 'hypothesis test', 'statistical hypothesis', 'data', 'errors', 'test', 'statistical', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"hypothesis tests are not flawless: we can make an incorrect decision in a statistical hypothesis test based on the data. for example, in the court system innocent people are sometimes wrongly convicted and the guilty sometimes walk free. one key distinction with statistical hypothesis tests is that we have the tools necessary to probabilistically quantify how often we make errors in our conclusions."
1299,1,"['hypothesis test', 'test', 'hypotheses', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"recall that there are two competing hypotheses: the null and the alternative. in a hypothesis test, we make a statement about which one might be true, but we might choose incorrectly. there are four possible scenarios, which are summarized in figure 5.8."
1300,0,[], Hypothesis testing for a proportion,seg_49,do not reject h0 reject h0 in favor of ha
1301,1,"['error', 'type 1 error']", Hypothesis testing for a proportion,seg_49,h0 true okay type 1 error
1302,1,"['type 2 error', 'error']", Hypothesis testing for a proportion,seg_49,truth ha true type 2 error okay
1303,1,"['hypothesis tests', 'tests', 'hypothesis']", Hypothesis testing for a proportion,seg_49,figure 5.8: four different scenarios for hypothesis tests.
1304,1,"['type 2 error', 'null hypothesis', 'error', 'type 1 error', 'hypothesis']", Hypothesis testing for a proportion,seg_49,a type 1 error is rejecting the null hypothesis when h0 is actually true. a type 2 error is failing to reject the null hypothesis when the alternative is actually true.
1305,1,"['error', 'type 1 error']", Hypothesis testing for a proportion,seg_49,"in a us court, the defendant is either innocent (h0) or guilty (ha). what does a type 1 error"
1306,1,"['type 2 error', 'error']", Hypothesis testing for a proportion,seg_49,19 represent in this context? what does a type 2 error represent? figure 5.8 may be useful.
1307,1,"['rate', 'type 2 error', 'error', 'type 1 error']", Hypothesis testing for a proportion,seg_49,how could we reduce the type 1 error rate in us courts? what influence would this have on the type 2 error rate?
1308,1,"['rate', 'type 2 errors', 'errors', 'standard', 'error', 'type 1 error']", Hypothesis testing for a proportion,seg_49,"to lower the type 1 error rate, we might raise our standard for conviction from “beyond a reasonable doubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted. however, this would also make it more difficult to convict the people who are actually guilty, so we would make more type 2 errors."
1309,1,"['rate', 'type 2 error', 'error', 'type 1 error']", Hypothesis testing for a proportion,seg_49,how could we reduce the type 2 error rate in us courts? what influence would this have on the type 1 error rate?20
1310,1,['error'], Hypothesis testing for a proportion,seg_49,"exercises 5.25-5.27 provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type."
1311,1,"['levels', 'null hypothesis', 'data', 'cases', 'mean', 'level', 'significance', 'significance level', 'significance levels', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"hypothesis testing is built around rejecting or failing to reject the null hypothesis. that is, we do not reject h0 unless we have strong evidence. but what precisely does strong evidence mean? as a general rule of thumb, for those cases where the null hypothesis is actually true, we do not want to incorrectly reject h0 more than 5% of the time. this corresponds to a significance level of 0.05. that is, if the null hypothesis is true, the significance level indicates how often the data lead us to incorrectly reject h0. we often write the significance level using α (the greek letter alpha): α = 0.05. we discuss the appropriateness of different significance levels in section 5.3.5."
1312,1,"['interval', 'significance', 'null hypothesis', 'estimate', 'parameter', 'population', 'standard', 'confidence', 'standard errors', 'error', 'confidence interval', 'level', 'test', 'hypothesis', 'hypothesis test', 'errors', 'point estimate', 'tail', 'significance level']", Hypothesis testing for a proportion,seg_49,"if we use a 95% confidence interval to evaluate a hypothesis test and the null hypothesis happens to be true, we will make an error whenever the point estimate is at least 1.96 standard errors away from the population parameter. this happens about 5% of the time (2.5% in each tail). similarly, using a 99% confidence interval to evaluate a hypothesis is equivalent to a significance level of α = 0.01."
1313,1,"['confidence intervals', 'interval', 'intervals', 'hypothesis', 'confidence', 'null hypothesis', 'confidence interval', 'sustainable']", Hypothesis testing for a proportion,seg_49,"a confidence interval is very helpful in determining whether or not to reject the null hypothesis. however, the confidence interval approach isn’t always sustainable. in several sections, we will encounter situations where a confidence interval cannot be constructed. for example, if we wanted to evaluate the hypothesis that several proportions are equal, it isn’t clear how to construct and compare many confidence intervals altogether."
1314,1,"['statistic', 'statistical', 'data']", Hypothesis testing for a proportion,seg_49,"next we will introduce a statistic called the p-value to help us expand our statistical toolkit, which will enable us to both better understand the strength of evidence and work in more complex data scenarios in later sections."
1315,0,[], Hypothesis testing for a proportion,seg_49,5.3.4 formal testing using p-values
1316,1,"['alternative hypothesis', 'confidence intervals', 'method', 'statistical hypothesis', 'intervals', 'statistical hypothesis testing', 'statistical', 'hypothesis testing', 'confidence', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,the p-value is a way of quantifying the strength of the evidence against the null hypothesis and in favor of the alternative hypothesis. statistical hypothesis testing typically uses the p-value method rather than making a decision based on confidence intervals.
1317,1,"['sample', 'data', 'statistic', 'hypotheses', 'set', 'probability', 'data set', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"the p-value is the probability of observing data at least as favorable to the alternative hypothesis as our current data set, if the null hypothesis were true. we typically use a summary statistic of the data, in this section the sample proportion, to help compute the p-value and evaluate the hypotheses."
1318,1,"['sample', 'random', 'set', 'hypotheses', 'random sample']", Hypothesis testing for a proportion,seg_49,pew research asked a random sample of 1000 american adults whether they supported the increased usage of coal to produce energy. set up hypotheses to evaluate whether a majority of american adults support or oppose the increased usage of coal.
1319,1,"['alternative hypothesis', 'hypotheses', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"the uninteresting result is that there is no majority either way: half of americans support and the other half oppose expanding the use of coal to produce energy. the alternative hypothesis would be that there is a majority support or oppose (though we do not known which one!) expanding the use of coal. if p represents the proportion supporting, then we can write the hypotheses as"
1320,1,"['null value', 'case']", Hypothesis testing for a proportion,seg_49,"in this case, the null value is p0 = 0.5."
1321,1,"['condition', 'method', 'evaluating', 'standard error', 'null value', 'case', 'hypotheses', 'standard', 'error']", Hypothesis testing for a proportion,seg_49,"when evaluating hypotheses for proportions using the p-value method, we will slightly modify how we check the success-failure condition and compute the standard error for the single proportion case. these changes aren’t dramatic, but pay close attention to how we use the null value, p0."
1322,1,"['sample', 'sampling', 'distribution', 'sampling distribution', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"pew research’s sample show that 37% of american adults support increased usage of coal. we now wonder, does 37% represent a real difference from the null hypothesis of 50%? what would the sampling distribution of p̂ look like if the null hypothesis were true?"
1323,1,"['null value', 'sampling', 'distribution', 'normal', 'population', 'sampling distribution', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"if the null hypothesis were true, the population proportion would be the null value, 0.5. we previously learned that the sampling distribution of p̂ will be normal when two conditions are met:"
1324,1,"['sample', 'random', 'simple random sample', 'independence', 'random sample']", Hypothesis testing for a proportion,seg_49,"independence. the poll was based on a simple random sample, so independence is satisfied."
1325,1,"['sample size', 'sample', 'condition']", Hypothesis testing for a proportion,seg_49,"success-failure. based on the poll’s sample size of n = 1000, the success-failure condition is met,"
1326,1,"['confidence intervals', 'condition', 'null value', 'intervals', 'confidence']", Hypothesis testing for a proportion,seg_49,"are both at least 10. note that the success-failure condition was checked using the null value, p0 = 0.5; this is the first procedural difference from confidence intervals."
1327,1,"['sample', 'error', 'standard error', 'observations', 'null value', 'normally distributed', 'sampling', 'distribution', 'standard', 'sampling distribution', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"if the null hypothesis were true, the sampling distribution indicates that a sample proportion based on n = 1000 observations would be normally distributed. next, we can compute the standard error, where we will again use the null value p0 = 0.5 in the calculation:"
1328,1,"['confidence intervals', 'null value', 'intervals', 'sampling', 'confidence']", Hypothesis testing for a proportion,seg_49,"this marks the other procedural difference from confidence intervals: since the sampling distribution is determined under the null proportion, the null value p0 was used for the proportion in the calculation rather than p̂."
1329,1,"['sample', 'error', 'standard error', 'distribution', 'normal', 'mean', 'standard', 'null hypothesis', 'normal distribution', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"ultimately, if the null hypothesis were true, then the sample proportion should follow a normal distribution with mean 0.5 and a standard error of 0.016. this distribution is shown in figure 5.9."
1330,1,"['distribution', 'normal', 'null hypothesis', 'normal distribution', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"figure 5.9: if the null hypothesis were true, this normal distribution describes the distribution of p̂."
1331,1,"['sample', 'method', 'standard error', 'hypothesis test', 'null value', 'standard', 'test', 'error', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"when using the p-value method to evaluate a hypothesis test, we check the conditions for p̂ and construct the standard error using the null value, p0, instead of using the sample proportion."
1332,1,"['error', 'interval', 'standard error', 'hypothesis test', 'standard', 'confidence', 'test', 'null hypothesis', 'confidence interval', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"in a hypothesis test with a p-value, we are supposing the null hypothesis is true, which is a different mindset than when we compute a confidence interval. this is why we use p0 instead of p̂ when we check conditions and compute the standard error in this context."
1333,1,"['estimate', 'null distribution', 'sampling', 'distribution', 'hypothesis', 'point estimate', 'probability', 'tail', 'sampling distribution', 'null hypothesis']", Hypothesis testing for a proportion,seg_49,"when we identify the sampling distribution under the null hypothesis, it has a special name: the null distribution. the p-value represents the probability of the observed p̂, or a p̂ that is more extreme, if the null hypothesis were true. to find the p-value, we generally find the null distribution, and then we find a tail area in that distribution corresponding to our point estimate."
1334,1,"['distribution', 'normal', 'mean', 'null distribution', 'tails', 'null hypothesis', 'normal distribution', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"if the null hypothesis were true, determine the chance of finding p̂ at least as far into the tails as 0.37 under the null distribution, which is a normal distribution with mean µ = 0.5 and se = 0.016."
1335,1,"['standard error', 'tail', 'normal', 'mean', 'probability', 'standard', 'error']", Hypothesis testing for a proportion,seg_49,"this is a normal probability problem where x = 0.37. first, we draw a simple graph to represent the situation, similar to what is shown in figure 5.9. since p̂ is so far out in the tail, we know the tail area is going to be very small. to find it, we start by computing the z-score using the mean of 0.5 and the standard error of 0.016:"
1336,1,"['table', 'probability table', 'normal', 'probability', 'tail', 'normal probability table']", Hypothesis testing for a proportion,seg_49,"we can use software to find the tail area: 2.2 × 10−16 (0.00000000000000022). if using the normal probability table in appendix c.1, we’d find that z = −8.125 is off the table, so we would use the smallest area listed: 0.0002."
1337,1,"['method', 'estimate', 'table', 'observations', 'hypothesis', 'tail']", Hypothesis testing for a proportion,seg_49,"the potential p̂’s in the upper tail beyond 0.63, which are shown in figure 5.10, also represent observations at least as extreme as the observed value of 0.37. to account for these values that are also more extreme under the hypothesis setup, we double the lower tail to get an estimate of the p-value: 4.4× 10−16 (or if using the table method, 0.0004)."
1338,1,"['sample', 'probability', 'hypothesis', 'null hypothesis']", Hypothesis testing for a proportion,seg_49,"the p-value represents the probability of observing such an extreme sample proportion by chance, if the null hypothesis were true."
1339,0,[], Hypothesis testing for a proportion,seg_49,tail area for p̂ equally unlikely if h0 is true
1340,0,[], Hypothesis testing for a proportion,seg_49,"figure 5.10: if h0 were true, then the values above 0.63 are just as unlikely as values below 0.37."
1341,1,"['hypotheses', 'standard', 'level', 'significance', 'significance level']", Hypothesis testing for a proportion,seg_49,how should we evaluate the hypotheses using the p-value of 4.4×10−16? use the standard significance level of α = 0.05.
1342,1,"['deviation', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"if the null hypothesis were true, there’s only an incredibly small chance of observing such an extreme deviation of p̂ from 0.5. this means one of the following must be true:"
1343,1,"['null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"1. the null hypothesis is true, and we just happened to get observe something so extreme that only happens about once in every 23 quadrillion times (1 quadrillion = 1 million × 1 billion)."
1344,1,"['alternative hypothesis', 'sample', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"2. the alternative hypothesis is true, which would be consistent with observing a sample proportion far from 0.5."
1345,0,[], Hypothesis testing for a proportion,seg_49,"the first scenario is laughably improbable, while the second scenario seems much more plausible."
1346,1,"['hypothesis test', 'case', 'data', 'hypothesis', 'level', 'significance', 'test', 'significance level', 'null hypothesis']", Hypothesis testing for a proportion,seg_49,"formally, when we evaluate a hypothesis test, we compare the p-value to the significance level, which in this case is α = 0.05. since the p-value is less than α, we reject the null hypothesis. that is, the data provide strong evidence against h0. the data indicate the direction of the difference: a majority of americans do not support expanding the use of coal-powered energy."
1347,1,"['alternative hypothesis', 'data', 'hypothesis', 'level', 'significance', 'significance level']", Hypothesis testing for a proportion,seg_49,"when the p-value is less than the significance level, α, reject h0. we would report a conclusion that the data provide strong evidence supporting the alternative hypothesis."
1348,1,"['null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"when the p-value is greater than α, do not reject h0, and report that we do not have sufficient evidence to reject the null hypothesis."
1349,1,"['case', 'data']", Hypothesis testing for a proportion,seg_49,"in either case, it is important to describe the conclusion in the context of the data."
1350,1,"['set', 'hypotheses']", Hypothesis testing for a proportion,seg_49,do a majority of americans support or oppose nuclear arms reduction? set up hypotheses to
1351,0,[], Hypothesis testing for a proportion,seg_49,21 evaluate this question.
1352,1,"['sample', 'random', 'simple random sample', 'random sample', 'level', 'significance', 'significance level']", Hypothesis testing for a proportion,seg_49,a simple random sample of 1028 us adults in march 2013 show that 56% support nuclear arms reduction. does this provide convincing evidence that a majority of americans supported nuclear arms reduction at the 5% significance level?
1353,0,[], Hypothesis testing for a proportion,seg_49,"first, check conditions:"
1354,1,"['sample', 'random', 'simple random sample', 'observations', 'random sample']", Hypothesis testing for a proportion,seg_49,"independence. the poll was of a simple random sample of us adults, meaning the observations"
1355,1,['independent'], Hypothesis testing for a proportion,seg_49,are independent.
1356,1,"['hypothesis test', 'test', 'condition', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"success-failure. in a one-proportion hypothesis test, this condition is checked using the null pro-"
1357,1,"['model', 'normal']", Hypothesis testing for a proportion,seg_49,"with these conditions verified, we can model p̂ using a normal model."
1358,1,"['standard error', 'hypothesis test', 'null value', 'standard', 'test', 'error', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"next the standard error can be computed. the null value p0 is used again here, because this is a hypothesis test for a single proportion."
1359,1,"['model', 'estimate', 'test statistic', 'statistic', 'normal', 'point estimate', 'test']", Hypothesis testing for a proportion,seg_49,"based on the normal model, the test statistic can be computed as the z-score of the point estimate:"
1360,1,['null value'], Hypothesis testing for a proportion,seg_49,point estimate− null value 0.56− 0.50 z = = = 3.85 se 0.0156
1361,1,"['null distribution', 'distribution', 'tail areas', 'tail']", Hypothesis testing for a proportion,seg_49,it’s generally helpful to draw null distribution and the tail areas of interest for computing the p-value:
1362,1,['tail'], Hypothesis testing for a proportion,seg_49,lower tail upper tail
1363,1,['tail'], Hypothesis testing for a proportion,seg_49,"the upper tail area is about 0.0001, and we double this tail area to get the p-value: 0.0002. because the p-value is smaller than 0.05, we reject h0. the poll provides convincing evidence that a majority of americans supported nuclear arms reduction efforts in march 2013."
1364,1,"['hypothesis test', 'test', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"once you’ve determined a one-proportion hypothesis test is the correct procedure, there are four steps to completing the test:"
1365,1,"['parameter of interest', 'hypotheses', 'parameter', 'level', 'significance', 'significance level']", Hypothesis testing for a proportion,seg_49,"prepare. identify the parameter of interest, list hypotheses, identify the significance level, and identify p̂ and n."
1366,1,"['condition', 'hypothesis tests', 'null value', 'normal', 'tests', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"check. verify conditions to ensure p̂ is nearly normal under h0. for one-proportion hypothesis tests, use the null value to check the success-failure condition."
1367,1,"['standard error', 'standard', 'error']", Hypothesis testing for a proportion,seg_49,"calculate. if the conditions hold, compute the standard error, again using p0, compute the z-score, and identify the p-value."
1368,1,"['test', 'hypothesis test', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"conclude. evaluate the hypothesis test by comparing the p-value to α, and provide a conclusion in the context of the problem."
1369,1,"['level', 'significance', 'significance level']", Hypothesis testing for a proportion,seg_49,5.3.5 choosing a significance level
1370,1,"['consequences', 'level', 'significance', 'test', 'significance level']", Hypothesis testing for a proportion,seg_49,"choosing a significance level for a test is important in many contexts, and the traditional level is α = 0.05. however, it can be helpful to adjust the significance level based on the application. we may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test."
1371,1,"['error', 'level', 'significance', 'significance level', 'null hypothesis', 'type 1 error', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"if making a type 1 error is dangerous or especially costly, we should choose a small significance level (e.g. 0.01). under this scenario we want to be very cautious about rejecting the null hypothesis, so we demand very strong evidence favoring ha before we would reject h0."
1372,1,"['alternative hypothesis', 'type 2 error', 'level', 'significance', 'significance level', 'error', 'type 1 error', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"if a type 2 error is relatively more dangerous or much more costly than a type 1 error, then we might choose a higher significance level (e.g. 0.10). here we want to be cautious about failing to reject h0 when the alternative hypothesis is actually true."
1373,1,"['rate', 'type 2 error', 'data', 'error', 'type 1 error']", Hypothesis testing for a proportion,seg_49,"additionally, if the cost of collecting data is small relative to the cost of a type 2 error, then it may also be a good strategy to collect more data. under this strategy, the type 2 error can be reduced while not affecting the type 1 error rate. of course, collecting extra data is often costly, so there is typically a cost-benefit analysis to be considered."
1374,1,"['hypothesis test', 'level', 'significance', 'test', 'significance level', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"a car manufacturer is considering switching to a new, higher quality piece of equipment that constructs vehicle door hinges. they figure that they will save money in the long run if this new machine produces hinges that have flaws less than 0.2% of the time. however, if the hinges are flawed more than 0.2% of the time, they wouldn’t get a good enough return-on-investment from the new piece of equipment, and they would lose money. is there good reason to modify the significance level in such a hypothesis test?"
1375,1,"['rate', 'error', 'marginal', 'type 2 error', 'level', 'significance', 'significance level', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"the null hypothesis would be that the rate of flawed hinges is 0.2%, while the alternative is that it the rate is different than 0.2%. this decision is just one of many that have a marginal impact on the car and company. a significance level of 0.05 seems reasonable since neither a type 1 or type 2 error should be dangerous or (relatively) much more expensive."
1376,1,"['level', 'significance', 'significance level']", Hypothesis testing for a proportion,seg_49,"the same car manufacturer is considering a slightly more expensive supplier for parts related to safety, not door hinges. if the durability of these safety components is shown to be better than the current supplier, they will switch manufacturers. is there good reason to modify the significance level in such an evaluation?"
1377,1,"['level', 'significance', 'significance level', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"the null hypothesis would be that the suppliers’ parts are equally reliable. because safety is involved, the car company should be eager to switch to the slightly more expensive manufacturer (reject h0), even if the evidence of increased safety is only moderately strong. a slightly larger significance level, such as α = 0.10, might be appropriate."
1378,1,"['functions', 'hypotheses', 'measurements', 'significance', 'test']", Hypothesis testing for a proportion,seg_49,"a part inside of a machine is very expensive to replace. however, the machine usually functions properly even if this part is broken, so the part is replaced only if we are extremely certain it is broken based on a series of measurements. identify appropriate hypotheses for this test (in plain language) and suggest an appropriate significance level.22"
1379,1,"['level', 'standard']", Hypothesis testing for a proportion,seg_49,"the α = 0.05 threshold is most common. but why? maybe the standard level should be smaller, or perhaps larger. if you’re a little puzzled, you’re reading with an extra critical eye – good job! we’ve made a 5-minute task to help clarify why 0.05 :"
1380,1,"['statistical', 'significance', 'statistical significance']", Hypothesis testing for a proportion,seg_49,5.3.6 statistical significance versus practical significance
1381,1,"['sample size', 'sample', 'experiment', 'estimates', 'null value', 'statistically significant', 'samples', 'cases', 'mean', 'point estimates']", Hypothesis testing for a proportion,seg_49,"when the sample size becomes larger, point estimates become more precise and any real differences in the mean and null value become easier to detect and recognize. even a very small difference would likely be detected if we took a large enough sample. sometimes researchers will take such large samples that even the slightest difference is detected, even differences where there is no practical value. in such cases, we still say the difference is statistically significant, but it is not practically significant. for example, an online experiment might identify that placing additional ads on a movie review website statistically significantly increases viewership of a tv show by 0.001%, but this increase might not have any practical value."
1382,1,"['sample size', 'sample', 'estimate', 'standard error', 'null value', 'data', 'information', 'risks', 'standard', 'error']", Hypothesis testing for a proportion,seg_49,"one role of a data scientist in conducting a study often includes planning the size of the study. the data scientist might first consult experts or scientific literature to learn what would be the smallest meaningful difference from the null value. she also would obtain other information, such as a very rough estimate of the true proportion p, so that she could roughly estimate the standard error. from here, she can suggest a sample size that is sufficiently large that, if there is a real difference that is meaningful, we could detect it. while larger sample sizes may still be used, these calculations are especially helpful when considering costs or potential risks, such as possible health impacts to volunteers in a medical study."
1383,1,"['hypothesis tests', 'tests', 'hypothesis']", Hypothesis testing for a proportion,seg_49,5.3.7 one-sided hypothesis tests (special topic)
1384,1,"['hypothesis tests', 'hypothesis test', 'null value', 'hypotheses', 'tests', 'test', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"so far we’ve only considered what are called two-sided hypothesis tests, where we care about detecting whether p is either above or below some null value p0. there is a second type of hypothesis test called a one-sided hypothesis test. for a one-sided hypothesis test, the hypotheses take one of the following forms:"
1385,1,"['alternative hypothesis', 'null value', 'case', 'parameter', 'population', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"1. there’s only value in detecting if the population parameter is less than some value p0. in this case, the alternative hypothesis is written as p < p0 for some null value p0."
1386,1,"['alternative hypothesis', 'case', 'parameter', 'population', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"2. there’s only value in detecting if the population parameter is more than some value p0: in this case, the alternative hypothesis is written as p > p0."
1387,1,"['alternative hypothesis', 'hypothesis test', 'case', 'test', 'null hypothesis', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"while we adjust the form of the alternative hypothesis, we continue to write the null hypothesis using an equals-sign in the one-sided hypothesis test case."
1388,1,"['alternative hypothesis', 'tests', 'evaluating', 'hypothesis test', 'hypothesis testing', 'hypothesis', 'level', 'tail', 'test']", Hypothesis testing for a proportion,seg_49,"in the entire hypothesis testing procedure, there is only one difference in evaluating a onesided hypothesis test vs a two-sided hypothesis test: how to compute the p-value. in a one-sided hypothesis test, we compute the p-value as the tail area in the direction of the alternative hypothesis only, meaning it is represented by a single tail area. herein lies the reason why one-sided tests are sometimes interesting: if we don’t have to double the tail area to get the p-value, then the p-value is smaller and the level of evidence required to identify an interesting finding in the direction of the alternative hypothesis goes down. however, one-sided tests aren’t all sunshine and rainbows: the heavy price paid is that any interesting findings in the opposite direction must be disregarded."
1389,1,"['risk', 'test', 'data']", Hypothesis testing for a proportion,seg_49,"in section 1.1, we encountered an example where doctors were interested in determining whether stents would help people who had a high risk of stroke. the researchers believed the stents would help. unfortunately, the data showed the opposite: patients who received stents actually did worse. why was using a two-sided test so important in this context?"
1390,1,['test'], Hypothesis testing for a proportion,seg_49,"before the study, researchers had reason to believe that stents would help patients since existing research suggested stents helped in patients with heart attacks. it would surely have been tempting to use a one-sided test in this situation, and had they done this, they would have limited their ability to identify potential harm to patients."
1391,1,"['risk', 'data', 'random', 'test', 'error', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"example 5.37 highlights that using a one-sided hypothesis creates a risk of overlooking data supporting the opposite conclusion. we could have made a similar error when reviewing the roslings’ question data this section; if we had a pre-conceived notion that college-educated people wouldn’t do worse than random guessing and so used a one-sided test, we would have missed the really interesting finding that many people have incorrect knowledge about global public health."
1392,1,['test'], Hypothesis testing for a proportion,seg_49,"when might a one-sided test be appropriate to use? very rarely. should you ever find yourself considering using a one-sided test, carefully answer the following question:"
1393,1,"['alternative hypothesis', 'data', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"what would i, or others, conclude if the data happens to go clearly in the opposite direction than my alternative hypothesis?"
1394,1,"['hypothesis test', 'data', 'tests', 'test', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"if you or others would find any value in making a conclusion about the data that goes in the opposite direction of a one-sided test, then a two-sided hypothesis test should actually be used. these considerations can be subtle, so exercise caution. we will only apply two-sided tests in the rest of this book."
1395,1,"['test', 'data']", Hypothesis testing for a proportion,seg_49,why can’t we simply run a one-sided test that goes in the direction of the data?
1396,1,"['hypothesis test', 'level', 'significance', 'test', 'significance level', 'error', 'type 1 error', 'hypothesis']", Hypothesis testing for a proportion,seg_49,"we’ve been building a careful framework that controls for the type 1 error, which is the significance level α in a hypothesis test. we’ll use the α = 0.05 below to keep things simple."
1397,1,"['test', 'data']", Hypothesis testing for a proportion,seg_49,imagine we could pick the one-sided test after we saw the data. what will go wrong?
1398,1,"['null value', 'observation', 'distribution', 'mean', 'null distribution', 'tail', 'test']", Hypothesis testing for a proportion,seg_49,"• if p̂ is smaller than the null value, then a one-sided test where p < p0 would mean that any observation in the lower 5% tail of the null distribution would lead to us rejecting h0."
1399,1,"['null value', 'observation', 'distribution', 'mean', 'null distribution', 'tail', 'test']", Hypothesis testing for a proportion,seg_49,"• if p̂ is larger than the null value, then a one-sided test where p > p0 would mean that any observation in the upper 5% tail of the null distribution would lead to us rejecting h0."
1400,1,"['tests', 'tails', 'error']", Hypothesis testing for a proportion,seg_49,"then if h0 were true, there’s a 10% chance of being in one of the two tails, so our testing error is actually α = 0.10, not 0.05. that is, not being careful about when to use one-sided tests effectively undermines the methods we’re working so hard to develop and utilize."
1401,1,"['goodness of fit', 'independence', 'sample', 'uncertainty', 'data', 'hypothesis testing', 'model', 'contingency tables', 'categorical', 'distribution', 'hypothesis', 'categorical data', 'tables', 'normal', 'normal distribution']",Chapter  Inference for categorical data,seg_51,"6.1 inference for a single proportion 6.2 difference of two proportions 6.3 testing for goodness of fit using chi-square 6.4 testing for independence in two-way tables in this chapter, we apply the methods and ideas from chapter 5 in several contexts for categorical data. we’ll start by revisiting what we learned for a single proportion, where the normal distribution can be used to model the uncertainty in the sample proportion. next, we apply these same ideas to analyze the difference of two proportions using the normal model. later in the chapter, we apply inference techniques to contingency tables; while we will use a different distribution in this context, the core ideas of hypothesis testing remain the same. for videos, slides, and other resources, please visit www.openintro.org/os"
1402,1,"['sample size', 'sample', 'tests', 'confidence intervals', 'hypothesis tests', 'data', 'intervals', 'confidence', 'hypothesis']", Inference for a single proportion,seg_53,"we encountered inference methods for a single proportion in chapter 5, exploring point estimates, confidence intervals, and hypothesis tests. in this section, we’ll do a review of these topics and also how to choose an appropriate sample size when collecting data for single proportion contexts."
1403,1,"['sample', 'normal']", Inference for a single proportion,seg_53,6.1.1 identifying when the sample proportion is nearly normal
1404,1,"['sample size', 'sample', 'independent', 'observations', 'distribution', 'normal', 'normal distribution']", Inference for a single proportion,seg_53,a sample proportion p̂ can be modeled using a normal distribution when the sample observations are independent and the sample size is sufficiently large.
1405,1,"['sample', 'sampling', 'distribution', 'normal', 'population', 'sampling distribution']", Inference for a single proportion,seg_53,the sampling distribution for p̂ based on a sample of size n from a population with a true proportion p is nearly normal when:
1406,1,"['sample', 'independent', 'random', 'simple random sample', 'observations', 'random sample']", Inference for a single proportion,seg_53,"1. the sample’s observations are independent, e.g. are from a simple random sample."
1407,1,"['sample', 'condition', 'failures', 'successes']", Inference for a single proportion,seg_53,"2. we expected to see at least 10 successes and 10 failures in the sample, i.e. np ≥ 10 and n(1− p) ≥ 10. this is called the success-failure condition."
1408,1,"['standard error', 'sampling', 'distribution', 'normal', 'mean', 'standard', 'sampling distribution', 'error']", Inference for a single proportion,seg_53,"when these conditions are met, then the sampling distribution of p̂ is nearly normal with mean p(1−p) p and standard error se = √ n ."
1409,1,"['confidence intervals', 'null hypothesis', 'sample', 'estimate', 'hypothesis tests', 'intervals', 'standard', 'tests', 'confidence', 'error', 'condition', 'null value', 'hypothesis', 'standard error']", Inference for a single proportion,seg_53,"typically we don’t know the true proportion p, so we substitute some value to check conditions and estimate the standard error. for confidence intervals, the sample proportion p̂ is used to check the success-failure condition and compute the standard error. for hypothesis tests, typically the null value – that is, the proportion claimed in the null hypothesis – is used in place of p."
1410,1,"['confidence intervals', 'intervals', 'confidence']", Inference for a single proportion,seg_53,6.1.2 confidence intervals for a proportion
1411,1,"['confidence interval', 'interval', 'range', 'distribution', 'normal', 'parameter', 'confidence', 'normal distribution']", Inference for a single proportion,seg_53,"a confidence interval provides a range of plausible values for the parameter p, and when p̂ can be modeled using a normal distribution, the confidence interval for p takes the form"
1412,1,"['sample', 'model', 'simple random sample', 'random sample', 'distribution', 'normal', 'random', 'normal distribution']", Inference for a single proportion,seg_53,a simple random sample of 826 payday loan borrowers was surveyed to better understand their interests around regulation and costs. 70% of the responses supported new regulations on payday lenders. is it reasonable to model p̂ = 0.70 using a normal distribution?
1413,1,"['sample', 'random', 'independent', 'observations', 'data', 'population', 'random sample', 'representative']", Inference for a single proportion,seg_53,"the data are a random sample, so the observations are independent and representative of the population of interest."
1414,1,"['interval', 'condition', 'confidence', 'confidence interval']", Inference for a single proportion,seg_53,"we also must check the success-failure condition, which we do using p̂ in place of p when computing a confidence interval:"
1415,1,"['distribution', 'model', 'normal', 'normal distribution']", Inference for a single proportion,seg_53,"since both values are at least 10, we can use the normal distribution to model p̂."
1416,1,"['interval', 'standard error', 'standard', 'confidence', 'error', 'confidence interval']", Inference for a single proportion,seg_53,"estimate the standard error of p̂ = 0.70. because p is unknown and the standard error is for a confidence interval, use p̂ in place of p in the formula.1"
1417,1,"['confidence', 'interval', 'confidence interval']", Inference for a single proportion,seg_53,"construct a 95% confidence interval for p, the proportion of payday borrowers who support increased regulation for payday lenders."
1418,1,"['interval', 'estimate', 'standard error', 'point estimate', 'standard', 'confidence', 'error', 'confidence interval']", Inference for a single proportion,seg_53,"using the point estimate 0.70, z? = 1.96 for a 95% confidence interval, and the standard error se = 0.016 from guided practice 6.2, the confidence interval is"
1419,1,['estimate'], Inference for a single proportion,seg_53,"point estimate ± z? × se → 0.70 ± 1.96× 0.016 → (0.669, 0.731)"
1420,1,['confident'], Inference for a single proportion,seg_53,we are 95% confident that the true proportion of payday borrowers who supported regulation at the time of the poll was between 0.669 and 0.731.
1421,1,"['confidence', 'interval', 'confidence interval']", Inference for a single proportion,seg_53,"once you’ve determined a one-proportion confidence interval would be helpful for an application, there are four steps to constructing the interval:"
1422,1,"['level', 'confidence', 'confidence level']", Inference for a single proportion,seg_53,"prepare. identify p̂ and n, and determine what confidence level you wish to use."
1423,1,"['confidence intervals', 'condition', 'intervals', 'normal', 'confidence']", Inference for a single proportion,seg_53,"check. verify the conditions to ensure p̂ is nearly normal. for one-proportion confidence intervals, use p̂ in place of p to check the success-failure condition."
1424,1,['interval'], Inference for a single proportion,seg_53,"calculate. if the conditions hold, compute se using p̂, find z?, and construct the interval."
1425,1,"['confidence', 'interval', 'confidence interval']", Inference for a single proportion,seg_53,conclude. interpret the confidence interval in the context of the problem.
1426,1,"['confidence', 'interval', 'confidence interval']", Inference for a single proportion,seg_53,"for additional one-proportion confidence interval examples, see section 5.2."
1427,1,"['hypothesis testing', 'hypothesis']", Inference for a single proportion,seg_53,6.1.3 hypothesis testing for a proportion
1428,0,[], Inference for a single proportion,seg_53,one possible regulation for payday lenders is that they would be required to do a credit check and evaluate debt payments against the borrower’s finances. we would like to know: would borrowers support this form of regulation?
1429,1,['hypotheses'], Inference for a single proportion,seg_53,set up hypotheses to evaluate whether borrowers have a majority support or majority opposition
1430,0,[], Inference for a single proportion,seg_53,2 for this type of regulation.
1431,1,"['condition', 'hypothesis test', 'null value', 'distribution', 'independence', 'normal', 'test', 'normal distribution', 'hypothesis']", Inference for a single proportion,seg_53,"to apply the normal distribution framework in the context of a hypothesis test for a proportion, the independence and success-failure conditions must be satisfied. in a hypothesis test, the successfailure condition is checked using the null proportion: we verify np0 and n(1 − p0) are at least 10, where p0 is the null value."
1432,1,"['sample', 'model', 'hypothesis test', 'random sample', 'distribution', 'normal', 'random', 'test', 'normal distribution', 'hypothesis']", Inference for a single proportion,seg_53,"do payday loan borrowers support a regulation that would require lenders to pull their credit report and evaluate their debt payments? from a random sample of 826 borrowers, 51% said they would support such a regulation. is it reasonable to model p̂ = 0.51 using a normal distribution for a hypothesis test here?3"
1433,1,"['hypotheses', 'data']", Inference for a single proportion,seg_53,"using the hypotheses and data from guided practice 6.4 and 6.5, evaluate whether the poll provides convincing evidence that a majority of payday loan borrowers support a new regulation that would require lenders to pull credit reports and evaluate debt payments."
1434,1,"['standard error', 'hypothesis test', 'null value', 'set', 'hypotheses', 'standard', 'test', 'error', 'hypothesis']", Inference for a single proportion,seg_53,"with hypotheses already set up and conditions checked, we can move onto calculations. the standard error in the context of a one-proportion hypothesis test is computed using the null value, p0:"
1435,1,"['model', 'normal']", Inference for a single proportion,seg_53,a picture of the normal model is shown below with the p-value represented by the shaded region.
1436,1,"['model', 'estimate', 'test statistic', 'statistic', 'normal', 'point estimate', 'test']", Inference for a single proportion,seg_53,"based on the normal model, the test statistic can be computed as the z-score of the point estimate:"
1437,1,['null value'], Inference for a single proportion,seg_53,point estimate− null value 0.51− 0.50 z = = = 0.59 se 0.017
1438,1,"['tail areas', 'tail']", Inference for a single proportion,seg_53,"the single tail area is 0.2776, and the p-value, represented by both tail areas together, is 0.5552. because the p-value is larger than 0.05, we do not reject h0. the poll does not provide convincing evidence that a majority of payday loan borrowers support or oppose regulations around credit checks and evaluation of debt payments."
1439,1,"['hypothesis test', 'test', 'hypothesis']", Inference for a single proportion,seg_53,"once you’ve determined a one-proportion hypothesis test is the correct procedure, there are four steps to completing the test:"
1440,1,"['parameter of interest', 'hypotheses', 'parameter', 'level', 'significance', 'significance level']", Inference for a single proportion,seg_53,"prepare. identify the parameter of interest, list hypotheses, identify the significance level, and identify p̂ and n."
1441,1,"['condition', 'hypothesis tests', 'null value', 'normal', 'tests', 'hypothesis']", Inference for a single proportion,seg_53,"check. verify conditions to ensure p̂ is nearly normal under h0. for one-proportion hypothesis tests, use the null value to check the success-failure condition."
1442,1,"['standard error', 'standard', 'error']", Inference for a single proportion,seg_53,"calculate. if the conditions hold, compute the standard error, again using p0, compute the z-score, and identify the p-value."
1443,1,"['test', 'hypothesis test', 'hypothesis']", Inference for a single proportion,seg_53,"conclude. evaluate the hypothesis test by comparing the p-value to α, and provide a conclusion in the context of the problem."
1444,1,"['hypothesis test', 'test', 'hypothesis']", Inference for a single proportion,seg_53,"for additional one-proportion hypothesis test examples, see section 5.3."
1445,0,[], Inference for a single proportion,seg_53,6.1.4 when one or more conditions aren’t met
1446,1,"['confidence intervals', 'interval', 'case', 'independence', 'hypothesis tests', 'intervals', 'tests', 'confidence', 'condition', 'distribution', 'hypothesis', 'normal', 'normal distribution']", Inference for a single proportion,seg_53,"we’ve spent a lot of time discussing conditions for when p̂ can be reasonably modeled by a normal distribution. what happens when the success-failure condition fails? what about when the independence condition fails? in either case, the general ideas of confidence intervals and hypothesis tests remain the same, but the strategy or technique used to generate the interval or p-value change."
1447,1,"['simulation', 'condition', 'hypothesis test', 'null value', 'case', 'distribution', 'null distribution', 'test', 'hypothesis']", Inference for a single proportion,seg_53,"when the success-failure condition isn’t met for a hypothesis test, we can simulate the null distribution of p̂ using the null value, p0. the simulation concept is similar to the ideas used in the malaria case study presented in section 2.3, and an online section outlines this strategy:"
1448,1,"['interval', 'condition', 'confidence', 'confidence interval']", Inference for a single proportion,seg_53,"for a confidence interval when the success-failure condition isn’t met, we can use what’s called the clopper-pearson interval. the details are beyond the scope of this book. however, there are many internet resources covering this topic."
1449,1,"['convenience sample', 'sample', 'method', 'condition', 'statistics', 'data', 'independence', 'biases', 'cluster sample', 'statistical']", Inference for a single proportion,seg_53,"the independence condition is a more nuanced requirement. when it isn’t met, it is important to understand how and why it isn’t met. for example, if we took a cluster sample (see section 1.3), suitable statistical methods are available but would be beyond the scope of even most second or third courses in statistics. on the other hand, we’d be stretched to find any method that we could confidently apply to correct the inherent biases of data from a convenience sample."
1450,1,"['range', 'data', 'statistical']", Inference for a single proportion,seg_53,"while this book is scoped to well-constrained statistical problems, do remember that this is just the first book in what is a large library of statistical methods that are suitable for a very wide range of data and contexts."
1451,1,"['sample size', 'sample']", Inference for a single proportion,seg_53,6.1.5 choosing a sample size when estimating a proportion
1452,1,"['sample size', 'sample', 'interval', 'estimate', 'data', 'margin of error', 'point estimate', 'confidence', 'error', 'confidence interval']", Inference for a single proportion,seg_53,"when collecting data, we choose a sample size suitable for the purpose of the study. often times this means choosing a sample size large enough that the margin of error – which is the part we add and subtract from the point estimate in a confidence interval – is sufficiently small that the sample is useful. for example, our task might be to find a sample size n so that the sample proportion is within ±0.04 of the actual proportion in a 95% confidence interval."
1453,1,"['sample', 'margin of error', 'level', 'confidence', 'confidence level', 'error']", Inference for a single proportion,seg_53,a university newspaper is conducting a survey to determine what fraction of students support a $200 per year increase in fees to pay for a new football stadium. how big of a sample is required to ensure the margin of error is smaller than 0.04 using a 95% confidence level?
1454,1,"['sample', 'margin of error', 'error']", Inference for a single proportion,seg_53,the margin of error for a sample proportion is
1455,1,"['sample size', 'sample', 'margin of error', 'level', 'confidence', 'confidence level', 'error']", Inference for a single proportion,seg_53,"our goal is to find the smallest sample size n so that this margin of error is smaller than 0.04. for a 95% confidence level, the value z? corresponds to 1.96:"
1456,1,"['margin of error', 'error', 'case', 'estimate']", Inference for a single proportion,seg_53,"there are two unknowns in the equation: p and n. if we have an estimate of p, perhaps from a prior survey, we could enter in that value and solve for n. if we have no such estimate, we must use some other value for p. it turns out that the margin of error is largest when p is 0.5, so we typically use this worst case value if no estimate of the proportion is available:"
1457,1,"['sample', 'confidence']", Inference for a single proportion,seg_53,"we would need over 600.25 participants, which means we need 601 participants or more, to ensure the sample proportion is within 0.04 of the true proportion with 95% confidence."
1458,1,"['case', 'estimate']", Inference for a single proportion,seg_53,"when an estimate of the proportion is available, we use it in place of the worst case proportion value, 0.5."
1459,1,"['sample size', 'sample', 'model', 'rate', 'estimate', 'failure rate', 'failure', 'control', 'level', 'confidence', 'confidence level', 'quality control', 'rates']", Inference for a single proportion,seg_53,"a manager is about to oversee the mass production of a new tire model in her factory, and she would like to estimate what proportion of these tires will be rejected through quality control. the quality control team has monitored the last three tire models produced by the factory, failing 1.7% of tires in the first model, 6.2% of the second model, and 1.3% of the third model. the manager would like to examine enough tires to estimate the failure rate of the new tire model to within about 1% with a 90% confidence level. there are three different failure rates to choose from. perform the sample size computation for each separately, and identify three sample sizes to consider.4"
1460,1,"['sample', 'vary']", Inference for a single proportion,seg_53,the sample sizes vary widely in guided practice 6.8. which of the three would you suggest using? what would influence your choice?
1461,1,"['sample size', 'sample', 'model', 'estimates', 'samples']", Inference for a single proportion,seg_53,"we could examine which of the old models is most like the new model, then choose the corresponding sample size. or if two of the previous estimates are based on small samples while the other is based on a larger sample, we might consider the value corresponding to the larger sample. there are also other reasonable approaches."
1462,1,"['sample', 'rate', 'normal approximation', 'condition', 'interval', 'approximation', 'failure rate', 'failure', 'normal', 'confidence', 'statistical', 'confidence interval']", Inference for a single proportion,seg_53,"also observe that the success-failure condition would need to be checked in the final sample. for instance, if we sampled n = 1584 tires and found a failure rate of 0.5%, the normal approximation would not be reasonable, and we would require more advanced statistical methods for creating the confidence interval."
1463,1,"['sample', 'margin of error', 'error']", Inference for a single proportion,seg_53,"suppose we want to continually track the support of payday borrowers for regulation on lenders, where we would conduct a new poll every month. running such frequent polls is expensive, so we decide a wider margin of error of 5% for each individual survey would be acceptable. based on the original sample of borrowers where 70% supported some form of regulation, how big should our monthly sample be for a margin of error of 0.05 with 95% confidence?5"
1464,1,"['confidence intervals', 'sample', 'estimate', 'intervals', 'population', 'standard', 'tests', 'confidence', 'error', 'distribution', 'processes', 'standard error', 'normal', 'point estimate', 'normal distribution']", Difference of two proportions,seg_55,"we would like to extend the methods from section 6.1 to apply confidence intervals and hypothesis tests to differences in population proportions: p1 − p2. in our investigations, we’ll identify a reasonable point estimate of p1 − p2 based on the sample, and you may have already guessed its form: p̂1 − p̂2. next, we’ll apply the same processes we used in the single-proportion context: we verify that the point estimate can be modeled using a normal distribution, we compute the estimate’s standard error, and we apply our inferential framework."
1465,1,"['sampling', 'sampling distribution', 'distribution']", Difference of two proportions,seg_55,6.2.1 sampling distribution of the difference of two proportions
1466,1,"['sample', 'condition', 'distribution', 'independence', 'normal', 'normal distribution']", Difference of two proportions,seg_55,"like with p̂, the difference of two sample proportions p̂1 − p̂2 can be modeled using a normal distribution when certain conditions are met. first, we require a broader independence condition, and secondly, the success-failure condition must be met by both groups."
1467,1,"['distribution', 'normal', 'normal distribution']", Difference of two proportions,seg_55,the difference p̂1 − p̂2 can be modeled using a normal distribution when
1468,1,"['independent', 'random samples', 'experiment', 'data', 'independence', 'samples', 'randomized experiment', 'random']", Difference of two proportions,seg_55,"• independence, extended. the data are independent within and between the two groups. generally this is satisfied if the data come from two independent random samples or if the data come from a randomized experiment."
1469,1,"['condition', 'failures', 'successes']", Difference of two proportions,seg_55,"• success-failure condition. the success-failure condition holds for both groups, where we check successes and failures in each group separately."
1470,1,"['standard error', 'error', 'standard']", Difference of two proportions,seg_55,"when these conditions are satisfied, the standard error of p̂1 − p̂2 is"
1471,1,"['sample', 'population']", Difference of two proportions,seg_55,"where p1 and p2 represent the population proportions, and n1 and n2 represent the sample sizes."
1472,1,"['confidence intervals', 'intervals', 'confidence']", Difference of two proportions,seg_55,6.2.2 confidence intervals for p1 − p2
1473,1,"['interval', 'estimate', 'point estimate', 'confidence', 'confidence interval']", Difference of two proportions,seg_55,"we can apply the generic confidence interval formula for a difference of two proportions, where we use p̂1 − p̂2 as the point estimate and substitute the se formula:"
1474,1,"['point estimate', 'estimate']", Difference of two proportions,seg_55,p (1− p ) p (1− p ) point estimate ± z? × se → p̂1 − p̂2 ± z? ×√ 1 n1
1475,1,"['interval', 'hypothesis test', 'statistical', 'confidence', 'test', 'confidence interval', 'hypothesis']", Difference of two proportions,seg_55,"we can also follow the same prepare, check, calculate, conclude steps for computing a confidence interval or completing a hypothesis test. the details change a little, but the general approach remain the same. think about these steps when you apply statistical methods."
1476,1,"['sample', 'model', 'treatment group', 'experiment', 'treatment', 'results', 'distribution', 'variable', 'control group', 'normal', 'outcome', 'control', 'normal distribution']", Difference of two proportions,seg_55,we consider an experiment for patients who underwent cardiopulmonary resuscitation (cpr) for a heart attack and were subsequently admitted to a hospital. these patients were randomly divided into a treatment group where they received a blood thinner or the control group where they did not receive a blood thinner. the outcome variable of interest was whether the patients survived for at least 24 hours. the results are shown in figure 6.1. check whether we can model the difference in sample proportions using the normal distribution.
1477,1,"['condition', 'experiment', 'independence', 'randomized experiment']", Difference of two proportions,seg_55,"we first check for independence: since this is a randomized experiment, this condition is satisfied."
1478,1,"['condition', 'failures', 'experiment', 'successes']", Difference of two proportions,seg_55,"next, we check the success-failure condition for each group. we have at least 10 successes and 10 failures in each experiment arm (11, 14, 39, 26), so this condition is also satisfied."
1479,1,"['sample', 'data', 'distribution', 'normal', 'normal distribution']", Difference of two proportions,seg_55,"with both conditions satisfied, the difference in sample proportions can be reasonably modeled using a normal distribution for these data."
1480,1,"['treatment', 'control']", Difference of two proportions,seg_55,survived died total control 11 39 50 treatment 14 26 40 total 25 65 90
1481,1,"['treatment group', 'treatment', 'results', 'control group', 'control']", Difference of two proportions,seg_55,"figure 6.1: results for the cpr study. patients in the treatment group were given a blood thinner, and patients in the control group were not."
1482,1,"['interval', 'confidence', 'confidence interval', 'rates']", Difference of two proportions,seg_55,create and interpret a 90% confidence interval of the difference for the survival rates in the cpr study.
1483,1,"['rate', 'treatment group', 'treatment', 'control group', 'control']", Difference of two proportions,seg_55,we’ll use pt for the survival rate in the treatment group and pc for the control group:
1484,1,"['sample', 'interval', 'estimates', 'standard error', 'case', 'standard', 'confidence', 'error', 'confidence interval']", Difference of two proportions,seg_55,"we use the standard error formula provided on page 217. as with the one-sample proportion case, we use the sample estimates of each proportion in the formula in the confidence interval context:"
1485,1,"['confidence', 'interval', 'confidence interval']", Difference of two proportions,seg_55,"for a 90% confidence interval, we use z? = 1.65:"
1486,1,['estimate'], Difference of two proportions,seg_55,"point estimate ± z? × se → 0.13 ± 1.65× 0.095 → (−0.027, 0.287)"
1487,1,"['rate', 'percentage', 'interval', 'confident', 'information']", Difference of two proportions,seg_55,"we are 90% confident that blood thinners have a difference of -2.7% to +28.7% percentage point impact on survival rate for patients who are like those in the study. because 0% is contained in the interval, we do not have enough information to say whether blood thinners help or harm heart attack patients who have been admitted after they have undergone cpr."
1488,1,"['experiment', 'treatment', 'treatment groups', 'events', 'outcomes']", Difference of two proportions,seg_55,"a 5-year experiment was conducted to evaluate the effectiveness of fish oils on reducing cardiovascular events, where each subject was randomized into one of two treatment groups. we’ll consider heart attack outcomes in these patients:"
1489,1,['event'], Difference of two proportions,seg_55,heart attack no event total fish oil 145 12788 12933 placebo 200 12738 12938
1490,1,"['confidence', 'interval', 'confidence interval']", Difference of two proportions,seg_55,create a 95% confidence interval for the effect of fish oils on heart attacks for patients who are well-represented by those in the study. also interpret the interval in the context of the study.16
1491,1,"['hypothesis tests', 'tests', 'hypothesis']", Difference of two proportions,seg_55,6.2.3 hypothesis tests for the difference of two proportions
1492,1,"['hypothesis tests', 'tests', 'hypothesis']", Difference of two proportions,seg_55,"a mammogram is an x-ray procedure used to check for breast cancer. whether mammograms should be used is part of a controversial discussion, and it’s the topic of our next example where we learn about 2-proportion hypothesis tests when h0 is p1 − p2 = 0 (or equivalently, p1 = p2)."
1493,1,['results'], Difference of two proportions,seg_55,"a 30-year study was conducted with nearly 90,000 female participants. during a 5-year screening period, each woman was randomized to one of two groups: in the first group, women received regular mammograms to screen for breast cancer, and in the second group, women received regular non-mammogram breast cancer exams. no intervention was made during the following 25 years of the study, and we’ll consider death resulting from breast cancer over the full 30-year period. results from the study are summarized in figure 6.2."
1494,1,"['control group', 'control']", Difference of two proportions,seg_55,"if mammograms are much more effective than non-mammogram breast cancer exams, then we would expect to see additional deaths from breast cancer in the control group. on the other hand, if mammograms are not as effective as regular breast cancer exams, we would expect to see an increase in breast cancer deaths in the mammogram group."
1495,1,['control'], Difference of two proportions,seg_55,"death from breast cancer? yes no mammogram 500 44,425 control 505 44,405"
1496,1,['results'], Difference of two proportions,seg_55,figure 6.2: summary results for breast cancer study.
1497,1,['experiment'], Difference of two proportions,seg_55,is this study an experiment or an observational study?17
1498,1,"['hypotheses', 'control', 'test']", Difference of two proportions,seg_55,set up hypotheses to test whether there was a difference in breast cancer deaths in the mammogram and control groups.18
1499,1,"['confidence intervals', 'condition', 'results', 'intervals', 'distribution', 'normal', 'pooled proportion', 'confidence', 'null hypothesis', 'normal distribution', 'hypothesis']", Difference of two proportions,seg_55,"in example 6.16, we will check the conditions for using a normal distribution to analyze the results of the study. the details are very similar to that of confidence intervals. however, when the null hypothesis is that p1 − p2 = 0, we use a special proportion called the pooled proportion to check the success-failure condition:"
1500,0,[], Difference of two proportions,seg_55,# of patients who died from breast cancer in the entire study p̂pooled = # of patients in the entire study
1501,1,"['rate', 'error', 'standard', 'estimate', 'standard error', 'pooled proportion', 'null hypothesis', 'hypothesis']", Difference of two proportions,seg_55,"this proportion is an estimate of the breast cancer death rate across the entire study, and it’s our best estimate of the proportions pmgm and pctrl if the null hypothesis is true that pmgm = pctrl. we will also use this pooled proportion when computing the standard error."
1502,1,"['distribution', 'model', 'normal', 'normal distribution']", Difference of two proportions,seg_55,is it reasonable to model the difference in proportions using a normal distribution in this study?
1503,1,"['independent', 'condition', 'estimate', 'samples', 'pooled proportion', 'null hypothesis', 'hypothesis']", Difference of two proportions,seg_55,"because the patients are randomized, they can be treated as independent, both within and between groups. we also must check the success-failure condition for each group. under the null hypothesis, the proportions pmgm and pctrl are equal, so we check the success-failure condition with our best estimate of these values under h0, the pooled proportion from the two samples, p̂pooled = 0.0112:"
1504,1,"['model', 'condition', 'distribution', 'normal', 'normal distribution']", Difference of two proportions,seg_55,"the success-failure condition is satisfied since all values are at least 10. with both conditions satisfied, we can safely model the difference in proportions using a normal distribution."
1505,1,"['error', 'standard', 'condition', 'estimate', 'standard error', 'pooled proportion', 'null hypothesis', 'hypothesis']", Difference of two proportions,seg_55,"when the null hypothesis is that the proportions are equal, use the pooled proportion (p̂pooled) to verify the success-failure condition and estimate the standard error:"
1506,1,['cases'], Difference of two proportions,seg_55,number of “successes” p̂1n1 + p̂2n2 p̂pooled = = number of cases n1 + n2
1507,1,"['sample', 'successes']", Difference of two proportions,seg_55,here p̂1n1 represents the number of successes in sample 1 since
1508,1,"['sample', 'successes']", Difference of two proportions,seg_55,number of successes in sample 1 p̂1 = n1
1509,1,"['sample', 'successes']", Difference of two proportions,seg_55,"similarly, p̂2n2 represents the number of successes in sample 2."
1510,1,"['standard', 'standard error', 'pooled proportion', 'error']", Difference of two proportions,seg_55,"in example 6.16, the pooled proportion was used to check the success-failure condition.19 in the next example, we see the second place where the pooled proportion comes into play: the standard error calculation."
1511,1,"['standard', 'estimate', 'standard error', 'point estimate', 'pooled proportion', 'error', 'rates']", Difference of two proportions,seg_55,"compute the point estimate of the difference in breast cancer death rates in the two groups, and use the pooled proportion p̂pooled = 0.0112 to calculate the standard error."
1512,1,"['rates', 'point estimate', 'estimate']", Difference of two proportions,seg_55,the point estimate of the difference in breast cancer death rates is
1513,1,"['rate', 'standard', 'standard error', 'control group', 'pooled proportion', 'control', 'error']", Difference of two proportions,seg_55,"the breast cancer death rate in the mammogram group was 0.012% less than in the control group. next, the standard error is calculated using the pooled proportion, p̂pooled:"
1514,1,"['estimate', 'standard error', 'hypothesis test', 'hypothesis', 'standard', 'point estimate', 'test', 'error']", Difference of two proportions,seg_55,"using the point estimate p̂mgm − p̂ctrl = −0.00012 and standard error se = 0.00070, calculate a p-value for the hypothesis test and write a conclusion."
1515,1,"['tests', 'test statistic', 'statistic', 'test']", Difference of two proportions,seg_55,"just like in past tests, we first compute a test statistic and draw a picture:"
1516,1,['null value'], Difference of two proportions,seg_55,point estimate− null value −0.00012− 0 z = = = −0.17 se 0.00070
1517,1,"['rates', 'hypothesis', 'tail', 'null hypothesis']", Difference of two proportions,seg_55,"the lower tail area is 0.4325, which we double to get the p-value: 0.8650. because this p-value is larger than 0.05, we do not reject the null hypothesis. that is, the difference in breast cancer death rates is reasonably explained by chance, and we do not observe benefits or harm from mammograms relative to a regular breast exam."
1518,0,[], Difference of two proportions,seg_55,can we conclude that mammograms have no benefits or harm? here are a few considerations to keep in mind when reviewing the mammogram study as well as any other medical study:
1519,1,"['null hypothesis', 'hypothesis']", Difference of two proportions,seg_55,"• we do not accept the null hypothesis, which means we don’t have sufficient evidence to conclude that mammograms reduce or increase breast cancer deaths."
1520,1,['data'], Difference of two proportions,seg_55,"• if mammograms are helpful or harmful, the data suggest the effect isn’t very large."
1521,0,[], Difference of two proportions,seg_55,"• are mammograms more or less expensive than a non-mammogram breast exam? if one option is much more expensive than the other and doesn’t offer clear benefits, then we should lean towards the less expensive option."
1522,1,['treatment'], Difference of two proportions,seg_55,"• the study’s authors also found that mammograms led to overdiagnosis of breast cancer, which means some breast cancers were found (or thought to be found) but that these cancers would not cause symptoms during patients’ lifetimes. that is, something else would kill the patient before breast cancer symptoms appeared. this means some patients may have been treated for breast cancer unnecessarily, and this treatment is another cost to consider. it is also important to recognize that overdiagnosis can cause unnecessary physical or emotional harm to patients."
1523,1,['treatment'], Difference of two proportions,seg_55,these considerations highlight the complexity around medical care and treatment recommendations. experts and medical boards who study medical treatments use considerations like those above to provide their best recommendation based on the current evidence.
1524,1,"['hypothesis tests', 'tests', 'hypothesis']", Difference of two proportions,seg_55,6.2.4 more on 2-proportion hypothesis tests (special topic)
1525,1,"['error', 'condition', 'standard error', 'hypothesis test', 'standard', 'test', 'null hypothesis', 'hypothesis']", Difference of two proportions,seg_55,"when we conduct a 2-proportion hypothesis test, usually h0 is p1 − p2 = 0. however, there are rare situations where we want to check for some difference in p1 and p2 that is some value other than 0. for example, maybe we care about checking a null hypothesis where p1 − p2 = 0.1. in contexts like these, we generally use p̂1 and p̂2 to check the success-failure condition and construct the standard error."
1526,0,[], Difference of two proportions,seg_55,"a quadcopter company is considering a new manufacturer for rotor blades. the new manufacturer would be more expensive, but they claim their higher-quality blades are more reliable, with 3% more"
1527,1,"['set', 'test', 'hypotheses']", Difference of two proportions,seg_55,20 blades passing inspection than their competitor. set up appropriate hypotheses for the test.
1528,1,"['sample', 'quality control', 'data', 'hypotheses', 'level', 'significance', 'control', 'significance level']", Difference of two proportions,seg_55,"the quality control engineer from guided practice 6.19 collects a sample of blades, examining 1000 blades from each company, and she finds that 899 blades pass inspection from the current supplier and 958 pass inspection from the prospective supplier. using these data, evaluate the hypotheses from guided practice 6.19 with a significance level of 5%."
1529,1,"['sample', 'independent', 'condition', 'distribution', 'normal', 'random', 'normal distribution']", Difference of two proportions,seg_55,"first, we check the conditions. the sample is not necessarily random, so to proceed we must assume the blades are all independent; for this sample we will suppose this assumption is reasonable, but the engineer would be more knowledgeable as to whether this assumption is appropriate. the success-failure condition also holds for each sample. thus, the difference in sample proportions, 0.958− 0.899 = 0.059, can be said to come from a nearly normal distribution."
1530,1,"['sample', 'standard', 'standard error', 'pooled proportion', 'error']", Difference of two proportions,seg_55,the standard error is computed using the two sample proportions since we do not use a pooled proportion for this context:
1531,1,"['sample', 'standard', 'standard error', 'hypothesis test', 'pooled proportion', 'test', 'error', 'hypothesis']", Difference of two proportions,seg_55,"in this hypothesis test, because the null is that p1 − p2 = 0.03, the sample proportions were used for the standard error calculation rather than a pooled proportion."
1532,1,"['test statistic', 'statistic', 'test']", Difference of two proportions,seg_55,"next, we compute the test statistic and use it to find the p-value, which is depicted in figure 6.4."
1533,1,['null value'], Difference of two proportions,seg_55,point estimate− null value 0.059− 0.03 z = = = 2.54 se 0.0114
1534,1,"['standard normal', 'test statistic', 'distribution', 'statistically significant', 'statistic', 'normal', 'standard', 'standard normal distribution', 'tail', 'test', 'null hypothesis', 'normal distribution', 'hypothesis']", Difference of two proportions,seg_55,"using a standard normal distribution for this test statistic, we identify the right tail area as 0.006, and we double it to get the p-value: 0.012. we reject the null hypothesis because 0.012 is less than 0.05. since we observed a larger-than-3% increase in blades that pass inspection, we have statistically significant evidence that the higher-quality blades pass inspection more than 3% as often as the currently used blades, exceeding the company’s claims."
1535,1,"['test statistic', 'distribution', 'statistic', 'hypothesis', 'test', 'null hypothesis']", Difference of two proportions,seg_55,figure 6.4: distribution of the test statistic if the null hypothesis was true. the p-value is represented by the shaded areas.
1536,1,"['standard error', 'error', 'standard']", Difference of two proportions,seg_55,6.2.5 examining the standard error formula (special topic)
1537,1,"['standard error', 'error', 'probability', 'standard']", Difference of two proportions,seg_55,"this subsection covers more theoretical topics that offer deeper insights into the origins of the standard error formula for the difference of two proportions. ultimately, all of the standard error formulas we encounter in this chapter and in chapter 7 can be derived from the probability principles of section 3.4."
1538,1,"['sample', 'standard error', 'errors', 'standard', 'standard errors', 'error']", Difference of two proportions,seg_55,the formula for the standard error of the difference in two proportions can be deconstructed into the formulas for the standard errors of the individual sample proportions. recall that the standard error of the individual sample proportions p̂1 and p̂2 are
1539,1,"['sample', 'standard error', 'errors', 'standard', 'standard errors', 'error']", Difference of two proportions,seg_55,the standard error of the difference of two sample proportions can be deconstructed from the standard errors of the separate sample proportions:
1540,1,"['probability theory', 'probability']", Difference of two proportions,seg_55,this special relationship follows from probability theory.
1541,0,[], Difference of two proportions,seg_55,prerequisite: section 3.4. we can rewrite the equation above in a different way:
1542,1,"['variability', 'random']", Difference of two proportions,seg_55,explain where this formula comes from using the formula for the variability of the sum of two random
1543,1,['variables'], Difference of two proportions,seg_55,21 variables.
1544,1,"['model', 'method', 'data']", Testing for goodness of fit using chisquare,seg_57,"in this section, we develop a method for assessing a null model when the data are binned. this technique is commonly used in two circumstances:"
1545,1,"['sample', 'cases', 'population', 'representative']", Testing for goodness of fit using chisquare,seg_57,"• given a sample of cases that can be classified into several groups, determine if the sample is representative of the general population."
1546,1,"['geometric distribution', 'data', 'distribution', 'normal', 'geometric', 'normal distribution']", Testing for goodness of fit using chisquare,seg_57,"• evaluate whether data resemble a particular distribution, such as a normal distribution or a geometric distribution."
1547,1,"['statistical test', 'test', 'statistical']", Testing for goodness of fit using chisquare,seg_57,each of these scenarios can be addressed using the same statistical test: a chi-square test.
1548,1,"['sample', 'random', 'case', 'data', 'county', 'population', 'random sample', 'representative']", Testing for goodness of fit using chisquare,seg_57,"in the first case, we consider data from a random sample of 275 jurors in a small county. jurors identified their racial group, as shown in figure 6.5, and we would like to determine if these jurors are racially representative of the population. if the jury is representative of the population, then the proportions in the sample should roughly reflect the population of eligible jurors, i.e. registered voters."
1549,0,[], Testing for goodness of fit using chisquare,seg_57,race white black hispanic other total representation in juries 205 26 25 19 275 registered voters 0.72 0.07 0.12 0.09 1.00
1550,1,['population'], Testing for goodness of fit using chisquare,seg_57,figure 6.5: representation by race in a city’s juries and population.
1551,1,"['sample', 'data', 'population', 'representative']", Testing for goodness of fit using chisquare,seg_57,"while the proportions in the juries do not precisely represent the population proportions, it is unclear whether these data provide convincing evidence that the sample is not representative. if the jurors really were randomly sampled from the registered voters, we might expect small differences due to chance. however, unusually large differences may provide convincing evidence that the juries were not representative."
1552,1,"['distribution', 'independent']", Testing for goodness of fit using chisquare,seg_57,"a second application, assessing the fit of a distribution, is presented at the end of this section. daily stock returns from the s&p500 for 25 years are used to assess whether stock activity each day is independent of the stock’s behavior on previous days."
1553,1,"['bins', 'test statistic', 'statistic', 'test']", Testing for goodness of fit using chisquare,seg_57,"in these problems, we would like to examine all bins simultaneously, not simply compare one or two bins at a time, which will require us to develop a new test statistic."
1554,1,"['test statistic', 'tables', 'statistic', 'test']", Testing for goodness of fit using chisquare,seg_57,6.3.1 creating a test statistic for one-way tables
1555,0,[], Testing for goodness of fit using chisquare,seg_57,"of the people in the city, 275 served on a jury. if the individuals are randomly selected to serve on a jury, about how many of the 275 people would we expect to be white? how many would we expect to be black?"
1556,1,['population'], Testing for goodness of fit using chisquare,seg_57,"about 72% of the population is white, so we would expect about 72% of the jurors to be white: 0.72× 275 = 198."
1557,0,[], Testing for goodness of fit using chisquare,seg_57,"similarly, we would expect about 7% of the jurors to be black, which would correspond to about 0.07× 275 = 19.25 black jurors."
1558,1,"['population', 'percent']", Testing for goodness of fit using chisquare,seg_57,twelve percent of the population is hispanic and 9% represent other races. how many of the 275 jurors would we expect to be hispanic or from another race? answers can be found in figure 6.6.
1559,1,"['sample', 'sampling', 'variation']", Testing for goodness of fit using chisquare,seg_57,"the sample proportion represented from each race among the 275 jurors was not a precise match for any ethnic group. while some sampling variation is expected, we would expect the"
1560,1,['data'], Testing for goodness of fit using chisquare,seg_57,race white black hispanic other total observed data 205 26 25 19 275 expected counts 198 19.25 33 24.75 275
1561,0,[], Testing for goodness of fit using chisquare,seg_57,figure 6.6: actual and expected make-up of the jurors.
1562,1,"['sample', 'bias', 'random', 'hypotheses', 'population', 'random sample', 'test']", Testing for goodness of fit using chisquare,seg_57,sample proportions to be fairly similar to the population proportions if there is no bias on juries. we need to test whether the differences are strong enough to provide convincing evidence that the jurors are not a random sample. these ideas can be organized into hypotheses:
1563,1,"['sample', 'bias', 'random', 'random sample']", Testing for goodness of fit using chisquare,seg_57,"h0: the jurors are a random sample, i.e. there is no racial bias in who serves on a jury, and the"
1564,1,['sampling'], Testing for goodness of fit using chisquare,seg_57,observed counts reflect natural sampling fluctuation.
1565,1,['bias'], Testing for goodness of fit using chisquare,seg_57,"ha: the jurors are not randomly sampled, i.e. there is racial bias in juror selection."
1566,1,"['alternative hypothesis', 'sampling', 'deviations', 'hypotheses', 'variation', 'hypothesis']", Testing for goodness of fit using chisquare,seg_57,"to evaluate these hypotheses, we quantify how different the observed counts are from the expected counts. strong evidence for the alternative hypothesis would come in the form of unusually large deviations in the groups from what would be expected based on sampling variation alone."
1567,1,"['test statistic', 'statistic', 'test']", Testing for goodness of fit using chisquare,seg_57,6.3.2 the chi-square test statistic
1568,1,"['hypothesis tests', 'test statistic', 'statistic', 'tests', 'test', 'hypothesis']", Testing for goodness of fit using chisquare,seg_57,"in previous hypothesis tests, we constructed a test statistic of the following form:"
1569,1,['null value'], Testing for goodness of fit using chisquare,seg_57,point estimate− null value
1570,1,"['point estimate', 'estimate']", Testing for goodness of fit using chisquare,seg_57,se of point estimate
1571,1,"['error', 'estimate', 'standard error', 'test statistic', 'data', 'expected value', 'statistic', 'point estimate', 'standard', 'test', 'null hypothesis', 'hypothesis']", Testing for goodness of fit using chisquare,seg_57,"this construction was based on (1) identifying the difference between a point estimate and an expected value if the null hypothesis was true, and (2) standardizing that difference using the standard error of the point estimate. these two ideas will help in the construction of an appropriate test statistic for count data."
1572,1,"['null hypothesis', 'hypothesis']", Testing for goodness of fit using chisquare,seg_57,"our strategy will be to first compute the difference between the observed counts and the counts we would expect if the null hypothesis was true, then we will standardize the difference:"
1573,0,[], Testing for goodness of fit using chisquare,seg_57,observed white count− null white count z1 = se of observed white count
1574,1,"['estimate', 'standard error', 'data', 'point estimate', 'standard', 'error']", Testing for goodness of fit using chisquare,seg_57,the standard error for the point estimate of the count in binned data is the square root of the count under the null.32 therefore:
1575,1,"['statistics', 'test statistics', 'test']", Testing for goodness of fit using chisquare,seg_57,"the fraction is very similar to previous test statistics: first compute a difference, then standardize it. these computations should also be completed for the black, hispanic, and other groups:"
1576,1,"['absolute value', 'test statistic', 'standardized', 'statistic', 'test']", Testing for goodness of fit using chisquare,seg_57,"we would like to use a single test statistic to determine if these four standardized differences are irregularly far from zero. that is, z1, z2, z3, and z4 must be combined somehow to help determine if they – as a group – tend to be unusually far from zero. a first thought might be to take the absolute value of these four standardized differences and add them up:"
1577,0,[], Testing for goodness of fit using chisquare,seg_57,"indeed, this does give one number summarizing how far the actual counts are from what was expected. however, it is more common to add the squared values:"
1578,1,['standardized'], Testing for goodness of fit using chisquare,seg_57,squaring each standardized difference before adding them together does two things:
1579,1,['standardized'], Testing for goodness of fit using chisquare,seg_57,• any standardized difference that is squared will now be positive.
1580,1,['standardized'], Testing for goodness of fit using chisquare,seg_57,• differences that already look unusual – e.g. a standardized difference of 2.5 – will become much larger after being squared.
1581,1,"['test statistic', 'statistic', 'test']", Testing for goodness of fit using chisquare,seg_57,"the test statistic x2, which is the sum of the z2 values, is generally used for these reasons. we can also write an equation for x2 using the observed counts and null counts:"
1582,1,"['distribution', 'hypotheses', 'hypothesis', 'null hypothesis']", Testing for goodness of fit using chisquare,seg_57,"the final number x2 summarizes how strongly the observed counts tend to deviate from the null counts. in section 6.3.4, we will see that if the null hypothesis is true, then x2 follows a new distribution called a chi-square distribution. using this distribution, we will be able to obtain a p-value to evaluate the hypotheses."
1583,1,['distribution'], Testing for goodness of fit using chisquare,seg_57,6.3.3 the chi-square distribution and finding areas
1584,1,"['degrees of freedom', 'statistics', 'data', 'sets', 'mean', 'parameter', 'standard', 'standard deviation', 'data sets', 'parameters', 'skewed', 'distribution', 'deviation', 'normal', 'normal distribution']", Testing for goodness of fit using chisquare,seg_57,"the chi-square distribution is sometimes used to characterize data sets and statistics that are always positive and typically right skewed. recall a normal distribution had two parameters – mean and standard deviation – that could be used to describe its exact characteristics. the chisquare distribution has just one parameter called degrees of freedom (df), which influences the shape, center, and spread of the distribution."
1585,1,"['degrees of freedom', 'variability', 'distribution', 'distributions']", Testing for goodness of fit using chisquare,seg_57,figure 6.7 shows three chi-square distributions. (a) how does the center of the distribution change when the degrees of freedom is larger? (b) what about the variability (spread)? (c) how does the shape change?33
1586,1,"['degrees of freedom', 'varying', 'distributions']", Testing for goodness of fit using chisquare,seg_57,figure 6.7: three chi-square distributions with varying degrees of freedom.
1587,1,"['degrees of freedom', 'variability', 'symmetric', 'distribution']", Testing for goodness of fit using chisquare,seg_57,"figure 6.7 and guided practice 6.24 demonstrate three general properties of chi-square distributions as the degrees of freedom increases: the distribution becomes more symmetric, the center moves to the right, and the variability inflates."
1588,1,"['table', 'distribution', 'tail']", Testing for goodness of fit using chisquare,seg_57,"our principal interest in the chi-square distribution is the calculation of p-values, which (as we have seen before) is related to finding the relevant area in the tail of a distribution. the most common ways to do this are using computer software, using a graphing calculator, or using a table. for folks wanting to use the table option, we provide an outline of how to read the chi-square table in appendix c.3, which is also where you may find the table. for the examples below, use your preferred approach to confirm you get the same answers."
1589,1,"['degrees of freedom', 'distribution', 'tail']", Testing for goodness of fit using chisquare,seg_57,figure 6.8(a) shows a chi-square distribution with 3 degrees of freedom and an upper shaded tail starting at 6.25. find the shaded area.
1590,1,"['degrees of freedom', 'distribution', 'tail', 'statistical']", Testing for goodness of fit using chisquare,seg_57,"using statistical software or a graphing calculator, we can find that the upper tail area for a chisquare distribution with 3 degrees of freedom (df) and a cutoff of 6.25 is 0.1001. that is, the shaded upper tail of figure 6.8(a) has area 0.1."
1591,1,"['degrees of freedom', 'distribution', 'tail']", Testing for goodness of fit using chisquare,seg_57,figure 6.8(b) shows the upper tail of a chi-square distribution with 2 degrees of freedom. the bound for this upper tail is at 4.3. find the tail area.
1592,1,"['range', 'tail', 'table']", Testing for goodness of fit using chisquare,seg_57,"using software, we can find that the tail area shaded in figure 6.8(b) to be 0.1165. if using a table, we would only be able to find a range of values for the tail area: between 0.1 and 0.2."
1593,1,"['degrees of freedom', 'distribution', 'tail']", Testing for goodness of fit using chisquare,seg_57,figure 6.8(c) shows an upper tail for a chi-square distribution with 5 degrees of freedom and a cutoff of 5.1. find the tail area.
1594,1,"['tail', 'table']", Testing for goodness of fit using chisquare,seg_57,"using software, we would obtain a tail area of 0.4038. if using the table in appendix c.3, we would have identified that the tail area is larger than 0.3 but not be able to give the precise value."
1595,1,"['degrees of freedom', 'distribution']", Testing for goodness of fit using chisquare,seg_57,figure 6.8(d) shows a cutoff of 11.7 on a chi-square distribution with 7 degrees of freedom. find the area of the upper tail.34
1596,1,"['degrees of freedom', 'distribution']", Testing for goodness of fit using chisquare,seg_57,figure 6.8(e) shows a cutoff of 10 on a chi-square distribution with 4 degrees of freedom. find the area of the upper tail.35
1597,1,['distribution'], Testing for goodness of fit using chisquare,seg_57,figure 6.8(f) shows a cutoff of 9.21 with a chi-square distribution with 3 df. find the area of the upper tail.36
1598,1,"['degrees of freedom', 'distribution']", Testing for goodness of fit using chisquare,seg_57,"figure 6.8: (a) chi-square distribution with 3 degrees of freedom, area above 6.25 shaded. (b) 2 degrees of freedom, area above 4.3 shaded. (c) 5 degrees of freedom, area above 5.1 shaded. (d) 7 degrees of freedom, area above 11.7 shaded. (e) 4 degrees of freedom, area above 10 shaded. (f) 3 degrees of freedom, area above 9.21 shaded."
1599,1,['distribution'], Testing for goodness of fit using chisquare,seg_57,6.3.4 finding a p-value for a chi-square distribution
1600,1,"['alternative hypothesis', 'bias', 'test statistic', 'statistic', 'test', 'null hypothesis', 'hypothesis']", Testing for goodness of fit using chisquare,seg_57,"in section 6.3.2, we identified a new test statistic (x2) within the context of assessing whether there was evidence of racial bias in how jurors were sampled. the null hypothesis represented the claim that jurors were randomly sampled and there was no racial bias. the alternative hypothesis was that there was racial bias in how the jurors were sampled."
1601,1,"['bins', 'alternative hypothesis', 'bias', 'degrees of freedom', 'test statistic', 'case', 'distribution', 'statistic', 'test', 'null hypothesis', 'hypothesis']", Testing for goodness of fit using chisquare,seg_57,"we determined that a large x2 value would suggest strong evidence favoring the alternative hypothesis: that there was racial bias. however, we could not quantify what the chance was of observing such a large test statistic (x2 = 5.89) if the null hypothesis actually was true. this is where the chi-square distribution becomes useful. if the null hypothesis was true and there was no racial bias, then x2 would follow a chi-square distribution, with three degrees of freedom in this case. under certain conditions, the statistic x2 follows a chi-square distribution with k − 1 degrees of freedom, where k is the number of bins."
1602,1,"['degrees of freedom', 'distribution', 'associated', 'categories']", Testing for goodness of fit using chisquare,seg_57,how many categories were there in the juror example? how many degrees of freedom should be associated with the chi-square distribution used for x2?
1603,1,"['degrees of freedom', 'test statistic', 'distribution', 'statistic', 'categories', 'test']", Testing for goodness of fit using chisquare,seg_57,"in the jurors example, there were k = 4 categories: white, black, hispanic, and other. according to the rule above, the test statistic x2 should then follow a chi-square distribution with k − 1 = 3 degrees of freedom if h0 is true."
1604,1,"['sample size', 'sample', 'model', 'condition', 'test statistic', 'distribution', 'statistic', 'normal', 'test', 'normal distribution']", Testing for goodness of fit using chisquare,seg_57,"just like we checked sample size conditions to use a normal distribution in earlier sections, we must also check a sample size condition to safely apply the chi-square distribution for x2. each expected count must be at least 5. in the juror example, the expected counts were 198, 19.25, 33, and 24.75, all easily above 5, so we can apply the chi-square model to the test statistic, x2 = 5.89."
1605,1,"['degrees of freedom', 'test statistic', 'distribution', 'statistic', 'hypothesis', 'associated', 'test', 'null hypothesis']", Testing for goodness of fit using chisquare,seg_57,"if the null hypothesis is true, the test statistic x2 = 5.89 would be closely associated with a chisquare distribution with three degrees of freedom. using this distribution and test statistic, identify the p-value."
1606,1,"['bias', 'table', 'data', 'distribution', 'hypothesis', 'tail', 'statistical', 'null hypothesis']", Testing for goodness of fit using chisquare,seg_57,"the chi-square distribution and p-value are shown in figure 6.9. because larger chi-square values correspond to stronger evidence against the null hypothesis, we shade the upper tail to represent the p-value. using statistical software (or the table in appendix c.3), we can determine that the area is 0.1171. generally we do not reject the null hypothesis with such a large p-value. in other words, the data do not provide convincing evidence of racial bias in the juror selection."
1607,1,"['hypothesis test', 'distribution', 'hypothesis', 'test']", Testing for goodness of fit using chisquare,seg_57,figure 6.9: the p-value for the juror hypothesis test is shaded in the chi-square distribution with df = 3.
1608,1,"['degrees of freedom', 'test statistic', 'distribution', 'statistic', 'set', 'categories', 'test', 'null hypothesis', 'hypothesis']", Testing for goodness of fit using chisquare,seg_57,"suppose we are to evaluate whether there is convincing evidence that a set of observed counts o1, o2, ..., ok in k categories are unusually different from what might be expected under a null hypothesis. call the expected counts that are based on the null hypothesis e1, e2, ..., ek. if each expected count is at least 5 and the null hypothesis is true, then the test statistic below follows a chi-square distribution with k − 1 degrees of freedom:"
1609,1,"['test statistic', 'statistic', 'hypothesis', 'tail', 'test', 'null hypothesis']", Testing for goodness of fit using chisquare,seg_57,the p-value for this test statistic is found by looking at the upper tail of this chi-square distribution. we consider the upper tail because larger values of x2 would provide greater evidence against the null hypothesis.
1610,1,['test'], Testing for goodness of fit using chisquare,seg_57,there are two conditions that must be checked before performing a chi-square test:
1611,1,"['cases', 'independent', 'case', 'table']", Testing for goodness of fit using chisquare,seg_57,independence. each case that contributes a count to the table must be independent of all the other cases in the table.
1612,1,"['distribution', 'cases']", Testing for goodness of fit using chisquare,seg_57,sample size / distribution. each particular scenario (i.e. cell count) must have at least 5 expected cases.
1613,1,"['error', 'rates']", Testing for goodness of fit using chisquare,seg_57,failing to check conditions may affect the test’s error rates.
1614,1,"['bin', 'bins', 'table']", Testing for goodness of fit using chisquare,seg_57,"when examining a table with just two bins, pick a single bin and use the one-proportion methods introduced in section 6.1."
1615,1,"['distribution', 'goodness of fit', 'evaluating']", Testing for goodness of fit using chisquare,seg_57,6.3.5 evaluating goodness of fit for a distribution
1616,1,"['model', 'independent', 'evaluating', 'data', 'statistical', 'set', 'data set', 'statistical model', 'test']", Testing for goodness of fit using chisquare,seg_57,"section 4.2 would be useful background reading for this example, but it is not a prerequisite. we can apply the chi-square testing framework to the second problem in this section: evaluating whether a certain statistical model fits a data set. daily stock returns from the s&p500 for 10 can be used to assess whether stock activity each day is independent of the stock’s behavior on previous days. this sounds like a very complex question, and it is, but a chi-square test can be used to study the problem. we will label each day as up or down (d) depending on whether the market was up or down that day. for example, consider the following changes in price, their new labels of up and down, and then the number of days that must be observed before each up day:"
1617,1,"['independent', 'failures', 'geometric distribution', 'data', 'distribution', 'success', 'probability', 'geometric', 'trial']", Testing for goodness of fit using chisquare,seg_57,"if the days really are independent, then the number of days until a positive trading day should follow a geometric distribution. the geometric distribution describes the probability of waiting for the kth trial to observe the first success. here each up day (up) represents a success, and down (d) days represent failures. in the data above, it took only one day until the market was up, so the first wait time was 1 day. it took two more days before we observed our next up trading day, and two more for the third up day. we would like to determine if these counts (1, 2, 2, 1, 4, and so on) follow the geometric distribution. figure 6.10 shows the number of waiting days for a positive trading day during 10 years for the s&p500."
1618,1,['distribution'], Testing for goodness of fit using chisquare,seg_57,figure 6.10: observed distribution of the waiting time until a positive trading day for the s&p500.
1619,1,"['independent', 'geometric distribution', 'distribution', 'probability', 'geometric', 'hypothesis']", Testing for goodness of fit using chisquare,seg_57,"we consider how many days one must wait until observing an up day on the s&p500 stock index. if the stock activity was independent from one day to the next and the probability of a positive trading day was constant, then we would expect this waiting time to follow a geometric distribution. we can organize this into a hypothesis framework:"
1620,1,['independent'], Testing for goodness of fit using chisquare,seg_57,h0: the stock market being up or down on a given day is independent from all other days. we
1621,1,"['distribution', 'geometric distribution', 'geometric', 'hypothesis']", Testing for goodness of fit using chisquare,seg_57,"will consider the number of days that pass until an up day is observed. under this hypothesis, the number of days until an up day should follow a geometric distribution."
1622,1,['independent'], Testing for goodness of fit using chisquare,seg_57,ha: the stock market being up or down on a given day is not independent from all other days.
1623,1,"['alternative hypothesis', 'geometric distribution', 'distribution', 'deviations', 'geometric', 'hypothesis']", Testing for goodness of fit using chisquare,seg_57,"since we know the number of days until an up day would follow a geometric distribution under the null, we look for deviations from the geometric distribution, which would support the alternative hypothesis."
1624,1,['information'], Testing for goodness of fit using chisquare,seg_57,"there are important implications in our result for stock traders: if information from past trading days is useful in telling what will happen today, that information may provide an advantage over other traders."
1625,1,['data'], Testing for goodness of fit using chisquare,seg_57,we consider data for the s&p500 and summarize the waiting times in figure 6.11 and figure 6.12. the s&p500 was positive on 54.5% of those days.
1626,1,"['bin', 'model', 'method', 'data', 'cases', 'associated', 'geometric']", Testing for goodness of fit using chisquare,seg_57,"because applying the chi-square framework requires expected counts to be at least 5, we have binned together all the cases where the waiting time was at least 7 days to ensure each expected count is well above this minimum. the actual data, shown in the observed row in figure 6.11, can be compared to the expected counts from the geometric model row. the method for computing expected counts is discussed in figure 6.11. in general, the expected counts are determined by (1) identifying the null proportion associated with each bin, then (2) multiplying each null proportion by the total count to obtain the expected counts. that is, this strategy identifies what proportion of the total count we would expect to be in each bin."
1627,1,"['model', 'geometric']", Testing for goodness of fit using chisquare,seg_57,days 1 2 3 4 5 6 7+ total observed 717 369 155 69 28 14 10 1362 geometric model 743 338 154 70 32 14 12 1362
1628,1,"['distribution', 'model', 'geometric', 'probability']", Testing for goodness of fit using chisquare,seg_57,"figure 6.11: distribution of the waiting time until a positive trading day. the expected counts based on the geometric model are shown in the last row. to find each expected count, we identify the probability of waiting d days based on the geometric model (p (d) = (1 − 0.545)d−1(0.545)) and multiply by the total number of streaks, 1362. for example, waiting for three days occurs under the geometric model about 0.4552× 0.545 = 11.28% of the time, which corresponds to 0.1128× 1362 = 154 streaks."
1629,0,[], Testing for goodness of fit using chisquare,seg_57,800 observed expected 600 ycneu 400 qerf 200 0 1 2 3 4 5 6 7+ wait until positive day
1630,1,"['plot', 'bar plot']", Testing for goodness of fit using chisquare,seg_57,figure 6.12: side-by-side bar plot of the observed and expected counts for each waiting time.
1631,1,['deviations'], Testing for goodness of fit using chisquare,seg_57,do you notice any unusually large deviations in the graph? can you tell if these deviations are due to chance just by looking?
1632,1,"['data', 'distribution', 'deviations', 'test', 'null hypothesis', 'hypothesis']", Testing for goodness of fit using chisquare,seg_57,"it is not obvious whether differences in the observed counts and the expected counts from the geometric distribution are significantly different. that is, it is not clear whether these deviations might be due to chance or whether they are so strong that the data provide convincing evidence against the null hypothesis. however, we can perform a chi-square test using the counts in figure 6.11."
1633,1,"['geometric distribution', 'test statistic', 'data', 'distribution', 'set', 'statistic', 'test', 'geometric']", Testing for goodness of fit using chisquare,seg_57,"figure 6.11 provides a set of count data for waiting times (o1 = 717, o2 = 369, ...) and expected counts under the geometric distribution (e1 = 743, e2 = 338, ...). compute the chi-square test statistic, x2.37"
1634,1,"['degrees of freedom', 'distribution']", Testing for goodness of fit using chisquare,seg_57,"because the expected counts are all at least 5, we can safely apply the chi-square distribution to x2. however, how many degrees of freedom should we use?38"
1635,1,"['model', 'geometric', 'test statistic', 'information', 'distribution', 'statistic', 'test']", Testing for goodness of fit using chisquare,seg_57,"if the observed counts follow the geometric model, then the chi-square test statistic x2 = 4.61 would closely follow a chi-square distribution with df = 6. using this information, compute a p-value."
1636,1,"['independent', 'geometric distribution', 'data', 'distribution', 'geometric']", Testing for goodness of fit using chisquare,seg_57,"figure 6.13 shows the chi-square distribution, cutoff, and the shaded p-value. using software, we can find the p-value: 0.5951. ultimately, we do not have sufficient evidence to reject the notion that the wait times follow a geometric distribution for the last 10 years of data for the s&p500, i.e. we cannot reject the notion that trading days are independent."
1637,0,[], Testing for goodness of fit using chisquare,seg_57,area representing the p−value 0 5 10 15 20 25
1638,1,"['degrees of freedom', 'distribution']", Testing for goodness of fit using chisquare,seg_57,figure 6.13: chi-square distribution with 6 degrees of freedom. the p-value for the stock analysis is shaded.
1639,1,"['independent', 'null hypothesis', 'data', 'hypothesis']", Testing for goodness of fit using chisquare,seg_57,"in example 6.36, we did not reject the null hypothesis that the trading days are independent during the last 10 of data. why is this so important?"
1640,1,['dependence'], Testing for goodness of fit using chisquare,seg_57,"it may be tempting to think the market is “due” for an up day if there have been several consecutive days where it has been down. however, we haven’t found strong evidence that there’s any such property where the market is “due” for a correction. at the very least, the analysis suggests any dependence between days is very weak."
1641,0,[], Testing for independence in twoway tables,seg_59,"we all buy used products – cars, computers, textbooks, and so on – and we sometimes assume the sellers of those products will be forthright about any underlying problems with what they’re selling. this is not something we should take for granted. researchers recruited 219 participants in a study where they would sell a used ipod40 that was known to have frozen twice in the past. the participants were incentivized to get as much money as they could for the ipod since they would receive a 5% cut of the sale on top of $10 for participating. the researchers wanted to understand what types of questions would elicit the seller to disclose the freezing issue."
1642,1,['likelihood'], Testing for independence in twoway tables,seg_59,"unbeknownst to the participants who were the sellers in the study, the buyers were collaborating with the researchers to evaluate the influence of different questions on the likelihood of getting the sellers to disclose the past issues with the ipod. the scripted buyers started with “okay, i guess i’m supposed to go first. so you’ve had the ipod for 2 years ...” and ended with one of three questions:"
1643,0,[], Testing for independence in twoway tables,seg_59,• general: what can you tell me about it?
1644,0,[], Testing for independence in twoway tables,seg_59,"• positive assumption: it doesn’t have any problems, does it?"
1645,0,[], Testing for independence in twoway tables,seg_59,• negative assumption: what problems does it have?
1646,1,"['treatment', 'results', 'response', 'data']", Testing for independence in twoway tables,seg_59,"the question is the treatment given to the sellers, and the response is whether the question prompted them to disclose the freezing issue with the ipod. the results are shown in figure 6.14, and the data suggest that asking the, what problems does it have?, was the most effective at getting the seller to disclose the past freezing issues. however, you should also be asking yourself: could we see these results due to chance alone, or is this in fact evidence that some questions are more effective for getting at the truth?"
1647,0,[], Testing for independence in twoway tables,seg_59,general positive assumption negative assumption total disclose problem 2 23 36 61 hide problem 71 50 37 158 total 73 73 73 219
1648,0,[], Testing for independence in twoway tables,seg_59,"figure 6.14: summary of the ipod study, where a question was posed to the study participant who acted"
1649,1,"['outcomes', 'independent', 'table', 'variables', 'dependent', 'variable', 'combinations', 'outcome']", Testing for independence in twoway tables,seg_59,"a one-way table describes counts for each outcome in a single variable. a two-way table describes counts for combinations of outcomes for two variables. when we consider a two-way table, we often would like to know, are these variables related in any way? that is, are they dependent (versus independent)?"
1650,1,"['independent', 'experiment', 'hypothesis test', 'success', 'test', 'hypothesis']", Testing for independence in twoway tables,seg_59,"the hypothesis test for the ipod experiment is really about assessing whether there is statistically significant evidence that the success each question had on getting the participant to disclose the problem with the ipod. in other words, the goal is to check whether the buyer’s question was independent of whether the seller disclosed a problem."
1651,1,['tables'], Testing for independence in twoway tables,seg_59,6.4.1 expected counts in two-way tables
1652,1,"['tables', 'table', 'estimated']", Testing for independence in twoway tables,seg_59,"like with one-way tables, we will need to compute estimated counts for each cell in a two-way table."
1653,1,['experiment'], Testing for independence in twoway tables,seg_59,"from the experiment, we can compute the proportion of all sellers who disclosed the freezing problem as 61/219 = 0.2785. if there really is no difference among the questions and 27.85% of sellers were going to disclose the freezing problem no matter the question that was put to them, how many of the 73 people in the general group would we have expected to disclose the freezing problem?"
1654,1,"['vary', 'variation']", Testing for independence in twoway tables,seg_59,"we would predict that 0.2785×73 = 20.33 sellers would disclose the problem. obviously we observed fewer than this, though it is not yet clear if that is due to chance variation or whether that is because the questions vary in how effective they are at getting to the truth."
1655,0,[], Testing for independence in twoway tables,seg_59,"if the questions were actually equally effective, meaning about 27.85% of respondents would disclose the freezing issue regardless of what question they were asked, about how many sellers would we expect to hide the freezing problem from the positive assumption group?41"
1656,0,[], Testing for independence in twoway tables,seg_59,"we can compute the expected number of sellers who we would expect to disclose or hide the freezing issue for all groups, if the questions had no impact on what they disclosed, using the same strategy employed in example 6.38 and guided practice 6.39. these expected counts were used to construct figure 6.15, which is the same as figure 6.14, except now the expected counts have been added in parentheses."
1657,0,[], Testing for independence in twoway tables,seg_59,figure 6.15: the observed counts and the (expected counts).
1658,1,"['column totals', 'table', 'row totals']", Testing for independence in twoway tables,seg_59,"the examples and exercises above provided some help in computing expected counts. in general, expected counts for a two-way table may be computed using the row totals, column totals, and the table total. for instance, if there was no difference between the groups, then about 27.85% of each column should be in the first row:"
1659,0,[], Testing for independence in twoway tables,seg_59,looking back to how 0.2785 was computed – as the fraction of sellers who disclosed the freezing issue (158/219) – these three expected counts could have been computed as
1660,1,"['association', 'table', 'variable', 'test']", Testing for independence in twoway tables,seg_59,this leads us to a general formula for computing expected counts in a two-way table when we would like to test whether there is strong evidence of an association between the column variable and row variable.
1661,0,[], Testing for independence in twoway tables,seg_59,"to identify the expected count for the ith row and jth column, compute"
1662,1,['table'], Testing for independence in twoway tables,seg_59,"(row i total)× (column j total) expected countrow i, col j = table total"
1663,1,"['tables', 'test']", Testing for independence in twoway tables,seg_59,6.4.2 the chi-square test for two-way tables
1664,1,"['table', 'test statistic', 'statistic', 'test']", Testing for independence in twoway tables,seg_59,"the chi-square test statistic for a two-way table is found the same way it is found for a one-way table. for each table count, compute"
1665,0,[], Testing for independence in twoway tables,seg_59,(observed count − expected count)2 general formula expected count
1666,1,"['test statistic', 'statistic', 'test']", Testing for independence in twoway tables,seg_59,adding the computed value for each cell gives the chi-square test statistic x2:
1667,1,"['degrees of freedom', 'test statistic', 'distribution', 'statistic', 'test']", Testing for independence in twoway tables,seg_59,"just like before, this test statistic follows a chi-square distribution. however, the degrees of freedom"
1668,1,"['degrees of freedom', 'tables', 'table']", Testing for independence in twoway tables,seg_59,"42 are computed a little differently for a two-way table. for two way tables, the degrees of freedom is equal to"
1669,0,[], Testing for independence in twoway tables,seg_59,df = (number of rows minus 1)× (number of columns minus 1)
1670,1,"['degrees of freedom', 'parameter']", Testing for independence in twoway tables,seg_59,"in our example, the degrees of freedom parameter is"
1671,1,"['degrees of freedom', 'experiment', 'test statistic', 'information', 'distribution', 'statistic', 'test', 'null hypothesis', 'hypothesis']", Testing for independence in twoway tables,seg_59,"if the null hypothesis is true (i.e. the questions had no impact on the sellers in the experiment), then the test statistic x2 = 40.13 closely follows a chi-square distribution with 2 degrees of freedom. using this information, we can compute the p-value for the test, which is depicted in figure 6.16."
1672,1,"['test', 'table']", Testing for independence in twoway tables,seg_59,"when applying the chi-square test to a two-way table, we use"
1673,1,['table'], Testing for independence in twoway tables,seg_59,where r is the number of rows in the table and c is the number of columns.
1674,1,"['contingency tables', 'tables']", Testing for independence in twoway tables,seg_59,"when analyzing 2-by-2 contingency tables, one guideline is to use the two-proportion methods introduced in section 6.2."
1675,0,[], Testing for independence in twoway tables,seg_59,tail area (1 / 500 million) is too small to see 0 10 20 30 40 50
1676,0,[], Testing for independence in twoway tables,seg_59,figure 6.16: visualization of the p-value for x2 = 40.13 when df = 2.
1677,1,['likelihood'], Testing for independence in twoway tables,seg_59,find the p-value and draw a conclusion about whether the question affects the sellers likelihood of reporting the freezing problem.
1678,1,"['degrees of freedom', 'table', 'likelihood', 'data', 'distribution', 'significance level', 'tail', 'level', 'significance', 'null hypothesis', 'hypothesis']", Testing for independence in twoway tables,seg_59,"using a computer, we can compute a very precise value for the tail area above x2 = 40.13 for a chi-square distribution with 2 degrees of freedom: 0.000000002. (if using the table in appendix c.3, we would identify the p-value is smaller than 0.001.) using a significance level of α = 0.05, the null hypothesis is rejected since the p-value is smaller. that is, the data provide convincing evidence that the question asked did affect a seller’s likelihood to tell the truth about problems with the ipod."
1679,1,"['experiment', 'evaluating', 'treatment', 'results', 'failure', 'success', 'hypotheses', 'outcome', 'control', 'test']", Testing for independence in twoway tables,seg_59,"figure 6.17 summarizes the results of an experiment evaluating three treatments for type 2 diabetes in patients aged 10-17 who were being treated with metformin. the three treatments considered were continued treatment with metformin (met), treatment with metformin combined with rosiglitazone (rosi), or a lifestyle intervention program. each patient had a primary outcome, which was either lacked glycemic control (failure) or did not lack that control (success). what are appropriate hypotheses for this test?"
1680,0,[], Testing for independence in twoway tables,seg_59,h0: there is no difference in the effectiveness of the three treatments.
1681,0,[], Testing for independence in twoway tables,seg_59,"ha: there is some difference in effectiveness between the three treatments, e.g. perhaps the rosi"
1682,0,[], Testing for independence in twoway tables,seg_59,treatment performed better than lifestyle.
1683,1,['success'], Testing for independence in twoway tables,seg_59,failure success total lifestyle 109 125 234 met 120 112 232 rosi 90 143 233 total 319 380 699
1684,1,['results'], Testing for independence in twoway tables,seg_59,figure 6.17: results for the type 2 diabetes study.
1685,1,"['table', 'expected values', 'hypotheses', 'test']", Testing for independence in twoway tables,seg_59,"a chi-square test for a two-way table may be used to test the hypotheses in example 6.41. as a first step, compute the expected values for each of the six table cells.43"
1686,1,"['test statistic', 'data', 'statistic', 'test']", Testing for independence in twoway tables,seg_59,44 compute the chi-square test statistic for the data in figure 6.17.
1687,1,"['degrees of freedom', 'level', 'significance', 'test', 'significance level', 'null hypothesis', 'hypothesis']", Testing for independence in twoway tables,seg_59,"because there are 3 rows and 2 columns, the degrees of freedom for the test is df = (3−1)×(2−1) = 2. use x2 = 8.16, df = 2, evaluate whether to reject the null hypothesis using a significance level"
1688,1,"['confidence intervals', 'estimates', 'case', 'statistic', 'paired', 'sample', 'estimate', 'data', 'intervals', 'confidence', 'statistical', 'distributions', 'paired data', 'statistical inference', 'test statistic', 'distribution', 'point estimates', 'test', 'anova', 'normal', 'hypotheses', 'point estimate', 'normal distribution']",Chapter  Inference for numerical data,seg_61,"7.1 one-sample means with the t-distribution 7.2 paired data 7.3 difference of two means 7.4 power calculations for a difference of means 7.5 comparing many means with anova chapter 5 introduced a framework for statistical inference based on confidence intervals and hypotheses using the normal distribution for sample proportions. in this chapter, we encounter several new point estimates and a couple new distributions. in each case, the inference ideas remain the same: determine which point estimate or test statistic is useful, identify an appropriate distribution for the point estimate or test statistic, and apply the ideas of inference. for videos, slides, and other resources, please visit www.openintro.org/os"
1689,1,"['confidence intervals', 'sample', 'sample mean', 'hypothesis tests', 'intervals', 'mean', 'tests', 'confidence', 'model', 'distribution', 'hypothesis', 'normal', 'normal distribution']", Onesample means with the tdistribution,seg_63,"similar to how we can model the behavior of the sample proportion p̂ using a normal distribution, the sample mean x̄ can also be modeled using a normal distribution when certain conditions are met. however, we’ll soon learn that a new distribution, called the t-distribution, tends to be more useful when working with the sample mean. we’ll first learn about this new distribution, then we’ll use it to construct confidence intervals and conduct hypothesis tests for the mean."
1690,1,"['sampling', 'sampling distribution', 'distribution']", Onesample means with the tdistribution,seg_63,7.1.1 the sampling distribution of x̄
1691,1,"['sample size', 'sample', 'deviation', 'population standard deviation', 'sample mean', 'standard error', 'population mean', 'distribution', 'normal', 'mean', 'population', 'standard', 'standard deviation', 'error', 'normal distribution']", Onesample means with the tdistribution,seg_63,"the sample mean tends to follow a normal distribution centered at the population mean, µ, when certain conditions are met. additionally, we can compute a standard error for the sample mean using the population standard deviation σ and the sample size n."
1692,1,"['deviation', 'sample', 'independent', 'observations', 'sampling', 'distribution', 'normal', 'mean', 'population', 'standard', 'standard deviation', 'sampling distribution']", Onesample means with the tdistribution,seg_63,"when we collect a sufficiently large sample of n independent observations from a population with mean µ and standard deviation σ, the sampling distribution of x̄ will be nearly normal with"
1693,1,"['standard error', 'mean', 'standard', 'error']", Onesample means with the tdistribution,seg_63,σ mean = µ standard error (se) = √n
1694,1,"['tests', 'confidence intervals', 'hypothesis tests', 'intervals', 'confidence', 'hypothesis']", Onesample means with the tdistribution,seg_63,"before diving into confidence intervals and hypothesis tests using x̄, we first need to cover two topics:"
1695,1,"['distribution', 'normal', 'normal distribution']", Onesample means with the tdistribution,seg_63,"• when we modeled p̂ using the normal distribution, certain conditions had to be satisfied. the conditions for working with x̄ are a little more complex, and we’ll spend section 7.1.2 discussing how to check conditions for inference."
1696,1,"['deviation', 'standard', 'estimate', 'standard error', 'estimation', 'dependent', 'distribution', 'population', 'standard deviation', 'population standard deviation', 'error']", Onesample means with the tdistribution,seg_63,"• the standard error is dependent on the population standard deviation, σ. however, we rarely know σ, and instead we must estimate it. because this estimation is itself imperfect, we use a new distribution called the t-distribution to fix this problem, which we discuss in section 7.1.3."
1697,1,['evaluating'], Onesample means with the tdistribution,seg_63,7.1.2 evaluating the two conditions required for modeling x̄
1698,1,"['sample', 'sample mean', 'mean', 'central limit theorem', 'limit']", Onesample means with the tdistribution,seg_63,two conditions are required to apply the central limit theorem for a sample mean x̄:
1699,1,"['sample', 'independent', 'observations']", Onesample means with the tdistribution,seg_63,"independence. the sample observations must be independent, the most common way to satisfy"
1700,1,"['random process', 'sample', 'random', 'condition', 'simple random sample', 'data', 'independence', 'population', 'random sample', 'process']", Onesample means with the tdistribution,seg_63,"this condition is when the sample is a simple random sample from the population. if the data come from a random process, analogous to rolling a die, this would also satisfy the independence condition."
1701,1,"['sample', 'observations']", Onesample means with the tdistribution,seg_63,"normality. when a sample is small, we also require that the sample observations come from a"
1702,1,"['sample', 'condition', 'population']", Onesample means with the tdistribution,seg_63,"normally distributed population. we can relax this condition more and more for larger and larger sample sizes. this condition is obviously vague, making it difficult to evaluate, so next we introduce a couple rules of thumb to make checking this condition easier."
1703,1,"['normality', 'condition']", Onesample means with the tdistribution,seg_63,"there is no perfect way to check the normality condition, so instead we use two rules of thumb:"
1704,1,"['sample size', 'sample', 'outliers', 'condition', 'data', 'distribution', 'normal', 'normal distribution']", Onesample means with the tdistribution,seg_63,"n < 30: if the sample size n is less than 30 and there are no clear outliers in the data, then we typically assume the data come from a nearly normal distribution to satisfy the condition."
1705,1,"['sample size', 'sample', 'outliers', 'extreme outliers', 'observations', 'sampling', 'distribution', 'normal', 'sampling distribution']", Onesample means with the tdistribution,seg_63,"n ≥ 30: if the sample size n is at least 30 and there are no particularly extreme outliers, then we typically assume the sampling distribution of x̄ is nearly normal, even if the underlying distribution of individual observations is not."
1706,1,"['condition', 'statistics', 'cases']", Onesample means with the tdistribution,seg_63,"in this first course in statistics, you aren’t expected to develop perfect judgement on the normality condition. however, you are expected to be able to handle clear cut cases based on the rules of thumb.1"
1707,1,"['sample', 'random samples', 'plots', 'populations', 'samples', 'random']", Onesample means with the tdistribution,seg_63,consider the following two plots that come from simple random samples from different populations. their sample sizes are n1 = 15 and n2 = 50.
1708,1,"['sample', 'observations']", Onesample means with the tdistribution,seg_63,4 25 20 3 y y c c n n 15 e e u 2 u q q e e 10 r r f f 1 5 0 0 0 2 4 6 0 10 20 sample 1 observations (n = 15) sample 2 observations (n = 50)
1709,1,"['case', 'independence', 'normality']", Onesample means with the tdistribution,seg_63,are the independence and normality conditions met in each case?
1710,1,"['sample', 'condition', 'simple random sample', 'random sample', 'independence', 'normality', 'samples', 'population', 'random']", Onesample means with the tdistribution,seg_63,"each samples is from a simple random sample of its respective population, so the independence condition is satisfied. let’s next check the normality condition for each using the rule of thumb."
1711,1,"['outliers', 'sample', 'histogram', 'condition', 'observations', 'normality']", Onesample means with the tdistribution,seg_63,"the first sample has fewer than 30 observations, so we are watching for any clear outliers. none are present; while there is a small gap in the histogram on the right, this gap is small and 20% of the observations in this small sample are represented in that far right bar of the histogram, so we can hardly call these clear outliers. with no clear outliers, the normality condition is reasonably met."
1712,1,"['sample size', 'sample', 'condition', 'observation', 'distribution', 'normality', 'outlier', 'extreme outlier']", Onesample means with the tdistribution,seg_63,"the second sample has a sample size greater than 30 and includes an outlier that appears to be roughly 5 times further from the center of the distribution than the next furthest observation. this is an example of a particularly extreme outlier, so the normality condition would not be satisfied."
1713,1,"['sample size', 'sample', 'outliers', 'condition', 'extreme outliers', 'skewed', 'data', 'distribution', 'normality', 'skewed distribution', 'population', 'skew']", Onesample means with the tdistribution,seg_63,"in practice, it’s typical to also do a mental check to evaluate whether we have reason to believe the underlying population would have moderate skew (if n < 30) or have particularly extreme outliers (n ≥ 30) beyond what we observe in the data. for example, consider the number of followers for each individual account on twitter, and then imagine this distribution. the large majority of accounts have built up a couple thousand followers or fewer, while a relatively tiny fraction have amassed tens of millions of followers, meaning the distribution is extremely skewed. when we know the data come from such an extremely skewed distribution, it takes some effort to understand what sample size is large enough for the normality condition to be satisfied."
1714,0,[], Onesample means with the tdistribution,seg_63,7.1.3 introducing the t-distribution
1715,1,"['deviation', 'sample', 'population standard deviation', 'standard error', 'sample standard deviation', 'population', 'standard', 'standard deviation', 'error']", Onesample means with the tdistribution,seg_63,"in practice, we cannot directly calculate the standard error for x̄ since we do not know the population standard deviation, σ. we encountered a similar issue when computing the standard error for a sample proportion, which relied on the population proportion, p. our solution in the proportion context was to use sample value in place of the population value when computing the standard error. we’ll employ a similar strategy for computing the standard error of x̄, using the sample standard deviation s in place of σ:"
1716,1,"['model', 'estimate', 'data', 'distribution', 'samples', 'normal', 'normal distribution']", Onesample means with the tdistribution,seg_63,"this strategy tends to work well when we have a lot of data and can estimate σ using s accurately. however, the estimate is less precise with smaller samples, and this leads to problems when using the normal distribution to model x̄."
1717,1,"['observations', 'distribution', 'normal', 'mean', 'standard', 'tails', 'normal distribution']", Onesample means with the tdistribution,seg_63,"we’ll find it useful to use a new distribution for inference calculations called the t-distribution. a t-distribution, shown as a solid line in figure 7.1, has a bell shape. however, its tails are thicker than the normal distribution’s, meaning observations are more likely to fall beyond two standard deviations from the mean than under the normal distribution. the extra thick tails of the t-distribution are exactly the correction needed to resolve the problem of using s in place of σ in the se calculation."
1718,1,"['distribution', 'normal', 'normal distribution']", Onesample means with the tdistribution,seg_63,figure 7.1: comparison of a t-distribution and a normal distribution.
1719,1,"['degrees of freedom', 'distribution', 'normal', 'parameter', 'normal distribution']", Onesample means with the tdistribution,seg_63,the t-distribution is always centered at zero and has a single parameter: degrees of freedom. the degrees of freedom (df ) describes the precise form of the bell-shaped t-distribution. several t-distributions are shown in figure 7.2 in comparison to the normal distribution.
1720,1,"['sample size', 'sample', 'model', 'degrees of freedom', 'sample mean', 'standard normal', 'observations', 'distribution', 'normal', 'mean', 'standard', 'standard normal distribution', 'normal distribution']", Onesample means with the tdistribution,seg_63,"in general, we’ll use a t-distribution with df = n−1 to model the sample mean when the sample size is n. that is, when we have more observations, the degrees of freedom will be larger and the t-distribution will look more like the standard normal distribution; when the degrees of freedom is about 30 or more, the t-distribution is nearly indistinguishable from the normal distribution."
1721,1,"['degrees of freedom', 'standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Onesample means with the tdistribution,seg_63,"figure 7.2: the larger the degrees of freedom, the more closely the t-distribution resembles the standard normal distribution."
1722,1,"['degrees of freedom', 'model', 'distribution', 'normal']", Onesample means with the tdistribution,seg_63,"the degrees of freedom describes the shape of the t-distribution. the larger the degrees of freedom, the more closely the distribution approximates the normal model."
1723,0,[], Onesample means with the tdistribution,seg_63,"when modeling x̄ using the t-distribution, use df = n− 1."
1724,1,"['method', 'table', 'data', 'distribution', 'normal', 'numerical', 'statistical', 'normal distribution']", Onesample means with the tdistribution,seg_63,"the t-distribution allows us greater flexibility than the normal distribution when analyzing numerical data. in practice, it’s common to use statistical software, such as r, python, or sas for these analyses. alternatively, a graphing calculator or a t-table may be used; the t-table is similar to the normal distribution table, and it may be found in appendix c.2, which includes usage instructions and examples for those who wish to use this option. no matter the approach you choose, apply your method using the examples below to confirm your working understanding of the t-distribution."
1725,1,['degrees of freedom'], Onesample means with the tdistribution,seg_63,figure 7.3: the t-distribution with 18 degrees of freedom. the area below -2.10 has been shaded.
1726,1,['degrees of freedom'], Onesample means with the tdistribution,seg_63,"figure 7.4: left: the t-distribution with 20 degrees of freedom, with the area above 1.65 shaded. right: the t-distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded."
1727,1,['degrees of freedom'], Onesample means with the tdistribution,seg_63,what proportion of the t-distribution with 18 degrees of freedom falls below -2.10?
1728,1,"['statistical', 'normal', 'probability']", Onesample means with the tdistribution,seg_63,"just like a normal probability problem, we first draw the picture in figure 7.3 and shade the area below -2.10. using statistical software, we can obtain a precise value: 0.0250."
1729,1,"['degrees of freedom', 'estimate', 'distribution']", Onesample means with the tdistribution,seg_63,a t-distribution with 20 degrees of freedom is shown in the left panel of figure 7.4. estimate the proportion of the distribution falling above 1.65.
1730,1,"['distribution', 'normal', 'statistical', 'normal distribution']", Onesample means with the tdistribution,seg_63,"with a normal distribution, this would correspond to about 0.05, so we should expect the tdistribution to give us a value in this neighborhood. using statistical software: 0.0573."
1731,1,"['degrees of freedom', 'estimate', 'distribution', 'mean']", Onesample means with the tdistribution,seg_63,a t-distribution with 2 degrees of freedom is shown in the right panel of figure 7.4. estimate the proportion of the distribution falling more than 3 units from the mean (above or below).
1732,1,"['degrees of freedom', 'distribution', 'normal', 'tails', 'normal distribution']", Onesample means with the tdistribution,seg_63,"with so few degrees of freedom, the t-distribution will give a more notably different value than the normal distribution. under a normal distribution, the area would be about 0.003 using the 68-95- 99.7 rule. for a t-distribution with df = 2, the area in both tails beyond 3 units totals 0.0955. this area is dramatically different than what we obtain from the normal distribution."
1733,1,['degrees of freedom'], Onesample means with the tdistribution,seg_63,what proportion of the t-distribution with 19 degrees of freedom falls above -1.79 units? use your
1734,1,"['tail areas', 'tail', 'method']", Onesample means with the tdistribution,seg_63,2 preferred method for finding tail areas.
1735,1,"['sample', 'intervals']", Onesample means with the tdistribution,seg_63,7.1.4 one sample t-confidence intervals
1736,0,[], Onesample means with the tdistribution,seg_63,"let’s get our first taste of applying the t-distribution in the context of an example about the mercury content of dolphin muscle. elevated mercury concentrations are an important problem for both dolphins and other animals, like humans, who occasionally eat them."
1737,1,"['outliers', 'sample', 'interval', 'data', 'confidence', 'average', 'confidence interval']", Onesample means with the tdistribution,seg_63,we will identify a confidence interval for the average mercury content in dolphin muscle using a sample of 19 risso’s dolphins from the taiji area in japan. the data are summarized in figure 7.6. the minimum and maximum observed values can be used to evaluate whether or not there are clear outliers.
1738,1,['measurements'], Onesample means with the tdistribution,seg_63,figure 7.6: summary of mercury content in the muscle of 19 risso’s dolphins from the taiji area. measurements are in micrograms of mercury per wet gram of muscle (µg/wet g).
1739,1,"['data', 'independence', 'normality', 'set', 'data set']", Onesample means with the tdistribution,seg_63,are the independence and normality conditions satisfied for this data set?
1740,1,"['outliers', 'observations', 'statistics', 'independence', 'random', 'random sample', 'sample', 'simple random sample', 'mean', 'standard', 'condition', 'standard deviations', 'deviations', 'normality']", Onesample means with the tdistribution,seg_63,"the observations are a simple random sample, therefore independence is reasonable. the summary statistics in figure 7.6 do not suggest any clear outliers, since all observations are within 2.5 standard deviations of the mean. based on this evidence, the normality condition seems reasonable."
1741,1,"['model', 'interval', 'standard error', 'normal', 'standard', 'confidence', 'error', 'confidence interval']", Onesample means with the tdistribution,seg_63,"in the normal model, we used z? and the standard error to determine the width of a confidence interval. we revise the confidence interval formula slightly when using the t-distribution:"
1742,1,"['point estimate', 'estimate']", Onesample means with the tdistribution,seg_63,s point estimate ± t?df × se → x̄ ± t?df × √n
1743,1,"['standard error', 'statistics', 'standard', 'average', 'error']", Onesample means with the tdistribution,seg_63,"using the summary statistics in figure 7.6, compute the standard error for the average mercury content in the n = 19 dolphins."
1744,1,"['degrees of freedom', 'distribution', 'normal', 'level', 'confidence', 'confidence level', 'normal distribution']", Onesample means with the tdistribution,seg_63,the value t?df is a cutoff we obtain based on the confidence level and the t-distribution with df degrees of freedom. that cutoff is found in the same way as with a normal distribution: we find t?df such that the fraction of the t-distribution with df degrees of freedom within a distance t?df of 0 matches the confidence level of interest.
1745,1,"['degrees of freedom', 'level', 'confidence', 'confidence level']", Onesample means with the tdistribution,seg_63,"when n = 19, what is the appropriate degrees of freedom? find t?df for this degrees of freedom and the confidence level of 95%"
1746,1,['degrees of freedom'], Onesample means with the tdistribution,seg_63,the degrees of freedom is easy to calculate: df = n− 1 = 18.
1747,1,"['tail', 'statistical']", Onesample means with the tdistribution,seg_63,"using statistical software, we find the cutoff where the upper tail is equal to 2.5%: t?18 = 2.10. the area below -2.10 will also be equal to 2.5%. that is, 95% of the t-distribution with df = 18 lies within 2.10 units of 0."
1748,1,"['interval', 'confidence', 'average', 'confidence interval']", Onesample means with the tdistribution,seg_63,compute and interpret the 95% confidence interval for the average mercury content in risso’s dolphins.
1749,1,"['confidence', 'interval', 'confidence interval']", Onesample means with the tdistribution,seg_63,we can construct the confidence interval as
1750,1,"['average', 'confident']", Onesample means with the tdistribution,seg_63,"we are 95% confident the average mercury content of muscles in risso’s dolphins is between 3.29 and 5.51 µg/wet gram, which is considered extremely high."
1751,1,"['sample', 'independent', 'interval', 'observations', 'population mean', 'normal', 'mean', 'population', 'confidence', 'confidence interval']", Onesample means with the tdistribution,seg_63,"based on a sample of n independent and nearly normal observations, a confidence interval for the population mean is"
1752,1,"['point estimate', 'estimate']", Onesample means with the tdistribution,seg_63,s point estimate ± t?df × se → x̄ ± t?df × √n
1753,1,"['sample', 'degrees of freedom', 'estimated', 'sample mean', 'standard error', 'mean', 'standard', 'level', 'confidence', 'confidence level', 'error']", Onesample means with the tdistribution,seg_63,"where x̄ is the sample mean, t?df corresponds to the confidence level and degrees of freedom df , and se is the standard error as estimated by the sample."
1754,1,"['deviation', 'sample', 'independent', 'condition', 'sample mean', 'observations', 'statistics', 'data', 'normality', 'mean', 'standard', 'standard deviation']", Onesample means with the tdistribution,seg_63,"the fda’s webpage provides some data on mercury content of fish. based on a sample of 15 croaker white fish (pacific), a sample mean and standard deviation were computed as 0.287 and 0.069 ppm (parts per million), respectively. the 15 observations ranged from 0.18 to 0.41 ppm. we will assume these observations are independent. based on the summary statistics of the data, do you have any objections to the normality condition of the individual observations?3"
1755,1,"['degrees of freedom', 'interval', 'standard error', 'data', 'mean', 'standard', 'confidence', 'error', 'confidence interval']", Onesample means with the tdistribution,seg_63,"estimate the standard error of x̄ = 0.287 ppm using the data summaries in guided practice 7.10. if we are to use the t-distribution to create a 90% confidence interval for the actual mean of the mercury content, identify the degrees of freedom and t?df ."
1756,1,"['standard error', 'error', 'standard']", Onesample means with the tdistribution,seg_63,0.069 the standard error: se = = 0.0178.
1757,1,"['confidence', 'interval', 'confidence interval']", Onesample means with the tdistribution,seg_63,"since the goal is a 90% confidence interval, we choose t?14 so that the two-tail area is 0.1: t?14 = 1.76."
1758,1,"['confidence', 'interval', 'confidence interval']", Onesample means with the tdistribution,seg_63,"once you’ve determined a one-mean confidence interval would be helpful for an application, there are four steps to constructing the interval:"
1759,1,"['level', 'confidence', 'confidence level']", Onesample means with the tdistribution,seg_63,"prepare. identify x̄, s, n, and determine what confidence level you wish to use."
1760,1,['normal'], Onesample means with the tdistribution,seg_63,check. verify the conditions to ensure x̄ is nearly normal.
1761,1,['interval'], Onesample means with the tdistribution,seg_63,"calculate. if the conditions hold, compute se, find t?df , and construct the interval."
1762,1,"['confidence', 'interval', 'confidence interval']", Onesample means with the tdistribution,seg_63,conclude. interpret the confidence interval in the context of the problem.
1763,1,"['results', 'information']", Onesample means with the tdistribution,seg_63,"using the information and results of guided practice 7.10 and example 7.11, compute a 90% con-"
1764,1,"['average', 'interval']", Onesample means with the tdistribution,seg_63,4 fidence interval for the average mercury content of croaker white fish (pacific).
1765,1,"['levels', 'interval', 'confidence', 'confidence interval']", Onesample means with the tdistribution,seg_63,the 90% confidence interval from guided practice 7.12 is 0.256 ppm to 0.318 ppm. can we say that 90% of croaker white fish (pacific) have mercury levels between 0.256 and 0.318 ppm?5
1766,1,['sample'], Onesample means with the tdistribution,seg_63,7.1.5 one sample t-tests
1767,0,[], Onesample means with the tdistribution,seg_63,"is the typical us runner getting faster or slower over time? we consider this question in the context of the cherry blossom race, which is a 10-mile race in washington, dc each spring."
1768,1,"['average', 'data']", Onesample means with the tdistribution,seg_63,"the average time for all runners who finished the cherry blossom race in 2006 was 93.29 minutes (93 minutes and about 17 seconds). we want to determine using data from 100 participants in the 2017 cherry blossom race whether runners in this race are getting faster or slower, versus the other possibility that there has been no change."
1769,1,['hypotheses'], Onesample means with the tdistribution,seg_63,what are appropriate hypotheses for this context?6
1770,1,"['sample', 'histogram', 'independent', 'condition', 'simple random sample', 'observations', 'random sample', 'data', 'normality', 'random']", Onesample means with the tdistribution,seg_63,"the data come from a simple random sample of all participants, so the observations are independent. however, should we be worried about the normality condition? see figure 7.7 for a histogram of the differences and evaluate if we can move forward.7"
1771,1,"['process', 'standard error', 'hypothesis test', 'null value', 'sampling', 'distribution', 'tail', 'mean', 'standard', 'test', 'sampling distribution', 'error', 'hypothesis']", Onesample means with the tdistribution,seg_63,"when completing a hypothesis test for the one-sample mean, the process is nearly identical to completing a hypothesis test for a single proportion. first, we find the z-score using the observed value, null value, and standard error; however, we call it a t-score since we use a t-distribution for calculating the tail area. then we find the p-value using the same ideas we used previously: find the one-tail area under the sampling distribution, and double it."
1772,0,[], Onesample means with the tdistribution,seg_63,20 15 ycneu 10 qerf 5 0 60 80 100 120 140 time (minutes)
1773,1,"['sample', 'histogram', 'data']", Onesample means with the tdistribution,seg_63,figure 7.7: a histogram of time for the sample cherry blossom race data.
1774,1,"['independence', 'statistic', 'sample', 'sample mean', 'mean', 'standard', 'standard deviation', 'sample size', 'test statistic', 'test', 'hypothesis', 'deviation', 'sample standard deviation', 'hypothesis test', 'normality', 'average']", Onesample means with the tdistribution,seg_63,"with both the independence and normality conditions satisfied, we can proceed with a hypothesis test using the t-distribution. the sample mean and sample standard deviation of the sample of 100 runners from the 2017 cherry blossom race are 97.32 and 16.98 minutes, respectively. recall that the sample size is 100 and the average run time in 2006 was 93.29 minutes. find the test statistic and p-value. what is your conclusion?"
1775,1,"['standard error', 'test statistic', 'statistic', 'standard', 'test', 'error']", Onesample means with the tdistribution,seg_63,"to find the test statistic (t-score), we first must determine the standard error:"
1776,1,"['sample', 'sample mean', 'null value', 'mean']", Onesample means with the tdistribution,seg_63,"now we can compute the t-score using the sample mean (97.32), null value (93.29), and se:"
1777,1,['statistical'], Onesample means with the tdistribution,seg_63,"for df = 100 − 1 = 99, we can determine using statistical software (or a t-table) that the one-tail area is 0.01, which we double to get the p-value: 0.02."
1778,1,"['null value', 'data', 'hypothesis', 'average', 'null hypothesis']", Onesample means with the tdistribution,seg_63,"because the p-value is smaller than 0.05, we reject the null hypothesis. that is, the data provide strong evidence that the average run time for the cherry blossom run in 2017 is different than the 2006 average. since the observed value is above the null value and we have rejected the null hypothesis, we would conclude that runners in the race were slower on average in 2017 than in 2006."
1779,1,"['hypothesis test', 'test', 'hypothesis']", Onesample means with the tdistribution,seg_63,"once you’ve determined a one-mean hypothesis test is the correct procedure, there are four steps to completing the test:"
1780,1,"['parameter of interest', 'hypotheses', 'parameter', 'level', 'significance', 'significance level']", Onesample means with the tdistribution,seg_63,"prepare. identify the parameter of interest, list out hypotheses, identify the significance level, and identify x̄, s, and n."
1781,1,['normal'], Onesample means with the tdistribution,seg_63,check. verify conditions to ensure x̄ is nearly normal.
1782,0,[], Onesample means with the tdistribution,seg_63,"calculate. if the conditions hold, compute se, compute the t-score, and identify the p-value."
1783,1,"['test', 'hypothesis test', 'hypothesis']", Onesample means with the tdistribution,seg_63,"conclude. evaluate the hypothesis test by comparing the p-value to α, and provide a conclusion in the context of the problem."
1784,1,['average'], Paired data,seg_65,"in an earlier edition of this textbook, we found that amazon prices were, on average, lower than those of the ucla bookstore for ucla courses in 2010. it’s been several years, and many stores have adapted to the online market, so we wondered, how is the ucla bookstore doing today?"
1785,1,"['data', 'set', 'data set']", Paired data,seg_65,"we sampled 201 ucla courses. of those, 68 required books could be found on amazon. a portion of the data set from these courses is shown in figure 7.8, where prices are in us dollars."
1786,1,"['data', 'cases', 'set', 'data set']", Paired data,seg_65,figure 7.8: four cases of the textbooks data set.
1787,1,"['observations', 'paired', 'paired observations']", Paired data,seg_65,7.2.1 paired observations
1788,1,"['observations', 'data', 'sets', 'set', 'data set', 'paired']", Paired data,seg_65,"each textbook has two corresponding prices in the data set: one for the ucla bookstore and one for amazon. when two sets of observations have this special correspondence, they are said to be paired."
1789,1,"['observations', 'data', 'observation', 'sets', 'set', 'data set', 'paired']", Paired data,seg_65,two sets of observations are paired if each observation in one set has a special correspondence or connection with exactly one observation in the other data set.
1790,1,"['paired data', 'outcomes', 'observations', 'data', 'variable', 'set', 'data set', 'paired']", Paired data,seg_65,"to analyze paired data, it is often useful to look at the difference in outcomes of each pair of observations. in the textbook data, we look at the differences in prices, which is represented as the price difference variable in the data set. here the differences are taken as"
1791,0,[], Paired data,seg_65,ucla bookstore price−amazon price
1792,1,"['paired data', 'histogram', 'observations', 'data', 'paired observations', 'paired']", Paired data,seg_65,"it is important that we always subtract using a consistent order; here amazon prices are always subtracted from ucla prices. the first difference shown in figure 7.8 is computed as 47.97 − 47.45 = 0.52. similarly, the second difference is computed as 14.26 − 13.55 = 0.71, and the third is 13.50 − 12.53 = 0.97. a histogram of the differences is shown in figure 7.9. using differences between paired observations is a common and useful way to analyze paired data."
1793,1,['histogram'], Paired data,seg_65,figure 7.9: histogram of the difference in price for each book sampled.
1794,1,"['paired data', 'data', 'paired']", Paired data,seg_65,7.2.2 inference for paired data
1795,1,"['paired data', 'data', 'set', 'data set', 'paired']", Paired data,seg_65,"to analyze a paired data set, we simply analyze the differences. we can use the same tdistribution techniques we applied in section 7.1."
1796,1,['statistics'], Paired data,seg_65,figure 7.10: summary statistics for the 68 price differences.
1797,1,"['hypothesis test', 'hypothesis', 'test', 'average']", Paired data,seg_65,"set up a hypothesis test to determine whether, on average, there is a difference between amazon’s price for a book and the ucla bookstore’s price. also, check the conditions for whether we can move forward with the test using the t-distribution."
1798,1,['average'], Paired data,seg_65,we are considering two scenarios: there is no difference or there is some difference in average prices.
1799,1,['average'], Paired data,seg_65,h0: µdiff = 0. there is no difference in the average textbook price.
1800,1,['average'], Paired data,seg_65,ha: µdiff =6 0. there is a difference in average prices.
1801,1,"['outliers', 'sample', 'simple random sample', 'observations', 'random sample', 'normality', 'independence', 'random']", Paired data,seg_65,"next, we check the independence and normality conditions. the observations are based on a simple random sample, so independence is reasonable. while there are some outliers, n = 68 and none of the outliers are particularly extreme, so the normality of x̄ is satisfied. with these conditions satisfied, we can move forward with the t-distribution."
1802,1,"['hypothesis test', 'test', 'hypothesis']", Paired data,seg_65,complete the hypothesis test started in example 7.17.
1803,1,"['deviation', 'standard', 'standard error', 'associated', 'standard deviation', 'test', 'error']", Paired data,seg_65,to compute the test compute the standard error associated with x̄diff using the standard deviation of the differences (sdiff = 13.42) and the number of differences (ndiff = 68):
1804,1,"['condition', 'test statistic', 'statistic', 'mean', 'test']", Paired data,seg_65,the test statistic is the t-score of x̄diff under the null condition that the actual mean difference is 0:
1805,1,"['sampling', 'distribution', 'tails', 'sampling distribution']", Paired data,seg_65,"to visualize the p-value, the sampling distribution of x̄diff is drawn as though h0 is true, and the p-value is represented by the two shaded tails:"
1806,1,"['degrees of freedom', 'statistical']", Paired data,seg_65,"the degrees of freedom is df = 68 − 1 = 67. using statistical software, we find the one-tail area of 0.0156. doubling this area gives the p-value: 0.0312."
1807,1,"['average', 'null hypothesis', 'hypothesis']", Paired data,seg_65,"because the p-value is less than 0.05, we reject the null hypothesis. amazon prices are, on average, lower than the ucla bookstore prices for ucla courses."
1808,1,"['interval', 'confidence', 'average', 'confidence interval']", Paired data,seg_65,create a 95% confidence interval for the average price difference between books at the ucla bookstore and books on amazon.10
1809,1,['average'], Paired data,seg_65,"we have strong evidence that amazon is, on average, less expensive. how should this conclusion affect ucla student buying habits? should ucla students always buy their books on amazon?11"
1810,0,[], Paired data,seg_65,differences in number of days
1811,1,['scores'], Paired data,seg_65,0 20 −20 −10 0 10 20 read write differences in scores (read − write) y
1812,1,"['sample', 'condition', 'estimate', 'standard error', 'data', 'point estimate', 'standard', 'population', 'error', 'paired']", Difference of two means,seg_67,"in this section we consider a difference in two population means, µ1 − µ2, under the condition that the data are not paired. just as with a single sample, we identify conditions to ensure we can use the t-distribution with a point estimate of the difference, x̄1− x̄2, and a new standard error formula. other than these two differences, the details are almost identical to the one-mean procedures."
1813,1,"['statistically significant', 'variation', 'function', 'average']", Difference of two means,seg_67,"we apply these methods in three contexts: determining whether stem cells can improve heart function, exploring the relationship between pregnant womens’ smoking habits and birth weights of newborns, and exploring whether there is statistically significant evidence that one variation of an exam is harder than another variation. this section is motivated by questions like “is there convincing evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who don’t smoke?”"
1814,1,"['confidence', 'interval', 'confidence interval']", Difference of two means,seg_67,7.3.1 confidence interval for a difference of means
1815,1,"['interval', 'statistics', 'control group', 'function', 'experiment', 'data', 'sets', 'histograms', 'confidence', 'data sets', 'confidence interval', 'control', 'test', 'treatment']", Difference of two means,seg_67,"does treatment using embryonic stem cells (escs) help improve heart function following a heart attack? figure 7.11 contains summary statistics for an experiment to test escs in sheep that had a heart attack. each of these sheep was randomly assigned to the esc or control group, and the change in their hearts’ pumping capacity was measured in the study. figure 7.12 provides histograms of the two data sets. a positive value corresponds to increased pumping capacity, which generally suggests a stronger recovery. our goal will be to identify a 95% confidence interval for the effect of escs on the change in heart pumping capacity relative to the control group."
1816,1,['control'], Difference of two means,seg_67,n x̄ s escs 9 3.50 5.17 control 9 -4.33 2.76
1817,1,['statistics'], Difference of two means,seg_67,figure 7.11: summary statistics of the embryonic stem cell study.
1818,1,"['sample', 'estimate', 'sample means', 'variable', 'point estimate']", Difference of two means,seg_67,the point estimate of the difference in the heart pumping variable is straightforward to find: it is the difference in the sample means.
1819,1,"['outliers', 'model', 'independent', 'confident', 'normality', 'cases', 'independence']", Difference of two means,seg_67,"for the question of whether we can model this difference using a t-distribution, we’ll need to check new conditions. like the 2-proportion cases, we will require a more robust version of independence so we are confident the two groups are also independent. secondly, we also check for normality in each group separately, which in practice is a check for outliers."
1820,1,['standardized'], Difference of two means,seg_67,the t-distribution can be used for inference when working with the standardized difference of two means if
1821,1,"['independent', 'random samples', 'experiment', 'data', 'independence', 'samples', 'randomized experiment', 'random']", Difference of two means,seg_67,"• independence, extended. the data are independent within and between the two groups, e.g. the data come from independent random samples or from a randomized experiment."
1822,1,"['outliers', 'normality']", Difference of two means,seg_67,• normality. we check the outliers rules of thumb for each group separately.
1823,1,"['standard error', 'error', 'standard']", Difference of two means,seg_67,the standard error may be computed as
1824,1,['degrees of freedom'], Difference of two means,seg_67,"the official formula for the degrees of freedom is quite complex and is generally computed using software, so instead you may use the smaller of n1 − 1 and n2 − 1 for the degrees of freedom if software isn’t readily available."
1825,1,"['point estimate', 'estimate']", Difference of two means,seg_67,"can the t-distribution be used to make inference using the point estimate, x̄esc − x̄control = 7.83?"
1826,1,['independence'], Difference of two means,seg_67,"first, we check for independence. because the sheep were randomized into the groups, independence within and between groups is satisfied."
1827,1,"['outliers', 'variability']", Difference of two means,seg_67,"figure 7.12 does not reveal any clear outliers in either group. (the esc group does look a bit more variability, but this is not the same as having clear outliers.)"
1828,1,"['sample', 'model', 'sample means']", Difference of two means,seg_67,"with both conditions met, we can use the t-distribution to model the difference of sample means."
1829,0,[], Difference of two means,seg_67,embryonic stem cell transplant
1830,1,['function'], Difference of two means,seg_67,change in heart pumping function
1831,1,['treatment'], Difference of two means,seg_67,control (no treatment)
1832,1,['function'], Difference of two means,seg_67,change in heart pumping function
1833,1,"['control group', 'control', 'histograms']", Difference of two means,seg_67,figure 7.12: histograms for both the embryonic stem cell and control group.
1834,1,"['sample', 'sample standard deviations', 'standard deviations', 'standard error', 'case', 'deviations', 'population', 'standard', 'error']", Difference of two means,seg_67,"as with the one-sample case, we always compute the standard error using sample standard deviations rather than population standard deviations:"
1835,1,"['degrees of freedom', 'case', 'tail areas', 'tail', 'statistical']", Difference of two means,seg_67,"generally, we use statistical software to find the appropriate degrees of freedom, or if software isn’t available, we can use the smaller of n1 − 1 and n2 − 1 for the degrees of freedom, e.g. if using a t-table to find tail areas. for transparency in the examples and guided practice, we’ll use the latter approach for finding df ; in the case of the esc example, this means we’ll use df = 8."
1836,1,"['confidence', 'interval', 'confidence interval']", Difference of two means,seg_67,calculate a 95% confidence interval for the effect of escs on the change in heart pumping capacity of sheep after they’ve suffered a heart attack.
1837,1,"['sample', 'standard error', 'standard', 'error']", Difference of two means,seg_67,we will use the sample difference and the standard error that we computed earlier calculations:
1838,1,"['confidence', 'interval', 'critical value', 'confidence interval']", Difference of two means,seg_67,"using df = 8, we can identify the critical value of t?8 = 2.31 for a 95% confidence interval. finally, we can enter the values into the confidence interval formula:"
1839,1,['estimate'], Difference of two means,seg_67,"point estimate ± t? × se → 7.83 ± 2.31× 1.95 → (3.32, 12.34)"
1840,1,"['function', 'confident']", Difference of two means,seg_67,we are 95% confident that embryonic stem cells improve the heart’s pumping function in sheep that have suffered a heart attack by 3.32% to 12.34%.
1841,1,"['statistical', 'statistical inference']", Difference of two means,seg_67,"as with past statistical inference applications, there is a well-trodden procedure."
1842,1,"['set', 'hypotheses', 'information']", Difference of two means,seg_67,"prepare. retrieve critical contextual information, and if appropriate, set up hypotheses."
1843,0,[], Difference of two means,seg_67,check. ensure the required conditions are reasonably satisfied.
1844,1,"['interval', 'standard error', 'standard', 'confidence', 'error', 'confidence interval']", Difference of two means,seg_67,"calculate. find the standard error, and then construct a confidence interval, or if conducting a"
1845,1,"['test statistic', 'statistic', 'test']", Difference of two means,seg_67,"hypothesis test, find a test statistic and p-value."
1846,1,['results'], Difference of two means,seg_67,conclude. interpret the results in the context of the application.
1847,0,[], Difference of two means,seg_67,"the details change a little from one setting to the next, but this general approach remain the same."
1848,1,"['hypothesis tests', 'tests', 'hypothesis']", Difference of two means,seg_67,7.3.2 hypothesis tests for the difference of two means
1849,1,"['sample', 'variables', 'random sample', 'data', 'cases', 'variable', 'set', 'data set', 'random', 'average']", Difference of two means,seg_67,"a data set called ncbirths represents a random sample of 150 cases of mothers and their newborns in north carolina over a year. four cases from this data set are represented in figure 7.13. we are particularly interested in two variables: weight and smoke. the weight variable represents the weights of the newborns and the smoke variable describes which mothers smoked during pregnancy. we would like to know, is there convincing evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who don’t smoke? we will use the north carolina sample to try to answer this question. the smoking group includes 50 cases and the nonsmoking group contains 100 cases."
1850,1,"['data', 'cases', 'variable', 'set', 'data set']", Difference of two means,seg_67,"figure 7.13: four cases from the ncbirths data set. the value “na”, shown for the first two entries of the first variable, indicates that piece of data is missing."
1851,1,"['average', 'hypotheses']", Difference of two means,seg_67,set up appropriate hypotheses to evaluate whether there is a relationship between a mother smoking and average birth weight.
1852,1,"['null hypothesis', 'case', 'hypothesis']", Difference of two means,seg_67,the null hypothesis represents the case of no difference between the groups.
1853,1,['average'], Difference of two means,seg_67,h0: there is no difference in average birth weight for newborns from mothers who did and did not
1854,1,['statistical'], Difference of two means,seg_67,"smoke. in statistical notation: µn−µs = 0, where µn represents non-smoking mothers and µs represents mothers who smoked."
1855,1,['average'], Difference of two means,seg_67,ha: there is some difference in average newborn weights from mothers who did and did not smoke
1856,1,"['sample', 'model', 'sample means']", Difference of two means,seg_67,we check the two conditions necessary to model the difference in sample means using the t-distribution.
1857,1,"['sample', 'random', 'independent', 'simple random sample', 'observations', 'data', 'samples', 'random sample']", Difference of two means,seg_67,"• because the data come from a simple random sample, the observations are independent, both within and between samples."
1858,1,"['outliers', 'extreme outliers', 'observations', 'data', 'sets', 'data sets']", Difference of two means,seg_67,"• with both data sets over 30 observations, we inspect the data in figure 7.14 for any particularly extreme outliers and find none."
1859,1,"['sample', 'sample means']", Difference of two means,seg_67,"since both conditions are satisfied, the difference in sample means may be modeled using a tdistribution."
1860,0,[], Difference of two means,seg_67,mothers who smoked mothers who did not smoke
1861,0,[], Difference of two means,seg_67,ycneuqerf 0 2 4 6 8 10 0 2 4 6 8 10 newborn weights (lbs) newborn weights (lbs)
1862,0,[], Difference of two means,seg_67,figure 7.14: the left panel represents birth weights for infants whose mothers smoked. the right panel represents the birth weights for infants whose mothers who did not smoke.
1863,1,['statistics'], Difference of two means,seg_67,the summary statistics in figure 7.15 may be useful for this guided practice.13
1864,1,"['population', 'point estimate', 'estimate']", Difference of two means,seg_67,"(a) what is the point estimate of the population difference, µn − µs?"
1865,1,"['estimate', 'standard error', 'point estimate', 'standard', 'error']", Difference of two means,seg_67,(b) compute the standard error of the point estimate from part (a).
1866,1,['mean'], Difference of two means,seg_67,smoker nonsmoker mean 6.78 7.18 st. dev. 1.43 1.60 samp. size 50 100
1867,1,"['statistics', 'data', 'set', 'data set']", Difference of two means,seg_67,figure 7.15: summary statistics for the ncbirths data set.
1868,1,"['sample', 'hypothesis test', 'level', 'significance', 'test', 'significance level', 'hypothesis']", Difference of two means,seg_67,"complete the hypothesis test started in example 7.23 and guided practice 7.24. use a significance level of α = 0.05. for reference, x̄n − x̄s = 0.40, se = 0.26, and the sample sizes were nn = 100 and ns = 50."
1869,1,"['test statistic', 'statistic', 'test']", Difference of two means,seg_67,we can find the test statistic for this test using the values from guided practice 7.24:
1870,1,"['plot', 'tails']", Difference of two means,seg_67,the p-value is represented by the two shaded tails in the following plot:
1871,1,"['degrees of freedom', 'tail']", Difference of two means,seg_67,"we find the single tail area using software (or the t-table in appendix c.2). we’ll use the smaller of nn−1 = 99 and ns−1 = 49 as the degrees of freedom: df = 49. the one tail area is 0.065; doubling this value gives the two-tail area and p-value, 0.135."
1872,1,"['hypothesis', 'significance', 'average', 'null hypothesis']", Difference of two means,seg_67,"the p-value is larger than the significance value, 0.05, so we do not reject the null hypothesis. there is insufficient evidence to say there is a difference in average birth weight of newborns from north carolina mothers who did smoke during pregnancy and newborns from north carolina mothers who did not smoke during pregnancy."
1873,0,[], Difference of two means,seg_67,"we’ve seen much research suggesting smoking is harmful during pregnancy, so how could we fail to"
1874,1,"['null hypothesis', 'hypothesis']", Difference of two means,seg_67,14 reject the null hypothesis in example 7.25?
1875,1,"['type 2 error', 'error', 'data collection', 'data']", Difference of two means,seg_67,"if we made a type 2 error and there is a difference, what could we have done differently in data collection to be more likely to detect the difference?15"
1876,1,"['data', 'sets', 'set', 'data set', 'data sets']", Difference of two means,seg_67,"public service announcement: while we have used this relatively small data set as an example, larger data sets show that women who smoke tend to have smaller newborns. in fact, some in the tobacco industry actually had the audacity to tout that as a benefit of smoking:"
1877,0,[], Difference of two means,seg_67,"it’s true. the babies born from women who smoke are smaller, but they’re just as healthy as the babies born from women who do not smoke. and some women would prefer having smaller babies."
1878,0,[], Difference of two means,seg_67,"- joseph cullman, philip morris’ chairman of the board ...on cbs’ face the nation, jan 3, 1971"
1879,0,[], Difference of two means,seg_67,fact check: the babies from women who smoke are not actually as healthy as the babies from women
1880,1,['case'], Difference of two means,seg_67,7.3.3 case study: two versions of a course exam
1881,1,"['random', 'statistics', 'variations', 'average']", Difference of two means,seg_67,"an instructor decided to run two slight variations of the same exam. prior to passing out the exams, she shuffled the exams together to ensure each student received a random version. summary statistics for how students performed on these two exams are shown in figure 7.16. anticipating complaints from students who took version b, she would like to evaluate whether the difference observed in the groups is so large that it provides convincing evidence that version b was more difficult (on average) than version a."
1882,1,"['statistics', 'scores']", Difference of two means,seg_67,figure 7.16: summary statistics of scores for each exam version.
1883,1,"['sample', 'sample means', 'hypotheses']", Difference of two means,seg_67,"construct hypotheses to evaluate whether the observed difference in sample means, x̄a − x̄b = 5.3, is due to chance. we will later evaluate these hypotheses using α = 0.01.17"
1884,1,['hypotheses'], Difference of two means,seg_67,"to evaluate the hypotheses in guided practice 7.28 using the t-distribution, we must first verify conditions.18"
1885,1,"['independent', 'scores']", Difference of two means,seg_67,(a) does it seem reasonable that the scores are independent?
1886,1,['outliers'], Difference of two means,seg_67,(b) any concerns about outliers?
1887,1,"['sample', 'independent', 'estimate', 'standard error', 'case', 'data', 'samples', 'scores', 'point estimate', 'standard', 'test', 'average', 'error']", Difference of two means,seg_67,"after verifying the conditions for each sample and confirming the samples are independent of each other, we are ready to conduct the test using the t-distribution. in this case, we are estimating the true difference in average test scores using the sample data, so the point estimate is x̄a−x̄b = 5.3. the standard error of the estimate can be calculated as"
1888,1,"['test statistic', 'statistic', 'test']", Difference of two means,seg_67,"finally, we construct the test statistic:"
1889,1,['null value'], Difference of two means,seg_67,point estimate− null value (79.4− 74.1)− 0 t = = = 1.15 se 4.62
1890,1,['degrees of freedom'], Difference of two means,seg_67,"if we have a computer handy, we can identify the degrees of freedom as 45.97. otherwise we use the smaller of n1 − 1 and n2 − 1: df = 26."
1891,1,['degrees of freedom'], Difference of two means,seg_67,figure 7.17: the t-distribution with 26 degrees of freedom and the p-value from exam example represented as the shaded areas.
1892,1,['case'], Difference of two means,seg_67,"identify the p-value depicted in figure 7.17 using df = 26, and provide a conclusion in the context of the case study."
1893,0,[], Difference of two means,seg_67,"using software, we can find the one-tail area (0.13) and then double this value to get the two-tail area, which is the p-value: 0.26. (alternatively, we could use the t-table in appendix c.2.)"
1894,1,"['data', 'scores', 'hypothesis', 'null hypothesis']", Difference of two means,seg_67,"in guided practice 7.28, we specified that we would use α = 0.01. since the p-value is larger than α, we do not reject the null hypothesis. that is, the data do not convincingly show that one exam version is more difficult than the other, and the teacher should not be convinced that she should add points to the version b exam scores."
1895,1,"['deviation', 'estimate', 'standard', 'pooled standard deviation', 'standard deviation']", Difference of two means,seg_67,7.3.4 pooled standard deviation estimate (special topic)
1896,1,"['deviation', 'standard deviations', 'populations', 'data', 'cases', 'deviations', 'standard', 'pooled standard deviation', 'standard deviation', 'historical data']", Difference of two means,seg_67,"occasionally, two populations will have standard deviations that are so similar that they can be treated as identical. for example, historical data or a well-understood biological mechanism may justify this strong assumption. in such cases, we can make the t-distribution approach slightly more precise by using a pooled standard deviation."
1897,1,"['deviation', 'standard deviations', 'estimate', 'standard error', 'data', 'deviations', 'samples', 'population', 'standard', 'pooled standard deviation', 'standard deviation', 'error', 'variances']", Difference of two means,seg_67,"the pooled standard deviation of two groups is a way to use data from both samples to better estimate the standard deviation and standard error. if s1 and s2 are the standard deviations of groups 1 and 2 and there are very good reasons to believe that the population standard deviations are equal, then we can obtain an improved estimate of the group variances by pooling their data:"
1898,1,"['sample', 'statistic']", Difference of two means,seg_67,"where n1 and n2 are the sample sizes, as before. to use this new statistic, we substitute s2pooled in"
1899,1,"['degrees of freedom', 'standard error', 'standard', 'error']", Difference of two means,seg_67,"place of s21 and s22 in the standard error formula, and we use an updated formula for the degrees of freedom:"
1900,1,"['deviation', 'degrees of freedom', 'model', 'standard deviations', 'estimate', 'sampling', 'distribution', 'deviations', 'parameter', 'standard', 'standard deviation', 'sampling distribution']", Difference of two means,seg_67,"the benefits of pooling the standard deviation are realized through obtaining a better estimate of the standard deviation for each group and using a larger degrees of freedom parameter for the t-distribution. both of these changes may permit a more accurate model of the sampling distribution of x̄1 − x̄2, if the standard deviations of the two groups are indeed equal."
1901,1,"['sample size', 'sample', 'deviation', 'condition', 'standard deviations', 'data', 'deviations', 'standard', 'pooled standard deviation', 'standard deviation']", Difference of two means,seg_67,"a pooled standard deviation is only appropriate when background research indicates the population standard deviations are nearly equal. when the sample size is large and the condition may be adequately checked with data, the benefits of pooling the standard deviations greatly diminishes."
1902,0,[], Difference of two means,seg_67,friday the 6th friday the 13th difference
1903,0,[], Difference of two means,seg_67,friday the 6th friday the 13th difference
1904,0,[], Difference of two means,seg_67,150 100 casein horsebean linseed meatmeal soybean sunflower
1905,0,[], Difference of two means,seg_67,automatic manual city mpg
1906,0,[], Difference of two means,seg_67,automatic manual hwy mpg
1907,1,['treatment'], Difference of two means,seg_67,treatment 1 treatment 2 treatment 3
1908,1,['experiment'], Power calculations for a difference of means,seg_69,"often times in experiment planning, there are two competing considerations:"
1909,1,['data'], Power calculations for a difference of means,seg_69,• we want to collect enough data that we can detect important effects.
1910,1,"['experiments', 'risk', 'data']", Power calculations for a difference of means,seg_69,"• collecting data can be expensive, and in experiments involving people, there may be some risk to patients."
1911,1,"['sample size', 'sample', 'experiment', 'trial']", Power calculations for a difference of means,seg_69,"in this section, we focus on the context of a clinical trial, which is a health-related experiment where the subject are people, and we will determine an appropriate sample size where we can be 80% sure"
1912,0,[], Power calculations for a difference of means,seg_69,25 that we would detect any practically important effects.
1913,1,['test'], Power calculations for a difference of means,seg_69,7.4.1 going through the motions of a test
1914,1,"['sample size', 'sample', 'hypothesis test', 'test', 'hypothesis']", Power calculations for a difference of means,seg_69,we’re going to go through the motions of a hypothesis test. this will help us frame our calculations for determining an appropriate sample size for the study.
1915,1,"['experiment', 'hypothesis test', 'control group', 'hypotheses', 'standard', 'control', 'test', 'trial', 'hypothesis']", Power calculations for a difference of means,seg_69,"suppose a pharmaceutical company has developed a new drug for lowering blood pressure, and they are preparing a clinical trial (experiment) to test the drug’s effectiveness. they recruit people who are taking a particular standard blood pressure medication. people in the control group will continue to take their current medication through generic-looking pills to ensure blinding. write down the hypotheses for a two-sided hypothesis test in this context."
1916,1,"['alternative hypothesis', 'trials', 'hypotheses', 'hypothesis']", Power calculations for a difference of means,seg_69,"generally, clinical trials use a two-sided alternative hypothesis, so below are suitable hypotheses for this context:"
1917,1,['standard'], Power calculations for a difference of means,seg_69,h0: the new drug performs exactly as well as the standard medication.
1918,1,['standard'], Power calculations for a difference of means,seg_69,ha: the new drug’s performance differs from the standard medication.
1919,1,"['deviation', 'standard error', 'distribution', 'standard', 'standard deviation', 'trial', 'error']", Power calculations for a difference of means,seg_69,"the researchers would like to run the clinical trial on patients with systolic blood pressures between 140 and 180 mmhg. suppose previously published studies suggest that the standard deviation of the patients’ blood pressures will be about 12 mmhg and the distribution of patient blood pressures will be approximately symmetric.26 if we had 100 patients per group, what would be the approximate standard error for x̄trmt − x̄ctrl?"
1920,1,"['standard error', 'error', 'standard']", Power calculations for a difference of means,seg_69,the standard error is calculated as follows:
1921,1,"['deviation', 'estimate', 'standard', 'standard deviation']", Power calculations for a difference of means,seg_69,"this may be an imperfect estimate of sex̄trmt−x̄ctrl , since the standard deviation estimate we used may not be perfectly correct for this group of patients. however, it is sufficient for our purposes."
1922,1,"['distribution', 'null distribution']", Power calculations for a difference of means,seg_69,what does the null distribution of x̄trmt − x̄ctrl look like?
1923,1,"['deviation', 'degrees of freedom', 'error', 'standard error', 'distribution', 'normal', 'mean', 'standard', 'standard deviation', 'null hypothesis', 'hypothesis']", Power calculations for a difference of means,seg_69,"the degrees of freedom are greater than 30, so the distribution of x̄trmt− x̄ctrl will be approximately normal. the standard deviation of this distribution (the standard error) would be about 1.70, and under the null hypothesis, its mean would be 0."
1924,1,['distribution'], Power calculations for a difference of means,seg_69,null distribution −9 −6 −3 0 3 6 9 xtrmt − xctrl
1925,1,"['null hypothesis', 'hypothesis']", Power calculations for a difference of means,seg_69,for what values of x̄trmt − x̄ctrl would we reject the null hypothesis?
1926,1,['tail'], Power calculations for a difference of means,seg_69,"for α = 0.05, we would reject h0 if the difference is in the lower 2.5% or upper 2.5% tail:"
1927,1,"['model', 'errors', 'normal', 'standard', 'standard errors']", Power calculations for a difference of means,seg_69,"lower 2.5%: for the normal model, this is 1.96 standard errors below 0, so any difference smaller"
1928,1,"['model', 'errors', 'normal', 'standard', 'standard errors']", Power calculations for a difference of means,seg_69,"upper 2.5%: for the normal model, this is 1.96 standard errors above 0, so any difference larger"
1929,1,['rejection regions'], Power calculations for a difference of means,seg_69,the boundaries of these rejection regions are shown below:
1930,1,['distribution'], Power calculations for a difference of means,seg_69,null distribution do not reject h0 reject h0 reject h0 −9 −6 −3 0 3 6 9 xtrmt − xctrl
1931,1,"['alternative hypothesis', 'null hypothesis', 'probability', 'hypothesis']", Power calculations for a difference of means,seg_69,"next, we’ll perform some hypothetical calculations to determine the probability we reject the null hypothesis, if the alternative hypothesis were actually true."
1932,1,['test'], Power calculations for a difference of means,seg_69,7.4.2 computing the power for a 2-sample test
1933,1,"['sample', 'effect sizes', 'probability']", Power calculations for a difference of means,seg_69,"when planning a study, we want to know how likely we are to detect an effect we care about. in other words, if there is a real effect, and that effect is large enough that it has practical value, then what’s the probability that we detect that effect? this probability is called the power, and we can compute it for different sample sizes or for different effect sizes."
1934,1,"['effect size', 'standard']", Power calculations for a difference of means,seg_69,"we first determine what is a practically significant result. suppose that the company researchers care about finding any effect on blood pressure that is 3 mmhg or larger vs the standard medication. here, 3 mmhg is the minimum effect size of interest, and we want to know how likely we are to detect this size of an effect in the study."
1935,1,"['treatment group', 'treatment', 'probability', 'standard']", Power calculations for a difference of means,seg_69,suppose we decided to move forward with 100 patients per treatment group and the new drug reduces blood pressure by an additional 3 mmhg relative to the standard medication. what is the probability that we detect a drop?
1936,0,[], Power calculations for a difference of means,seg_69,"before we even do any calculations, notice that if x̄trmt− x̄ctrl = −3 mmhg, there wouldn’t even be sufficient evidence to reject h0. that’s not a good sign."
1937,1,['probability'], Power calculations for a difference of means,seg_69,"to calculate the probability that we will reject h0, we need to determine a few things:"
1938,1,"['sampling', 'distribution', 'null distribution', 'sampling distribution']", Power calculations for a difference of means,seg_69,"• the sampling distribution for x̄trmt − x̄ctrl when the true difference is -3 mmhg. this is the same as the null distribution, except it is shifted to the left by 3:"
1939,1,"['distribution', 'null distribution']", Power calculations for a difference of means,seg_69,distribution with null distribution µtrmt − µctrl = −3 −9 −6 −3 0 3 6 9 xtrmt − xctrl
1940,1,['rejection regions'], Power calculations for a difference of means,seg_69,"• the rejection regions, which are outside of the dotted lines above."
1941,1,"['distribution', 'rejection region']", Power calculations for a difference of means,seg_69,• the fraction of the distribution that falls in the rejection region.
1942,1,"['deviation', 'distribution', 'normal', 'mean', 'probability', 'standard', 'standard deviation', 'normal distribution']", Power calculations for a difference of means,seg_69,"in short, we need to calculate the probability that x < −3.332 for a normal distribution with mean -3 and standard deviation 1.7. to do so, we first shade the area we want to calculate:"
1943,1,"['distribution', 'null distribution']", Power calculations for a difference of means,seg_69,distribution with null distribution µtrmt − µctrl = −3 −9 −6 −3 0 3 6 9 xtrmt − xctrl
1944,1,"['degrees of freedom', 'normal approximation', 'table', 'approximation', 'probability table', 'normal', 'tail', 'probability', 'statistical']", Power calculations for a difference of means,seg_69,"we’ll use a normal approximation, which is good approximation when the degrees of freedom is about 30 or more. we’ll start by calculating the z-score and find the tail area using either statistical software or the probability table:"
1945,1,"['sample size', 'sample', 'test']", Power calculations for a difference of means,seg_69,the power for the test is about 42% when µtrmt − µctrl = −3 and each group has a sample size of 100.
1946,1,"['rejection region', 'null hypothesis', 'hypothesis']", Power calculations for a difference of means,seg_69,"in example 7.35, we ignored the upper rejection region in the calculation, which was in the opposite direction of the hypothetical truth, i.e. -3. the reasoning? there wouldn’t be any value in rejecting the null hypothesis and concluding there was an increase when in fact there was a decrease."
1947,1,"['sample size', 'sample', 'distribution', 'normal', 'normal distribution']", Power calculations for a difference of means,seg_69,"we’ve also used a normal distribution instead of the t-distribution. this is a convenience, and if the sample size is too small, we’d need to revert back to using the t-distribution. we’ll discuss this a bit further at the end of this section."
1948,1,"['sample size', 'sample']", Power calculations for a difference of means,seg_69,7.4.3 determining a proper sample size
1949,1,"['sample size', 'sample', 'alternative hypothesis', 'effect size', 'data', 'probability', 'hypothesis']", Power calculations for a difference of means,seg_69,"in the last example, we found that if we have a sample size of 100 in each group, we can only detect an effect size of 3 mmhg with a probability of about 0.42. suppose the researchers moved forward and only used 100 patients per group, and the data did not support the alternative hypothesis, i.e. the researchers did not reject h0. this is a very bad situation to be in for a few reasons:"
1950,1,['sample'], Power calculations for a difference of means,seg_69,"• in the back of the researchers’ minds, they’d all be wondering, maybe there is a real and meaningful difference, but we weren’t able to detect it with such a small sample."
1951,1,"['uncertainty', 'experiment']", Power calculations for a difference of means,seg_69,"• the company probably invested hundreds of millions of dollars in developing the new drug, so now they are left with great uncertainty about its potential since the experiment didn’t have a great shot at detecting effects that could still be important."
1952,0,[], Power calculations for a difference of means,seg_69,"• patients were subjected to the drug, and we can’t even say with much certainty that the drug doesn’t help (or harm) patients."
1953,1,['trial'], Power calculations for a difference of means,seg_69,"• another clinical trial may need to be run to get a more conclusive answer as to whether the drug does hold any practical value, and conducting a second clinical trial may take years and many millions of dollars."
1954,1,"['sample size', 'sample', 'confident']", Power calculations for a difference of means,seg_69,"we want to avoid this situation, so we need to determine an appropriate sample size to ensure we can be pretty confident that we’ll detect any effects that are practically important. as mentioned earlier, a change of 3 mmhg was deemed to be the minimum difference that was practically important. as a first step, we could calculate power for several different sample sizes. for instance, let’s try 500 patients per group."
1955,1,"['sample size', 'sample']", Power calculations for a difference of means,seg_69,27 calculate the power to detect a change of -3 mmhg when using a sample size of 500 per group.
1956,1,"['deviation', 'standard error', 'standard', 'standard deviation', 'error']", Power calculations for a difference of means,seg_69,(a) determine the standard error (recall that the standard deviation for patients was expected to
1957,0,[], Power calculations for a difference of means,seg_69,be about 12 mmhg).
1958,1,"['distribution', 'rejection regions', 'null distribution']", Power calculations for a difference of means,seg_69,(b) identify the null distribution and rejection regions.
1959,1,['distribution'], Power calculations for a difference of means,seg_69,(c) identify the alternative distribution when µtrmt − µctrl = −3.
1960,1,"['null hypothesis', 'probability', 'hypothesis']", Power calculations for a difference of means,seg_69,(d) compute the probability we reject the null hypothesis.
1961,1,"['sample size', 'sample', 'trial']", Power calculations for a difference of means,seg_69,"the researchers decided 3 mmhg was the minimum difference that was practically important, and with a sample size of 500, we can be very certain (97.7% or better) that we will detect any such difference. we now have moved to another extreme where we are exposing an unnecessary number of patients to the new drug in the clinical trial. not only is this ethically questionable, but it would also cost a lot more money than is necessary to be quite sure we’d detect any important effects."
1962,1,"['sample size', 'sample', 'treatment']", Power calculations for a difference of means,seg_69,"the most common practice is to identify the sample size where the power is around 80%, and sometimes 90%. other values may be reasonable for a specific context, but 80% and 90% are most commonly targeted as a good balance between high power and not exposing too many patients to a new treatment (or wasting too much money)."
1963,1,"['sample', 'power of the test', 'test']", Power calculations for a difference of means,seg_69,"we could compute the power of the test at several other possible sample sizes until we find one that’s close to 80%, but there’s a better way. we should solve the problem backwards."
1964,1,"['distribution', 'null distribution']", Power calculations for a difference of means,seg_69,distribution with null distribution µtrmt − µctrl = −3 −9 −6 −3 0 3 6 9 xtrmt − xctrl
1965,1,"['sample size', 'sample']", Power calculations for a difference of means,seg_69,what sample size will lead to a power of 80%?
1966,1,"['sample', 'degrees of freedom', 'test statistic', 'approximation', 'distribution', 'statistic', 'normal', 'test', 'normal distribution']", Power calculations for a difference of means,seg_69,"we’ll assume we have a large enough sample that the normal distribution is a good approximation for the test statistic, since the normal distribution and the t-distribution look almost identical when the degrees of freedom are moderately large (e.g. df ≥ 30). if that doesn’t turn out to be true, then we’d need to make a correction."
1967,1,"['sample size', 'sample', 'tail']", Power calculations for a difference of means,seg_69,"we start by identifying the z-score that would give us a lower tail of 80%. for a moderately large sample size per group, the z-score for a lower tail of 80% would be about z = 0.84."
1968,1,"['distribution', 'null distribution']", Power calculations for a difference of means,seg_69,distribution with null distribution µtrmt − µctrl = −3
1969,1,"['standard error', 'distribution', 'rejection region', 'standard', 'null distribution', 'error', 'distributions']", Power calculations for a difference of means,seg_69,"additionally, the rejection region extends 1.96 × se from the center of the null distribution for α = 0.05. this allows us to calculate the target distance between the center of the null and alternative distributions in terms of the standard error:"
1970,1,"['effect size', 'standard error', 'set', 'standard', 'error']", Power calculations for a difference of means,seg_69,"in our example, we want the distance between the null and alternative distributions’ centers to equal the minimum effect size of interest, 3 mmhg, which allows us to set up an equation between this difference and the standard error:"
1971,1,"['level', 'significance', 'significance level']", Power calculations for a difference of means,seg_69,we should target 251 patients per group in order to achieve 80% power at the 0.05 significance level for this context.
1972,1,"['standard error', 'standard', 'level', 'significance', 'significance level', 'error']", Power calculations for a difference of means,seg_69,"the standard error difference of 2.8 × se is specific to a context where the targeted power is 80% and the significance level is α = 0.05. if the targeted power is 90% or if we use a different significance level, then we’ll use something a little different than 2.8× se."
1973,1,"['sample size', 'sample', 'degrees of freedom']", Power calculations for a difference of means,seg_69,"had the suggested sample size been relatively small – roughly 30 or smaller – it would have been a good idea to rework the calculations using the degrees of fredom for the smaller sample size under that initial sample size. that is, we would have revised the 0.84 and 1.96 values based on degrees of freedom implied by the initial sample size. the revised sample size target would generally have then been a little larger."
1974,1,"['effect size', 'errors', 'distribution', 'standard', 'standard errors']", Power calculations for a difference of means,seg_69,"suppose the targeted power was 90% and we were using α = 0.01. how many standard errors should separate the centers of the null and alternative distribution, where the alternative distribution is centered at the minimum effect size of interest?28"
1975,0,[], Power calculations for a difference of means,seg_69,what are some considerations that are important in determining what the power should be for an experiment?29
1976,1,"['sample', 'curve']", Power calculations for a difference of means,seg_69,"figure 7.18 shows the power for sample sizes from 20 patients to 5,000 patients when α = 0.05 and the true difference is -3. this curve was constructed by writing a program to compute the power for many different sample sizes."
1977,1,"['sample size', 'sample']", Power calculations for a difference of means,seg_69,1.0 0.8 r 0.6 ewop 0.4 0.2 0.0 20 50 100 200 500 1000 2000 5000 sample size per group
1978,1,"['sample', 'curve', 'observations']", Power calculations for a difference of means,seg_69,figure 7.18: the curve shows the power for different sample sizes in the context of the blood pressure example when the true difference is -3. having more than about 250 to 350 observations doesn’t provide much additional value in detecting an effect when α = 0.05.
1979,1,"['sample size', 'sample', 'experiment', 'estimate', 'experiments']", Power calculations for a difference of means,seg_69,"power calculations for expensive or risky experiments are critical. however, what about experiments that are inexpensive and where the ethical considerations are minimal? for example, if we are doing final testing on a new feature on a popular website, how would our sample size considerations change? as before, we’d want to make sure the sample is big enough. however, suppose the feature has undergone some testing and is known to perform well (e.g. the website’s users seem to enjoy the feature). then it may be reasonable to run a larger experiment if there’s value from having a more precise estimate of the feature’s effect, such as helping guide the development of the next useful feature."
1980,1,"['populations', 'pairwise comparisons', 'mean', 'test', 'anova']", Comparing many means with ANOVA,seg_71,"sometimes we want to compare means across many groups. we might initially think to do pairwise comparisons. for example, if there were three groups, we might be tempted to compare the first mean with the second, then with the third, and then finally compare the second and third means for a total of three comparisons. however, this strategy can be treacherous. if we have many groups and do many comparisons, it is likely that we will eventually find a difference just by chance, even if there is no difference in the populations. instead, we should apply a holistic test to check whether there is evidence that at least one pair groups are in fact different, and this is where anova saves the day."
1981,1,['anova'], Comparing many means with ANOVA,seg_71,7.5.1 core ideas of anova
1982,1,"['method', 'test statistic', 'anova', 'hypothesis test', 'statistic', 'analysis of variance', 'variance', 'test', 'hypothesis']", Comparing many means with ANOVA,seg_71,"in this section, we will learn a new method called analysis of variance (anova) and a new test statistic called f . anova uses a single hypothesis test to check whether the means across many groups are equal:"
1983,1,"['mean', 'outcome', 'statistical']", Comparing many means with ANOVA,seg_71,"h0: the mean outcome is the same across all groups. in statistical notation, µ1 = µ2 = · · · = µk"
1984,1,"['observations', 'mean', 'outcome']", Comparing many means with ANOVA,seg_71,where µi represents the mean of the outcome for observations in category i.
1985,1,['mean'], Comparing many means with ANOVA,seg_71,ha: at least one mean is different.
1986,1,"['data', 'anova']", Comparing many means with ANOVA,seg_71,generally we must check three conditions on the data before performing anova:
1987,1,"['observations', 'independent']", Comparing many means with ANOVA,seg_71,"• the observations are independent within and across groups,"
1988,1,"['normal', 'data']", Comparing many means with ANOVA,seg_71,"• the data within each group are nearly normal, and"
1989,1,['variability'], Comparing many means with ANOVA,seg_71,• the variability across the groups is about equal.
1990,1,"['data', 'hypothesis', 'null hypothesis', 'anova']", Comparing many means with ANOVA,seg_71,"when these three conditions are met, we may perform an anova to determine whether the data provide strong evidence against the null hypothesis that all the µi are equal."
1991,1,"['statistics', 'statistically significant', 'hypotheses', 'scores']", Comparing many means with ANOVA,seg_71,"college departments commonly run multiple lectures of the same introductory course each semester because of high demand. consider a statistics department that runs three lectures of an introductory statistics course. we might like to determine whether there are statistically significant differences in first exam scores in these three classes (a, b, and c). describe appropriate hypotheses to determine whether there are any differences between the three classes."
1992,1,['hypotheses'], Comparing many means with ANOVA,seg_71,the hypotheses may be written in the following form:
1993,1,['average'], Comparing many means with ANOVA,seg_71,h0: the average score is identical in all lectures. any observed difference is due to chance. nota-
1994,1,"['average', 'null hypothesis', 'hypothesis']", Comparing many means with ANOVA,seg_71,ha: the average score varies by class. we would reject the null hypothesis in favor of the alternative
1995,0,[], Comparing many means with ANOVA,seg_71,hypothesis if there were larger differences among the class averages than what we might expect from chance alone.
1996,1,"['alternative hypothesis', 'variability', 'anova', 'observations', 'success', 'hypothesis']", Comparing many means with ANOVA,seg_71,strong evidence favoring the alternative hypothesis in anova is described by unusually large differences among the group means. we will soon learn that assessing the variability of the group means relative to the variability among individual observations within each group is key to anova’s success.
1997,0,[], Comparing many means with ANOVA,seg_71,"examine figure 7.19. compare groups i, ii, and iii. can you visually determine if the differences in the group centers is due to chance or not? now compare groups iv, v, and vi. do these differences appear to be due to chance?"
1998,1,"['variability', 'observations', 'data', 'mean', 'outcome', 'average']", Comparing many means with ANOVA,seg_71,"any real difference in the means of groups i, ii, and iii is difficult to discern, because the data within each group are very volatile relative to any differences in the average outcome. on the other hand, it appears there are differences in the centers of groups iv, v, and vi. for instance, group v appears to have a higher mean than that of the other two groups. investigating groups iv, v, and vi, we see the differences in the groups’ centers are noticeable because those differences are large relative to the variability in the individual observations within each group."
1999,1,"['plot', 'outcomes', 'dot plot']", Comparing many means with ANOVA,seg_71,figure 7.19: side-by-side dot plot for the outcomes for six groups.
2000,0,[], Comparing many means with ANOVA,seg_71,7.5.2 is batting performance related to player position in mlb?
2001,1,"['percentage', 'data', 'cases', 'variable', 'set', 'data set', 'outcome']", Comparing many means with ANOVA,seg_71,"we would like to discern whether there are real differences between the batting performance of baseball players according to their position: outfielder (of), infielder (if), and catcher (c). we will use a data set called bat18, which includes batting records of 429 major league baseball (mlb) players from the 2018 season who had at least 100 at bats. six of the 429 cases represented in bat18 are shown in figure 7.20, and descriptions for each variable are provided in figure 7.21. the measure we will use for the player batting performance (the outcome variable) is on-base percentage (obp). the on-base percentage roughly represents the fraction of the time a player successfully gets on base or hits a home run."
2002,1,"['cases', 'data matrix', 'data']", Comparing many means with ANOVA,seg_71,figure 7.20: six cases from the bat18 data matrix.
2003,1,"['variables', 'data', 'set', 'data set']", Comparing many means with ANOVA,seg_71,figure 7.21: variables and their descriptions for the bat18 data set.
2004,1,"['null hypothesis', 'hypothesis']", Comparing many means with ANOVA,seg_71,the null hypothesis under consideration is the following: µof = µif = µc. write the null and
2005,1,"['hypotheses', 'alternative hypotheses']", Comparing many means with ANOVA,seg_71,30 corresponding alternative hypotheses in plain language.
2006,1,"['percentage', 'point estimate', 'estimate']", Comparing many means with ANOVA,seg_71,"the player positions have been divided into three groups: outfield (of), infield (if), and catcher (c). what would be an appropriate point estimate of the on-base percentage by outfielders, µof?"
2007,1,"['sample', 'percentage', 'estimate', 'sample average', 'average']", Comparing many means with ANOVA,seg_71,a good estimate of the on-base percentage by outfielders would be the sample average of obp for just those players whose position is outfield: x̄of = 0.320.
2008,1,"['plot', 'percentage', 'variability', 'anova', 'statistics', 'box plot', 'variance']", Comparing many means with ANOVA,seg_71,figure 7.22 provides summary statistics for each group. a side-by-side box plot for the onbase percentage is shown in figure 7.23. notice that the variability appears to be approximately constant across groups; nearly constant variance across groups is an important assumption that must be satisfied before we consider the anova approach.
2009,1,"['statistics', 'percentage']", Comparing many means with ANOVA,seg_71,"figure 7.22: summary statistics of on-base percentage, split by player position."
2010,1,"['outliers', 'plot', 'percentage', 'box plot']", Comparing many means with ANOVA,seg_71,"figure 7.23: side-by-side box plot of the on-base percentage for 429 players across three groups. with over a hundred players in both the infield and outfield groups, the apparent outliers are not a concern."
2011,1,"['sample', 'sample means', 'hypotheses']", Comparing many means with ANOVA,seg_71,the largest difference between the sample means is between the catcher and the outfielder positions. consider again the original hypotheses:
2012,1,"['average', 'percentage']", Comparing many means with ANOVA,seg_71,ha: the average on-base percentage (µi) varies across some (or all) groups.
2013,1,"['statistically significant', 'level', 'significance', 'test', 'significance level']", Comparing many means with ANOVA,seg_71,why might it be inappropriate to run the test by simply estimating whether the difference of µc and µof is statistically significant at a 0.05 significance level?
2014,1,"['rate', 'data', 'test', 'data snooping', 'error', 'type 1 error']", Comparing many means with ANOVA,seg_71,"the primary issue here is that we are inspecting the data before picking the groups that will be compared. it is inappropriate to examine all data by eye (informal testing) and only afterwards decide which parts to formally test. this is called data snooping or data fishing. naturally, we would pick the groups with the large differences for the formal test, and this would leading to an inflation in the type 1 error rate. to understand this better, let’s consider a slightly different problem."
2015,1,"['cases', 'test', 'random']", Comparing many means with ANOVA,seg_71,"suppose we are to measure the aptitude for students in 20 classes in a large elementary school at the beginning of the year. in this school, all students are randomly assigned to classrooms, so any differences we observe between the classes at the start of the year are completely due to chance. however, with so many groups, we will probably observe a few groups that look rather different from each other. if we select only these classes that look so different and then perform a formal test, we will probably make the wrong conclusion that the assignment wasn’t random. while we might only formally test differences for a few pairs of classes, we informally evaluated the other classes by eye before choosing the most extreme cases for a comparison."
2016,1,['information'], Comparing many means with ANOVA,seg_71,"for additional information on the ideas expressed in example 7.44, we recommend reading about the prosecutor’s fallacy.31"
2017,1,"['sample', 'sample means', 'statistic', 'population', 'test', 'anova']", Comparing many means with ANOVA,seg_71,in the next section we will learn how to use the f statistic and anova to test whether observed differences in sample means could have happened just by chance even if there was no difference in the respective population means.
2018,1,"['analysis of variance', 'variance', 'anova']", Comparing many means with ANOVA,seg_71,7.5.3 analysis of variance (anova) and the f -test
2019,1,"['sample', 'degrees of freedom', 'method', 'variability', 'sample means', 'mean square', 'associated', 'mean', 'analysis of variance', 'variation', 'variance', 'null hypothesis', 'hypothesis']", Comparing many means with ANOVA,seg_71,"the method of analysis of variance in this context focuses on answering one question: is the variability in the sample means so large that it seems unlikely to be from chance alone? this question is different from earlier testing procedures since we will simultaneously consider many groups, and evaluate whether their sample means differ more than we would expect from natural variation. we call this variability the mean square between groups (msg), and it has an associated degrees of freedom, dfg = k − 1 when there are k groups. the msg can be thought of as a scaled variance formula for means. if the null hypothesis is true, any variation in the sample means is due"
2020,0,[], Comparing many means with ANOVA,seg_71,"32 to chance and shouldn’t be too large. details of msg calculations are provided in the footnote. however, we typically use software for these computations."
2021,1,"['mse', 'degrees of freedom', 'pooled variance', 'associated', 'null hypothesis', 'sample', 'estimate', 'sample means', 'mean', 'error', 'test', 'variance', 'anova', 'hypothesis', 'variability', 'hypothesis test', 'mean square', 'mean square error']", Comparing many means with ANOVA,seg_71,"the mean square between the groups is, on its own, quite useless in a hypothesis test. we need a benchmark value for how much variability should be expected among the sample means if the null hypothesis is true. to this end, we compute a pooled variance estimate, often abbreviated as the mean square error (mse), which has an associated degrees of freedom value dfe = n − k. it is helpful to think of mse as a measure of the variability within the groups. details of the computations of the mse and a link to an extra online section for anova calculations are provided in the footnote33 for interested readers."
2022,1,"['mse', 'sample', 'test statistic', 'sample means', 'anova', 'statistic', 'test', 'null hypothesis', 'hypothesis']", Comparing many means with ANOVA,seg_71,"when the null hypothesis is true, any differences among the sample means are only due to chance, and the msg and mse should be about equal. as a test statistic for anova, we examine the fraction of msg and mse:"
2023,1,['mse'], Comparing many means with ANOVA,seg_71,msg f = mse
2024,1,"['mse', 'variability']", Comparing many means with ANOVA,seg_71,"the msg represents a measure of the between-group variability, and mse measures the variability within each of the groups."
2025,1,"['mse', 'degrees of freedom', 'data']", Comparing many means with ANOVA,seg_71,"for the baseball data, msg = 0.00803 and mse = 0.00158. identify the degrees of freedom"
2026,1,"['mse', 'statistic', 'associated']", Comparing many means with ANOVA,seg_71,34 associated with msg and mse and verify the f statistic is approximately 5.077.
2027,1,"['degrees of freedom', 'f distribution', 'parameters', 'anova', 'hypothesis test', 'distribution', 'statistic', 'hypotheses', 'associated', 'test', 'hypothesis']", Comparing many means with ANOVA,seg_71,"we can use the f statistic to evaluate the hypotheses in what is called an f -test. a p-value can be computed from the f statistic using an f distribution, which has two associated parameters: df1 and df2. for the f statistic in anova, df1 = dfg and df2 = dfe . an f distribution with 2 and 426 degrees of freedom, corresponding to the f statistic for the baseball hypothesis test, is shown in figure 7.24."
2028,1,['tail'], Comparing many means with ANOVA,seg_71,small tail area 0 2 4 6 8
2029,1,"['distribution', 'f distribution']", Comparing many means with ANOVA,seg_71,figure 7.24: an f distribution with df1 = 2 and df2 = 426.
2030,1,"['mse', 'sample', 'variability', 'sample means', 'observations', 'distribution', 'tail', 'null hypothesis', 'hypothesis']", Comparing many means with ANOVA,seg_71,"the larger the observed variability in the sample means (msg) relative to the within-group observations (mse), the larger f will be and the stronger the evidence against the null hypothesis. because larger values of f represent stronger evidence against the null hypothesis, we use the upper tail of the distribution to compute a p-value."
2031,1,"['statistic', 'sample', 'sample means', 'standardized', 'mean', 'model', 'parameters', 'test statistic', 'distribution', 'outcome', 'test', 'variance', 'anova', 'f distribution', 'variability', 'tail']", Comparing many means with ANOVA,seg_71,"analysis of variance (anova) is used to test whether the mean outcome differs across 2 or more groups. anova uses a test statistic f , which represents a standardized ratio of variability in the sample means relative to the variability within the groups. if h0 is true and the model conditions are satisfied, the statistic f follows an f distribution with parameters df1 = k − 1 and df2 = n− k. the upper tail of the f distribution is used to represent the p-value."
2032,1,"['null hypothesis', 'hypothesis']", Comparing many means with ANOVA,seg_71,the p-value corresponding to the shaded area in figure 7.24 is equal to about 0.0066. does this provide strong evidence against the null hypothesis?
2033,1,"['percentage', 'data', 'hypothesis', 'level', 'significance', 'average', 'significance level', 'null hypothesis']", Comparing many means with ANOVA,seg_71,"the p-value is smaller than 0.05, indicating the evidence is strong enough to reject the null hypothesis at a significance level of 0.05. that is, the data provide strong evidence that the average on-base percentage varies by player’s primary field position."
2034,1,"['anova table', 'table', 'anova']", Comparing many means with ANOVA,seg_71,7.5.4 reading an anova table from software
2035,1,"['statistic', 'statistical', 'error', 'anova']", Comparing many means with ANOVA,seg_71,"the calculations required to perform an anova by hand are tedious and prone to human error. for these reasons, it is common to use statistical software to calculate the f statistic and p-value."
2036,1,"['percentage', 'table', 'anova', 'regression', 'statistic', 'mean', 'test']", Comparing many means with ANOVA,seg_71,"an anova can be summarized in a table very similar to that of a regression summary, which we will see in chapters 8 and 9. figure 7.25 shows an anova summary to test whether the mean of on-base percentage varies by player positions in the mlb. many of these values should look familiar; in particular, the f -test statistic and p-value can be retrieved from the last two columns."
2037,1,"['residuals', 'mean']", Comparing many means with ANOVA,seg_71,df sum sq mean sq f value pr(>f) position 2 0.0161 0.0080 5.0766 0.0066 residuals 426 0.6740 0.0016 spooled = 0.040 on df = 423
2038,1,"['percentage', 'anova', 'average']", Comparing many means with ANOVA,seg_71,figure 7.25: anova summary for testing whether the average on-base percentage differs across player positions.
2039,1,"['graphical', 'anova']", Comparing many means with ANOVA,seg_71,7.5.5 graphical diagnostics for an anova analysis
2040,1,"['independent', 'observations', 'data', 'normal', 'variance', 'anova']", Comparing many means with ANOVA,seg_71,"there are three conditions we must check for an anova analysis: all observations must be independent, the data in each group must be nearly normal, and the variance within each group must be approximately equal."
2041,1,"['sample', 'random', 'condition', 'simple random sample', 'data', 'random sample', 'processes']", Comparing many means with ANOVA,seg_71,"independence. if the data are a simple random sample, this condition is satisfied. for processes"
2042,1,"['independent', 'observations', 'data', 'independence', 'experiments']", Comparing many means with ANOVA,seg_71,"and experiments, carefully consider whether the data may be independent (e.g. no pairing). for example, in the mlb data, the data were not sampled. however, there are not obvious reasons why independence would not hold for most or all observations."
2043,1,"['normality', 'normal']", Comparing many means with ANOVA,seg_71,"approximately normal. as with oneand two-sample testing for means, the normality assump-"
2044,1,"['sample size', 'sample', 'outliers', 'histogram', 'observations']", Comparing many means with ANOVA,seg_71,"tion is especially important when the sample size is quite small when it is ironically difficult to check for non-normality. a histogram of the observations from each group is shown in figure 7.26. since each of the groups we’re considering have relatively large sample sizes, what we’re looking for are major outliers. none are apparent, so this conditions is reasonably met."
2045,0,[], Comparing many means with ANOVA,seg_71,outfielders in−fielders catchers
2046,1,['histograms'], Comparing many means with ANOVA,seg_71,figure 7.26: histograms of obp for each field position.
2047,1,['variance'], Comparing many means with ANOVA,seg_71,constant variance. the last assumption is that the variance in the groups is about equal from
2048,1,"['plot', 'deviation', 'outcomes', 'standard', 'variability', 'table', 'case', 'box plot', 'vary', 'standard deviation']", Comparing many means with ANOVA,seg_71,"one group to the next. this assumption can be checked by examining a side-by-side box plot of the outcomes across the groups, as in figure 7.23 on page 287. in this case, the variability is similar in the four groups but not identical. we see in table 7.22 on page 287 that the standard deviation doesn’t vary much from one group to the next."
2049,1,"['sample', 'condition', 'normality', 'variance', 'anova']", Comparing many means with ANOVA,seg_71,independence is always important to an anova analysis. the normality condition is very important when the sample sizes for each group are relatively small. the constant variance condition is especially important when the sample sizes differ between groups.
2050,1,"['rate', 'multiple comparisons', 'error', 'type 1 error']", Comparing many means with ANOVA,seg_71,7.5.6 multiple comparisons and controlling type 1 error rate
2051,1,"['deviation', 'level', 'estimate', 'anova table', 'anova', 'table', 'standard', 'pooled standard deviation', 'standard deviation', 'significance', 'significance level', 'null hypothesis', 'hypothesis']", Comparing many means with ANOVA,seg_71,"when we reject the null hypothesis in an anova analysis, we might wonder, which of these groups have different means? to answer this question, we compare the means of each possible pair of groups. for instance, if there are three groups and there is strong evidence that there are some differences in the group means, there are three comparisons to make: group 1 to group 2, group 1 to group 3, and group 2 to group 3. these comparisons can be accomplished using a two-sample t-test, but we use a modified significance level and a pooled estimate of the standard deviation across groups. usually this pooled standard deviation can be found in the anova table, e.g. along the bottom of figure 7.25."
2052,1,"['plot', 'statistics', 'data', 'box plot', 'deviations', 'anova']", Comparing many means with ANOVA,seg_71,"example 7.40 on page 285 discussed three statistics lectures, all taught during the same semester. figure 7.27 shows summary statistics for these three courses, and a side-by-side box plot of the data is shown in figure 7.28. we would like to conduct an anova for these data. do you see any deviations from the three conditions for anova?"
2053,1,"['independence', 'case', 'data']", Comparing many means with ANOVA,seg_71,"in this case (like many others) it is difficult to check independence in a rigorous way. instead, the best we can do is use common sense to consider reasons the assumption of independence may not hold. for instance, the independence assumption may not be reasonable if there is a star teaching assistant that only half of the students may access; such a scenario would divide a class into two subgroups. no such situations were evident for these particular data, and we believe that independence is acceptable."
2054,1,"['outliers', 'plot', 'symmetric', 'box plot', 'distributions']", Comparing many means with ANOVA,seg_71,the distributions in the side-by-side box plot appear to be roughly symmetric and show no noticeable outliers.
2055,1,"['variability', 'plots', 'box plots', 'variance']", Comparing many means with ANOVA,seg_71,"the box plots show approximately equal variability, which can be verified in figure 7.27, supporting the constant variance assumption."
2056,1,"['statistics', 'scores']", Comparing many means with ANOVA,seg_71,figure 7.27: summary statistics for the first midterm scores in three different lectures of the same course.
2057,1,"['plot', 'box plot', 'scores']", Comparing many means with ANOVA,seg_71,figure 7.28: side-by-side box plot for the first midterm scores in three different lectures of the same course.
2058,1,"['results', 'data']", Comparing many means with ANOVA,seg_71,"anova was conducted for the midterm data, and summary results are shown in figure 7.29. what should we conclude?35"
2059,1,"['residuals', 'mean']", Comparing many means with ANOVA,seg_71,df sum sq mean sq f value pr(>f) lecture 2 1290.11 645.06 3.48 0.0330 residuals 161 29810.13 185.16 spooled = 13.61 on df = 161
2060,1,"['table', 'data', 'anova']", Comparing many means with ANOVA,seg_71,figure 7.29: anova summary table for the midterm data.
2061,1,"['tests', 'rate', 'level', 'significance', 'test', 'significance level', 'error', 'type 1 error']", Comparing many means with ANOVA,seg_71,"there is strong evidence that the different means in each of the three classes is not simply due to chance. we might wonder, which of the classes are actually different? as discussed in earlier chapters, a two-sample t-test could be used to test for differences in each possible pair of groups. however, one pitfall was discussed in example 7.44 on page 288: when we run so many tests, the type 1 error rate increases. this issue is resolved by using a modified significance level."
2062,1,"['tests', 'bonferroni correction', 'multiple comparisons', 'level', 'significance', 'significance level']", Comparing many means with ANOVA,seg_71,the scenario of testing many pairs of groups is called multiple comparisons. the bonferroni correction suggests that a more stringent significance level is more appropriate for these tests:
2063,0,[], Comparing many means with ANOVA,seg_71,"where k is the number of comparisons being considered (formally or informally). if there are k(k−1) k groups, then usually all possible pairs are compared and k = . 2"
2064,1,"['pairwise comparisons', 'bonferroni correction', 'average']", Comparing many means with ANOVA,seg_71,"in guided practice 7.48, you found strong evidence of differences in the average midterm grades between the three lectures. complete the three possible pairwise comparisons using the bonferroni correction and report any differences."
2065,1,"['deviation', 'estimate', 'table', 'standard', 'level', 'standard deviation', 'significance', 'significance level', 'anova']", Comparing many means with ANOVA,seg_71,"we use a modified significance level of α? = 0.05/3 = 0.0167. additionally, we use the pooled estimate of the standard deviation: spooled = 13.61 on df = 161, which is provided in the anova summary table."
2066,1,"['estimated', 'standard error', 'standard', 'error']", Comparing many means with ANOVA,seg_71,"lecture a versus lecture b: the estimated difference and standard error are, respectively,"
2067,1,"['results', 'associated', 'level', 'significance', 'statistical', 'significance level']", Comparing many means with ANOVA,seg_71,"(see section 7.3.4 on page 273 for additional details.) this results in a t-score of 1.21 on df = 161 (we use the df associated with spooled). statistical software was used to precisely identify the twosided p-value since the modified significance level of 0.0167 is not found in the t-table. the p-value (0.228) is larger than α∗ = 0.0167, so there is not strong evidence of a difference in the means of lectures a and b."
2068,1,"['estimated', 'standard error', 'results', 'standard', 'error']", Comparing many means with ANOVA,seg_71,"lecture a versus lecture c: the estimated difference and standard error are 3.8 and 2.61, respectively. this results in a t score of 1.46 on df = 161 and a two-sided p-value of 0.1462. this p-value is larger than α∗, so there is not strong evidence of a difference in the means of lectures a and c."
2069,1,"['estimated', 'standard error', 'results', 'standard', 'error']", Comparing many means with ANOVA,seg_71,"lecture b versus lecture c: the estimated difference and standard error are 6.9 and 2.65, respectively. this results in a t score of 2.60 on df = 161 and a two-sided p-value of 0.0102. this p-value is smaller than α∗. here we find strong evidence of a difference in the means of lectures b and c."
2070,0,[], Comparing many means with ANOVA,seg_71,we might summarize the findings of the analysis from example 7.49 using the following notation:
2071,1,"['pairwise comparisons', 'mean', 'null hypothesis', 'hypothesis']", Comparing many means with ANOVA,seg_71,"the midterm mean in lecture a is not statistically distinguishable from those of lectures b or c. however, there is strong evidence that lectures b and c are different. in the first two pairwise comparisons, we did not have sufficient evidence to reject the null hypothesis. recall that failing to reject h0 does not imply h0 is true."
2072,1,"['anova', 'pairwise comparisons', 'null hypothesis', 'hypothesis']", Comparing many means with ANOVA,seg_71,"it is possible to reject the null hypothesis using anova and then to not subsequently identify differences in the pairwise comparisons. however, this does not invalidate the anova conclusion. it only means we have not been able to successfully identify which specific groups differ in their means."
2073,1,"['statistically significant', 'confidence', 'test', 'anova']", Comparing many means with ANOVA,seg_71,"the anova procedure examines the big picture: it considers all groups simultaneously to decipher whether there is evidence that some difference exists. even if the test indicates that there is strong evidence of differences in group means, identifying with high confidence a specific difference as statistically significant is more difficult."
2074,1,"['rate', 'prediction', 'data', 'success', 'anova']", Comparing many means with ANOVA,seg_71,"consider the following analogy: we observe a wall street firm that makes large quantities of money based on predicting mergers. mergers are generally difficult to predict, and if the prediction success rate is extremely high, that may be considered sufficiently strong evidence to warrant investigation by the securities and exchange commission (sec). while the sec may be quite certain that there is insider trading taking place at the firm, the evidence against any single trader may not be very strong. it is only when the sec considers all the data that they identify the pattern. this is effectively the strategy of anova: stand back and consider all the groups simultaneously."
2075,0,[], Comparing many means with ANOVA,seg_71,150 100 casein horsebean linseed meatmeal soybean sunflower
2076,0,[], Comparing many means with ANOVA,seg_71,2.5 arts and humanities natural sciences social sciences
2077,0,[], Comparing many means with ANOVA,seg_71,ke 80 ew re 60 p de r k 40 ow sr 20 uoh 0 less than hs hs jr coll bachelor's graduate
2078,0,[], Comparing many means with ANOVA,seg_71,0 primary school lower middle school upper middle school technical or vocational college
2079,0,[], Comparing many means with ANOVA,seg_71,8 10 12 14 16 18 max 18 number of credits
2080,0,[], Comparing many means with ANOVA,seg_71,"heat wd−40 royal purple pb blaster liquid wrench aerokroil acetone/atf none 90 100 110 120 130 140 torque required to loosen rusty bolt, in foot−pounds"
2081,0,[], Comparing many means with ANOVA,seg_71,number of exclusive relationships
2082,0,[], Comparing many means with ANOVA,seg_71,age at first marriage
2083,1,"['outliers', 'prediction', 'residuals', 'correlation', 'numerical', 'linear', 'statistical', 'least squares regression', 'regression', 'linear regression', 'scatterplots', 'variables', 'least squares']",Chapter  Introduction to linear regression,seg_73,"8.1 fitting a line, residuals, and correlation 8.2 least squares regression 8.3 types of outliers in linear regression 8.4 inference for linear regression linear regression is a very powerful statistical technique. many people have some familiarity with regression just from reading the news, where straight lines are overlaid on scatterplots. linear models can be used for prediction or to evaluate whether there is a linear relationship between two numerical variables. for videos, slides, and other resources, please visit www.openintro.org/os"
2084,1,"['linear', 'model', 'linear model', 'correlation', 'statistic', 'process']", Fitting a line residuals and correlation,seg_75,"it’s helpful to think deeply about the line fitting process. in this section, we define the form of a linear model, explore criteria for what makes a good fit, and introduce a new statistic called correlation."
2085,1,['data'], Fitting a line residuals and correlation,seg_75,8.1.1 fitting a line to data
2086,1,['variables'], Fitting a line residuals and correlation,seg_75,figure 8.1 shows two variables whose relationship can be modeled perfectly with a straight line. the equation for the line is
2087,1,"['linear', 'factors', 'prediction', 'information', 'process']", Fitting a line residuals and correlation,seg_75,"consider what a perfect linear relationship means: we know the exact value of y just by knowing the value of x. this is unrealistic in almost any natural process. for example, if we took family income (x), this value would provide some useful information about how much financial support a college may offer a prospective student (y). however, the prediction would be far from perfect, since other factors play a role in financial support beyond a family’s finances."
2088,1,['linear'], Fitting a line residuals and correlation,seg_75,"figure 8.1: requests from twelve separate buyers were simultaneously placed with a trading company to purchase target corporation stock (ticker tgt, december 28th, 2018), and the total cost of the shares were reported. because the cost is computed using a linear formula, the linear fit is perfect."
2089,1,"['method', 'variables', 'data', 'regression', 'statistical', 'error']", Fitting a line residuals and correlation,seg_75,"linear regression is the statistical method for fitting a line to data where the relationship between two variables, x and y, can be modeled by a straight line with some error:"
2090,1,"['predictor variable', 'estimates', 'prediction', 'predictor', 'data', 'error', 'model', 'parameters', 'point estimates', 'outcome', 'response', 'estimated', 'variable', 'explanatory', 'average']", Fitting a line residuals and correlation,seg_75,"the values β0 and β1 represent the model’s parameters (β is the greek letter beta), and the error is represented by ε (the greek letter epsilon). the parameters are estimated using data, and we write their point estimates as b0 and b1. when we use x to predict y, we usually call x the explanatory or predictor variable, and we call y the response; we also often drop the term when writing down the model since our main focus is often on the prediction of the average outcome."
2091,1,"['plot', 'linear', 'variability', 'observations', 'case', 'data', 'cloud of points']", Fitting a line residuals and correlation,seg_75,"it is rare for all of the data to fall perfectly on a straight line. instead, it’s more common for data to appear as a cloud of points, such as those examples shown in figure 8.2. in each case, the data fall around a straight line, even if none of the observations fall exactly on the line. the first plot shows a relatively strong downward linear trend, where the remaining variability in the data around the line is minor relative to the strength of the relationship between x and y. the second plot shows an upward trend that, while evident, is not as strong as the first. the last plot shows a"
2092,1,"['linear', 'model', 'linear model', 'data', 'sets', 'data sets']", Fitting a line residuals and correlation,seg_75,figure 8.2: three data sets where a linear model may be useful even though the data do not all fall exactly on the line.
2093,0,[], Fitting a line residuals and correlation,seg_75,v ra ● best fitting straight line is flat (!) ●
2094,1,"['linear', 'model', 'linear model', 'experiment', 'nonlinear', 'data', 'case']", Fitting a line residuals and correlation,seg_75,figure 8.3: a linear model is not useful in this nonlinear case. these data are from an introductory physics experiment.
2095,1,"['model', 'parameters', 'estimates', 'uncertainty', 'data', 'associated']", Fitting a line residuals and correlation,seg_75,"very weak downward trend in the data, so slight we can hardly notice it. in each of these examples, we will have some uncertainty regarding our estimates of the model parameters, β0 and β1. for instance, we might wonder, should we move the line up or down a little, or should we tilt it more or less? as we move forward in this chapter, we will learn about criteria for line-fitting, and we will also learn about the uncertainty associated with estimates of model parameters."
2096,1,"['linear', 'variables', 'nonlinear', 'data', 'case', 'cases']", Fitting a line residuals and correlation,seg_75,"there are also cases where fitting a straight line to the data, even if there is a clear relationship between the variables, is not helpful. one such case is shown in figure 8.3 where there is a very clear relationship between the variables even though the trend is not linear. we discuss nonlinear trends in this chapter and the next, but details of fitting nonlinear models are saved for a later course."
2097,1,"['linear', 'linear regression', 'regression']", Fitting a line residuals and correlation,seg_75,8.1.2 using linear regression to predict possum head lengths
2098,1,"['tail', 'measurements']", Fitting a line residuals and correlation,seg_75,"brushtail possums are a marsupial that lives in australia, and a photo of one is shown in figure 8.4. researchers captured 104 of these animals and took body measurements before releasing the animals back into the wild. we consider two of these measurements: the total length of each possum, from head to tail, and the length of each possum’s head."
2099,1,"['linear', 'variables', 'scatterplot', 'data', 'associated', 'average']", Fitting a line residuals and correlation,seg_75,"figure 8.5 shows a scatterplot for the head length and total length of the possums. each point represents a single possum from the data. the head and total length variables are associated: possums with an above average total length also tend to have above average head lengths. while the relationship is not perfectly linear, it could be helpful to partially explain the connection between these variables with a straight line."
2100,1,['scatterplot'], Fitting a line residuals and correlation,seg_75,figure 8.5: a scatterplot showing head length against total length for 104 brushtail possums. a point representing a possum with head length 94.1mm and total length 89cm is highlighted.
2101,1,"['linear', 'predictor variable', 'variables', 'data', 'variable', 'set', 'data set', 'predictor']", Fitting a line residuals and correlation,seg_75,"we want to describe the relationship between the head length and total length variables in the possum data set using a line. in this example, we will use the total length as the predictor variable, x, to predict a possum’s head length, y. we could fit the linear relationship by eye, as in figure 8.6. the equation for this line is"
2102,1,['estimate'], Fitting a line residuals and correlation,seg_75,"a “hat” on y is used to signify that this is an estimate. we can use this line to discuss properties of possums. for instance, the equation predicts a possum with a total length of 80 cm will have a head length of"
2103,1,"['prediction', 'average', 'information', 'estimate']", Fitting a line residuals and correlation,seg_75,"the estimate may be viewed as an average: the equation predicts that possums with a total length of 80 cm will have an average head length of 88.2 mm. absent further information about an 80 cm possum, the prediction for head length that uses the average is a reasonable estimate."
2104,1,"['linear', 'model', 'linear model']", Fitting a line residuals and correlation,seg_75,figure 8.6: a reasonable linear model was fit to represent the relationship between head length and total length.
2105,1,['variables'], Fitting a line residuals and correlation,seg_75,what other variables might help us predict the head length of a possum besides its length?
2106,1,"['linear', 'model', 'predictor', 'linear model']", Fitting a line residuals and correlation,seg_75,"perhaps the relationship would be a little different for male possums than female possums, or perhaps it would differ for possums from one region of australia versus another region. in chapter 9, we’ll learn about how we can include more than one predictor. before we get there, we first need to better understand how to best build a simple linear model with one predictor."
2107,1,['residuals'], Fitting a line residuals and correlation,seg_75,8.1.3 residuals
2108,1,"['model', 'data', 'variation']", Fitting a line residuals and correlation,seg_75,residuals are the leftover variation in the data after accounting for the model fit:
2109,1,['residual'], Fitting a line residuals and correlation,seg_75,data = fit + residual
2110,1,"['linear', 'model', 'linear model', 'residual', 'observations', 'residuals', 'data', 'regression', 'observation', 'regression line']", Fitting a line residuals and correlation,seg_75,"each observation will have a residual, and three of the residuals for the linear model we fit for the possum data is shown in figure 8.6. if an observation is above the regression line, then its residual, the vertical distance from the observation to the line, is positive. observations below the line have negative residuals. one goal in picking the right linear model is for these residuals to be as small as possible."
2111,1,"['residual', 'residuals', 'absolute value', 'observation']", Fitting a line residuals and correlation,seg_75,"let’s look closer at the three residuals featured in figure 8.6. the observation marked by an “×” has a small, negative residual of about -1; the observation marked by “+” has a large residual of about +7; and the observation marked by “4” has a moderate residual of about -4. the size of a residual is usually discussed in terms of its absolute value. for example, the residual for “4” is larger than that of “×” because | − 4| is larger than | − 1|."
2112,1,"['residual', 'model', 'response', 'observation']", Fitting a line residuals and correlation,seg_75,"the residual of the ith observation (xi, yi) is the difference of the observed response (yi) and the response we would predict based on the model fit (ŷi):"
2113,1,['model'], Fitting a line residuals and correlation,seg_75,we typically identify ŷi by plugging xi into the model.
2114,1,"['residual', 'linear', 'observation', 'estimate']", Fitting a line residuals and correlation,seg_75,"the linear fit shown in figure 8.6 is given as ŷ = 41 + 0.59x. based on this line, formally compute the residual of the observation (77.0, 85.3). this observation is denoted by “×” in figure 8.6. check it against the earlier visual estimate, -1."
2115,1,"['predicted', 'model']", Fitting a line residuals and correlation,seg_75,we first compute the predicted value of point “×” based on the model:
2116,1,['predicted'], Fitting a line residuals and correlation,seg_75,next we compute the difference of the actual head length and the predicted head length:
2117,1,"['linear', 'model', 'linear model', 'estimate', 'residual', 'error']", Fitting a line residuals and correlation,seg_75,"the model’s error is e× = −1.1mm, which is very close to the visual estimate of -1mm. the negative residual indicates that the linear model overpredicted head length for this particular possum."
2118,1,"['residual', 'model', 'observation']", Fitting a line residuals and correlation,seg_75,"if a model underestimates an observation, will the residual be positive or negative? what about if it overestimates the observation?1"
2119,1,"['linear', 'residuals', 'observation']", Fitting a line residuals and correlation,seg_75,"compute the residuals for the “+” observation (85.0, 98.6) and the “4” observation (95.5, 94.0) in the figure using the linear relationship ŷ = 41 + 0.59x.2"
2120,1,"['plot', 'model', 'linear', 'linear model', 'evaluating', 'residual', 'residuals', 'scatterplot', 'data', 'residual plot', 'regression', 'set', 'data set', 'locations', 'regression line']", Fitting a line residuals and correlation,seg_75,"residuals are helpful in evaluating how well a linear model fits a data set. we often display them in a residual plot such as the one shown in figure 8.7 for the regression line in figure 8.6. the residuals are plotted at their original horizontal locations but with the vertical coordinate as the residual. for instance, the point (85.0, 98.6)+ had a residual of 7.45, so in the residual plot it is placed at (85.0, 7.45). creating a residual plot is sort of like tipping the scatterplot over so the regression line is horizontal."
2121,1,"['plot', 'model', 'residual', 'residual plot']", Fitting a line residuals and correlation,seg_75,figure 8.7: residual plot for the model in figure 8.6.
2122,1,"['sample', 'residual', 'plots', 'data', 'residual plots']", Fitting a line residuals and correlation,seg_75,figure 8.8: sample data with their best fitting lines (top row) and their corresponding residual plots (bottom row).
2123,1,"['linear', 'model', 'residual', 'plots', 'residuals', 'data', 'scatterplots', 'residual plots']", Fitting a line residuals and correlation,seg_75,one purpose of residual plots is to identify characteristics or patterns still apparent in data after fitting a model. figure 8.8 shows three scatterplots with linear models in the first row and residual plots in the second row. can you identify any patterns remaining in the residuals?
2124,1,"['residuals', 'data', 'set', 'data set']", Fitting a line residuals and correlation,seg_75,"in the first data set (first column), the residuals show no obvious patterns. the residuals appear to be scattered randomly around the dashed line that represents 0."
2125,1,"['plot', 'model', 'residual', 'residuals', 'scatterplot', 'residual plot', 'data', 'set', 'data set']", Fitting a line residuals and correlation,seg_75,"the second data set shows a pattern in the residuals. there is some curvature in the scatterplot, which is more obvious in the residual plot. we should not use a straight line to model these data. instead, a more advanced technique should be used."
2126,1,"['plot', 'linear', 'model', 'linear model', 'estimate', 'residuals', 'data', 'statistically significant', 'point estimate', 'parameter', 'slope']", Fitting a line residuals and correlation,seg_75,"the last plot shows very little upwards trend, and the residuals also show no obvious patterns. it is reasonable to try to fit a linear model to the data. however, it is unclear whether there is statistically significant evidence that the slope parameter is different from zero. the point estimate of the slope parameter, labeled b1, is not zero, but we might wonder if this could just be due to chance. we will address this sort of scenario in section 8.4."
2127,1,"['linear', 'correlation']", Fitting a line residuals and correlation,seg_75,8.1.4 describing linear relationships with correlation
2128,1,"['linear', 'statistic', 'plots']", Fitting a line residuals and correlation,seg_75,we’ve seen plots with strong linear relationships and others with very weak linear relationships. it would be useful if we could quantify the strength of these linear relationships with a statistic.
2129,1,"['variables', 'linear', 'correlation']", Fitting a line residuals and correlation,seg_75,"correlation, which always takes values between -1 and 1, describes the strength of the linear relationship between two variables. we denote the correlation by r."
2130,1,"['deviation', 'sample', 'sample mean', 'statistics', 'correlation', 'mean']", Fitting a line residuals and correlation,seg_75,"we can compute the correlation using a formula, just as we did with the sample mean and standard deviation. this formula is rather complex,3 and like with other statistics, we generally perform"
2131,1,"['sample', 'variables', 'correlations', 'variable', 'scatterplots', 'associated']", Fitting a line residuals and correlation,seg_75,"figure 8.9: sample scatterplots and their correlations. the first row shows variables with a positive relationship, represented by the trend up and to the right. the second row shows variables with a negative trend, where a large value in one variable is associated with a low value in the other."
2132,1,"['linear', 'variables', 'plots', 'correlation', 'correlations']", Fitting a line residuals and correlation,seg_75,"the calculations on a computer or calculator. figure 8.9 shows eight plots and their corresponding correlations. only when the relationship is perfectly linear is the correlation either -1 or 1. if the relationship is strong and positive, the correlation will be near +1. if it is strong and negative, it will be near -1. if there is no apparent linear relationship between the variables, then the correlation will be near zero."
2133,1,"['correlations', 'linear', 'nonlinear', 'correlation']", Fitting a line residuals and correlation,seg_75,"the correlation is intended to quantify the strength of a linear trend. nonlinear trends, even when strong, sometimes produce correlations that do not reflect the strength of the relationship; see three such examples in figure 8.10."
2134,1,"['sample', 'variables', 'nonlinear', 'case', 'correlation', 'correlations', 'scatterplots']", Fitting a line residuals and correlation,seg_75,"figure 8.10: sample scatterplots and their correlations. in each case, there is a strong relationship between the variables. however, because the relationship is nonlinear, the correlation is relatively weak."
2135,1,"['plot', 'curve', 'nonlinear', 'data', 'sets', 'nonlinear curves', 'data sets']", Fitting a line residuals and correlation,seg_75,"no straight line is a good fit for the data sets represented in figure 8.10. try drawing nonlinear curves on each plot. once you create a curve for each, describe what is important in your fit.4"
2136,0,[], Fitting a line residuals and correlation,seg_75,60 70 80 60 70 80 height (in inches) height (in inches)
2137,0,[], Fitting a line residuals and correlation,seg_75,0 100 200 300 distance (miles)
2138,0,[], Fitting a line residuals and correlation,seg_75,g i centimeters? ew 60 40 80 90 100 110 120 130 hip girth (cm)
2139,1,"['least squares regression', 'linear', 'least squares', 'regression']", Least squares regression,seg_77,"fitting linear models by eye is open to criticism since it is based on an individual’s preference. in this section, we use least squares regression as a more rigorous approach."
2140,0,[], Least squares regression,seg_77,8.2.1 gift aid for freshman at elmhurst college
2141,1,"['sample', 'linear', 'random', 'scatterplot', 'data', 'random sample']", Least squares regression,seg_77,"this section considers family income and gift aid data from a random sample of fifty students in the freshman class of elmhurst college in illinois. gift aid is financial aid that does not need to be paid back, as opposed to a loan. a scatterplot of the data is shown in figure 8.11 along with two linear fits. the lines follow a negative trend in the data; students who have higher family incomes tended to have lower gift aid from the university."
2142,1,"['sample', 'random', 'data', 'least squares', 'least squares line', 'random sample']", Least squares regression,seg_77,"figure 8.11: gift aid and family income for a random sample of 50 freshman students from elmhurst college. two lines are fit to the data, the solid line being the least squares line."
2143,1,['correlation'], Least squares regression,seg_77,is the correlation positive or negative in figure 8.11?8
2144,0,[], Least squares regression,seg_77,8.2.2 an objective measure for finding the best line
2145,1,"['residual', 'residuals', 'mean']", Least squares regression,seg_77,"we begin by thinking about what we mean by “best”. mathematically, we want a line that has small residuals. the first option that may come to mind is to minimize the sum of the residual magnitudes:"
2146,1,['residuals'], Least squares regression,seg_77,"which we could accomplish with a computer program. the resulting dashed line shown in figure 8.11 demonstrates this fit can be quite reasonable. however, a more common practice is to choose the line that minimizes the sum of the squared residuals:"
2147,1,"['least squares criterion', 'least squares line', 'least squares', 'residual']", Least squares regression,seg_77,the line that minimizes this least squares criterion is represented as the solid line in figure 8.11. this is commonly called the least squares line. the following are three possible reasons to choose this option instead of trying to minimize the sum of residual magnitudes without any squaring:
2148,1,['method'], Least squares regression,seg_77,1. it is the most commonly used method.
2149,1,"['statistical', 'least squares line', 'least squares']", Least squares regression,seg_77,2. computing the least squares line is widely supported in statistical software.
2150,1,"['residual', 'residuals']", Least squares regression,seg_77,"3. in many applications, a residual twice as large as another residual is more than twice as bad. for example, being off by 4 is usually more than twice as bad as being off by 2. squaring the residuals accounts for this discrepancy."
2151,1,"['least squares criterion', 'least squares']", Least squares regression,seg_77,the first two reasons are largely for tradition and convenience; the last reason explains why the least squares criterion is typically most helpful.9
2152,1,"['least squares', 'least squares line']", Least squares regression,seg_77,8.2.3 conditions for the least squares line
2153,1,"['least squares line', 'least squares']", Least squares regression,seg_77,"when fitting a least squares line, we generally require"
2154,1,"['linear', 'nonlinear', 'data']", Least squares regression,seg_77,linearity. the data should show a linear trend. if there is a nonlinear trend (e.g. left panel of
2155,1,"['method', 'regression']", Least squares regression,seg_77,"figure 8.12), an advanced regression method from another book or later course should be applied."
2156,1,"['residuals', 'normal', 'condition']", Least squares regression,seg_77,"nearly normal residuals. generally, the residuals must be nearly normal. when this condition"
2157,1,"['outliers', 'residual', 'observation', 'regression', 'regression line']", Least squares regression,seg_77,"is found to be unreasonable, it is usually because of outliers or concerns about influential points, which we’ll talk about more in sections 8.3. an example of a residual that would be a potentially concern is shown in figure 8.12, where one observation is clearly much further from the regression line than the others."
2158,1,"['least squares line', 'least squares', 'variability']", Least squares regression,seg_77,constant variability. the variability of points around the least squares line remains roughly con-
2159,1,"['condition', 'variability']", Least squares regression,seg_77,"stant. an example of non-constant variability is shown in the third panel of figure 8.12, which represents the most common pattern observed when this condition fails: the variability of y is larger when x is larger."
2160,1,"['observations', 'data', 'regression', 'time series']", Least squares regression,seg_77,"independent observations. be cautious about applying regression to time series data, which"
2161,1,"['model', 'independent', 'observations', 'data', 'correlations', 'set', 'data set']", Least squares regression,seg_77,"are sequential observations in time such as a stock price each day. such data may have an underlying structure that should be considered in a model and analysis. an example of a data set where successive observations are not independent is shown in the fourth panel of figure 8.12. there are also other instances where correlations within the data are important, which is further discussed in chapter 9."
2162,1,"['least squares regression', 'least squares', 'data', 'regression']", Least squares regression,seg_77,should we have concerns about applying least squares regression to the elmhurst data in figure 8.11?10
2163,1,"['outliers', 'variability', 'observations', 'data', 'errors', 'set', 'data set', 'correlated', 'time series']", Least squares regression,seg_77,"figure 8.12: four examples showing when the methods in this chapter are insufficient to apply to the data. first panel: linearity fails. second panel: there are outliers, most especially one point that is very far away from the line. third panel: the variability of the errors is related to the value of x. fourth panel: a time series data set is shown, where successive observations are highly correlated."
2164,1,"['least squares line', 'least squares']", Least squares regression,seg_77,8.2.4 finding the least squares line
2165,1,"['least squares regression', 'least squares', 'data', 'regression', 'regression line']", Least squares regression,seg_77,"for the elmhurst data, we could write the equation of the least squares regression line as"
2166,1,"['parameters', 'regression', 'set', 'regression line']", Least squares regression,seg_77,"here the equation is set up to predict gift aid based on a student’s family income, which would be useful to students considering elmhurst. these two values, β0 and β1, are the parameters of the regression line."
2167,1,"['sample', 'estimated', 'parameters', 'sample mean', 'estimates', 'estimation', 'data', 'least squares', 'least squares line', 'mean', 'parameter']", Least squares regression,seg_77,"as in chapters 5, 6, and 7, the parameters are estimated using observed data. in practice, this estimation is done using a computer in the same way that other estimates, like a sample mean, can be estimated using a computer or calculator. however, we can also find the parameter estimates by applying two properties of the least squares line:"
2168,1,"['estimated', 'least squares', 'least squares line', 'slope']", Least squares regression,seg_77,• the slope of the least squares line can be estimated by
2169,1,"['sample', 'sample standard deviations', 'variables', 'standard deviations', 'correlation', 'deviations', 'variable', 'explanatory variable', 'response', 'explanatory', 'standard']", Least squares regression,seg_77,"where r is the correlation between the two variables, and sx and sy are the sample standard deviations of the explanatory variable and response, respectively."
2170,1,"['sample', 'sample mean', 'least squares', 'variable', 'explanatory variable', 'least squares line', 'mean', 'explanatory']", Least squares regression,seg_77,"• if x̄ is the sample mean of the explanatory variable and ȳ is the sample mean of the vertical variable, then the point (x̄, ȳ) is on the least squares line."
2171,1,"['plot', 'sample', 'sample means', 'least squares', 'least squares line']", Least squares regression,seg_77,"figure 8.13 shows the sample means for the family income and gift aid as $101,780 and $19,940, respectively. we could plot the point (101.8, 19.94) on figure 8.11 on page 317 to verify it falls on the least squares line (the solid line)."
2172,1,"['parameters', 'estimates', 'point estimates']", Least squares regression,seg_77,"next, we formally find the point estimates b0 and b1 of the parameters β0 and β1."
2173,1,['statistics'], Least squares regression,seg_77,figure 8.13: summary statistics for family income and gift aid.
2174,1,"['statistics', 'regression', 'regression line', 'slope']", Least squares regression,seg_77,"using the summary statistics in figure 8.13, compute the slope for the regression line of gift aid against family income.11"
2175,1,"['model', 'slope', 'estimate']", Least squares regression,seg_77,"you might recall the point-slope form of a line from math class, which we can use to find the model fit, including the estimate of b0. given the slope of a line and a point on the line, (x0, y0), the equation for the line can be written as"
2176,0,[], Least squares regression,seg_77,y − y0 = slope× (x− x0)
2177,1,"['statistics', 'least squares line', 'least squares']", Least squares regression,seg_77,to identify the least squares line from summary statistics:
2178,1,"['slope', 'parameter', 'estimate']", Least squares regression,seg_77,"• estimate the slope parameter, b1 = (sy/sx)r."
2179,1,"['least squares line', 'least squares']", Least squares regression,seg_77,"• noting that the point (x̄, ȳ) is on the least squares line, use x0 = x̄ and y0 = ȳ with the point-slope equation: y − ȳ = b1(x− x̄)."
2180,1,"['sample', 'estimate', 'sample means', 'slope']", Least squares regression,seg_77,"using the point (101780, 19940) from the sample means and the slope estimate b1 = −0.0431 from guided practice 8.9, find the least-squares line for predicting aid based on family income."
2181,1,['slope'], Least squares regression,seg_77,"apply the point-slope equation using (101.78, 19.94) and the slope b1 = −0.0431:"
2182,0,[], Least squares regression,seg_77,"expanding the right side and then adding 19,940 to each side, the equation simplifies:"
2183,1,"['variable', 'predicted']", Least squares regression,seg_77,"here we have replaced y with âid and x with family income to put the equation in context. the final equation should always include a “hat” on the variable being predicted, whether it is a generic “y” or a named variable like “aid”."
2184,1,"['estimates', 'table', 'results', 'least squares', 'regression', 'least squares line', 'regression line', 'error']", Least squares regression,seg_77,"a computer is usually used to compute the least squares line, and a summary table generated using software for the elmhurst regression line is shown in figure 8.14. the first column of numbers provides estimates for b0 and b1, respectively. these results match those from example 8.10 (with some minor rounding error)."
2185,1,['error'], Least squares regression,seg_77,estimate std. error t value pr(>|t|)
2186,1,['intercept'], Least squares regression,seg_77,(intercept) 24319.3 1291.5 18.83 <0.0001 family income -0.0431 0.0108 -3.98 0.0002
2187,1,"['estimates', 'results', 'data', 'least squares', 'parameter']", Least squares regression,seg_77,figure 8.14: summary of least squares fit for the elmhurst data. compare the parameter estimates in the first column to the results of example 8.10.
2188,0,[], Least squares regression,seg_77,"examine the second, third, and fourth columns in figure 8.14. can you guess what they represent? (if you have not reviewed any inference chapter yet, skip this example.)"
2189,1,"['alternative hypothesis', 'error', 'estimate', 'standard error', 'statistic', 'point estimate', 'standard', 'null hypothesis', 'hypothesis']", Least squares regression,seg_77,"we’ll describe the meaning of the columns using the second row, which corresponds to β1. the first column provides the point estimate for β1, as we calculated in an earlier example: b1 = −0.0431. the second column is a standard error for this point estimate: seb1 = 0.0108. the third column is a t-test statistic for the null hypothesis that β1 = 0: t = −3.98. the last column is the p-value for the t-test statistic for the null hypothesis β1 = 0 and a two-sided alternative hypothesis: 0.0002. we will get into more of these details in section 8.4."
2190,1,"['linear', 'estimated', 'linear equation']", Least squares regression,seg_77,suppose a high school senior is considering elmhurst college. can she simply use the linear equation that we have estimated to calculate her financial aid from the university?
2191,1,"['linear', 'estimate', 'predicted', 'data', 'linear equation']", Least squares regression,seg_77,"she may use it as an estimate, though some qualifiers on this approach are important. first, the data all come from one freshman class, and the way aid is determined by the university may change from year to year. second, the equation will provide an imperfect estimate. while the linear equation is good at capturing the trend in the data, no individual student’s aid will be perfectly predicted."
2192,1,"['model', 'regression model', 'estimates', 'regression', 'parameter']", Least squares regression,seg_77,8.2.5 interpreting regression model parameter estimates
2193,1,"['model', 'regression model', 'parameters', 'regression']", Least squares regression,seg_77,interpreting parameters in a regression model is often one of the most important steps in the analysis.
2194,1,"['estimates', 'intercept', 'data', 'mean', 'slope']", Least squares regression,seg_77,"the intercept and slope estimates for the elmhurst data are b0 = 24,319 and b1 = −0.0431. what do these numbers really mean?"
2195,1,"['model', 'association', 'variables', 'data', 'parameter', 'coefficient', 'average', 'slope']", Least squares regression,seg_77,"interpreting the slope parameter is helpful in almost any application. for each additional $1,000 of family income, we would expect a student to receive a net difference of $1,000×(−0.0431) = −$43.10 in aid on average, i.e. $43.10 less. note that a higher family income corresponds to less aid because the coefficient of family income is negative in the model. we must be cautious in this interpretation: while there is a real association, we cannot interpret a causal connection between the variables because these data are observational. that is, increasing a student’s family income may not cause the student’s aid to drop. (it would be reasonable to contact the college and ask if the relationship is causal, i.e. if elmhurst college’s aid decisions are partially based on students’ family income.)"
2196,1,"['estimated', 'observations', 'intercept', 'average']", Least squares regression,seg_77,"the estimated intercept b0 = 24,319 describes the average aid if a student’s family had no income. the meaning of the intercept is relevant to this application since the family income for some students at elmhurst is $0. in other applications, the intercept may have little or no practical value if there are no observations where x is near zero."
2197,1,"['linear', 'model', 'estimated', 'linear model', 'case', 'intercept', 'variable', 'explanatory variable', 'explanatory', 'outcome', 'average', 'slope']", Least squares regression,seg_77,"the slope describes the estimated difference in the y variable if the explanatory variable x for a case happened to be one unit larger. the intercept describes the average outcome of y if x = 0 and the linear model is valid all the way to x = 0, which in many applications is not the case."
2198,1,['extrapolation'], Least squares regression,seg_77,8.2.6 extrapolation is treacherous
2199,0,[], Least squares regression,seg_77,"stephen colbert april 6th, 201012"
2200,1,"['linear', 'variables', 'data', 'regression', 'linear regression']", Least squares regression,seg_77,"linear models can be used to approximate the relationship between two variables. however, these models have real limitations. linear regression is simply a modeling framework. the truth is almost always much more complex than our simple line. for example, we do not know how the data outside of our limited window will behave."
2201,1,"['model', 'estimate']", Least squares regression,seg_77,"use the model âid = 24,319−0.0431×family income to estimate the aid of another freshman student whose family had income of $1 million."
2202,0,[], Least squares regression,seg_77,"we want to calculate the aid for family income = 1,000,000:"
2203,1,['model'], Least squares regression,seg_77,"the model predicts this student will have -$18,781 in aid (!). however, elmhurst college does not offer negative aid where they select some students to pay extra on top of tuition to attend."
2204,1,"['linear', 'model', 'linear model', 'estimate', 'variables', 'approximation', 'data']", Least squares regression,seg_77,"applying a model estimate to values outside of the realm of the original data is called extrapolation. generally, a linear model is only an approximation of the real relationship between two variables. if we extrapolate, we are making an unreliable bet that the approximate linear relationship will be valid in places where it has not been analyzed."
2205,0,[], Least squares regression,seg_77,8.2.7 using r2 to describe the strength of a fit
2206,1,"['model', 'linear', 'linear model', 'variables', 'data', 'correlation']", Least squares regression,seg_77,"we evaluated the strength of the linear relationship between two variables earlier using the correlation, r. however, it is more common to explain the strength of a linear fit using r2, called r-squared. if provided with a linear model, we might like to describe how closely the data cluster around the linear fit."
2207,1,"['least squares regression', 'sample', 'random', 'least squares', 'regression', 'random sample', 'regression line']", Least squares regression,seg_77,"figure 8.15: gift aid and family income for a random sample of 50 freshman students from elmhurst college, shown with the least squares regression line."
2208,1,"['model', 'linear', 'linear model', 'variability', 'uncertainty', 'residuals', 'data', 'least squares', 'variable', 'response', 'response variable', 'least squares line', 'variation', 'variance']", Least squares regression,seg_77,"the r2 of a linear model describes the amount of variation in the response that is explained by the least squares line. for example, consider the elmhurst data, shown in figure 8.15. the variance of the response variable, aid received, is about s2aid ≈ 29.8 million. however, if we apply our least squares line, then this model reduces our uncertainty in predicting aid using a student’s family income. the variability in the residuals describes how much variation remains after using the model: s2"
2209,0,[], Least squares regression,seg_77,"res ≈ 22.4 million. in short, there was a reduction of"
2210,1,"['linear', 'model', 'linear model', 'information', 'variation']", Least squares regression,seg_77,or about 25% in the data’s variation by using information about family income for predicting aid using a linear model. this corresponds exactly to the r-squared value:
2211,1,"['linear', 'model', 'linear model', 'correlation', 'response', 'explanatory', 'variation']", Least squares regression,seg_77,"if a linear model has a very strong negative relationship with a correlation of -0.97, how much of the variation in the response is explained by the explanatory variable?13"
2212,1,"['levels', 'categorical']", Least squares regression,seg_77,8.2.8 categorical predictors with two levels
2213,1,"['plot', 'outcomes', 'levels', 'condition', 'categorical', 'variables', 'data', 'level', 'predictor']", Least squares regression,seg_77,"categorical variables are also useful in predicting outcomes. here we consider a categorical predictor with two levels (recall that a level is the same as a category). we’ll consider ebay auctions for a video game, mario kart for the nintendo wii, where both the total price of the auction and the condition of the game were recorded. here we want to predict total price based on game condition, which takes values used and new. a plot of the auction data is shown in figure 8.16."
2214,1,"['least squares regression', 'condition', 'least squares', 'regression', 'regression line']", Least squares regression,seg_77,"figure 8.16: total auction prices for the video game mario kart, divided into used (x = 0) and new (x = 1) condition games. the least squares regression line is also shown."
2215,1,"['model', 'linear', 'linear model', 'condition', 'regression', 'variable', 'categories', 'indicator', 'indicator variable', 'numerical']", Least squares regression,seg_77,"to incorporate the game condition variable into a regression equation, we must convert the categories into a numerical form. we will do so using an indicator variable called cond new, which takes value 1 when the game is new and 0 when the game is used. using this indicator variable, the linear model may be written as"
2216,1,['error'], Least squares regression,seg_77,estimate std. error t value pr(>|t|)
2217,1,['intercept'], Least squares regression,seg_77,(intercept) 42.87 0.81 52.67 <0.0001 cond new 10.90 1.26 8.66 <0.0001
2218,1,"['least squares regression', 'least squares', 'condition', 'regression']", Least squares regression,seg_77,figure 8.17: least squares regression summary for the final auction price against the condition of the game.
2219,1,"['estimates', 'model', 'parameter']", Least squares regression,seg_77,"the parameter estimates are given in figure 8.17, and the model equation can be summarized as"
2220,1,"['levels', 'categorical', 'residuals', 'data', 'normal', 'variance']", Least squares regression,seg_77,"for categorical predictors with just two levels, the linearity assumption will always be satisfied. however, we must evaluate whether the residuals in each group are approximately normal and have approximately equal variance. as can be seen in figure 8.16, both of these conditions are reasonably satisfied by the auction data."
2221,1,"['model', 'estimated', 'parameters']", Least squares regression,seg_77,interpret the two parameters estimated in the model for the price of mario kart in ebay auctions.
2222,1,"['estimated', 'condition', 'intercept', 'average']", Least squares regression,seg_77,"the intercept is the estimated price when cond new takes value 0, i.e. when the game is in used condition. that is, the average selling price of a used version of the game is $42.87."
2223,1,"['average', 'slope']", Least squares regression,seg_77,"the slope indicates that, on average, new games sell for about $10.90 more than used games."
2224,1,"['estimated', 'intercept', 'variable', 'response', 'response variable', 'categories', 'indicator', 'average', 'slope']", Least squares regression,seg_77,the estimated intercept is the value of the response variable for the first category (i.e. the category corresponding to an indicator value of 0). the estimated slope is the average change in the response variable between the two categories.
2225,1,"['variables', 'regression', 'multiple regression', 'predictor variables', 'predictor']", Least squares regression,seg_77,"we’ll elaborate further on this topic in chapter 9, where we examine the influence of many predictor variables simultaneously using multiple regression."
2226,1,['residuals'], Least squares regression,seg_77,0 0 0 5000 15000 25000 0 5000 15000 25000 −1500 −750 0 750 1500 number of tourists (thousands) number of tourists (thousands) residuals
2227,1,['residuals'], Least squares regression,seg_77,−20 20 0 100 200 300 400 500 100 200 300 400 500 −40 −20 0 20 40 calories calories residuals
2228,1,"['outliers', 'observations', 'least squares', 'regression', 'least squares line', 'cloud of points']", Types of outliers in linear regression,seg_79,"in this section, we identify criteria for determining which outliers are important and influential. outliers in regression are observations that fall far from the cloud of points. these points are especially important because they can have a strong influence on the least squares line."
2229,1,"['outliers', 'plot', 'residual', 'plots', 'scatterplot', 'residual plot', 'least squares', 'outlier', 'least squares line', 'residual plots']", Types of outliers in linear regression,seg_79,"there are six plots shown in figure 8.18 along with the least squares line and residual plots. for each scatterplot and residual plot pair, identify the outliers and note how they influence the least squares line. recall that an outlier is any point that doesn’t appear to belong with the vast majority of the other points."
2230,1,['outlier'], Types of outliers in linear regression,seg_79,"(1) there is one outlier far from the other points, though it only appears to slightly influence"
2231,1,"['outlier', 'least squares line', 'least squares']", Types of outliers in linear regression,seg_79,"(2) there is one outlier on the right, though it is quite close to the least squares line, which"
2232,0,[], Types of outliers in linear regression,seg_79,suggests it wasn’t very influential.
2233,1,"['outlier', 'least squares']", Types of outliers in linear regression,seg_79,"(3) there is one point far away from the cloud, and this outlier appears to pull the least squares"
2234,0,[], Types of outliers in linear regression,seg_79,line up on the right; examine how the line around the primary cloud doesn’t appear to fit very well.
2235,1,['outliers'], Types of outliers in linear regression,seg_79,(4) there is a primary cloud and then a small secondary cloud of four outliers. the secondary
2236,0,[], Types of outliers in linear regression,seg_79,"cloud appears to be influencing the line somewhat strongly, making the least square line fit poorly almost everywhere. there might be an interesting explanation for the dual clouds, which is something that could be investigated."
2237,1,"['outlier', 'cloud of points']", Types of outliers in linear regression,seg_79,(5) there is no obvious trend in the main cloud of points and the outlier on the right appears to
2238,1,"['control', 'least squares line', 'least squares', 'slope']", Types of outliers in linear regression,seg_79,largely control the slope of the least squares line.
2239,1,"['outlier', 'least squares line', 'least squares']", Types of outliers in linear regression,seg_79,"(6) there is one outlier far from the cloud. however, it falls quite close to the least squares line"
2240,0,[], Types of outliers in linear regression,seg_79,and does not appear to be very influential.
2241,1,"['outliers', 'residual', 'plots', 'data', 'outlier', 'cases', 'least squares', 'residual plots', 'slope']", Types of outliers in linear regression,seg_79,"examine the residual plots in figure 8.18. you will probably find that there is some trend in the main clouds of (3) and (4). in these cases, the outliers influenced the slope of the least squares lines. in (5), data with no clear trend were assigned a line with a large trend simply due to one outlier (!)."
2242,1,"['high leverage', 'leverage']", Types of outliers in linear regression,seg_79,"points that fall horizontally away from the center of the cloud tend to pull harder on the line, so we call them points with high leverage."
2243,1,"['high leverage', 'least squares', 'cases', 'influential point', 'least squares line', 'leverage points', 'leverage', 'slope']", Types of outliers in linear regression,seg_79,"points that fall horizontally far from the line are points of high leverage; these points can strongly influence the slope of the least squares line. if one of these high leverage points does appear to actually invoke its influence on the slope of the line – as in cases (3), (4), and (5) of example 8.17 – then we call it an influential point. usually we can say a point is influential if, had we fitted the line without it, the influential point would have been unusually far from the least squares line."
2244,1,"['outliers', 'cases']", Types of outliers in linear regression,seg_79,"it is tempting to remove outliers. don’t do this without a very good reason. models that ignore exceptional (and interesting) cases often perform poorly. for instance, if a financial firm ignored the largest market swings – the “outliers” – they would soon go bankrupt by making poorly thought-out investments."
2245,1,"['plot', 'residual', 'plots', 'residual plot', 'data', 'outlier', 'sets', 'least squares', 'least squares line', 'data sets']", Types of outliers in linear regression,seg_79,"figure 8.18: six plots, each with a least squares line and residual plot. all data sets have at least one outlier."
2246,1,"['population', 'percent']", Types of outliers in linear regression,seg_79,e servation? p 40% 60% 80% 100% percent urban population
2247,1,"['estimates', 'uncertainty', 'errors', 'regression', 'standard', 'point estimates', 'standard errors', 'regression line', 'slope']", Inference for linear regression,seg_81,"in this section, we discuss uncertainty in the estimates of the slope and y-intercept for a regression line. just as we identified standard errors for point estimates in previous chapters, we first discuss standard errors for these new estimates."
2248,0,[], Inference for linear regression,seg_81,8.4.1 midterm elections and unemployment
2249,1,"['rate', 'states', 'set']", Inference for linear regression,seg_81,"elections for members of the united states house of representatives occur every two years, coinciding every four years with the u.s. presidential election. the set of house elections occurring during the middle of a presidential term are called midterm elections. in america’s two-party system, one political theory suggests the higher the unemployment rate, the worse the president’s party will do in the midterm elections."
2250,1,"['regression line', 'historical data', 'data', 'regression']", Inference for linear regression,seg_81,"to assess the validity of this claim, we can compile historical data and look for a connection. we consider every midterm election from 1898 to 2018, with the exception of those elections during the great depression. figure 8.19 shows these data and the least-squares regression line:"
2251,0,[], Inference for linear regression,seg_81,% change in house seats for president’s party
2252,1,['rate'], Inference for linear regression,seg_81,= −7.36− 0.89× (unemployment rate)
2253,1,"['rate', 'percent']", Inference for linear regression,seg_81,we consider the percent change in the number of seats of the president’s party (e.g. percent change in the number of seats for republicans in 2018) against the unemployment rate.
2254,1,"['outliers', 'observations', 'data', 'correlation', 'deviations', 'variance']", Inference for linear regression,seg_81,"examining the data, there are no clear deviations from linearity, the constant variance condition, or substantial outliers. while the data are collected sequentially, a separate analysis was used to check for any apparent correlation between successive observations; no such correlation was found."
2255,0,[], Inference for linear regression,seg_81,● democrat 10% republican
2256,1,['percent'], Inference for linear regression,seg_81,r ● p −30% 4% 8% 12% percent unemployment
2257,1,"['least squares regression', 'rate', 'data', 'regression', 'least squares', 'regression line', 'percent']", Inference for linear regression,seg_81,"figure 8.19: the percent change in house seats for the president’s party in each election from 1898 to 2010 plotted against the unemployment rate. the two points for the great depression have been removed, and a least squares regression line has been fit to the data."
2258,1,"['rate', 'data']", Inference for linear regression,seg_81,"the data for the great depression (1934 and 1938) were removed because the unemployment rate was 21% and 18%, respectively. do you agree that they should be removed for this investigation? why or why not?17"
2259,1,"['linear', 'model', 'rate', 'linear model', 'estimates', 'hypothesis test', 'statistical hypothesis', 'slope', 'data', 'statistical', 'parameter', 'predictor', 'test', 'hypothesis']", Inference for linear regression,seg_81,"there is a negative slope in the line shown in figure 8.19. however, this slope (and the yintercept) are only estimates of the parameter values. we might wonder, is this convincing evidence that the “true” linear model has a negative slope? that is, do the data provide strong evidence that the political theory is accurate, where the unemployment rate is a useful predictor of the midterm election? we can frame this investigation into a statistical hypothesis test:"
2260,1,"['linear', 'model', 'linear model', 'slope']", Inference for linear regression,seg_81,h0: β1 = 0. the true linear model has slope zero.
2261,1,"['linear', 'model', 'linear model', 'slope']", Inference for linear regression,seg_81,ha: β1 =6 0. the true linear model has a slope different than zero. the unemployment is predictive
2262,0,[], Inference for linear regression,seg_81,of whether the president’s party wins or loses seats in the house of representatives.
2263,1,"['estimate', 'standard error', 'test statistic', 'slope', 'data', 'statistic', 'hypotheses', 'parameter', 'standard', 'test', 'error']", Inference for linear regression,seg_81,"we would reject h0 in favor of ha if the data provide strong evidence that the true slope parameter is different than zero. to assess the hypotheses, we identify a standard error for the estimate, compute an appropriate test statistic, and identify the p-value."
2264,1,['regression'], Inference for linear regression,seg_81,8.4.2 understanding regression output from software
2265,1,"['estimates', 'standard error', 'test statistic', 'statistic', 'standard', 'point estimates', 'test', 'error']", Inference for linear regression,seg_81,"just like other point estimates we have seen before, we can compute a standard error and test statistic for b1. we will generally label the test statistic using a t , since it follows the t-distribution."
2266,1,"['statistics', 'estimate', 'information', 'standard', 'coefficient', 'statistical', 'error', 'least squares regression', 'regression', 'test', 'regression line', 'hypothesis', 'standard error', 'hypothesis test', 'least squares', 'variable', 'point estimate', 'slope']", Inference for linear regression,seg_81,"we will rely on statistical software to compute the standard error and leave the explanation of how this standard error is determined to a second or third statistics course. figure 8.20 shows software output for the least squares regression line in figure 8.19. the row labeled unemp includes the point estimate and other hypothesis test information for the slope, which is the coefficient of the unemployment variable."
2267,1,['error'], Inference for linear regression,seg_81,estimate std. error t value pr(>|t|)
2268,1,['intercept'], Inference for linear regression,seg_81,(intercept) -7.3644 5.1553 -1.43 0.1646 unemp -0.8897 0.8350 -1.07 0.2961 df = 27
2269,1,"['regression', 'response', 'statistical', 'regression line']", Inference for linear regression,seg_81,figure 8.20: output from statistical software for the regression line modeling the midterm election losses for the president’s party as a response to unemployment.
2270,0,[], Inference for linear regression,seg_81,what do the first and second columns of figure 8.20 represent?
2271,1,"['estimate', 'estimates', 'least squares estimates', 'least squares', 'regression', 'errors', 'standard', 'standard errors', 'regression line']", Inference for linear regression,seg_81,"the entries in the first column represent the least squares estimates, b0 and b1, and the values in the second column correspond to the standard errors of each estimate. using the estimates, we could write the equation for the least square regression line as"
2272,1,"['predicted', 'rate', 'case']", Inference for linear regression,seg_81,"where ŷ in this case represents the predicted change in the number of seats for the president’s party, and x represents the unemployment rate."
2273,1,"['test statistic', 'null value', 'slope', 'data', 'regression', 'hypothesis testing', 'statistic', 'hypotheses', 'test', 'numerical', 'hypothesis']", Inference for linear regression,seg_81,"we previously used a t-test statistic for hypothesis testing in the context of numerical data. regression is very similar. in the hypotheses we consider, the null value for the slope is 0, so we can compute the test statistic using the t (or z) score formula:"
2274,1,['null value'], Inference for linear regression,seg_81,estimate− null value −0.8897− 0 t = = = −1.07 se 0.8350
2275,0,[], Inference for linear regression,seg_81,this corresponds to the third column of figure 8.20.
2276,1,"['table', 'hypothesis test', 'hypothesis', 'test']", Inference for linear regression,seg_81,use the table in figure 8.20 to determine the p-value for the hypothesis test.
2277,1,"['rate', 'table', 'hypothesis test', 'data', 'hypothesis', 'coefficient', 'test']", Inference for linear regression,seg_81,"the last column of the table gives the p-value for the two-sided hypothesis test for the coefficient of the unemployment rate: 0.2961. that is, the data do not provide convincing evidence that a higher unemployment rate has any correspondence with smaller or larger losses for the president’s party in the house of representatives in midterm elections."
2278,1,"['estimates', 'statistics', 'test statistics', 'standard', 'standard errors', 'statistical', 'point estimates', 'test', 'method', 'errors']", Inference for linear regression,seg_81,"we usually rely on statistical software to identify point estimates, standard errors, test statistics, and p-values in practice. however, be aware that software will not generally check whether the method is appropriate, meaning we must still verify conditions are met."
2279,1,"['hypothesis test', 'slope', 'test', 'hypothesis']", Inference for linear regression,seg_81,"examine figure 8.15 on page 322, which relates the elmhurst college aid and student family income. how sure are you that the slope is statistically significantly different from zero? that is, do you think a formal hypothesis test would reject the claim that the true slope of the line should be zero?"
2280,1,"['variables', 'hypothesis test', 'slope', 'data', 'test', 'hypothesis']", Inference for linear regression,seg_81,"while the relationship between the variables is not perfect, there is an evident decreasing trend in the data. this suggests the hypothesis test will reject the null claim that the slope is zero."
2281,1,"['least squares regression', 'least squares', 'regression', 'statistical', 'regression line']", Inference for linear regression,seg_81,figure 8.21 shows statistical software output from fitting the least squares regression line shown in figure 8.15. use this output to formally evaluate the following hypotheses.18
2282,1,['coefficient'], Inference for linear regression,seg_81,h0: the true coefficient for family income is zero.
2283,1,['coefficient'], Inference for linear regression,seg_81,ha: the true coefficient for family income is not zero.
2284,1,['error'], Inference for linear regression,seg_81,estimate std. error t value pr(>|t|)
2285,1,['intercept'], Inference for linear regression,seg_81,(intercept) 24319.3 1291.5 18.83 <0.0001 family income -0.0431 0.0108 -3.98 0.0002 df = 48
2286,1,"['least squares', 'data']", Inference for linear regression,seg_81,"figure 8.21: summary of least squares fit for the elmhurst college data, where we are predicting the gift aid by the university based on the family income of students."
2287,1,"['coefficient', 'confidence', 'interval', 'confidence interval']", Inference for linear regression,seg_81,8.4.3 confidence interval for a coefficient
2288,1,"['model', 'interval', 'hypothesis test', 'regression', 'coefficient', 'confidence', 'test', 'confidence interval', 'hypothesis']", Inference for linear regression,seg_81,"similar to how we can conduct a hypothesis test for a model coefficient using regression output, we can also construct a confidence interval for that coefficient."
2289,1,"['interval', 'table', 'regression', 'coefficient', 'confidence', 'confidence interval']", Inference for linear regression,seg_81,compute the 95% confidence interval for the family income coefficient using the regression output from table 8.21.
2290,1,"['degrees of freedom', 'model', 'interval', 'estimate', 'standard error', 'regression', 'distribution', 'point estimate', 'standard', 'coefficient', 'confidence', 'error', 'confidence interval']", Inference for linear regression,seg_81,"the point estimate is -0.0431 and the standard error is se = 0.0108. when constructing a confidence interval for a model coefficient, we generally use a t-distribution. the degrees of freedom for the distribution are noted in the regression output, df = 48, allowing us to identify t?48 = 2.01 for use in the confidence interval."
2291,1,"['confidence', 'interval', 'confidence interval']", Inference for linear regression,seg_81,we can now construct the confidence interval in the usual way:
2292,0,[], Inference for linear regression,seg_81,"point estimate± t?48 × se → −0.0431± 2.01× 0.0108 → (−0.0648,−0.0214)"
2293,1,"['predicted', 'average', 'confident']", Inference for linear regression,seg_81,"we are 95% confident that with each dollar increase in family income, the university’s gift aid is predicted to decrease on average by $0.0214 to $0.0648."
2294,1,"['model', 'intervals', 'coefficients']", Inference for linear regression,seg_81,confidence intervals for model coefficients can be computed using the t-distribution:
2295,1,"['degrees of freedom', 'level', 'confidence', 'confidence level']", Inference for linear regression,seg_81,where t?df is the appropriate t-value corresponding to the confidence level with the model’s degrees of freedom.
2296,1,"['model', 'confidence intervals', 'prediction intervals', 'interval', 'parameters', 'prediction', 'mean response value', 'regression', 'intervals', 'mean', 'confidence', 'response', 'response value']", Inference for linear regression,seg_81,"on the topic of intervals in this book, we’ve focused exclusively on confidence intervals for model parameters. however, there are other types of intervals that may be of interest, including prediction intervals for a response value and also confidence intervals for a mean response value in the context of regression. these two interval types are introduced in an online extra that you may download at"
2297,1,"['linear', 'linear regression', 'regression']", Inference for linear regression,seg_81,www.openintro.org/d?file=stat extra linear regression supp
2298,1,"['least squares regression', 'least squares', 'regression', 'regression line']", Inference for linear regression,seg_81,"in the following exercises, visually check the conditions for fitting a least squares regression line. however, you do not need to report these conditions in your solutions."
2299,1,['population'], Inference for linear regression,seg_81,% urban population
2300,1,"['residuals', 'data collection', 'data']", Inference for linear regression,seg_81,05 −1 1− 0 −1 0 1 2 0 100 200 300 400 −2 −1 0 1 2 beauty order of data collection residuals
2301,1,"['simple linear regression', 'levels', 'range', 'case', 'predictor', 'linear', 'logistic', 'model selection', 'multiple regression', 'model', 'linear model', 'categorical', 'logistic regression', 'regression', 'linear regression', 'outcomes']",Chapter  Multiple and logistic regression,seg_83,"9.1 introduction to multiple regression 9.2 model selection 9.3 checking model conditions using graphs 9.4 multiple regression case study: mario kart 9.5 introduction to logistic regression the principles of simple linear regression lay the foundation for more sophisticated regression models used in a wide range of challenging settings. in chapter 9, we explore multiple regression, which introduces the possibility of more than one predictor in a linear model, and logistic regression, a technique for predicting categorical outcomes with two levels. for videos, slides, and other resources, please visit www.openintro.org/os"
2302,1,"['method', 'variables', 'case', 'regression', 'response']", Introduction to multiple regression,seg_85,"multiple regression extends simple two-variable regression to the case that still has one response but many predictors (denoted x1, x2, x3, ...). the method is motivated by scenarios where many variables may be simultaneously connected to an output."
2303,1,"['rate', 'data', 'regression', 'information', 'variable', 'set', 'data set', 'multiple regression', 'outcome']", Introduction to multiple regression,seg_85,"we will consider data about loans from the peer-to-peer lender, lending club, which is a data set we first encountered in chapters 1 and 2. the loan data includes terms of the loan as well as information about the borrower. the outcome variable we would like to better understand is the interest rate assigned to the loan. for instance, all other characteristics held constant, does it matter how much debt someone already has? does it matter if their income has been verified? multiple regression will help us answer these and other questions."
2304,1,"['model', 'categorical', 'variables', 'observations', 'results', 'data', 'regression', 'variable', 'set', 'data set', 'indicator', 'indicator variable']", Introduction to multiple regression,seg_85,"the data set loans includes results from 10,000 loans, and we’ll be looking at a subset of the available variables, some of which will be new from those we saw in earlier chapters. the first six observations in the data set are shown in figure 9.1, and descriptions for each variable are shown in figure 9.2. notice that the past bankruptcy variable (bankruptcy) is an indicator variable, where it takes the value 1 if the borrower had a past bankruptcy in their record and 0 if not. using an indicator variable in place of a category name allows for these variables to be directly used in regression. two of the other variables are categorical (income ver and issued), each of which can take one of a few different non-numerical values; we’ll discuss how these are handled in the model in section 9.1.1."
2305,1,"['set', 'data set', 'data']", Introduction to multiple regression,seg_85,figure 9.1: first six rows from the loans data set.
2306,1,"['variables', 'data', 'set', 'data set']", Introduction to multiple regression,seg_85,figure 9.2: variables and their descriptions for the loans data set.
2307,1,"['categorical', 'variables', 'indicator', 'categorical variables']", Introduction to multiple regression,seg_85,9.1.1 indicator and categorical variables as predictors
2308,1,"['linear', 'model', 'rate', 'regression model', 'regression', 'linear regression', 'predictor', 'linear regression model']", Introduction to multiple regression,seg_85,let’s start by fitting a linear regression model for interest rate with a single predictor indicating whether or not a person has a bankruptcy in their record:
2309,1,['model'], Introduction to multiple regression,seg_85,results of this model are shown in figure 9.3.
2310,1,"['error', 'intercept']", Introduction to multiple regression,seg_85,estimate std. error t value pr(>|t|) (intercept) 12.3380 0.0533 231.49 <0.0001 bankruptcy 0.7368 0.1529 4.82 <0.0001 df = 9998
2311,1,"['linear', 'model', 'rate', 'linear model']", Introduction to multiple regression,seg_85,figure 9.3: summary of a linear model for predicting interest rate based on whether the borrower has a bankruptcy in their record.
2312,1,"['variable', 'model', 'coefficient']", Introduction to multiple regression,seg_85,interpret the coefficient for the past bankruptcy variable in the model. is this coefficient significantly different from 0?
2313,1,"['model', 'rate', 'categorical', 'variables', 'slope', 'regression', 'variable', 'categorical predictor variables', 'predictor variables', 'coefficient', 'predictor']", Introduction to multiple regression,seg_85,"the bankruptcy variable takes one of two values: 1 when the borrower has a bankruptcy in their history and 0 otherwise. a slope of 0.74 means that the model predicts a 0.74% higher interest rate for those borrowers with a bankruptcy in their record. (see section 8.2.8 for a review of the interpretation for two-level categorical predictor variables.) examining the regression output in figure 9.3, we can see that the p-value for bankruptcy is very close to zero, indicating there is strong evidence the coefficient is different from zero when using this simple one-predictor model."
2314,1,"['model', 'levels', 'categorical', 'regression', 'variable', 'categorical variable', 'reference level', 'level']", Introduction to multiple regression,seg_85,"suppose we had fit a model using a 3-level categorical variable, such as income ver. the output from software is shown in figure 9.4. this regression output provides multiple rows for the income ver variable. each row represents the relative difference for each level of income ver. however, we are missing one of the levels: not (for not verified). the missing level is called the reference level, and it represents the default level that other levels are measured against."
2315,1,"['error', 'intercept']", Introduction to multiple regression,seg_85,estimate std. error t value pr(>|t|) (intercept) 11.0995 0.0809 137.18 <0.0001 income ver: source only 1.4160 0.1107 12.79 <0.0001 income ver: verified 3.2543 0.1297 25.09 <0.0001 df = 9998
2316,1,"['linear', 'model', 'levels', 'linear model', 'rate', 'results', 'regression', 'predictor']", Introduction to multiple regression,seg_85,"figure 9.4: summary of a linear model for predicting interest rate based on whether the borrower’s income source and amount has been verified. this predictor has three levels, which results in 2 rows in the regression output."
2317,1,"['model', 'regression model', 'regression']", Introduction to multiple regression,seg_85,how would we write an equation for this regression model?
2318,1,"['model', 'regression model', 'regression']", Introduction to multiple regression,seg_85,the equation for the regression model may be written as a model with two predictors:
2319,1,"['categorical', 'variables', 'variable', 'categorical variable', 'indicator variables', 'indicator']", Introduction to multiple regression,seg_85,"we use the notation variablelevel to represent indicator variables for when the categorical variable takes a particular value. for example, income versource only would take a value of 1 if income ver was source only for a loan, and it would take a value of 0 otherwise. likewise, income ververified would take a value of 1 if income ver took a value of verified and 0 if it took any other value."
2320,1,"['level', 'variable']", Introduction to multiple regression,seg_85,the notation used in example 9.2 may feel a bit confusing. let’s figure out how to use the equation for each level of the income ver variable.
2321,1,"['rate', 'model', 'average']", Introduction to multiple regression,seg_85,"using the model from example 9.2, compute the average interest rate for borrowers whose income source and amount are both unverified."
2322,1,"['functions', 'set', 'indicator']", Introduction to multiple regression,seg_85,"when income ver takes a value of not, then both indicator functions in the equation from example 9.2 are set to zero:"
2323,1,"['rate', 'levels', 'variable', 'level', 'coefficient', 'average']", Introduction to multiple regression,seg_85,"the average interest rate for these borrowers is 11.1%. because the not level does not have its own coefficient and it is the reference value, the indicators for the other levels for this variable all drop out."
2324,1,"['rate', 'model', 'average']", Introduction to multiple regression,seg_85,"using the model from example 9.2, compute the average interest rate for borrowers whose income source is verified but the amount is not."
2325,1,['variable'], Introduction to multiple regression,seg_85,"when income ver takes a value of source only, then the corresponding variable takes a value of 1 while the other (income ververified) is 0:"
2326,1,"['rate', 'average']", Introduction to multiple regression,seg_85,the average interest rate for these borrowers is 12.52%.
2327,1,"['rate', 'average']", Introduction to multiple regression,seg_85,compute the average interest rate for borrowers whose income source and amount are both verified.1
2328,1,"['model', 'levels', 'regression model', 'categorical', 'regression', 'variable', 'categorical variable', 'reference level', 'level', 'coefficient', 'coefficients']", Introduction to multiple regression,seg_85,"when fitting a regression model with a categorical variable that has k levels where k > 2, software will provide a coefficient for k − 1 of those levels. for the last level that does not receive a coefficient, this is the reference level, and the coefficients listed for the other levels are all considered relative to this reference level."
2329,1,['coefficients'], Introduction to multiple regression,seg_85,interpret the coefficients in the income ver model.2
2330,1,"['model', 'risk', 'rate', 'variables', 'data', 'set', 'confounding', 'data set', 'confounding variables']", Introduction to multiple regression,seg_85,"the higher interest rate for borrowers who have verified their income source or amount is surprising. intuitively, we’d think that a loan would look less risky if the borrower’s income has been verified. however, note that the situation may be more complex, and there may be confounding variables that we didn’t account for. for example, perhaps lender require borrowers with poor credit to verify their income. that is, verifying income in our data set might be a signal of some concerns about the borrower rather than a reassurance that the borrower will pay back the loan. for this reason, the borrower could be deemed higher risk, resulting in a higher interest rate. (what other confounding variables might explain this counter-intuitive relationship suggested by the model?)"
2331,1,['rate'], Introduction to multiple regression,seg_85,how much larger of an interest rate would we expect for a borrower who has verified their income source and amount vs a borrower whose income source has only been verified?3
2332,1,"['variables', 'model']", Introduction to multiple regression,seg_85,9.1.2 including and assessing many variables in a model
2333,1,"['rate', 'factors', 'data', 'regression', 'multiple regression', 'variable', 'statistical', 'observational data']", Introduction to multiple regression,seg_85,"the world is complex, and it can be helpful to consider many factors at once in statistical modeling. for example, we might like to use the full context of borrower to predict the interest rate they receive rather than using a single variable. this is the strategy used in multiple regression. while we remain cautious about making any causal interpretations using multiple regression on observational data, such models are a common first step in gaining insights or providing some evidence of a causal connection."
2334,1,"['model', 'variables', 'data', 'set', 'data set']", Introduction to multiple regression,seg_85,"we want to construct a model that accounts for not only for any past bankruptcy or whether the borrower had their income source or amount verified, but simultaneously accounts for all the variables in the data set: income ver, debt to income, credit util, bankruptcy, term, issued, and credit checks."
2335,1,"['categorical', 'variables', 'categorical variables', 'coefficients']", Introduction to multiple regression,seg_85,"this equation represents a holistic approach for modeling all of the variables simultaneously. notice that there are two coefficients for income ver and also two coefficients for issued, since both are 3-level categorical variables."
2336,1,"['parameters', 'estimate', 'residuals', 'case', 'predictor']", Introduction to multiple regression,seg_85,"we estimate the parameters β0, β1, β2, ..., β9 in the same way as we did in the case of a single predictor. we select b0, b1, b2, ..., b9 that minimize the sum of the squared residuals:"
2337,1,"['sample', 'model', 'estimated', 'estimates', 'residuals', 'case', 'observation', 'sum of squares', 'point estimates', 'rates']", Introduction to multiple regression,seg_85,"where yi and ŷi represent the observed interest rates and their estimated values according to the model, respectively. 10,000 residuals are calculated, one for each observation. we typically use a computer to minimize the sum of squares and compute point estimates, as shown in the sample output in figure 9.5. using this output, we identify the point estimates bi of each βi, just as we did in the one-predictor case."
2338,1,['error'], Introduction to multiple regression,seg_85,estimate std. error t value pr(>|t|)
2339,1,['intercept'], Introduction to multiple regression,seg_85,(intercept) 1.9251 0.2102 9.16 <0.0001 income ver: source only 0.9750 0.0991 9.83 <0.0001 income ver: verified 2.5374 0.1172 21.65 <0.0001 debt to income 0.0211 0.0029 7.18 <0.0001 credit util 4.8959 0.1619 30.24 <0.0001 bankruptcy 0.3864 0.1324 2.92 0.0035 term 0.1537 0.0039 38.96 <0.0001 issued: jan2018 0.0276 0.1081 0.26 0.7981 issued: mar2018 -0.0397 0.1065 -0.37 0.7093 credit checks 0.2282 0.0182 12.51 <0.0001 df = 9990
2340,1,"['model', 'rate', 'regression model', 'variables', 'regression', 'outcome']", Introduction to multiple regression,seg_85,"figure 9.5: output for the regression model, where interest rate is the outcome and the variables listed are the predictors."
2341,1,"['linear', 'model', 'regression model', 'linear model', 'regression', 'multiple regression']", Introduction to multiple regression,seg_85,"a multiple regression model is a linear model with many predictors. in general, we write the model as"
2342,1,"['parameters', 'estimate', 'statistical']", Introduction to multiple regression,seg_85,when there are k predictors. we always estimate the βi parameters using statistical software.
2343,1,"['model', 'regression model', 'estimates', 'regression', 'point estimates']", Introduction to multiple regression,seg_85,write out the regression model using the point estimates from figure 9.5. how many predictors are there in this model?
2344,1,"['rate', 'model']", Introduction to multiple regression,seg_85,the fitted model for the interest rate is given by:
2345,1,"['model', 'levels', 'regression model', 'categorical', 'regression', 'multiple regression', 'predictor', 'coefficients']", Introduction to multiple regression,seg_85,"if we count up the number of predictor coefficients, we get the effective number of predictors in the model: k = 9. notice that the issued categorical predictor counts as two, once for the two levels shown in the model. in general, a categorical predictor with p different levels will be represented by p− 1 terms in a multiple regression model."
2346,1,"['estimate', 'variable', 'point estimate', 'coefficient']", Introduction to multiple regression,seg_85,"what does β4, the coefficient of variable credit util, represent? what is the point estimate of β4?4"
2347,1,"['residual', 'observation']", Introduction to multiple regression,seg_85,compute the residual of the first observation in figure 9.1 on page 343 using the equation identified in guided practice 9.9.
2348,1,"['rate', 'residual', 'prediction', 'predicted']", Introduction to multiple regression,seg_85,"to compute the residual, we first need the predicted value, which we compute by plugging values into the equation from example 9.9. for example, income versource only takes a value of 0, income ververified takes a value of 1 (since the borrower’s income source and amount were verified), debt to income was 18.01, and so on. this leads to a prediction of r̂ate1 = 18.09. the observed interest rate was 14.07%, which leads to a residual of e1 = 14.07− 18.09 = −4.02."
2349,1,"['simple linear regression', 'linear', 'estimated', 'estimate', 'standard error', 'regression', 'multiple regression', 'linear regression', 'standard', 'coefficient', 'error']", Introduction to multiple regression,seg_85,we estimated a coefficient for bankruptcy in section 9.1.1 of b4 = 0.74 with a standard error of seb1 = 0.15 when using simple linear regression. why is there a difference between that estimate and the estimated coefficient of 0.39 in the multiple regression setting?
2350,1,"['simple linear regression', 'predictor', 'correlated', 'linear', 'bias', 'rate', 'data', 'confounding', 'model', 'regression', 'confounding variables', 'linear regression', 'outcome', 'control', 'estimated', 'variables']", Introduction to multiple regression,seg_85,"if we examined the data carefully, we would see that some predictors are correlated. for instance, when we estimated the connection of the outcome interest rate and predictor bankruptcy using simple linear regression, we were unable to control for other variables like whether the borrower had her income verified, the borrower’s debt-to-income ratio, and other variables. that original model was constructed in a vacuum and did not consider the full context. when we include all of the variables, underlying and unintentional bias that was missed by these other variables is reduced or eliminated. of course, bias can still exist from other confounding variables."
2351,1,"['collinear', 'correlation', 'predictor variables', 'predictor', 'correlated', 'data', 'multiple regression', 'model', 'estimation', 'regression', 'collinearity', 'observational data', 'variables', 'experiments']", Introduction to multiple regression,seg_85,"example 9.12 describes a common issue in multiple regression: correlation among predictor variables. we say the two predictor variables are collinear (pronounced as co-linear) when they are correlated, and this collinearity complicates model estimation. while it is impossible to prevent collinearity from arising in observational data, experiments are usually designed to prevent predictors from being collinear."
2352,1,"['estimated', 'variables', 'predicted', 'intercept', 'coefficient']", Introduction to multiple regression,seg_85,"the estimated value of the intercept is 1.925, and one might be tempted to make some interpretation of this coefficient, such as, it is the model’s predicted price when each of the variables take value zero: income source is not verified, the borrower has no debt (debt-to-income and credit utilization are zero), and so on. is this reasonable? is there any value gained by making this interpretation?5"
2353,1,"['multiple regression', 'adjusted', 'adjusted r2', 'regression']", Introduction to multiple regression,seg_85,9.1.3 adjusted r2 as a better tool for multiple regression
2354,1,"['model', 'response', 'variability']", Introduction to multiple regression,seg_85,we first used r2 in section 8.2 to determine the amount of variability in the response that was explained by the model:
2355,1,"['variability', 'residuals', 'outcome']", Introduction to multiple regression,seg_85,variability in residuals v ar(ei) r2 = 1− = 1− variability in the outcome v ar(yi)
2356,1,"['model', 'residuals', 'regression', 'multiple regression', 'outcomes']", Introduction to multiple regression,seg_85,"where ei represents the residuals of the model and yi the outcomes. this equation remains valid in the multiple regression framework, but a small enhancement can make it even more informative when comparing models."
2357,1,"['residuals', 'model', 'variance']", Introduction to multiple regression,seg_85,"the variance of the residuals for the model given in guided practice 9.9 is 18.53, and the variance of the total price in all the auctions is 25.01. calculate r2 for this model.6"
2358,1,"['sample', 'model', 'variability', 'variables', 'estimate', 'biased', 'data', 'variable', 'adjusted', 'adjusted r2']", Introduction to multiple regression,seg_85,"this strategy for estimating r2 is acceptable when there is just a single variable. however, it becomes less helpful when there are many variables. the regular r2 is a biased estimate of the amount of variability explained by the model when applied to a new sample of data. to get a better estimate, we use the adjusted r2."
2359,1,"['adjusted', 'adjusted r2']", Introduction to multiple regression,seg_85,the adjusted r2 is computed as
2360,1,"['model', 'levels', 'categorical', 'variables', 'cases', 'predictor variables', 'predictor']", Introduction to multiple regression,seg_85,where n is the number of cases used to fit the model and k is the number of predictor variables in the model. remember that a categorical predictor with p levels will contribute p− 1 to the number of variables in the model.
2361,1,"['degrees of freedom', 'bias', 'model', 'data', 'regression', 'multiple regression', 'adjusted', 'adjusted r2', 'associated', 'variance', 'predictions']", Introduction to multiple regression,seg_85,"because k is never negative, the adjusted r2 will be smaller – often times just a little smaller – than the unadjusted r2. the reasoning behind the adjusted r2 lies in the degrees of freedom associated with each variance, which is equal to n− k − 1 for the multiple regression context. if we were to make predictions for new data using our current model, we would find that the unadjusted r2 would tend to be slightly overly optimistic, while the adjusted r2 formula helps correct this bias."
2362,1,"['model', 'variables', 'data', 'set', 'data set', 'predictor variables', 'predictor', 'variances']", Introduction to multiple regression,seg_85,"there were n = 10000 auctions in the loans data set and k = 9 predictor variables in the model. use n, k, and the variances from guided practice 9.14 to calculate ra"
2363,1,['rate'], Introduction to multiple regression,seg_85,2dj for the interest rate model.7
2364,1,"['model', 'errors', 'adjusted', 'adjusted r2', 'predictor', 'variance']", Introduction to multiple regression,seg_85,"suppose you added another predictor to the model, but the variance of the errors v ar(ei) didn’t go down. what would happen to the r2? what would happen to the adjusted r2? 8"
2365,1,"['adjusted', 'model', 'predictor', 'adjusted r2']", Introduction to multiple regression,seg_85,"adjusted r2 could have been used in chapter 8. however, when there is only k = 1 predictors, adjusted r2 is very close to regular r2, so this nuance isn’t typically important when the model has only one predictor."
2366,1,"['error', 'intercept']", Introduction to multiple regression,seg_85,estimate std. error t value pr(>|t|) (intercept) -57.99 8.64 -6.71 0.00 height 0.34 0.13 2.61 0.01 diameter 4.71 0.26 17.82 0.00
2367,1,"['model', 'variables', 'model selection', 'variable', 'statistical', 'predictions']", Model selection,seg_87,"the best model is not always the most complicated. sometimes including variables that are not evidently important can actually reduce the accuracy of predictions. in this section, we discuss model selection strategies, which will help us eliminate variables from the model that are found to be less important. it’s common (and hip, at least in the statistical world) to refer to models that have undergone such variable pruning as parsimonious."
2368,1,"['model', 'variables', 'explanatory', 'full model']", Model selection,seg_87,"in practice, the model that includes all available explanatory variables is often referred to as the full model. the full model may not be the best model, and if it isn’t, we want to identify a smaller model that is preferable."
2369,1,"['variables', 'model']", Model selection,seg_87,9.2.1 identifying variables in the model that may not be helpful
2370,1,"['model', 'evaluating', 'outcomes']", Model selection,seg_87,"adjusted r2 describes the strength of a model fit, and it is a useful tool for evaluating which predictors are adding value to the model, where adding value means they are (likely) improving the accuracy in predicting future outcomes."
2371,1,"['model', 'table', 'variable', 'tables', 'full model']", Model selection,seg_87,"let’s consider two models, which are shown in tables 9.6 and 9.7. the first table summarizes the full model since it includes all predictors, while the second does not include the issued variable."
2372,1,['error'], Model selection,seg_87,estimate std. error t value pr(>|t|)
2373,1,['intercept'], Model selection,seg_87,(intercept) 1.9251 0.2102 9.16 <0.0001 income ver: source only 0.9750 0.0991 9.83 <0.0001 income ver: verified 2.5374 0.1172 21.65 <0.0001 debt to income 0.0211 0.0029 7.18 <0.0001 credit util 4.8959 0.1619 30.24 <0.0001 bankruptcy 0.3864 0.1324 2.92 0.0035 term 0.1537 0.0039 38.96 <0.0001 issued: jan2018 0.0276 0.1081 0.26 0.7981 issued: mar2018 -0.0397 0.1065 -0.37 0.7093 credit checks 0.2282 0.0182 12.51 <0.0001 ra
2374,1,"['full regression model', 'model', 'regression model', 'regression', 'adjusted', 'adjusted r2']", Model selection,seg_87,"figure 9.6: the fit for the full regression model, including the adjusted r2."
2375,1,['error'], Model selection,seg_87,estimate std. error t value pr(>|t|)
2376,1,['intercept'], Model selection,seg_87,(intercept) 1.9213 0.1982 9.69 <0.0001 income ver: source only 0.9740 0.0991 9.83 <0.0001 income ver: verified 2.5355 0.1172 21.64 <0.0001 debt to income 0.0211 0.0029 7.19 <0.0001 credit util 4.8958 0.1619 30.25 <0.0001 bankruptcy 0.3869 0.1324 2.92 0.0035 term 0.1537 0.0039 38.97 <0.0001 credit checks 0.2283 0.0182 12.51 <0.0001
2377,1,"['model', 'regression model', 'regression', 'variable']", Model selection,seg_87,figure 9.7: the fit for the regression model after dropping the issued variable.
2378,0,[], Model selection,seg_87,which of the two models is better?
2379,1,"['adjusted', 'model', 'adjusted r2']", Model selection,seg_87,we compare the adjusted r2 of each model to determine which to choose. since the first model has an ra
2380,1,['model'], Model selection,seg_87,"2dj smaller than the ra 2dj of the second model, we prefer the second model to the first."
2381,1,"['adjusted', 'model', 'adjusted r2']", Model selection,seg_87,"will the model without issued be better than the model with issued? we cannot know for sure, but based on the adjusted r2, this is our best assessment."
2382,1,"['model selection', 'model']", Model selection,seg_87,9.2.2 two model selection strategies
2383,1,"['model', 'regression model', 'variables', 'regression', 'model selection', 'variable', 'multiple regression', 'forward selection', 'backward elimination']", Model selection,seg_87,"two common strategies for adding or removing variables in a multiple regression model are called backward elimination and forward selection. these techniques are often referred to as stepwise model selection strategies, because they add or delete one variable at a time as they “step” through the candidate predictors."
2384,1,"['model', 'variables', 'variable', 'adjusted', 'adjusted r2', 'predictor variables', 'predictor']", Model selection,seg_87,backward elimination starts with the model that includes all potential predictor variables. variables are eliminated one-at-a-time from the model until we cannot improve the adjusted r2. the strategy within each elimination step is to eliminate the variable that leads to the largest improvement in adjusted r2.
2385,1,"['model', 'backward elimination', 'data', 'full model']", Model selection,seg_87,results corresponding to the full model for the loans data are shown in figure 9.6. how should we proceed under the backward elimination strategy?
2386,1,"['adjusted', 'model', 'adjusted r2', 'full model']", Model selection,seg_87,our baseline adjusted r2 from the full model is ra
2387,0,[], Model selection,seg_87,"2dj = 0.25843, and we need to determine whether"
2388,1,"['adjusted', 'predictor', 'adjusted r2']", Model selection,seg_87,"dropping a predictor will improve the adjusted r2. to check, we fit models that each drop a different predictor, and we record the adjusted r2:"
2389,0,[], Model selection,seg_87,exclude ... income ver debt to income credit util bankruptcy ra
2390,0,[], Model selection,seg_87,term issued credit checks ra
2391,1,"['adjusted', 'model', 'adjusted r2', 'full model']", Model selection,seg_87,"the model without issued has the highest adjusted r2 of 0.25854, higher than the adjusted r2 for the full model. because eliminating issued leads to a model with a higher adjusted r2, we drop issued from the model."
2392,1,"['adjusted', 'model', 'predictor', 'adjusted r2']", Model selection,seg_87,"since we eliminated a predictor from the model in the first step, we see whether we should eliminate any additional predictors. our baseline adjusted r2 is now ra"
2393,0,[], Model selection,seg_87,"2dj = 0.25854. we now fit new models, which consider eliminating each of the remaining predictors in addition to issued:"
2394,0,[], Model selection,seg_87,exclude issued and ... income ver debt to income credit util ra
2395,0,[], Model selection,seg_87,bankruptcy term credit checks ra
2396,1,"['model', 'adjusted', 'adjusted r2', 'backward elimination', 'coefficients']", Model selection,seg_87,"none of these models lead to an improvement in adjusted r2, so we do not eliminate any of the remaining predictors. that is, after backward elimination, we are left with the model that keeps all predictors except issued, which we can summarize using the coefficients from figure 9.7:"
2397,1,"['model', 'variables', 'adjusted', 'adjusted r2', 'forward selection', 'backward elimination']", Model selection,seg_87,"the forward selection strategy is the reverse of the backward elimination technique. instead of eliminating variables one-at-a-time, we add variables one-at-a-time until we cannot find any variables that improve the model (as measured by adjusted r2)."
2398,1,"['model', 'data', 'set', 'data set', 'forward selection']", Model selection,seg_87,construct a model for the loans data set using the forward selection strategy.
2399,1,"['model', 'variables', 'variable', 'adjusted', 'adjusted r2']", Model selection,seg_87,"we start with the model that includes no variables. then we fit each of the possible models with just one variable. that is, we fit the model including just income ver, then the model including just debt to income, then a model with just credit util, and so on. then we examine the adjusted r2 for each of these models:"
2400,0,[], Model selection,seg_87,add ... income ver debt to income credit util bankruptcy ra
2401,0,[], Model selection,seg_87,term issued credit checks ra
2402,1,"['adjusted', 'model', 'adjusted r2']", Model selection,seg_87,"in this first step, we compare the adjusted r2 against a baseline model that has no predictors. the no-predictors model always has ra"
2403,1,"['model', 'predictor']", Model selection,seg_87,2dj = 0. the model with one predictor that has the largest
2404,1,"['adjusted', 'model', 'predictor', 'adjusted r2']", Model selection,seg_87,"adjusted r2 is the model with the term predictor, and because this adjusted r2 is larger than the adjusted r2 from the model with no predictors (ra"
2405,1,"['variable', 'model']", Model selection,seg_87,"2dj = 0), we will add this variable to our model."
2406,1,['process'], Model selection,seg_87,"we repeat the process again, this time considering 2-predictor models where one of the predictors is term and with a new baseline of ra"
2407,0,[], Model selection,seg_87,add term and ... income ver debt to income credit util ra
2408,0,[], Model selection,seg_87,bankruptcy issued credit checks ra
2409,1,"['adjusted', 'model', 'predictor', 'adjusted r2']", Model selection,seg_87,"the best second predictor, credit util, has a higher adjusted r2 (0.20046) than the baseline (0.12855), so we also add credit util to the model."
2410,1,"['variable', 'model']", Model selection,seg_87,"since we have again added a variable to the model, we continue and see whether it would be beneficial to add a third variable:"
2411,0,[], Model selection,seg_87,"add term, credit util, and ... income ver debt to income ra"
2412,0,[], Model selection,seg_87,bankruptcy issued credit checks ra
2413,1,"['adjusted', 'model', 'adjusted r2']", Model selection,seg_87,"the model adding income ver improved adjusted r2 (0.24183 to 0.20046), so we add income ver to the model."
2414,1,['variable'], Model selection,seg_87,"we continue on in this way, next adding debt to income, then credit checks, and bankruptcy. at this point, we come again to the issued variable: adding this variable leads to ra"
2415,1,['variables'], Model selection,seg_87,while keeping all the other variables but excluding issued leads to a higher ra
2416,1,"['model', 'backward elimination']", Model selection,seg_87,"means we do not add issued. in this example, we have arrived at the same model that we identified from backward elimination."
2417,1,"['model', 'variables', 'forward selection']", Model selection,seg_87,"backward elimination begins with the model having the largest number of predictors and eliminates variables one-by-one until we are satisfied that all remaining variables are important to the model. forward selection starts with no variables included in the model, then it adds in variables according to their importance until no other important variables are found."
2418,1,"['model', 'forward selection']", Model selection,seg_87,"backward elimination and forward selection sometimes arrive at different final models. if trying both techniques and this happens, it’s common to choose the model with the larger ra"
2419,1,"['adjusted', 'adjusted r2']", Model selection,seg_87,"9.2.3 the p-value approach, an alternative to adjusted r2"
2420,0,[], Model selection,seg_87,the p-value may be used as an alternative to ra
2421,1,"['model selection', 'model']", Model selection,seg_87,2dj for model selection:
2422,1,['backward elimination'], Model selection,seg_87,"backward elimination with the p-value approach. in backward elimination, we would iden-"
2423,1,"['model', 'variable', 'predictor', 'level', 'significance', 'significance level']", Model selection,seg_87,"tify the predictor corresponding to the largest p-value. if the p-value is above the significance level, usually α = 0.05, then we would drop that variable, refit the model, and repeat the process. if the largest p-value is less than α = 0.05, then we would not eliminate any predictors and the current model would be our best-fitting model."
2424,1,['forward selection'], Model selection,seg_87,"forward selection with the p-value approach. in forward selection with p-values, we reverse"
2425,1,"['model', 'variables', 'predictor', 'process']", Model selection,seg_87,"the process. we begin with a model that has no predictors, then we fit a model for each possible predictor, identifying the model where the corresponding predictor’s p-value is smallest. if that p-value is smaller than α = 0.05, we add it to the model and repeat the process, considering whether to add more variables one-at-a-time. when none of the remaining predictors can be added to the model and have a p-value less than 0.05, then we stop adding variables and the current model would be our best-fitting model."
2426,1,"['model', 'variables', 'variable', 'backward elimination']", Model selection,seg_87,"examine figure 9.7 on page 353, which considers the model including all variables except the variable for the month the loan was issued. if we were using the p-value approach with backward elimination and we were considering this model, which of these variables would be up for elimination? would we drop that variable, or would we keep it in the model?12"
2427,1,"['adjusted', 'adjusted r2']", Model selection,seg_87,"while the adjusted r2 and p-value approaches are similar, they sometimes lead to different models, with the ra"
2428,1,['model'], Model selection,seg_87,2dj approach tending to include more predictors in the final model.
2429,1,"['prediction', 'case', 'machine learning']", Model selection,seg_87,"when the sole goal is to improve prediction accuracy, use ra 2dj . this is commonly the case in machine learning applications."
2430,1,"['model', 'variables', 'prediction', 'statistically significant', 'response']", Model selection,seg_87,"when we care about understanding which variables are statistically significant predictors of the response, or if there is interest in producing a simpler model at the potential cost of a little prediction accuracy, then the p-value approach is preferred."
2431,1,"['model', 'variable', 'forward selection']", Model selection,seg_87,"regardless of whether you use ra 2dj or the p-value approach, or if you use the backward elimination of forward selection strategy, our job is not done after variable selection. we must still verify the model conditions are reasonable."
2432,1,"['model', 'regression']", Checking model conditions using graphs,seg_89,multiple regression methods using the model
2433,0,[], Checking model conditions using graphs,seg_89,generally depend on the following four conditions:
2434,1,"['model', 'residuals', 'data', 'sets', 'normal', 'data sets']", Checking model conditions using graphs,seg_89,"1. the residuals of the model are nearly normal (less important for larger data sets),"
2435,1,"['residuals', 'variability']", Checking model conditions using graphs,seg_89,"2. the variability of the residuals is nearly constant,"
2436,1,"['residuals', 'independent']", Checking model conditions using graphs,seg_89,"3. the residuals are independent, and"
2437,1,"['variable', 'outcome']", Checking model conditions using graphs,seg_89,4. each variable is linearly related to the outcome.
2438,1,"['plots', 'diagnostic plots']", Checking model conditions using graphs,seg_89,9.3.1 diagnostic plots
2439,1,"['model', 'plots', 'data']", Checking model conditions using graphs,seg_89,"diagnostic plots can be used to check each of these conditions. we will consider the model from the lending club loans data, and check whether there are any notable concerns:"
2440,1,"['outliers', 'residuals', 'distribution', 'normal']", Checking model conditions using graphs,seg_89,"check for outliers. in theory, the distribution of the residuals should be nearly normal; in prac-"
2441,1,"['outliers', 'histogram', 'observations', 'residuals', 'case', 'data', 'normality', 'set', 'data set']", Checking model conditions using graphs,seg_89,"tice, normality can be relaxed for most applications. instead, we examine a histogram of the residuals to check if there are any outliers: figure 9.8 is a histogram of these outliers. since this is a very large data set, only particularly extreme observations would be a concern in this particular case. there are no extreme observations that might cause a concern."
2442,1,"['prediction intervals', 'prediction', 'observations', 'residuals', 'intervals', 'normal']", Checking model conditions using graphs,seg_89,"if we intended to construct what are called prediction intervals for future observations, we would be more strict and require the residuals to be nearly normal. prediction intervals are further discussed in an online extra on the openintro website:"
2443,1,"['linear', 'linear regression', 'regression']", Checking model conditions using graphs,seg_89,www.openintro.org/d?id=stat extra linear regression supp
2444,1,['residuals'], Checking model conditions using graphs,seg_89,2000 1500 ycneuq 1000 erf 500 0 −10% −5% 0% 5% 10% 15% 20% residuals
2445,1,"['histogram', 'residuals']", Checking model conditions using graphs,seg_89,figure 9.8: a histogram of the residuals.
2446,1,"['plot', 'absolute value', 'residuals', 'fitted values']", Checking model conditions using graphs,seg_89,absolute values of residuals against fitted values. a plot of the absolute value of the resid-
2447,1,"['plot', 'condition', 'variability', 'residuals', 'fitted values', 'variance']", Checking model conditions using graphs,seg_89,"uals against their corresponding fitted values (ŷi) is shown in figure 9.9. this plot is helpful to check the condition that the variance of the residuals is approximately constant, and a smoothed line has been added to represent the approximate trend in this plot. there is more evident variability for fitted values that are larger, which we’ll discuss further."
2448,1,['fitted values'], Checking model conditions using graphs,seg_89,s la u 15% d is er f o 10% e ul av et lu 5% o sb a 0% 10 15 20 fitted values
2449,1,"['absolute value', 'residuals', 'deviations', 'fitted values', 'variance']", Checking model conditions using graphs,seg_89,figure 9.9: comparing the absolute value of the residuals against the fitted values (ŷi) is helpful in identifying deviations from the constant variance assumption.
2450,1,"['plot', 'observations', 'data', 'data collection']", Checking model conditions using graphs,seg_89,residuals in order of their data collection. this type of plot can be helpful when observations
2451,1,"['plot', 'data', 'cases', 'set', 'data set', 'time series']", Checking model conditions using graphs,seg_89,"were collected in a sequence. such a plot is helpful in identifying any connection between cases that are close to one another. the loans in this data set were issued over a 3 month period, and the month the loan was issued was not found to be important, suggesting this is not a concern for this data set. in cases where a data set does show some pattern for this check, time series methods may be useful."
2452,1,"['plot', 'predictor variable', 'residuals', 'variable', 'predictor']", Checking model conditions using graphs,seg_89,residuals against each predictor variable. we consider a plot of the residuals against each of
2453,1,"['variability', 'plots', 'data', 'box plots', 'outcomes', 'numerical']", Checking model conditions using graphs,seg_89,"the predictors in figure 9.10. for those instances where there are only 2-3 groups, box plots are shown. for the numerical outcomes, a smoothed line has been fit to the data to make it easier to review. ultimately, we are looking for any notable change in variability between groups or pattern in the data."
2454,1,['plots'], Checking model conditions using graphs,seg_89,here are the things of importance from these plots:
2455,1,['variability'], Checking model conditions using graphs,seg_89,• there is some minor differences in variability between the verified income groups.
2456,1,"['observations', 'skewed', 'variable']", Checking model conditions using graphs,seg_89,• there is a very clear pattern for the debt-to-income variable. what also stands out is that this variable is very strongly right skewed: there are few observations with very high debt-to-income ratios.
2457,1,"['curve', 'plots']", Checking model conditions using graphs,seg_89,• the downward curve on the right side of the credit utilization and credit check plots suggests some minor misfitting for those larger values.
2458,1,"['model', 'plots', 'diagnostic plots']", Checking model conditions using graphs,seg_89,"having reviewed the diagnostic plots, there are two options. the first option is to, if we’re not concerned about the issues observed, use this as the final model; if going this route, it is important to still note any abnormalities observed in the diagnostics. the second option is to try to improve the model, which is what we’ll try to do with this particular model fit."
2459,1,"['variability', 'plots', 'diagnostic plots', 'residuals', 'data', 'box plots', 'numerical']", Checking model conditions using graphs,seg_89,"figure 9.10: diagnostic plots for residuals against each of the predictors. for the box plots, we’re looking for notable differences in variability. for numerical predictors, we also check for trends or other structure in the data."
2460,1,['model'], Checking model conditions using graphs,seg_89,9.3.2 options for improving the model fit
2461,1,"['model', 'variability', 'variables', 'outcome', 'nonlinear', 'transforming']", Checking model conditions using graphs,seg_89,"there are several options for improvement of a model, including transforming variables, seeking out additional variables to fill model gaps, or using more advanced methods that would account for challenges around inconsistent variability or nonlinear relationships between predictors and the outcome."
2462,1,"['model', 'predictor variable', 'nonlinear', 'variable', 'outcome', 'predictor']", Checking model conditions using graphs,seg_89,"the main concern for the initial model is that there is a notable nonlinear relationship between the debt-to-income variable observed in figure 9.10. to resolve this issue, we’re going to consider a couple strategies for adjusting the relationship between the predictor variable and the outcome."
2463,1,"['histogram', 'skewed', 'leverage', 'variable']", Checking model conditions using graphs,seg_89,"let’s start by taking a look at a histogram of debt to income in figure 9.11. the variable is extremely skewed, and upper values will have a lot of leverage on the fit. below are several options:"
2464,1,['transformation'], Checking model conditions using graphs,seg_89,"• log transformation (log x),"
2465,1,['transformation'], Checking model conditions using graphs,seg_89,"• square root transformation (√x),"
2466,1,['transformation'], Checking model conditions using graphs,seg_89,"• inverse transformation (1/x),"
2467,0,[], Checking model conditions using graphs,seg_89,• truncation (cap the max value possible)
2468,1,"['observations', 'data', 'transformations', 'variable', 'transformation']", Checking model conditions using graphs,seg_89,"if we inspected the data more closely, we’d observe some instances where the variable takes a value of 0, and since log(0) and 1/x are undefined when x = 0, we’ll exclude these transformations from further consideration.13 a square root transformation is valid for all values the variable takes, and truncating some of the larger observations is also a valid approach. we’ll consider both of these approaches."
2469,1,"['histogram', 'skew']", Checking model conditions using graphs,seg_89,"figure 9.11: histogram of debt to income, where extreme skew is evident."
2470,1,"['transformed', 'variables', 'variable', 'transforming']", Checking model conditions using graphs,seg_89,"to try transforming the variable, we make two new variables representing the transformed versions:"
2471,1,['variable'], Checking model conditions using graphs,seg_89,"square root. we create a new variable, sqrt debt to income, where all the values are simply the"
2472,1,['model'], Checking model conditions using graphs,seg_89,"square roots of the values in debt to income, and then refit the model as before. the result is shown in the left panel of figure 9.12. the square root pulled in the higher values a bit, but the fit still doesn’t look great since the smoothed line is still wavy."
2473,1,['variable'], Checking model conditions using graphs,seg_89,"truncate at 50. we create a new variable, debt to income 50, where any values in debt to"
2474,1,"['plot', 'model', 'variable']", Checking model conditions using graphs,seg_89,"income that are greater than 50 are shrunk to exactly 50. refitting the model once more, the diagnostic plot for this new variable is shown in the right panel of figure 9.12. here the fit looks much more reasonable, so this appears to be a reasonable approach."
2475,1,"['results', 'transformations', 'cases', 'transformation']", Checking model conditions using graphs,seg_89,"the downside of using transformations is that it reduces the ease of interpreting the results. fortunately, since the truncation transformation only affects a relatively small number of cases, the interpretation isn’t dramatically impacted."
2476,1,"['histogram', 'skew']", Checking model conditions using graphs,seg_89,"figure 9.12: histogram of debt to income, where extreme skew is evident."
2477,1,"['model', 'variance', 'control']", Checking model conditions using graphs,seg_89,"as a next step, we’d evaluate the new model using the truncated version of debt to income, we would complete all the same procedures as before. the other two issues noted while inspecting diagnostics in section 9.3.1 are still present in the updated model. if we choose to report this model, we would want to also discuss these shortcomings to be transparent in our work. depending on what the model will be used, we could either try to bring those under control, or we could stop since those issues aren’t severe. had the non-constant variance been a little more dramatic, it would be a higher priority. ultimately we decided that the model was reasonable, and we report its final form here:"
2478,1,"['high leverage', 'model', 'variable', 'coefficient', 'leverage']", Checking model conditions using graphs,seg_89,"a sharp eye would notice that the coefficient for debt to income 50 is more than twice as large as what the coefficient had been for the debt to income variable in the earlier model. this suggests those larger values not only were points with high leverage, but they were influential points that were dramatically impacting the coefficient."
2479,1,['model'], Checking model conditions using graphs,seg_89,"the truth is that no model is perfect. however, even imperfect models can be useful. reporting a flawed model can be reasonable so long as we are clear and report the model’s shortcomings."
2480,1,"['model', 'results', 'interaction', 'nonlinear', 'data', 'nonlinear curves', 'interaction terms', 'statistical']", Checking model conditions using graphs,seg_89,"don’t report results when conditions are grossly violated. while there is a little leeway in model conditions, don’t go too far. if model conditions are very clearly violated, consider a new model, even if it means learning more statistical methods or hiring someone who can help. to help you get started, we’ve developed a couple additional sections that you may find on openintro’s website. these sections provide a light introduction to what are called interaction terms and to fitting nonlinear curves to data, respectively:"
2481,1,['interaction'], Checking model conditions using graphs,seg_89,www.openintro.org/d?file=stat extra interaction effects
2482,1,['nonlinear'], Checking model conditions using graphs,seg_89,www.openintro.org/d?file=stat extra nonlinear relationships
2483,1,"['residuals', 'fitted values']", Checking model conditions using graphs,seg_89,300 40 250 200 slau 0 150 di se 100 r 50 −40 0 −60 −40 −20 0 20 40 60 80 120 160 residuals fitted values 40 40 s s l l a au 0 u 0 d d i is se er r −40 −40 150 200 250 300 350 0 400 800 1200 length of gestation order of collection 40 40 s s l l a a u 0 u 0 d d i i s s e e r r −40 −40 0 1 55 60 65 70 parity height of mother 40 40 s s l l a a u 0 u 0 d d i i s s e e r r −40 −40 100 150 200 250 0 1 weight of mother smoke
2484,0,[], Checking model conditions using graphs,seg_89,is comedy er drama 200 25 horror
2485,1,['fitted values'], Checking model conditions using graphs,seg_89,residuals fitted values
2486,0,[], Checking model conditions using graphs,seg_89,action adventure comedy drama horror
2487,0,[], Checking model conditions using graphs,seg_89,genre order of collection 80 60 slaud 40 iser 20 0 2010 2012 2014 2016 2018 release year
2488,1,"['variables', 'regression', 'multiple regression', 'variable', 'associated', 'outcome', 'average']", Multiple regression case study Mario Kart,seg_91,"we’ll consider ebay auctions of a video game called mario kart for the nintendo wii. the outcome variable of interest is the total price of an auction, which is the highest bid plus the shipping cost. we will try to determine how total price is related to each characteristic in an auction while simultaneously controlling for other variables. for instance, all other characteristics held constant, are longer auctions associated with higher or lower prices? and, on average, how much more do buyers tend to pay for additional wii wheels (plastic steering wheels that attach to the wii controller) in auctions? multiple regression will help us answer these and other questions."
2489,1,"['model', 'data', 'set', 'data set', 'full model']", Multiple regression case study Mario Kart,seg_91,9.4.1 data set and the full model
2490,1,"['condition', 'variables', 'observations', 'results', 'data', 'variable', 'set', 'data set', 'indicator variables', 'indicator']", Multiple regression case study Mario Kart,seg_91,"the mariokart data set includes results from 141 auctions. four observations from this data set are shown in figure 9.13, and descriptions for each variable are shown in figure 9.14. notice that the condition and stock photo variables are indicator variables, similar to bankruptcy in the loan data set."
2491,1,"['observations', 'data', 'set', 'data set']", Multiple regression case study Mario Kart,seg_91,figure 9.13: four observations from the mariokart data set.
2492,1,"['variables', 'data', 'set', 'data set']", Multiple regression case study Mario Kart,seg_91,figure 9.14: variables and their descriptions for the mariokart data set.
2493,1,"['linear', 'model', 'regression model', 'condition', 'results', 'regression', 'linear regression', 'predictor', 'linear regression model']", Multiple regression case study Mario Kart,seg_91,we fit a linear regression model with the game’s condition as a predictor of auction price. results of this model are summarized below:
2494,1,['error'], Multiple regression case study Mario Kart,seg_91,estimate std. error t value pr(>|t|)
2495,1,['intercept'], Multiple regression case study Mario Kart,seg_91,(intercept) 42.8711 0.8140 52.67 <0.0001 cond new 10.8996 1.2583 8.66 <0.0001 df = 139
2496,1,"['model', 'slope']", Multiple regression case study Mario Kart,seg_91,"write down the equation for the model, note whether the slope is statistically different from zero,"
2497,1,['coefficient'], Multiple regression case study Mario Kart,seg_91,15 and interpret the coefficient.
2498,1,"['model', 'predictor variable', 'variables', 'variable', 'predictor variables', 'outcome', 'predictor']", Multiple regression case study Mario Kart,seg_91,"sometimes there are underlying structures or relationships between predictor variables. for instance, new games sold on ebay tend to come with more wii wheels, which may have led to higher prices for those auctions. we would like to fit a model that includes all potentially important variables simultaneously. this would help us evaluate the relationship between a predictor variable and the outcome while controlling for the potential influence of other variables."
2499,1,"['variables', 'model', 'condition']", Multiple regression case study Mario Kart,seg_91,"we want to construct a model that accounts for not only the game condition, as in guided practice 9.21, but simultaneously accounts for three other variables:"
2500,1,"['model', 'estimates', 'point estimates', 'coefficient', 'full model']", Multiple regression case study Mario Kart,seg_91,"figure 9.15 summarizes the full model. using this output, we identify the point estimates of each coefficient."
2501,1,['error'], Multiple regression case study Mario Kart,seg_91,estimate std. error t value pr(>|t|)
2502,1,['intercept'], Multiple regression case study Mario Kart,seg_91,(intercept) 36.2110 1.5140 23.92 <0.0001 cond new 5.1306 1.0511 4.88 <0.0001 stock photo 1.0803 1.0568 1.02 0.3085 duration -0.0268 0.1904 -0.14 0.8882 wheels 7.2852 0.5547 13.13 <0.0001 df = 136
2503,1,"['model', 'regression model', 'regression', 'outcome']", Multiple regression case study Mario Kart,seg_91,"figure 9.15: output for the regression model where price is the outcome and cond new, stock photo, duration, and wheels are the predictors."
2504,1,"['estimates', 'point estimates']", Multiple regression case study Mario Kart,seg_91,write out the model’s equation using the point estimates from figure 9.15. how many predictors are there in this model?16
2505,1,"['estimate', 'variable', 'point estimate', 'coefficient']", Multiple regression case study Mario Kart,seg_91,"what does β4, the coefficient of variable x4 (wii wheels), represent? what is the point estimate of β4?17"
2506,1,"['residual', 'observation']", Multiple regression case study Mario Kart,seg_91,compute the residual of the first observation in figure 9.13 using the equation identified in guided
2507,1,"['simple linear regression', 'linear', 'estimated', 'estimate', 'standard error', 'regression', 'multiple regression', 'linear regression', 'standard', 'coefficient', 'error']", Multiple regression case study Mario Kart,seg_91,we estimated a coefficient for cond new in section 9.21 of b1 = 10.90 with a standard error of seb1 = 1.26 when using simple linear regression. why might there be a difference between that estimate and the one in the multiple regression setting?
2508,1,"['simple linear regression', 'predictor', 'linear', 'bias', 'biased', 'data', 'confounding', 'model', 'regression', 'confounding variables', 'linear regression', 'outcome', 'control', 'collinearity', 'estimated', 'variables', 'confounding variable', 'variable']", Multiple regression case study Mario Kart,seg_91,"if we examined the data carefully, we would see that there is collinearity among some predictors. for instance, when we estimated the connection of the outcome price and predictor cond new using simple linear regression, we were unable to control for other variables like the number of wii wheels included in the auction. that model was biased by the confounding variable wheels. when we use both variables, this particular underlying and unintentional bias is reduced or eliminated (though bias from other confounding variables may still remain)."
2509,1,"['model selection', 'model']", Multiple regression case study Mario Kart,seg_91,9.4.2 model selection
2510,1,"['model selection', 'model', 'full model']", Multiple regression case study Mario Kart,seg_91,let’s revisit the model for the mario kart auction and complete model selection using backwards selection. recall that the full model took the following form:
2511,1,"['model', 'variables', 'data', 'full model']", Multiple regression case study Mario Kart,seg_91,"results corresponding to the full model for the mariokart data were shown in figure 9.15 on the facing page. for this model, we consider what would happen if dropping each of the variables in the model:"
2512,0,[], Multiple regression case study Mario Kart,seg_91,exclude ... cond new stock photo duration wheels ra
2513,1,"['model', 'full model']", Multiple regression case study Mario Kart,seg_91,"for the full model, ra"
2514,1,['backward elimination'], Multiple regression case study Mario Kart,seg_91,2dj = 0.7108. how should we proceed under the backward elimination strategy?
2515,1,['model'], Multiple regression case study Mario Kart,seg_91,the third model without duration has the highest ra
2516,1,['model'], Multiple regression case study Mario Kart,seg_91,full model. because eliminating duration leads to a model with a higher ra
2517,0,[], Multiple regression case study Mario Kart,seg_91,"2dj , we drop duration"
2518,1,['model'], Multiple regression case study Mario Kart,seg_91,from the model.
2519,1,"['variable', 'model']", Multiple regression case study Mario Kart,seg_91,"in example 9.26, we eliminated the duration variable, which resulted in a model with ra"
2520,1,"['variable', 'model']", Multiple regression case study Mario Kart,seg_91,let’s look at if we would eliminate another variable from the model using backwards elimination:
2521,0,[], Multiple regression case study Mario Kart,seg_91,exclude duration and ... cond new stock photo wheels ra
2522,1,['variable'], Multiple regression case study Mario Kart,seg_91,"should we eliminate any additional variable, and if so, which variable should we eliminate?19"
2523,1,['model'], Multiple regression case study Mario Kart,seg_91,"after eliminating the auction’s duration from the model, we are left with the following reduced model:"
2524,1,['data'], Multiple regression case study Mario Kart,seg_91,"how much would you predict for the total price for the mario kart game if it was used, used a stock photo, and included two wheels and put up for auction during the time period that the mario kart data were collected?20"
2525,0,[], Multiple regression case study Mario Kart,seg_91,would you be surprised if the seller from guided practice 9.28 didn’t get the exact price predicted?21
2526,1,['model'], Multiple regression case study Mario Kart,seg_91,9.4.3 checking model conditions using graphs
2527,1,['model'], Multiple regression case study Mario Kart,seg_91,let’s take a closer look at the diagnostics for the mario kart model to check if the model we have identified is reasonable.
2528,1,"['outliers', 'histogram', 'residuals', 'data', 'set', 'data set']", Multiple regression case study Mario Kart,seg_91,check for outliers. a histogram of the residuals is shown in figure 9.16. with a data set well
2529,1,"['outliers', 'data', 'outlier', 'set', 'data set']", Multiple regression case study Mario Kart,seg_91,"over a hundred, we’re primarily looking for major outliers. while one minor outlier appears on the upper end, it is not a concern for this large of a data set."
2530,1,['residuals'], Multiple regression case study Mario Kart,seg_91,30 25 yc 20 neu 15 qer 10 f 5 0 −10 −5 0 5 10 15 residuals
2531,1,"['outliers', 'histogram', 'residuals']", Multiple regression case study Mario Kart,seg_91,figure 9.16: histogram of the residuals. no clear outliers are evident.
2532,1,"['plot', 'absolute value', 'residuals', 'fitted values']", Multiple regression case study Mario Kart,seg_91,absolute values of residuals against fitted values. a plot of the absolute value of the resid-
2533,1,"['variance', 'fitted values', 'deviations']", Multiple regression case study Mario Kart,seg_91,uals against their corresponding fitted values (ŷi) is shown in figure 9.17. we don’t see any obvious deviations from constant variance in this example.
2534,1,"['plot', 'residuals', 'data', 'data collection']", Multiple regression case study Mario Kart,seg_91,residuals in order of their data collection. a plot of the residuals in the order their corre-
2535,0,[], Multiple regression case study Mario Kart,seg_91,sponding auctions were observed is shown in figure 9.18. here we see no structure that indicates a problem.
2536,1,"['plot', 'predictor variable', 'residuals', 'variable', 'predictor']", Multiple regression case study Mario Kart,seg_91,residuals against each predictor variable. we consider a plot of the residuals against the
2537,1,"['condition', 'variability', 'plots', 'residuals', 'variable']", Multiple regression case study Mario Kart,seg_91,"cond new variable, the residuals against the stock photo variable, and the residuals against the wheels variable. these plots are shown in figure 9.19. for the two-level condition variable, we are guaranteed not to see any remaining trend, and instead we are checking that the variability doesn’t fluctuate across groups, which it does not. however, looking at the stock"
2538,1,['fitted values'], Multiple regression case study Mario Kart,seg_91,$0 ● $35 $40 $45 $50 $55 $60 $65 fitted values
2539,1,"['residuals', 'fitted values', 'absolute value']", Multiple regression case study Mario Kart,seg_91,figure 9.17: absolute value of the residuals against the fitted values. no patterns are evident.
2540,0,[], Multiple regression case study Mario Kart,seg_91,−$10 ● 0 20 40 60 80 100 120 140 order of collection
2541,1,"['observations', 'residuals']", Multiple regression case study Mario Kart,seg_91,figure 9.18: residuals in the order that their corresponding observations were collected. there are no evident patterns.
2542,1,"['linear', 'variability', 'residuals', 'variable']", Multiple regression case study Mario Kart,seg_91,"photo variable, we find that there is some difference in the variability of the residuals in the two groups. additionally, when we consider the residuals against the wheels variable, we see some possible structure. there appears to be curvature in the residuals, indicating the relationship is probably not linear."
2543,1,"['model', 'results', 'nonlinear', 'data', 'case', 'information', 'variable', 'variance']", Multiple regression case study Mario Kart,seg_91,"as with the loans analysis, we would summarize diagnostics when reporting the model results. in the case of this auction data, we would report that there appears to be non-constant variance in the stock photo variable and that there may be a nonlinear relationship between the total price and the number of wheels included for an auction. this information would be important to buyers and sellers who may review the analysis, and omitting this information could be a setback to the very people who the model might assist."
2544,0,[], Multiple regression case study Mario Kart,seg_91,note: there are no exercises for this section.
2545,1,"['plot', 'condition', 'variability', 'variables', 'residuals', 'case', 'distribution', 'variable', 'numerical']", Multiple regression case study Mario Kart,seg_91,"figure 9.19: for the condition and stock photo variables, we check for differences in the distribution shape or variability of the residuals. in the case of the stock photos variable, we see a little less variability in the unique photo group than the stock photo group. for numerical predictors, we also check for trends or other structure. we see some slight bowing in the residuals against the wheels variable in the bottom plot."
2546,1,"['levels', 'residuals', 'linear', 'logistic', 'multiple regression', 'glm', 'model', 'linear model', 'categorical', 'response variables', 'logistic regression', 'generalized linear model', 'regression', 'distribution', 'response', 'variables', 'variable', 'normal', 'response variable', 'normal distribution']", Introduction to logistic regression,seg_93,"in this section we introduce logistic regression as a tool for building models when there is a categorical response variable with two levels, e.g. yes and no. logistic regression is a type of generalized linear model (glm) for response variables where regular multiple regression does not work very well. in particular, the response variable in these settings often takes a form where residuals look completely different from the normal distribution."
2547,1,"['poisson', 'glm', 'model', 'regression', 'distribution', 'variable', 'probability distribution', 'multiple regression', 'response', 'binomial', 'probability', 'parameter', 'poisson distribution', 'response variable']", Introduction to logistic regression,seg_93,"glms can be thought of as a two-stage modeling approach. we first model the response variable using a probability distribution, such as the binomial or poisson distribution. second, we model the parameter of the distribution using a collection of predictors and a special form of multiple regression. ultimately, the application of a glm will feel very similar to multiple regression, even if some of the details are different."
2548,1,['data'], Introduction to logistic regression,seg_93,9.5.1 resume data
2549,1,"['factors', 'experiment', 'data', 'set', 'data set', 'rates']", Introduction to logistic regression,seg_93,"we will consider experiment data from a study that sought to understand the effect of race and sex on job application callback rates; details of the study and a link to the data set may be found in appendix b.9. to evaluate which factors were important, job postings were identified in boston and chicago for the study, and researchers created many fake resumes to send off to these jobs to see which would elicit a callback. the researchers enumerated important characteristics, such as years of experience and education details, and they used these characteristics to randomly generate the resumes. finally, they randomly assigned a name to each resume, where the name would imply the applicant’s sex and race."
2550,1,"['association', 'experiment', 'set', 'associated', 'test']", Introduction to logistic regression,seg_93,"the first names that were used and randomly assigned in this experiment were selected so that they would predominantly be recognized as belonging to black or white individuals; other races were not considered in this study. while no name would definitively be inferred as pertaining to a black individual or to a white individual, the researchers conducted a survey to check for racial association of the names; names that did not pass this survey check were excluded from usage in the experiment. you can find the full set of names that did pass the survey test and were ultimately used in the study in figure 9.20. for example, lakisha was a name that their survey indicated would be interpreted as a black woman, while greg was a name that would generally be interpreted to be associated with a white male."
2551,1,['associated'], Introduction to logistic regression,seg_93,figure 9.20: list of all 36 unique names along with the commonly inferred race and sex associated with these names.
2552,1,"['factors', 'variables', 'states', 'variable', 'set', 'response', 'protected classes', 'response variable']", Introduction to logistic regression,seg_93,"the response variable of interest is whether or not there was a callback from the employer for the applicant, and there were 8 attributes that were randomly assigned that we’ll consider, with special interest in the race and sex variables. race and sex are protected classes in the united states, meaning they are not legally permitted factors for hiring or employment decisions. the full set of attributes considered is provided in figure 9.21."
2553,1,"['variables', 'data', 'variable', 'set', 'data set', 'indicator variables', 'indicator']", Introduction to logistic regression,seg_93,"figure 9.21: descriptions for the callback variable along with 8 other variables in the resume data set. many of the variables are indicator variables, meaning they take the value 1 if the specified characteristic is present and 0 otherwise."
2554,1,"['rate', 'experimental', 'variables', 'statistically significant', 'variable']", Introduction to logistic regression,seg_93,"all of the attributes listed on each resume were randomly assigned. this means that no attributes that might be favorable or detrimental to employment would favor one demographic over another on these resumes. importantly, due to the experimental nature of this study, we can infer causation between these variables and the callback rate, if the variable is statistically significant. our analysis will allow us to compare the practical importance of each of the variables relative to each other."
2555,1,"['probability of an event', 'probability', 'event']", Introduction to logistic regression,seg_93,9.5.2 modeling the probability of an event
2556,1,"['observation', 'probability', 'predictor variables', 'predictor', 'rates', 'linear', 'categorical variable', 'model', 'linear model', 'categorical', 'generalized linear model', 'regression', 'level', 'outcome', 'variables', 'variable']", Introduction to logistic regression,seg_93,"logistic regression is a generalized linear model where the outcome is a two-level categorical variable. the outcome, yi, takes the value 1 (in our application, this represents a callback for the resume) with probability pi and the value 0 with probability 1− pi. because each observation has a slightly different context, e.g. different education level or a different number of years of experience, the probability pi will differ for each observation. ultimately, it is this probability that we model in relation to the predictor variables: we will examine which resume characteristics correspond to higher or lower callback rates."
2557,1,"['glm', 'observation', 'variable', 'outcome']", Introduction to logistic regression,seg_93,"the outcome variable for a glm is denoted by yi, where the index i is used to represent observation i. in the resume application, yi will be used to represent whether resume i received a callback (yi = 1) or not (yi = 0)."
2558,1,"['variables', 'observation', 'variable', 'predictor variables', 'predictor']", Introduction to logistic regression,seg_93,"the predictor variables are represented as follows: x1,i is the value of variable 1 for observation i, x2,i is the value of variable 2 for observation i, and so on."
2559,1,"['model', 'logistic', 'regression model', 'logistic regression', 'logistic regression model', 'regression', 'multiple regression', 'probability']", Introduction to logistic regression,seg_93,"the logistic regression model relates the probability a resume would receive a callback (pi) to the predictors x1,i, x2,i, ..., xk,i through a framework much like that of multiple regression:"
2560,1,"['range', 'logit transformation', 'logit', 'transformation']", Introduction to logistic regression,seg_93,"we want to choose a transformation in the equation that makes practical and mathematical sense. for example, we want a transformation that makes the range of possibilities on the left hand side of the equation equal to the range of possibilities for the right hand side; if there was no transformation for this equation, the left hand side could only take values between 0 and 1, but the right hand side could take values outside of this range. a common transformation for pi is the logit transformation, which may be written as"
2561,0,[], Introduction to logistic regression,seg_93,pi logit(pi) = loge(1− pi)
2562,1,"['transformation', 'logit transformation', 'logit']", Introduction to logistic regression,seg_93,"the logit transformation is shown in figure 9.22. below, we rewrite the equation relating yi to its"
2563,0,[], Introduction to logistic regression,seg_93,figure 9.22: values of pi against values of logit(pi).
2564,1,"['transformation', 'logit transformation', 'logit']", Introduction to logistic regression,seg_93,predictors using the logit transformation of pi:
2565,1,"['model', 'linear', 'variables', 'regression', 'multiple regression', 'predictor variables', 'logit', 'generalized linear models', 'function', 'predictor', 'logit function', 'coefficients']", Introduction to logistic regression,seg_93,"in our resume example, there are 8 predictor variables, so k = 8. while the precise choice of a logit function isn’t intuitive, it is based on theory that underpins generalized linear models, which is beyond the scope of this book. fortunately, once we fit a model using software, it will start to feel like we’re back in the multiple regression context, even if the interpretation of the coefficients is more complex."
2566,1,"['model', 'logistic', 'regression model', 'logistic regression', 'logistic regression model', 'regression', 'variable', 'predictor', 'statistical']", Introduction to logistic regression,seg_93,"we start by fitting a model with a single predictor: honors. this variable indicates whether the applicant had any type of honors listed on their resume, such as employee of the month. the following logistic regression model was fit using statistical software:"
2567,1,['probability'], Introduction to logistic regression,seg_93,"(a) if a resume is randomly selected from the study and it does not have any honors listed, what is the probability resulted in a callback? (b) what would the probability be if the resume did list some honors?"
2568,1,['model'], Introduction to logistic regression,seg_93,"(a) if a randomly chosen resume from those sent out is considered, and it does not list honors, then honors takes value 0 and the right side of the model equation equals -2.4998. solving for pi:"
2569,1,"['probability', 'regression']", Introduction to logistic regression,seg_93,"−2.4998 = 0.076. just as we labeled a fitted value of yi with a “hat” in single-variable and multiple 1+e regression, we do the same for this probability: p̂i = 0.076."
2570,1,"['model', 'probability']", Introduction to logistic regression,seg_93,"(b) if the resume had listed some honors, then the right side of the model equation is −2.4998 + 0.8668× 1 = −1.6330, which corresponds to a probability p̂i = 0.163."
2571,1,"['probability', 'estimate']", Introduction to logistic regression,seg_93,notice that we could examine -2.4998 and -1.6330 in figure 9.22 to estimate the probability before formally calculating the value.
2572,1,"['model', 'logistic', 'regression model', 'logistic regression', 'regression']", Introduction to logistic regression,seg_93,"to convert from values on the logistic regression scale (e.g. -2.4998 and -1.6330 in example 9.31), use the following formula, which is the result of solving for pi in the regression model:"
2573,1,"['probabilities', 'parameters', 'estimates', 'data', 'point estimates']", Introduction to logistic regression,seg_93,"as with most applied data problems, we substitute the point estimates for the parameters (the βi) so that we can make use of this formula. in example 9.31, the probabilities were calculated as"
2574,1,['variables'], Introduction to logistic regression,seg_93,"while knowing whether a resume listed honors provides some signal when predicting whether or not the employer would call, we would like to account for many different variables at once to understand how each of the different resume characteristics affected the chance of a callback."
2575,1,"['model', 'logistic', 'variables', 'logistic model']", Introduction to logistic regression,seg_93,9.5.3 building the logistic model with many variables
2576,1,"['model', 'logistic', 'regression model', 'table', 'logistic regression', 'logistic regression model', 'regression', 'distribution', 'multiple regression', 'normal', 'statistical', 'normal distribution']", Introduction to logistic regression,seg_93,"we used statistical software to fit the logistic regression model with all 8 predictors described in figure 9.21. like multiple regression, the result may be presented in a summary table, which is shown in figure 9.23. the structure of this table is almost identical to that of multiple regression; the only notable difference is that the p-values are calculated using the normal distribution rather than the t-distribution."
2577,1,['error'], Introduction to logistic regression,seg_93,estimate std. error z value pr(>|z|)
2578,1,['intercept'], Introduction to logistic regression,seg_93,(intercept) -2.6632 0.1820 -14.64 <0.0001 job city: chicago -0.4403 0.1142 -3.85 0.0001 college degree -0.0666 0.1211 -0.55 0.5821 years experience 0.0200 0.0102 1.96 0.0503 honors 0.7694 0.1858 4.14 <0.0001 military -0.3422 0.2157 -1.59 0.1127 email address 0.2183 0.1133 1.93 0.0541 race: white 0.4424 0.1080 4.10 <0.0001 sex: male -0.1818 0.1376 -1.32 0.1863
2579,1,"['model', 'logistic', 'regression model', 'table', 'logistic regression', 'logistic regression model', 'regression']", Introduction to logistic regression,seg_93,figure 9.23: summary table for the full logistic regression model for the resume callback example.
2580,1,"['model', 'variables', 'regression', 'akaike information criterion', 'variable', 'statistic', 'information', 'adjusted', 'multiple regression', 'backward elimination']", Introduction to logistic regression,seg_93,"just like multiple regression, we could trim some variables from the model. here we’ll use a statistic called akaike information criterion (aic), which is an analog to how we used adjusted r-squared in multiple regression, and we look for models with a lower aic through a backward elimination strategy. after using this criteria, the college degree variable is eliminated, giving the smaller model summarized in figure 9.24, which is what we’ll rely on for the remainder of this section."
2581,1,['error'], Introduction to logistic regression,seg_93,estimate std. error z value pr(>|z|)
2582,1,['intercept'], Introduction to logistic regression,seg_93,(intercept) -2.7162 0.1551 -17.51 <0.0001 job city: chicago -0.4364 0.1141 -3.83 0.0001 years experience 0.0206 0.0102 2.02 0.0430 honors 0.7634 0.1852 4.12 <0.0001 military -0.3443 0.2157 -1.60 0.1105 email address 0.2221 0.1130 1.97 0.0494 race: white 0.4429 0.1080 4.10 <0.0001 sex: male -0.1959 0.1352 -1.45 0.1473
2583,1,"['model', 'logistic', 'regression model', 'table', 'logistic regression', 'logistic regression model', 'regression', 'variable']", Introduction to logistic regression,seg_93,"figure 9.24: summary table for the logistic regression model for the resume callback example, where variable selection has been performed using aic."
2584,1,"['model', 'levels', 'factor', 'results', 'variable']", Introduction to logistic regression,seg_93,"the race variable had taken only two levels: black and white. based on the model results, was race a meaningful factor for if a prospective employer would call back?"
2585,1,"['rate', 'data', 'statistically significant', 'level', 'coefficient']", Introduction to logistic regression,seg_93,"we see that the p-value for this coefficient is very small (very nearly zero), which implies that race played a statistically significant role in whether a candidate received a callback. additionally, we see that the coefficient shown corresponds to the level of white, and it is positive. this positive coefficient reflects a positive gain in callback rate for resumes where the candidate’s first name implied they were white. the data provide very strong evidence of racism by prospective employers that favors resumes where the first name is typically interpreted to be white."
2586,1,"['model', 'experiment', 'variables', 'estimates', 'data', 'point estimates', 'coefficient', 'observational data', 'full model']", Introduction to logistic regression,seg_93,"the coefficient of racewhite in the full model in figure 9.23, is nearly identical to the model shown in figure 9.24. the predictors in this experiment were thoughtfully laid out so that the coefficient estimates would typically not be much influenced by which other predictors were in the model, which aligned with the motivation of the study to tease out which effects were important to getting a callback. in most observational data, it’s common for point estimates to change a little, and sometimes a lot, depending on which other variables are included in the model."
2587,1,"['model', 'probability', 'estimate']", Introduction to logistic regression,seg_93,"use the model summarized in figure 9.24 to estimate the probability of receiving a callback for a job in chicago where the candidate lists 14 years experience, no honors, no military experience, includes an email address, and has a first name that implies they are a white male."
2588,1,"['model', 'variable', 'coefficients']", Introduction to logistic regression,seg_93,"we can start by writing out the equation using the coefficients from the model, then we can add in the corresponding values of each variable for this individual:"
2589,0,[], Introduction to logistic regression,seg_93,we can now back-solve for p: the chance such an individual will receive a callback is about 8.35%.
2590,1,['probability'], Introduction to logistic regression,seg_93,compute the probability of a callback for an individual with a name commonly inferred to be from a black male but who otherwise has the same characteristics as the one described in example 9.33.
2591,1,"['results', 'variable', 'probability', 'indicator', 'indicator variable']", Introduction to logistic regression,seg_93,"we can complete the same steps for an individual with the same characteristics who is black, where the only difference in the calculation is that the indicator variable racewhite will take a value of 0. doing so yields a probability of 0.0553. let’s compare the results with those of example 9.33."
2592,0,[], Introduction to logistic regression,seg_93,"in practical terms, an individual perceived as white based on their first name would need to apply"
2593,1,['average'], Introduction to logistic regression,seg_93,"1 to 0.0835 ≈ 12 jobs on average to receive a callback, while an individual perceived as black based on"
2594,1,['average'], Introduction to logistic regression,seg_93,"1 their first name would need to apply to 0.0553 ≈ 18 jobs on average to receive a callback. that is, applicants who are perceived as black need to apply to 50% more employers to receive a callback than someone who is perceived as white based on their first name for jobs like those in the study."
2595,1,"['experiment', 'cases', 'discrimination']", Introduction to logistic regression,seg_93,"what we’ve quantified in this section is alarming and disturbing. however, one aspect that makes this racism so difficult to address is that the experiment, as well-designed as it is, cannot send us much signal about which employers are discriminating. it is only possible to say that discrimination is happening, even if we cannot say which particular callbacks – or non-callbacks – represent discrimination. finding strong evidence of racism for individual cases is a persistent challenge in enforcing anti-discrimination laws."
2596,1,"['rate', 'model']", Introduction to logistic regression,seg_93,9.5.4 diagnostics for the callback rate model
2597,1,"['model', 'logistic', 'regression model', 'logistic regression', 'logistic regression model', 'regression']", Introduction to logistic regression,seg_93,there are two key conditions for fitting a logistic regression model:
2598,1,"['outcome', 'outcomes', 'independent']", Introduction to logistic regression,seg_93,1. each outcome yi is independent of the other outcomes.
2599,1,['predictor'], Introduction to logistic regression,seg_93,2. each predictor xi is linearly related to logit(pi) if all other predictors are held constant.
2600,1,"['model', 'logistic', 'regression model', 'condition', 'experiment', 'logistic regression', 'logistic regression model', 'regression', 'independence', 'outcomes']", Introduction to logistic regression,seg_93,the first logistic regression model condition – independence of the outcomes – is reasonable for the experiment since characteristics of resumes were randomly assigned to the resumes that were sent out.
2601,1,"['model', 'logistic', 'probabilities', 'condition', 'regression model', 'logistic regression', 'logistic regression model', 'data', 'regression', 'set', 'data set', 'plotting']", Introduction to logistic regression,seg_93,"the second condition of the logistic regression model is not easily checked without a fairly sizable amount of data. luckily, we have 4870 resume submissions in the data set! let’s first visualize these data by plotting the true classification of the resumes against the model’s fitted probabilities, as shown in figure 9.25."
2602,1,"['predicted', 'probability']", Introduction to logistic regression,seg_93,1 (callback) 0 (no callback) 0.0 0.2 0.4 0.6 0.8 1.0 predicted probability
2603,1,"['predicted', 'results', 'probability', 'random']", Introduction to logistic regression,seg_93,"figure 9.25: the predicted probability that each of the 4870 resumes results in a callback. noise (small, random vertical shifts) have been added to each point so points with nearly identical values aren’t plotted exactly on top of one another."
2604,1,['probability'], Introduction to logistic regression,seg_93,"observations are bucketed, ● then we compute the observed probability in each bucket (y)"
2605,1,"['predicted', 'average', 'probability']", Introduction to logistic regression,seg_93,● against the average predicted probability (x)
2606,1,"['confidence intervals', 'intervals', 'confidence']", Introduction to logistic regression,seg_93,● ●● for each of the buckets with 95% confidence intervals.
2607,1,"['predicted', 'probability']", Introduction to logistic regression,seg_93,0 (no callback) 0.0 0.2 0.4 0.6 0.8 1.0 predicted probability
2608,1,"['logistic', 'confidence intervals', 'confidence bound', 'intervals', 'confidence']", Introduction to logistic regression,seg_93,"figure 9.26: the dashed line is within the confidence bound of the 95% confidence intervals of each of the buckets, suggesting the logistic fit is reasonable."
2609,1,"['plot', 'model', 'data']", Introduction to logistic regression,seg_93,"we’d like to assess the quality of the model. for example, we might ask: if we look at resumes that we modeled as having a 10% chance of getting a callback, do we find about 10% of them actually receive a callback? we can check this for groups of the data by constructing a plot as follows:"
2610,1,"['predicted', 'probabilities', 'data']", Introduction to logistic regression,seg_93,1. bucket the data into groups based on their predicted probabilities.
2611,1,"['predicted', 'average', 'probability']", Introduction to logistic regression,seg_93,2. compute the average predicted probability for each group.
2612,1,"['interval', 'confidence', 'probability', 'confidence interval']", Introduction to logistic regression,seg_93,"3. compute the observed probability for each group, along with a 95% confidence interval."
2613,1,"['plot', 'confidence intervals', 'probabilities', 'predicted', 'intervals', 'confidence', 'average']", Introduction to logistic regression,seg_93,4. plot the observed probabilities (with 95% confidence intervals) against the average predicted probabilities for each group.
2614,1,"['plot', 'confidence intervals', 'probabilities', 'predicted', 'intervals', 'confidence']", Introduction to logistic regression,seg_93,"the points plotted should fall close to the line y = x, since the predicted probabilities should be similar to the observed probabilities. we can use the confidence intervals to roughly gauge whether anything might be amiss. such a plot is shown in figure 9.26."
2615,1,"['plot', 'plots', 'residuals', 'deviations', 'outcome', 'predictor']", Introduction to logistic regression,seg_93,"additional diagnostics may be created that are similar to those featured in section 9.3. for instance, we could compute residuals as the observed outcome minus the expected outcome (ei = yi − p̂i), and then we could create plots of these residuals against each predictor. we might also create a plot like that in figure 9.26 to better understand the deviations."
2616,1,['discrimination'], Introduction to logistic regression,seg_93,9.5.5 exploring discrimination between groups of different sizes
2617,1,"['discrimination', 'data']", Introduction to logistic regression,seg_93,"any form of discrimination is concerning, and this is why we decided it was so important to discuss this topic using data. the resume study also only examined discrimination in a single aspect: whether a prospective employer would call a candidate who submitted their resume. there was a 50% higher barrier for resumes simply when the candidate had a first name that was perceived to be from a black individual. it’s unlikely that discrimination would stop there."
2618,0,[], Introduction to logistic regression,seg_93,"22 let’s consider a sex-imbalanced company that consists of 20% women and 80% men, and we’ll suppose that the company is very large, consisting of perhaps 20,000 employees. suppose when someone goes up for promotion at this company, 5 of their colleagues are randomly chosen to provide feedback on their work."
2619,0,[], Introduction to logistic regression,seg_93,"now let’s imagine that 10% of the people in the company are prejudiced against the other sex. that is, 10% of men are prejudiced against women, and similarly, 10% of women are prejudiced against men."
2620,0,[], Introduction to logistic regression,seg_93,"who is discriminated against more at the company, men or women?"
2621,1,"['discrimination', 'biased', 'random']", Introduction to logistic regression,seg_93,"let’s suppose we took 100 men who have gone up for promotion in the past few years. for these men, 5 × 100 = 500 random colleagues will be tapped for their feedback, of which about 20% will be women (100 women). of these 100 women, 10 are expected to be biased against the man they are reviewing. then, of the 500 colleagues reviewing them, men will experience discrimination by about 2% of their colleagues when they go up for promotion."
2622,1,"['bias', 'random']", Introduction to logistic regression,seg_93,"let’s do a similar calculation for 100 women who have gone up for promotion in the last few years. they will also have 500 random colleagues providing feedback, of which about 400 (80%) will be men. of these 400 men, about 40 (10%) hold a bias against women. of the 500 colleagues providing feedback on the promotion packet for these women, 8% of the colleagues hold a bias against the women."
2623,1,['population'], Introduction to logistic regression,seg_93,"example 9.35 highlights something profound: even in a hypothetical setting where each demographic has the same degree of prejudice against the other demographic, the smaller group experiences the negative effects more frequently. additionally, if we would complete a handful of examples like the one above with different numbers, we’d learn that the greater the imbalance in the population groups, the more the smaller group is disproportionately impacted.23"
2624,1,"['discrimination', 'factors']", Introduction to logistic regression,seg_93,"of course, there are other considerable real-world omissions from the hypothetical example. for example, studies have found instances where people from an oppressed group also discriminate against others within their own oppressed group. as another example, there are also instances where a majority group can be oppressed, with apartheid in south africa being one such historic example. ultimately, discrimination is complex, and there are many factors at play beyond the mathematics property we observed in example 9.35."
2625,1,"['model', 'data', 'statistical model', 'statistical']", Introduction to logistic regression,seg_93,"we close this book on this serious topic, and we hope it inspires you to think about the power of reasoning with data. whether it is with a formal statistical model or by using critical thinking skills to structure a problem, we hope the ideas you have learned will help you do more and do better in life."
2626,1,['data'],Exercise solutions,seg_95,1 introduction to data
2627,1,['data'],Exercise solutions,seg_95,2 summarizing data
2628,1,['probability'],Exercise solutions,seg_95,3 probability
2629,1,"['box plots', 'plots']",Exercise solutions,seg_95,"no, 0.741 can construct passed? negative, 0.926 0.741*0.926 = 0.6862 box plots? yes, 0.86 0.8*0.86 = 0.688 yes, 0.8 no, 0.14 0.8*0.14 = 0.112 3.43 (a) e = $3.90. sd = $0.34. yes, 0.65 (b) e = $27.30. sd = $0.89."
2630,1,"['random variables', 'variables', 'random', 'distributions']",Exercise solutions,seg_95,4 distributions of random variables
2631,0,[],Exercise solutions,seg_95,5 foundations for inference
2632,1,"['categorical data', 'data', 'categorical']",Exercise solutions,seg_95,6 inference for categorical data
2633,1,"['numerical', 'data']",Exercise solutions,seg_95,7 inference for numerical data
2634,1,"['linear', 'linear regression', 'regression']",Exercise solutions,seg_95,8 introduction to linear regression
2635,1,"['logistic regression', 'logistic', 'regression']",Exercise solutions,seg_95,9 multiple and logistic regression
2636,1,"['data', 'information', 'sets', 'set', 'data set', 'data sets']",Data sets within the text,seg_97,"each data set within the text is described in this appendix, and there is a corresponding page for each of these data sets at openintro.org/data. this page also includes additional data sets that can be used for honing your skills. each data set has its own page with the following information:"
2637,1,"['variables', 'data']",Data sets within the text,seg_97,• list of the data set’s variables.
2638,0,[],Data sets within the text,seg_97,• r object file download.
2639,1,['data'],Data sets within the text,seg_97,b.1 introduction to data
2640,1,"['results', 'data', 'sets', 'data sets']",Data sets within the text,seg_97,"1.1 stent30, stent365 → the stent data is split across two data sets, one for days 0-30 results"
2641,1,['results'],Data sets within the text,seg_97,"and one for days 0-365 results. chimowitz mi, lynn mj, derdeyn cp, et al. 2011. stenting versus aggressive medical therapy for intracranial arterial stenosis. new england journal of medicine 365:993-1003. www.nejm.org/doi/full/10.1056/nejmoa1105335. ny times article: www.nytimes.com/2011/09/08/health/research/08stent.html."
2642,1,['data'],Data sets within the text,seg_97,"1.2 loan50, loans full schema→ this data comes from lending club (lendingclub.com), which"
2643,1,"['sample', 'data', 'set']",Data sets within the text,seg_97,"provides a large set of data on the people who received loans through their platform. the data used in the textbook comes from a sample of the loans made in q1 (jan, feb, march) 2018."
2644,1,"['county complete', 'county', 'data']",Data sets within the text,seg_97,"1.2 county, county complete → these data come from several government sources. for those"
2645,1,"['statistics', 'county', 'data', 'set', 'data set']",Data sets within the text,seg_97,"variables included in the county data set, only the most recent data is reported, as of what was available in late 2018. data prior to 2011 is all from census.gov, where the specific quick facts page providing the data is no longer available. the more recent data comes from usda (ers.usda.gov), bureau of labor statistics (bls.gov/lau), saipe (census.gov/did/www/saipe), and american community survey (census.gov/programs-surveys/acs)."
2646,1,"['data', 'information', 'set', 'data set']",Data sets within the text,seg_97,"1.3 nurses’ health study → for more information on this data set, see"
2647,1,['randomization'],Data sets within the text,seg_97,1.4 the study we had in mind when discussing the simple randomization (no blocking) study was
2648,1,['trial'],Data sets within the text,seg_97,anturane reinfarction trial research group. 1980. sulfinpyrazone in the prevention of sudden death after myocardial infarction. new england journal of medicine 302(5):250-256.
2649,1,['data'],Data sets within the text,seg_97,b.2 summarizing data
2650,1,"['county', 'data', 'sets', 'data sets']",Data sets within the text,seg_97,"2.1 loan50, county → these data sets are described in data appendix b.1."
2651,1,"['county', 'data', 'sets', 'data sets']",Data sets within the text,seg_97,"2.2 loan50, county → these data sets are described in data appendix b.1."
2652,0,[],Data sets within the text,seg_97,2.3 malaria → lyke et al. 2017. pfspz vaccine induces strain-transcending t cells and durable
2653,0,[],Data sets within the text,seg_97,protection against heterologous controlled human malaria infection. pnas 114(10):2711-2716. www.pnas.org/content/114/10/2711
2654,1,['probability'],Data sets within the text,seg_97,b.3 probability
2655,1,"['county', 'data', 'sets', 'data sets']",Data sets within the text,seg_97,"3.1 loan50, county → these data sets are described in data appendix b.1."
2656,1,"['data', 'set', 'data set', 'standard']",Data sets within the text,seg_97,3.1 playing cards → data set describing the 52 cards in a standard deck.
2657,1,"['simulated', 'data', 'population']",Data sets within the text,seg_97,3.2 family college → simulated data based on real population summaries at
2658,0,[],Data sets within the text,seg_97,3.2 smallpox → fenner f. 1988. smallpox and its eradication (history of international public
2659,1,['probabilities'],Data sets within the text,seg_97,"3.2 mammogram screening, probabilities→ the probabilities reported were obtained using studies"
2660,0,[],Data sets within the text,seg_97,reported at www.breastcancer.org and www.ncbi.nlm.nih.gov/pmc/articles/pmc1173421.
2661,1,['probabilities'],Data sets within the text,seg_97,"3.2 jose campus visits, probabilities → this example was made up."
2662,1,"['data', 'sets', 'data sets']",Data sets within the text,seg_97,3.3 no data sets were described in this section.
2663,1,['probabilities'],Data sets within the text,seg_97,3.4 course material purchases and probabilities → this example was made up.
2664,0,[],Data sets within the text,seg_97,3.4 auctions for tv and toaster → this example was made up.
2665,0,[],Data sets within the text,seg_97,"3.4 stocks 18 → monthly returns for caterpillar, exxon mobil corp, and google for november"
2666,0,[],Data sets within the text,seg_97,2015 to october 2018.
2667,1,"['sample', 'random', 'simple random sample', 'population', 'random sample']",Data sets within the text,seg_97,3.5 fcid → this sample can be considered a simple random sample from the us population. it
2668,0,[],Data sets within the text,seg_97,relies on the usda food commodity intake database.
2669,1,"['random variables', 'variables', 'random', 'distributions']",Data sets within the text,seg_97,b.4 distributions of random variables
2670,1,"['distribution', 'data', 'distributions']",Data sets within the text,seg_97,"4.1 sat and act score distributions → the sat score data comes from the 2018 distribution,"
2671,1,"['distribution', 'normal', 'data']",Data sets within the text,seg_97,"which is provided at reports.collegeboard.org/pdf/2018-total-group-sat-suite-assessments-annual-report.pdf the act score data is available at act.org/content/dam/act/unsecured/documents/cccr2018/p 99 999999 n s n00 act-gcpr national.pdf we also acknowledge that the actual act score distribution is not nearly normal. however, since the topic is very accessible, we decided to keep the context and examples."
2672,1,['distribution'],Data sets within the text,seg_97,4.1 male heights → the distribution is based on the usda food commodity intake database.
2673,1,"['sample', 'parameters', 'distribution']",Data sets within the text,seg_97,4.1 possum → the distribution parameters are based on a sample of possums from australia
2674,1,"['data', 'variation']",Data sets within the text,seg_97,"and new guinea. the original source of this data is as follows. lindenmayer db, et al. 1995. morphological variation among columns of the mountain brushtail possum, trichosurus caninus ogilby (phalangeridae: marsupiala). australian journal of zoology 43: 449-458."
2675,1,['statistics'],Data sets within the text,seg_97,4.2 exceeding insurance deductible → these statistics were made up but are possible values one
2676,0,[],Data sets within the text,seg_97,might observe for low-deductible plans.
2677,1,['statistics'],Data sets within the text,seg_97,4.3 exceeding insurance deductible → these statistics were made up but are possible values one
2678,0,[],Data sets within the text,seg_97,might observe for low-deductible plans.
2679,1,['information'],Data sets within the text,seg_97,"4.3 smoking friends→ unfortunately, we don’t currently have additional information on the source"
2680,1,['statistic'],Data sets within the text,seg_97,"for the 30% statistic, so don’t consider this one as fact since we cannot verify it was from a reputable source."
2681,1,['rate'],Data sets within the text,seg_97,4.3 us smoking rate → the 15% smoking rate in the us figure is close to the value from the
2682,1,"['estimate', 'control']",Data sets within the text,seg_97,"centers for disease control and prevention website, which reports a value of 14% as of the 2017 estimate: cdc.gov/tobacco/data statistics/fact sheets/adult data/cig smoking/index.htm"
2683,0,[],Data sets within the text,seg_97,4.4 football kicker → this example was made up.
2684,0,[],Data sets within the text,seg_97,"4.4 heart attack admissions → this example was made up, though the heart attack admissions"
2685,0,[],Data sets within the text,seg_97,are realistic for some hospitals.
2686,1,"['data', 'set', 'data set', 'simulated']",Data sets within the text,seg_97,4.5 ami occurrences→ this is a simulated data set but resembles actual ami data for new york
2687,1,['rates'],Data sets within the text,seg_97,city based on typical ami incidence rates.
2688,0,[],Data sets within the text,seg_97,b.5 foundations for inference
2689,1,"['observations', 'data']",Data sets within the text,seg_97,5.1 pew energy 2018→ the actual data has more observations than were referenced in this chap-
2690,1,"['variability', 'statistics', 'data', 'set', 'data set']",Data sets within the text,seg_97,"ter. that is, we used a subsample since it helped smooth some of the examples to have a bit more variability. the pew energy 2018 data set represents the full data set for each of the different energy source questions, which covers solar, wind, offshore drilling, hydrolic fracturing, and nuclear energy. the statistics used to construct the data are from the following page:"
2691,1,"['set', 'data set', 'data']",Data sets within the text,seg_97,5.2 pew energy 2018 → see the details for this data set above in the section 5.1 data section.
2692,0,[],Data sets within the text,seg_97,"5.2 ebola survey → in new york city on october 23rd, 2014, a doctor who had recently been"
2693,0,[],Data sets within the text,seg_97,"treating ebola patients in guinea went to the hospital with a slight fever and was subsequently diagnosed with ebola. soon thereafter, an nbc 4 new york/the wall street journal/marist poll found that 82% of new yorkers favored a “mandatory 21-day quarantine for anyone who has come in contact with an ebola patient”. this poll included responses of 1,042 new york adults between oct 26th and 28th, 2014. poll id ny141026 on maristpoll.marist.edu."
2694,1,"['set', 'data set', 'data']",Data sets within the text,seg_97,5.3 pew energy 2018 → see the details for this data set above in the section 5.1 data section.
2695,1,['samples'],Data sets within the text,seg_97,5.3 rosling questions → we noted much smaller samples than the roslings’ describe in their
2696,1,"['populations', 'samples', 'rates']",Data sets within the text,seg_97,"book, factfulness, the samples we describe are similar but not the same as the actual rates. the approximate rates for the correct answers for the two questions for (sometimes different) populations discussed in the book, as reported in factfulness, are"
2697,0,[],Data sets within the text,seg_97,– 80% of the world’s 1 year olds have been vaccinated against some disease: 13% get this correct (17% in the us). gapm.io/q9
2698,0,[],Data sets within the text,seg_97,– number of children in the world in 2100: 9% correct. gapm.io/q5
2699,1,['percent'],Data sets within the text,seg_97,here are a few more questions and a rough percent of people who get them correct:
2700,0,[],Data sets within the text,seg_97,"– in all low-income countries across the world today, how many girls finish primary school: 20%, 40%, or 60%? answer: 60%. about 7% of people get this question correct. gapm.io/q1"
2701,0,[],Data sets within the text,seg_97,"– what is the life expectancy of the world today: 50 years, 60 years, or 70 years? answer: 70 years. in the us, about 43% of people get this question correct. gapm.io/q4"
2702,0,[],Data sets within the text,seg_97,"– in 1996, tigers, giant pandas, and black rhinos were all listed as endangered. how many of these three species are more critically endangered today: two of them, one of them, none of them? answer: none of them. about 7% of people get this question correct. gapm.io/q11"
2703,0,[],Data sets within the text,seg_97,"– how many people in the world have some access to electricity? 20%, 50%, 80%. answer: 80%. about 22% of people get this correct. gapm.io/q12"
2704,1,['information'],Data sets within the text,seg_97,"for more information, check out the book, factfulness."
2705,1,"['set', 'data set', 'data']",Data sets within the text,seg_97,5.3 pew energy 2018 → see the details for this data set above in the section 5.1 data section.
2706,1,"['sample', 'random', 'simple random sample', 'random sample']",Data sets within the text,seg_97,"5.3 nuclear survey → a simple random sample of 1,028 us adults in march 2013 found that"
2707,0,[],Data sets within the text,seg_97,56% of us adults support nuclear arms reduction. www.gallup.com/poll/161198/favor-russian-nuclear-arms-reductions.aspx
2708,0,[],Data sets within the text,seg_97,5.3 car manufacturing → this example was made up.
2709,1,"['data', 'sets', 'data sets']",Data sets within the text,seg_97,"5.3 stent30, stent365 → these data sets are described in data appendix b.1."
2710,1,"['categorical data', 'data', 'categorical']",Data sets within the text,seg_97,b.6 inference for categorical data
2711,1,['statistics'],Data sets within the text,seg_97,6.1 payday loans → the statistics come from the following source:
2712,0,[],Data sets within the text,seg_97,6.1 tire factory → this example was made up.
2713,0,[],Data sets within the text,seg_97,6.2 cpr → böttiger et al. efficacy and safety of thrombolytic therapy after initially unsuccessful
2714,1,['trial'],Data sets within the text,seg_97,"cardiopulmonary resuscitation: a prospective clinical trial. the lancet, 2001."
2715,0,[],Data sets within the text,seg_97,"6.2 fish oil 18 → manson je, et al. 2018. marine n-3 fatty acids and prevention of cardio-"
2716,0,[],Data sets within the text,seg_97,vascular disease and cancer. nejmoa1811403.
2717,0,[],Data sets within the text,seg_97,6.2 mammogram → miller ab. 2014. twenty five year follow-up for breast cancer incidence and
2718,1,['trial'],Data sets within the text,seg_97,mortality of the canadian national breast screening study: randomised screening trial. bmj 2014;348:g366.
2719,1,"['data', 'set', 'data set', 'control', 'quality control']",Data sets within the text,seg_97,6.2 drone blades → the quality control data set for quadcopter drone blades is a made-up data
2720,1,"['data', 'set', 'data set', 'simulated']",Data sets within the text,seg_97,set for an example. we provide the simulated data in the drone blades data set.
2721,1,"['data', 'discrimination', 'set', 'data set']",Data sets within the text,seg_97,6.3 jury→ the jury data set for examining discrimination is a made-up data set an example. we
2722,1,"['data', 'set', 'data set', 'simulated']",Data sets within the text,seg_97,provide the simulated data in the jury data set.
2723,1,['data'],Data sets within the text,seg_97,6.3 sp500 1950 2018 → data is sourced from finance.yahoo.com.
2724,0,[],Data sets within the text,seg_97,"6.4 ask → minson ja, ruedy ne, schweitzer me. there is such a thing as a stupid question:"
2725,1,"['control', 'trial']",Data sets within the text,seg_97,"6.4 diabetes2 → zeitler p, et al. 2012. a clinical trial to maintain glycemic control in youth"
2726,0,['n'],Data sets within the text,seg_97,with type 2 diabetes. n engl j med.
2727,1,"['numerical', 'data']",Data sets within the text,seg_97,b.7 inference for numerical data
2728,1,"['samples', 'levels']",Data sets within the text,seg_97,7.1 risso’s dolphins → endo t and haraguchi k. 2009. high mercury levels in hair samples from
2729,0,[],Data sets within the text,seg_97,"residents of taiji, a japanese whaling town. marine pollution bulletin 60(5):743-747."
2730,1,"['sample', 'random', 'simple random sample', 'random sample']",Data sets within the text,seg_97,"taiji was featured in the movie the cove, and it is a significant source of dolphin and whale meat in japan. thousands of dolphins pass through the taiji area annually, and we assumes these 19 dolphins reasonably represent a simple random sample from those dolphins."
2731,0,[],Data sets within the text,seg_97,7.1 croaker white fish → fda.gov/food/foodborneillnesscontaminants/metals/ucm115644.htm
2732,1,['data'],Data sets within the text,seg_97,"7.2 textbooks, ucla textbooks f18→ data were collected by openintro staff in 2010 and again"
2733,1,"['sample', 'information']",Data sets within the text,seg_97,"in 2018. for the 2018 sample, we sampled 201 ucla courses. of those, 68 required books that could be found on amazon. the websites where information was retrieved: sa.ucla.edu/ro/public/soc, ucla.verbacompare.com, and amazon.com."
2734,0,[],Data sets within the text,seg_97,"7.3 stem cells→menard c, et al. 2005. transplantation of cardiac-committed mouse embryonic"
2735,0,[],Data sets within the text,seg_97,"stem cells to infarcted sheep myocardium: a preclinical study. the lancet: 366:9490, p1005- 1012."
2736,0,[],Data sets within the text,seg_97,"7.3 ncbirths → birth records released by north carolina in 2004. unfortunately, we don’t cur-"
2737,1,"['data', 'information', 'set', 'data set']",Data sets within the text,seg_97,rently have additional information on the source for this data set.
2738,0,[],Data sets within the text,seg_97,7.3 exam versions → this example was made up.
2739,1,"['deviation', 'statistics', 'standard', 'standard deviation']",Data sets within the text,seg_97,7.4 blood pressure statistics → the blood pressure standard deviation for patients with blood
2740,1,['data'],Data sets within the text,seg_97,pressure ranging from 140 to 180 mmhg is guessed and may be a little (but likely not dramatically) imprecise from what we’d observe in actual data.
2741,1,"['data', 'anova']",Data sets within the text,seg_97,"7.5 toy anova → data used for figure 7.19, where this data was made up."
2742,1,['data'],Data sets within the text,seg_97,7.5 mlb players 18 → data were retrieved from mlb.mlb.com/stats. only players with at least
2743,0,[],Data sets within the text,seg_97,100 at bats were considered during the analysis.
2744,0,[],Data sets within the text,seg_97,7.5 classdata → this example was made up.
2745,1,"['linear', 'linear regression', 'regression']",Data sets within the text,seg_97,b.8 introduction to linear regression
2746,1,"['plot', 'simulated scatter', 'linear', 'plots', 'data', 'simulated']",Data sets within the text,seg_97,8.1 simulated scatter → fake data used for the first three plots. the perfect linear plot uses
2747,1,"['linear', 'curve', 'residual', 'plots', 'data', 'correlation', 'variable', 'set', 'data set', 'scatterplots', 'residual plots']",Data sets within the text,seg_97,"group 4 data, where group variable in the data set (figure 8.1). the group of 3 imperfect linear plots use groups 1-3 (figure 8.2). the sinusoidal curve uses group 5 data (figure 8.3). the group of 3 scatterplots with residual plots use groups 6-8 (figure 8.8). the correlation plots uses groups 9-19 data (figures 8.9 and 8.10)."
2748,1,"['set', 'data set', 'data']",Data sets within the text,seg_97,8.1 possum → this data set is described in data appendix b.4.
2749,1,"['data', 'table']",Data sets within the text,seg_97,8.2 elmhurst→ these data were sampled from a table of data for all freshman from the 2011 class
2750,0,[],Data sets within the text,seg_97,at elmhurst college that accompanied an article titled what students really pay to go to college published online by the chronicle of higher education: chronicle.com/article/what- students-really-pay-to-go/131435.
2751,1,"['plots', 'simulated']",Data sets within the text,seg_97,8.2 simulated scatter→ the plots for things that can go wrong uses groups 20-23 (figure 8.12).
2752,1,['data'],Data sets within the text,seg_97,8.2 mariokart → auction data from ebay (ebay.com) for the game mario kart for the nintendo
2753,1,"['set', 'data set', 'data']",Data sets within the text,seg_97,"wii. this data set was collected in early october, 2009."
2754,1,"['outliers', 'simulated scatter', 'plots', 'simulated']",Data sets within the text,seg_97,8.3 simulated scatter → the plots for types of outliers uses groups 24-29 (figure 8.18).
2755,1,['data'],Data sets within the text,seg_97,8.4 midterms house → data was retrieved from wikipedia.
2756,1,"['logistic regression', 'logistic', 'regression']",Data sets within the text,seg_97,b.9 multiple and logistic regression
2757,1,"['set', 'data set', 'data']",Data sets within the text,seg_97,9.1 loans full schema → this data set is described in data appendix b.1.
2758,1,"['set', 'data set', 'data']",Data sets within the text,seg_97,9.2 loans full schema → this data set is described in data appendix b.1.
2759,1,"['set', 'data set', 'data']",Data sets within the text,seg_97,9.3 loans full schema → this data set is described in data appendix b.1.
2760,1,"['set', 'data set', 'data']",Data sets within the text,seg_97,9.4 mariokart → this data set is described in data appendix b.8.
2761,0,[],Data sets within the text,seg_97,"9.5 resume → bertrand m, mullainathan s. 2004. are emily and greg more employable than"
2762,1,"['discrimination', 'experiment']",Data sets within the text,seg_97,lakisha and jamal? a field experiment on labor market discrimination. the american economic review 94:4 (991-1013). www.nber.org/papers/w9873
2763,1,"['combination', 'variables', 'design', 'estimates', 'data', 'standard', 'point estimates']",Data sets within the text,seg_97,"we did omit discussion of some structure in the data for the analysis presented: the experiment design included blocking, where typically four resumes were sent to each job: one for each inferred race/sex combination (as inferred based on the first name). we did not worry about this blocking aspect, since accounting for the blocking would reduce the standard error without notably changing the point estimates for the race and sex variables versus the analysis performed in the section. that is, the most interesting conclusions in the study are unaffected even when completing a more sophisticated analysis."
2764,1,"['table', 'probability table', 'normal', 'probability', 'normal probability table']",Distribution tables,seg_99,c.1 normal probability table
2765,1,"['table', 'probability table', 'distribution', 'percentiles', 'normal', 'probability', 'normal distribution', 'normal probability table']",Distribution tables,seg_99,"a normal probability table may be used to find percentiles of a normal distribution using a z-score, or vice-versa. such a table lists z-scores and the corresponding percentiles. an abbreviated probability table is provided in figure c.1 that we’ll use for the examples in this appendix. a full table may be found on page 410."
2766,1,"['percentile', 'table', 'probability table', 'random variable', 'variable', 'normal', 'probability', 'random', 'normal random variable', 'normal probability table']",Distribution tables,seg_99,"figure c.1: a section of the normal probability table. the percentile for a normal random variable with z = 1.00 has been highlighted, and the percentile closest to 0.8000 has also been highlighted."
2767,1,"['percentile', 'normal probability table', 'table', 'probability table', 'intersection', 'observation', 'normal', 'probability']",Distribution tables,seg_99,"when using a normal probability table to find a percentile for z (rounded to two decimals), identify the proper row in the normal probability table up through the first decimal, and then determine the column representing the second decimal value. the intersection of this row and column is the percentile of the observation. for instance, the percentile of z = 0.45 is shown in row 0.4 and column 0.05 in figure c.1: 0.6736, or the 67.36th percentile."
2768,1,"['percentile', 'observation']",Distribution tables,seg_99,figure c.2: the area to the left of z represents the percentile of the observation.
2769,1,"['percentile', 'distribution', 'normal', 'scores', 'normal distribution']",Distribution tables,seg_99,"sat scores follow a normal distribution, n(1100, 200). ann earned a score of 1300 on her sat with a corresponding z-score of z = 1. she would like to know what percentile she falls in among all sat test-takers."
2770,1,"['percentile', 'percentage']",Distribution tables,seg_99,ann’s percentile is the percentage of people who earned a lower sat score than her. we shade the area representing those individuals in the following graph:
2771,1,"['curve', 'percentile', 'normal probability table', 'table', 'probability table', 'normal', 'probability']",Distribution tables,seg_99,"the total area under the normal curve is always equal to 1, and the proportion of people who scored below ann on the sat is equal to the area shaded in the graph. we find this area by looking in row 1.0 and column 0.00 in the normal probability table: 0.8413. in other words, ann is in the 84th percentile of sat takers."
2772,1,['tail'],Distribution tables,seg_99,how do we find an upper tail area?
2773,1,"['table', 'probability table', 'normal', 'probability', 'tail', 'test', 'normal probability table']",Distribution tables,seg_99,"the normal probability table always gives the area to the left. this means that if we want the area to the right, we first find the lower tail and then subtract it from 1. for instance, 84.13% of sat takers scored below ann, which means 15.87% of test takers scored higher than ann."
2774,1,"['percentile', 'table', 'associated']",Distribution tables,seg_99,"we can also find the z-score associated with a percentile. for example, to identify z for the 80th percentile, we look for the value closest to 0.8000 in the middle portion of the table: 0.7995. we determine the z-score for the 80th percentile by combining the row and column z values: 0.84."
2775,1,['percentile'],Distribution tables,seg_99,find the sat score for the 80th percentile.
2776,1,"['set', 'table']",Distribution tables,seg_99,"we look for the are to the value in the table closest to 0.8000. the closest value is 0.7995, which corresponds to z = 0.84, where 0.8 comes from the row value and 0.04 comes from the column value. next, we set up the equation for the z-score and the unknown value x as follows, and then we solve for x:"
2777,1,"['percentile', 'scores']",Distribution tables,seg_99,"the college board scales scores to increments of 10, so the 80th percentile is 1270. (reporting 1268 would have been perfectly okay for our purposes.)"
2778,1,"['table', 'probability table', 'distribution', 'normal', 'probability', 'normal distribution', 'normal probability table']",Distribution tables,seg_99,"for additional details about working with the normal distribution and the normal probability table, see section 4.1, which starts on page 133."
2779,1,['table'],Distribution tables,seg_99,c.2 t-probability table
2780,1,"['degrees of freedom', 'tail areas', 'symmetric', 'percentiles', 'tail probabilities', 'distribution', 'probabilities', 'table', 'normal', 'tail', 'normal distribution']",Distribution tables,seg_99,"a t-probability table may be used to find tail areas of a t-distribution using a t-score, or viceversa. such a table lists t-scores and the corresponding percentiles. a partial t-table is shown in figure c.3, and the complete table starts on page 414. each row in the t-table represents a t-distribution with different degrees of freedom. the columns correspond to tail probabilities. for instance, if we know we are working with the t-distribution with df = 18, we can examine row 18, which is highlighted in figure c.3. if we want the value in this row that identifies the t-score (cutoff) for an upper tail of 10%, we can look in the column where one tail is 0.100. this cutoff is 1.33. if we had wanted the cutoff for the lower 10%, we would use -1.33. just like the normal distribution, all t-distributions are symmetric."
2781,1,"['tail', 'tails']",Distribution tables,seg_99,one tail 0.100 0.050 0.025 0.010 0.005 two tails 0.200 0.100 0.050 0.020 0.010 df 1 3.08 6.31 12.71 31.82 63.66 2 1.89 2.92 4.30 6.96 9.92 3 1.64 2.35 3.18 4.54 5.84 . . . . . . . . . . . . . . . 17 1.33 1.74 2.11 2.57 2.90 18 1.33 1.73 2.10 2.55 2.88 19 1.33 1.73 2.09 2.54 2.86 20 1.33 1.72 2.09 2.53 2.85 . . . . . . . . . . . . . . . 400 1.28 1.65 1.97 2.34 2.59 500 1.28 1.65 1.96 2.33 2.59 ∞ 1.28 1.64 1.96 2.33 2.58
2782,1,"['tail areas', 'tail']",Distribution tables,seg_99,figure c.3: an abbreviated look at the t-table. each row represents a different t-distribution. the columns describe the cutoffs for specific tail areas. the row with df = 18 has been highlighted.
2783,1,['degrees of freedom'],Distribution tables,seg_99,what proportion of the t-distribution with 18 degrees of freedom falls below -2.10?
2784,1,"['normal', 'probability']",Distribution tables,seg_99,"just like a normal probability problem, we first draw the picture and shade the area below -2.10:"
2785,1,"['distribution', 'tail', 'absolute value', 'table']",Distribution tables,seg_99,"to find this area, we first identify the appropriate row: df = 18. then we identify the column containing the absolute value of -2.10; it is the third column. because we are looking for just one tail, we examine the top line of the table, which shows that a one tail area for a value in the third row corresponds to 0.025. that is, 2.5% of the distribution falls below -2.10."
2786,1,"['case', 'table']",Distribution tables,seg_99,in the next example we encounter a case where the exact t-score is not listed in the table.
2787,1,"['degrees of freedom', 'estimate', 'distribution']",Distribution tables,seg_99,a t-distribution with 20 degrees of freedom is shown in the left panel of figure c.4. estimate the proportion of the distribution falling above 1.65.
2788,1,"['degrees of freedom', 'standard deviations', 'distribution', 'deviations', 'mean', 'standard', 'tail areas', 'tail', 'statistical']",Distribution tables,seg_99,"we identify the row in the t-table using the degrees of freedom: df = 20. then we look for 1.65; it is not listed. it falls between the first and second columns. since these values bound 1.65, their tail areas will bound the tail area corresponding to 1.65. we identify the one tail area of the first and second columns, 0.050 and 0.10, and we conclude that between 5% and 10% of the distribution is more than 1.65 standard deviations above the mean. if we like, we can identify the precise area using statistical software: 0.0573."
2789,1,['degrees of freedom'],Distribution tables,seg_99,"figure c.4: left: the t-distribution with 20 degrees of freedom, with the area above 1.65 shaded. right: the t-distribution with 475 degrees of freedom, with the area further than 2 units from 0 shaded."
2790,1,"['degrees of freedom', 'estimate', 'distribution', 'mean']",Distribution tables,seg_99,a t-distribution with 475 degrees of freedom is shown in the right panel of figure c.4. estimate the proportion of the distribution falling more than 2 units from the mean (above or below).
2791,1,"['symmetric', 'case', 'tail areas', 'tail', 'tails']",Distribution tables,seg_99,"as before, first identify the appropriate row: df = 475. this row does not exist! when this happens, we use the next smaller row, which in this case is df = 400. next, find the columns that capture 2.00; because 1.97 < 3 < 2.34, we use the third and fourth columns. finally, we find bounds for the tail areas by looking at the two tail values: 0.02 and 0.05. we use the two tail values because we are looking for two symmetric tails in the t-distribution."
2792,1,['degrees of freedom'],Distribution tables,seg_99,what proportion of the t-distribution with 19 degrees of freedom falls above -1.79 units?1
2793,1,"['degrees of freedom', 'distribution']",Distribution tables,seg_99,"find the value of t?18 using the t-table, where t?18 is the cutoff for the t-distribution with 18 degrees of freedom where 95% of the distribution lies between -t?18 and +t?18."
2794,1,"['degrees of freedom', 'interval', 'confidence', 'tails', 'confidence interval']",Distribution tables,seg_99,"for a 95% confidence interval, we want to find the cutoff t?18 such that 95% of the t-distribution is between -t?18 and t?18; this is the same as where the two tails have a total area of 0.05. we look in the t-table on page 412, find the column with area totaling 0.05 in the two tails (third column), and then the row with 18 degrees of freedom: t?18 = 2.10."
2795,1,"['tail', 'tails']",Distribution tables,seg_99,one tail one tail two tails
2796,1,"['tail', 'tails']",Distribution tables,seg_99,one tail 0.100 0.050 0.025 0.010 0.005 two tails 0.200 0.100 0.050 0.020 0.010 df 1 3.08 6.31 12.71 31.82 63.66 2 1.89 2.92 4.30 6.96 9.92 3 1.64 2.35 3.18 4.54 5.84 4 1.53 2.13 2.78 3.75 4.60 5 1.48 2.02 2.57 3.36 4.03 6 1.44 1.94 2.45 3.14 3.71 7 1.41 1.89 2.36 3.00 3.50 8 1.40 1.86 2.31 2.90 3.36 9 1.38 1.83 2.26 2.82 3.25 10 1.37 1.81 2.23 2.76 3.17
2797,1,"['tail', 'tails']",Distribution tables,seg_99,one tail one tail two tails
2798,1,"['tail', 'tails']",Distribution tables,seg_99,one tail 0.100 0.050 0.025 0.010 0.005 two tails 0.200 0.100 0.050 0.020 0.010 df 31 1.31 1.70 2.04 2.45 2.74 32 1.31 1.69 2.04 2.45 2.74 33 1.31 1.69 2.03 2.44 2.73 34 1.31 1.69 2.03 2.44 2.73 35 1.31 1.69 2.03 2.44 2.72 36 1.31 1.69 2.03 2.43 2.72 37 1.30 1.69 2.03 2.43 2.72 38 1.30 1.69 2.02 2.43 2.71 39 1.30 1.68 2.02 2.43 2.71 40 1.30 1.68 2.02 2.42 2.70
2799,1,"['table', 'probability table', 'probability']",Distribution tables,seg_99,c.3 chi-square probability table
2800,1,"['degrees of freedom', 'tail areas', 'table', 'range', 'probability table', 'distribution', 'normal', 'tables', 'probability', 'tail', 'distributions']",Distribution tables,seg_99,"a chi-square probability table may be used to find tail areas of a chi-square distribution. the chi-square table is partially shown in figure c.5, and the complete table may be found on page 417. when using a chi-square table, we examine a particular row for distributions with different degrees of freedom, and we identify a range for the area (e.g. 0.025 to 0.05). note that the chi-square table provides upper tail values, which is different than the normal and t-distribution tables."
2801,1,['table'],Distribution tables,seg_99,figure c.5: a section of the chi-square table. a complete table is in appendix c.3.
2802,1,"['degrees of freedom', 'estimate', 'distribution', 'tail']",Distribution tables,seg_99,figure c.6(a) shows a chi-square distribution with 3 degrees of freedom and an upper shaded tail starting at 6.25. use figure c.5 to estimate the shaded area.
2803,1,"['degrees of freedom', 'table', 'distribution', 'tail']",Distribution tables,seg_99,"this distribution has three degrees of freedom, so only the row with 3 degrees of freedom (df) is relevant. this row has been italicized in the table. next, we see that the value – 6.25 – falls in the column with upper tail area 0.1. that is, the shaded upper tail of figure c.6(a) has area 0.1."
2804,1,"['table', 'estimate', 'range', 'tail']",Distribution tables,seg_99,"this example was unusual, in that we observed the exact value in the table. in the next examples, we encounter situations where we cannot precisely estimate the tail area and must instead provide a range of values."
2805,1,"['degrees of freedom', 'distribution', 'tail']",Distribution tables,seg_99,figure c.6(b) shows the upper tail of a chi-square distribution with 2 degrees of freedom. the area above value 4.3 has been shaded; find this tail area.
2806,1,"['tail areas', 'degrees of freedom', 'tail']",Distribution tables,seg_99,"the cutoff 4.3 falls between the second and third columns in the 2 degrees of freedom row. because these columns correspond to tail areas of 0.2 and 0.1, we can be certain that the area shaded in figure c.6(b) is between 0.1 and 0.2."
2807,1,"['degrees of freedom', 'distribution', 'tail']",Distribution tables,seg_99,figure c.6(c) shows an upper tail for a chi-square distribution with 5 degrees of freedom and a cutoff of 5.1. find the tail area.
2808,0,[],Distribution tables,seg_99,"looking in the row with 5 df, 5.1 falls below the smallest cutoff for this row (6.06). that means we can only say that the area is greater than 0.3."
2809,1,"['degrees of freedom', 'distribution', 'tail']",Distribution tables,seg_99,figure c.6(d) shows a cutoff of 11.7 on a chi-square distribution with 7 degrees of freedom. find the area of the upper tail.
2810,1,"['degrees of freedom', 'distribution']",Distribution tables,seg_99,"figure c.6: (a) chi-square distribution with 3 degrees of freedom, area above 6.25 shaded. (b) 2 degrees of freedom, area above 4.3 shaded. (c) 5 degrees of freedom, area above 5.1 shaded. (d) 7 degrees of freedom, area above 11.7 shaded."
2811,1,"['adjusted', 'complement', 'adjusted r2']",Distribution tables,seg_99,addition rule 83 complement 88 adjusted r2 (ra
2812,1,['condition'],Distribution tables,seg_99,2dj) 349 349 condition 97
2813,1,"['interval', 'confident', 'conditional', 'probability', 'associated', 'information', 'confidence', 'confidence interval', 'alternative hypothesis', 'anecdotal evidence', 'regression', 'analysis of variance', 'level', 'variance', 'average', 'hypothesis', 'anova', 'conditional probability', 'confidence level']",Distribution tables,seg_99,"akaike information criterion (aic) 374 conditional probability 97 97–99, 108 alternative hypothesis (ha) 189 confidence interval 169 181 181–186 ami occurrences 404 95% 182 analysis of variance (anova) 285 285–294 confidence level 183 anecdotal evidence 22 interpretation 186 ask 406 regression 334 associated 16 confident 181 average 43 95% confident 181"
2814,1,"['plot', 'bar plot', 'factor', 'confounding variable', 'variable', 'confounding', 'confounding factor', 'backward elimination']",Distribution tables,seg_99,confounder 25 backward elimination 354 confounding factor 25 bar plot 61 confounding variable 25
2815,1,"['bayesian', 'column totals', 'contingency table', 'bayesian statistics', 'statistics', 'correlation', 'control group', 'sample', 'bias', 'bar plot', 'row totals', 'convenience sample', 'bonferroni correction', 'continuous', 'control', 'bimodal', 'plot', 'table', 'county', 'box plot', 'county complete']",Distribution tables,seg_99,"segmented bar plot 64 contingency table 61 side-by-side 64 column proportion 62 stacked bar plot 64 column totals 61 bayes’ theorem 106 104–108 row proportions 62 bayesian statistics 108 row totals 61 bias 24 22–24, 170 186 continuous 15 bimodal 46 control 32 blind 34 control group 9 32 blocking 32 convenience sample 24 blocks 32 correlation 305 310 310–311 bonferroni correction 293 county 403 404 box plot 49 county complete 403"
2816,1,"['plot', 'box plot']",Distribution tables,seg_99,side-by-side box plot 68 cpr 406
2817,1,"['categorical', 'data', 'variable', 'categorical variable', 'central limit theorem', 'limit']",Distribution tables,seg_99,case 12 data 8 403–407 categorical 15 baby smoke 269–271 categorical variable 343 breast cancer 219–221 central limit theorem 172 251 coal power support 194–196
2818,1,"['collinear', 'column totals', 'interval', 'probability table', 'statistic', 'probability', 'data', 'distribution', 'cloud of points', 'table', 'county', 'normal']",Distribution tables,seg_99,"independence 172 county 13–18, 52–53, 67–68 normal data 252 cpr and blood thinner 217–218 proportion 172 diabetes 243–244 chi-square distribution 231 dolphins and mercury 255–256 chi-square probability table 416 ebola poll 185 chi-square statistic 231 ipod 240–243 chi-square table 416 loan50 12 41–51 classdata 406 loans 61–66, 84 86 343 clopper-pearson interval 211 malaria vaccine 71–74 cloud of points 305 mammography 219–221 code comment 171 mario kart 362 cohort 18 midterm elections 331–333 collections 84 mlb batting 286–291 collinear 348 367 nuclear arms reduction 197 column totals 61 payday regulation poll 208–210, 213"
2819,1,"['false positive', 'finite population correction factor', 'factor', 'factorial', 'function', 'first quartile', 'rate', 'correction factor', 'experiment', 'data', 'quartile', 'population', 'false negative', 'failure rate', 'failure', 'expected value', 'expectation', 'extrapolation', 'data density', 'variable', 'explanatory variable', 'explanatory', 'exponentially']",Distribution tables,seg_99,"photo classify 95–99 expectation 116–117 possum 306–309 expected value 116 racial make-up of jury 229–231, 234 experiment 18 32 resume 371–377 explanatory variable 18 305 s&p500 stock data 236–239 exponentially 145 smallpox 99–102 extrapolation 322 solar survey 170–186 f -test 289 stem cells, heart function 267–269 face card 85 stroke 9–10, 15 factorial 150 student football stadium 212 failure 144 textbooks 262–264 false negative 105 tire failure rate 213 false positive 105 two exam comparison 272–273 family college 404 us adult heights 125–127 fcid 404 white fish and mercury 256–257 finite population correction factor 173 wind turbine survey 186 first quartile 49 data density 45"
2820,1,['data'],Distribution tables,seg_99,fish oil 18 406 data fishing 288
2821,1,"['data matrix', 'data']",Distribution tables,seg_99,forward selection 354 data matrix 12
2822,1,"['model', 'data snooping', 'data']",Distribution tables,seg_99,full model 353 data snooping 288
2823,0,[],Distribution tables,seg_99,deck of cards 85 gambler’s fallacy 101
2824,1,"['general addition rule', 'addition rule']",Distribution tables,seg_99,degrees of freedom (df) general addition rule 86
2825,1,"['degrees of freedom', 'linear', 'model', 'multiplication rule', 'linear model', 'generalized linear model', 'general multiplication rule']",Distribution tables,seg_99,t-distribution 253 general multiplication rule 100 degrees of freedom (df) generalized linear model 164 371
2826,1,['glm'],Distribution tables,seg_99,anova 289 glm 371 chi-square 231 greek
2827,1,['dependent'],Distribution tables,seg_99,regression 349 alpha (α) 193 density 126 beta (β) 305 dependent 16 18 epsilon (ε) 305
2828,1,['degrees of freedom'],Distribution tables,seg_99,deviation 47 lambda (λ) 163 df see degrees of freedom (df) mu (µ) 43 116
2829,1,"['plots', 'diagnostic plots']",Distribution tables,seg_99,diabetes2 406 sigma (σ) 47 118 diagnostic plots 358
2830,1,"['high leverage', 'disjoint', 'histogram', 'hollow histogram', 'distribution', 'discrimination', 'hypotheses', 'leverage']",Distribution tables,seg_99,discrete 15 175 high leverage 328 discrimination 378 histogram 45 disjoint 83 83–84 hollow histogram 68 125–126 distribution 43 126 hypotheses 189
2831,1,"['normal approximation', 'geometric', 'approximation', 'errors', 'negative binomial', 'hypothesis testing', 'normal', 'binomial', 'level', 'significance', 'significance level', 'hypothesis']",Distribution tables,seg_99,"bernoulli 144 144 hypothesis testing 189–199, 201 binomial 149 149–155 decision errors 193 normal approximation 153–155 p-value 194 194 geometric 145 146 145–147 significance level 193 198–199 negative binomial 158 158–161"
2832,1,['independence'],Distribution tables,seg_99,normal 133 133–143 independence 172
2833,1,"['poisson', 'plot', 'intensity', 'interaction term', 'independent', 'intensity map', 'interaction', 'variable', 'influential point', 'indicator', 'indicator variable', 'dot plot']",Distribution tables,seg_99,standard 184 independent 17 18 89 172 poisson 163 163–164 independent and identically distributed (iid) 145 t 252–254 indicator variable 323 343 344 365 372 dot plot 42 influential point 328 double-blind 34 intensity map 53 drone blades 406 interaction term 362
2834,1,"['range', 'effect size']",Distribution tables,seg_99,interquartile range 49 50 ebola survey 405 iqr 49 effect size 204 279
2835,1,"['estimate', 'joint', 'probability', 'error', 'joint probability']",Distribution tables,seg_99,elmhurst 407 joint probability 96 96–97 error 170 jury 406 estimate 170
2836,1,"['least squares criterion', 'law of large numbers', 'least squares line', 'least squares']",Distribution tables,seg_99,event 84 84 law of large numbers 82 e(x) 116 least squares criterion 318 exampleforresumeandblackquantified 375 least squares line 318
2837,1,"['observational study', 'regression']",Distribution tables,seg_99,least squares regression 317–321 observational study 18
2838,1,"['levels', 'outlier', 'observational unit', 'linear', 'ordinal', 'outcome of interest', 'parameters', 'combination', 'regression', 'linear regression', 'outcome', 'test', 'leverage', 'hypothesis', 'linear combination', 'hypothesis test']",Distribution tables,seg_99,extrapolation 322 observational unit 12 interpreting parameters 321 one-sided hypothesis test 200 r-squared (r2) 322 322–323 ordinal 15 levels 15 outcome 82 leverage 328 outcome of interest 97 linear combination 120 outlier 50 linear regression see also regression
2839,1,"['logistic', 'logistic regression', 'regression', 'parameter', 'paired']",Distribution tables,seg_99,p-value 194 loan50 403 404 paired 262 262–264 loans full schema 403 407 parameter 133 170 305 319 logistic regression see also regression
2840,1,"['transformation', 'logit transformation', 'logit']",Distribution tables,seg_99,parsimonious 353 logit transformation 372
2841,1,"['tail', 'long tail']",Distribution tables,seg_99,patients 32 long tail 45
2842,1,"['variable', 'lurking variable']",Distribution tables,seg_99,percentile 49 136 138 409 lurking variable 25
2843,1,"['chart', 'pie chart', 'error', 'estimate', 'machine learning', 'marginal', 'margin of error', 'mean', 'point estimate', 'probability', 'marginal probability']",Distribution tables,seg_99,pew energy 2018 405 machine learning (ml) 95 pie chart 66 malaria 403 placebo 18 34 mammogram 406 placebo effect 34 margin of error 184 212 212–213 playing cards 404 marginal probability 96 96–97 plug-in principle 174 mariokart 407 point estimate 44 170 170–171 mean 43 difference of means 267
2844,1,"['mse', 'interval', 'prediction', 'multiple comparisons', 'multimodal', 'probability', 'function', 'predictor', 'prediction interval', 'mutually exclusive', 'association', 'model selection', 'multiple regression', 'mean', 'standard', 'population', 'standard deviation', 'error', 'density function', 'model', 'median', 'regression', 'distribution', 'response', 'response value', 'plot', 'deviation', 'multiplication rule', 'positive association', 'mosaic plot', 'mean response value', 'weighted mean', 'mean square', 'pooled proportion', 'pooled standard deviation', 'mean square error']",Distribution tables,seg_99,average 43 difference of proportions 217 weighted mean 44 single mean 251 mean response value 334 single proportion 208 mean square between groups (msg) 289 point-slope 320 mean square error (mse) 289 pooled proportion 220 median 49 pooled standard deviation 273 midterm election 331 population 22 22–24 midterms house 407 positive association 17 mlb players 18 406 possum 404 407 mode 46 power 279 model selection 353–356 practically significant 199 mosaic plot 65 prediction interval 334 358 multimodal 46 predictor 305 multiple comparisons 293 primary 102 multiple regression see also regression probability 82 80–108 multiplication rule 90 density function 126 mutually exclusive 83 83–84 distribution 87
2845,1,"['probability table', 'negative association', 'probability', 'first quartile', 'prospective study', 'probability sample', 'sample', 'bias', 'rate', 'association', 'quartile', 'success', 'curve', 'n choose k', 'nonlinear', 'distribution', 'nonlinear curve', 'table', 'nominal', 'normal', 'protected classes', 'normal distribution']",Distribution tables,seg_99,probability of a success 144 n choose k 150 probability sample see sample ncbirths 406 probability table 136 negative association 17 prominent 46 noise 376 prosecutor’s fallacy 288 nominal 15 prospective study 25 non-response bias 24 protected classes 371 non-response rate 24 nonlinear 41 306 quartile nonlinear curve 362 first quartile 49 normal distribution 133 133 133–143 q1 49
2846,1,"['table', 'probability table', 'quartile', 'third quartile', 'normal', 'probability', 'normal probability table']",Distribution tables,seg_99,standard 133 184 q3 49 normal probability table 408 third quartile 49 nuclear survey 405
2847,1,"['distribution', 'null distribution']",Distribution tables,seg_99,r 171 null distribution 195
2848,1,"['null hypothesis', 'hypothesis']",Distribution tables,seg_99,r-squared (r2) 322 null hypothesis (h0) 189
2849,1,['null value'],Distribution tables,seg_99,random noise 72 null value 190
2850,1,"['random variable', 'variable', 'random', 'process', 'numerical']",Distribution tables,seg_99,random process 82 82–83 numerical 15 random variable 115 116 115–123
2851,1,"['data', 'randomization']",Distribution tables,seg_99,observational data 25 randomization 72
2852,1,"['rate', 'experiment', 'symmetric', 'skewed', 'regression', 'reference level', 'level', 'tail', 'long tail']",Distribution tables,seg_99,"randomized experiment 18 32 long tail 45 rate 163 right skewed 45 reference level 344 345 strong 45 50 regression 304 304–334, 343–377 symmetric 45"
2853,1,"['replicate', 'statistics', 'statistic', 'substitution approximation', 'standard normal distribution', 'logistic', 'approximation', 'row totals', 'statistically significant', 'stepwise', 'standard', 'standard deviation', 'rejection regions', 'error', 'strata', 'interaction term', 'model', 'curve', 'standard normal', 'nonlinear', 'residual plot', 'distribution', 'response', 'plot', 'deviation', 'robust statistics', 'nonlinear curve', 'residual', 'standard error', 'retrospective studies', 'interaction', 'variable', 'normal', 'tail', 'response variable', 'representative', 'normal distribution']",Distribution tables,seg_99,conditions 358–362 tail 45 interaction term 362 smallpox 404 logistic 371 371–377 sp500 1950 2018 406 model assumptions 358–362 standard deviation 47 118 model conditions 358–362 standard error (se) 171 181 multiple 346 343–362 difference in means 267 nonlinear curve 362 difference in proportions 217 technical conditions 358–362 single proportion 208 rejection regions 279 standard normal distribution 133 184 replicate 32 statistic see also summary statistic representative 24 statistically significant 199 residual 308 308–310 stem cells 406 residual plot 309 stent30 403 405 response variable 18 stent365 403 405 resume 407 stepwise 354 retrospective studies 25 stocks 18 404 robust statistics 51 strata 27 row totals 61 study participants 32 run17 406 substitution approximation 174
2854,1,"['sample', 'condition', 'success']",Distribution tables,seg_99,success 144 s 88 success-failure condition 172 208 sample 22 22–24 suits 85
2855,1,"['treatment group', 'sum of squared', 'sum of squares', 'statistic', 'sample statistic', 'random sample', 'random', 'transformation', 'sum of squares between groups', 'significance', 'cluster sample', 'sample', 'bias', 'rate', 'stratified sampling', 'symmetric', 'uncertainty', 'scatterplot', 'quartile', 'third quartile', 'sets', 'sample space', 'trial', 'error', 'random sampling', 'strata', 'sample size', 'convenience sample', 'simple random sampling', 'table proportions', 'test statistic', 'tree diagram', 'distribution', 'cluster sampling', 'level', 'test', 'sampling error', 'time series', 'plot', 'anova', 'sampling distribution', 'table', 'treatment', 'box plot', 'errors', 'sampling', 'tail', 'sampling uncertainty', 'significance level']",Distribution tables,seg_99,bias 23 23–24 sum of squared errors (sse) 289 cluster 27 sum of squares between groups 289 cluster sample 27 sum of squares total (sst ) 289 cluster sampling 28 summary statistic 10 16 51 convenience sample 24 symmetric 45 multistage sample 27 multistage sampling 28 t-distribution 253 251–254 non-response bias 24 t-probability table 412 non-response rate 24 t-score 257 random sample 23–24 t-table 253 412 simple random sampling 26 27 table proportions 96 strata 27 tail 45 stratified sampling 26 27 test statistic 136 sample proportion 144 textbooks 406 sample size 170 third quartile 49 sample space 88 time series 318 359 sample statistic 51 toy anova 406 sampling distribution 171 transformation 52 sampling error 170 inverse 361 sampling uncertainty 170 log 361 scatterplot 16 41 square root 361 sets 84 truncation 361 sham surgery 34 treatment group 9 32 side-by-side box plot 68 tree diagram 102 102–108 significance level 193 198–199 trial 144
2856,1,"['sample', 'simulated scatter', 'simulation', 'hypothesis tests', 'type 2 error', 'simple random sample', 'random sample', 'simulated', 'random', 'tests', 'error', 'type 1 error', 'hypothesis']",Distribution tables,seg_99,multiple comparisons 292–294 truncation 361 simple random sample 24 two-sided hypothesis tests 200 simulated scatter 407 type 1 error 193 simulation 72 73 type 2 error 193
2857,0,[],Distribution tables,seg_99,skew ucla textbooks f18 406
2858,1,"['unimodal', 'unbiased', 'skewed']",Distribution tables,seg_99,extreme 52 unbiased 178 left skewed 45 unimodal 46
2859,1,['observation'],Distribution tables,seg_99,unit of observation 12
2860,1,"['variable', 'venn diagrams', 'venn', 'variance']",Distribution tables,seg_99,variability 47 49 variable 12 variance 47 118 venn diagrams 85 volunteers 32
2861,1,"['with replacement', 'without replacement', 'whiskers', 'replacement', 'mean']",Distribution tables,seg_99,weighted mean 44 whiskers 50 with replacement 113 without replacement 113
2862,0,[],Distribution tables,seg_99,z 134 z-score 134
