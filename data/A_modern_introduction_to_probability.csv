,Relevance,Tags,Heading,Seg,Sentence
0,1,"['statistics', 'probability']", Why probability and statistics,seg_1,"is everything on this planet determined by randomness? this question is open to philosophical debate. what is certain is that every day thousands and thousands of engineers, scientists, business persons, manufacturers, and others are using tools from probability and statistics."
1,1,"['statistics', 'probability']", Why probability and statistics,seg_1,"the theory and practice of probability and statistics were developed during the last century and are still actively being refined and extended. in this book we will introduce the basic notions and ideas, and in this first chapter we present a diverse collection of examples where randomness plays a role."
2,1,"['random', 'percentage', 'correlation']", Biometry iris recognition,seg_3,"biometry is the art of identifying a person on the basis of his or her personal biological characteristics, such as fingerprints or voice. from recent research it appears that with the human iris one can beat all existing automatic human identification systems. iris recognition technology is based on the visible qualities of the iris. it converts these—via a video camera—into an “iris code” consisting of just 2048 bits. this is done in such a way that the code is hardly sensitive to the size of the iris or the size of the pupil. however, at different times and different places the iris code of the same person will not be exactly the same. thus one has to allow for a certain percentage of mismatching bits when identifying a person. in fact, the system allows about 34% mismatches! how can this lead to a reliable identification system? the miracle is that different persons have very different irides. in particular, over a large collection of different irides the code bits take the values 0 and 1 about half of the time. but that is certainly not sufficient: if one bit would determine the other 2047, then we could only distinguish two persons. in other words, single bits may be random, but the correlation between bits is also crucial (we will discuss correlation at length in chapter 10). john daugman who has developed the iris recognition technology made comparisons between 222743 pairs of iris"
3,1,"['interval', 'data', 'probability', 'histograms']", Biometry iris recognition,seg_3,"codes and concluded that of the 2048 bits 266 may be considered as uncorrelated ([6]). he then argues that we may consider an iris code as the result of 266 coin tosses with a fair coin. this implies that if we compare two such codes from different persons, then there is an astronomically small probability that these two differ in less than 34% of the bits—almost all pairs will differ in about 50% of the bits. this is illustrated in figure 1.1, which originates from [6], and was kindly provided by john daugman. the iris code data consist of numbers between 0 and 1, each a hamming distance (the fraction of mismatches) between two iris codes. the data have been summarized in two histograms, that is, two graphs that show the number of counts of hamming distances falling in a certain interval. we will encounter histograms and other summaries of data in chapter 15. one sees from the figure that for codes from the same iris (left side) the mismatch fraction is only about 0.09, while for different irides (right side) it is about 0.46."
4,1,['events'], Biometry iris recognition,seg_3,"you may still wonder how it is possible that irides distinguish people so well. what about twins, for instance? the surprising thing is that although the color of eyes is hereditary, many features of iris patterns seem to be produced by so-called epigenetic events. this means that during embryo development the iris structure develops randomly. in particular, the iris patterns of (monozygotic) twins are as discrepant as those of two arbitrary individuals."
5,0,[], Biometry iris recognition,seg_3,"for this reason, as early as in the 1930s, eye specialists proposed that iris patterns might be used for identification purposes."
6,1,"['interval', 'combination', 'data', 'statistical', 'population', 'standard', 'confidence', 'average', 'confidence interval']", Killer football,seg_5,"a couple of years ago the prestigious british medical journal published a paper with the title “cardiovascular mortality in dutch men during 1996 european football championship: longitudinal population study” ([41]). the authors claim to have shown that the effect of a single football match is detectable in national mortality data. they consider the mortality from infarctions (heart attacks) and strokes, and the “explanation” of the increase is a combination of heavy alcohol consumption and stress caused by watching the football match on june 22 between the netherlands and france (lost by the dutch team!). the authors mainly support their claim with a figure like figure 1.2, which shows the number of deaths from the causes mentioned (for men over 45), during the period june 17 to june 27, 1996. the middle horizontal line marks the average number of deaths on these days, and the upper and lower horizontal lines mark what the authors call the 95% confidence interval. the construction of such an interval is usually performed with standard statistical techniques, which you will learn in chapter 23. the interpretation of such an interval is rather tricky. that the bar on june 22 sticks out off the confidence interval should support the “killer claim.”"
7,1,"['poisson', 'model', 'poisson process', 'probability', 'process']", Killer football,seg_5,"it is rather surprising that such a conclusion is based on a single football match, and one could wonder why no probability model is proposed in the paper. in fact, as we shall see in chapter 12, it would not be a bad idea to model the time points at which deaths occur as a so-called poisson process."
8,1,"['poisson', 'sample', 'model', 'process', 'independent', 'sample mean', 'interval', 'poisson process', 'estimation', 'data', 'distribution', 'events', 'mean', 'probability', 'poisson distribution', 'average']", Killer football,seg_5,"once we have done this, we can compute how often a pattern like the one in the figure might occur—without paying attention to football matches and other high-risk national events. to do this we need the mean number of deaths per day. this number can be obtained from the data by an estimation procedure (the subject of chapters 19 to 23). we use the sample mean, which is equal to (10 · 27.2 + 41)/11 = 313/11 = 28.45. (here we have to make a computation like this because we only use the data in the paper: 27.2 is the average over the 5 days preceding and following the match, and 41 is the number of deaths on the day of the match.) now let phigh be the probability that there are 41 or more deaths on a day, and let pusual be the probability that there are between 21 and 34 deaths on a day—here 21 and 34 are the lowest and the highest number that fall in the interval in figure 1.2. from the formula of the poisson distribution given in chapter 12 one can compute that phigh = 0.008 and pusual = 0.820. since events on different days are independent according to the poisson process model, the probability p of a pattern as in the figure is"
9,1,['law of large numbers'], Killer football,seg_5,"from this it can be shown by (a generalization of) the law of large numbers (which we will study in chapter 13) that such a pattern would appear about once every 1/0.0011 = 899 days. so it is not overwhelmingly exceptional to find such a pattern, and the fact that there was an important football match on the day in the middle of the pattern might just have been a coincidence."
10,1,['states'], Cars and goats the Monty Hall dilemma,seg_7,"on sunday september 9, 1990, the following question appeared in the “ask marilyn” column in parade, a sunday supplement to many newspapers across the united states:"
11,0,[], Cars and goats the Monty Hall dilemma,seg_7,"suppose you’re on a game show, and you’re given the choice of three doors; behind one door is a car; behind the others, goats. you pick a door, say no. 1, and the host, who knows what’s behind the doors, opens another door, say no. 3, which has a goat. he then says to you, “do you want to pick door no. 2?” is it to your advantage to switch your choice?—craig f. whitaker, columbia, md."
12,1,['estimated'], Cars and goats the Monty Hall dilemma,seg_7,"marilyn’s answer—one should switch—caused an avalanche of reactions, in total an estimated 10 000. some of these reactions were not so flattering (“you are the goat”), quite a lot were by professional mathematicians (“you blew it, and blew it big,” “you are utterly incorrect . . . . how many irate mathematicians are needed to change your mind?”). perhaps some of the reactions were so strong, because marilyn vos savant, the author of the column, is in the guinness book of records for having one of the highest iqs in the world."
13,0,[], Cars and goats the Monty Hall dilemma,seg_7,"the switching question was inspired by monty hall’s “let’s make a deal” game show, which ran with small interruptions for 23 years on various u.s. television networks."
14,1,"['simulation', 'case', 'probability']", Cars and goats the Monty Hall dilemma,seg_7,"although it is not explicitly stated in the question, the game show host will always open a door with a goat after you make your initial choice. many people would argue that in this situation it does not matter whether one would change or not: one door has a car behind it, the other a goat, so the odds to get the car are fifty-fifty. to see why they are wrong, consider the following argument. in the original situation two of the three doors have a goat behind them, so with probability 2/3 your initial choice was wrong, and with probability 1/3 it was right. now the host opens a door with a goat (note that he can always do this). in case your initial choice was wrong the host has only one option to show a door with a goat, and switching leads you to the door with the car. in case your initial choice was right the host has two goats to choose from, so switching will lead you to a goat. we see that switching is the best strategy, doubling our chances to win. to stress this argument, consider the following generalization of the problem: suppose there are 10 000 doors, behind one is a car and behind the rest, goats. after you make your choice, the host will open 9998 doors with goats, and offers you the option to switch. to change or not to change, that’s the question! still not convinced? use your internet browser to find one of the zillion sites where one can run a simulation of the monty hall problem (more about simulation in chapter 6)."
15,1,"['moment', 'law of total probability', 'variations', 'probability', 'total probability']", Cars and goats the Monty Hall dilemma,seg_7,"in fact, there are quite a lot of variations on the problem. for example, the situation that there are four doors: you select a door, the host always opens a door with a goat, and offers you to select another door. after you have made up your mind he opens a door with a goat, and again offers you to switch. after you have decided, he opens the door you selected. what is now the best strategy? in this situation switching only at the last possible moment yields a probability of 3/4 to bring the car home. using the law of total probability from section 3.3 you will find that this is indeed the best possible strategy."
16,1,['space shuttle challenger'], The space shuttle Challenger,seg_9,"on january 28, 1986, the space shuttle challenger exploded about one minute after it had taken off from the launch pad at kennedy space center in florida. the seven astronauts on board were killed and the spacecraft was destroyed. the cause of the disaster was explosion of the main fuel tank, caused by flames of hot gas erupting from one of the so-called solid rocket boosters."
17,1,[], The space shuttle Challenger,seg_9,"these solid rocket boosters had been cause for concern since the early years of the shuttle. they are manufactured in segments, which are joined at a later stage, resulting in a number of joints that are sealed to protect against leakage. this is done with so-called o-rings, which in turn are protected by a layer of putty. when the rocket motor ignites, high pressure and high temperature"
18,1,[], The space shuttle Challenger,seg_9,"build up within. in time these may burn away the putty and subsequently erode the o-rings, eventually causing hot flames to erupt on the outside. in a nutshell, this is what actually happened to the challenger."
19,1,"['predicted', 'set', 'limit']", The space shuttle Challenger,seg_9,"after the explosion, an investigative commission determined the causes of the disaster, and a report was issued with many findings and recommendations ([24]). on the evening of january 27, a decision to launch the next day had been made, notwithstanding the fact that an extremely low temperature of 31◦f had been predicted, well below the operating limit of 40◦f set by morton thiokol, the manufacturer of the solid rocket boosters. apparently, a “man- agement decision” was made to overrule the engineers’ recommendation not to launch. the inquiry faulted both nasa and morton thiokol management for giving in to the pressure to launch, ignoring warnings about problems with the seals."
20,1,"['failures', 'data', 'case']", The space shuttle Challenger,seg_9,"the challenger launch was the 24th of the space shuttle program, and we shall look at the data on the number of failed o-rings, available from previous launches (see [5] for more details). each rocket has three o-rings, and two rocket boosters are used per launch, so in total six o-rings are used each time. because low temperatures are known to adversely affect the o-rings, we also look at the corresponding launch temperature. in figure 1.3 the dots show the number of failed o-rings per mission (there are 23 dots—one time the boosters could not be recovered from the ocean; temperatures are rounded to the nearest degree fahrenheit; in case of two or more equal data points these are shifted slightly.). if you ignore the dots representing zero failures, which all occurred at high temperatures, a temperature effect is not apparent."
21,1,"['model', 'logistic', 'data', 'distribution', 'binomial', 'probability', 'dependence', 'binomial distribution', 'logistic model']", The space shuttle Challenger,seg_9,"in a model to describe these data, the probability p(t) that an individual o-ring fails should depend on the launch temperature t. per mission, the number of failed o-rings follows a so-called binomial distribution: six o-rings, and each may fail with probability p(t); more about this distribution and the circumstances under which it arises can be found in chapter 4. a logistic model was used in [5] to describe the dependence on t:"
22,1,"['estimated', 'method', 'failures', 'probabilities', 'method of maximum likelihood', 'results', 'likelihood', 'data', 'maximum likelihood', 'probability', 'function']", The space shuttle Challenger,seg_9,"a high value of a + b · t corresponds to a high value of p(t), a low value to low p(t). values of a and b were determined from the data, according to the following principle: choose a and b so that the probability that we get data as in figure 1.3 is as high as possible. this is an example of the use of the method of maximum likelihood, which we shall discuss in chapter 21. this results in a = 5.085 and b = −0.1156, which indeed leads to lower probabilities at higher temperatures, and to p(31) = 0.8178. we can also compute the (estimated) expected number of failures, 6 ·p(t), as a function of the launch temperature t; this is the plotted line in the figure."
23,1,"['estimated', 'probabilities', 'estimates', 'failure', 'events', 'probability']", The space shuttle Challenger,seg_9,"combining the estimates with estimated probabilities of other events that should happen for a complete failure of the field-joint, the estimated probability of such a failure is 0.023. with six field-joints, the probability of at least one complete failure is then 1 − (1 − 0.023)6 = 0.13!"
24,1,"['estimates', 'information']", Statistics versus intelligence agencies,seg_11,"during world war ii, information about germany’s war potential was essential to the allied forces in order to schedule the time of invasions and to carry out the allied strategic bombing program. methods for estimating german production used during the early phases of the war proved to be inadequate. in order to obtain more reliable estimates of german war production, experts from the economic warfare division of the american embassy and the british ministry of economic warfare started to analyze markings and serial numbers obtained from captured german equipment."
25,1,"['information', 'control', 'location']", Statistics versus intelligence agencies,seg_11,"each piece of enemy equipment was labeled with markings, which included all or some portion of the following information: (a) the name and location of the marker; (b) the date of manufacture; (c) a serial number; and (d) miscellaneous markings such as trademarks, mold numbers, casting numbers, etc. the purpose of these markings was to maintain an effective check on production standards and to perform spare parts control. however, these same markings offered allied intelligence a wealth of information about german industry."
26,0,[], Statistics versus intelligence agencies,seg_11,"the first products to be analyzed were tires taken from german aircraft shot over britain and from supply dumps of aircraft and motor vehicle tires captured in north africa. the marking on each tire contained the maker’s name,"
27,1,"['table', 'variations']", Statistics versus intelligence agencies,seg_11,"a serial number, and a two-letter code for the date of manufacture. the first step in analyzing the tire markings involved breaking the two-letter date code. it was conjectured that one letter represented the month and the other the year of manufacture, and that there should be 12 letter variations for the month code and 3 to 6 for the year code. this, indeed, turned out to be true. the following table presents examples of the 12 letter variations used by four different manufacturers."
28,1,"['method', 'estimate', 'estimation', 'average']", Statistics versus intelligence agencies,seg_11,"for instance, the dunlop code was dunlop arbeit spelled backwards. next, the year code was broken and the numbering system was solved so that for each manufacturer individually the serial numbers could be dated. moreover, for each month, the serial numbers could be recoded to numbers running from 1 to some unknown largest number n , and the observed (recoded) serial numbers could be seen as a subset of this. the objective was to estimate n for each month and each manufacturer separately by means of the observed (recoded) serial numbers. in chapter 20 we discuss two different methods of estimation, and we show that the method based on only the maximum observed (recoded) serial number is much better than the method based on the average observed (recoded) serial numbers."
29,1,"['sample', 'estimated', 'estimates', 'table', 'statistics', 'average']", Statistics versus intelligence agencies,seg_11,"with a sample of about 1400 tires from five producers, individual monthly output figures were obtained for almost all months over a period from 1939 to mid-1943. the following table compares the accuracy of estimates of the average monthly production of all manufacturers of the first quarter of 1943 with the statistics of the speer ministry that became available after the war. the accuracy of the estimates can be appreciated even more if we compare them with the figures obtained by allied intelligence agencies. they estimated, using other methods, the production between 900 000 and 1 200 000 per month!"
30,1,['estimated'], Statistics versus intelligence agencies,seg_11,type of tire estimated production actual production
31,1,"['experimental', 'interval', 'observations', 'measurements', 'experiments', 'statistical']", The speed of light,seg_13,"in 1983 the definition of the meter (the si unit of one meter) was changed to: the meter is the length of the path traveled by light in vacuum during a time interval of 1/299 792 458 of a second. this implicitly defines the speed of light as 299 792 458 meters per second. it was done because one thought that the speed of light was so accurately known that it made more sense to define the meter in terms of the speed of light rather than vice versa, a remarkable end to a long story of scientific discovery. for a long time most scientists believed that the speed of light was infinite. early experiments devised to demonstrate the finiteness of the speed of light failed because the speed is so extraordinarily high. in the 18th century this debate was settled, and work started on determination of the speed, using astronomical observations, but a century later scientists turned to earth-based experiments. albert michelson refined experimental arrangements from two previous experiments and conducted a series of measurements in june and early july of 1879, at the u.s. naval academy in annapolis. in this section we give a very short summary of his work. it is extracted from an article in statistical science ([18])."
32,1,"['measurement', 'interval']", The speed of light,seg_13,"the principle of speed measurement is easy, of course: measure a distance and the time it takes to travel that distance, the speed equals distance divided by time. for an accurate determination, both the distance and the time need to be measured accurately, and with the speed of light this is a problem: either we should use a very large distance and the accuracy of the distance measurement is a problem, or we have a very short time interval, which is also very difficult to measure accurately."
33,1,['experimental'], The speed of light,seg_13,"in michelson’s time it was known that the speed of light was about 300 000 km/s, and he embarked on his study with the goal of an improved value of the speed of light. his experimental setup is depicted schematically in figure 1.4. light emitted from a light source is aimed, through a slit in a fixed plate, at a rotating mirror; we call its distance from the plate the radius. at one particular angle, this rotating mirror reflects the beam in the direction of a distant (fixed) flat mirror. on its way the light first passes through a focusing lens. this second mirror is positioned in such a way that it reflects the beam back in the direction of the rotating mirror. in the time it takes the light to travel back and forth between the two mirrors, the rotating mirror has moved by an angle α, resulting in a reflection on the plate that is displaced with respect to the source beam that passed through the slit. the radius and the displacement determine the angle α because"
34,0,[], The speed of light,seg_13,displacement tan 2α = radius
35,0,[], The speed of light,seg_13,"and combined with the number of revolutions per seconds (rps) of the mirror, this determines the elapsed time:"
36,0,[], The speed of light,seg_13,"during this time the light traveled twice the distance between the mirrors, so the speed of light in air now follows:"
37,0,[], The speed of light,seg_13,2 · distance cair = . time
38,1,['error'], The speed of light,seg_13,"all in all, it looks simple: just measure the four quantities—distance, radius, displacement and the revolutions per second—and do the calculations. this is much harder than it looks, and problems in the form of inaccuracies are lurking everywhere. an error in any of these quantities translates directly into some error in the final result."
39,1,"['nominal', 'measuring', 'error', 'errors']", The speed of light,seg_13,"michelson did the utmost to reduce errors. for example, the distance between the mirrors was about 2000 feet, and to measure it he used a steel measuring tape. its nominal length was 100 feet, but he carefully checked this using a copy of the official “standard yard.” he found that the tape was in fact 100.006 feet. this way he eliminated a (small) systematic error."
40,1,"['average', 'measuring', 'error', 'variation']", The speed of light,seg_13,"now imagine using the tape to measure a distance of 2000 feet: you have to use the tape 20 times, each time marking the next 100 feet. do it again, and you probably find a slightly different answer, no matter how hard you try to be very precise in every step of the measuring procedure. this kind of variation is inevitable: sometimes we end up with a value that is a bit too high, other times it is too low, but on average we’re doing okay—assuming that we have eliminated sources of systematic error, as in the measuring tape. michelson measured the distance five times, which resulted in values between 1984.93 and 1985.17 feet (after correcting for the temperature-dependent stretch), and he used the average as the “true distance.”"
41,1,"['errors', 'systematic errors', 'process', 'measuring']", The speed of light,seg_13,in many phases of the measuring process michelson attempted to identify and determine systematic errors and subsequently applied corrections. he
42,1,"['confidence intervals', 'interval', 'measurements', 'dataset', 'uncertainty', 'results', 'data', 'intervals', 'confidence', 'error', 'confidence interval', 'variability', 'variables', 'table', 'measuring']", The speed of light,seg_13,"also systematically repeated measuring steps and averaged the results to reduce variability. his final dataset consists of 100 separate measurements (see table 17.1), but each is in fact summarized and averaged from repeated measurements on several variables. the final result he reported was that the speed of light in vacuum (this involved a conversion) was 299 944± 51 km/s, where the 51 is an indication of the uncertainty in the answer. in retrospect, we must conclude that, in spite of michelson’s admirable meticulousness, some source of error must have slipped his attention, as his result is off by about 150 km/s. with current methods we would derive from his data a so-called 95% confidence interval: 299 944 ± 15.5 km/s, suggesting that michelson’s uncertainty analysis was a little conservative. the methods used to construct confidence intervals are the topic of chapters 23 and 24."
43,1,"['sample', 'model', 'experiment', 'events', 'probability', 'random', 'sample space', 'outcomes', 'event']", Outcomes events and probability,seg_15,"the world around us is full of phenomena we perceive as random or unpredictable. we aim to model these phenomena as outcomes of some experiment, where you should think of experiment in a very general sense. the outcomes are elements of a sample space ω, and subsets of ω are called events.the events will be assigned a probability, a number between 0 and 1 that expresses how likely the event is to occur."
44,1,"['experiment', 'sets', 'outcomes']", Sample spaces,seg_17,sample spaces are simply sets whose elements describe the outcomes of the experiment in which we are interested.
45,1,"['sample', 'experiment', 'tails', 'set', 'associated', 'sample space', 'outcomes']", Sample spaces,seg_17,"we start with the most basic experiment: the tossing of a coin. assuming that we will never see the coin land on its rim, there are two possible outcomes: heads and tails. we therefore take as the sample space associated with this experiment the set ω = {h, t }."
46,1,"['sample', 'experiment', 'sample space']", Sample spaces,seg_17,in another experiment we ask the next person we meet on the street in which month her birthday falls. an obvious choice for the sample space is
47,1,"['sample', 'model', 'experiment', 'outcome', 'sample space', 'outcomes', 'limit']", Sample spaces,seg_17,"in a third experiment we load a scale model for a bridge up to the point where the structure collapses. the outcome is the load at which this occurs. in reality, one can only measure with finite accuracy, e.g., to five decimals, and a sample space with just those numbers would strictly be adequate. however, in principle, the load itself could be any positive number and therefore ω = (0,∞) is the right choice. even though in reality there may also be an upper limit to what loads are conceivable, it is not necessary or practical to try to limit the outcomes correspondingly."
48,1,"['sample', 'experiment', 'sample space']", Sample spaces,seg_17,"in a fourth experiment, we find on our doormat three envelopes, sent to us by three different persons, and we look in which order the envelopes lie on top of each other. coding them 1, 2, and 3, the sample space would be"
49,1,"['sample', 'sample space']", Sample spaces,seg_17,"quick exercise 2.1 if we received mail from four different persons, how many elements would the corresponding sample space have?"
50,1,"['permutation', 'permutations']", Sample spaces,seg_17,"in general one might consider the order in which n different objects can be placed. this is called a permutation of the n objects. as we have seen, there are 6 possible permutations of 3 objects, and 4 · 6 = 24 of 4 objects. what happens is that if we add the nth object, then this can be placed in any of n positions in any of the permutations of n − 1 objects. therefore there are"
51,1,"['permutations', 'standard']", Sample spaces,seg_17,possible permutations of n objects. here n! is the standard notation for this product and is pronounced “n factorial.” it is convenient to define 0! = 1.
52,1,"['sample', 'experiment', 'events', 'set', 'event', 'outcome', 'sample space', 'outcomes']", Events,seg_19,"subsets of the sample space are called events . we say that an event a occurs if the outcome of the experiment is an element of the set a. for example, in the birthday experiment we can ask for the outcomes that correspond to a long month, i.e., a month with 31 days. this is the event"
53,1,['set'], Events,seg_19,events may be combined according to the usual set operations.
54,1,['event'], Events,seg_19,"for example if r is the event that corresponds to the months that have the letter r in their (full) name (so r = {jan, feb, mar, apr, sep, oct, nov, dec}), then the long months that contain the letter r are"
55,1,"['intersection', 'sets', 'events', 'complement', 'set', 'union', 'event']", Events,seg_19,"the set l∩r is called the intersection of l and r and occurs if both l and r occur. similarly, we have the union a∪b of two sets a and b, which occurs if at least one of the events a and b occurs. another common operation is taking complements. the event ac = {ω ∈ ω : ω ∈/ a} is called the complement of a; it occurs if and only if a does not occur. the complement of ω is denoted ∅, the empty set, which represents the impossible event. figure 2.1 illustrates these three set operations."
56,1,"['disjoint', 'mutually exclusive', 'set', 'events', 'event', 'outcomes']", Events,seg_19,"we call events a and b disjoint or mutually exclusive if a and b have no outcomes in common; in set terminology: a∩b = ∅. for example, the event l “the birthday falls in a long month” and the event {feb} are disjoint."
57,1,"['set', 'event', 'outcomes']", Events,seg_19,"finally, we say that event a implies event b if the outcomes of a also lie in b. in set notation: a ⊂ b; see figure 2.2."
58,0,[], Events,seg_19,some people like to use double negations:
59,0,[], Events,seg_19,“it is certainly not true that neither john nor mary is to blame.”
60,1,['events'], Events,seg_19,"this is equivalent to: “john or mary is to blame, or both.” the following useful rules formalize this mental operation to a manipulation with events."
61,1,['events'], Events,seg_19,demorgan’s laws. for any two events a and b we have
62,1,"['events', 'event']", Events,seg_19,"quick exercise 2.2 let j be the event “john is to blame” and m the event “mary is to blame.” express the two statements above in terms of the events j, jc, m , and m c, and check the equivalence of the statements by means of demorgan’s laws."
63,1,"['probabilities', 'events', 'probability', 'event', 'function', 'probability function']", Probability,seg_21,"we want to express how likely it is that an event occurs. to do this we will assign a probability to each event. the assignment of probabilities to events is in general not an easy task, and some of the coming chapters will be dedicated directly or indirectly to this problem. since each event has to be assigned a probability, we speak of a probability function. it has to satisfy two basic properties."
64,1,"['sample', 'disjoint', 'sample space', 'probability', 'event', 'function', 'probability function']", Probability,seg_21,"definition. a probability function p on a finite sample space ω assigns to each event a in ω a number p(a) in [0,1] such that (i) p(ω) = 1, and (ii) p(a ∪ b) = p(a) + p(b) if a and b are disjoint. the number p(a) is called the probability that a occurs."
65,1,"['disjoint events', 'probability', 'function', 'sample', 'experiment', 'sets', 'events', 'sample space', 'disjoint', 'additivity property', 'additivity of the probability function', 'outcome', 'probability function', 'additivity property of a probability function']", Probability,seg_21,"property (i) expresses that the outcome of the experiment is always an element of the sample space, and property (ii) is the additivity property of a probability function. it implies additivity of the probability function over more than two sets; e.g., if a, b, and c are disjoint events, then the two events a ∪ b and c are also disjoint, so"
66,1,"['experiment', 'outcome', 'tails']", Probability,seg_21,"we will now look at some examples. when we want to decide whether peter or paul has to wash the dishes, we might toss a coin. the fact that we consider this a fair way to decide translates into the opinion that heads and tails are equally likely to occur as the outcome of the coin-tossing experiment. so we"
67,1,"['events', 'set', 'probability', 'function', 'outcomes', 'probability function']", Probability,seg_21,"formally we have to write {h} for the set consisting of the single element h , because a probability function is defined on events, not on outcomes. from now on we shall drop these brackets."
68,1,"['distribution', 'asymmetric', 'case']", Probability,seg_21,"now it might happen, for example due to an asymmetric distribution of the mass over the coin, that the coin is not completely fair. for example, it might be the case that"
69,1,"['probabilities', 'experiment', 'experiments', 'outcomes']", Probability,seg_21,"more generally we can consider experiments with two possible outcomes, say “failure” and “success”, which have probabilities 1− p and p to occur, where p is a number between 0 and 1. for example, when our experiment consists of buying a ticket in a lottery with 10 000 tickets and only one prize, where “success” stands for winning the prize, then p = 10−4."
70,1,"['probabilities', 'experiment']", Probability,seg_21,"how should we assign probabilities in the second experiment, where we ask for the month in which the next person we meet has his or her birthday? in analogy with what we have just done, we put"
71,0,[], Probability,seg_21,"some of you might object to this and propose that we put, for example,"
72,0,[], Probability,seg_21,because we have long months and short months. but then the very precise among us might remark that this does not yet take care of leap years.
73,1,"['approximation', 'probability']", Probability,seg_21,"quick exercise 2.3 if you would take care of the leap years, assuming that one in every four years is a leap year (which again is an approximation to reality!), how would you assign a probability to each month?"
74,1,"['sample', 'sample spaces', 'experiment', 'probability', 'outcome', 'outcomes']", Probability,seg_21,"in the third experiment (the buckling load of a bridge), where the outcomes are real numbers, it is impossible to assign a positive probability to each outcome (there are just too many outcomes!). we shall come back to this problem in chapter 5, restricting ourselves in this chapter to finite and countably infinite1 sample spaces."
75,1,"['probabilities', 'experiment', 'outcomes']", Probability,seg_21,in the fourth experiment it makes sense to assign equal probabilities to all six outcomes:
76,1,"['additivity property', 'probabilities', 'experiment', 'events', 'probability', 'event', 'experiments', 'outcomes']", Probability,seg_21,"until now we have only assigned probabilities to the individual outcomes of the experiments. to assign probabilities to events we use the additivity property. for instance, to find the probability p(t ) of the event t that in the three envelopes experiment envelope 2 is on top we note that"
77,1,"['probabilities', 'probability of an event', 'probability', 'event', 'outcomes']", Probability,seg_21,"in general, additivity of p implies that the probability of an event is obtained by summing the probabilities of the outcomes belonging to the event."
78,1,['experiment'], Probability,seg_21,quick exercise 2.4 compute p(l) and p(r) in the birthday experiment.
79,1,"['disjoint', 'probabilities', 'events', 'union']", Probability,seg_21,"finally we mention a rule that permits us to compute probabilities of events a and b that are not disjoint. note that we can write a = (a∩b) ∪ (a∩bc), which is a disjoint union; hence"
80,1,['events'], Probability,seg_21,"if we split a ∪ b in the same way with b and bc, we obtain the events (a∪b)∩b, which is simply b and (a∪b)∩bc, which is nothing but a∩bc."
81,0,[], Probability,seg_21,eliminating p(a ∩ bc) from these two equations we obtain the following rule.
82,1,"['events', 'union', 'probability']", Probability,seg_21,the probability of a union. for any two events a and b we have p(a ∪ b) = p(a) + p(b) − p(a ∩ b) .
83,1,"['additivity property', 'probabilities', 'events']", Probability,seg_21,"from the additivity property we can also find a way to compute probabilities of complements of events: from a ∪ ac = ω, we deduce that"
84,1,"['sample', 'experiment', 'statistics', 'set', 'associated', 'sample space']", Products of sample spaces,seg_23,"basic to statistics is that one usually does not consider one experiment, but that the same experiment is performed several times. for example, suppose we throw a coin two times. what is the sample space associated with this new experiment? it is clear that it should be the set"
85,1,"['probabilities', 'experiment', 'outcomes']", Products of sample spaces,seg_23,"if in the original experiment we had a fair coin, i.e., p(h) = p(t ), then in this new experiment all 4 outcomes again have equal probabilities:"
86,1,"['sample', 'sample spaces', 'experiment', 'set', 'experiments', 'sample space']", Products of sample spaces,seg_23,"somewhat more generally, if we consider two experiments with sample spaces ω1 and ω2 then the combined experiment has as its sample space the set"
87,1,"['probabilities', 'experiment', 'case', 'mean', 'probability', 'outcome', 'experiments', 'outcomes']", Products of sample spaces,seg_23,"if ω1 has r elements and ω2 has s elements, then ω1 × ω2 has rs elements. now suppose that in the first, the second, and the combined experiment all outcomes are equally likely to occur. then the outcomes in the first experiment have probability 1/r to occur, those of the second experiment 1/s, and those of the combined experiment probability 1/rs. motivated by the fact that 1/rs = (1/r) × (1/s), we will assign probability pipj to the outcome (ωi, ωj) in the combined experiment, in the case that ωi has probability pi and ωj has probability pj to occur. one should realize that this is by no means the only way to assign probabilities to the outcomes of a combined experiment. the preceding choice corresponds to the situation where the two experiments do not influence each other in any way. what we mean by this influence will be explained in more detail in the next chapter."
88,1,"['sample', 'probabilities', 'experiment', 'probability', 'associated', 'outcome', 'sample space']", Products of sample spaces,seg_23,"quick exercise 2.5 consider the sample space {a1, a2, a3, a4, a5, a6} of some experiment, where outcome ai has probability pi for i = 1, . . . , 6. we perform this experiment twice in such a way that the associated probabilities are"
89,1,"['sample', 'experiment', 'probability', 'function', 'sample space', 'probability function']", Products of sample spaces,seg_23,"check that p is a probability function on the sample space ω = {a1, . . . , a6}× {a1, . . . , a6} of the combined experiment. what is the relationship between the first experiment and the second experiment that is determined by this probability function?"
90,1,"['experiment', 'failure', 'set', 'success', 'associated', 'outcomes', 'event']", Products of sample spaces,seg_23,"we started this section with the experiment of throwing a coin twice. if we want to learn more about the randomness associated with a particular experiment, then we should repeat it more often, say n times. for example, if we perform an experiment with outcomes 1 (success) and 0 (failure) five times, and we consider the event a “exactly one experiment was a success,” then this event is given by the set"
91,1,"['success', 'failure', 'probability']", Products of sample spaces,seg_23,"in ω = {0, 1} × {0, 1} × {0, 1} × {0, 1} × {0, 1}. moreover, if success has probability p and failure probability 1 − p, then"
92,1,"['probability', 'event', 'outcomes']", Products of sample spaces,seg_23,"since there are five outcomes in the event a, each having probability (1−p)4 ·p."
93,1,"['probability of the event', 'probability', 'event', 'experiments']", Products of sample spaces,seg_23,quick exercise 2.6 what is the probability of the event b “exactly two experiments were successful”?
94,1,"['sample', 'experiment', 'sample space']", Products of sample spaces,seg_23,"in general, when we perform an experiment n times, then the corresponding sample space is"
95,1,"['sample', 'probabilities', 'standard', 'sample space', 'outcomes']", Products of sample spaces,seg_23,"where ωi for i = 1, . . . , n is a copy of the sample space of the original experiment. moreover, we assign probabilities to the outcomes (ω1, . . . , ωn) in the standard way described earlier, i.e.,"
96,1,['probability'], Products of sample spaces,seg_23,if each ωi has probability pi.
97,1,"['outcome', 'outcomes', 'experiment']", An infinite sample space,seg_25,we end this chapter with an example of an experiment with infinitely many outcomes. we toss a coin repeatedly until the first head turns up. the outcome
98,1,"['sample', 'experiment', 'sample space']", An infinite sample space,seg_25,of the experiment is the number of tosses it takes to have this first occurrence of a head. our sample space is the space of all positive natural numbers
99,1,"['experiment', 'probability', 'function', 'probability function']", An infinite sample space,seg_25,what is the probability function p for this experiment?
100,1,"['probability', 'event', 'outcome', 'tails']", An infinite sample space,seg_25,"suppose the coin has probability p of falling on heads and probability 1−p to fall on tails, where 0 < p < 1. we determine the probability p(n) for each n. clearly p(1) = p, the probability that we have a head right away. the event {2} corresponds to the outcome (t, h) in {h, t }×{h, t }, so we should have"
101,1,"['outcome', 'event']", An infinite sample space,seg_25,"similarly, the event {n} corresponds to the outcome (t, t, . . . , t, t, h) in the space {h, t } × · · · × {h, t }. hence we should have, in general,"
102,1,"['sample', 'probability', 'function', 'sample space', 'probability function']", An infinite sample space,seg_25,"does this define a probability function on ω = {1, 2, 3, . . .}? then we should at least have p(ω) = 1. it is not directly clear how to calculate p(ω): since the sample space is no longer finite we have to amend the definition of a probability function."
103,1,"['sample', 'disjoint', 'sample space', 'disjoint events', 'events', 'probability', 'event', 'function', 'probability function']", An infinite sample space,seg_25,"definition. a probability function on an infinite (or finite) sample space ω assigns to each event a in ω a number p(a) in [0, 1] such that (i) p(ω) = 1, and (ii) p(a1 ∪ a2 ∪ a3 ∪ · · ·) = p(a1) + p(a2) + p(a3) + · · · if a1, a2, a3, . . . are disjoint events."
104,1,['additivity property'], An infinite sample space,seg_25,"note that this new additivity property is an extension of the previous one because if we choose a3 = a4 = · · · = ∅, then"
105,1,['probability'], An infinite sample space,seg_25,now we can compute the probability of ω:
106,1,"['geometric', 'geometric series']", An infinite sample space,seg_25,"the sum 1 + (1 − p) + · · · + (1 − p)n−1 + · · · is an example of a geometric series. it is well known that when |1 − p| < 1,"
107,1,"['experiment', 'successful', 'success', 'probability of success', 'probability']", An infinite sample space,seg_25,"quick exercise 2.7 suppose an experiment in a laboratory is repeated every day of the week until it is successful, the probability of success being p. the first experiment is started on a monday. what is the probability that the series ends on the next sunday?"
108,1,"['sample', 'experiment', 'sample space', 'outcomes']", Solutions to the quick exercises,seg_27,"2.1 the sample space is ω = {1234, 1243, 1324, 1342, . . . , 4321}. the best way to count its elements is by noting that for each of the 6 outcomes of the threeenvelope experiment we can put a fourth envelope in any of 4 positions. hence ω has 4 · 6 = 24 elements."
109,1,['event'], Solutions to the quick exercises,seg_27,"2.2 the statement “it is certainly not true that neither john nor mary is to blame” corresponds to the event (jc ∩m c)c. the statement “john or mary is to blame, or both” corresponds to the event j ∪ m . equivalence now follows from demorgan’s laws."
110,1,['probability'], Solutions to the quick exercises,seg_27,"2.3 in four years we have 365×3+366 = 1461 days. hence long months each have a probability 4 × 31/1461 = 124/1461, and short months a probability 120/1461 to occur. moreover, {feb} has probability 113/1461."
111,1,"['function', 'probability function', 'probability']", Solutions to the quick exercises,seg_27,"2.5 checking that p is a probability function ω amounts to verifying that 0 ≤ p((ai, aj)) ≤ 1 for all i and j and noting that"
112,1,"['outcome', 'experiments']", Solutions to the quick exercises,seg_27,the two experiments are totally coupled: one has outcome ai if and only if the other has outcome ai.
113,1,"['outcomes', 'probability']", Solutions to the quick exercises,seg_27,"2.6 now there are 10 outcomes in b (for example (0,1,0,1,0)), each having probability (1 − p)3p2. hence p(b) = 10(1 − p)3p2."
114,1,"['success', 'probability', 'experiment']", Solutions to the quick exercises,seg_27,"2.7 this happens if and only if the experiment fails on monday,. . . , saturday, and is a success on sunday. this has probability p(1 − p)6 to happen."
115,1,"['sample', 'events', 'sample space']", Exercises,seg_29,"2.1 let a and b be two events in a sample space for which p(a) = 2/3, p(b) = 1/6, and p(a ∩ b) = 1/9. what is p(a ∪ b)?"
116,1,"['events', 'probability']", Exercises,seg_29,2.2 let e and f be two events for which one knows that the probability that at least one of them occurs is 3/4. what is the probability that neither e nor f occurs? hint: use one of demorgan’s laws: ec ∩ f c = (e ∪ f )c.
117,1,['events'], Exercises,seg_29,"2.3 let c and d be two events for which one knows that p(c) = 0.3, p(d) = 0.4, and p(c ∩ d) = 0.2. what is p(cc ∩ d)?"
118,1,"['experiment', 'events', 'probability']", Exercises,seg_29,"2.4 we consider events a, b, and c, which can occur in some experiment. is it true that the probability that only a occurs (and not b or c) is equal to p(a ∪ b ∪ c) − p(b) − p(c) + p(b ∩ c)?"
119,1,['event'], Exercises,seg_29,"2.5 the event a∩bc that a occurs but not b is sometimes denoted as a\b. here \ is the set-theoretic minus sign. show that p(a \ b) = p(a) − p(b) if b implies a, i.e., if b ⊂ a."
120,1,"['events', 'probability']", Exercises,seg_29,"2.7 let a and b be two events. suppose that p(a) = 0.4, p(b) = 0.5, and p(a ∩ b) = 0.1. find the probability that a or b occurs, but not both."
121,1,"['events', 'probability']", Exercises,seg_29,"2.8 suppose the events d1 and d2 represent disasters, which are rare: p(d1) ≤ 10−6 and p(d2) ≤ 10−6. what can you say about the probability that at least one of the disasters occurs? what about the probability that they both occur?"
122,1,"['sample', 'experiment', 'sample space']", Exercises,seg_29,2.9 we toss a coin three times. for this experiment we choose the sample space
123,1,['tails'], Exercises,seg_29,where t stands for tails and h for heads.
124,1,"['set', 'outcomes']", Exercises,seg_29,a. write down the set of outcomes corresponding to each of the following
125,1,"['results', 'tails']", Exercises,seg_29,a : “we throw tails exactly two times.” b : “we throw tails at least two times.” c : “tails did not appear before a head appeared.” d : “the first throw results in tails.”
126,1,"['set', 'outcomes']", Exercises,seg_29,b. write down the set of outcomes corresponding to each of the following
127,1,"['sample', 'events', 'event', 'sample space']", Exercises,seg_29,"2.10 in some sample space we consider two events a and b. let c be the event that a or b occurs, but not both. express c in terms of a and b, using only the basic operations “union,” “intersection,” and “complement.”"
128,1,"['experiment', 'probability', 'outcomes']", Exercises,seg_29,"2.11 an experiment has only two outcomes. the first has probability p to occur, the second probability p2. what is p?"
129,1,['probability'], Exercises,seg_29,"2.12 in the uefa euro 2004 playoffs draw 10 national football teams were matched in pairs. a lot of people complained that “the draw was not fair,” because each strong team had been matched with a weak team (this is commercially the most interesting). it was claimed that such a matching is extremely unlikely. we will compute the probability of this “dream draw” in this exercise. in the spirit of the three-envelope example of section 2.1 we put the names of the 5 strong teams in envelopes labeled 1, 2, 3, 4, and 5 and of the 5 weak teams in envelopes labeled 6, 7, 8, 9, and 10. we shuffle the 10 envelopes and then match the envelope on top with the next envelope, the third envelope with the fourth envelope, and so on. one particular way a “dream draw” occurs is when the five envelopes labeled 1, 2, 3, 4, 5 are in the odd numbered positions (in any order!) and the others are in the even numbered positions. this way corresponds to the situation where the first match of each strong team is a home match. since for each pair there are two possibilities for the home match, the total number of possibilities for the “dream draw” is 25 = 32 times as large."
130,1,"['outcome', 'experiment']", Exercises,seg_29,"a. an outcome of this experiment is a sequence like 4, 9, 3, 7, 5, 10, 1, 8, 2, 6 of"
131,1,"['outcome', 'probability']", Exercises,seg_29,labels of envelopes. what is the probability of an outcome?
132,1,"['outcomes', 'event']", Exercises,seg_29,b. how many outcomes are there in the event “the five envelopes labeled
133,1,['probability'], Exercises,seg_29,c. what is the probability of a “dream draw”?
134,1,"['sample', 'sample spaces', 'experiment']", Exercises,seg_29,"2.13 in some experiment first an arbitrary choice is made out of four possibilities, and then an arbitrary choice is made out of the remaining three possibilities. one way to describe this is with a product of two sample spaces {a, b, c, d}:"
135,1,"['outcomes', 'probabilities', 'table']", Exercises,seg_29,a. make a 4×4 table in which you write the probabilities of the outcomes.
136,1,['event'], Exercises,seg_29,b. describe the event “c is one of the chosen possibilities” and determine its
137,1,"['sample', 'sample space']", Exercises,seg_29,"2.14 consider the monty hall “experiment” described in section 1.3. the door behind which the car is parked we label a, the other two b and c. as the sample space we choose a product space"
138,0,[], Exercises,seg_29,"here the first entry gives the choice of the candidate, and the second entry the choice of the quizmaster."
139,1,"['outcomes', 'probabilities', 'table']", Exercises,seg_29,a. make a 3×3 table in which you write the probabilities of the outcomes.
140,0,[], Exercises,seg_29,"n.b. you should realize that the candidate does not know that the car is in a, but the quizmaster will never open the door labeled a because he knows that the car is there. you may assume that the quizmaster makes an arbitrary choice between the doors labeled b and c, when the candidate chooses door a."
141,0,[], Exercises,seg_29,b. consider the situation of a “no switching” candidate who will stick to his
142,1,"['probability', 'event']", Exercises,seg_29,"or her choice. what is the event “the candidate wins the car,” and what is its probability?"
143,0,[], Exercises,seg_29,c. consider the situation of a “switching” candidate who will not stick to
144,1,"['probability', 'event']", Exercises,seg_29,"her choice. what is now the event “the candidate wins the car,” and what is its probability?"
145,1,"['events', 'union', 'probability']", Exercises,seg_29,"2.15 the rule p(a ∪ b) = p(a) + p(b)−p(a ∩ b) from section 2.3 is often useful to compute the probability of the union of two events. what would be the corresponding rule for three events a, b, and c? it should start with"
146,0,[], Exercises,seg_29,"hint: you could use the sum rule suitably, or you could make a diagram as in figure 2.1."
147,1,"['events', 'information']", Exercises,seg_29,"2.16 three events e, f , and g cannot occur simultaneously. further it is known that p(e ∩ f ) = p(f ∩ g) = p(e ∩ g) = 1/3. can you determine p(e)? hint: if you try to use the formula of exercise 2.15 then it seems that you do not have enough information; make a diagram instead."
148,1,"['sample', 'sample space']", Exercises,seg_29,"2.17 a post office has two counters where customers can buy stamps, etc. if you are interested in the number of customers in the two queues that will form for the counters, what would you take as sample space?"
149,1,"['experiment', 'successful', 'success', 'probability of success', 'scores', 'probability', 'experiments']", Exercises,seg_29,"2.18 in a laboratory, two experiments are repeated every day of the week in different rooms until at least one is successful, the probability of success being p for each experiment. supposing that the experiments in different rooms and on different days are performed independently of each other, what is the probability that the laboratory scores its first successful experiment on day n?"
150,1,"['experiment', 'probability', 'outcome', 'tail']", Exercises,seg_29,"2.19 we repeatedly toss a coin. a head has probability p, and a tail probability 1 − p to occur, where 0 < p < 1. the outcome of the experiment we are interested in is the number of tosses it takes until a head occurs for the second time."
151,1,"['sample', 'sample space']", Exercises,seg_29,a. what would you choose as the sample space?
152,1,['probability'], Exercises,seg_29,b. what is the probability that it takes 5 tosses?
153,1,"['independent', 'probabilities', 'conditional probabilities', 'conditional probability', 'conditional', 'events', 'probability', 'event']", Conditional probability and independence,seg_31,"knowing that an event has occurred sometimes forces us to reassess the probability of another event; the new probability is the conditional probability. if the conditional probability equals what the probability was before, the events involved are called independent. often, conditional probabilities and independence are needed if we want to compute probabilities, and in many other situations they simplify the work."
154,1,"['events', 'probabilities']", Conditional probability,seg_33,"in the previous chapter we encountered the events l, “born in a long month,” and r, “born in a month with the letter r.” their probabilities are easy to compute: since l = {jan, mar, may, jul, aug, oct, dec} and r = {jan, feb, mar, apr, sep, oct, nov, dec}, one finds"
155,1,"['sample', 'conditional', 'information', 'probability', 'conditional probability', 'sample space', 'outcomes']", Conditional probability,seg_33,"now suppose that it is known about the person we meet in the street that he was born in a “long month,” and we wonder whether he was born in a “month with the letter r.” the information given excludes five outcomes of our sample space: it cannot be february, april, june, september, or november. seven possible outcomes are left, of which only four—those in r ∩ l = {jan, mar, oct, dec}—are favorable, so we reassess the probability as 4/7. we call this the conditional probability of r given l, and we write:"
156,1,"['conditional probability', 'conditional', 'probability', 'event']", Conditional probability,seg_33,quick exercise 3.1 let n = rc be the event “born in a month without r.” what is the conditional probability p(n |l)?
157,1,"['probabilities', 'information', 'symmetry', 'events', 'event']", Conditional probability,seg_33,"recalling the three envelopes on our doormat, consider the events “envelope 1 is the middle one” (call this event a) and “envelope 2 is the middle one” (b). then p(a) = p(213 or 312) = 1/3; by symmetry, the same is found for p(b). we say that the envelopes are in order if their order is either 123 or 321. suppose we know that they are not in order, but otherwise we do not know anything; what are the probabilities of a and b, given this information?"
158,1,"['probabilities', 'probability', 'event']", Conditional probability,seg_33,"let c be the event that the envelopes are not in order, so: c = {123, 321}c = {132, 213, 231, 312}. we ask for the probabilities of a and b, given that c occurs. event c consists of four elements, two of which also belong to a: a ∩ c = {213, 312}, so p(a |c) = 1/2. the probability of a ∩ c is half of p(c). no element of c also belongs to b, so p(b |c) = 0."
159,1,"['probability of an event', 'probability', 'event']", Conditional probability,seg_33,"in general, computing the probability of an event a, given that an event c occurs, means finding which fraction of the probability of c is also in the event a."
160,1,"['probability', 'conditional', 'conditional probability']", Conditional probability,seg_33,definition. the conditional probability of a given c is given by:
161,1,"['probabilities', 'conditional probabilities', 'conditional probability', 'conditional', 'events', 'probability', 'event', 'function', 'outcomes', 'probability function']", Conditional probability,seg_33,"this exercise shows that the rule p(ac) = 1−p(a) also holds for conditional probabilities. in fact, even more is true: if we have a fixed conditioning event c and define q(a) = p(a |c) for events a ⊂ ω, then q is a probability function and hence satisfies all the rules as described in chapter 2. the definition of conditional probability agrees with our intuition and it also works in situations where computing probabilities by counting outcomes does not."
162,1,['residence times'], Conditional probability,seg_33,a chemical reactor: residence times
163,1,"['distribution', 'residence time']", Conditional probability,seg_33,"consider a continuously stirred reactor vessel where a chemical reaction takes place. on one side fluid or gas flows in, mixes with whatever is already present in the vessel, and eventually flows out on the other side. one characteristic of each particular reaction setup is the so-called residence time distribution, which tells us how long particles stay inside the vessel before moving on. we consider a continuously stirred tank: the contents of the vessel are perfectly mixed at all times."
164,1,"['probabilities', 'continuous', 'residence time', 'event']", Conditional probability,seg_33,"let rt denote the event “the particle has a residence time longer than t seconds.” in section 5.3 we will see how continuous stirring determines the probabilities; here we just use that in a particular continuously stirred tank,"
165,1,['probability'], Conditional probability,seg_33,−t rt has probability e . so:
166,1,"['probability', 'conditional', 'conditional probability']", Conditional probability,seg_33,we can use the definition of conditional probability to find the probability that a particle that has stayed more than 3 seconds will stay more than 4:
167,0,[], Conditional probability,seg_33,quick exercise 3.4 calculate p(r3 |r4
168,1,"['residence time', 'distributions']", Conditional probability,seg_33,"for more details on the subject of residence time distributions see, for example, the book on reaction engineering by fogler ([11])."
169,1,"['probability', 'conditional', 'conditional probability']", The multiplication rule,seg_35,from the definition of conditional probability we derive a useful rule by multiplying left and right by p(c).
170,1,"['events', 'multiplication rule']", The multiplication rule,seg_35,the multiplication rule. for any events a and c:
171,1,['probability'], The multiplication rule,seg_35,"computing the probability of a∩c can hence be decomposed into two parts, computing p(c) and p(a |c) separately, which is often easier than computing p(a ∩ c) directly."
172,1,['probability'], The multiplication rule,seg_35,the probability of no coincident birthdays
173,1,"['probability', 'event']", The multiplication rule,seg_35,"suppose you meet two arbitrarily chosen people. what is the probability their birthdays are different? let b2 denote the event that this happens. whatever the birthday of the first person is, there is only one day the second person cannot “pick” as birthday, so:"
174,1,"['probabilities', 'conditional probabilities', 'intersection', 'conditional', 'event']", The multiplication rule,seg_35,"when the same question is asked with three people, conditional probabilities become helpful. the event b3 can be seen as the intersection of the event b2,"
175,1,"['multiplication rule', 'event']", The multiplication rule,seg_35,"“the first two have different birthdays,” with event a3 “the third person has a birthday that does not coincide with that of one of the first two persons.” using the multiplication rule:"
176,1,"['random', 'conditional', 'probability', 'conditional probability']", The multiplication rule,seg_35,"the conditional probability p(a3 |b2) is the probability that, when two days are already marked on the calendar, a day picked at random is not marked,"
177,1,"['probability', 'event']", The multiplication rule,seg_35,"we are already halfway to solving the general question: in a group of n arbitrarily chosen people, what is the probability there are no coincident birthdays? the event bn of no coincident birthdays among the n persons is the same as: “the birthdays of the first n − 1 persons are different” (the event bn−1) and “the birthday of the nth person does not coincide with a birthday of any of the first n − 1 persons” (the event an), that is,"
178,1,['multiplication rule'], The multiplication rule,seg_35,applying the multiplication rule yields:
179,1,['probability'], The multiplication rule,seg_35,"this can be used to compute the probability for arbitrary n. for example, we find: p(b22) = 0.5243 and p(b23) = 0.4927. in figure 3.1 the probability"
180,0,[], The multiplication rule,seg_35,p(bn) .......................·
181,1,['probability'], The multiplication rule,seg_35,"p(bn) is plotted for n = 1, . . . , 100, with dotted lines drawn at n = 23 and at probability 0.5. it may be hard to believe, but with just 23 people the probability of all birthdays being different is less than 50%!"
182,1,['probability'], The multiplication rule,seg_35,quick exercise 3.5 compute the probability that three arbitrary people are born in different months. can you give the formula for n people?
183,1,[], The multiplication rule,seg_35,it matters how one conditions
184,1,['condition'], The multiplication rule,seg_35,"conditioning can help to make computations easier, but it matters how it is applied. to compute p(a ∩ c) we may condition on c to get"
185,1,['condition'], The multiplication rule,seg_35,or we may condition on a and get
186,0,[], The multiplication rule,seg_35,"both ways are valid, but often one of p(a |c) and p(c |a) is easy and the other is not. for example, in the birthday example one could have tried:"
187,1,"['probability', 'conditional', 'conditional probability']", The multiplication rule,seg_35,but just trying to understand the conditional probability p(b2 |a3) already is confusing:
188,1,['probability'], The multiplication rule,seg_35,the probability that the first two persons’ birthdays differ given that the third person’s birthday does not coincide with the birthday of one of the first two . . . ?
189,1,['probabilities'], The multiplication rule,seg_35,"conditioning should lead to easier probabilities; if not, it is probably the wrong approach."
190,1,"['probabilities', 'conditional probabilities', 'conditional', 'probability']", The law of total probability and Bayes rule,seg_37,we will now discuss two important rules that help probability computations by means of conditional probabilities. we introduce both of them in the next example.
191,1,[], The law of total probability and Bayes rule,seg_37,testing for mad cow disease
192,1,"['false positive', 'false negative', 'false positives and false negatives', 'false negatives', 'false positives', 'tests', 'test']", The law of total probability and Bayes rule,seg_37,"in early 2001 the european commission introduced massive testing of cattle to determine infection with the transmissible form of bovine spongiform encephalopathy (bse) or “mad cow disease.” as no test is 100% accurate, most tests have the problem of false positives and false negatives. a false positive means that according to the test the cow is infected, but in actuality it is not. a false negative means an infected cow is not detected by the test."
193,1,"['results', 'samples', 'event', 'tests', 'test']", The law of total probability and Bayes rule,seg_37,"imagine we test a cow. let b denote the event “the cow has bse” and t the event “the test comes up positive” (this is test jargon for: according to the test we should believe the cow is infected with bse). one can “test the test” by analyzing samples from cows that are known to be infected or known to be healthy and so determine the effectiveness of the test. the european commission had this done for four tests in 1999 (see [19]) and for several more later. the results for what the report calls test a may be summarized as follows: an infected cow has a 70% chance of testing positive, and a healthy cow just 10%; in formulas:"
194,1,"['combination', 'events', 'probability', 'event', 'tests']", The law of total probability and Bayes rule,seg_37,suppose we want to determine the probability p(t ) that an arbitrary cow tests positive. the tested cow is either infected or it is not: event t occurs in combination with b or with bc (there are no other possibilities). in terms of events
195,1,"['disjoint', 'multiplication rule', 'conditional probabilities', 'probabilities', 'conditional']", The law of total probability and Bayes rule,seg_37,"because t ∩b and t ∩bc are disjoint. next, apply the multiplication rule (in such a way that the known conditional probabilities appear!):"
196,1,"['sample', 'disjoint', 'disjoint events', 'events', 'law of total probability', 'probability', 'total probability']", The law of total probability and Bayes rule,seg_37,this is an application of the law of total probability: computing a probability through conditioning on several disjoint events that make up the whole sample
197,1,['case'], The law of total probability and Bayes rule,seg_37,space (in this case two). suppose1 p(b) = 0.02; then from the last equation we conclude: p(t ) = 0.02 · 0.70 + (1 − 0.02) · 0.10 = 0.112.
198,0,[], The law of total probability and Bayes rule,seg_37,following is a general statement of the law.
199,1,"['disjoint', 'disjoint events', 'events', 'law of total probability', 'probability', 'event', 'total probability']", The law of total probability and Bayes rule,seg_37,"the law of total probability. suppose c1, c2, . . . , cm are disjoint events such that c1 ∪c2 ∪ · · · ∪cm = ω. the probability of an arbitrary event a can be expressed as:"
200,1,"['disjoint', 'multiplication rule', 'states', 'union', 'event']", The law of total probability and Bayes rule,seg_37,"figure 3.2 illustrates the law for m = 5. the event a is the disjoint union of a∩ci, for i = 1, . . . , 5, so p(a) = p(a ∩ c1)+ · · ·+p(a ∩ c5), and for each i the multiplication rule states p(a ∩ ci) = p(a |ci) · p(ci)."
201,1,"['mutually exclusive', 'events', 'mutually exclusive events']", The law of total probability and Bayes rule,seg_37,"in the bse example, we have just two mutually exclusive events: substitute m = 2, c1 = b, c2 = bc, and a = t to obtain (3.2)."
202,1,"['conditional', 'information', 'probability', 'conditional probability', 'tests', 'test']", The law of total probability and Bayes rule,seg_37,"another, perhaps more pertinent, question about the bse test is the following: suppose my cow tests positive; what is the probability it really has bse? translated, this asks for the value of p(b |t ). the information we were given is p(t |b), a conditional probability, but the wrong one. we would like to switch t and b."
203,1,"['probability', 'conditional', 'conditional probability']", The law of total probability and Bayes rule,seg_37,start with the definition of conditional probability and then use equations (3.1) and (3.2):
204,1,"['test', 'probabilities']", The law of total probability and Bayes rule,seg_37,"and by a similar calculation: p(b |t c) = 0.0068. these probabilities reflect that this test a is not a very good test; a perfect test would result in p(b |t ) = 1 and p(b |t c) = 0. in exercise 3.4 we redo this calculation, replacing p(b) = 0.02 with a more realistic number."
205,1,['bayes'], The law of total probability and Bayes rule,seg_37,"what we have just seen is known as bayes’ rule, after the english clergyman thomas bayes who derived this in the 18th century. the general statement follows."
206,1,"['disjoint', 'conditional probability', 'conditional', 'events', 'probability', 'event']", The law of total probability and Bayes rule,seg_37,"bayes’ rule. suppose the events c1, c2, . . . , cm are disjoint and c1 ∪c2 ∪ · · · ∪cm = ω. the conditional probability of ci, given an arbitrary event a, can be expressed as:"
207,1,[], The law of total probability and Bayes rule,seg_37,this is the traditional form of bayes’ formula. it follows from
208,1,"['combination', 'law of total probability', 'probability', 'total probability']", The law of total probability and Bayes rule,seg_37,"in combination with the law of total probability applied to p(a) in the denominator. purists would refer to (3.3) as bayes’ rule, and perhaps they are right."
209,1,['probabilities'], Independence,seg_39,consider three probabilities from the previous section:
210,1,[], Independence,seg_39,"if we know nothing about a cow, we would say that there is a 2% chance it is infected. however, if we know it tested positive, we can say there is a 12.5%"
211,1,"['events', 'probability']", Independence,seg_39,"chance the cow is infected. on the other hand, if it tested negative, there is only a 0.68% chance. we see that the two events are related in some way: the probability of b depends on whether t occurs."
212,1,"['independent', 'case', 'probability', 'outcome', 'test']", Independence,seg_39,"imagine the opposite: the test is useless. whether the cow is infected is unrelated to the outcome of the test, and knowing the outcome of the test does not change our probability of b: p(b |t ) = p(b). in this case we would call b independent of t ."
213,1,"['independent', 'event']", Independence,seg_39,definition. an event a is called independent of b if
214,1,['independent'], Independence,seg_39,a independent of b ⇔ ac independent of b. (3.4)
215,1,"['independence', 'multiplication rule', 'independent']", Independence,seg_39,"by application of the multiplication rule, if a is independent of b, then p(a ∩ b) = p(a |b)p(b) = p(a) p(b). on the other hand, if p(a ∩ b) = p(a) p(b), then p(a |b) = p(a) follows from the definition of independence. this shows:"
216,1,['independent'], Independence,seg_39,a independent of b ⇔ p(a ∩ b) = p(a) p(b) .
217,1,"['independent', 'conditional', 'probability', 'conditional probability']", Independence,seg_39,"finally, by definition of conditional probability, if a is independent of b, then"
218,1,['independent'], Independence,seg_39,"that is, b is independent of a. this works in reverse, too, so we have:"
219,1,['independent'], Independence,seg_39,a independent of b ⇔ b independent of a. (3.5)
220,1,"['independence', 'independent']", Independence,seg_39,"this statement says that in fact, independence is a mutual property. therefore, the expressions “a is independent of b” and “a and b are independent” are used interchangeably. from the three ⇔-statements it follows that there are in fact 12 ways to show that a and b are independent; and if they are, there are 12 ways to use that."
221,1,['independent'], Independence,seg_39,independence. to show that a and b are independent it suffices to prove just one of the following:
222,1,"['events', 'independent', 'dependent']", Independence,seg_39,"where a may be replaced by ac and b replaced by bc, or both. if one of these statements holds, all of them are true. if two events are not independent, they are called dependent."
223,1,"['events', 'independent', 'event']", Independence,seg_39,"recall the birthday events l “born in a long month” and r “born in a month with the letter r.” let h be the event “born in the first half of the year,” so p(h) = 1/2. also, p(h |r) = 1/2. so h and r are independent, and we conclude, for example, p(rc |hc) = p(rc) = 1 − 8/12 = 1/3."
224,1,['dependent'], Independence,seg_39,"we know that p(l ∩ h) = 1/4 and p(l) = 7/12. checking 1/2×7/12 = 1/4, you conclude that l and h are dependent."
225,1,['independent'], Independence,seg_39,quick exercise 3.8 derive the statement “rc is independent of hc” from “h is independent of r” using rules (3.4) and (3.5).
226,1,"['independence', 'dependence', 'statistical']", Independence,seg_39,"since the words dependence and independence have several meanings, one sometimes uses the terms stochastic or statistical dependence and independence to avoid ambiguity."
227,1,['events'], Independence,seg_39,independence of two or more events
228,1,"['independence', 'events']", Independence,seg_39,when more than two events are involved we need a more elaborate definition of independence. the reason behind this is explained by an example following the definition.
229,1,"['events', 'independent']", Independence,seg_39,"independence of two or more events. events a1, a2, . . . , am are called independent if"
230,1,['events'], Independence,seg_39,"and this statement also holds when any number of the events a1, . . . , am are replaced by their complements throughout the formula."
231,1,"['independence', 'events']", Independence,seg_39,"you see that we need to check 2m equations to establish the independence of m events. in fact, m + 1 of those equations are redundant, but we chose this version of the definition because it is easier."
232,1,"['independence', 'events']", Independence,seg_39,the reason we need to do so much more checking to establish independence for multiple events is that there are subtle ways in which events may depend on each other. consider the question:
233,1,"['independent', 'independence', 'events']", Independence,seg_39,"is independence for three events a, b, and c the same as: a and b are independent; b and c are independent; and a and c are independent?"
234,1,"['independent', 'event']", Independence,seg_39,"the answer is “no,” as the following example shows. perform two independent tosses of a coin. let a be the event “heads on toss 1,” b the event “heads on toss 2,” and c “the two tosses are equal.”"
235,1,['probabilities'], Independence,seg_39,"first, get the probabilities. of course, p(a) = p(b) = 1/2, but also"
236,1,"['independent', 'independence', 'events']", Independence,seg_39,"what about independence? events a and b are independent by assumption, so check the independence of a and c. given that the first toss is heads (a occurs), c occurs if and only if the second toss is heads as well (b occurs), so"
237,1,"['independent', 'independence', 'symmetry', 'pairwise independent']", Independence,seg_39,"by symmetry, also p(c |b) = p(c), so all pairs taken from a, b, c are independent: the three are called pairwise independent. checking the full conditions for independence, we find, for example:"
238,1,['outcomes'], Independence,seg_39,the reason for this is clear: whether c occurs follows deterministically from the outcomes of tosses 1 and 2.
239,1,['outcomes'], Solutions to the quick exercises,seg_41,"3.1 n = {may, jun, jul, aug}, l = {jan, mar, may, jul, aug, oct, dec}, and n ∩ l = {may, jul, aug}. three out of seven outcomes of l belong to n as well, so p(n |l) = 3/7."
240,1,['event'], Solutions to the quick exercises,seg_41,"3.2 the event a is contained in c. so when a occurs, c also occurs; therefore p(c |a) = 1."
241,1,['outcomes'], Solutions to the quick exercises,seg_41,"since cc = {123, 321} and a∪b = {123, 321, 312, 213}, one can see that two of the four outcomes of a ∪ b belong to cc as well, so p(cc |a ∪ b) = 1/2."
242,0,[], Solutions to the quick exercises,seg_41,3.3 using the definition we find:
243,1,['disjoint'], Solutions to the quick exercises,seg_41,because c can be split into disjoint parts a ∩ c and ac ∩ c and therefore
244,1,['probability'], Solutions to the quick exercises,seg_41,"3.4 this asks for the probability that the particle stays more than 3 seconds, given that it does not stay longer than 4 seconds, so 4 or less. from the"
245,1,['event'], Solutions to the quick exercises,seg_41,the event r3 ∩ r4
246,1,"['disjoint', 'events', 'union']", Solutions to the quick exercises,seg_41,"c describes: longer than 3 but not longer than 4 seconds. furthermore, r3 is the disjoint union of the events r3∩r4"
247,1,['complement'], Solutions to the quick exercises,seg_41,c) = p(r3) − p(r4) = e−3 − e−4. using the complement rule: p(r4
248,1,['event'], Solutions to the quick exercises,seg_41,"3.5 instead of a calendar of 365 days, we have one with just 12 months. let cn be the event n arbitrary persons have different months of birth. then"
249,0,[], Solutions to the quick exercises,seg_41,and it is no surprise that this is much smaller than p(b3). the general formula
250,1,['case'], Solutions to the quick exercises,seg_41,"note that it is correct even if n is 13 or more, in which case p(cn) = 0."
251,0,[], Solutions to the quick exercises,seg_41,3.6 repeating the calculation we find:
252,1,['independent'], Solutions to the quick exercises,seg_41,h independent of r ⇔ hc independent of r by (3.4)
253,1,['independent'], Solutions to the quick exercises,seg_41,hc independent of r ⇔ r independent of hc by (3.5)
254,1,['independent'], Solutions to the quick exercises,seg_41,r independent of hc ⇔ rc independent of hc by (3.4).
255,1,"['moment', 'probability']", Exercises,seg_43,"3.1 your lecturer wants to walk from a to b (see the map). to do so, he first randomly selects one of the paths to c, d, or e. next he selects randomly one of the possible paths at that moment (so if he first selected the path to e, he can either select the path to a or the path to f ), etc. what is the probability that he will reach b after two selections?"
256,1,['event'], Exercises,seg_43,"3.2 a fair die is thrown twice. a is the event “sum of the throws equals 4,” b is “at least one of the throws is a 3.”"
257,1,"['independent', 'events', 'independent events']", Exercises,seg_43,b. are a and b independent events?
258,1,['event'], Exercises,seg_43,"3.3 we draw two cards from a regular deck of 52. let s1 be the event “the first one is a spade,” and s2 “the second one is a spade.”"
259,1,[], Exercises,seg_43,b. compute p(s2) by conditioning on whether the first card is a spade.
260,1,"['risk', 'test', 'estimated']", Exercises,seg_43,"3.4 a dutch cow is tested for bse, using test a as described in section 3.3, with p(t |b) = 0.70 and p(t |bc) = 0.10. assume that the bse risk for the netherlands is the same as in 2003, when it was estimated to be p(b) = 1.3 · 10−5. compute p(b |t ) and p(b |t c)."
261,1,"['probability', 'random']", Exercises,seg_43,"3.5 a ball is drawn at random from an urn containing one red and one white ball. if the white ball is drawn, it is put back into the urn. if the red ball is drawn, it is returned to the urn together with two more red balls. then a second draw is made. what is the probability a red ball was drawn on both the first and the second draws?"
262,1,"['events', 'independent', 'probability']", Exercises,seg_43,"3.6 we choose a month of the year, in such a manner that each month has the same probability. find out whether the following events are independent:"
263,1,['events'], Exercises,seg_43,"a. the events “outcome is an even numbered month” (i.e., february, april,"
264,0,[], Exercises,seg_43,"june, etc.) and “outcome is in the first half of the year.”"
265,1,['events'], Exercises,seg_43,"b. the events “outcome is an even numbered month” (i.e., february, april,"
266,1,"['probability', 'event']", Exercises,seg_43,"3.8 spaceman spiff’s spacecraft has a warning light that is supposed to switch on when the freem blasters are overheated. let w be the event “the warning light is switched on” and f “the freem blasters are overheated.” suppose the probability of freem blaster overheating p(f ) is 0.1, that the light is switched on when they actually are overheated is 0.99, and that there is a 2% chance that it comes on when nothing is wrong: p(w |f c) = 0.02."
267,1,['probability'], Exercises,seg_43,a. determine the probability that the warning light is switched on.
268,1,"['probability', 'conditional', 'conditional probability']", Exercises,seg_43,b. determine the conditional probability that the freem blasters are over-
269,0,[], Exercises,seg_43,"heated, given that the warning light is on."
270,1,"['probability', 'event']", Exercises,seg_43,"3.9 a certain grapefruit variety is grown in two regions in southern spain. both areas get infested from time to time with parasites that damage the crop. let a be the event that region r1 is infested with parasites and b that region r2 is infested. suppose p(a) = 3/4, p(b) = 2/5 and p(a ∪b) = 4/5. if the food inspection detects the parasite in a ship carrying grapefruits from r1, what is the probability region r2 is infested as well?"
271,1,"['probability', 'random']", Exercises,seg_43,"3.10 a student takes a multiple-choice exam. suppose for each question he either knows the answer or gambles and chooses an option at random. further suppose that if he knows the answer, the probability of a correct answer is 1, and if he gambles this probability is 1/4. to pass, students need to answer at least 60% of the questions correctly. the student has “studied for a minimal pass,” i.e., with probability 0.6 he knows the answer to a question. given that he answers a question correctly, what is the probability that he actually knows the answer?"
272,1,"['percentage', 'set', 'test', 'limit']", Exercises,seg_43,"3.11 a breath analyzer, used by the police to test whether drivers exceed the legal limit set for the blood alcohol percentage while driving, is known to satisfy"
273,1,"['percentage', 'event', 'limit']", Exercises,seg_43,where a is the event “breath analyzer indicates that legal limit is exceeded” and b “driver’s blood alcohol percentage exceeds legal limit.” on saturday night about 5% of the drivers are known to exceed the limit.
274,1,[], Exercises,seg_43,a. describe in words the meaning of p(bc |a).
275,1,['events'], Exercises,seg_43,"3.12 the events a, b, and c satisfy: p(a |b ∩ c) = 1/4, p(b |c) = 1/3, and p(c) = 1/2. calculate p(ac ∩ b ∩ c)."
276,1,"['events', 'probability', 'outcomes']", Exercises,seg_43,"3.13 in exercise 2.12 we computed the probability of a “dream draw” in the uefa playoffs lottery by counting outcomes. recall that there were ten teams in the lottery, five considered “strong” and five considered “weak.” introduce events di, “the ith pair drawn is a dream combination,” where a “dream combination” is a pair of a strong team with a weak team, and i = 1, . . . , 5."
277,1,['probability'], Exercises,seg_43,d. continue the procedure to obtain the probability of a “dream draw”:
278,1,['event'], Exercises,seg_43,"3.14 recall the monty hall problem from section 1.3. let r be the event “the prize is behind the door you chose initially,” and w the event “you win the prize by switching doors.”"
279,1,"['law of total probability', 'total probability', 'probability']", Exercises,seg_43,b. compute p(w ) using the law of total probability.
280,1,"['independent', 'events', 'independent events']", Exercises,seg_43,"3.15 two independent events a and b are given, and p(b |a ∪ b) = 2/3, p(a |b) = 1/2. what is p(b)?"
281,1,"['test', 'event']", Exercises,seg_43,3.16 you are diagnosed with an uncommon disease. you know that there only is a 1% chance of getting it. use the letter d for the event “you have the disease” and t for “the test says so.” it is known that the test is imperfect: p(t |d) = 0.98 and p(t c |dc) = 0.95.
282,1,"['test', 'probability']", Exercises,seg_43,"a. given that you test positive, what is the probability that you really have"
283,1,"['test', 'independent']", Exercises,seg_43,b. you obtain a second opinion: an independent repetition of the test. you
284,1,['probability'], Exercises,seg_43,"test positive again. given this, what is the probability that you really have the disease?"
285,1,"['independent', 'probability', 'event', 'outcome']", Exercises,seg_43,"3.17 you and i play a tennis match. it is deuce, which means if you win the next two rallies, you win the game; if i win both rallies, i win the game; if we each win one rally, it is deuce again. suppose the outcome of a rally is independent of other rallies, and you win a rally with probability p. let w be the event “you win the game,” g “the game ends after the next two rallies,” and d “it becomes deuce again.”"
286,0,[], Exercises,seg_43,c. explain why the answers are the same.
287,1,['events'], Exercises,seg_43,3.18 suppose a and b are events with 0 < p(a) < 1 and 0 < p(b) < 1.
288,1,"['disjoint', 'independent']", Exercises,seg_43,"a. if a and b are disjoint, can they be independent?"
289,1,"['disjoint', 'independent']", Exercises,seg_43,"b. if a and b are independent, can they be disjoint?"
290,1,['independent'], Exercises,seg_43,"c. if a ⊂ b, can a and b be independent?"
291,1,['independent'], Exercises,seg_43,"d. if a and b are independent, can a and a ∪ b be independent?"
292,1,"['geometric random variables', 'discrete', 'probability', 'random', 'function', 'associated', 'geometric', 'sample', 'experiment', 'events', 'sample space', 'binomial', 'probabilistic', 'probability function', 'random variables', 'variables', 'discrete random variables', 'bernoulli']", Discrete random variables,seg_45,"the sample space associated with an experiment, together with a probability function defined on all its events, is a complete probabilistic description of that experiment. often we are interested only in certain features of this description. we focus on these features using random variables . in this chapter we discuss discrete random variables, and in the next we will consider continuous random variables. we introduce the bernoulli, binomial, and geometric random variables."
293,1,"['sample', 'sample space', 'independent']", Random variables,seg_47,"suppose we are playing the board game “snakes and ladders,” where the moves are determined by the sum of two independent throws with a die. an obvious choice of the sample space is"
294,1,"['function', 'outcomes']", Random variables,seg_47,"however, as players of the game, we are only interested in the sum of the outcomes of the two throws, i.e., in the value of the function s : ω → r, given by"
295,1,"['table', 'results', 'event', 'function']", Random variables,seg_47,"in table 4.1 the possible results of the first throw (top margin), those of the second throw (left margin), and the corresponding values of s (body) are given. note that the values of s are constant on lines perpendicular to the diagonal. we denote the event that the function s attains the value k by {s = k}, which is an abbreviation of “the subset of those ω = (ω1, ω2) ∈ ω for which s(ω1, ω2 ) = ω1 + ω2 = k,” i.e.,"
296,1,"['outcomes', 'event']", Random variables,seg_47,quick exercise 4.1 list the outcomes in the event {s = 8}.
297,1,"['probability', 'probability of the event', 'event']", Random variables,seg_47,we denote the probability of the event {s = k} by
298,1,['probability'], Random variables,seg_47,"although formally we should write p({s = k}) instead of p(s = k). in our example, s attains only the values k = 2, 3, . . . , 12 with positive probability."
299,0,[], Random variables,seg_47,"for example, 1"
300,1,[], Random variables,seg_47,because 13 is an “impossible outcome.”
301,1,['table'], Random variables,seg_47,"quick exercise 4.2 use table 4.1 to determine p(s = k) for k = 4, 5, . . . , 12."
302,1,"['function', 'independent', 'case']", Random variables,seg_47,"now suppose that for some other game the moves are given by the maximum of two independent throws. in this case we are interested in the value of the function m : ω → r, given by"
303,1,"['functions', 'random variables', 'table', 'variables', 'discrete random variables', 'results', 'discrete', 'random']", Random variables,seg_47,"in table 4.2 the possible results of the first throw (top margin), those of the second throw (left margin), and the corresponding values of m (body) are given. the functions s and m are examples of what we call discrete random variables."
304,1,"['sample', 'discrete random variable', 'discrete', 'variable', 'random variable', 'random', 'function', 'sample space']", Random variables,seg_47,"definition. let ω be a sample space. a discrete random variable is a function x : ω → r that takes on a finite number of values a1, a2, . . . , an or an infinite number of values a1, a2, . . . ."
305,1,"['sample', 'discrete random variable', 'random', 'probabilities', 'discrete', 'distribution', 'random variable', 'events', 'probability distribution', 'variable', 'probability', 'sample space']", Random variables,seg_47,"in a way, a discrete random variable x “transforms” a sample space ω to a more “tangible” sample space ω̃, whose events are more directly related to what you are interested in. for instance, s transforms ω = {(1, 1), (1, 2), . . . , (1, 6), (2, 1), . . . , (6, 5), (6, 6)} to ω̃ = {2, . . . , 12}, and m transforms ω to ω̃ = {1, . . . , 6}. of course, there is a price to pay: one has to calculate the probabilities of x . or, to say things more formally, one has to determine the probability distribution of x , i.e., to describe how the probability mass is distributed over possible values of x ."
306,1,"['sample', 'discrete random variable', 'probabilities', 'mass function', 'probability mass function', 'discrete', 'information', 'random variable', 'variable', 'probability', 'random', 'function', 'sample space']", The probability distribution of a discrete random variable,seg_49,"once a discrete random variable x is introduced, the sample space ω is no longer important. it suffices to list the possible values of x and their corresponding probabilities. this information is contained in the probability mass function of x ."
307,1,"['discrete random variable', 'mass function', 'probability mass function', 'discrete', 'random variable', 'variable', 'probability', 'random', 'function']", The probability distribution of a discrete random variable,seg_49,"definition. the probability mass function p of a discrete random variable x is the function p : r → [0, 1], defined by"
308,1,"['discrete random variable', 'discrete', 'random variable', 'variable', 'random']", The probability distribution of a discrete random variable,seg_49,"if x is a discrete random variable that takes on the values a1, a2, . . ., then"
309,1,"['mass function', 'probability mass function', 'probability', 'function']", The probability distribution of a discrete random variable,seg_49,as an example we give the probability mass function p of m .
310,1,"['distribution function', 'distribution', 'random variable', 'variable', 'random', 'function']", The probability distribution of a discrete random variable,seg_49,the distribution function of a random variable
311,1,"['continuous random variables', 'cumulative distribution function', 'random variables', 'continuous', 'mass function', 'variables', 'probability mass function', 'distribution function', 'discrete', 'distribution', 'random variable', 'variable', 'probability', 'random', 'function']", The probability distribution of a discrete random variable,seg_49,"as we will see, so-called continuous random variables cannot be specified by giving a probability mass function. however, the distribution function of a random variable x (also known as the cumulative distribution function) allows us to treat discrete and continuous random variables in the same way."
312,1,"['distribution function', 'distribution', 'random variable', 'variable', 'random', 'function']", The probability distribution of a discrete random variable,seg_49,"definition. the distribution function f of a random variable x is the function f : r → [0, 1], defined by"
313,1,"['discrete random variable', 'mass function', 'probability mass function', 'distribution function', 'probabilistic', 'discrete', 'distribution', 'random variable', 'variable', 'information', 'probability', 'random', 'function']", The probability distribution of a discrete random variable,seg_49,"both the probability mass function and the distribution function of a discrete random variable x contain all the probabilistic information of x ; the probability distribution of x is determined by either of them. in fact, the distribution function f of a discrete random variable x can be expressed in terms of the probability mass function p of x and vice versa. if x attains values a1, a2, . . ., such that"
314,1,"['discrete random variable', 'distribution function', 'discrete', 'distribution', 'random variable', 'variable', 'random', 'function']", The probability distribution of a discrete random variable,seg_49,"we see that, for a discrete random variable x , the distribution function f jumps in each of the ai, and is constant between successive ai. the height of the jump at ai is p(ai); in this way p can be retrieved from f . for example, see figure 4.1, where p and f are displayed for the random variable m ."
315,1,"['distribution function', 'distribution', 'random variable', 'variable', 'random', 'function']", The probability distribution of a discrete random variable,seg_49,we end this section with three properties of the distribution function f of a random variable x :
316,1,['event'], The probability distribution of a discrete random variable,seg_49,consequence of the fact that a ≤ b implies that the event {x ≤ a} is contained in the event {x ≤ b}.
317,1,"['distribution function', 'distribution', 'probability', 'function']", The probability distribution of a discrete random variable,seg_49,"2. since f (a) is a probability, the value of the distribution function is always"
318,0,[], The probability distribution of a discrete random variable,seg_49,"between 0 and 1. moreover,"
319,0,[], The probability distribution of a discrete random variable,seg_49,this is indicated in figure 4.1 by bullets. henceforth we will omit these bullets.
320,1,"['distribution function', 'distribution', 'random variable', 'variable', 'random', 'function']", The probability distribution of a discrete random variable,seg_49,"conversely, any function f satisfying 1, 2, and 3 is the distribution function of some random variable (see remarks 6.1 and 6.2)."
321,1,"['discrete random variable', 'discrete', 'random variable', 'variable', 'random']", The probability distribution of a discrete random variable,seg_49,"quick exercise 4.3 let x be a discrete random variable, and let a be such that p(a) > 0. show that f (a) = p(x < a) + p(a)."
322,1,"['random variables', 'variables', 'discrete random variables', 'discrete', 'random']", The probability distribution of a discrete random variable,seg_49,there are many discrete random variables that arise in a natural way. we introduce three of them in the next two sections.
323,1,"['model', 'experiment', 'distribution', 'bernoulli', 'bernoulli distribution', 'outcomes']", The Bernoulli and binomial distributions,seg_51,"the bernoulli distribution is used to model an experiment with only two possible outcomes, often referred to as “success” and “failure”, usually encoded as 1 and 0."
324,1,"['discrete random variable', 'mass function', 'probability mass function', 'discrete', 'bernoulli', 'random variable', 'variable', 'probability', 'random', 'function', 'parameter']", The Bernoulli and binomial distributions,seg_51,"definition. a discrete random variable x has a bernoulli distribution with parameter p, where 0 ≤ p ≤ 1, if its probability mass function is given by"
325,1,['distribution'], The Bernoulli and binomial distributions,seg_51,we denote this distribution by ber(p).
326,1,"['mass function', 'probability mass function', 'distribution', 'bernoulli', 'probability', 'bernoulli distribution', 'function', 'parameter', 'dependence']", The Bernoulli and binomial distributions,seg_51,note that we wrote px instead of p for the probability mass function of x . this was done to emphasize its dependence on x and to avoid possible confusion with the parameter p of the bernoulli distribution.
327,1,"['probability', 'random']", The Bernoulli and binomial distributions,seg_51,"consider the (fictitious) situation that you attend, completely unprepared, a multiple-choice exam. it consists of 10 questions, and each question has four alternatives (of which only one is correct). you will pass the exam if you answer six or more questions correctly. you decide to answer each of the questions in a random way, in such a way that the answer of one question is not affected by the answers of the others. what is the probability that you will pass?"
328,0,[], The Bernoulli and binomial distributions,seg_51,1 if the ith answer is correct
329,0,[], The Bernoulli and binomial distributions,seg_51,"ri = {0 if the ith answer is incorrect,"
330,0,[], The Bernoulli and binomial distributions,seg_51,the number of correct answers x is given by
331,1,['probability'], The Bernoulli and binomial distributions,seg_51,quick exercise 4.4 calculate the probability that you answered the first question correctly and the second one incorrectly.
332,1,"['events', 'independent', 'case']", The Bernoulli and binomial distributions,seg_51,"clearly, x attains only the values 0, 1, . . . , 10. let us first consider the case x = 0. since the answers to the different questions do not influence each other, we conclude that the events {r1 = a1}, . . . , {r10 = a10} are independent for every choice of the ai, where each ai is 0 or 1. we find"
333,1,['probability'], The Bernoulli and binomial distributions,seg_51,the probability that we have answered exactly one question correctly equals
334,1,['probability'], The Bernoulli and binomial distributions,seg_51,"which is the probability that the answer is correct times the probability that the other nine answers are wrong, times the number of ways in which this can occur:"
335,1,['independence'], The Bernoulli and binomial distributions,seg_51,"in general we find for k = 0, 1, . . . , 10, again using independence, that"
336,1,['probability'], The Bernoulli and binomial distributions,seg_51,"which is the probability that k questions were answered correctly times the probability that the other 10−k answers are wrong, times the number of ways c10,k this can occur."
337,0,[], The Bernoulli and binomial distributions,seg_51,"so c10,k is the number of different ways in which one can choose k correct answers from the list of 10. we already have seen that c10,0 = 1, because there is only one way to do everything wrong; and that c10,1 = 10, because each of the 10 questions may have been answered correctly."
338,0,['n'], The Bernoulli and binomial distributions,seg_51,"more generally, if we have to choose k different objects out of an ordered list of n objects, and the order in which we pick the objects matters, then for the first object you have n possibilities, and no matter which object you pick, for the second one there are n − 1 possibilities. for the third there are n − 2 possibilities, and so on, with n− (k − 1) possibilities for the kth. so there are"
339,0,[], The Bernoulli and binomial distributions,seg_51,ways to choose the k objects.
340,1,['permutations'], The Bernoulli and binomial distributions,seg_51,"in how many ways can we choose three questions? when the order matters, there are 10 · 9 · 8 ways. however, the order in which these three questions are selected does not matter: to answer questions 2, 5, and 8 correctly is the same as answering questions 8, 2, and 5 correctly, and so on. the triplet {2, 5, 8} can be chosen in 3 · 2 · 1 different orders, all with the same result. there are six permutations of the numbers 2, 5, and 8 (see page 14)."
341,0,[], The Bernoulli and binomial distributions,seg_51,"thus, compensating for this six-fold overcount, the number c10,3 of ways to correctly answer 3 questions out of 10 becomes"
342,0,['n'], The Bernoulli and binomial distributions,seg_51,"note that this is equal to n! ,"
343,0,['n'], The Bernoulli and binomial distributions,seg_51,which is usually denoted by (n
344,0,[], The Bernoulli and binomial distributions,seg_51,"k). moreover, in accordance with"
345,0,[], The Bernoulli and binomial distributions,seg_51,quick exercise 4.5 show that (
346,1,['probability'], The Bernoulli and binomial distributions,seg_51,"since p(x ≥ 6) = p(x = 6) + · · · + p(x = 10), it is now an easy (but tedious) exercise to determine the probability that you will pass. one finds that p(x ≥ 6) = 0.0197. it pays to study, doesn’t it?!"
347,1,"['parameters', 'distribution', 'random variable', 'variable', 'binomial', 'random', 'binomial distribution']", The Bernoulli and binomial distributions,seg_51,the preceding random variable x is an example of a random variable with a binomial distribution with parameters n = 10 and p = 1/4.
348,1,"['discrete random variable', 'parameters', 'mass function', 'probability mass function', 'discrete', 'random variable', 'variable', 'binomial', 'probability', 'random', 'function']", The Bernoulli and binomial distributions,seg_51,"definition. a discrete random variable x has a binomial distribution with parameters n and p, where n = 1, 2, . . . and 0 ≤ p ≤ 1, if its probability mass function is given by"
349,1,['distribution'], The Bernoulli and binomial distributions,seg_51,"we denote this distribution by bin(n, p)."
350,1,"['mass function', 'probability mass function', 'distribution function', 'distribution', 'probability', 'function']", The Bernoulli and binomial distributions,seg_51,"figure 4.2 shows the probability mass function px and distribution function fx of a bin(10, 1"
351,1,"['random variable', 'variable', 'random']", The Bernoulli and binomial distributions,seg_51,4 ) distributed random variable.
352,1,['moment'], The geometric distribution,seg_53,"in 1986, weinberg and gladen [38] investigated the number of menstrual cycles it took women to become pregnant, measured from the moment they had"
353,1,"['model', 'random variable', 'variable', 'random']", The geometric distribution,seg_53,decided to become pregnant. we model the number of cycles up to pregnancy by a random variable x .
354,1,"['independence', 'independent', 'probability']", The geometric distribution,seg_53,"assume that the probability that a woman becomes pregnant during a particular cycle is equal to p, for some p with 0 < p ≤ 1, independent of the previous cycles. then clearly p(x = 1) = p. due to the independence of consecutive cycles, one finds for k = 1, 2, . . . that"
355,1,"['geometric distribution', 'distribution', 'random variable', 'variable', 'parameter', 'random', 'geometric']", The geometric distribution,seg_53,this random variable x is an example of a random variable with a geometric distribution with parameter p.
356,1,"['discrete random variable', 'mass function', 'probability mass function', 'discrete', 'random variable', 'variable', 'probability', 'random', 'function', 'parameter', 'geometric']", The geometric distribution,seg_53,"definition. a discrete random variable x has a geometric distribution with parameter p, where 0 < p ≤ 1, if its probability mass function is given by"
357,1,['distribution'], The geometric distribution,seg_53,we denote this distribution by geo(p).
358,1,"['mass function', 'probability mass function', 'distribution function', 'distribution', 'probability', 'function']", The geometric distribution,seg_53,figure 4.3 shows the probability mass function px and distribution function fx of a geo(1
359,1,"['random variable', 'variable', 'random']", The geometric distribution,seg_53,4 ) distributed random variable.
360,1,['distribution'], The geometric distribution,seg_53,"quick exercise 4.6 let x have a geo(p) distribution. for n ≥ 0, show that p(x > n) = (1 − p)n."
361,1,"['geometric distribution', 'distribution', 'geometric']", The geometric distribution,seg_53,"the geometric distribution has a remarkable property, which is known as the memoryless property.1 for n, k = 0, 1, 2, . . . one has"
362,0,[], The geometric distribution,seg_53,we can derive this equality using the result from quick exercise 4.6:
363,1,['table'], Solutions to the quick exercises,seg_55,"4.1 from table 4.1, one finds that"
364,1,['table'], Solutions to the quick exercises,seg_55,"4.2 from table 4.1, one determines the following table."
365,0,[], Solutions to the quick exercises,seg_55,not very interestingly: this also holds if p(a) = 0.
366,1,"['independence', 'probability']", Solutions to the quick exercises,seg_55,"4.4 the probability that you answered the first question correctly and the second one incorrectly is given by p(r1 = 1, r2 = 0). due to independence,"
367,0,[], Solutions to the quick exercises,seg_55,4.5 rewriting yields
368,1,"['success', 'probability']", Solutions to the quick exercises,seg_55,"4.6 there are two ways to show that p(x > n) = (1 − p)n. the easiest way is to realize that p(x > n) is the probability that we had “no success in the first n trials,” which clearly equals (1 − p)n. a more involved way is by calculation:"
369,0,[], Solutions to the quick exercises,seg_55,if we recall from calculus that
370,0,[], Solutions to the quick exercises,seg_55,the answer follows immediately.
371,1,['independent'], Exercises,seg_57,"4.1 let z represent the number of times a 6 appeared in two independent throws of a die, and let s and m be as in section 4.1."
372,1,"['distribution', 'probability distribution', 'probability']", Exercises,seg_57,"a. describe the probability distribution of z, by giving either the probability"
373,1,"['parameters', 'distribution function', 'distribution', 'function']", Exercises,seg_57,"mass function pz of z or the distribution function fz of z. what type of distribution does z have, and what are the values of its parameters?"
374,1,"['events', 'outcomes']", Exercises,seg_57,"b. list the outcomes in the events {m = 2, z = 0}, {s = 5, z = 1}, and"
375,1,['probabilities'], Exercises,seg_57,"{s = 8, z = 1}. what are their probabilities?"
376,1,"['events', 'independent']", Exercises,seg_57,c. determine whether the events {m = 2} and {z = 0} are independent.
377,1,"['discrete random variable', 'mass function', 'probability mass function', 'discrete', 'random variable', 'variable', 'probability', 'random', 'function']", Exercises,seg_57,4.2 let x be a discrete random variable with probability mass function p given by:
378,1,"['random variable', 'variable', 'random']", Exercises,seg_57,"a. let the random variable y be defined by y = x2, i.e., if x = 2, then"
379,1,"['mass function', 'probability mass function', 'probability', 'function']", Exercises,seg_57,y = 4. calculate the probability mass function of y .
380,1,"['distribution', 'functions']", Exercises,seg_57,"b. calculate the value of the distribution functions of x and y in a = 1,"
381,1,"['discrete random variable', 'distribution function', 'discrete', 'distribution', 'random variable', 'variable', 'random', 'function']", Exercises,seg_57,4.3 suppose that the distribution function of a discrete random variable x is given by
382,1,"['mass function', 'probability mass function', 'probability', 'function']", Exercises,seg_57,determine the probability mass function of x .
383,1,"['tails', 'probability']", Exercises,seg_57,"4.4 you toss n coins, each showing heads with probability p, independently of the other tosses. each coin that shows tails is tossed again. let x be the total number of heads."
384,1,['distribution'], Exercises,seg_57,a. what type of distribution does x have? specify its parameter(s).
385,1,"['mass function', 'probability mass function', 'probability', 'function']", Exercises,seg_57,b. what is the probability mass function of the total number of heads x?
386,1,"['distribution function', 'results', 'distribution', 'random variable', 'variable', 'random', 'function']", Exercises,seg_57,"4.5 a fair die is thrown until the sum of the results of the throws exceeds 6. the random variable x is the number of throws needed for this. let f be the distribution function of x . determine f (1), f (2), and f (7)."
387,1,[], Exercises,seg_57,4.6 three times we randomly draw a number from the following numbers:
388,1,"['mass function', 'probability mass function', 'probability', 'function']", Exercises,seg_57,"if xi represents the ith draw, i = 1, 2, 3, then the probability mass function of xi is given by"
389,1,"['average', 'independent']", Exercises,seg_57,"and p(xi = a) = 0 for all other a. we assume that each draw is independent of the previous draws. let x̄ be the average of x1, x2, and x3, i.e.,"
390,1,"['mass function', 'probability mass function', 'probability', 'function']", Exercises,seg_57,a. determine the probability mass function px̄ of x̄.
391,1,['probability'], Exercises,seg_57,b. compute the probability that exactly two draws are equal to 1.
392,0,[], Exercises,seg_57,4.7 a shop receives a batch of 1000 cheap lamps. the odds that a lamp is defective are 0.1%. let x be the number of defective lamps in the batch.
393,1,['distribution'], Exercises,seg_57,a. what kind of distribution does x have? what is/are the value(s) of pa-
394,1,['distribution'], Exercises,seg_57,rameter(s) of this distribution?
395,1,['probability'], Exercises,seg_57,b. what is the probability that the batch contains no defective lamps? one
396,0,[], Exercises,seg_57,defective lamp? more than two defective ones?
397,1,['probability'], Exercises,seg_57,4.8 in section 1.4 we saw that each space shuttle has six o-rings and that each o-ring fails with probability
398,0,[], Exercises,seg_57,"where a = 5.085, b = −0.1156, and t is the temperature (in degrees fahrenheit) at the time of the launch of the space shuttle. at the time of the fatal launch of the challenger, t = 31, yielding p(31) = 0.8178."
399,1,[], Exercises,seg_57,a. let x be the number of failing o-rings at launch temperature 31◦f. what
400,1,"['parameters', 'distribution', 'probability distribution', 'probability']", Exercises,seg_57,"type of probability distribution does x have, and what are the values of its parameters?"
401,1,['probability'], Exercises,seg_57,b. what is the probability p(x ≥ 1) that at least one o-ring fails?
402,1,"['failure', 'probability']", Exercises,seg_57,"4.9 for simplicity’s sake, let us assume that all space shuttles will be launched at 81◦f (which is the highest recorded launch temperature in figure 1.3). with this temperature, the probability of an o-ring failure is equal to p(81) = 0.0137 (see section 1.4 or exercise 4.8)."
403,1,['probability'], Exercises,seg_57,"a. what is the probability that during 23 launches no o-ring will fail, but"
404,1,[], Exercises,seg_57,that at least one o-ring will fail during the 24th launch of a space shuttle?
405,1,['probability'], Exercises,seg_57,b. what is the probability that no o-ring fails during 24 launches?
406,1,"['random variables', 'variables', 'probability', 'random']", Exercises,seg_57,"4.10 early in the morning, a group of m people decides to use the elevator in an otherwise deserted building of 21 floors. each of these persons chooses his or her floor independently of the others, and—from our point of view— completely at random, so that each person selects a floor with probability 1/21. let sm be the number of times the elevator stops. in order to study sm, we introduce for i = 1, 2, . . . , 21 random variables ri, given by"
407,0,[], Exercises,seg_57,1 if the elevator stops at the ith floor
408,0,[], Exercises,seg_57,ri = {0 if the elevator does not stop at the ith floor.
409,1,['distribution'], Exercises,seg_57,a. each ri has a ber(p) distribution. show that p = 1 − ( 2
410,0,[], Exercises,seg_57,"b. from the way we defined sm, it follows that"
411,1,['distribution'], Exercises,seg_57,"can we conclude that sm has a bin(21, p) distribution, with p as in part a? why or why not?"
412,1,['distribution'], Exercises,seg_57,and that s3 has the following distribution.
413,1,"['distribution', 'parameter']", Exercises,seg_57,"4.11 you decide to play monthly in two different lotteries, and you stop playing as soon as you win a prize in one (or both) lotteries of at least one million euros. suppose that every time you participate in these lotteries, the probability to win one million (or more) euros is p1 for one of the lotteries and p2 for the other. let m be the number of times you participate in these lotteries until winning at least one prize. what kind of distribution does m have, and what is its parameter?"
414,1,"['independent', 'probability']", Exercises,seg_57,"4.12 you and a friend want to go to a concert, but unfortunately only one ticket is still available. the man who sells the tickets decides to toss a coin until heads appears. in each toss heads appears with probability p, where 0 < p < 1, independent of each of the previous tosses. if the number of tosses needed is odd, your friend is allowed to buy the ticket; otherwise you can buy it. would you agree to this arrangement?"
415,1,"['estimate', 'distribution', 'probability distribution', 'probability', 'random']", Exercises,seg_57,"4.13 a box contains an unknown number n of identical bolts. in order to get an idea of the size n , we randomly mark one of the bolts from the box. next we select at random a bolt from the box. if this is the marked bolt we stop, otherwise we return the bolt to the box, and we randomly select a second one, etc. we stop when the selected bolt is the marked one. let x be the number of times a bolt was selected. later (in exercise 21.11) we will try to find an estimate of n . here we look at the probability distribution of x ."
416,1,"['distribution', 'probability distribution', 'probability']", Exercises,seg_57,a. what is the probability distribution of x? specify its parameter(s)!
417,0,[], Exercises,seg_57,b. the drawback of this approach is that x can attain any of the values
418,1,"['sample', 'uniform distribution', 'discrete', 'sampling', 'distribution', 'random', 'discrete uniform distribution']", Exercises,seg_57,"1, 2, 3, . . . , so that if n is large we might be sampling from the box for quite a long time. we decide to sample from the box in a slightly different way: after we have randomly marked one of the bolts in the box, we select at random a bolt from the box. if this is the marked one, we stop, otherwise we randomly select a second bolt (we do not return the selected bolt). we stop when we select the marked bolt. let y be the number of times a bolt was selected. show that p(y = k) = 1/n for k = 1, 2, . . . , n (y has a so-called discrete uniform distribution)."
419,1,[], Exercises,seg_57,"c. instead of randomly marking one bolt in the box, we mark m bolts, with"
420,1,['sample'], Exercises,seg_57,"m smaller than n . next, we randomly select r bolts; z is the number of marked bolts in the sample. show that"
421,1,"['parameters', 'distribution', 'hypergeometric', 'hypergeometric distribution']", Exercises,seg_57,"(z has a so-called hypergeometric distribution, with parameters m, n , and r.)"
422,1,"['outcome', 'results', 'probability']", Exercises,seg_57,"4.14 we throw a coin until a head turns up for the second time, where p is the probability that a throw results in a head and we assume that the outcome"
423,1,"['outcomes', 'independent']", Exercises,seg_57,of each throw is independent of the previous outcomes. let x be the number of times we have thrown the coin.
424,1,"['model', 'continuous random variables', 'random variables', 'variables', 'random', 'continuous', 'experiments', 'outcomes']", Continuous random variables,seg_59,"many experiments have outcomes that take values on a continuous scale. for example, in chapter 2 we encountered the load at which a model of a bridge collapses. these experiments have continuous random variables naturally associated with them."
425,1,"['interval', 'discrete', 'probability', 'associated', 'random', 'process', 'experiment', 'random variable', 'mean', 'continuous random variables', 'continuous', 'outcomes', 'discrete random variable', 'random variables', 'probabilities', 'variables', 'discrete random variables', 'variable']", Probability density functions,seg_61,"one way to look at continuous random variables is that they arise by a (neverending) process of refinement from discrete random variables. suppose, for example, that a discrete random variable associated with some experiment takes on the value 6.283 with probability p. if we refine, in the sense that we also get to know the fourth decimal, then the probability p is spread over the outcomes 6.2830, 6.2831, . . . , 6.2839. usually this will mean that each of these new values is taken on with a probability that is much smaller than p—the sum of the ten probabilities is p. continuing the refinement process to more and more decimals, the probabilities of the possible values of the outcomes become smaller and smaller, approaching zero. however, the probability that the possible values lie in some fixed interval [a, b] will settle down. this is closely related to the way sums converge to an integral in the definition of the integral and motivates the following definition."
426,1,"['continuous', 'random variable', 'variable', 'random', 'function']", Probability density functions,seg_61,"definition. a random variable x is continuous if for some function f : r → r and for any numbers a and b with a ≤ b,"
427,1,['function'], Probability density functions,seg_61,the function f has to satisfy f(x) ≥ 0 for all x and ∫−
428,1,"['density function', 'probability density function', 'probability', 'function']", Probability density functions,seg_61,∞ ∞ f(x) dx = 1. we call f the probability density function (or probability density) of x.
429,1,"['density function', 'probability density function', 'interval', 'probability', 'function']", Probability density functions,seg_61,"note that the probability that x lies in an interval [a, b] is equal to the area under the probability density function f of x over the interval [a, b]; this is illustrated in figure 5.1. so if the interval gets smaller and smaller, the probability will go to zero: for any positive ε"
430,0,[], Probability density functions,seg_61,"and sending ε to 0, it follows that for any a"
431,1,"['continuous random variables', 'random variables', 'variables', 'intervals', 'random', 'continuous']", Probability density functions,seg_61,this implies that for continuous random variables you may be careless about the precise form of the intervals:
432,1,['probability'], Probability density functions,seg_61,"for small positive ε. hence f(a) can be interpreted as a (relative) measure of how likely it is that x will be near a. however, do not think of f(a) as a probability: f(a) can be arbitrarily large. an example of such an f is given in the following exercise."
433,1,"['density function', 'probability density function', 'random variable', 'variable', 'probability', 'random', 'function']", Probability density functions,seg_61,"quick exercise 5.1 let the function f be defined by f(x) = 0 if x ≤ 0 or x ≥ 1, and f(x) = 1/(2√x) for 0 < x < 1. you can check quickly that f satisfies the two properties of a probability density function. let x be a random variable with f as its probability density function. compute the probability that x lies between 10−4 and 10−2."
434,1,"['interval', 'probability mass function', 'distribution function', 'discrete', 'probability', 'random', 'function', 'mass function', 'cases', 'events', 'event', 'density function', 'disjoint', 'continuous random variables', 'probability density function', 'distribution', 'union', 'continuous', 'random variables', 'variables', 'discrete random variables']", Probability density functions,seg_61,"you should realize that discrete random variables do not have a probability density function f and continuous random variables do not have a probability mass function p, but that both have a distribution function f (a) = p(x ≤ a). using the fact that for a < b the event {x ≤ b} is a disjoint union of the events {x ≤ a} and {a < x ≤ b}, we can express the probability that x lies in an interval (a, b] directly in terms of f for both cases:"
435,1,"['density function', 'continuous', 'distribution function', 'continuous random variable', 'distribution', 'random variable', 'variable', 'random', 'function']", Probability density functions,seg_61,there is a simple relation between the distribution function f and the probability density function f of a continuous random variable. it follows from integral calculus that
436,1,"['density function', 'probability density function', 'distribution function', 'probabilistic', 'information', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'function']", Probability density functions,seg_61,both the probability density function and the distribution function of a continuous random variable x contain all the probabilistic information about x ; the probability distribution of x is described by either of them.
437,1,"['model', 'probability', 'experiment']", Probability density functions,seg_61,"we illustrate all this with an example. suppose we want to make a probability model for an experiment that can be described as “an object hits a disc of radius r in a completely arbitrary way” (of course, this is not you playing darts—nevertheless we will refer to this example as the darts example). we are interested in the distance x between the hitting point and the center of the disc. since distances cannot be negative, we have f (b) = p(x ≤ b) = 0 when b < 0. since the object hits the disc, we have f (b) = 1 when b > r. that the dart hits the disk in a completely arbitrary way we interpret as that the probability of hitting any region is proportional to the area of that region. in particular, because the disc has area πr2 and the disc with radius b has area πb2, we should put"
438,1,"['density function', 'probability density function', 'interval', 'probability', 'function']", Probability density functions,seg_61,"then the probability density function f of x is equal to 0 outside the interval [0, r] and"
439,1,['probability'], Probability density functions,seg_61,"quick exercise 5.2 compute for the darts example the probability that 0 < x ≤ r/2, and the probability that r/2 < x ≤ r."
440,1,"['interval', 'measurements', 'probability', 'associated', 'random', 'function', 'experiment', 'random variable', 'density function', 'probability density function', 'continuous', 'continuous random variable', 'outcome', 'outcomes', 'variable', 'experiments']", The uniform distribution,seg_63,"in this section we encounter a continuous random variable that describes an experiment where the outcome is completely arbitrary, except that we know that it lies between certain bounds. many experiments of physical origin have this kind of behavior. for instance, suppose we measure for a long time the emission of radioactive particles of some material. suppose that the experiment consists of recording in each hour at what times the particles are emitted. then the outcomes will lie in the interval [0,60] minutes. if the measurements would concentrate in any way, there is either something wrong with your geiger counter or you are about to discover some new physical law. not concentrating in any way means that subintervals of the same length should have the same probability. it is then clear (cf. equation (5.1)) that the probability density function associated with this experiment should be constant on [0, 60]. this motivates the following definition."
441,1,"['density function', 'probability density function', 'interval', 'continuous', 'continuous random variable', 'random variable', 'variable', 'probability', 'random', 'function']", The uniform distribution,seg_63,"definition. a continuous random variable has a uniform distribution on the interval [α, β] if its probability density function f is given by f(x) = 0 if x is not in [α, β] and"
442,1,['distribution'], The uniform distribution,seg_63,"we denote this distribution by u(α, β)."
443,1,"['distribution function', 'distribution', 'random variable', 'variable', 'random', 'function']", The uniform distribution,seg_63,"quick exercise 5.3 argue that the distribution function f of a random variable that has a u(α, β) distribution is given by f (x) = 0 if x < α, f (x) = 1 if x > β, and f (x) = (x − α)/(β − α) for α ≤ x ≤ β."
444,1,"['density function', 'probability density function', 'distribution function', 'distribution', 'probability', 'function']", The uniform distribution,seg_63,"in figure 5.3 the probability density function and the distribution function of a u(0, 1"
445,1,['distribution'], The uniform distribution,seg_63,3 ) distribution are depicted.
446,1,"['interval', 'probability', 'random', 'residence time', 'geometric', 'rate', 'exponential distribution', 'intervals', 'random variable', 'success', 'exponential', 'geometric distribution', 'distribution', 'independent', 'variable', 'uniformly distributed']", The exponential distribution,seg_65,"we already encountered the exponential distribution in the chemical reactor example of chapter 3. we will give an argument why it appears in that example. let v be the effluent volumetric flow rate, i.e., the volume that leaves the reactor over a time interval [0, t] is vt (and an equal volume enters the vessel at the other end). let v be the volume of the reactor vessel. then in total a fraction (v/v ) · t will have left the vessel during [0, t], when t is not too large. let the random variable t be the residence time of a particle in the vessel. to compute the distribution of t , we divide the interval [0, t] in n small intervals of equal length t/n. assuming perfect mixing, so that the particle’s position is uniformly distributed over the volume, the particle has probability p = (v/v ) · t/n to have left the vessel during any of the n intervals of length t/n. if we assume that the behavior of the particle in different time intervals of length t/n is independent, we have, if we call “leaving the vessel” a success, that t has a geometric distribution with success probability p. it follows (see also quick exercise 4.6) that the probability p(t > t) that the particle is still in the vessel at time t is, for large n, well approximated by"
447,1,['limit'], The exponential distribution,seg_65,"but then, letting n → ∞, we obtain (recall a well-known limit from your calculus course)"
448,1,"['density function', 'probability density function', 'distribution function', 'distribution', 'probability', 'function']", The exponential distribution,seg_65,"− v t it follows that the distribution function of t equals 1 − e v , and differentiating we obtain that the probability density function ft of t is equal to"
449,1,"['distribution', 'exponential', 'exponential distribution', 'parameter']", The exponential distribution,seg_65,"this is an example of an exponential distribution, with parameter v/v ."
450,1,"['density function', 'probability density function', 'continuous', 'continuous random variable', 'random variable', 'variable', 'exponential', 'probability', 'random', 'function', 'parameter']", The exponential distribution,seg_65,definition. a continuous random variable has an exponential distribution with parameter λ if its probability density function f is given by f(x) = 0 if x < 0 and
451,1,['distribution'], The exponential distribution,seg_65,we denote this distribution by exp(λ).
452,1,"['distribution', 'function', 'distribution function']", The exponential distribution,seg_65,the distribution function f of an exp(λ) distribution is given by
453,1,"['density function', 'probability density function', 'distribution function', 'distribution', 'probability', 'function']", The exponential distribution,seg_65,in figure 5.4 we show the probability density function and the distribution function of the exp(0.25) distribution.
454,1,"['memoryless property', 'exponential distribution', 'geometric distribution', 'distribution', 'exponential', 'geometric']", The exponential distribution,seg_65,"since we obtained the exponential distribution directly from the geometric distribution it should not come as a surprise that the exponential distribution also satisfies the memoryless property, i.e., if x has an exponential distribution, then for all s, t > 0,"
455,0,[], The exponential distribution,seg_65,"actually, this follows directly from"
456,1,"['parameter', 'probability', 'exponentially distributed', 'response', 'exponentially']", The exponential distribution,seg_65,quick exercise 5.4 a study of the response time of a certain computer system yields that the response time in seconds has an exponentially distributed time with parameter 0.25. what is the probability that the response time exceeds 5 seconds?
457,1,"['functions', 'densities', 'random variables', 'level', 'variables', 'distribution', 'pareto', 'probability', 'random']", The Pareto distribution,seg_67,"more than a century ago the economist vilfredo pareto ([20]) noticed that the number of people whose income exceeded level x was well approximated by c/xα, for some constants c and α > 0 (it appears that for all countries α is around 1.5). a similar phenomenon occurs with city sizes, earthquake rupture areas, insurance claims, and sizes of commercial companies. when these quantities are modeled as realizations of random variables x , then their distribution functions are of the type f (x) = 1 − 1/xα for x ≥ 1. (here 1 is a more or less arbitrarily chosen starting point—what matters is the behavior for large x.) differentiating, we obtain probability densities of the form f(x) = α/xα+1. this motivates the following definition."
458,1,"['density function', 'probability density function', 'continuous', 'continuous random variable', 'distribution', 'random variable', 'variable', 'pareto', 'probability', 'random', 'function', 'parameter', 'pareto distribution']", The Pareto distribution,seg_67,definition. a continuous random variable has a pareto distribution with parameter α > 0 if its probability density function f is given by f(x) = 0 if x < 1 and
459,1,['distribution'], The Pareto distribution,seg_67,we denote this distribution by par(α).
460,1,"['distribution', 'probability']", The Pareto distribution,seg_67,in figure 5.5 we depicted the probability density f and the distribution function f of the par(0.5) distribution.
461,1,"['probability', 'random', 'probability theory', 'independent random variables', 'model', 'distribution', 'random variables', 'independent', 'variables', 'errors', 'probability distribution', 'normal', 'average', 'normal distribution']", The normal distribution,seg_69,"the normal distribution plays a central role in probability theory and statistics. one of its first applications was due to c.f. gauss, who used it in 1809 to model observational errors in astronomy; see [13]. we will see in chapter 14 that the normal distribution is an important tool to approximate the probability distribution of the average of independent random variables."
462,1,"['density function', 'probability density function', 'parameters', 'continuous', 'continuous random variable', 'random variable', 'variable', 'normal', 'probability', 'random', 'function']", The normal distribution,seg_69,definition. a continuous random variable has a normal distribution with parameters µ and σ2 > 0 if its probability density function f is given by
463,1,['distribution'], The normal distribution,seg_69,"we denote this distribution by n(µ, σ2)."
464,1,"['density function', 'probability density function', 'distribution function', 'distribution', 'normal', 'probability', 'function', 'normal distribution']", The normal distribution,seg_69,in figure 5.6 the graphs of the probability density function f and distribution function f of the normal distribution with µ = 3 and σ2 = 6.25 are displayed.
465,1,"['distribution', 'function', 'distribution function']", The normal distribution,seg_69,"if x has an n(µ, σ2) distribution, then its distribution function is given by"
466,1,"['density function', 'standard normal distribution', 'random', 'probability density function', 'table', 'standard normal', 'distribution', 'random variable', 'variable', 'normal', 'probability', 'standard', 'function', 'transformation', 'normal distribution']", The normal distribution,seg_69,"unfortunately there is no explicit expression for f ; f has no antiderivative. however, as we shall see in chapter 8, any n(µ, σ2) distributed random variable can be turned into an n(0, 1) distributed random variable by a simple transformation. as a consequence, a table of the n(0, 1) distribution suffices. the latter is called the standard normal distribution, and because of its special role the letter φ has been reserved for its probability density function:"
467,1,"['right tail probabilities', 'distribution function', 'probability', 'random', 'function', 'symmetric', 'random variable', 'tail probabilities', 'standard', 'normal random variable', 'standard normal', 'distribution', 'standard normal random variable', 'probabilities', 'table', 'variable', 'normal', 'tail']", The normal distribution,seg_69,"note that φ is symmetric around zero: φ(−x) = φ(x) for each x. the corresponding distribution function is denoted by φ. the table for the standard normal distribution (see table b.1) does not contain the values of φ(a), but rather the so-called right tail probabilities 1−φ(a). if, for instance, we want to know the probability that a standard normal random variable z is smaller than or equal to 1, we use that p(z ≤ 1) = 1 − p(z ≥ 1). in the table we find that p(z ≥ 1) = 1−φ(1) is equal to 0.1587. hence p(z ≤ 1) = 1−0.1587 = 0.8413. with the table you can handle tail probabilities with numbers a given to two decimals. to find, for instance, p(z > 1.07), we stay in the same row in the table but move to the seventh column to find that p(z > 1.07) = 0.1423."
468,1,"['random', 'table', 'standard normal', 'distribution', 'random variable', 'variable', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", The normal distribution,seg_69,quick exercise 5.5 let the random variable z have a standard normal distribution. use table b.1 to find p(z ≤ 0.75). how do you know—without doing any calculations—that the answer should be larger than 0.5?
469,1,"['exponential distribution', 'distribution', 'exponential', 'mean', 'parameter', 'residence time', 'control', 'process']", Quantiles,seg_71,"recall the chemical reactor example, where the residence time t , measured in minutes, has an exponential distribution with parameter λ = v/v = 0.25. as we shall see in the next chapters, a consequence of this choice of λ is that the mean time the particle stays in the vessel is 4 minutes. however, from the viewpoint of process control this is not the quantity of interest. often, there will be some minimal amount of time the particle has to stay in the vessel to participate in the chemical reaction, and we would want that at least 90% of the particles stay in the vessel this minimal amount of time. in other words, we are interested in the number q with the property that p(t > q) = 0.9, or equivalently,"
470,1,"['quantile', 'percentile', 'case', 'distribution']", Quantiles,seg_71,the number q is called the 0.1th quantile or 10th percentile of the distribution. in the case at hand it is easy to determine. we should have
471,1,"['residence time', 'mean']", Quantiles,seg_71,"−0.25q this holds exactly when e = 0.9 or when −0.25q = ln(0.9) = −0.105. so q = 0.42. hence, although the mean residence time is 4 minutes, 10% of"
472,0,[], Quantiles,seg_71,"the particles stays less than 0.42 minute in the vessel, which is just slightly more than 25 seconds! we use the following general definition."
473,1,"['quantile', 'percentile', 'continuous random variable', 'distribution', 'random variable', 'variable', 'random', 'continuous']", Quantiles,seg_71,definition. let x be a continuous random variable and let p be a number between 0 and 1. the pth quantile or 100pth percentile of the distribution of x is the smallest number qp such that
474,1,"['distribution', 'percentile', 'median']", Quantiles,seg_71,the median of a distribution is its 50th percentile.
475,1,"['distribution', 'median']", Quantiles,seg_71,"quick exercise 5.6 what is the median of the u(2, 7) distribution?"
476,1,"['continuous random variables', 'random variables', 'interval', 'variables', 'random', 'continuous']", Quantiles,seg_71,"for continuous random variables qp is often easy to determine. indeed, if f is strictly increasing from 0 to 1 on some interval (which may be infinite to one or both sides), then"
477,1,['distribution'], Quantiles,seg_71,where f inv is the inverse of f . this is illustrated in figure 5.7 for the exp(0.25) distribution.
478,1,"['distribution function', 'probability', 'function', 'standard normal distribution', 'exponential distribution', 'quantiles', 'exponential', 'standard', 'density function', 'probability density function', 'percentile', 'standard normal', 'distribution', 'table', 'normal', 'normal distribution']", Quantiles,seg_71,"for an exponential distribution it is easy to compute quantiles. this is different for the standard normal distribution, where we have to use a table (like table b.1). for example, the 90th percentile of a standard normal is the number q0.9 such that φ(q0.9) = 0.9, which is the same as 1 − φ(q0.9) = 0.1, and the table gives us q0.9 = 1.28. this is illustrated in figure 5.8, with both the probability density function and the distribution function of the standard normal distribution."
479,1,"['quantile', 'standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Quantiles,seg_71,"quick exercise 5.7 find the 0.95th quantile q0.95 of a standard normal distribution, accurate to two decimals."
480,1,['probability'], Solutions to the quick exercises,seg_73,"1 1/(2√x) dx = 1 (so f is a probability density function—nonnegativity being obvious), and"
481,1,"['random variable', 'variable', 'random']", Solutions to the quick exercises,seg_73,"actually, the random variable x arises in a natural way; see equation (7.1)."
482,1,"['distribution function', 'distribution', 'function']", Solutions to the quick exercises,seg_73,in other words; the distribution function increases linearly from the value 0 in α to the value 1 in β.
483,1,['response'], Solutions to the quick exercises,seg_73,"5.4 if x is the response time, we ask for p(x > 5). this equals"
484,1,"['table', 'standard normal', 'distribution', 'symmetry', 'normal', 'probability', 'standard', 'standard normal distribution', 'normal distribution']", Solutions to the quick exercises,seg_73,"5.5 in the eighth row and sixth column of the table, we find that 1−φ(0.75) = 0.2266. hence the answer is 1− 0.2266 = 0.7734. because of the symmetry of the probability density φ, half of the mass of a standard normal distribution lies on the negative axis. hence for any number a > 0, it should be true that p(z ≤ a) > p(z ≤ 0) = 0.5."
485,1,"['interval', 'median', 'distribution function', 'distribution', 'function']", Solutions to the quick exercises,seg_73,"5.6 the median is the number q0.5 = f inv(0.5). you either see directly that you have got half of the mass to both sides of the middle of the interval, hence q0.5 = (2 + 7)/2 = 4.5, or you solve with the distribution function:"
486,1,['table'], Solutions to the quick exercises,seg_73,"5.7 since φ(q0.95) = 0.95 is the same as 1 − φ(q0.95) = 0.05, the table gives us q0.95 = 1.64, or more precisely, if we interpolate between the fourth and the fifth column; 1.645."
487,1,"['density function', 'probability density function', 'continuous', 'continuous random variable', 'random variable', 'variable', 'probability', 'random', 'function']", Exercises,seg_75,5.1 let x be a continuous random variable with probability density function
488,1,"['distribution', 'function', 'distribution function']", Exercises,seg_75,"b. determine the distribution function f of x , and draw its graph."
489,1,"['random variable', 'variable', 'random']", Exercises,seg_75,"5.2 let x be a random variable that takes values in [0, 1], and is further given by"
490,1,"['continuous', 'distribution function', 'continuous random variable', 'distribution', 'random variable', 'variable', 'random', 'function']", Exercises,seg_75,"5.3 let a continuous random variable x be given that takes values in [0, 1], and whose distribution function f satisfies"
491,1,"['density function', 'probability density function', 'probability', 'function']", Exercises,seg_75,4 ≤ x ≤ 3 4). b. what is the probability density function of x?
492,1,"['continuous random variable', 'distribution', 'random variable', 'variable', 'random', 'continuous']", Exercises,seg_75,"5.4 jensen, arriving at a bus stop, just misses the bus. suppose that he decides to walk if the (next) bus takes longer than 5 minutes to arrive. suppose also that the time in minutes between the arrivals of buses at the bus stop is a continuous random variable with a u(4, 6) distribution. let x be the time that jensen will wait."
493,1,['probability'], Exercises,seg_75,a. what is the probability that x is less than 4 1
494,1,['probability'], Exercises,seg_75,b. what is the probability that x equals 5 (minutes)?
495,1,"['discrete random variable', 'continuous random variable', 'discrete', 'random variable', 'variable', 'random', 'continuous']", Exercises,seg_75,c. is x a discrete random variable or a continuous random variable?
496,1,"['density function', 'probability density function', 'continuous', 'continuous random variable', 'random variable', 'variable', 'probability', 'random', 'function']", Exercises,seg_75,5.5 the probability density function f of a continuous random variable x
497,1,"['distribution', 'function', 'distribution function']", Exercises,seg_75,b. compute the distribution function of x .
498,1,['distribution'], Exercises,seg_75,5.6 let x have an exp(0.2) distribution. compute p(x > 5).
499,1,"['density function', 'model', 'probability density function', 'continuous', 'experiment', 'continuous random variable', 'random variable', 'variable', 'probability', 'random', 'function']", Exercises,seg_75,"5.7 the score of a student on a certain exam is represented by a number between 0 and 1. suppose that the student passes the exam if this number is at least 0.55. suppose we model this experiment by a continuous random variable s, the score, whose probability density function is given by"
500,1,['probability'], Exercises,seg_75,a. what is the probability that the student fails the exam?
501,0,[], Exercises,seg_75,"b. what is the score that he will obtain with a 50% chance, in other words,"
502,1,"['distribution', 'percentile']", Exercises,seg_75,what is the 50th percentile of the score distribution?
503,1,"['distribution', 'function', 'distribution function']", Exercises,seg_75,5.8 consider quick exercise 5.2. for another dart thrower it is given that his distance to the center of the disc y is described by the following distribution function:
504,1,"['density function', 'probability density function', 'probability', 'function']", Exercises,seg_75,a. sketch the probability density function g(y) = d g(y).
505,0,[], Exercises,seg_75,dy b. is this person “better” than the person in quick exercise 5.2?
506,1,"['distribution function', 'distribution', 'associated', 'function']", Exercises,seg_75,c. sketch a distribution function associated to a person who in 90% of his
507,0,[], Exercises,seg_75,throws hits the disc no further than 0.1 · r of the center.
508,1,"['random variable', 'variable', 'random']", Exercises,seg_75,"5.9 suppose we choose arbitrarily a point from the square with corners at (2,1), (3,1), (2,2), and (3,2). the random variable a is the area of the triangle with its corners at (2,1), (3,1) and the chosen point (see figure 5.9)."
509,1,['set'], Exercises,seg_75,"a. what is the largest area a that can occur, and what is the set of points"
510,1,"['distribution', 'function', 'distribution function']", Exercises,seg_75,b. determine the distribution function f of a.
511,1,"['density function', 'probability density function', 'probability', 'function']", Exercises,seg_75,c. determine the probability density function f of a.
512,1,"['percentage', 'mean', 'parameter', 'residence time']", Exercises,seg_75,5.10 consider again the chemical reactor example with parameter λ = 0.5. we saw in section 5.6 that 10% of the particles stay in the vessel no longer than about 12 seconds—while the mean residence time is 2 minutes. which percentage of the particles stay no longer than 2 minutes in the vessel?
513,1,"['distribution', 'median']", Exercises,seg_75,5.11 compute the median of an exp(λ) distribution.
514,1,"['distribution', 'median']", Exercises,seg_75,5.12 compute the median of a par(1) distribution.
515,1,"['random', 'standard normal', 'distribution', 'random variable', 'variable', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Exercises,seg_75,5.13 we consider a random variable z with a standard normal distribution.
516,1,"['density function', 'probability density function', 'symmetry', 'probability', 'function']", Exercises,seg_75,a. show why the symmetry of the probability density function φ of z implies
517,1,"['percentile', 'standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Exercises,seg_75,5.14 determine the 10th percentile of a standard normal distribution.
518,1,"['model', 'random variables', 'simulation', 'probabilistic models', 'variables', 'probabilistic', 'stochastic simulation', 'random', 'outcomes']", Simulation,seg_77,"sometimes probabilistic models are so complex that the tools of mathematical analysis are not sufficient to answer all relevant questions about them. stochastic simulation is an alternative approach: values are generated for the random variables and inserted into the model, thus mimicking outcomes for the whole system. it is shown in this chapter how one can use uniform random number generators to mimic random variables. also two larger simulation examples are presented."
519,1,"['model', 'rates']", What is simulation,seg_79,"in many areas of science, technology, government, and business, models are used to gain understanding of some part of reality (the portion of interest is often referred to as “the system”). sometimes these are physical models, such as a scale model of an airplane in a wind tunnel or a scale model of a chemical plant. other models are abstract, such as macroeconomic models consisting of equations relating things like interest rates, unemployment, and inflation or partial differential equations describing global weather patterns."
520,1,"['model', 'rate', 'simulation', 'distribution', 'experiments', 'average', 'response']", What is simulation,seg_79,"in simulation, one uses a model to create specific situations in order to study the response of the model to them and then interprets this in terms of what would happen to the system “in the real world.” in this way, one can carry out experiments that are impossible, too dangerous, or too expensive to do in the real world—addressing questions like: what happens to the average temperature if we reduce the greenhouse gas emissions globally by 50%? can the plane still fly if engines 3 and 4 stop in midair? what happens to the distribution of wealth if we halve the tax rate?"
521,1,"['random variables', 'simulation', 'variables', 'uncertainty', 'probabilistic', 'stochastic simulation', 'random']", What is simulation,seg_79,"more specifically, we focus on situations and problems where randomness or uncertainty or both play a significant or dominant role and should be modeled explicitly. models for such systems involve random variables, and we speak of probabilistic or stochastic models. simulating them is stochastic simulation. in"
522,1,"['model', 'random variables', 'simulation', 'variables', 'probability theory', 'stochastic simulation', 'random', 'probability', 'distributions']", What is simulation,seg_79,"the preceding chapters we have encountered some of the tools of probability theory, and we will encounter others in the chapters to come. with these tools we can compute quantities of interest explicitly for many models. stochastic simulation of a system means generating values for all the random variables in the model, according to their specified distributions, and recording and analyzing what happens. we refer to the generated values as realizations of the random variables."
523,1,"['random variables', 'simulation', 'variables', 'random variable', 'variable', 'stochastic simulation', 'random', 'realization']", What is simulation,seg_79,"for us, there are two reasons to learn about stochastic simulation. the first is that for complex systems, simulation can be an alternative to mathematical analysis, sometimes the only one. the second reason is that through simulation, we can get more feeling for random variables, and this is why we study stochastic simulation at this point in the book. we start by asking how we can generate a realization of a random variable."
524,1,"['random number', 'outcome', 'random variable', 'variable', 'random', 'transforming', 'realization']", Generating realizations of random variables,seg_81,"simulations are almost always done using computers, which usually have one or more so-called (pseudo) random number generators. a call to the random number generator returns a random number between 0 and 1, which mimics a realization of a u(0, 1) variable. with this source of uniform (pseudo) randomness we can construct any random variable we want by transforming the outcome, as we shall see."
525,0,[], Generating realizations of random variables,seg_81,quick exercise 6.1 describe how you can simulate a coin toss when instead of a coin you have a die. any ideas on how to simulate a roll of a die if you only have a coin?
526,1,"['random variables', 'variables', 'random']", Generating realizations of random variables,seg_81,bernoulli random variables
527,1,"['distribution', 'random variable', 'variable', 'random']", Generating realizations of random variables,seg_81,"suppose u has a u(0, 1) distribution. to construct a ber(p) random variable for some 0 < p < 1, we define"
528,1,"['bernoulli distribution', 'distribution', 'random variable', 'variable', 'bernoulli', 'parameter', 'random']", Generating realizations of random variables,seg_81,this random variable x has a bernoulli distribution with parameter p.
529,1,"['outcomes', 'probabilities', 'random variable', 'variable', 'random']", Generating realizations of random variables,seg_81,"quick exercise 6.2 a random variable y has outcomes 1, 3, and 4 with the following probabilities: p(y = 1) = 3/5, p(y = 3) = 1/5, and p(y = 4) = 1/5. describe how to construct y from a u(0, 1) random variable."
530,1,"['random variables', 'variables', 'random']", Generating realizations of random variables,seg_81,continuous random variables
531,1,"['interval', 'continuous', 'distribution function', 'continuous random variable', 'case', 'distribution', 'random variable', 'variable', 'random', 'function']", Generating realizations of random variables,seg_81,"suppose we have the distribution function f of a continuous random variable and we wish to construct a random variable with this distribution. we show how to do this if f is strictly increasing from 0 to 1 on an interval. in that case f has an inverse function f inv. figure 6.1 shows an example: f is strictly increasing on the interval [2, 10]; the inverse f inv is a function from the interval [0, 1] to the interval [2, 10]."
532,1,"['random variable', 'events', 'variable', 'random']", Generating realizations of random variables,seg_81,"note how u relates to f inv(u) as f (x) relates to x. we see that u ≤ f (x) is equivalent with f inv(u) ≤ x. if instead of a real number u we consider a u(0, 1) random variable u , we obtain that the corresponding events are the same:"
533,1,"['random variable', 'variable', 'random']", Generating realizations of random variables,seg_81,"we know about the u(0, 1) random variable u that p(u ≤ b) = b for any number 0 ≤ b ≤ 1. substituting b = f (x) we see"
534,0,[], Generating realizations of random variables,seg_81,"from equality (6.1), therefore,"
535,1,"['distribution function', 'distribution', 'random variable', 'variable', 'random', 'function']", Generating realizations of random variables,seg_81,"in other words, the random variable f inv(u) has distribution function f ."
536,1,['function'], Generating realizations of random variables,seg_81,what remains is to find the function f inv. from figure 6.1 we see
537,1,"['random variables', 'variables', 'random']", Generating realizations of random variables,seg_81,exponential random variables
538,1,"['method', 'exponential distribution', 'interval', 'distribution function', 'distribution', 'exponential', 'function']", Generating realizations of random variables,seg_81,"we apply this method to the exponential distribution. on the interval [0,∞), the exp(λ) distribution function is strictly increasing and given by"
539,1,"['distribution', 'random variable', 'variable', 'random']", Generating realizations of random variables,seg_81,"1 ln(1−u) and if u has a u(0, 1) distribution, then the random variable x defined by"
540,1,['distribution'], Generating realizations of random variables,seg_81,has an exp(λ) distribution.
541,1,"['distribution', 'efficient']", Generating realizations of random variables,seg_81,"in practice, one replaces 1−u with u , because both have a u(0, 1) distribution (see exercise 6.3). leaving out the subtraction leads to more efficient computer code. so instead of x we may use"
542,1,['distribution'], Generating realizations of random variables,seg_81,which also has an exp(λ) distribution.
543,1,"['distribution', 'function', 'distribution function']", Generating realizations of random variables,seg_81,"quick exercise 6.3 a distribution function f is 0 for x < 1 and 1 for x > 3,"
544,1,"['distribution', 'random variable', 'variable', 'random']", Generating realizations of random variables,seg_81,"1 (x − 1)2 if 1 ≤ x ≤ 3. let u be a u(0, 1) random variable. construct a random variable with distribution f from u ."
545,1,"['random variables', 'random', 'random number', 'variables', 'random variable', 'events', 'variable', 'simulations', 'realization']", Generating realizations of random variables,seg_81,"for simulations we often want to generate realizations for a large number of random variables. random number generators have been designed with this purpose in mind: each new call mimics a new u(0, 1) random variable. the sequence of numbers thus generated is considered as a realization of a sequence of u(0, 1) random variables u1, u2, u3,. . . with the special property that the events {ui ≤ ai} are independent1 for every choice of the ai."
546,1,"['operating rules', 'events']", Comparing two jury rules,seg_83,"at the olympic games there are several sports events that are judged by a jury, including gymnastics, figure skating, and ice dancing. during the 2002 winter games a dispute arose concerning the gold medal in ice dancing: there were allegations that the russian team had bribed a french jury member, thereby causing the russian pair to win just ahead of the canadians. we look into operating rules for juries, although we leave the effects of bribery to the exercises (exercise 6.11)."
547,1,['transformed'], Comparing two jury rules,seg_83,"suppose we have a jury of seven members, and for each performance each juror assigns a grade. the seven grades are to be transformed into a final score. two rules to do this are under consideration, and we want to choose"
548,1,"['average', 'scores']", Comparing two jury rules,seg_83,"the better one. for the first one, the highest and lowest scores are removed and the final score is the average of the remaining five. for the second rule, the scores are put in ascending order and the middle one is assigned as final score. before you continue reading, consider which rule is better and how you can verify this."
549,1,"['model', 'probabilistic model', 'probabilistic']", Comparing two jury rules,seg_83,a probabilistic model
550,1,"['model', 'scores', 'random']", Comparing two jury rules,seg_83,for our investigation we assume that the scores the jurors assign deviate by some random amount from the true or deserved score. we model the score that juror i assigns when the performance deserves a score g by
551,1,"['functions', 'random variables', 'variables', 'random']", Comparing two jury rules,seg_83,"where z1, . . . , z7 are random variables with values around zero. let h1 and h2 be functions implementing the two rules:"
552,1,['average'], Comparing two jury rules,seg_83,"h1(y1, . . . , y7) = average of the middle five of y1, . . . , y7,"
553,1,['deviations'], Comparing two jury rules,seg_83,we are interested in deviations from the deserved score g:
554,1,"['model', 'random variables', 'variables', 'errors', 'random', 'distributions']", Comparing two jury rules,seg_83,"the distributions of t and m depend on the individual jury grades, and through those, on the juror-deviations z1, z2, . . . , z7, which we model as u(−0.5, 0.5) variables. this more or less finishes the modeling phase: we have given a stochastic model that mimics the workings of a jury and have defined, in terms of the variables in the model, the random variables t and m that represent the errors that result after application of the jury rules."
555,1,"['model', 'simulation']", Comparing two jury rules,seg_83,"in any serious application, the model should be validated. this means that one tries to gather evidence to convince oneself and others that the model adequately reflects the workings of the real system. in this chapter we are more interested in showing what you can do with simulation once you have a model, so we skip the validation."
556,1,"['random variables', 'probabilities', 'variables', 'deviations', 'mean', 'random']", Comparing two jury rules,seg_83,"the next phase is analysis: which of the deviations is closer to zero? because t and m are random variables, we would have to clarify what we mean by that, and answering the question certainly involves computing probabilities about t and m . we cannot do this with what we have learned so far, but we know how to simulate, so this is what we do."
557,1,"['random variable', 'variable', 'random', 'realization']", Comparing two jury rules,seg_83,"to generate a realization of a u(−0.5, 0.5) random variable, we only need to subtract 0.5 from the result we obtain from a call to the random generator."
558,1,['deviations'], Comparing two jury rules,seg_83,"we do this 7 times and insert the resulting values in (6.2) as jury deviations z1, . . . , z7, and substitute them in equations (6.3) to obtain t and m (the value of g is irrelevant: it drops out of the calculation):"
559,1,['average'], Comparing two jury rules,seg_83,"t = average of the middle five of z1, . . . , z7, (6.4)"
560,1,"['random variables', 'simulation', 'table', 'variables', 'results', 'random']", Comparing two jury rules,seg_83,"in simulation terminology, this is called a run: we have gone through the whole procedure once, inserting realizations for the random variables. if we repeat the whole procedure, we have a second run; see table 6.1 for the results of five runs."
561,1,[], Comparing two jury rules,seg_83,"quick exercise 6.4 the next realizations for z1,. . . , z7 are: −0.05, 0.26, 0.25, 0.39, 0.22, 0.23, 0.13. determine the corresponding realizations of t and m ."
562,1,"['deviation', 'histogram', 'interval', 'moment', 'table', 'variation', 'deviations', 'process', 'vary', 'realization']", Comparing two jury rules,seg_83,"table 6.1 can be used to check some computations. we also see that the realization of t was closest to zero in runs 3 and 5, the realization of m was closest to zero in runs 1 and 4, and they were (about) the same in run 2. there is no clear conclusion from this, and even if there was, one could wonder whether the next five runs would yield the same picture. because the whole process mimics randomness, one has to expect some variation—or perhaps a lot. in later chapters we will get a better understanding of this variation; for the moment we just say that judgment based on a large number of runs is better. we do one thousand runs and exchange the table for pictures. figure 6.3 depicts, for juror 1, a histogram of all the deviations from the true score g. for each interval of length 0.05 we have counted the number of runs for which the deviation of juror 1 fell in that interval. these numbers vary from about 40 to about 60."
563,1,"['results', 'scores', 'histograms']", Comparing two jury rules,seg_83,"this is just to get an idea about the results for an individual juror. in figure 6.4 we see histograms for the final scores. comparing the histograms, it seems that the realizations of t are more concentrated near zero than those of m ."
564,1,"['plot', 'cases', 'scores', 'histograms']", Comparing two jury rules,seg_83,"however, the two histograms do not tell us anything about the relation between t and m , so we plot the realizations of pairs (t, m) for all one thousand runs (figure 6.5). from this plot we see that in most cases m and t go in the same direction: if t is positive, then usually m is also positive, and the same goes for negative values. in terms of the final scores, both rules generally overvalue and undervalue the performance simultaneously. on closer examination, with help of the line drawn from (−0.5,−0.5) to (0.5, 0.5), we see that the t values tend to be a little closer to zero than the m values."
565,1,"['deviation', 'histogram', 'deviations']", Comparing two jury rules,seg_83,"this suggests that we make a histogram that shows the difference of the absolute deviations from true score. for rule 1 this absolute deviation is |t |, for rule 2 it is |m |. if the difference |m | − |t | is positive, then t is closer to zero than m , and the difference tells us by how much. a negative difference"
566,1,"['cases', 'histogram']", Comparing two jury rules,seg_83,"means that m was closer. in figure 6.6 all the differences are shown in a histogram. the bars to the right of zero represent 696 runs. so, in about 70% of the runs, rule 1 resulted in a final score that is closer to the true score than rule 2. in about 30% of the cases, rule 2 was better, but generally by a smaller amount, as we see from the histogram."
567,1,"['information', 'locations']", The singleserver queue,seg_85,"there are many situations in life where you stand in a line waiting for some service: when you want to withdraw money from a cash dispenser, borrow books at the library, be admitted to the emergency room at the hospital, or pump gas at the gas station. many other queueing situations are hidden: an email message you send might be queued at the local server until it has sent all messages that were submitted ahead of yours; searching the internet, your browser sends and receives packets of information that are queued at various stages and locations; in assembly lines, partly finished products move from station to station, each time waiting for the next component to be added."
568,1,['model'], The singleserver queue,seg_85,"we are going to study one simple queueing model, the so-called single-server queue: it has one server or service mechanism, and the arriving customers await their turn in order of their arrival. for definiteness, think of an oasis with one big water well. people arrive at the well with bottles, jerry cans, and other types of containers, to pump water. the supply of water is large, but the pump capacity is limited. the pump is about to be replaced, and while it is clear that a larger pump capacity will result in shorter waiting times, more powerful pumps are also more expensive. therefore, to prepare a decision that balances costs and benefits, we wish to investigate the relationship between pump capacity and system performance."
569,1,"['model', 'random variables', 'random', 'variables', 'standard', 'average']", The singleserver queue,seg_85,"a stochastic model is in order: some general characteristics are known, such as how many people arrive per day and how much water they take on average, but the individual arrival times and amounts are unpredictable. we introduce random variables to describe them: let t1 be the time between the start at time zero and the arrival of the first customer, t2 the time between the arrivals of the first and the second customer, t3 the time between the second and the third, etc.; these are called the interarrival times. let si be the length of time that customer i needs to use the pump; in standard terminology this is called the service time. this is our description so far:"
570,1,"['model', 'random variable', 'variable', 'parameter', 'random']", The singleserver queue,seg_85,"the pump capacity v (liters per minute) is not a random variable but a model parameter or decision variable, whose “best” value we wish to determine. so if customer i requires ri liters of water, then her service time is"
571,1,"['model', 'random variables', 'variables', 'distribution', 'random']", The singleserver queue,seg_85,"to complete the model description, we need to specify the distribution of the random variables ti and ri:"
572,1,['distribution'], The singleserver queue,seg_85,"interarrival times: every ti has an exp(0.5) distribution (minutes); service requirement: every ri has a u(2, 5) distribution (liters)."
573,1,"['model', 'exponential distribution', 'uniform distribution', 'distribution', 'exponential', 'processes', 'distributions']", The singleserver queue,seg_85,"this particular choice of distributions would have to be supported by evidence that they are suited for the system at hand: a validation step as suggested for the jury model is appropriate here as well. for many arrival type processes, however, the exponential distribution is reasonable as a model for the interarrival times (see chapter 12). the particular uniform distribution chosen for the required amount of water says that all amounts between 2 and 5 liters are equally likely. so there is no sheik who owns a 5000-liter water truck in “our” oasis."
574,1,['model'], The singleserver queue,seg_85,"to evaluate system performance, we want to extract from the model the waiting times of the customers and how busy it is at the pump."
575,0,[], The singleserver queue,seg_85,"let wi denote the waiting time of customer i. the first customer is lucky; the system starts empty, and so w1 = 0. for customer i the waiting time depends on how long customer i−1 spent in the system compared to the time between their respective arrivals. we see that if the interarrival time ti is long, relatively speaking, then customer i arrives after the departure of customer i − 1, and so wi = 0:"
576,0,[], The singleserver queue,seg_85,"on the other hand, if customer i arrives before the departure, the waiting time wi equals whatever remains of wi−1 + si−1:"
577,1,['cases'], The singleserver queue,seg_85,"summarizing the two cases, we see obtain:"
578,1,"['model', 'simulation', 'table', 'cases']", The singleserver queue,seg_85,"to carry out a simulation, we start at time zero and generate realizations of the interarrival times (the ti) and service requirements (the ri) for as long as we want, computing the other quantities that follow from the model on the way. table 6.2 shows the values generated this way, for two pump capacities (v = 2 and 3) for the first six customers. note that in both cases we use the same realizations of ti and ri."
579,1,['table'], The singleserver queue,seg_85,quick exercise 6.5 the next four realizations are t7: 1.86; r7: 4.79; t8: 1.08; and r8: 2.33. complete the corresponding rows of the table.
580,1,"['simulations', 'average']", The singleserver queue,seg_85,"longer simulations produce so many numbers that we will drown in them unless we think of something. first, we summarize the waiting times of the first n customers with their average:"
581,1,"['plot', 'simulation', 'table', 'law of large numbers', 'average']", The singleserver queue,seg_85,"then, instead of giving a table, we plot the pairs (n, w̄n), for n = 1, 2, . . . until the end of the simulation. in figure 6.7 we see that both lines bounce up and down a bit. toward the end, the average waiting time for pump capacity 3 is about 0.5 and for v = 2 about 2. in a longer simulation we would see each of the averages converge to a limiting value (a consequence of the so-called law of large numbers, the topic of chapter 13)."
582,1,"['plot', 'moment']", The singleserver queue,seg_85,"to show how busy it is at the pump one could record how many customers are waiting in the queue and plot this quantity against time. a slightly different approach is to record at every moment how much work there is in the system, that is, how much time it would take to serve everyone present at that moment. for example, if i am halfway through filling my 4-liter jerry can and three persons are waiting who require 2, 3, and 5 liters, respectively, then there are 12 liters to go; at v = 2, there is 6 minutes of work in the system, and at v = 3 just 4."
583,0,[], The singleserver queue,seg_85,"the amount of work in the system just before a customer arrives equals the waiting time of that customer, because it is exactly the time it takes to finish the work for everybody ahead of her. the work-in-system at time t tells us how long the wait would be if somebody were to arrive at t. for this reason, this quantity is also called the virtual waiting time."
584,1,"['table', 'function', 'realization']", The singleserver queue,seg_85,"figure 6.8 shows the work-in-system as a function of time for the first 15 minutes, using the same realizations that were the basis for table 6.2. in the top graph, corresponding to v = 2, the work in the system jumps to 2.20 (which is the realization of r1/2) at t = 0.24, when the first customer arrives. so at t = 2.21, which is 1.97 later, there is 2.20− 1.97 = 0.23 minute of work left; this is the waiting time for customer 2, who brings an amount of work of 2.00 minutes, so the peak at 1.97 is 0.23 + 2.00 = 2.23, etc. in the bottom graph we see the work-in-system reach zero more often, because the individual (work) amounts are 2/3 of what they are when v = 2. more often, arriving"
585,0,[], The singleserver queue,seg_85,customers find the queue empty and the pump not in use; they do not have to wait.
586,1,['function'], The singleserver queue,seg_85,"in figure 6.9 the work-in-system is depicted as a function of time for the first 100 minutes of our run. at pump capacity 2 the virtual waiting time peaks at close to 11 minutes after about 55 minutes, whereas with v = 3 the corresponding peak is only about 4 minutes. there also is a marked difference in the proportion of time the system is empty."
587,1,"['outcome', 'tails', 'outcomes']", Solutions to the quick exercises,seg_87,"6.1 to simulate the coin, choose any three of the six possible outcomes of the die, report heads if one of these three outcomes turns up, and report tails otherwise. for example, heads if the outcome is odd, tails if it is even."
588,1,['table'], Solutions to the quick exercises,seg_87,to simulate the die using a coin is more difficult; one solution is as follows. toss the coin three times and use the following conversion table to map the result:
589,0,[], Solutions to the quick exercises,seg_87,coins hhh hht hth htt thh tht
590,0,[], Solutions to the quick exercises,seg_87,repeat the coin tosses if you get tth or ttt.
591,1,"['variable', 'set']", Solutions to the quick exercises,seg_87,"6.2 let the u(0, 1) variable be u and set:"
592,1,"['method', 'distribution function', 'distribution', 'function']", Solutions to the quick exercises,seg_87,"6.3 the given distribution function f is strictly increasing between 1 and 3, so we use the method with f inv. solve the equation f (x) = 4"
593,1,['set'], Solutions to the quick exercises,seg_87,"for x. this yields x = 1 + 2√u, so we can set x = 1 + 2√u . if you need to be convinced, determine fx ."
594,1,[], Solutions to the quick exercises,seg_87,input realizations v = 2 v = 3
595,1,['distribution'], Exercises,seg_89,"6.1 let u have a u(0, 1) distribution."
596,1,['outcome'], Exercises,seg_89,a. describe how to simulate the outcome of a roll with a die using u .
597,0,[], Exercises,seg_89,b. define y as follows: round 6u + 1 down to the nearest integer. what are
598,1,"['outcomes', 'probabilities']", Exercises,seg_89,the possible outcomes of y and their probabilities?
599,1,"['pseudo random', 'random variable', 'variable', 'random', 'realization']", Exercises,seg_89,6.2 we simulate the random variable x = 1 + 2√u constructed in quick exercise 6.3. as realization for u we obtain from the pseudo random generator the number u = 0.3782739.
600,1,"['random variable', 'variable', 'random', 'realization']", Exercises,seg_89,a. what is the corresponding realization x of the random variable x?
601,1,['random'], Exercises,seg_89,"b. if the next call to the random generator yields u = 0.3, will the corre-"
602,1,['realization'], Exercises,seg_89,sponding realization for x be larger or smaller than the value you found in a?
603,1,['probability'], Exercises,seg_89,c. what is the probability the next draw will be smaller than the value you
604,1,"['density function', 'probability density function', 'distribution function', 'distribution', 'probability', 'function']", Exercises,seg_89,"6.3 let u have a u(0, 1) distribution. show that z = 1 − u has a u(0, 1) distribution by deriving the probability density function or the distribution function."
605,1,"['distribution', 'function', 'distribution function']", Exercises,seg_89,"6.4 let f be the distribution function as given in quick exercise 6.3: f (x) is 0 for x < 1 and 1 for x > 3, and f (x) = 4"
606,1,"['distribution function', 'distribution', 'random variable', 'variable', 'random', 'function']", Exercises,seg_89,"answer it is claimed that x = 1 + 2√u has distribution function f , where u is a u(0, 1) random variable. verify this by computing p(x ≤ a) and checking that this equals f (a), for any a."
607,1,['distribution'], Exercises,seg_89,"6.5 we have seen that if u has a u(0, 1) distribution, then x = − lnu has"
608,1,['distribution'], Exercises,seg_89,−a an exp(1) distribution. check this by verifying that p(x ≤ a) = 1 − e for a ≥ 0.
609,1,"['random number', 'random variable', 'variable', 'random', 'random numbers']", Exercises,seg_89,"6.6 somebody messed up the random number generator in your computer: instead of uniform random numbers it generates numbers with an exp(2) distribution. describe how to construct a u(0, 1) random variable u from an exp(2) distributed x . hint: look at how you obtain an exp(2) random variable from a u(0, 1) random variable."
610,1,"['functions', 'random variables', 'variables', 'distribution', 'random', 'weibull']", Exercises,seg_89,"6.7 in models for the lifetimes of mechanical components one sometimes uses random variables with distribution functions from the so-called weibull family. here is an example: f (x) = 0 for x < 0, and"
611,1,"['distribution', 'random variable', 'variable', 'random']", Exercises,seg_89,"construct a random variable z with this distribution from a u(0, 1) variable."
612,1,"['distribution', 'random variable', 'variable', 'random']", Exercises,seg_89,"6.8 a random variable x has a par(3) distribution, so with distribution func-"
613,1,"['distribution', 'random variable', 'variable', 'pareto', 'random', 'pareto distribution']", Exercises,seg_89,"−3 tion f with f (x) = 0 for x < 1, and f (x) = 1− x for x ≥ 1. for details on the pareto distribution see section 5.4. describe how to construct x from a u(0, 1) random variable."
614,1,['simulated'], Exercises,seg_89,6.9 in quick exercise 6.1 we simulated a die by tossing three coins. recall that we might need several attempts before succeeding.
615,1,['probability'], Exercises,seg_89,a. what is the probability that we succeed on the first try?
616,1,['distribution'], Exercises,seg_89,b. let n be the number of tries that we need. determine the distribution
617,1,"['random variables', 'variables', 'geometric random variables', 'random variable', 'variable', 'random', 'geometric']", Exercises,seg_89,6.10 there is usually more than one way to simulate a particular random variable. in this exercise we consider two ways to generate geometric random variables.
618,1,"['random variables', 'independent', 'variables', 'random']", Exercises,seg_89,"a. we give you a sequence of independent u(0, 1) random variables u1, u2,"
619,1,"['bernoulli', 'random']", Exercises,seg_89,". . . . from this sequence, construct a sequence of bernoulli random vari-"
620,1,"['random variables', 'variables', 'bernoulli', 'random variable', 'variable', 'random']", Exercises,seg_89,"ables. from the sequence of bernoulli random variables, construct a (single) geo(p) random variable."
621,1,"['random variable', 'variable', 'random']", Exercises,seg_89,"b. it is possible to generate a geo(p) random variable using just one u(0, 1)"
622,1,"['discrete random variable', 'random', 'simulation', 'continuous', 'random number', 'discrete', 'distribution', 'random variable', 'variable', 'set', 'event']", Exercises,seg_89,"random variable. if calls to the random number generator take a lot of cpu time, this would lead to faster simulation programs. set λ = − ln(1− p) and let y have a exp(λ) distribution. we obtain z from y by rounding to the nearest integer greater than y . note that z is a discrete random variable, whereas y is a continuous one. show that, nevertheless, the event {z > n} is the same as {y > n}. use this to compute p(z > n) from the distribution of y . what is the distribution of z? (see quick exercise 4.6.)"
623,0,[], Exercises,seg_89,6.11 reconsider the jury example (see section 6.3). suppose the first jury member is bribed to vote in favor of the present candidate.
624,1,['model'], Exercises,seg_89,a. how should you now model y1? describe how you can investigate which
625,0,[], Exercises,seg_89,of the two rules is less sensitive to the effect of the bribery.
626,1,['union'], Exercises,seg_89,b. the international skating union decided to adopt a rule similar to the
627,1,"['average', 'scores']", Exercises,seg_89,"following: randomly discard two of the jury scores, then average the remaining scores. describe how to investigate this rule. do you expect this rule to be more sensitive to the bribery than the two rules already discussed, or less sensitive?"
628,1,['model'], Exercises,seg_89,"6.12 a tiny financial model. to investigate investment strategies, consider the following:"
629,1,"['rate', 'model', 'probability']", Exercises,seg_89,"you can choose to invest your money in one particular stock or put it in a savings account. your initial capital is 1000. the interest rate r is 0.5% per month and does not change. the initial stock price is 100. your stochastic model for the stock price is as follows: next month the price is the same as this month with probability 1/2, with probability 1/4 it is 5% lower, and with probability 1/4 it is 5% higher. this principle applies for every new month. there are no transaction costs when you buy or sell stock."
630,0,[], Exercises,seg_89,"your investment strategy for the next 5 years is: convert all your money to stock when the price drops below 95, and sell all stock and put the money in the bank when the stock price exceeds 110."
631,1,"['model', 'results']", Exercises,seg_89,describe how to simulate the results of this strategy for the model given.
632,0,[], Exercises,seg_89,"6.13 we give you an unfair coin and you do not know p(h) for this coin. can you simulate a fair coin, and how many tosses do you need for each fair coin toss?"
633,1,"['variables', 'information', 'distribution', 'expected value', 'random variable', 'variable', 'expectation', 'mean', 'variance', 'random', 'experiments', 'average']", Expectation and variance,seg_91,"random variables are complicated objects, containing a lot of information on the experiments that are modeled by them. if we want to summarize a random variable by a single number, then this number should undoubtedly be its expected value. the expected value, also called the expectation or mean, gives the center—in the sense of average value—of the distribution of the random variable. if we allow a second number to describe the random variable, then we look at its variance, which is a measure of spread of the distribution of the random variable."
634,1,"['average', 'weighted average', 'probabilities']", Expected values,seg_93,"an oil company needs drill bits in an exploration project. suppose that it is known that (after rounding to the nearest hour) drill bits of the type used in this particular project will last 2, 3, or 4 hours with probabilities 0.1, 0.7, and 0.2. if a drill bit is replaced by one of the same type each time it has worn out, how long could exploration be continued if in total the company would reserve 10 drill bits for the exploration job? what most people would do to answer this question is to take the weighted average"
635,1,"['weighted average', 'distribution', 'random variable', 'variable', 'expected value', 'expectation', 'random', 'average']", Expected values,seg_93,"and conclude that the exploration could continue for 10 × 3.1, or 31 hours. this weighted average is what we call the expected value or expectation of the random variable x whose distribution is given by"
636,1,['case'], Expected values,seg_93,"it might happen that the company is unlucky and that each of the 10 drill bits has worn out after two hours, in which case exploration ends after 20 hours. at the other extreme, they may be lucky and drill for 40 hours on these 10"
637,1,['probability'], Expected values,seg_93,"bits. however, it is a mathematical fact that the conclusion about a 31-hour total drilling time is correct in the following sense: for a large number n of drill bits the total running time will be around n times 3.1 hours with high probability. in the example, where n = 10, we have, for instance, that drilling will continue for 29, 30, 31, 32, or 33 hours with probability more than 0.86, while the probability that it will last only for 20, 21, 22, 23, or 24 hours is less than 0.00006. we will come back to this in chapters 13 and 14. this example illustrates the following definition."
638,1,"['discrete random variable', 'mass function', 'probability mass function', 'discrete', 'random variable', 'variable', 'expectation', 'probability', 'random', 'function']", Expected values,seg_93,"definition. the expectation of a discrete random variable x taking the values a1, a2, . . . and with probability mass function p is the number"
639,1,"['distribution', 'expected value', 'probability distribution', 'expectation', 'mean', 'probability']", Expected values,seg_93,"we also call e[x ] the expected value or mean of x . since the expectation is determined by the probability distribution of x only, we also speak of the expectation or mean of the distribution."
640,1,"['discrete random variable', 'discrete', 'random variable', 'variable', 'expectation', 'probability', 'random']", Expected values,seg_93,"quick exercise 7.1 let x be the discrete random variable that takes the values 1, 2, 4, 8, and 16, each with probability 1/5. compute the expectation of x ."
641,1,"['weighted average', 'random variable', 'variable', 'expectation', 'associated', 'random', 'average']", Expected values,seg_93,"looking at an expectation as a weighted average gives a more physical interpretation of this notion, namely as the center of gravity of weights p(ai) placed at the points ai. for the random variable associated with the drill bit, this is illustrated in figure 7.1."
642,1,"['density function', 'discrete random variable', 'probability density function', 'continuous', 'continuous random variable', 'discrete', 'expected value', 'random variable', 'variable', 'probability', 'random', 'function']", Expected values,seg_93,"this point of view also leads the way to how one should define the expected value of a continuous random variable. let, for example, x be a continuous random variable whose probability density function f is zero outside the interval [0, 1]. it seems reasonable to approximate x by the discrete random variable y , taking the values"
643,1,"['probabilities', 'intervals']", Expected values,seg_93,with as probabilities the masses that x assigns to the intervals [k−
644,1,['probability'], Expected values,seg_93,"we have a good idea of the size of this probability. for large n, it can be approximated well in terms of f :"
645,1,['expectation'], Expected values,seg_93,the “center-of-gravity” interpretation suggests that the expectation e[y ] of y should approximate the expectation e[x ] of x . we have
646,0,['n'], Expected values,seg_93,"by the definition of a definite integral, for large n the right-hand side is close to"
647,0,[], Expected values,seg_93,this motivates the following definition.
648,1,"['density function', 'probability density function', 'continuous', 'continuous random variable', 'random variable', 'variable', 'expectation', 'probability', 'random', 'function']", Expected values,seg_93,definition. the expectation of a continuous random variable x with probability density function f is the number
649,1,"['distribution', 'expected value', 'mean', 'function']", Expected values,seg_93,we also call e[x ] the expected value or mean of x . note that e[x ] is indeed the center of gravity of the mass distribution described by the function f :
650,0,[], Expected values,seg_93,this is illustrated in figure 7.2.
651,1,"['random variable', 'variable', 'expectation', 'random', 'uniformly distributed']", Expected values,seg_93,"quick exercise 7.2 compute the expectation of a random variable u that is uniformly distributed over [2, 5]."
652,1,"['distribution', 'geometric distribution', 'geometric']", Three examples,seg_95,the geometric distribution
653,1,"['geometric distribution', 'distribution', 'random variable', 'variable', 'parameter', 'random', 'geometric']", Three examples,seg_95,"if you buy a lottery ticket every week and you have a chance of 1 in 10 000 of winning the jackpot, what is the expected number of weeks you have to buy tickets before you get the jackpot? the answer is: 10 000 weeks (almost two centuries!). the number of weeks is modeled by a random variable with a geometric distribution with parameter p = 10−4."
654,1,"['geometric distribution', 'distribution', 'expectation', 'parameter', 'geometric']", Three examples,seg_95,the expectation of a geometric distribution. let x have a geometric distribution with parameter p; then
655,1,['probabilistic'], Three examples,seg_95,∞ =1 kxk−1 = 1/(1 − x)2 that has been derived in your calculus course. we will see a simple (probabilistic) way to obtain the value of this sum in chapter 11.
656,1,"['distribution', 'exponential', 'exponential distribution']", Three examples,seg_95,the exponential distribution
657,1,"['distribution', 'residence time', 'mean']", Three examples,seg_95,"in section 5.6 we considered the chemical reactor example, where the residence time t , measured in minutes, has an exp(0.5) distribution. we claimed that this implies that the mean time a particle stays in the vessel is 2 minutes. more generally, we have the following."
658,1,"['exponential distribution', 'distribution', 'expectation', 'exponential', 'parameter']", Three examples,seg_95,the expectation of an exponential distribution. let x have an exponential distribution with parameter λ; then
659,0,[], Three examples,seg_95,the integral has been determined in your calculus course (with the technique of integration by parts).
660,1,"['distribution', 'normal', 'normal distribution']", Three examples,seg_95,the normal distribution
661,1,['normal'], Three examples,seg_95,"here, using that the normal density integrates to 1 and applying the substitution z = (x − µ)/σ,"
662,1,['function'], Three examples,seg_95,"where the integral is 0, because the integrand is an odd function. we obtained the following rule."
663,1,"['distribution', 'random variable', 'variable', 'normal', 'expectation', 'random', 'normal distribution']", Three examples,seg_95,"the expectation of a normal distribution. let x be an n(µ, σ2) distributed random variable. then"
664,1,"['distribution function', 'distribution', 'random variable', 'variable', 'expected value', 'random', 'function', 'uniformly distributed']", The changeofvariable formula,seg_97,"often one does not want to compute the expected value of a random variable x but rather of a function of x , as, for example, x2. we then need to determine the distribution of y = x2, for example by computing the distribution function fy of y (this is an example of the general problem of how distributions change under transformations—this topic is the subject of chapter 8). for a concrete example, suppose an architect wants maximal variety in the sizes of buildings: these should be of the same width and depth x , but x is uniformly distributed between 0 and 10 meters. what is the distribution of the area x2 of a building; in particular, will this distribution be (anything near to) uniform? let us compute fy ; for 0 ≤ a ≤ 100:"
665,1,"['density function', 'probability density function', 'probability', 'function']", The changeofvariable formula,seg_97,"hence the probability density function fy of the area is, for 0 < y < 100 meters squared, given by"
666,1,[], The changeofvariable formula,seg_97,"this means that the buildings with small areas are heavily overrepresented, because fy explodes near 0—see also figure 7.3, in which we plotted fy ."
667,1,"['poisson', 'poisson process', 'locations', 'process']", The changeofvariable formula,seg_97,"surprisingly, this is not very visible in figure 7.4, an example where we should believe our calculations more than our eyes. in the figure the locations of the buildings are generated by a poisson process, the subject of chapter 12. suppose that a contractor has to make an offer on the price of the foundations of the buildings. the amount of concrete he needs will be proportional to the area x2 of a building. so his problem is: what is the expected area of a building? with fy from (7.1) he finds"
668,1,"['average', 'weighted average', 'probability']", The changeofvariable formula,seg_97,"it is interesting to note that we really need to do this calculation, because the expected area is not simply the product of the expected width and the expected depth, which is 25 m2. however, there is a much easier way in which the contractor could have obtained this result. he could have argued that the value of the area is x2 when x is the width, and that he should take the weighted average of those values, where the weight at width x is given by the value fx(x) of the probability density of x . then he would have computed"
669,1,"['functions', 'random variables', 'variables', 'random', 'expected values']", The changeofvariable formula,seg_97,it is indeed a mathematical theorem that this is always a correct way to compute expected values of functions of random variables.
670,1,"['discrete', 'random variable', 'variable', 'random', 'function']", The changeofvariable formula,seg_97,"the change-of-variable formula. let x be a random variable, and let g : r → r be a function. if x is discrete, taking the values a1, a2, . . . , then"
671,1,"['density function', 'probability density function', 'continuous', 'probability', 'function']", The changeofvariable formula,seg_97,"if x is continuous, with probability density function f , then"
672,1,['distribution'], The changeofvariable formula,seg_97,quick exercise 7.3 let x have a ber(p) distribution. compute e[2x].
673,1,"['continuous distribution', 'continuous', 'distribution', 'expectation', 'function']", The changeofvariable formula,seg_97,"an operation that occurs very often in practice is a change of units, e.g., from fahrenheit to celsius. what happens then to the expectation? here we have to apply the formula with the function g(x) = rx + s, where r and s are real numbers. when x has a continuous distribution, the change-of-variable formula yields:"
674,1,"['random variables', 'variables', 'discrete random variables', 'discrete', 'random']", The changeofvariable formula,seg_97,a similar computation with integrals replaced by sums gives the same result for discrete random variables.
675,1,"['deviation', 'information', 'expected value', 'random variable', 'variable', 'mean', 'random']", Variance,seg_99,"suppose you are offered an opportunity for an investment whose expected return is 500. if you are given the extra information that this expected value is the result of a 50% chance of a 450 return and a 50% chance of a 550 return, then you would not hesitate to spend 450 on this investment. however, if the expected return were the result of a 50% chance of a 0 return and a 50% chance of a 1000 return, then most people would be reluctant to spend such an amount. this demonstrates that the spread (around the mean) of a random variable is of great importance. usually this is measured by the expected squared deviation from the mean."
676,1,"['random variable', 'variable', 'random', 'variance']", Variance,seg_99,definition. the variance var(x) of a random variable x is the number var(x) = e[(x − e[x ])2] .
677,1,"['deviation', 'standard', 'variable', 'random variable', 'random', 'standard deviation', 'variance']", Variance,seg_99,"note that the variance of a random variable is always positive (or 0). furthermore, there is the question of existence and finiteness (cf. remark 7.1). in practical situations one often considers the standard deviation defined by √var(x), because it has the same dimension as e[x ]."
678,1,"['distribution', 'normal', 'variance', 'normal distribution']", Variance,seg_99,"as an example, let us compute the variance of a normal distribution. if x has an n(µ, σ2) distribution, then:"
679,0,[], Variance,seg_99,here we substituted z = (x − µ)/σ. using integration by parts one finds that
680,0,[], Variance,seg_99,we have found the following property.
681,1,"['distribution', 'random variable', 'variable', 'normal', 'random', 'normal distribution']", Variance,seg_99,"variance of a normal distribution. let x be an n(µ, σ2) distributed random variable. then"
682,1,"['deviation', 'standard', 'standard deviation', 'variance']", Variance,seg_99,"quick exercise 7.4 let us call the two returns discussed above y1 and y2, respectively. compute the variance and standard deviation of y1 and y2."
683,0,[], Variance,seg_99,"it is often not practical to compute var(x) directly from the definition, but one uses the following rule."
684,1,"['variable', 'variance']", Variance,seg_99,"an alternative expression for the variance. for any random variable x ,"
685,1,"['density function', 'probability density function', 'continuous', 'continuous random variable', 'case', 'discrete', 'random variable', 'variable', 'probability', 'random', 'function']", Variance,seg_99,"to see that this rule holds, we apply the change-of-variable formula. suppose x is a continuous random variable with probability density function f (the discrete case runs completely analogously). using the change-of-variable formula, well-known properties of the integral, and ∫−"
686,1,"['probabilities', 'second moment', 'moment']", Variance,seg_99,"with this rule we make two steps: first we compute e[x ], then we compute e[x2]. the latter is called the second moment of x. let us compare the computations, using the definition and this rule for the drill bit example. recall that for this example x takes the values 2, 3, and 4 with probabilities 0.1, 0.7, and 0.2. we found that e[x ]= 3.1. according to the definition"
687,0,[], Variance,seg_99,using the rule is neater and somewhat faster:
688,1,['variance'], Variance,seg_99,what happens to the variance if we change units? at the end of the previous section we showed that e[rx + s] = re[x ] + s. this can be used to obtain the corresponding rule for the variance under change of units (see also exercise 7.15).
689,1,"['random variable', 'variable', 'random', 'variance']", Variance,seg_99,"expectation and variance under change of units. for any random variable x and any real numbers r and s,"
690,1,['variance'], Variance,seg_99,note that the variance is insensitive to the shift over s. can you understand why this must be true without doing any computations?
691,1,"['density function', 'probability density function', 'probability', 'function']", Solutions to the quick exercises,seg_101,"7.2 the probability density function f of u is given by f(x) = 0 outside [2, 5] and f(x) = 1/3 for 2 ≤ x ≤ 5; hence"
692,1,[], Solutions to the quick exercises,seg_101,7.3 using the change-of-variable formula we obtain
693,1,['distribution'], Solutions to the quick exercises,seg_101,"you could also have noted that y = 2x has a distribution given by p(y = 1) = 1 − p, p(y = 2) = p; hence"
694,1,"['deviation', 'standard deviation', 'standard']", Solutions to the quick exercises,seg_101,so y1 has standard deviation 50 and
695,1,"['deviation', 'standard deviation', 'standard']", Solutions to the quick exercises,seg_101,so y2 has standard deviation 500.
696,1,['outcome'], Exercises,seg_103,7.1 let t be the outcome of a roll with a fair die.
697,1,"['distribution', 'probability distribution', 'probability', 'outcomes']", Exercises,seg_103,"a. describe the probability distribution of t , that is, list the outcomes and"
698,1,['probabilities'], Exercises,seg_103,the corresponding probabilities.
699,1,"['discrete random variable', 'discrete', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random']", Exercises,seg_103,7.2 the probability distribution of a discrete random variable x is given by
700,1,"['distribution', 'probability distribution', 'probability']", Exercises,seg_103,b. give the probability distribution of y = x2 and compute e[y ] using the
701,0,[], Exercises,seg_103,distribution of y .
702,1,[], Exercises,seg_103,c. determine e[x2] using the change-of-variable formula. check your an-
703,0,[], Exercises,seg_103,swer against the answer in b.
704,1,"['random variable', 'variable', 'random']", Exercises,seg_103,"7.3 for a certain random variable x it is known that e[x ] = 2, var(x) = 3. what is e[x2]?"
705,1,"['random variable', 'variable', 'expectation', 'random', 'variance']", Exercises,seg_103,"7.4 let x be a random variable with e[x ] = 2, var(x) = 4. compute the expectation and variance of 3 − 2x ."
706,1,"['distribution', 'expectation', 'variance']", Exercises,seg_103,7.5 determine the expectation and variance of the ber(p) distribution.
707,1,"['density function', 'probability density function', 'random variable', 'variable', 'probability', 'random', 'function']", Exercises,seg_103,7.6 the random variable z has probability density function f(z) = 3z2/19 for 2 ≤ z ≤ 3 and f(z) = 0 elsewhere. determine e[z]. before you do the calculation: will the answer lie closer to 2 than to 3 or the other way around?
708,1,"['density function', 'probability density function', 'random variable', 'variable', 'expectation', 'probability', 'random', 'function', 'variance']", Exercises,seg_103,"7.7 given is a random variable x with probability density function f given by f(x) = 0 for x < 0, and for x > 1, and f(x) = 4x − 4x3 for 0 ≤ x ≤ 1. determine the expectation and variance of the random variable 2x + 3."
709,1,"['continuous', 'distribution function', 'continuous random variable', 'distribution', 'random variable', 'variable', 'random', 'function']", Exercises,seg_103,"7.8 given is a continuous random variable x whose distribution function f satisfies f (x) = 0 for x < 0, f (x) = 1 for x > 1, and f (x) = x(2 − x) for 0 ≤ x ≤ 1. determine e[x ]."
710,1,"['distribution', 'random variable', 'variable', 'random']", Exercises,seg_103,"7.9 let u be a random variable with a u(α, β) distribution."
711,1,['expectation'], Exercises,seg_103,a. determine the expectation of u .
712,1,['variance'], Exercises,seg_103,b. determine the variance of u .
713,1,"['distribution', 'exponential', 'exponential distribution', 'parameter']", Exercises,seg_103,7.10 let x have an exponential distribution with parameter λ.
714,0,[], Exercises,seg_103,a. determine e[x ] and e[x2] using partial integration.
715,1,"['distribution', 'pareto', 'mean', 'pareto distribution']", Exercises,seg_103,7.11 in this exercise we take a look at the mean of a pareto distribution.
716,1,"['distribution', 'expectation']", Exercises,seg_103,a. determine the expectation of a par(2) distribution.
717,1,['expectation'], Exercises,seg_103,b. determine the expectation of a par(1
718,1,['distribution'], Exercises,seg_103,2 ) distribution.
719,1,['distribution'], Exercises,seg_103,c. let x have a par(α) distribution. show that e[x ] = α/(α− 1) if α > 1.
720,1,"['distribution', 'variance']", Exercises,seg_103,7.12 for which α is the variance of a par(α) distribution finite? compute the variance for these α.
721,0,[], Exercises,seg_103,7.13 remember that we found on page 95 that the expected area of a building was 333
722,1,"['random variable', 'variable', 'random']", Exercises,seg_103,"1 m2, whereas the square of the expected width was only 25 m2. this phenomenon is more general: show that for any random variable x one has"
723,1,"['random variable', 'variable', 'random']", Exercises,seg_103,"7.14 suppose we choose arbitrarily a point from the square with corners at (2,1), (3,1), (2,2), and (3,2). the random variable a is the area of the triangle with its corners at (2,1), (3,1), and the chosen point. (see also exercise 5.9 and figure 7.5.) compute e[a]."
724,1,"['random variable', 'variable', 'expectation', 'random']", Exercises,seg_103,7.15 let x be a random variable and r and s any real numbers. use the change-of-units rule e[rx + s] = re[x ] + s for the expectation to obtain a and b.
725,0,[], Exercises,seg_103,c. combine parts a and b to show that
726,1,"['density function', 'probability density function', 'random variable', 'variable', 'expectation', 'probability', 'random', 'function']", Exercises,seg_103,"7.16 the probability density function f of the random variable x used in figure 7.2 is given by f(x) = 0 outside (0, 1) and f(x) = −4x ln(x) for 0 < x < 1. compute the position of the balancing point in the figure, that is, compute the expectation of x ."
727,1,"['discrete random variable', 'probabilities', 'discrete', 'random variable', 'variable', 'random']", Exercises,seg_103,"7.17 let u be a discrete random variable taking the values a1, . . . , ar with probabilities p1, . . . , pr."
728,1,"['random variable', 'variable', 'random']", Exercises,seg_103,"b. suppose that v is a random variable taking the values b1, . . . , br with"
729,1,"['random variables', 'process', 'maxima and minima', 'variances', 'variables', 'transformed', 'inequality', 'random', 'expectations', 'distributions']", Computations with random variables,seg_105,"there are many ways to make new random variables from old ones. of course this is not a goal in itself; usually new variables are created naturally in the process of solving a practical problem. the expectations and variances of such new random variables can be calculated with the change-of-variable formula. however, often one would like to know the distributions of the new random variables. we shall show how to determine these distributions, how to compare expectations of random variables and their transformed versions (jensen’s inequality), and how to determine the distributions of maxima and minima of several random variables."
730,1,"['distribution', 'random variable', 'variable', 'random', 'function']", Transforming discrete random variables,seg_107,"the problem we consider in this section and the next is how the distribution of a random variable x changes if we apply a function g to it, thus obtaining a new random variable y :"
731,1,"['discrete random variable', 'discrete', 'distribution', 'random variable', 'variable', 'probability', 'random']", Transforming discrete random variables,seg_107,"when x is a discrete random variable this is usually not too hard to do: it is just a matter of bookkeeping. we illustrate this with an example. imagine an airline company that sells tickets for a flight with 150 available seats. it has no idea about how many tickets it will sell. suppose, to keep the example simple, that the number x of tickets that will be sold can be anything from 1 to 200. moreover, suppose that each possibility has equal probability to occur, i.e., p(x = j) = 1/200 for j = 1, 2, . . . , 200. the real interest of the airline company is in the random variable y, which is the number of passengers that have to be refused. what is the distribution of y ? to answer this, note that nobody will be refused when the passengers fit in the plane, hence"
732,1,['function'], Transforming discrete random variables,seg_107,"note that in this example the function g is given by g(x) = max{x − 150, 0}."
733,1,"['case', 'distribution', 'probability distribution', 'probability', 'function']", Transforming discrete random variables,seg_107,quick exercise 8.1 let z be the number of passengers that will be in the plane. determine the probability distribution of z. what is the function g in this case?
734,1,"['continuous random variables', 'random variables', 'transformed', 'continuous', 'variables', 'distribution function', 'continuous random variable', 'distribution', 'random variable', 'variable', 'probability', 'random', 'function']", Transforming continuous random variables,seg_109,"we now turn to continuous random variables. since single values occur with probability zero for a continuous random variable, the approach above does not work. the strategy now is to first determine the distribution function of the transformed random variable y = g(x) and then the probability density by differentiating. we shall illustrate this with the following example (actually we saw an example of such a computation in section 7.3 with the function g(x) = x2)."
735,1,"['interval', 'method', 'uniformly distributed', 'measuring', 'limit']", Transforming continuous random variables,seg_109,"we consider two methods that traffic police employ to determine whether you deserve a fine for speeding. from experience, the traffic police think that vehicles are driving at speeds ranging from 60 to 90 km/hour at a certain road section where the speed limit is 80 km/hour. they assume that the speed of the cars is uniformly distributed over this interval. the first method is measuring the speed at a fixed spot in the road section. with this method the police will find that about (90 − 80)/(90 − 60) = 1/3 of the cars will be fined."
736,1,"['distribution', 'uniform distribution', 'model', 'method']", Transforming continuous random variables,seg_109,"for the second method, cameras are put at the beginning and end of a 1-km road section, and a driver is fined if he spends less than a certain amount of time in the road section. cars driving at 60 km/hour need one minute, those driving at 90 km/hour only 40 seconds. let us therefore model the time t an arbitrary car spends in the section by a uniform distribution over (40, 60) seconds. what is the speed v we deduce from this travelling time? note that"
737,0,[], Transforming continuous random variables,seg_109,since there are 3600 seconds in an hour we have that
738,1,"['distribution', 'function', 'distribution function']", Transforming continuous random variables,seg_109,we therefore find for the distribution function fv (v) = p(v ≤ v) of the speed v that
739,1,['probability'], Transforming continuous random variables,seg_109,for all speeds v between 60 and 90. we can now obtain the probability density fv of v by differentiating:
740,1,['model'], Transforming continuous random variables,seg_109,it is amusing to note that with the second model the traffic police write fewer speeding tickets because
741,1,"['model', 'mean', 'probability', 'inequality']", Transforming continuous random variables,seg_109,"(with the first model we found probability 1/3 that a car drove faster than 80 km/hour.) this is related to a famous result in road traffic research, which is succinctly phrased as: “space mean speed < time mean speed” (see [37]). it is also related to jensen’s inequality, which we introduce in section 8.3."
742,1,"['continuous distribution', 'distribution', 'continuous', 'outcome']", Transforming continuous random variables,seg_109,"similar to the way this is done in the traffic example, one can determine the distribution of y = 1/x for any x with a continuous distribution. the outcome will be that if x has density fx , then the density fy of y is given"
743,1,"['distribution', 'continuous distribution', 'probability', 'continuous']", Transforming continuous random variables,seg_109,quick exercise 8.2 let x have a continuous distribution with probability density fx(x) = 1/[π(1 + x2)]. what is the distribution of y = 1/x?
744,1,['transformation'], Transforming continuous random variables,seg_109,"we turn to a second example. a very common transformation is a change of units, for instance, from celsius to fahrenheit. if x is temperature expressed in degrees celsius, then y = 9"
745,1,"['distribution', 'functions']", Transforming continuous random variables,seg_109,5x+32 is the temperature in degrees fahrenheit. let fx and fy be the distribution functions of x and y . then we have for any a
746,1,['probability'], Transforming continuous random variables,seg_109,"by differentiating fy (using the chain rule), we obtain the probability density fy (y) = 5"
747,0,[], Transforming continuous random variables,seg_109,"9fx( 5 9 (y − 32)). we can do this for more general changes of units, and we obtain the following useful rule."
748,1,"['density function', 'probability density function', 'continuous', 'distribution function', 'distribution', 'variable', 'probability', 'function', 'transformation']", Transforming continuous random variables,seg_109,"change-of-units transformation. let x be a continuous random variable with distribution function fx and probability density function fx . if we change units to y = rx+s for real numbers r > 0 and s, then"
749,1,"['distribution', 'random variable', 'variable', 'random']", Transforming continuous random variables,seg_109,"as an example, let x be a random variable with an n(µ, σ2) distribution, and let y = rx + s. then this rule gives us"
750,1,"['parameters', 'distribution', 'normal', 'probability', 'normal distribution']", Transforming continuous random variables,seg_109,for −∞ < y < ∞. on the right-hand side we recognize the probability density of a normal distribution with parameters rµ+ s and r2σ2. this illustrates the following rule.
751,1,"['random variables', 'variables', 'distribution', 'random variable', 'variable', 'random']", Transforming continuous random variables,seg_109,"normal random variables under change of units. let x be a random variable with an n(µ, σ2) distribution. for any r = 0 and any s, the random variable rx + s has an n(rµ + s, r2σ2) distribution."
752,1,['distribution'], Transforming continuous random variables,seg_109,"note that if x has an n(µ, σ2) distribution, then with r = 1/σ and s = −µ/σ we conclude that"
753,1,['distribution'], Transforming continuous random variables,seg_109,"has an n(0, 1) distribution. as a consequence"
754,1,"['random variable', 'variable', 'probability', 'random']", Transforming continuous random variables,seg_109,"so any probability for an n(µ, σ2) distributed random variable x can be expressed in terms of an n(0, 1) distributed random variable z."
755,1,"['distribution', 'probabilities']", Transforming continuous random variables,seg_109,"quick exercise 8.3 compute the probabilities p(x ≤ 5) and p(x ≥ 2) for x with an n(4, 25) distribution."
756,1,"['distribution', 'transformation']", Jensens inequality,seg_111,without actually computing the distribution of g(x) we can often tell how e[g(x)] relates to g(e[x ]). for the change-of-units transformation g(x) = rx + s we know that e[g(x)] = g(e[x ]) (see section 7.3). it is a common
757,1,"['functions', 'nonlinear']", Jensens inequality,seg_111,"error to equate these two sides for other functions g. in fact, equality will very rarely occur for nonlinear g."
758,1,['average'], Jensens inequality,seg_111,"for example, suppose that a company that produces microelectronic parts has a target production of 240 chips per day, but the yield has only been 40, 60, and 80 chips on three consecutive days. the average production over the three days then is 60 chips, so on average the production should have been 4 times higher to reach the target. however, one can also look at this in the following way: on the three days the production should have been 240/40 = 6, 240/60 = 4, and 240/80 = 3 times higher. on average that is"
759,1,"['outcomes', 'probabilities', 'random variable', 'variable', 'random']", Jensens inequality,seg_111,"times higher! what happens here can be explained (take for x the part of the target production that is realized, where you give equal probabilities to the three outcomes 1/6, 1/4, and 1/3) by the fact that if x is a random variable taking positive values, then always"
760,1,"['functions', 'case', 'convex functions', 'random', 'inequality']", Jensens inequality,seg_111,"unless var(x) = 0, which only happens if x is not random at all (cf. exercise 7.17). this inequality is the case g(x) = 1/x on (0,∞) of the following result that holds for general convex functions g."
761,1,"['random variable', 'variable', 'random', 'function', 'convex function', 'inequality']", Jensens inequality,seg_111,"jensen’s inequality. let g be a convex function, and let x be a random variable. then"
762,1,"['interval', 'random', 'function', 'inequality']", Jensens inequality,seg_111,"recall from calculus that a twice differentiable function g is convex on an interval i if g′′(x) ≥ 0 for all x in i, and strictly convex if g′′(x) > 0 for all x in i. when x takes its values in an interval i (this can, for instance, be i = (−∞,∞)), and g is strictly convex on i, then strict inequality holds: g(e[x ]) < e[g(x)], unless x is not random."
763,1,"['case', 'random variable', 'variable', 'probability', 'random']", Jensens inequality,seg_111,"in figure 8.1 we illustrate the way in which this result can be obtained for the special case of a random variable x that takes two values, a and b. in the figure, x takes these two values with probability 3/4 and 1/4 respectively. convexity of g forces any line segment connecting two points on the graph of g to lie above the part of the graph between these two points. so if we choose the line segment from (a, g(a)) to (b, g(b)), then it follows that the point"
764,1,['function'], Jensens inequality,seg_111,"a simple example is given by g(x) = x2. this function is convex (g′′(x) = 2 for all x), and hence"
765,0,[], Jensens inequality,seg_111,"note that this is exactly the same as saying that var(x) ≥ 0, which we have already seen in section 7.4."
766,1,"['random variable', 'variable', 'random']", Jensens inequality,seg_111,quick exercise 8.4 let x be a random variable with var(x) > 0. which is true: e[e−x] < e−e[x] or e[e−x] > e−e[x]?
767,1,"['contamination', 'random variables', 'variables', 'location', 'distribution', 'probability distribution', 'variable', 'probability', 'random', 'level']", Extremes,seg_113,"in many situations the maximum (or minimum) of a sequence x1, x2, . . . , xn of random variables is the variable of interest. for instance, let x1, x2, . . . , x365 be the water level of a river during the days of a particular year for a particular location. suppose there will be flooding if the level exceeds a certain height—usually the height of the dykes. the question whether flooding occurs during a year is completely answered by looking at the maximum of x1, x2, . . . , x365. if one wants to predict occurrence of flooding in the future, the probability distribution of this maximum is of great interest. similar models arise, for instance, when one is interested in possible damage from a series of shocks or in the extent of a contamination plume in the subsurface."
768,1,"['distribution', 'random variable', 'variable', 'random']", Extremes,seg_113,we want to find the distribution of the random variable
769,1,"['distribution', 'function', 'distribution function']", Extremes,seg_113,we can determine the distribution function of z by realizing that the maximum of the xi is smaller than a number a if and only if all xi are smaller than a:
770,1,"['random variables', 'random', 'independent', 'variables', 'case', 'random variables independent', 'independence', 'events']", Extremes,seg_113,"now suppose that the events {xi ≤ ai} are independent for every choice of the ai. in this case we call the random variables independent (see also chapter 9, where we study independence of random variables). in particular, the events {xi ≤ a} are independent for all a. it then follows that"
771,1,"['random variables', 'variables', 'distribution function', 'distribution', 'random', 'function']", Extremes,seg_113,"hence, if all random variables have the same distribution function f , then the following result holds."
772,1,"['random variables', 'independent', 'variables', 'distribution function', 'distribution', 'independent random variables', 'random', 'function']", Extremes,seg_113,"the distribution of the maximum. let x1, x2, . . . , xn be n independent random variables with the same distribution function f , and let z = max{x1, x2, . . . , xn}. then"
773,1,"['density function', 'random variables', 'probability density function', 'independent', 'variables', 'distribution', 'independent random variables', 'probability', 'random', 'function']", Extremes,seg_113,"quick exercise 8.5 let x1, x2, . . . , xn be independent random variables, all with a u(0, 1) distribution. let z = max{x1, . . . , xn}. compute the distribution function and the probability density function of z."
774,1,['distribution'], Extremes,seg_113,what can we say about the distribution of the minimum? let
775,1,"['distribution function', 'distribution', 'complement', 'event', 'function']", Extremes,seg_113,we can now find the distribution function fv of v by observing that the minimum of the xi is larger than a number a if and only if all xi are larger than a. the trick is to switch to the complement of the event {v ≤ a}:
776,1,['independence'], Extremes,seg_113,"so using independence and switching back again, we obtain"
777,0,[], Extremes,seg_113,we have found the following result for the minimum.
778,1,"['random variables', 'independent', 'variables', 'distribution function', 'distribution', 'independent random variables', 'random', 'function']", Extremes,seg_113,"the distribution of the minimum. let x1, x2, . . . , xn be n independent random variables with the same distribution function f , and let v = min{x1, x2, . . . , xn}. then"
779,1,"['density function', 'random variables', 'probability density function', 'independent', 'variables', 'distribution', 'independent random variables', 'probability', 'random', 'function']", Extremes,seg_113,"quick exercise 8.6 let x1, x2, . . . , xn be independent random variables, all with a u(0, 1) distribution. let v = min{x1, . . . , xn}. compute the distribution function and the probability density function of v ."
780,1,['probability'], Solutions to the quick exercises,seg_115,8.2 the probability density of y = 1/x is
781,1,"['distribution', 'cauchy', 'cauchy distribution', 'standard']", Solutions to the quick exercises,seg_115,"we see that 1/x has the same distribution as x ! (this distribution is called the standard cauchy distribution, it will be introduced in chapter 11.)"
782,1,"['distribution', 'table']", Solutions to the quick exercises,seg_115,"8.3 first define z = (x−4)/5, which has an n(0, 1) distribution. then from table b.1"
783,1,"['distribution', 'symmetry', 'normal', 'normal distribution']", Solutions to the quick exercises,seg_115,"similarly, using the symmetry of the normal distribution,"
784,1,['inequality'], Solutions to the quick exercises,seg_115,"8.4 if g(x) = e−x, then g′′(x) = e−x > 0; hence g is strictly convex. it follows from jensen’s inequality that"
785,1,['inequality'], Solutions to the quick exercises,seg_115,"moreover, if var(x) > 0, then the inequality is strict."
786,1,"['density function', 'probability density function', 'distribution function', 'distribution', 'probability', 'function']", Solutions to the quick exercises,seg_115,"8.5 the distribution function of the xi is given by f (x) = x on [0, 1]. therefore the distribution function fz of the maximum z is equal to fz(a) = (f (a))n = an. its probability density function is"
787,1,"['density function', 'probability density function', 'distribution function', 'distribution', 'probability', 'function']", Solutions to the quick exercises,seg_115,"8.6 the distribution function of the xi is given by f (x) = x on [0, 1]. therefore the distribution function fv of the minimum v is equal to fv (a) = 1 − (1 − a)n. its probability density function is"
788,1,"['deviation', 'probabilities', 'distribution', 'random variable', 'variable', 'mean', 'probability', 'random']", Exercises,seg_117,"8.1 often one is interested in the distribution of the deviation of a random variable x from its mean µ = e[x ]. let x take the values 80, 90, 100, 110, and 120, all with probability 0.2; then e[x ] = µ = 100. determine the distribution of y = |x − µ|. that is, specify the values y can take and give the corresponding probabilities."
789,1,"['distribution', 'uniform distribution']", Exercises,seg_117,"8.2 suppose x has a uniform distribution over the points {1, 2, 3, 4, 5, 6} and that g(x) = sin(π"
790,1,['distribution'], Exercises,seg_117,a. determine the distribution of y = g(x) = sin(π
791,1,['probabilities'], Exercises,seg_117,"2 x), that is, specify the values y can take and give the corresponding probabilities."
792,1,['distribution'], Exercises,seg_117,2 x). determine the distribution of z.
793,1,['distribution'], Exercises,seg_117,c. determine the distribution of w = y 2 + z2. warning: in this example
794,1,"['random variables', 'variables', 'distribution', 'random variable', 'variable', 'random', 'function']", Exercises,seg_117,"there is a very special dependency between y and z, and in general it is much harder to determine the distribution of a random variable that is a function of two other random variables. this is the subject of chapter 11."
795,1,"['continuous random variable', 'random variable', 'variable', 'random', 'continuous', 'uniformly distributed']", Exercises,seg_117,"8.3 the continuous random variable u is uniformly distributed over [0, 1]."
796,1,"['distribution', 'function', 'distribution function']", Exercises,seg_117,a. determine the distribution function of v = 2u + 7. what kind of distri-
797,0,[], Exercises,seg_117,bution does v have?
798,1,"['distribution', 'function', 'distribution function']", Exercises,seg_117,b. determine the distribution function of v = ru + s for all real numbers
799,0,[], Exercises,seg_117,r > 0 and s. see exercise 8.9 for what happens for negative r.
800,1,"['exponential distributions', 'exponential', 'transforming', 'distributions']", Exercises,seg_117,8.4 transforming exponential distributions.
801,1,"['distribution', 'function', 'distribution function']", Exercises,seg_117,2 ) distribution. determine the distribution function of 1 2x . what kind of distribution does 1 2x have?
802,1,"['distribution', 'function', 'distribution function']", Exercises,seg_117,b. let x have an exp(λ) distribution. determine the distribution function
803,1,['distribution'], Exercises,seg_117,of λx . what kind of distribution does λx have?
804,1,"['continuous random variable', 'random variable', 'variable', 'probability', 'random', 'continuous']", Exercises,seg_117,8.5 let x be a continuous random variable with probability density function
805,1,"['distribution', 'function', 'distribution function']", Exercises,seg_117,a. determine the distribution function fx .
806,1,"['distribution', 'function', 'distribution function']", Exercises,seg_117,b. let y = √x . determine the distribution function fy .
807,1,['probability'], Exercises,seg_117,c. determine the probability density of y .
808,1,"['continuous random variable', 'random variable', 'variable', 'probability', 'random', 'continuous']", Exercises,seg_117,8.6 let x be a continuous random variable with probability density fx that takes only positive values and let y = 1/x .
809,1,['probability'], Exercises,seg_117,"b. let z = 1/y . using a, determine the probability density fz of z, in terms"
810,1,"['distribution', 'function', 'distribution function']", Exercises,seg_117,8.7 let x have a par(α) distribution. determine the distribution function of ln x . what kind of a distribution does lnx have?
811,1,"['distribution function', 'distribution', 'random variable', 'variable', 'random', 'function']", Exercises,seg_117,"8.8 let x have an exp(1) distribution, and let α and λ be positive numbers. determine the distribution function of the random variable"
812,1,"['parameters', 'distribution', 'random variable', 'variable', 'random', 'weibull distribution', 'weibull']", Exercises,seg_117,the distribution of the random variable w is called the weibull distribution with parameters α and λ.
813,1,"['continuous', 'distribution function', 'continuous random variable', 'distribution', 'random variable', 'variable', 'probability', 'random', 'function']", Exercises,seg_117,8.9 let x be a continuous random variable. express the distribution function and probability density of the random variable y = −x in terms of those of x .
814,1,"['random variables', 'probabilities', 'table', 'variables', 'random variable', 'variable', 'normal', 'random']", Exercises,seg_117,"8.10 let x be an n(3, 4) distributed random variable. use the rule for normal random variables under change of units and table b.1 to determine the probabilities p(x ≥ 3) and p(x ≤ 1)."
815,1,"['functions', 'concave functions', 'random variable', 'variable', 'concave function', 'random', 'function']", Exercises,seg_117,"8.11 let x be a random variable, and let g be a twice differentiable function with g′′(x) ≤ 0 for all x. such a function is called a concave function. show that for concave functions always"
816,1,"['random variable', 'variable', 'probability', 'random']", Exercises,seg_117,8.12 let x be a random variable with the following probability mass function:
817,1,['distribution'], Exercises,seg_117,a. determine the distribution of y = √x.
818,1,['function'], Exercises,seg_117,"hint: use exercise 8.11, or start by showing that the function g(x) = −√x is convex."
819,0,[], Exercises,seg_117,√x] to check your answer (and to see that it
820,0,[], Exercises,seg_117,makes a big difference!).
821,1,['distribution'], Exercises,seg_117,"8.13 let w have a u(π, 2π) distribution. what is larger: e[sin(w )] or sin(e[w ])? check your answer by computing these two numbers."
822,1,"['function', 'inequality']", Exercises,seg_117,"8.14 in this exercise we take a look at jensen’s inequality for the function g(x) = x3 (which is neither convex nor concave on (−∞,∞))."
823,1,"['discrete', 'random variable', 'variable', 'random']", Exercises,seg_117,a. can you find a (discrete) random variable x with var(x) > 0 such that
824,1,"['random variable', 'variable', 'random', 'inequality']", Exercises,seg_117,b. under what kind of conditions on a random variable x will the inequality
825,1,"['random variables', 'independent', 'variables', 'distribution', 'independent random variables', 'random']", Exercises,seg_117,"8.15 let x1, x2, . . . , xn be independent random variables, all with a u(0, 1) distribution. let z = max{x1, . . . , xn} and v = min{x1, . . . , xn}."
826,1,['symmetry'], Exercises,seg_117,c. can you argue directly (using the symmetry of the uniform distribu-
827,1,['inequality'], Exercises,seg_117,8.16 in this exercise we derive a kind of jensen inequality for the minimum.
828,0,[], Exercises,seg_117,a. let a and b be real numbers. show that
829,1,"['random variables', 'independent', 'variables', 'distribution', 'independent random variables', 'random']", Exercises,seg_117,b. let x and y be independent random variables with the same distribution
830,1,['expectation'], Exercises,seg_117,and finite expectation. deduce from a that
831,1,"['distribution', 'inequality']", Exercises,seg_117,"remark: this is not so interesting, since min{e[x ] , e[y ]} = e[x ] = e[y ], but we will see in the exercises of chapter 11 that this inequality is also true for x and y, which do not have the same distribution."
832,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random', 'function']", Exercises,seg_117,"8.17 let x1, . . . , xn be n independent random variables with the same distribution function f ."
833,0,[], Exercises,seg_117,"a. convince yourself that for any numbers x1, . . . , xn it is true that"
834,1,['observation'], Exercises,seg_117,cise 8.9 and the observation in a to deduce the formula
835,0,[], Exercises,seg_117,directly from the formula
836,1,"['random variables', 'independent', 'variables', 'distribution function', 'distribution', 'independent random variables', 'random', 'function']", Exercises,seg_117,"8.18 let x1, x2, . . . , xn be independent random variables, all with an exp(λ) distribution. let v = min{x1, . . . , xn}. determine the distribution function of v . what kind of distribution is this?"
837,0,['n'], Exercises,seg_117,"8.19 from the “north pole” n of a circle with diameter 1, a point q on the circle is mapped to a point t on the line by its projection from n , as illustrated in figure 8.2."
838,1,['interval'], Exercises,seg_117,suppose that the point q is uniformly chosen on the circle. this is the same as saying that the angle ϕ is uniformly chosen from the interval [−π
839,1,"['uniformly distributed', 'interval']", Exercises,seg_117,"2 , π 2 ] (can you see this?). let x be this angle, so that x is uniformly distributed over the interval [−π"
840,1,"['random', 'distribution', 'random variable', 'variable', 'event']", Exercises,seg_117,"2 , π 2 ]. this means that p(x ≤ ϕ) = 1/2 + ϕ/π (cf. quick exercise 5.3). what will be the distribution of the projection of q on the line? let us call this random variable z. then it is clear that the event {z ≤ t} is equal to the event {x ≤ ϕ}, where t and ϕ correspond to each other under the projection. this means that tan(ϕ) = t, which is the same as saying that arctan(t) = ϕ."
841,1,['interval'], Exercises,seg_117,"a. what part of the circle is mapped to the interval [1,∞)?"
842,1,"['distribution', 'function', 'distribution function']", Exercises,seg_117,b. compute the distribution function of z using the correspondence between
843,1,"['density function', 'probability density function', 'probability', 'function']", Exercises,seg_117,c. compute the probability density function of z.
844,1,"['distribution', 'cauchy', 'cauchy distribution']", Exercises,seg_117,the distribution of z is called the cauchy distribution (which will be discussed in chapter 11).
845,1,"['continuous random variables', 'random variables', 'experiment', 'variables', 'discrete', 'distribution', 'joint', 'independence', 'random', 'continuous']", Joint distributions and independence,seg_119,"random variables related to the same experiment often influence one another. in order to capture this, we introduce the joint distribution of two or more random variables. we also discuss the notion of independence for random variables, which models the situation where random variables do not influence each other. as with single random variables we treat these topics for discrete and continuous random variables separately."
846,1,"['variables', 'information', 'jointly']", Joint distributions of discrete random variables,seg_121,"in a census one is usually interested in several variables, such as income, age, and gender. in itself these variables are interesting, but when two (or more) are studied simultaneously, detailed information is obtained on the society where the census is performed. for instance, studying income, age, and gender jointly might give insight to the emancipation of women."
847,1,"['random variables', 'independent', 'variables', 'discrete random variables', 'discrete', 'joint', 'random', 'joint distributions', 'distributions']", Joint distributions of discrete random variables,seg_121,"without mentioning it explicitly, we already encountered several examples of joint distributions of discrete random variables. for example, in chapter 4 we defined two random variables s and m , the sum and the maximum of two independent throws of a die."
848,1,"['probability', 'event']", Joint distributions of discrete random variables,seg_121,"quick exercise 9.1 list the elements of the event {s = 7, m = 4} and compute its probability."
849,1,"['sample', 'random variables', 'probabilities', 'variables', 'discrete random variables', 'discrete', 'distribution', 'joint', 'random', 'sample space']", Joint distributions of discrete random variables,seg_121,"in general, the joint distribution of two discrete random variables x and y , defined on the same sample space ω, is given by prescribing the probabilities of all possible values of the pair (x, y )."
850,1,"['random variables', 'mass function', 'variables', 'probability mass function', 'joint probability mass function', 'discrete random variables', 'discrete', 'joint', 'probability', 'random', 'function', 'joint probability']", Joint distributions of discrete random variables,seg_121,"definition. the joint probability mass function p of two discrete random variables x and y is the function p : r2 → [0, 1], defined by"
851,1,['dependence'], Joint distributions of discrete random variables,seg_121,"to stress the dependence on (x, y ), we sometimes write px,y instead of p."
852,1,"['random variables', 'table', 'variables', 'distribution', 'joint', 'random']", Joint distributions of discrete random variables,seg_121,"if x and y take on the values a1, a2, . . . , ak and b1, b2, . . . , b , respectively, the joint distribution of x and y can simply be described by listing all the possible values of p(ai, bj). for example, for the random variables s and m from chapter 4 we obtain table 9.1."
853,1,"['distribution', 'table']", Joint distributions of discrete random variables,seg_121,"from this table we can retrieve the distribution of s and of m . for example, because"
854,1,['events'], Joint distributions of discrete random variables,seg_121,and because the six events
855,1,['mutually exclusive'], Joint distributions of discrete random variables,seg_121,"are mutually exclusive, we find that"
856,1,"['joint probabilities', 'probabilities', 'table', 'marginal', 'distribution', 'probability distribution', 'joint', 'probability', 'marginal distribution']", Joint distributions of discrete random variables,seg_121,"thus we see that the probabilities of s can be obtained by taking the sum of the joint probabilities in the rows of table 9.1. this yields the probability distribution of s, i.e., all values of ps(a) for a = 2, . . . , 12. we speak of the marginal distribution of s. in table 9.2 we have added this distribution in the right “margin” of the table. similarly, summing over the columns of table 9.1 yields the marginal distribution of m , in the bottom margin of table 9.2."
857,1,"['marginal', 'probability mass function', 'probability', 'random', 'function', 'marginal probability', 'mass function', 'information', 'cases', 'joint probability mass function', 'distributions', 'functions', 'distribution', 'random variables', 'variables', 'marginal distributions', 'joint', 'joint probability']", Joint distributions of discrete random variables,seg_121,the joint distribution of two random variables contains a lot more information than the two marginal distributions. this can be illustrated by the fact that in many cases the joint probability mass function of x and y cannot be retrieved from the marginal probability mass functions px and py . a simple example is given in the following quick exercise.
858,1,"['random variables', 'table', 'mass function', 'variables', 'probability mass function', 'joint probability mass function', 'discrete random variables', 'discrete', 'joint', 'probability', 'random', 'function', 'joint probability']", Joint distributions of discrete random variables,seg_121,"quick exercise 9.2 let x and y be two discrete random variables, with joint probability mass function p, given by the following table, where ε is an arbitrary number between −1/4 and 1/4."
859,1,['table'], Joint distributions of discrete random variables,seg_121,"complete the table, and conclude that we cannot retrieve p from px and py ."
860,1,"['distribution', 'function', 'joint', 'distribution function']", Joint distributions of discrete random variables,seg_121,the joint distribution function
861,1,"['continuous random variables', 'random variables', 'continuous', 'variables', 'distribution function', 'case', 'discrete', 'distribution', 'random variable', 'variable', 'random', 'function']", Joint distributions of discrete random variables,seg_121,"as in the case of a single random variable, the distribution function enables us to treat pairs of discrete and pairs of continuous random variables in the same way."
862,1,"['distribution function', 'distribution', 'joint', 'random', 'function']", Joint distributions of discrete random variables,seg_121,"definition. the joint distribution function f of two random variables x and y is the function f : r2 → [0, 1] defined by"
863,1,"['distribution function', 'distribution', 'joint', 'function']", Joint distributions of discrete random variables,seg_121,"quick exercise 9.3 compute f (5, 3) for the joint distribution function f of the pair (s, m)."
864,1,"['functions', 'marginal', 'distribution function', 'distribution', 'joint', 'function', 'marginal distribution']", Joint distributions of discrete random variables,seg_121,"the distribution functions fx and fy can be obtained from the joint distribution function of x and y . as before, we speak of the marginal distribution functions. the following rule holds."
865,1,"['random variables', 'variables', 'marginal', 'distribution function', 'distribution', 'joint', 'random', 'function', 'marginal distribution']", Joint distributions of discrete random variables,seg_121,from joint to marginal distribution function. let f be the joint distribution function of random variables x and y . then the marginal distribution function of x is given for each a by
866,1,"['marginal', 'distribution function', 'distribution', 'function', 'marginal distribution']", Joint distributions of discrete random variables,seg_121,and the marginal distribution function of y is given for each b by
867,1,"['interval', 'probability', 'random', 'function', 'random variable', 'density function', 'continuous random variables', 'probability density function', 'joint probability density function', 'distribution', 'continuous', 'continuous random variable', 'random variables', 'variables', 'variable', 'joint', 'joint probability']", Joint distributions of continuous random variables,seg_123,"we saw in chapter 5 that the probability that a single continuous random variable x lies in an interval [a, b], is equal to the area under the probability density function f of x over the interval (see also figure 5.1). for the joint distribution of continuous random variables x and y the situation is analogous: the probability that the pair (x, y ) falls in the rectangle [a1, b1]×[a2, b2] is equal to the volume under the joint probability density function f(x, y) of (x, y ) over the rectangle. this is illustrated in figure 9.1, where a chunk of a joint probability density function f(x, y) is displayed for x between −0.5 and 1 and for y between −1.5 and 1. its volume represents the probability p(−0.5 ≤ x ≤ 1,−1.5 ≤ y ≤ 1). as the volume under f on [−0.5, 1]×[−1.5, 1] is equal to the integral of f over this rectangle, this motivates the following definition."
868,1,"['continuous distribution', 'random variables', 'continuous', 'variables', 'distribution', 'joint', 'random', 'function']", Joint distributions of continuous random variables,seg_123,"definition. random variables x and y have a joint continuous distribution if for some function f : r2 → r and for all numbers a1, a2 and b1, b2 with a1 ≤ b1 and a2 ≤ b2,"
869,1,['function'], Joint distributions of continuous random variables,seg_123,"the function f has to satisfy f(x, y) ≥ 0 for all x and y, and"
870,1,"['density function', 'probability density function', 'joint probability density function', 'joint', 'probability', 'function', 'joint probability']", Joint distributions of continuous random variables,seg_123,"∞ ∞ f(x, y) dxdy = 1. we call f the joint probability density function of x and y ."
871,1,"['density function', 'probability density function', 'joint probability density function', 'distribution function', 'case', 'distribution', 'joint', 'probability', 'function', 'joint probability']", Joint distributions of continuous random variables,seg_123,as in the one-dimensional case there is a simple relation between the joint distribution function f and the joint probability density function f :
872,1,"['density function', 'random variables', 'probability density function', 'joint probability density function', 'variables', 'joint', 'probability', 'random', 'function', 'bivariate', 'joint probability']", Joint distributions of continuous random variables,seg_123,a joint probability density function of two random variables is also called a bivariate probability density. an explicit example of such a density is the
873,1,"['normal distributions', 'normal', 'bivariate', 'distributions']", Joint distributions of continuous random variables,seg_123,"f(x, y) = e π for −∞ < x < ∞ and −∞ < y < ∞; see figure 9.2. this is an example of a bivariate normal density (see remark 11.2 for a full description of bivariate normal distributions)."
874,1,"['continuous distributions', 'joint', 'probability', 'joint probability', 'continuous', 'distributions']", Joint distributions of continuous random variables,seg_123,we illustrate a number of properties of joint continuous distributions by means of the following simple example. suppose that x and y have joint probability
875,1,['function'], Joint distributions of continuous random variables,seg_123,density function
876,1,"['joint', 'joint probabilities', 'probabilities']", Joint distributions of continuous random variables,seg_123,as an illustration of how to compute joint probabilities:
877,1,"['distribution function', 'distribution', 'joint', 'function']", Joint distributions of continuous random variables,seg_123,"next, for a between 0 and 3 and b between 1 and 2, we determine the expression of the joint distribution function. since f(x, y) = 0 for x < 0 or y < 1,"
878,1,"['marginal', 'distribution function', 'distribution', 'function', 'marginal distribution']", Joint distributions of continuous random variables,seg_123,"hence, applying (9.1) one finds the marginal distribution function of x :"
879,1,['probability'], Joint distributions of continuous random variables,seg_123,the probability density of x can be found by differentiating fx :
880,1,"['density function', 'joint probabilities', 'random variables', 'probability density function', 'probabilities', 'variables', 'discrete random variables', 'marginal', 'table', 'discrete', 'joint', 'probability', 'random', 'function']", Joint distributions of continuous random variables,seg_123,"for x between 0 and 3. it is also possible to obtain the probability density function of x directly from f(x, y). recall that we determined marginal probabilities of discrete random variables by summing over the joint probabilities (see table 9.2). in a similar way we can find fx . for x between 0 and 3,"
881,0,[], Joint distributions of continuous random variables,seg_123,this illustrates the following rule.
882,1,"['density function', 'densities', 'random variables', 'probability density function', 'joint probability density function', 'variables', 'marginal', 'marginal probability density', 'joint', 'probability', 'random', 'function', 'marginal probability', 'joint probability']", Joint distributions of continuous random variables,seg_123,from joint to marginal probability density function. let f be the joint probability density function of random variables x and y . then the marginal probability densities of x and y can be found as follows:
883,1,"['density function', 'random variables', 'probability density function', 'variables', 'variable', 'probability', 'random', 'function']", Joint distributions of continuous random variables,seg_123,hence the probability density function of each of the random variables x and y can easily be obtained by “integrating out” the other variable.
884,0,[], Joint distributions of continuous random variables,seg_123,quick exercise 9.5 determine fy (y).
885,1,"['sample', 'random variables', 'variables', 'distribution function', 'distribution', 'joint', 'probability', 'random', 'function', 'sample space']", More than two random variables,seg_125,"to determine the joint distribution of n random variables x1, x2, . . . , xn, all defined on the same sample space ω, we have to describe how the probability mass is distributed over all possible values of (x1, x2, . . . , xn). in fact, it suffices to specify the joint distribution function f of x1, x2, . . . , xn, which is defined by"
886,1,"['random variables', 'mass function', 'variables', 'probability mass function', 'joint probability mass function', 'case', 'discrete', 'distribution', 'joint', 'probability', 'random', 'function', 'joint probability']", More than two random variables,seg_125,"in case the random variables x1, x2, . . . , xn are discrete, the joint distribution can also be characterized by specifying the joint probability mass function p of x1, x2, . . . , xn, defined by"
887,1,"['without replacement', 'replacement']", More than two random variables,seg_125,drawing without replacement
888,1,"['probabilities', 'mass function', 'probability mass function', 'joint probability mass function', 'case', 'joint', 'probability', 'function', 'joint probability']", More than two random variables,seg_125,"let us illustrate the use of the joint probability mass function with an example. in the weekly dutch national lottery show, 6 balls are drawn from a vase that contains balls numbered from 1 to 41. clearly, the first number takes values 1, 2, . . . , 41 with equal probabilities. is this also the case for—say—the third ball?"
889,1,"['mass function', 'marginal', 'probability mass function', 'joint probability mass function', 'without replacement', 'replacement', 'joint', 'probability', 'combinations', 'function', 'marginal probability', 'joint probability']", More than two random variables,seg_125,"let us consider a more general situation. suppose a vase contains balls numbered 1, 2, . . . , n . we draw n balls without replacement from the vase. note that n cannot be larger than n . each ball is selected with equal probability, i.e., in the first draw each ball has probability 1/n , in the second draw each of the n −1 remaining balls has probability 1/(n −1), and so on. let xi denote the number on the ball in the i-th draw, for i = 1, 2, . . . , n. in order to obtain the marginal probability mass function of xi, we first compute the joint probability mass function of x1, x2, . . . , xn. since there are n(n−1) · · · (n−n+1) possible combinations for the values of x1, x2, . . . , xn, each having the same probability, the joint probability mass function is given by"
890,1,"['functions', 'table', 'mass function', 'marginal', 'probability mass function', 'joint probability mass function', 'distribution', 'joint', 'probability', 'function', 'marginal distribution', 'marginal probability', 'joint probability']", More than two random variables,seg_125,"for all distinct values a1, a2, . . . , an with 1 ≤ aj ≤ n . clearly x1, x2, . . . , xn influence each other. nevertheless, the marginal distribution of each xi is the same. this can be seen as follows. similar to obtaining the marginal probability mass functions in table 9.2, we can find the marginal probability mass function of xi by summing the joint probability mass function over all possible values of x1, . . . , xi−1, xi+1, . . . , xn:"
891,1,"['mass function', 'marginal', 'probability mass function', 'probability', 'combinations', 'function', 'marginal probability']", More than two random variables,seg_125,"where the sum runs over all distinct values a1, a2, . . . , an with 1 ≤ aj ≤ n and ai = k. since there are (n − 1)(n − 2) · · · (n −n+1) such combinations, we conclude that the marginal probability mass function of xi is given by"
892,1,"['mass function', 'marginal', 'probability mass function', 'probability', 'function', 'marginal probability']", More than two random variables,seg_125,"for k = 1, 2, . . . , n . we see that the marginal probability mass function of each xi is the same, assigning equal probability 1/n to each possible value."
893,1,"['random variables', 'continuous', 'variables', 'case', 'joint', 'random', 'function']", More than two random variables,seg_125,"in case the random variables x1, x2, . . . , xn are continuous, the joint distribution is defined in a similar way as in the case of two variables. we say that the random variables x1, x2, . . . , xn have a joint continuous distribution if for some function f : rn → r and for all numbers a1, a2, . . . , an and b1, b2, . . . , bn with ai ≤ bi,"
894,1,"['joint', 'probability', 'joint probability']", More than two random variables,seg_125,"again f has to satisfy f(x1, x2, . . . , xn) ≥ 0 and f has to integrate to 1. we call f the joint probability density of x1, x2, . . . , xn."
895,1,"['random variables', 'random', 'variables', 'independence', 'events']", Independent random variables,seg_127,"in earlier chapters we have spoken of independence of random variables, anticipating a formal definition. on page 46 we postulated that the events"
896,1,"['random variables', 'random', 'independent', 'variables', 'discrete random variables', 'discrete', 'bernoulli', 'events', 'union', 'independence', 'event']", Independent random variables,seg_127,"related to the bernoulli random variables r1, . . . , r10 are independent. how should one define independence of random variables? intuitively, random variables x and y are independent if every event involving only x is independent of every event involving only y . since for two discrete random variables x and y , any event involving x and y is the union of events of the type {x = a, y = b}, an adequate definition for independence would be"
897,1,"['continuous random variables', 'random variables', 'variables', 'case', 'discrete', 'random', 'continuous']", Independent random variables,seg_127,"for all possible values a and b. however, this definition is useless for continuous random variables. both the discrete and the continuous case are covered by the following definition."
898,1,"['random variables', 'independent', 'variables', 'distribution function', 'distribution', 'joint', 'random', 'function']", Independent random variables,seg_127,"definition. the random variables x and y , with joint distribution function f , are independent if"
899,1,"['random variables', 'variables', 'dependent', 'random']", Independent random variables,seg_127,for all possible values a and b. random variables that are not independent are called dependent.
900,1,"['independent', 'independence', 'joint', 'probability', 'joint probability']", Independent random variables,seg_127,"note that independence of x and y guarantees that the joint probability of {x ≤ a, y ≤ b} factorizes. more generally, the following is true: if x and y are independent, then"
901,1,"['marginal', 'case', 'discrete', 'independence', 'probability', 'random', 'intervals', 'continuous random variables', 'continuous', 'random variables', 'probabilities', 'independent', 'variables', 'discrete random variables', 'marginal probabilities']", Independent random variables,seg_127,"for all suitable a and b, such as intervals and points. as a special case we can take a = {a}, b = {b}, which yields that for independent x and y the probability of {x = a, y = b} equals the product of the marginal probabilities. in fact, for discrete random variables the definition of independence can be reduced—after cumbersome computations—to equality (9.3). for continuous random variables x and y we find, differentiating both sides of (9.4) with respect to x and y, that"
902,1,"['random variables', 'independent', 'variables', 'discrete random variables', 'discrete', 'random']", Independent random variables,seg_127,quick exercise 9.6 determine for which value of ε the discrete random variables x and y from quick exercise 9.2 are independent.
903,1,"['random variables', 'independent', 'variables', 'distribution', 'joint', 'random']", Independent random variables,seg_127,"more generally, random variables x1, x2, . . . , xn, with joint distribution function f , are independent if for all values a1, . . . , an,"
904,1,"['random variables', 'independent', 'variables', 'discrete random variables', 'case', 'discrete', 'random']", Independent random variables,seg_127,"as in the case of two discrete random variables, the discrete random variables x1, x2, . . . , xn are independent if"
905,1,"['random variables', 'variables', 'discrete random variables', 'discrete', 'random']", Independent random variables,seg_127,"for all possible values a1, . . . , an. thus we see that the definition of independence for discrete random variables is in agreement with our intuitive interpretation given earlier in (9.3)."
906,1,"['distribution function', 'case', 'probability', 'random', 'function', 'density function', 'continuous random variables', 'probability density function', 'joint probability density function', 'distribution', 'continuous', 'random variables', 'independent', 'variables', 'joint', 'joint probability']", Independent random variables,seg_127,"in case of independent continuous random variables x1, x2, . . . , xn with joint probability density function f , differentiating the joint distribution function with respect to all the variables gives that"
907,1,"['continuous', 'independence', 'case']", Independent random variables,seg_127,"for all values x1, . . . , xn. by integrating both sides over (−∞, a1]× (−∞, a2]× · · ·×(−∞, an], we find the definition of independence. hence in the continuous case, (9.6) is equivalent to the definition of independence."
908,1,"['random variables', 'transformed', 'interval', 'variables', 'independent', 'distribution function', 'distribution', 'joint', 'independent random variables', 'random', 'function']", Propagation of independence,seg_129,"a natural question is whether transformed independent random variables are again independent. we start with a simple example. let x and y be two independent random variables with joint distribution function f . take an interval i = (a, b] and define random variables u and v as follows:"
909,1,"['independence', 'independent']", Propagation of independence,seg_129,"are u and v independent? yes, they are! by using (9.5) and the independence of x and y , we can write"
910,0,[], Propagation of independence,seg_129,"by a similar reasoning one finds that for all values a and b,"
911,1,"['random variables', 'independent', 'variables', 'independence', 'independent random variables', 'random']", Propagation of independence,seg_129,"this illustrates the fact that for independent random variables x1, x2, . . . , xn, the random variables y1, y2, . . . , yn, where each yi is determined by xi only, inherit the independence from the xi. the general rule is given here."
912,1,"['random variables', 'variables', 'independence', 'random variable', 'variable', 'random', 'function']", Propagation of independence,seg_129,"propagation of independence. let x1, x2, . . . , xn be independent random variables. for each i, let hi : r → r be a function and define the random variable"
913,1,['independent'], Propagation of independence,seg_129,"then y1, y2, . . . , yn are also independent."
914,1,['functions'], Propagation of independence,seg_129,"often one uses this rule with all functions the same: hi = h. for instance, in the preceding example,"
915,1,"['random variables', 'independent', 'variables', 'transformations', 'independence', 'random']", Propagation of independence,seg_129,"the rule is also useful when we need different transformations for different xi. we already saw an example of this in chapter 6. in the single-server queue example in section 6.4, the exp(0.5) random variables t1, t2, . . . and u(2, 5) random variables s1, s2, . . . are required to be independent. they are generated according to the technique described in section 6.2. with a sequence u1, u2, . . . of independent u(0, 1) random variables we can accomplish independence of the ti and si as follows:"
916,1,"['functions', 'random variables', 'independent', 'variables', 'distribution', 'random']", Propagation of independence,seg_129,"where f and g are the distribution functions of the exp(0.5) distribution and the u(2, 5) distribution. the propagation-of-independence rule now guarantees that all random variables t1, s1, t2, s2, . . . are independent."
917,1,"['probability', 'combinations']", Solutions to the quick exercises,seg_131,"9.1 the only possibilities with the sum equal to 7 and the maximum equal to 4 are the combinations (3, 4) and (4, 3). they both have probability 1/36, so that p(s = 7, m = 4) = 2/36."
918,1,['information'], Solutions to the quick exercises,seg_131,"9.2 since px(0), px(1), py (0), and py (1) are all equal to 1/2, knowing only px and py yields no information on ε whatsoever. you have to be a student at hogwarts to be able to get the values of p right!"
919,1,"['random variables', 'probabilities', 'table', 'variables', 'discrete random variables', 'combinations', 'discrete', 'random']", Solutions to the quick exercises,seg_131,"9.3 since s and m are discrete random variables, f (5, 3) is the sum of the probabilities p(s = a, m = b) of all combinations (a, b) with a ≤ 5 and b ≤ 3. from table 9.2 we see that this sum is 8/36."
920,0,[], Solutions to the quick exercises,seg_131,1 5(3y3 + 18y2 − 21). differentiating with respect to y yields that
921,1,"['density function', 'probability density function', 'probability', 'function']", Solutions to the quick exercises,seg_131,"for y between 1 and 2 (and fy (y) = 0 otherwise). the probability density function of y can also be obtained directly from f(x, y). for y between 1 and 2:"
922,1,"['independent', 'case']", Solutions to the quick exercises,seg_131,"9.6 the number ε is between −1/4 and 1/4. now x and y are independent in case p(i, j) = p(x = i, y = j) = p(x = i)p(y = j) = px(i)py (j), for all i, j = 0, 1. if i = j = 0, we should have"
923,1,"['independent', 'dependent', 'combinations']", Solutions to the quick exercises,seg_131,"this implies that ε = 0. furthermore, for all other combinations (i, j) one can check that for ε = 0 also p(i, j) = px(i) py (j), so that x and y are independent. if ε = 0, we have p(0, 0) = px(0) py (0), so that x and y are dependent."
924,1,"['marginal', 'discrete', 'probability', 'random', 'marginal probability', 'probability distributions', 'distributions', 'joint probabilities', 'random variables', 'probabilities', 'table', 'variables', 'discrete random variables', 'joint']", Exercises,seg_133,"9.1 the joint probabilities p(x = a, y = b) of discrete random variables x and y are given in the following table (which is based on the magical square in albrecht dürer’s engraving melencolia i in figure 9.4). determine the marginal probability distributions of x and y , i.e., determine the probabilities p(x = a) and p(y = b) for a, b = 1, 2, 3, 4."
925,1,"['random variables', 'table', 'variables', 'discrete random variables', 'joint probability distribution', 'discrete', 'distribution', 'probability distribution', 'joint', 'probability', 'random', 'joint probability']", Exercises,seg_133,9.2 the joint probability distribution of two discrete random variables x and y is partly given in the following table.
926,1,['table'], Exercises,seg_133,a. complete the table.
927,1,"['independent', 'dependent']", Exercises,seg_133,b. are x and y dependent or independent?
928,1,"['random variables', 'table', 'variables', 'distribution', 'joint', 'random']", Exercises,seg_133,"9.3 let x and y be two random variables, with joint distribution the melencolia distribution, given by the table in exercise 9.1. what is"
929,1,"['random variables', 'table', 'probability distributions', 'variables', 'discrete random variables', 'marginal', 'discrete', 'probability', 'random', 'marginal probability', 'distributions']", Exercises,seg_133,9.4 this exercise will be easy for those familiar with japanese puzzles called nonograms. the marginal probability distributions of the discrete random variables x and y are given in the following table:
930,1,"['joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'joint probability']", Exercises,seg_133,"moreover, for a and b from 1 to 5 the joint probability p(x = a, y = b) is either 0 or 1/14. determine the joint probability distribution of x and y ."
931,1,"['joint probabilities', 'random variables', 'probabilities', 'table', 'variables', 'discrete random variables', 'discrete', 'joint', 'random']", Exercises,seg_133,"9.5 let η be an unknown real number, and let the joint probabilities p(x = a, y = b) of the discrete random variables x and y be given by the following table:"
932,0,[], Exercises,seg_133,a. which are the values η can attain?
933,1,['independent'], Exercises,seg_133,b. is there a value of η for which x and y are independent?
934,1,['independent'], Exercises,seg_133,9.6 let x and y be two independent ber(1
935,1,"['random variables', 'variables', 'random']", Exercises,seg_133,2 ) random variables. define random variables u and v by:
936,1,"['probability distributions', 'marginal', 'joint', 'probability', 'marginal probability', 'distributions']", Exercises,seg_133,a. determine the joint and marginal probability distributions of u and v .
937,1,"['independent', 'dependent']", Exercises,seg_133,b. find out whether u and v are dependent or independent.
938,1,"['data', 'table']", Exercises,seg_133,"9.7 to investigate the relation between hair color and eye color, the hair color and eye color of 5383 persons was recorded. the data are given in the following table:"
939,1,"['random variables', 'table', 'variables', 'joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'random', 'joint probability']", Exercises,seg_133,"eye color is encoded by the values 1 (light) and 2 (dark), and hair color by 1 (fair/red), 2 (medium), and 3 (dark/black). by dividing the numbers in the table by 5383, the table is turned into a joint probability distribution for random variables x (hair color) taking values 1 to 3 and y (eye color) taking values 1 and 2."
940,1,"['probability distributions', 'marginal', 'joint', 'probability', 'marginal probability', 'distributions']", Exercises,seg_133,a. determine the joint and marginal probability distributions of x and y .
941,1,"['independent', 'dependent']", Exercises,seg_133,b. find out whether x and y are dependent or independent.
942,1,"['random variables', 'independent', 'variables', 'independent random variables', 'probability', 'random']", Exercises,seg_133,9.8 let x and y be independent random variables with probability distributions given by
943,1,['distribution'], Exercises,seg_133,a. compute the distribution of z = x + y .
944,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random']", Exercises,seg_133,"b. let ỹ and z̃ be independent random variables, where ỹ has the same"
945,1,['distribution'], Exercises,seg_133,"distribution as y , and z̃ the same distribution as z. compute the distribution of x̃ = z̃ − ỹ ."
946,1,"['distribution function', 'distribution', 'joint', 'function']", Exercises,seg_133,9.9 suppose that the joint distribution function of x and y is given by
947,1,"['functions', 'marginal', 'distribution', 'marginal distribution']", Exercises,seg_133,a. determine the marginal distribution functions of x and y .
948,1,"['density function', 'probability density function', 'joint probability density function', 'joint', 'probability', 'function', 'joint probability']", Exercises,seg_133,b. determine the joint probability density function of x and y .
949,1,"['functions', 'density functions', 'marginal', 'marginal probability density', 'marginal probability density functions', 'probability', 'probability density functions', 'marginal probability']", Exercises,seg_133,c. determine the marginal probability density functions of x and y .
950,1,['independent'], Exercises,seg_133,d. find out whether x and y are independent.
951,1,"['density function', 'continuous random variables', 'random variables', 'continuous', 'variables', 'joint', 'random', 'function']", Exercises,seg_133,9.10 let x and y be two continuous random variables with joint probability density function
952,1,['probability'], Exercises,seg_133,a. find the probability p(1
953,1,"['distribution function', 'distribution', 'joint', 'function']", Exercises,seg_133,"4 ≤ x ≤ 1 2 , 1 3 ≤ y ≤ 2 3). b. determine the joint distribution function of x and y for a and b between"
954,1,"['density function', 'probability density function', 'probability', 'function']", Exercises,seg_133,d. apply the rule on page 122 to find the probability density function of x
955,1,"['density function', 'probability density function', 'joint probability density function', 'joint', 'probability', 'function', 'joint probability']", Exercises,seg_133,"from the joint probability density function f(x, y). use the result to verify your answer from c."
956,1,['independent'], Exercises,seg_133,e. find out whether x and y are independent.
957,1,"['density function', 'continuous random variables', 'random variables', 'probability density function', 'continuous', 'variables', 'joint probability density function', 'joint', 'probability', 'random', 'function', 'joint probability']", Exercises,seg_133,"9.11 let x and y be two continuous random variables, with the same joint probability density function as in exercise 9.10. find the probability p(x < y ) that x is smaller than y ."
958,1,"['density function', 'probability density function', 'joint probability density function', 'joint', 'probability', 'function', 'joint probability']", Exercises,seg_133,"9.12 the joint probability density function f of the pair (x, y ) is given by"
959,1,['probability'], Exercises,seg_133,b. determine the probability p(2x ≤ y ).
960,1,"['density function', 'probability density function', 'joint probability density function', 'joint', 'probability', 'function', 'joint probability']", Exercises,seg_133,"9.13 on a disc with origin (0, 0) and radius 1, a point (x, y ) is selected by throwing a dart that hits the disc in an arbitrary place. this is best described by the joint probability density function f of x and y , given by"
961,0,[], Exercises,seg_133,where c is some positive constant.
962,1,"['distribution', 'function', 'distribution function']", Exercises,seg_133,the distribution function fr.
963,1,"['density function', 'marginal', 'marginal density', 'function']", Exercises,seg_133,c. determine the marginal density function fx . without doing any calcula-
964,0,[], Exercises,seg_133,"tions, what can you say about fy ?"
965,1,['probability'], Exercises,seg_133,"9.14 an arbitrary point (x, y ) is drawn from the square [−1, 1] × [−1, 1]. this means that for any region g in the plane, the probability that (x, y ) is in g, is given by the area of g∩ divided by the area of , where denotes the square [−1, 1]× [−1, 1]:"
966,1,"['density function', 'probability density function', 'joint probability density function', 'joint', 'probability', 'function', 'joint probability']", Exercises,seg_133,"a. determine the joint probability density function of the pair (x, y )."
967,1,"['independent', 'random']", Exercises,seg_133,"b. check that x and y are two independent, u(−1, 1) distributed random"
968,1,"['distribution function', 'distribution', 'joint', 'function']", Exercises,seg_133,a. use figure 9.5 to show that the joint distribution function f of the pair
969,1,"['density function', 'probability density function', 'joint probability density function', 'joint', 'probability', 'function', 'joint probability']", Exercises,seg_133,"b. determine the joint probability density function f of the pair (x, y )."
970,1,['distribution'], Exercises,seg_133,"9.16 (continuation of exercise 9.15) an arbitrary point (u, v ) is drawn from the unit square [0, 1]× [0, 1]. let x and y be defined as in exercise 9.15. show that min{u, v } has the same distribution as x and that max{u, v } has the same distribution as y ."
971,1,"['random variables', 'independent', 'variables', 'distribution function', 'distribution', 'joint', 'independent random variables', 'random', 'function', 'uniformly distributed']", Exercises,seg_133,"9.17 let u1 and u2 be two independent random variables, both uniformly distributed over [0, a]. let v = min{u1, u2} and z = max{u1, u2}. show that the joint distribution function of v and z is given by"
972,1,"['mass function', 'marginal', 'probability mass function', 'without replacement', 'replacement', 'probability', 'function', 'marginal probability']", Exercises,seg_133,"9.18 suppose a vase contains balls numbered 1, 2, . . . , n . we draw n balls without replacement from the vase. each ball is selected with equal probability, i.e., in the first draw each ball has probability 1/n , in the second draw each of the n − 1 remaining balls has probability 1/(n − 1), and so on. for i = 1, 2, . . . , n, let xi denote the number on the ball in the ith draw. we have shown that the marginal probability mass function of xi is given by"
973,1,['variance'], Exercises,seg_133,b. compute the variance of xi. you may use the identity
974,1,"['density function', 'continuous random variables', 'random variables', 'continuous', 'variables', 'joint', 'random', 'function']", Exercises,seg_133,"9.19 let x and y be two continuous random variables, with joint probability density function"
975,0,[], Exercises,seg_133,"a. determine positive numbers a, b, and c such that"
976,1,[], Exercises,seg_133,b. setting µ = 4
977,0,[], Exercises,seg_133,and use this to show that
978,1,"['density function', 'probability density function', 'results', 'probability', 'function']", Exercises,seg_133,c. use the results from b to determine the probability density function fx
979,1,['distribution'], Exercises,seg_133,of x . what kind of distribution does x have?
980,0,[], Exercises,seg_133,"9.20 suppose we throw a needle on a large sheet of paper, on which horizontal lines are drawn, which are at needle-length apart (see also exercise 21.16). choose one of the horizontal lines as x-axis, and let (x, y ) be the center of the needle. furthermore, let z be the distance of this center (x, y ) to the nearest horizontal line under (x, y ), and let h be the angle between the needle and the positive x-axis."
981,0,[], Exercises,seg_133,"a. assuming that the length of the needle is equal to 1, argue that z has"
982,1,"['distribution', 'independent']", Exercises,seg_133,"a u(0, 1) distribution. also argue that h has a u(0, π) distribution and that z and h are independent."
983,0,[], Exercises,seg_133,b. show that the needle hits a horizontal line when
984,1,['probability'], Exercises,seg_133,c. show that the probability that the needle will hit one of the horizontal
985,1,"['random variables', 'combination', 'variables', 'correlation', 'distribution', 'expectation', 'joint', 'covariance', 'random', 'variance']", Covariance and correlation,seg_135,"in this chapter we see how the joint distribution of two or more random variables is used to compute the expectation of a combination of these random variables. we discuss the expectation and variance of a sum of random variables and introduce the notions of covariance and correlation, which express to some extent the way two random variables influence each other."
986,1,"['model', 'random variables', 'variables', 'expected value', 'random variable', 'variable', 'probability', 'random']", Expectation and joint distributions,seg_137,"china vases of various shapes are produced in the delftware factories in the old city of delft. one particular simple cylindrical model has height h and radius r centimeters. due to all kinds of circumstances—the place of the vase in the oven, the fact that the vases are handmade, etc.—h and r are not constants but are random variables. the volume of a vase is equal to the random variable v = πhr2, and one is interested in its expected value e[v ]. when fv denotes the probability density of v , then by definition"
987,1,"['joint', 'probability', 'function', 'joint probability']", Expectation and joint distributions,seg_137,"however, to obtain e[v ], we do not necessarily need to determine fv from the joint probability density f of h and r! since v is a function of h and r, we can use a rule similar to the change-of-variable formula from chapter 7:"
988,1,"['distribution', 'independent', 'case']", Expectation and joint distributions,seg_137,"suppose that h has a u(25, 35) distribution and that r has a u(7.5, 12.5) distribution. in the case that h and r are also independent, we have"
989,0,[], Expectation and joint distributions,seg_137,this illustrates the following general rule.
990,1,"['random variables', 'variables', 'discrete random variables', 'discrete', 'random', 'function']", Expectation and joint distributions,seg_137,"two-dimensional change-of-variable formula. let x and y be random variables, and let g : r2 → r be a function. if x and y are discrete random variables with values a1, a2, . . . and b1, b2, . . . , respectively, then"
991,1,"['density function', 'continuous random variables', 'random variables', 'probability density function', 'continuous', 'variables', 'joint probability density function', 'joint', 'probability', 'random', 'function', 'joint probability']", Expectation and joint distributions,seg_137,"if x and y are continuous random variables with joint probability density function f , then"
992,1,"['random variables', 'table', 'variables', 'discrete random variables', 'joint probability distribution', 'discrete', 'distribution', 'probability distribution', 'expectation', 'joint', 'probability', 'random', 'joint probability']", Expectation and joint distributions,seg_137,"as an example, take g(x, y) = xy for discrete random variables x and y with the joint probability distribution given in table 10.1. the expectation of xy is computed as follows:"
993,1,"['random variables', 'sum of two random variables', 'variables', 'random']", Expectation and joint distributions,seg_137,a natural question is whether this value can also be obtained from e[x ] e [y ]. we return to this question later in this chapter. first we address the expectation of the sum of two random variables.
994,1,"['random variables', 'table', 'variables', 'distribution', 'joint', 'random']", Expectation and joint distributions,seg_137,quick exercise 10.1 compute e[x + y ] for the random variables with the joint distribution given in table 10.1.
995,1,['discrete'], Expectation and joint distributions,seg_137,"for discrete x and y with values a1, a2, . . . and b1, b2, . . . , respectively, we see that"
996,1,"['continuous random variables', 'random variables', 'variables', 'case', 'random', 'continuous']", Expectation and joint distributions,seg_137,a similar line of reasoning applies in case x and y are continuous random variables. the following general rule holds.
997,1,"['random variables', 'variables', 'random', 'expectations']", Expectation and joint distributions,seg_137,"linearity of expectations. for all numbers r, s, and t and random variables x and y , one has"
998,1,"['random variables', 'table', 'variables', 'marginal', 'marginal distributions', 'distribution', 'joint', 'random', 'distributions']", Expectation and joint distributions,seg_137,"quick exercise 10.2 determine the marginal distributions for the random variables x and y with the joint distribution given in table 10.1, and use them to compute e[x ] en e[y ]. check that e[x ]+e[y ] is equal to e[x + y ], which was computed in quick exercise 10.1."
999,1,"['random variables', 'variables', 'random']", Expectation and joint distributions,seg_137,"more generally, for random variables x1, . . . , xn and numbers s1, . . . , sn and t,"
1000,1,"['distribution', 'random variable', 'variable', 'expectation', 'random']", Expectation and joint distributions,seg_137,"this rule is a powerful instrument. for example, it provides an easy way to compute the expectation of a random variable x with a bin(n, p) distribution. if we would use the definition of expectation, we have to compute"
1001,0,[], Expectation and joint distributions,seg_137,"to determine this sum is not straightforward. however, there is a simple alternative. recall the multiple-choice example from section 4.3. we represented"
1002,1,"['random variables', 'variables', 'distribution', 'random variable', 'variable', 'bernoulli', 'random']", Expectation and joint distributions,seg_137,"the number of correct answers out of 10 multiple-choice questions as a sum of 10 bernoulli random variables. more generally, any random variable x with a bin(n, p) distribution can be represented as"
1003,1,"['random variables', 'independent', 'variables', 'random']", Expectation and joint distributions,seg_137,"where r1, r2, . . . , rn are independent ber(p) random variables, i.e.,"
1004,1,['probability'], Expectation and joint distributions,seg_137,1 with probability p
1005,1,['probability'], Expectation and joint distributions,seg_137,ri = {0 with probability 1 − p.
1006,1,['expectations'], Expectation and joint distributions,seg_137,"since e[ri] = 0 · (1 − p) + 1 · p = p, for every i = 1, 2, . . . , n, the linearity-of- expectations rule yields"
1007,1,"['distribution', 'expectation']", Expectation and joint distributions,seg_137,"hence we conclude that the expectation of a bin(n, p) distribution equals np."
1008,1,"['random variables', 'variables', 'random']", Covariance,seg_139,in the previous section we have seen that for two random variables x and y always
1009,1,"['expectation', 'variance']", Covariance,seg_139,does such a simple relation also hold for the variance of the sum var(x + y ) or for expectation of the product e[xy ]? we will investigate this in the current section.
1010,1,"['variables', 'joint']", Covariance,seg_139,for the variables x and y from the example in section 9.2 with joint probability density
1011,0,[], Covariance,seg_139,one can show that
1012,1,['contrast'], Covariance,seg_139,"(see exercise 10.10). this shows, in contrast to the linearity-of-expectations rule, that var(x + y ) is generally not equal to var(x)+ var(y ). to determine var(x + y ), we exploit its definition:"
1013,1,['expectations'], Covariance,seg_139,"taking expectations on both sides, another application of the linearity-of- expectations rule gives"
1014,1,"['variance', 'variances']", Covariance,seg_139,"that is, the variance of the sum x + y equals the sum of the variances of x and y , plus an extra term 2e[(x − e[x ])(y − e[y ])]. to some extent this term expresses the way x and y influence each other."
1015,1,"['random variables', 'variables', 'covariance', 'random']", Covariance,seg_139,definition. let x and y be two random variables. the covariance between x and y is defined by
1016,1,"['case', 'covariance', 'correlated', 'realization']", Covariance,seg_139,"loosely speaking, if the covariance of x and y is positive, then if x has a realization larger than e[x ], it is likely that y will have a realization larger than e[y ], and the other way around. in this case we say that x and y are positively correlated . in case the covariance is negative, the opposite effect occurs; x and y are negatively correlated . in case cov(x, y ) = 0 we say that x and y are uncorrelated . an easy consequence of the linearity-of-expectations property (see exercise 10.19) is the following rule."
1017,1,"['random variables', 'variables', 'covariance', 'random']", Covariance,seg_139,"an alternative expression for the covariance. let x and y be two random variables, then"
1018,1,['correlated'], Covariance,seg_139,"for x and y from the example in section 9.2, we have e[x ] = 109/50, e[y ] = 157/100, and e[xy ] = 171/50 (see exercise 10.10). thus we see that x and y are negatively correlated:"
1019,1,"['contrast', 'expectation']", Covariance,seg_139,"moreover, this also illustrates that, in contrast to the expectation of the sum, for the expectation of the product, in general e[xy ] is not equal to e[x ] e [y ]."
1020,1,"['random variables', 'independent', 'variables', 'case', 'discrete', 'independent random variables', 'random']", Covariance,seg_139,"now let x and y be two independent random variables. one expects that x and y are uncorrelated: they have nothing to do with one another! this is indeed the case, for instance, if x and y are discrete; one finds that"
1021,1,"['continuous random variables', 'random variables', 'variables', 'case', 'observation', 'covariance', 'random', 'continuous']", Covariance,seg_139,a similar reasoning holds in case x and y are continuous random variables. the alternative expression for the covariance leads to the following important observation.
1022,1,"['random variables', 'independent', 'variables', 'random']", Covariance,seg_139,"independent versus uncorrelated. if two random variables x and y are independent, then x and y are uncorrelated."
1023,1,['independent'], Covariance,seg_139,"note that the reverse is not necessarily true. if x and y are uncorrelated, they need not be independent. this is illustrated in the next quick exercise."
1024,1,"['random variables', 'table', 'variables', 'dependent', 'distribution', 'joint', 'random']", Covariance,seg_139,"quick exercise 10.3 consider the random variables x and y with the joint distribution given in table 10.1. check that x and y are dependent, but that also e[xy ] = e[x ] e [y ]."
1025,1,"['random variables', 'sum of two random variables', 'variables', 'random', 'variance']", Covariance,seg_139,from the preceding we also deduce the following rule on the variance of the sum of two random variables.
1026,1,"['random variables', 'variables', 'random']", Covariance,seg_139,variance of the sum. let x and y be two random variables. then always
1027,0,[], Covariance,seg_139,"if x and y are uncorrelated,"
1028,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random', 'uncorrelated random variables']", Covariance,seg_139,"hence, we always have that e[x + y ] = e[x ]+e[y ], whereas var(x + y ) = var(x)+var(y ) only holds for uncorrelated random variables (and hence for independent random variables!)."
1029,1,"['random variables', 'variables', 'random', 'variance', 'uncorrelated random variables']", Covariance,seg_139,"as with the linearity-of-expectations rule, the rule for the variance of the sum of uncorrelated random variables holds more generally. for uncorrelated random variables x1, x2, . . . , xn, we have"
1030,1,"['distribution', 'random variable', 'variable', 'random', 'variance']", Covariance,seg_139,"this rule provides an easy way to compute the variance of a random variable with a bin(n, p) distribution. recall the representation for a bin(n, p) random variable x :"
1031,1,['variance'], Covariance,seg_139,each ri has variance
1032,1,"['independence', 'variance']", Covariance,seg_139,"using the independence of the ri, the rule for the variance of the sum yields"
1033,1,"['random variables', 'transformed', 'variables', 'random variable', 'variable', 'covariance', 'random']", The correlation coefficient,seg_141,"in the previous section we saw that the covariance between random variables gives an indication of how they influence one another. a disadvantage of the covariance is the fact that it depends on the units in which the random variables are represented. for instance, suppose that the length in inches and weight in kilograms of dutch citizens are modeled by random variables l and w . someone prefers to represent the length in centimeters. since 1 inch ≡ 2.53 cm, one is dealing with a transformed random variable 2.53l. the covariance between 2.53l and w is"
1034,1,"['factor', 'covariance', 'dependence']", The correlation coefficient,seg_141,"that is, the covariance increases with a factor 2.53, which is somewhat disturbing since changing from inches to centimeters does not essentially alter the dependence between length and weight. this illustrates that the covariance changes under a change of units. the following rule provides the exact relationship."
1035,1,"['random variables', 'variables', 'random']", The correlation coefficient,seg_141,covariance under change of units. let x and y be two random variables. then
1036,0,[], The correlation coefficient,seg_141,see exercise 10.14 for a derivation of this rule.
1037,1,"['standardized', 'correlation', 'covariance', 'correlation coefficient', 'dependence', 'coefficient']", The correlation coefficient,seg_141,"the preceding discussion indicates that the covariance cov(x, y ) may not always be suitable to express the dependence between x and y . for this reason there is a standardized version of the covariance called the correlation coefficient of x and y ."
1038,1,"['random variables', 'variables', 'correlation', 'correlation coefficient', 'random', 'coefficient']", The correlation coefficient,seg_141,"definition. let x and y be two random variables. the correlation coefficient ρ(x, y ) is defined to be 0 if var(x) = 0 or var(y ) = 0,"
1039,0,[], The correlation coefficient,seg_141,"note that ρ(x, y ) remains unaffected by a change of units, and therefore it is dimensionless. for instance, if x and y are measured in kilometers, then"
1040,1,"['random variables', 'variables', 'correlation', 'correlation coefficient', 'random', 'coefficient']", The correlation coefficient,seg_141,"the previous quick exercise illustrates the following linearity property for the correlation coefficient. for numbers r, s, t, and u fixed, r, t = 0, and random variables x and y :"
1041,1,"['coefficient', 'correlation coefficient', 'correlation']", The correlation coefficient,seg_141,"thus we see that the size of the correlation coefficient is unaffected by a change of units, but note the possibility of a change of sign."
1042,1,"['random variables', 'variables', 'case', 'random']", The correlation coefficient,seg_141,"two random variables x and y are “most correlated” if x = y or if x = −y . as a matter of fact, in the former case ρ(x, y ) = 1, while in the latter case ρ(x, y ) = −1. in general—for nonconstant random variables x and y —the following property holds:"
1043,0,[], The correlation coefficient,seg_141,"for a formal derivation of this property, see the next remark."
1044,1,['expectation'], Solutions to the quick exercises,seg_143,10.1 the expectation of x + y is computed as follows:
1045,1,"['table', 'marginal', 'marginal distributions', 'distributions']", Solutions to the quick exercises,seg_143,10.2 first complete table 10.1 with the marginal distributions:
1046,1,"['covariance', 'dependent', 'table']", Solutions to the quick exercises,seg_143,"10.3 from table 10.1, as completed in quick exercise 10.2, we see that x and y are dependent. for instance, p(x = 0, y = 0) = p(x = 0)p(y = 0). from quick exercise 10.2 we know that e[x ] = e[y ] = 1. because we already computed e[xy ] = 1, it follows that e[xy ] = e[x ] e [y ]. according to the alternative expression for the covariance this means that cov(x, y ) = 0, i.e., x and y are uncorrelated."
1047,1,[], Solutions to the quick exercises,seg_143,"10.4 we already computed cov(x, y ) = −13/5000 in section 10.2. hence, by the linearity-of-covariance rule cov(−2x + 7, 5y − 3) = (−2)·5·(−13/5000) = 13/500."
1048,1,"['correlation', 'correlation coefficient', 'coefficient', 'variances']", Solutions to the quick exercises,seg_143,"10.5 from quick exercise 10.4 we have cov(−2x + 7, 5y − 3) = 13/500. since var(x) = 989/2500 and var(y ) = 791/10 000, by definition of the correlation coefficient and the rule for variances,"
1049,1,"['joint probability distribution', 'data', 'distribution', 'probability distribution', 'joint', 'probability', 'joint probability', 'expectations', 'variances']", Exercises,seg_145,"10.1 consider the joint probability distribution of x and y from exercise 9.7, obtained from data on hair color and eye color, for which we already computed the expectations and variances of x and y , as well as e[xy ]."
1050,1,['correlated'], Exercises,seg_145,"a. compute cov(x, y ). are x and y positively correlated, negative corre-"
1051,0,[], Exercises,seg_145,"lated, or uncorrelated?"
1052,1,"['coefficient', 'correlation coefficient', 'correlation']", Exercises,seg_145,b. compute the correlation coefficient between x and y .
1053,1,"['random variables', 'variables', 'discrete random variables', 'discrete', 'joint', 'random']", Exercises,seg_145,10.2 consider the two discrete random variables x and y with joint distribution derived in exercise 9.2:
1054,1,['dependent'], Exercises,seg_145,b. note that x and y are dependent. show that x and y are uncorrelated.
1055,1,"['random variables', 'variables', 'joint probability distribution', 'dependent', 'distribution', 'probability distribution', 'joint', 'probability', 'random', 'joint probability']", Exercises,seg_145,10.3 let u and v be the two random variables from exercise 9.6. we have seen that u and v are dependent with joint probability distribution
1056,1,"['coefficient', 'covariance', 'correlation coefficient', 'correlation']", Exercises,seg_145,"determine the covariance cov(u, v ) and the correlation coefficient ρ(u, v )."
1057,1,"['joint probability distribution', 'discrete', 'distribution', 'probability distribution', 'joint', 'probability', 'random', 'joint probability']", Exercises,seg_145,"10.4 consider the joint probability distribution of the discrete random variables x and y from the melencolia exercise 9.1. compute cov(x, y )."
1058,1,"['random variables', 'variables', 'discrete random variables', 'marginal', 'discrete', 'marginal distributions', 'joint', 'random', 'distributions']", Exercises,seg_145,"10.5 suppose x and y are discrete random variables taking values 0,1, and 2. the following is given about the joint and marginal distributions:"
1059,1,['table'], Exercises,seg_145,a. complete the table.
1060,1,"['covariance', 'expectation']", Exercises,seg_145,b. compute the expectation of x and of y and the covariance between x
1061,1,['independent'], Exercises,seg_145,c. are x and y independent?
1062,1,"['random variables', 'variables', 'discrete random variables', 'marginal', 'discrete', 'marginal distributions', 'joint', 'random', 'distributions']", Exercises,seg_145,"10.6 suppose x and y are discrete random variables taking values c−1, c, and c + 1. the following is given about the joint and marginal distributions:"
1063,1,"['covariance', 'expectation']", Exercises,seg_145,a. take c = 0 and compute the expectation of x and of y and the covariance
1064,0,[], Exercises,seg_145,between x and y .
1065,0,[], Exercises,seg_145,"b. show that x and y are uncorrelated, no matter what the value of c is."
1066,1,['covariance'], Exercises,seg_145,"hint: one could compute cov(x, y ), but there is a short solution using the rule on the covariance under change of units (see page 141) together with part a."
1067,1,['independent'], Exercises,seg_145,c. are x and y independent?
1068,1,"['distribution', 'joint']", Exercises,seg_145,10.7 consider the joint distribution of quick exercise 9.2 and take ε fixed between −1/4 and 1/4:
1069,1,"['random variables', 'variables', 'random']", Exercises,seg_145,10.8 let x and y be random variables such that
1070,1,['expectation'], Exercises,seg_145,b. determine the expectation of −2x2 + y .
1071,1,"['test', 'probability']", Exercises,seg_145,10.9 suppose the blood of 1000 persons has to be tested to see which ones are infected by a (rare) disease. suppose that the probability that the test
1072,1,"['sample', 'results', 'tests', 'test']", Exercises,seg_145,"is positive is p = 0.001. the obvious way to proceed is to test each person, which results in a total of 1000 tests. an alternative procedure is the following. distribute the blood of the 1000 persons over 25 groups of size 40, and mix half of the blood of each of the 40 persons with that of the others in each group. now test the aggregated blood sample of each group: when the test is negative no one in that group has the disease; when the test is positive, at least one person in the group has the disease, and one will test the other half of the blood of all 40 persons of that group separately. in total, that gives 41 tests for that group. let xi be the total number of tests one has to perform for the ith group using this alternative procedure."
1073,1,"['distribution', 'probability distribution', 'probability']", Exercises,seg_145,"a. describe the probability distribution of xi, i.e., list the possible values it"
1074,1,['probabilities'], Exercises,seg_145,takes on and the corresponding probabilities.
1075,1,['tests'], Exercises,seg_145,b. what is the expected number of tests for the ith group? what is the
1076,1,['tests'], Exercises,seg_145,expected total number of tests? what do you think of this alternative procedure for blood testing?
1077,1,"['variables', 'joint', 'probability', 'joint probability']", Exercises,seg_145,10.10 consider the variables x and y from the example in section 9.2 with joint probability density
1078,1,"['densities', 'marginal', 'marginal probability', 'probability']", Exercises,seg_145,and marginal probability densities
1079,0,[], Exercises,seg_145,10.11 recall the relation between degrees celsius and degrees fahrenheit
1080,0,[], Exercises,seg_145,9 degrees fahrenheit = · degrees celsius + 32. 5
1081,1,['average'], Exercises,seg_145,"let x and y be the average daily temperatures in degrees celsius in amsterdam and antwerp. suppose that cov(x, y ) = 3 and ρ(x, y ) = 0.8. let t and s be the same temperatures in degrees fahrenheit. compute cov(t, s) and ρ(t, s)."
1082,1,"['random variables', 'independent', 'variables', 'distribution', 'independent random variables', 'random']", Exercises,seg_145,"10.12 consider the independent random variables h and r from the vase example, with a u(25, 35) and a u(7.5, 12.5) distribution. compute e[h ] and e[r2] and check that e[v ] = πe[h ] e[r2]."
1083,0,[], Exercises,seg_145,"10.13 let x and y be as in the triangle example in exercise 9.15. recall from exercise 9.16 that x and y represent the minimum and maximum coordinate of a point that is drawn from the unit square: x = min{u, v } and y = max{u, v }."
1084,0,[], Exercises,seg_145,hint: you might consult exercise 8.15.
1085,1,['independent'], Exercises,seg_145,"b. check that var(x + y ) = 1/6, by using that u and v are independent"
1086,1,"['covariance', 'results']", Exercises,seg_145,"c. determine the covariance cov(x, y ) using the results from a and b."
1087,1,"['random variables', 'variables', 'random']", Exercises,seg_145,"10.14 let x and y be two random variables and let r, s, t, and u be arbitrary real numbers."
1088,1,"['plot', 'random variables', 'simulation', 'variables', 'plots', 'joint', 'random', 'joint distributions', 'distributions']", Exercises,seg_145,"10.15 in figure 10.1 three plots are displayed. for each plot we carried out a simulation in which we generated 500 realizations of a pair of random variables (x, y ). we have chosen three different joint distributions of x and y ."
1089,1,"['plot', 'random variables', 'variables', 'random']", Exercises,seg_145,a. indicate for each plot whether it corresponds to random variables x and
1090,1,['correlated'], Exercises,seg_145,"y that are positively correlated, negatively correlated, or uncorrelated."
1091,1,"['plot', 'random variables', 'variables', 'random']", Exercises,seg_145,"b. which plot corresponds to random variables x and y for which |ρ(x, y )|"
1092,1,"['random variables', 'variables', 'random']", Exercises,seg_145,10.16 let x and y be random variables.
1093,1,['correlated'], Exercises,seg_145,"b. are x and x + y positively correlated, uncorrelated, or negatively cor-"
1094,0,[], Exercises,seg_145,"related, or can anything happen?"
1095,0,[], Exercises,seg_145,"c. same question as in part b, but now assume that x and y are uncorre-"
1096,1,"['random variables', 'variables', 'expectation', 'random', 'variance']", Exercises,seg_145,10.17 extending the variance of the sum rule. for mathematical convenience we first extend the sum rule to three random variables with zero expectation. next we further extend the rule to three random variables with nonzero expectation. by the same line of reasoning we extend the rule to n random variables.
1097,1,"['random variables', 'variables', 'expectation', 'random']", Exercises,seg_145,"a. let x, y and z be random variables with expectation 0. show that"
1098,0,[], Exercises,seg_145,"hint: directly apply that for real numbers y1, . . . , yn"
1099,1,['expectation'], Exercises,seg_145,"b. now show a for x, y , and z with nonzero expectation."
1100,1,"['covariance', 'variance']", Exercises,seg_145,hint: you might use the rules on pages 98 and 141 about variance and covariance under a change of units.
1101,1,['variance'], Exercises,seg_145,"c. derive a general variance of the sum rule, i.e., show that if x1, x2, . . . , xn"
1102,1,"['random variables', 'variables', 'random']", Exercises,seg_145,"are random variables, then"
1103,1,['variances'], Exercises,seg_145,d. show that if the variances are all equal to σ2 and the covariances are all
1104,0,[], Exercises,seg_145,"equal to some constant γ, then"
1105,1,"['probability', 'variance', 'without replacement', 'replacement']", Exercises,seg_145,"10.18 consider a vase containing balls numbered 1, 2, . . . , n . we draw n balls without replacement from the vase. each ball is selected with equal probability, i.e., in the first draw each ball has probability 1/n , in the second draw each of the n − 1 remaining balls has probability 1/(n − 1), and so on. for i = 1, 2, . . . , n, let xi denote the number on the ball in the ith draw. from exercise 9.18 we know that the variance of xi equals"
1106,1,['covariance'], Exercises,seg_145,"before you do the exercise: why do you think the covariance is negative? hint: use var(x1 + x2 + · · · + xn ) = 0 (why?), and apply exercise 10.17."
1107,1,"['linearity of expectations', 'covariance', 'expectations']", Exercises,seg_145,"10.19 derive the alternative expression for the covariance: cov(x, y ) = e[xy ] − e[x ] e [y ]. hint: work out (x − e[x ])(y − e[y ]) and use linearity of expectations."
1108,1,['distribution'], Exercises,seg_145,"10.20 determine ρ(u, u2) when u has a u(0, a) distribution. here a is a positive number."
1109,1,"['random variables', 'probability distributions', 'variables', 'distribution', 'expected value', 'probability distribution', 'random variable', 'variable', 'probability', 'random', 'variance', 'distributions']", More computations with more random variables,seg_147,"often one is interested in combining random variables, for instance, in taking the sum. in previous chapters, we have seen that it is fairly easy to describe the expected value and the variance of this new random variable. often more details are needed, and one also would like to have its probability distribution. in this chapter we consider the probability distributions of the sum, the product, and the quotient of two random variables."
1110,1,"['random variables', 'variables', 'set', 'probability', 'random']", Sums of discrete random variables,seg_149,"in a solo race across the pacific ocean, a ship has one spare radio set for communications. each of the two radios has probability p of failing each time it is switched on. the skipper uses the radio once every day. let x be the number of days the radio is switched on until it fails (so if the radio can be used for two days and fails on the third day, x attains the value 3). similarly, let y be the number of days the spare radio is switched on until it fails. note that these random variables are similar to the one discussed in section 4.4, which modeled the number of cycles until pregnancy. hence, x and y are geo(p) distributed random variables. suppose that p = 1/75 and that the trip will last 100 days. then at first sight the skipper does not need to worry about radio contact: the number of days the first radio lasts is x − 1 days, and similarly the spare radio lasts y −1 days. therefore the expected number of days he is able to have radio contact is"
1111,1,"['risk', 'probability']", Sums of discrete random variables,seg_149,the skipper—who has some training in probability theory—still has some concerns about the risk he runs with these two radios. what if the probability p(x + y − 2 ≤ 99) that his two radios break down before the end of the trip is large?
1112,1,"['random variables', 'mass function', 'variables', 'probability mass function', 'discrete random variables', 'discrete', 'random variable', 'variable', 'probability', 'random', 'function']", Sums of discrete random variables,seg_149,"this example illustrates that it is important to study the probability distribution of the sum z = x + y of two discrete random variables. the random variable z takes on values ai + bj , where ai is a possible value of x and bj of y . hence, the probability mass function of z is given by"
1113,1,['summation'], Sums of discrete random variables,seg_149,"where the sum runs over all possible values ai of x and bj of y such that ai + bj = c. because the sum only runs over values ai that are equal to c− bj, we simplify the summation and write"
1114,1,['independent'], Sums of discrete random variables,seg_149,"where the sum runs over all possible values bj of y . when x and y are independent, then p(x = c − bj , y = bj) = p(x = c − bj) p(y = bj). this leads to the following rule."
1115,1,"['functions', 'random variables', 'independent', 'mass function', 'variables', 'probability mass function', 'discrete random variables', 'discrete', 'probability', 'random', 'function']", Sums of discrete random variables,seg_149,"adding two independent discrete random variables. let x and y be two independent discrete random variables, with probability mass functions px and py . then the probability mass function pz of z = x + y satisfies"
1116,0,[], Sums of discrete random variables,seg_149,where the sum runs over all possible values bj of y .
1117,1,"['addition rule', 'independent', 'table']", Sums of discrete random variables,seg_149,"quick exercise 11.1 let s be the sum of two independent throws with a die, so s = x + y , where x and y are independent, and p(x = k) = p(y = k) = 1/6, for k = 1, . . . , 6. use the addition rule to compute p(s = 3) and p(s = 8), and compare your answers with table 9.2."
1118,1,"['random variables', 'independent', 'variables', 'random']", Sums of discrete random variables,seg_149,"in the solo race example, x and y are independent geo(p) distributed random variables. let z = x + y ; then by the above rule for k ≥ 2"
1119,1,"['distribution', 'geometric distribution', 'geometric']", Sums of discrete random variables,seg_149,note that x + y does not have a geometric distribution.
1120,0,[], Sums of discrete random variables,seg_149,"returning to the solo race example, it is clear that the skipper does have grounds to worry:"
1121,1,"['random variables', 'variables', 'binomial', 'random']", Sums of discrete random variables,seg_149,the sum of two binomial random variables
1122,1,"['discrete', 'probability', 'random', 'successes', 'independent random variables', 'success', 'trials', 'addition rule', 'distribution', 'random variables', 'independent', 'variables', 'discrete random variables']", Sums of discrete random variables,seg_149,"it is not always necessary to use the addition rule for two independent discrete random variables to find the distribution of their sum. for example, let x and y be two independent random variables, where x has a bin(n, p) distribution and y has a bin(m, p) distribution. since a bin(n, p) distribution models the number of successes in n independent trials with success probability p, heuristically, x + y represents the number of successes in n + m trials with success probability p and should therefore have a bin(n + m, p) distribution."
1123,0,[], Sums of discrete random variables,seg_149,a more formal reasoning is the following. let
1124,1,"['random variables', 'independent', 'variables', 'addition rule', 'distribution', 'random variable', 'variable', 'random']", Sums of discrete random variables,seg_149,"be independent ber(p) distributed random variables. recall that a bin(n, p) distributed random variable has the same distribution as the sum of n independent ber(p) distributed random variables (see section 4.3 or 10.2). hence x has the same distribution as r1 + r2 + · · · + rn and y has the same distribution as s1 + s2 + · · ·+ sm. this means that x + y has the same distribution as the sum of n+m independent ber(p) variables and therefore has a bin(n + m, p) distribution. this can also be verified analytically by means of the addition rule, using that x and y are also independent."
1125,1,"['independent', 'random variable', 'variable', 'random']", Sums of discrete random variables,seg_149,"quick exercise 11.2 for i = 1, 2, 3, let xi be a bin(ni, p) distributed random variable, and suppose that x1, x2, and x3 are independent. argue that z = x1 + x2 + x3 is a bin(n1 + n2 + n3, p) distributed random variable."
1126,1,"['density function', 'continuous random variables', 'random variables', 'probability density function', 'continuous', 'variables', 'independent', 'probability', 'random', 'function', 'uniformly distributed']", Sums of continuous random variables,seg_151,"let x and y be two continuous random variables. what can we say about the probability density function of z = x+y ? we start with an example. suppose that x and y are two independent, u(0, 1) distributed random variables. one might be tempted to think that z is also uniformly distributed."
1127,1,"['functions', 'density function', 'probability density function', 'joint probability density function', 'marginal', 'joint', 'probability', 'function', 'marginal probability', 'joint probability']", Sums of continuous random variables,seg_151,note that the joint probability density function f of x and y is equal to the product of the marginal probability functions fx and fy :
1128,1,"['distribution', 'function', 'distribution function']", Sums of continuous random variables,seg_151,"and f(x, y) = 0 otherwise. let us compute the distribution function fz of z. it is easy to see that fz(a) = 0 for a ≤ 0 and fz(a) = 1 for a ≥ 2. for a between 0 and 1, let g be that part of the plane below the line x+ y = a, and let ∆ be the triangle with vertices (0, 0), (a, 0), and (0, a); see figure 11.1."
1129,1,"['distribution', 'function', 'distribution function']", Sums of continuous random variables,seg_151,"since f(x, y) = 0 outside [0, 1] × [0, 1], the distribution function of z is given by"
1130,1,['case'], Sums of continuous random variables,seg_151,"for 0 < a < 1. for the case where 1 ≤ a < 2 one can draw a similar figure (see figure 11.2), from which one can find that"
1131,1,['uniformly distributed'], Sums of continuous random variables,seg_151,we see that z is not uniformly distributed.
1132,1,"['continuous', 'variables', 'distribution function', 'distribution', 'function']", Sums of continuous random variables,seg_151,"in general, the distribution function fz of the sum z of two continuous random variables x and y is given by"
1133,1,[], Sums of continuous random variables,seg_151,"the double integral on the right-hand side can be written as a repeated integral, first over x and then over y. note that x and y are between minus and plus infinity and that they also have to satisfy x + y ≤ a or, equivalently, x ≤ a − y. this means that the integral over x runs from minus infinity to y − a, and the integral over y runs from minus infinity to plus infinity. hence"
1134,1,"['independent', 'case']", Sums of continuous random variables,seg_151,"in case x and y are independent, the last double integral can be written as"
1135,0,[], Sums of continuous random variables,seg_151,and we find that
1136,0,[], Sums of continuous random variables,seg_151,for −∞ < a < ∞. differentiating fz we find the following rule.
1137,1,"['functions', 'density function', 'continuous random variables', 'random variables', 'probability density function', 'continuous', 'variables', 'density functions', 'independent', 'probability', 'random', 'function', 'probability density functions']", Sums of continuous random variables,seg_151,"adding two independent continuous random variables. let x and y be two independent continuous random variables, with probability density functions fx and fy . then the probability density function fz of z = x + y is given by"
1138,0,[], Sums of continuous random variables,seg_151,the single-server queue revisited
1139,1,"['model', 'distribution', 'expectations']", Sums of continuous random variables,seg_151,"in the single-server queue model from section 6.4, t1 is the time between the start at time zero and the arrival of the first customer and ti is the time between the arrival of the (i − 1)th and ith customer at a well. we are interested in the arrival time of the nth customer at the well. for n ≥ 1, let zn be the arrival time of the nth customer at the well: zn = t1 + · · · + tn. since each ti has an exp(0.5) distribution, it follows from the linearity-of- expectations rule in section 10.1 that the expected arrival time of the nth customer is"
1140,1,"['independent', 'case', 'random']", Sums of continuous random variables,seg_151,"we would like to know whether the pump capacity is sufficient; for instance, when the service times si are independent u(2, 5) distributed random variables (this is the case when the pump capacity v = 1). in that case, at most 30 customers can pump water at the well in the first hour. if p(z30 ≤ 60) is large, one might be tempted to increase the capacity of the well."
1141,1,"['random variables', 'independent', 'variables', 'addition rule', 'random']", Sums of continuous random variables,seg_151,"recalling that the ti are independent exp(λ) random variables, it follows from the addition rule that ft1+t2(z) = 0 if z < 0, and for z ≥ 0 that"
1142,1,['addition rule'], Sums of continuous random variables,seg_151,"viewing t1 + t2 + t3 as the sum of t1 and t2 + t3, we find, by applying the addition rule again, that fz3(z) = 0 if z < 0, and for z ≥ 0 that"
1143,1,['probability'], Sums of continuous random variables,seg_151,"even if each customer fills his jerrican in the minimum time of 2 minutes, we see that after an hour with probability 0.524, people will be waiting at the pump!"
1144,1,"['gamma random variable', 'gamma', 'random variable', 'variable', 'random']", Sums of continuous random variables,seg_151,"the random variable zn is an example of a gamma random variable, defined as follows."
1145,1,"['density function', 'probability density function', 'gamma', 'continuous', 'parameters', 'continuous random variable', 'random variable', 'variable', 'probability', 'random', 'function']", Sums of continuous random variables,seg_151,definition. a continuous random variable x has a gamma distribution with parameters α > 0 and λ > 0 if its probability density function f is given by f(x) = 0 for x < 0 and
1146,1,['distribution'], Sums of continuous random variables,seg_151,"where the quantity γ(α) is a normalizing constant such that f integrates to 1. we denote this distribution by gam(α, λ)."
1147,1,"['random variables', 'variables', 'distribution', 'parameter', 'random']", Sums of continuous random variables,seg_151,"(see also exercise 11.12). it follows from our example that the sum of n independent exp(λ) distributed random variables has a gam(n, λ) distribution, also known as the erlang-n distribution with parameter λ."
1148,1,"['random variables', 'independent', 'variables', 'normal', 'random']", Sums of continuous random variables,seg_151,the sum of independent normal random variables
1149,1,"['random variables', 'independent', 'variables', 'addition rule', 'normally distributed', 'random']", Sums of continuous random variables,seg_151,using the addition rule you can show that the sum of two independent normally distributed random variables is again a normally distributed random
1150,1,"['random variables', 'independent', 'variables', 'variable', 'random']", Sums of continuous random variables,seg_151,"variable. for instance, if x and y are independent n(0, 1) distributed random variables, one has"
1151,1,"['variables', 'change of variables']", Sums of continuous random variables,seg_151,"to prepare a change of variables, we subtract the term 2"
1152,0,[], Sums of continuous random variables,seg_151,to complete the square in the exponent:
1153,1,['variables'], Sums of continuous random variables,seg_151,in this way we find with changing integration variables t = √2(y − z/2):
1154,1,"['standard normal', 'distribution', 'normal', 'probability', 'standard', 'standard normal distribution', 'normal distribution']", Sums of continuous random variables,seg_151,"since φ is the probability density of the standard normal distribution, it in-"
1155,1,"['distribution', 'probability', 'normal', 'normal distribution']", Sums of continuous random variables,seg_151,"which is the probability density of the n(0, 2) distribution. thus, x + y also has a normal distribution. this is more generally true."
1156,1,"['random variables', 'independent', 'variables', 'distribution', 'normal', 'independent random variables', 'random', 'normal distribution']", Sums of continuous random variables,seg_151,"the sum of independent normal random variables. if x and y are independent random variables with a normal distribution, then x + y also has a normal distribution."
1157,1,"['random variables', 'parameters', 'independent', 'variables', 'normally distributed', 'distribution', 'random variable', 'variable', 'independent random variables', 'random']", Sums of continuous random variables,seg_151,"quick exercise 11.3 let x and y be independent random variables, where x has an n(3, 16) distribution, and y an n(5, 9) distribution. then x + y is a normally distributed random variable. what are its parameters?"
1158,1,['independence'], Sums of continuous random variables,seg_151,"rather surprisingly, independence of x and y is not a prerequisite, as can be seen in the following remark."
1159,1,"['distribution', 'uniformly distributed', 'independent']", Product and quotient of two random variables,seg_153,"recall from chapter 7 the example of the architect who wants maximal variety in the sizes of buildings. the architect wants more variety and therefore replaces the square buildings by rectangular buildings: the buildings should be of width x and depth y , where x and y are independent and uniformly distributed between 0 and 10 meters. since x and y are independent, the expected area of a building equals e[xy ] = e[x ] e [y ] = 5 · 5 = 25 m2. but what can one say about the distribution of the area z = xy of an arbitrary building?"
1160,1,"['distribution', 'function', 'distribution function']", Product and quotient of two random variables,seg_153,let us calculate the distribution function of z. clearly fz(a) = 0 if a < 0 and fz(a) = 1 if a > 100. for a between 0 and 100 we can compute fz(a) with the help of figure 11.3.
1161,0,[], Product and quotient of two random variables,seg_153,area of the shaded region in figure 11.3
1162,1,"['density function', 'probability density function', 'probability', 'function']", Product and quotient of two random variables,seg_153,hence the probability density function fz of z is given by
1163,1,"['density function', 'continuous random variables', 'random variables', 'probability density function', 'continuous', 'variables', 'independent', 'probability', 'random', 'function']", Product and quotient of two random variables,seg_153,"this computation can be generalized to arbitrary independent continuous random variables, and we obtain the following formula for the probability density function of the product of two random variables."
1164,1,"['density function', 'densities', 'continuous random variables', 'random variables', 'probability density function', 'continuous', 'variables', 'independent', 'probability', 'random', 'function']", Product and quotient of two random variables,seg_153,product of independent continuous random variables. let x and y be two independent continuous random variables with probability densities fx and fy . then the probability density function fz of z = xy is given by
1165,1,"['density function', 'random variables', 'probability density function', 'independent', 'variables', 'independence', 'independent random variables', 'probability', 'random', 'function']", Product and quotient of two random variables,seg_153,"for the quotient z = x/y of two independent random variables x and y it is now fairly easy to derive the probability density function. since the independence of x and y implies that x and 1/y are independent, the preceding rule yields"
1166,1,"['density function', 'probability density function', 'probability', 'function']", Product and quotient of two random variables,seg_153,recall from section 8.2 that the probability density function of 1/y is given
1167,1,['variable'], Product and quotient of two random variables,seg_153,"substituting this in the integral, after changing the variable of integration, we find the following rule."
1168,1,"['densities', 'continuous random variables', 'random variables', 'independent', 'variables', 'probability', 'random', 'continuous']", Product and quotient of two random variables,seg_153,quotient of independent continuous random variables. let x and y be two independent continuous random variables with probability densities fx and fy . then the probability density function fz of z = x/y is given by
1169,1,"['random variables', 'independent', 'variables', 'normal', 'random']", Product and quotient of two random variables,seg_153,the quotient of two independent normal random variables
1170,1,"['random variables', 'random', 'independent', 'variables', 'cauchy', 'standard normal', 'cauchy distribution', 'distribution', 'normal', 'independent random variables', 'standard', 'standard normal distribution', 'normal distribution']", Product and quotient of two random variables,seg_153,"let x and y be independent random variables, both having a standard normal distribution. when we compute the quotient z of x and y , we find a so-called standard cauchy distribution:"
1171,1,"['case', 'distributions']", Product and quotient of two random variables,seg_153,"this is the special case α = 0, β = 1 of the following family of distributions."
1172,1,"['density function', 'probability density function', 'parameters', 'continuous', 'cauchy', 'continuous random variable', 'random variable', 'variable', 'probability', 'random', 'function']", Product and quotient of two random variables,seg_153,definition. a continuous random variable has a cauchy distribution with parameters α and β > 0 if its probability density function f is given by
1173,1,['distribution'], Product and quotient of two random variables,seg_153,"we denote this distribution by cau(α, β)."
1174,1,"['cauchy', 'distribution function', 'distribution', 'function']", Product and quotient of two random variables,seg_153,"by integrating, we find that the distribution function f of a cauchy distribution is given by"
1175,1,"['distribution function', 'case', 'distribution', 'expected value', 'symmetry', 'probability', 'parameter', 'function']", Product and quotient of two random variables,seg_153,"the parameter α is the point of symmetry of the probability density function f . note that α is not the expected value of z. as a matter of fact, it was shown in remark 7.1 that the expected value does not exist! the probability density f is shown together with the distribution function f for the case α = 2, β = 5 in figure 11.4."
1176,1,"['distribution', 'cauchy', 'cauchy distribution', 'standard']", Product and quotient of two random variables,seg_153,"quick exercise 11.4 argue—without doing any calculations—that if z has a standard cauchy distribution, 1/z also has a standard cauchy distribution."
1177,1,['addition rule'], Solutions to the quick exercises,seg_155,11.1 using the addition rule we find
1178,1,"['random variable', 'variable', 'random']", Solutions to the quick exercises,seg_155,"11.2 we have seen that x1 + x2 is a bin(n1 + n2, p) distributed random variable. viewing x1 + x2 + x3 as the sum of x1 + x2 and x3, it follows that x1 + x2 + x3 is a bin(n1 + n2 + n3, p) distributed random variable."
1179,1,"['random variables', 'parameters', 'variables', 'normally distributed', 'linearity of expectations', 'random variable', 'normal', 'variable', 'expectation', 'random', 'expectations', 'variance']", Solutions to the quick exercises,seg_155,11.3 the sum rule for two normal random variables tells us that x + y is a normally distributed random variable. its parameters are expectation and variance of x + y . hence by linearity of expectations
1180,1,['variance'], Solutions to the quick exercises,seg_155,and by the rule for the variance of the sum
1181,1,['independence'], Solutions to the quick exercises,seg_155,"using that cov(x, y ) = 0 due to independence of x and y ."
1182,1,"['random variables', 'random', 'independent', 'standard normal random variables', 'variables', 'cauchy', 'standard normal', 'cauchy distribution', 'distribution', 'random variable', 'variable', 'normal', 'standard']", Solutions to the quick exercises,seg_155,"11.4 in the examples we have seen that the quotient x/y of two independent standard normal random variables has a standard cauchy distribution. since z = x/y , the random variable 1/z = y/x . this is also the quotient of two independent standard normal random variables, and it has a standard cauchy distribution."
1183,1,"['functions', 'random variables', 'independent', 'variables', 'uniform distribution', 'discrete', 'distribution', 'independent random variables', 'probability', 'random', 'discrete uniform distribution']", Exercises,seg_157,"11.1 let x and y be independent random variables with a discrete uniform distribution, i.e., with probability mass functions"
1184,1,"['random variables', 'mass function', 'variables', 'probability mass function', 'discrete random variables', 'addition rule', 'discrete', 'cases', 'probability', 'random', 'function']", Exercises,seg_157,use the addition rule for discrete random variables on page 152 to determine the probability mass function of z = x + y for the following two cases.
1185,0,['n'], Exercises,seg_157,"a. suppose n = 6, so that x and y represent two throws with a die. show"
1186,0,[], Exercises,seg_157,you may check this with quick exercise 11.1.
1187,0,['n'], Exercises,seg_157,b. determine the expression for pz(k) for general n .
1188,1,"['discrete random variable', 'probabilities', 'discrete', 'random variable', 'variable', 'random']", Exercises,seg_157,"11.2 consider a discrete random variable x taking values k = 0, 1, 2, . . . with probabilities"
1189,1,"['poisson', 'independent', 'variables', 'distribution', 'parameter', 'poisson distribution']", Exercises,seg_157,where µ > 0. this is the poisson distribution with parameter µ. we will learn more about this distribution in chapter 12. this exercise illustrates that the sum of independent poisson variables again has a poisson distribution.
1190,1,"['poisson', 'random variables', 'independent', 'variables', 'independent random variables', 'random']", Exercises,seg_157,"a. let x and y be independent random variables, each having a poisson"
1191,1,"['poisson', 'random variables', 'independent', 'variables', 'independent random variables', 'random']", Exercises,seg_157,"b. let x and y be independent random variables, each having a poisson"
1192,1,['parameters'], Exercises,seg_157,"distribution with parameters λ and µ. show that for k = 0, 1, 2, . . ."
1193,1,"['poisson', 'distribution', 'parameter', 'poisson distribution']", Exercises,seg_157,by using ∑k =0 (k)p (1 − p)k− = 1 for p = µ/(λ + µ). we conclude that x +y has a poisson distribution with parameter λ+µ.
1194,1,"['random variables', 'independent', 'variables', 'distribution', 'independent random variables', 'binomial', 'random', 'binomial distribution']", Exercises,seg_157,"11.3 let x and y be two independent random variables, where x has a ber(p) distribution, and y has a ber(q) distribution. when p = q = r, we know that x + y has a bin(2, r) distribution. suppose that p = 1/2 and q = 1/4. determine p(x + y = k), for k = 0, 1, 2, and conclude that x + y does not have a binomial distribution."
1195,1,"['random variables', 'independent', 'variables', 'distribution', 'independent random variables', 'random']", Exercises,seg_157,"11.4 let x and y be two independent random variables, where x has an n(2, 5) distribution and y has an n(5, 9) distribution. define z = 3x−2y +1."
1196,1,['distribution'], Exercises,seg_157,b. what is the distribution of z?
1197,1,"['density function', 'continuous random variables', 'random variables', 'probability density function', 'continuous', 'variables', 'independent', 'probability', 'random', 'function']", Exercises,seg_157,"11.5 let x and y be two independent, u(0, 1) distributed random variables. use the rule on addition of independent continuous random variables on page 156 to show that the probability density function of x + y is given"
1198,1,"['random variables', 'independent', 'variables', 'independent random variables', 'probability', 'random']", Exercises,seg_157,11.6 let x and y be independent random variables with probability den-
1199,1,"['continuous random variables', 'random variables', 'independent', 'variables', 'probability', 'random', 'continuous']", Exercises,seg_157,use the rule on addition of independent continuous random variables to determine the probability density of z = x + y .
1200,1,"['random variables', 'variables', 'cases', 'random']", Exercises,seg_157,"11.7 the two random variables in exercise 11.6 are special cases of gam(α, λ) variables, namely with α = 2 and λ = 1/2. more generally, let"
1201,1,"['random variables', 'independent', 'variables', 'distribution', 'random']", Exercises,seg_157,"x1, . . . , xn be independent gam(k, λ) distributed random variables, where λ > 0 and k is a positive integer. argue—without doing any calculations— that x1 + · · · + xn has a gam(nk, λ) distribution."
1202,1,"['distribution', 'cauchy', 'cauchy distribution']", Exercises,seg_157,11.8 we investigate the effect on the cauchy distribution under a change of units.
1203,1,"['distribution', 'cauchy', 'cauchy distribution', 'standard']", Exercises,seg_157,a. let x have a standard cauchy distribution. what is the distribution of
1204,1,['distribution'], Exercises,seg_157,"b. let x have a cau(α, β) distribution. what is the distribution of the"
1205,1,['variable'], Exercises,seg_157,random variable (x − α)/β?
1206,1,"['random variables', 'independent', 'variables', 'distribution', 'independent random variables', 'random']", Exercises,seg_157,11.9 let x and y be independent random variables with a par(α) and par(β) distribution.
1207,1,['probability'], Exercises,seg_157,a. take α = 3 and β = 1 and determine the probability density of z = xy .
1208,1,['probability'], Exercises,seg_157,b. determine the probability density of z = xy for general α and β.
1209,1,"['random variables', 'independent', 'variables', 'distribution', 'independent random variables', 'random']", Exercises,seg_157,11.10 let x and y be independent random variables with a par(α) and par(β) distribution.
1210,1,['probability'], Exercises,seg_157,a. take α = β = 2. show that z = x/y has probability density
1211,1,['probability'], Exercises,seg_157,"b. for general α, β > 0, show that z = x/y has probability density"
1212,1,"['random variables', 'independent', 'variables', 'random']", Exercises,seg_157,"11.11 let x1, x2, and x3 be three independent geo(p) distributed random variables, and let z = x1 + x2 + x3."
1213,1,"['mass function', 'probability mass function', 'probability', 'function']", Exercises,seg_157,a. show for k ≥ 3 that the probability mass function pz of z is given by
1214,0,[], Exercises,seg_157,"11.12 show that γ(1) = 1, and use integration by parts to show that"
1215,0,['n'], Exercises,seg_157,"use this last expression to show for n = 1, 2, . . . that"
1216,1,"['distribution', 'parameter']", Exercises,seg_157,11.13 let zn have an erlang-n distribution with parameter λ.
1217,0,['n'], Exercises,seg_157,a. use integration by parts to show that for a ≥ 0 and n ≥ 2:
1218,1,"['poisson', 'random process', 'random variables', 'poisson process', 'homogeneity', 'variables', 'case', 'independence', 'random', 'process', 'processes']", The Poisson process,seg_159,"in many random phenomena we encounter, it is not just one or two random variables that play a role but a whole collection. in that case one often speaks of a random process. the poisson process is a simple kind of random process, which models the occurrence of random points in time or space. there are numerous ways in which processes of random points arise: some examples are presented in the first section. the poisson process describes in a certain sense the most random way to distribute points in time or space. this is made more precise with the notions of homogeneity and independence."
1219,1,['random'], Random points,seg_161,"typical examples of the occurrence of random time points are: arrival times of email messages at a server, the times at which asteroids hit the earth, arrival times of radioactive particles at a geiger counter, times at which your computer crashes, the times at which electronic components fail, and arrival times of people at a pump in an oasis."
1220,1,"['random', 'locations']", Random points,seg_161,"examples of the occurrence of random points in space are: the locations of asteroid impacts with earth (2-dimensional), the locations of imperfections in a material (3-dimensional), and the locations of trees in a forest (2-dimensional)."
1221,1,"['poisson', 'model', 'poisson process', 'locations', 'probability', 'population', 'process']", Random points,seg_161,"some of these phenomena are better modeled by the poisson process than others. loosely speaking, one might say that the poisson process model often applies in situations where there is a very large population, and each member of the population has a very small probability to produce a point of the process. this is, for instance, well fulfilled in the geiger counter example where, in a huge collection of atoms, just a few will emit a radioactive particle (see [28]). a property of the poisson process—as we will see shortly—is that points may lie arbitrarily close together. therefore the tree locations are not so well modeled by the poisson process."
1222,1,"['poisson', 'process', 'poisson process']", Taking a closer look at random arrivals,seg_163,a well-known example that is usually modeled by the poisson process is that of calls arriving at a telephone exchange—the exchange is connected to a large number of people who make phone calls now and then. this will be our leading example in this section.
1223,1,"['interval', 'random']", Taking a closer look at random arrivals,seg_163,"telephone calls arrive at random times x1, x2, . . . at the telephone exchange during a time interval [0, t]."
1224,1,['random'], Taking a closer look at random arrivals,seg_163,the two basic assumptions we make on these random arrivals are
1225,1,"['rate', 'homogeneity']", Taking a closer look at random arrivals,seg_163,1. (homogeneity) the rate λ at which arrivals occur is constant over time:
1226,1,['expectation'], Taking a closer look at random arrivals,seg_163,in a subinterval of length u the expectation of the number of telephone calls is λu.
1227,1,"['independence', 'disjoint', 'intervals']", Taking a closer look at random arrivals,seg_163,2. (independence) the numbers of arrivals in disjoint time intervals are in-
1228,1,"['random variables', 'variables', 'random']", Taking a closer look at random arrivals,seg_163,dependent random variables.
1229,1,"['interval', 'stationarity', 'homogeneity']", Taking a closer look at random arrivals,seg_163,"homogeneity is also called weak stationarity. we denote the total number of calls in an interval i by n(i), abbreviating n([0, t]) to nt. homogeneity then implies that we require"
1230,1,"['distribution', 'interval', 'intervals']", Taking a closer look at random arrivals,seg_163,"to get hold of the distribution of nt we divide the interval [0, t] into n intervals of length t/n. when n is large enough, every interval ij,n = ((j − 1) t/n, j t/n] will contain either 0 or 1 arrival: for such a large n (which also satisfies"
1231,1,"['interval', 'homogeneity', 'distribution', 'random variable', 'variable', 'bernoulli', 'random', 'bernoulli random variable']", Taking a closer look at random arrivals,seg_163,"n > λt), let rj be the number of arrivals in the time interval ij,n. since rj is 0 or 1, rj has a ber(pj) distribution for some pj . recall that for a bernoulli random variable e[rj ] = 0 · (1 − pj) + 1 · pj = pj. by the homogeneity assumption, for each j"
1232,1,['intervals'], Taking a closer look at random arrivals,seg_163,"summing the number of calls in the intervals gives the total number of calls, hence"
1233,1,"['random variables', 'independent', 'variables', 'distribution', 'independence', 'independent random variables', 'random']", Taking a closer look at random arrivals,seg_163,"by the independence assumption, the rj are independent random variables, therefore nt has a bin(n, p) distribution, with p = λt/n."
1234,1,['approximation'], Taking a closer look at random arrivals,seg_163,we have found that (at least in first approximation)
1235,1,"['distribution', 'probability distribution', 'parameter', 'probability']", Taking a closer look at random arrivals,seg_163,"in this analysis n is a rather artificial parameter, of which we only know that it should not be “too small.” it therefore seems a good idea to get rid of n by letting n go to infinity, hoping that the probability distribution of nt will settle down. note that"
1236,0,[], Taking a closer look at random arrivals,seg_163,and from calculus we know that
1237,1,[], Taking a closer look at random arrivals,seg_163,"we obtain, combining these three limits, that"
1238,1,"['distribution', 'probability distribution', 'probabilities', 'probability']", Taking a closer look at random arrivals,seg_163,"we have indeed run into a probability distribution on the numbers 0, 1, 2, . . . . note that all these probabilities are determined by the single value λt. this motivates the following definition."
1239,1,"['poisson', 'discrete random variable', 'mass function', 'probability mass function', 'discrete', 'random variable', 'variable', 'probability', 'random', 'function', 'parameter']", Taking a closer look at random arrivals,seg_163,"definition. a discrete random variable x has a poisson distribution with parameter µ, where µ > 0 if its probability mass function p is given by"
1240,1,['distribution'], Taking a closer look at random arrivals,seg_163,we denote this distribution by pois(µ).
1241,1,"['poisson', 'functions', 'distribution', 'probability', 'poisson distribution']", Taking a closer look at random arrivals,seg_163,figure 12.1 displays the graphs of the probability mass functions of the poisson distribution with µ = 0.9 (left) and the poisson distribution with µ = 5 (right).
1242,1,['event'], Taking a closer look at random arrivals,seg_163,quick exercise 12.1 consider the event “exactly one call arrives in the
1243,1,"['interval', 'probability of the event', 'probability', 'event', 'probability of this event']", Taking a closer look at random arrivals,seg_163,"−λ·2s interval [0, 2s].” the probability of this event is p(n2s = 1) = λ · 2s · e . but note that this event is the same as “there is exactly one call in the interval [0, s) and no calls in the interval [s, 2s], or no calls in [0, s) and exactly one call in [s, 2s].” verify (using assumptions 1 and 2) that you get the same answer if you compute the probability of the event in this way."
1244,1,"['poisson', 'poisson random variable', 'distribution', 'random variable', 'variable', 'expectation', 'random', 'poisson distribution', 'variance']", Taking a closer look at random arrivals,seg_163,"we do have a hint1 about what the expectation and variance of a poisson random variable might be: since e[nt] = λt for all n, we anticipate that the limiting poisson distribution will have expectation λt. similarly, since nt has a bin(n, λ"
1245,1,"['distribution', 'variance']", Taking a closer look at random arrivals,seg_163,"t ) distribution, we anticipate that the variance will be"
1246,1,"['poisson', 'poisson random variable', 'random variable', 'variable', 'expectation', 'parameter', 'random']", Taking a closer look at random arrivals,seg_163,"actually, the expectation of a poisson random variable x with parameter µ is easy to compute:"
1247,1,['variance'], Taking a closer look at random arrivals,seg_163,"in a similar way the variance can be determined (see exercise 12.8), and we arrive at the following rule."
1248,1,"['poisson', 'distribution', 'expectation', 'parameter', 'poisson distribution', 'variance']", Taking a closer look at random arrivals,seg_163,the expectation and variance of a poisson distribution. let x have a poisson distribution with parameter µ; then
1249,1,"['interval', 'distribution', 'random variable', 'variable', 'random']", The onedimensional Poisson process,seg_165,"we will derive some properties of the sequence of random points x1, x2, . . . that we considered in the previous section. what we derived so far is that for any interval (s, s + t] the number n((s, s + t]) of points xi in that interval is a random variable with a pois(λt) distribution."
1250,1,"['distribution', 'probability distribution', 'probability', 'event']", The onedimensional Poisson process,seg_165,"are called interarrival times. here we define t1 = x1, the time of the first arrival. to determine the probability distribution of t1, we observe that the event {t1 > t} that the first call arrives after time t is the same as the event {nt = 0} that no calls have been made in [0, t]. but this implies that"
1251,1,"['distribution', 'exponential', 'exponential distribution', 'parameter']", The onedimensional Poisson process,seg_165,therefore t1 has an exponential distribution with parameter λ.
1252,1,"['independent', 'conditional', 'intervals', 'distribution', 'joint', 'probability', 'conditional probability']", The onedimensional Poisson process,seg_165,"to compute the joint distribution of t1 and t2, we consider the conditional probability that t2 > t, given that t1 = s, and use the property that arrivals in different intervals are independent:"
1253,1,['independent'], The onedimensional Poisson process,seg_165,"since this answer does not depend on s, we conclude that t1 and t2 are independent, and"
1254,1,"['poisson', 'independent', 'exponential distribution', 'method', 'interval', 'poisson process', 'distribution', 'exponential', 'probability', 'event', 'parameter', 'process']", The onedimensional Poisson process,seg_165,"i.e., t2 also has an exponential distribution with parameter λ. actually, although the conclusion is correct, the method to derive it is not, because we conditioned on the event {t1 = s}, which has zero probability. this problem could be circumvented by conditioning on the event that t1 lies in some small interval, but that will not be done here. analogously, one can show that the ti are independent and have an exp(λ) distribution. this nice property allows us to give a simple definition of the one-dimensional poisson process."
1255,1,"['poisson', 'intensity', 'random variables', 'independent', 'poisson process', 'variables', 'distribution', 'independent random variables', 'random', 'process']", The onedimensional Poisson process,seg_165,"definition. the one-dimensional poisson process with intensity λ is a sequence x1, x2, x3, . . . of random variables having the property that the interarrival times x1, x2−x1, x3−x2, . . . are independent random variables, each with an exp(λ) distribution."
1256,0,[], The onedimensional Poisson process,seg_165,note that the connection with nt is as follows: nt is equal to the number of xi that are smaller than (or equal to) t.
1257,1,"['poisson', 'model', 'intensity', 'process', 'poisson process', 'expectation', 'average']", The onedimensional Poisson process,seg_165,quick exercise 12.2 we model the arrivals of email messages at a server as a poisson process. suppose that on average 330 messages arrive per minute. what would you choose for the intensity λ in messages per second? what is the expectation of the interarrival time?
1258,1,"['random variables', 'independent', 'variables', 'distribution', 'random', 'exponentially distributed', 'exponentially']", The onedimensional Poisson process,seg_165,"an obvious question is: what is the distribution of xi? this has already been answered in chapter 11: since xi is a sum of i independent exponentially distributed random variables, we have the following."
1259,1,"['poisson', 'poisson process', 'distribution', 'random variable', 'variable', 'random', 'process']", The onedimensional Poisson process,seg_165,"the points of the poisson process. for i = 1, 2, . . . the random variable xi has a gam(i, λ) distribution."
1260,1,['distribution'], The onedimensional Poisson process,seg_165,the distribution of points
1261,1,"['distribution', 'case', 'interval', 'location']", The onedimensional Poisson process,seg_165,"another interesting question is: if we know that n points are generated in an interval, where do these points lie? since the distribution of the number of points only depends on the length of the interval, and not on its location, it suffices to determine this for an interval starting at 0. let this interval be [0, a]. we start with the simplest case, where there is one point in [0, a]: suppose that n([0, a]) = 1. then, for 0 < s < a:"
1262,1,"['interval', 'conditional', 'random variable', 'variable', 'random', 'uniformly distributed', 'event']", The onedimensional Poisson process,seg_165,"we find that conditional on the event {n([0, a]) = 1}, the random variable x1 is uniformly distributed over the interval [0, a]."
1263,0,[], The onedimensional Poisson process,seg_165,"now suppose that it is given that there are two points in [0, a]: n([0, a]) = 2. in a way similar to what we did for one point, we can show that (see exercise 12.12)"
1264,1,"['random variables', 'independent', 'variables', 'independent random variables', 'joint', 'random', 'function', 'uniformly distributed']", The onedimensional Poisson process,seg_165,"now recall the result of exercise 9.17: if u1 and u2 are two independent random variables, both uniformly distributed over [0, a], then the joint distribution function of v = min(u1, u2) and z = max(u1, u2) is given by"
1265,1,"['uniformly distributed', 'independent']", The onedimensional Poisson process,seg_165,"thus we have found that, if we forget about their order, the two points in [0, a] are independent and uniformly distributed over [0, a]. with somewhat more work, this generalizes to an arbitrary number of points, and we arrive at the following property."
1266,1,"['poisson', 'poisson process', 'interval', 'uniform distribution', 'locations', 'location', 'distribution', 'process']", The onedimensional Poisson process,seg_165,"location of the points, given their number. given that the poisson process has n points in the interval [a, b], the locations of these points are independently distributed, each with a uniform distribution on [a, b]."
1267,1,"['poisson', 'poisson process', 'homogeneity', 'distribution', 'set', 'poisson distribution', 'process']", Higherdimensional Poisson processes,seg_167,"our definition of the one-dimensional poisson process, starting with the interarrival times, does not generalize easily, because it is based on the ordering of the real numbers. however, we can easily extend the assumptions of independence, homogeneity, and the poisson distribution property. to do this we need a higher-dimensional version of the concept of length. we denote the kdimensional volume of a set a in k-dimensional space by m(a). for instance, in the plane m(a) is the area of a, and in space m(a) is the volume of a."
1268,1,"['poisson', 'intensity', 'disjoint', 'independent', 'poisson process', 'homogeneity', 'independence', 'random variable', 'variable', 'sets', 'set', 'parameter', 'random', 'process', 'disjoint sets']", Higherdimensional Poisson processes,seg_167,"definition. the k-dimensional poisson process with intensity λ is a collection x1, x2, x3, . . . of random points having the property that if n(a) denotes the number of points in the set a, then 1. (homogeneity) the random variable n(a) has a poisson distribution with parameter λm(a). 2. (independence) for disjoint sets a1, a2, . . . , an the random variables n(a1), n(a2), . . . , n(an) are independent."
1269,1,"['poisson', 'model', 'process', 'poisson process', 'locations', 'probability', 'average']", Higherdimensional Poisson processes,seg_167,quick exercise 12.3 suppose that the locations of defects in a certain type of material follow the two-dimensional poisson process model. for this material it is known that it contains on average five defects per square meter. what is the probability that a strip of length 2 meters and width 5 cm will be without defects?
1270,1,"['poisson', 'poisson process', 'uniformly distributed', 'locations', 'distribution', 'set', 'parameter', 'poisson distribution', 'process']", Higherdimensional Poisson processes,seg_167,"in figure 7.4 the locations of the buildings the architect wanted to distribute over a 100-by-300-m terrain have been generated by a two-dimensional poisson process. this has been done in the following way. one can again show that given the total number of points in a set, these points are uniformly distributed over the set. this leads to the following procedure: first one generates a value n from a poisson distribution with the appropriate parameter (λ times the area), then one generates n times a point uniformly distributed over the 100- by-300 rectangle."
1271,1,"['poisson', 'poisson process', 'exponentially distributed', 'process', 'exponentially']", Higherdimensional Poisson processes,seg_167,"actually one can generate a higher-dimensional poisson process in a way that is very similar to the natural way this can be done for the one-dimensional process. directly from the definition of the one-dimensional process we see that it can be obtained by consecutively generating points with exponentially distributed gaps. we will explain a similar procedure for dimension two. for s > 0, let"
1272,1,"['poisson', 'distribution', 'parameter', 'poisson distribution']", Higherdimensional Poisson processes,seg_167,"where cs is the circular region of radius s, centered at the origin. since cs has area πs2, ms has a poisson distribution with parameter λπs2. let ri denote the distance of the ith closest point to the origin. this is illustrated in figure 12.2."
1273,1,"['poisson', 'process', 'poisson process']", Higherdimensional Poisson processes,seg_167,note that ri is the analogue of the ith arrival time for the one-dimensional poisson process: we have in fact that
1274,0,[], Higherdimensional Poisson processes,seg_167,"in other words: r12 is exp(λπ) distributed. for general i, we can similarly write"
1275,1,"['independent', 'gamma', 'gamma distributions', 'exponential', 'distributions']", Higherdimensional Poisson processes,seg_167,"which means that ri2 has a gam(i, λπ) distribution—as we saw on page 157. since gamma distributions arise as sums of independent exponential distributions, we can also write"
1276,1,"['poisson', 'random variables', 'independent', 'poisson process', 'variables', 'case', 'distribution', 'random', 'process']", Higherdimensional Poisson processes,seg_167,"where the ti are independent exp(λπ) random variables (and where r0 = 0). note that this is quite similar to the one-dimensional case. to simulate the two-dimensional poisson process from a sequence u1, u2, . . . of independent u(0, 1) random variables, one can therefore proceed as follows (recall from section 6.2 that −(1/λ) ln(ui) has an exp(λ) distribution): for i = 1, 2, . . . put"
1277,1,"['poisson', 'process', 'independent', 'poisson process', 'uniformly distributed']", Higherdimensional Poisson processes,seg_167,"this gives the distance of the ith point to the origin, and then put the point on this circle according to an angle value generated by 2πu2i−1. this is the correct way to do it, because one can show that in polar coordinates the radius and the angle of a poisson process point are independent of each other, and the angle is uniformly distributed over [0, 2π]. the latter is called the isotropy property of the poisson process."
1278,1,['probability'], Solutions to the quick exercises,seg_169,"12.1 the probability of exactly one call in [0, s) and no calls in [s, 2s] equals"
1279,1,"['independence', 'probability', 'homogeneity']", Solutions to the quick exercises,seg_169,"because of independence and homogeneity. in the same way, the probability"
1280,0,[], Solutions to the quick exercises,seg_169,"12.2 because there are 60 seconds in a minute, we have 60λ = 330. it follows that λ = 5 1"
1281,1,['distribution'], Solutions to the quick exercises,seg_169,"2 . since the interarrival times have an exp(λ) distribution, the expected time between messages is 1/λ = 0.18 second."
1282,1,"['intensity', 'process', 'probability']", Solutions to the quick exercises,seg_169,12.3 the intensity of this process is λ = 5 per m2. the area of the strip is 2 · (1/20) = 1/10 m2. hence the probability that no defects occur in the strip is e−λ·(area of strip) = e−5·(1/10) = e−1/2 = 0.60.
1283,1,"['poisson', 'model', 'process', 'poisson process']", Exercises,seg_171,"12.1 in each of the following examples, try to indicate whether the poisson process would be a good model."
1284,1,['states'], Exercises,seg_171,a. the times of bankruptcy of enterprises in the united states.
1285,0,[], Exercises,seg_171,b. the times a chicken lays its eggs.
1286,0,[], Exercises,seg_171,c. the times of airplane crashes in a worldwide registration.
1287,1,['locations'], Exercises,seg_171,d. the locations of worngly spelled words in a book.
1288,0,['e'], Exercises,seg_171,e. the times of traffic accidents at a crossroad.
1289,1,"['poisson', 'distribution', 'probability', 'poisson distribution']", Exercises,seg_171,12.2 the number of customers that visit a bank on a day is modeled by a poisson distribution. it is known that the probability of no customers at all is 0.00001. what is the expected number of customers?
1290,1,['distribution'], Exercises,seg_171,12.3 let n have a pois(4) distribution. what is p(n = 4)?
1291,1,['distribution'], Exercises,seg_171,12.4 let x have a pois(2) distribution. what is p(x ≤ 1)?
1292,1,"['poisson', 'poisson random variable', 'errors', 'random variable', 'variable', 'expectation', 'random', 'error']", Exercises,seg_171,"12.5 the number of errors on a hard disk is modeled as a poisson random variable with expectation one error in every mb, that is, in every 220 bytes."
1293,1,"['error', 'probability']", Exercises,seg_171,a. what is the probability of at least one error in a sector of 512 bytes?
1294,0,[], Exercises,seg_171,b. the hard disk is an 18.62-gb disk drive with 39 054 015 sectors. what is
1295,1,"['error', 'probability']", Exercises,seg_171,the probability of at least one error on the hard disk?
1296,1,"['poisson', 'model', 'poisson process', 'locations', 'probability', 'process']", Exercises,seg_171,12.6 a certain brand of copper wire has flaws about every 40 centimeters. model the locations of the flaws as a poisson process. what is the probability of two flaws in 1 meter of wire?
1297,1,"['poisson', 'model', 'poisson process', 'interval', 'process']", Exercises,seg_171,"12.7 the poisson model is sometimes used to study the flow of traffic ([15]). if the traffic can flow freely, it behaves like a poisson process. a 20-minute time interval is divided into 10-second time slots. at a certain point along the highway the number of passing cars is registered for each 10-second time slot. let nj be the number of slots in which j cars have passed for j = 0, . . . , 9. suppose that one finds"
1298,0,[], Exercises,seg_171,note that the total number of cars passing in these 20 minutes is 230.
1299,1,"['intensity', 'parameter']", Exercises,seg_171,a. what would you choose for the intensity parameter λ?
1300,1,"['estimates', 'probability']", Exercises,seg_171,b. suppose one estimates the probability of 0 cars passing in a 10-second
1301,0,[], Exercises,seg_171,time slot by n0 divided by the total number of time slots. does that (reasonably) agree with the value that follows from your answer in a?
1302,1,['probability'], Exercises,seg_171,c. what would you take for the probability that 10 cars pass in a 10-second
1303,1,"['poisson', 'poisson random variable', 'random variable', 'variable', 'parameter', 'random']", Exercises,seg_171,12.8 let x be a poisson random variable with parameter µ.
1304,1,"['poisson', 'intensity', 'disjoint', 'random variables', 'independent', 'poisson process', 'variables', 'addition rule', 'intervals', 'distribution', 'parameter', 'random', 'poisson distribution', 'process']", Exercises,seg_171,"12.9 let y1 and y2 be independent poisson random variables with parameter µ1, respectively µ2. show that y = y1 + y2 also has a poisson distribution. instead of using the addition rule in section 11.1 as in exercise 11.2, you can prove this without doing any computations by considering the number of points of a poisson process (with intensity 1) in two disjoint intervals of length µ1 and µ2."
1305,1,"['probabilities', 'distribution', 'random variable', 'variable', 'random']", Exercises,seg_171,"12.10 let x be a random variable with a pois(µ) distribution. show the following. if µ < 1, then the probabilities p(x = k) are strictly decreasing in k. if µ > 1, then the probabilities p(x = k) are first increasing, then decreasing (cf. figure 12.1). what happens if µ = 1?"
1306,1,"['poisson', 'intensity', 'poisson process', 'process']", Exercises,seg_171,"12.11 consider the one-dimensional poisson process with intensity λ. show that the number of points in [0, t], given that the number of points in [0, 2t] is equal to n, has a bin(n, 1"
1307,1,"['independent', 'intersection', 'distribution', 'events', 'event']", Exercises,seg_171,"2 ) distribution. hint: write the event {n([0, s]) = k, n([0, 2s]) = n} as the intersection of the (independent!) events {n([0, s]) = k} and {n((s, 2s]) = n − k}."
1308,1,"['poisson', 'poisson process', 'conditional', 'distribution', 'locations', 'joint', 'process']", Exercises,seg_171,"12.12 we consider the one-dimensional poisson process. suppose for some a > 0 it is given that there are exactly two points in [0, a], or in other words: na = 2. the goal of this exercise is to determine the joint distribution of x1 and x2, the locations of the two points, conditional on na = 2."
1309,0,[], Exercises,seg_171,b. deduce from a that
1310,1,"['poisson', 'model', 'intensity', 'poisson process', 'process']", Exercises,seg_171,"12.13 walking through a meadow we encounter two kinds of flowers, daisies and dandelions. as we walk in a straight line, we model the positions of the flowers we encounter with a one-dimensional poisson process with intensity λ. it appears that about one in every four flowers is a daisy. forgetting about the dandelions, what does the process of the daisies look like? this question will be answered with the following steps."
1311,0,[], Exercises,seg_171,"a. let nt be the total number of flowers, xt the number of daisies, and yt"
1312,1,"['independent', 'probability']", Exercises,seg_171,"be the number of dandelions we encounter during the first t minutes of our walk. note that xt + yt = nt. suppose that each flower is a daisy with probability 1/4, independent of the other flowers. argue that"
1313,1,[], Exercises,seg_171,by conditioning on nt and using a.
1314,1,"['poisson', 'intensity', 'disjoint', 'independent', 'poisson process', 'intervals', 'process']", Exercises,seg_171,"since it is clear that the numbers of daisies that we encounter in disjoint time intervals are independent, we may conclude from c that the process (xt) is again a poisson process, with intensity λ/4. one often says that the process (xt) is obtained by thinning the process (nt). in our example this corresponds to picking all the dandelions."
1315,1,"['random variables', 'variables', 'distribution', 'random variable', 'variable', 'expectation', 'random', 'expectations', 'distributions']", Exercises,seg_171,"12.14 in this exercise we look at a simple example of random variables xn that have the property that their distributions converge to the distribution of a random variable x as n → ∞, while it is not true that their expectations converge to the expectation of x . let for n = 1, 2, . . . the random variables xn be defined by"
1316,1,"['random variable', 'variable', 'probability', 'random']", Exercises,seg_171,a. let x be the random variable that is equal to 0 with probability 1. show
1317,1,"['functions', 'mass function', 'probability mass function', 'probability', 'function']", Exercises,seg_171,that for all a the probability mass functions pxn(a) of the xn converge to the probability mass function px(a) of x as n → ∞. note that e[x ]=0.
1318,1,"['observations', 'measurements', 'random', 'experiment', 'results', 'independent random variables', 'model', 'factors', 'distribution', 'probabilistic', 'outcomes', 'random variables', 'independent', 'variables', 'law of large numbers', 'variation', 'experiments', 'measuring']", The law of large numbers,seg_173,"for many experiments and observations concerning natural phenomena—such as measuring the speed of light—one finds that performing the procedure twice under (what seem) identical conditions results in two different outcomes. uncontrollable factors cause “random” variation. in practice one tries to overcome this as follows: the experiment is repeated a number of times and the results are averaged in some way. in this chapter we will see why this works so well, using a model for repeated measurements. we view them as a sequence of independent random variables, each with the same unknown distribution. it is a probabilistic fact that from such a sequence—in principle—any feature of the distribution can be recovered. this is a consequence of the law of large numbers."
1319,1,"['experimental', 'levels', 'combination', 'variability', 'statistical', 'measurements', 'inequality']", Averages vary less,seg_175,"scientists and engineers involved in experimental work have known for centuries that more accurate answers are obtained when measurements or experiments are repeated a number of times and one averages the individual outcomes.1 for example, if you read a description of a.a. michelson’s work done in 1879 to determine the speed of light, you would find that for each value he collected, repeated measurements at several levels were performed. in an article in statistical science describing his work ([18]), r.j. mackay and r.w. oldford state: “it is clear that michelson appreciated the power of averaging to reduce variability in measurement.” we shall see that we can understand this reduction using only what we have learned so far about probability in combination with a simple inequality called chebyshev’s inequality."
1320,1,"['random variables', 'measurement', 'experiment', 'variables', 'random']", Averages vary less,seg_175,"throughout this chapter we consider a sequence of random variables x1, x2, x3, . . . . you should think of xi as the result of the ith repetition of a particular measurement or experiment. we confine ourselves to the situation where"
1321,1,"['random', 'function', 'experiment', 'random variable', 'standard', 'standard deviation', 'distribution', 'expectation', 'outcome', 'independent and identically distributed sequence', 'outcomes', 'deviation', 'random variables', 'independent', 'variables', 'variable', 'experiments']", Averages vary less,seg_175,"experimental conditions of subsequent experiments are identical, and the outcome of any one experiment does not influence the outcomes of others. under those circumstances, the random variables of the sequence are independent, and all have the same distribution, and we therefore call x1, x2, x3, . . . an independent and identically distributed sequence. we shall denote the distribution function of each random variable xi by f , its expectation by µ, and the standard deviation by σ."
1322,1,"['random variables', 'variables', 'random', 'average']", Averages vary less,seg_175,the average of the first n random variables in the sequence is
1323,1,"['expectations', 'linearity of expectations']", Averages vary less,seg_175,and using linearity of expectations we find:
1324,1,['independence'], Averages vary less,seg_175,"by the variance-of-the-sum rule, using the independence of x1, . . . , xn,"
1325,0,[], Averages vary less,seg_175,this establishes the following rule.
1326,1,"['random variables', 'independent', 'variables', 'independent random variables', 'expectation', 'variance', 'random', 'average']", Averages vary less,seg_175,"expectation and variance of an average. if x̄n is the average of n independent random variables with the same expectation µ and variance σ2, then"
1327,1,"['deviation', 'factor', 'expectation', 'standard', 'standard deviation']", Averages vary less,seg_175,"the expectation of x̄n is again µ, and its standard deviation is less than that of a single xi by a factor √n; the “typical distance” from µ is √n smaller. the latter property is what michelson used to gain accuracy. to illustrate this, we analyze an example."
1328,1,"['random variables', 'variables', 'distribution', 'probability', 'random', 'continuous']", Averages vary less,seg_175,"suppose the random variables x1, x2, . . . are continuous with a gam(2, 1) distribution, so with probability density:"
1329,1,"['random variables', 'independent', 'variables', 'distribution', 'probability', 'random']", Averages vary less,seg_175,"recall from section 11.2 that this means that each xi is distributed as the sum of two independent exp(1) random variables. hence, sn = x1 + · · ·+xn is distributed as the sum of 2n independent exp(1) random variables, which has a gam(2n, 1) distribution, with probability density"
1330,1,"['distribution', 'probability']", Averages vary less,seg_175,"this is the probability density of the gam(2n, n) distribution."
1331,1,"['densities', 'plots', 'distribution', 'probability', 'plotting']", Averages vary less,seg_175,"so we have determined the distribution of x̄n explicitly and we can investigate what happens as n increases, for example, by plotting probability densities. in the left-hand column of figure 13.1 you see plots of fx̄ for n = 1, 2, 4, 9,"
1332,1,"['density function', 'densities', 'probability density function', 'gamma', 'results', 'bimodal density', 'probability', 'function', 'bimodal']", Averages vary less,seg_175,"n 16, and 400 (note that for n = 1 this is just f itself). for comparison, we take as a second example a so-called bimodal density function: a density with two bumps, formally called modes. for the same values of n we determined the probability density function of x̄n (unlike the previous example, we are not concerned with the computations, just with the results). the graphs of these densities are given side by side with the gamma densities in figure 13.1."
1333,1,"['densities', 'gamma', 'expected value', 'probability', 'bimodal']", Averages vary less,seg_175,"the graphs clearly show that, as n increases, there is “contraction” of the probability mass near the expected value µ (for the gamma densities this is 2, for the bimodal densities 2.625)."
1334,1,"['expected value', 'probabilities', 'gamma', 'case']", Averages vary less,seg_175,"quick exercise 13.1 compare the probabilities that x̄n is within 0.5 of its expected value for n = 1, 4, 16, and 400. do this for the gamma case only by estimating the probabilities from the graphs in the left-hand column of figure 13.1."
1335,1,"['random', 'interval', 'standard deviations', 'distribution', 'random variable', 'probability distribution', 'variable', 'expectation', 'deviations', 'probability', 'standard']", Chebyshevs inequality,seg_177,"the contraction of probability mass near the expectation is a consequence of the fact that, for any probability distribution, most probability mass is within a few standard deviations from the expectation. to show this we will employ the following tool, which provides a bound for the probability that the random variable y is outside the interval (e[y ] − a, e[y ] + a)."
1336,1,"['random variable', 'variable', 'random', 'inequality']", Chebyshevs inequality,seg_177,chebyshev’s inequality. for an arbitrary random variable y and any a > 0:
1337,1,"['density function', 'probability density function', 'continuous', 'case', 'discrete', 'probability', 'function', 'inequality']", Chebyshevs inequality,seg_177,we shall derive this inequality for continuous y (the discrete case is similar). let fy be the probability density function of y . let µ denote e[y ]. then:
1338,1,['inequality'], Chebyshevs inequality,seg_177,"dividing both sides of the resulting inequality by a2, we obtain chebyshev’s inequality."
1339,1,"['standard deviations', 'deviations', 'expectation', 'probability', 'standard']", Chebyshevs inequality,seg_177,denote var(y ) by σ2 and consider the probability that y is within a few standard deviations from its expectation µ:
1340,1,['inequality'], Chebyshevs inequality,seg_177,"where k is a small integer. setting a = kσ in chebyshev’s inequality, we find"
1341,1,"['inequality', 'distributions']", Chebyshevs inequality,seg_177,"for k = 2, 3, 4 the right-hand side is 3/4, 8/9, and 15/16, respectively. this suggests that with chebyshev’s inequality we can make very strong statements. for most distributions, however, the actual value of p(|y − µ| < kσ) is even higher than the lower bound (13.1). we summarize this as a somewhat loose rule."
1342,1,"['standard', 'standard deviations', 'deviations', 'variable', 'random variable', 'probability', 'random']", Chebyshevs inequality,seg_177,the “µ ± a few σ” rule. most of the probability mass of a random variable is within a few standard deviations from its expectation.
1343,1,"['distribution', 'inequality']", Chebyshevs inequality,seg_177,"quick exercise 13.2 calculate p(|y − µ| < kσ) exactly for k = 1, 2, 3, 4 when y has an exp(1) distribution and compare this with the bounds from chebyshev’s inequality."
1344,1,"['independent', 'variables', 'inequality', 'expectation', 'variance', 'independent and identically distributed sequence', 'average']", The law of large numbers,seg_179,"we return to the independent and identically distributed sequence of random variables x1, x2, . . . with expectation µ and variance σ2. we apply chebyshev’s inequality to the average x̄n, where we use e[x̄n] = µ and"
1345,0,['n'], The law of large numbers,seg_179,"the right-hand side vanishes as n goes to infinity, no matter how small ε is. this proves the following law."
1346,1,"['random variables', 'independent', 'variables', 'law of large numbers', 'independent random variables', 'expectation', 'variance', 'random', 'average']", The law of large numbers,seg_179,"the law of large numbers. if x̄n is the average of n independent random variables with expectation µ and variance σ2, then for any ε > 0: lim p(|x̄n − µ| > ε) = 0. n→∞"
1347,1,['experimental'], The law of large numbers,seg_179,a connection with experimental work
1348,1,"['experimental', 'gamma', 'estimate', 'law of large numbers', 'distribution', 'expectation', 'measurements', 'vary', 'experiments', 'average', 'gamma distribution']", The law of large numbers,seg_179,"let us try to interpret the law of large numbers from an experimenter’s perspective. imagine you conduct a series of experiments. the experimental setup is complicated and your measurements vary quite a bit around the “true” value you are after. suppose (unknown to you) your measurements have a gamma distribution, and its expectation is what you want to determine. you decide to do a certain number of measurements, say n, and to use their average as your estimate of the expectation."
1349,1,"['simulation', 'results', 'distribution', 'expectation', 'measurements', 'average']", The law of large numbers,seg_179,"we can simulate all this, and figure 13.2 shows the results of a simulation, where we chose the same gam(2, 1) distribution, i.e., with expectation µ = 2. we anticipated that you might want to do as many as 500 measurements, so we generated realizations for x1, x2, . . . , x500. for each n we computed the average of the first n values and plotted these averages against n in figure 13.2."
1350,1,"['simulation', 'expectation', 'probability', 'average', 'limit']", The law of large numbers,seg_179,"if your decision is to do 200 repetitions, you would find (in this simulation) a value of about 2.09 (slightly too high, but you wouldn’t know!), whereas with n = 400 you would be almost exactly correct with 1.99, and with n = 500 again a little farther away with 2.06. for another sequence of realizations, the details in the pattern that you see in figure 13.2 would be different, but the general dampening of the oscillations would still be present. this follows from what we saw earlier, that as n is larger, the probability for the average to be within a certain distance of the expectation increases, in the limit even to 1. in practice it may happen that with a large number of repetitions your average is farther from the “true” value than with a smaller number of repetitions—if it is, then you had bad luck, because the odds are in your favor."
1351,1,[], The law of large numbers,seg_179,the averages may fail to converge
1352,1,"['pareto distributions', 'case', 'probability', 'random', 'simulation', 'symmetry', 'distributions', 'densities', 'gamma', 'distribution', 'expectation', 'pareto', 'average', 'random variables', 'variables', 'cauchy', 'law of large numbers', 'tails', 'realization']", The law of large numbers,seg_179,"the law of large numbers is valid if the expectation of the distribution f is finite. this is not always the case. for example, the cauchy and some pareto distributions have heavy tails: their probability densities do go to 0 as x becomes large, but (too) slowly.2 on the left in figure 13.3 you see the result of a simulation with cau(2, 1) random variables. as in the gamma case, the averages tend to go toward 2 (which is the point of symmetry of the cau(2, 1) density), but once in a while a very large (positive or negative) realization of an xi throws off the average."
1353,1,"['plot', 'simulation', 'expectation', 'level', 'average']", The law of large numbers,seg_179,"on the right in figure 13.3 the result of a simulation with a par(0.99) distribution is shown. its expectation is infinite. in the plot we see segments where the average “drifts downward,” separated by upward jumps, which correspond to xi with extremely large values. the effect of the jumps dominates: it can be shown that x̄n grows beyond any level."
1354,1,"['simulations', 'results', 'simulation']", The law of large numbers,seg_179,"you might think that these patterns are phenomena that occur because of the short length of the simulation and that in longer simulations they would disappear after some value of n. however, the patterns as described will continue to occur and the results of a longer simulation, say to n = 5000, would not look any “better.”"
1355,1,"['random variables', 'independent', 'variables', 'distribution function', 'distribution', 'random variable', 'variable', 'expectation', 'independent random variables', 'probability', 'random', 'function', 'realization']", Consequences of the law of large numbers,seg_181,"we continue with the sequence x1, x2, . . . of independent random variables with distribution function f . in the previous section we saw how we could recover the (unknown) expectation µ from a realization of the sequence. we shall see that in fact we can recover any feature of the probability distribution. in order to avoid unnecessary indices, as in e[x1] and p(x1 ∈ c), we introduce an additional random variable x that also has f as its distribution function."
1356,1,"['probability of an event', 'probability', 'event']", Consequences of the law of large numbers,seg_181,recovering the probability of an event
1357,1,"['probability of an event', 'probability', 'event']", Consequences of the law of large numbers,seg_181,"suppose that, rather than being interested in µ = e[x ], we want to know the probability of an event, for example,"
1358,1,"['estimate', 'relative frequency', 'set', 'frequency', 'probability', 'event']", Consequences of the law of large numbers,seg_181,"if you do not know this probability p, you would probably estimate it from how often the event {xi ∈ c} occurs in the sequence. you would use the relative frequency of xi ∈ c among x1, . . . , xn: the number of times the set c was hit divided by n. define for each i:"
1359,1,"['indicator random variable', 'random variable', 'variable', 'expectation', 'set', 'random', 'indicator', 'event']", Consequences of the law of large numbers,seg_181,"the random variable yi indicates whether the corresponding xi hits the set c; it is called an indicator random variable. in general, an indicator random variable for an event a is a random variable that is 1 when a occurs and 0 when ac occurs. using this terminology, yi is the indicator random variable of the event xi ∈ c. its expectation is given by"
1360,1,"['random variables', 'independent', 'variables', 'relative frequency', 'independence', 'frequency', 'random']", Consequences of the law of large numbers,seg_181,"using the yi, the relative frequency is expressed as (y1+y2+· · ·+yn)/n = ȳn. note that the random variables y1, y2, . . . are independent; the xi form an independent sequence, and yi is determined from xi only (this is an application of the rule about propagation of independence; see page 126)."
1361,1,"['random variables', 'independent', 'variables', 'law of large numbers', 'independent random variables', 'expectation', 'variance', 'random', 'average']", Consequences of the law of large numbers,seg_181,"the law of large numbers, with p in the role of µ, can now be applied to ȳn; it is the average of n independent random variables with expectation p and variance p(1 − p), so"
1362,1,"['probability', 'estimate']", Consequences of the law of large numbers,seg_181,"for any ε > 0. by reasoning along the same lines as in the previous section, we see that from a long sequence of realizations we can get an accurate estimate of the probability p."
1363,1,"['function', 'density function', 'probability density function', 'probability']", Consequences of the law of large numbers,seg_181,recovering the probability density function
1364,1,"['density function', 'probability density function', 'continuous', 'case', 'probability', 'function']", Consequences of the law of large numbers,seg_181,"consider the continuous case, where f is the probability density function corresponding with f , and now choose c = (a − h, a + h], for some (small) positive h. by equation (13.2), for large n:"
1365,1,"['probability', 'estimate']", Consequences of the law of large numbers,seg_181,this relationship suggests to estimate the probability density in a as follows:
1366,1,"['interval', 'estimate', 'results', 'plotting']", Consequences of the law of large numbers,seg_181,"in figure 13.4 we have done so for h = 0.25 and two values of a: 2 and 4. rather than plotting the estimate in just one point, we use the same value for the whole interval (a− h, a + h]. this results in a vertical bar, whose area corresponds to ȳn:"
1367,1,"['random variables', 'independent', 'variables', 'estimates', 'random']", Consequences of the law of large numbers,seg_181,"these estimates are based on the realizations of 500 independent gam(2, 1) distributed random variables. in order to be able to see how well things came"
1368,1,"['function', 'density function', 'estimate']", Consequences of the law of large numbers,seg_181,"out, the gam(2, 1) density function is shown as well; near a = 2 the estimate is very accurate, but around a = 4 it is a little too low."
1369,1,"['histogram', 'estimated', 'bar graph', 'sets']", Consequences of the law of large numbers,seg_181,"there really is no reason to derive estimated values around just a few points, as is done in figure 13.4. we might as well cover the whole x-axis with a grid (with grid size 2h) and do the computation for each point in the grid, thus covering the axis with a series of bars. the resulting bar graph is called a histogram. figure 13.5 shows the result for two sets of realizations."
1370,1,"['set', 'histograms']", Consequences of the law of large numbers,seg_181,"the top graph is constructed from the same realizations as figure 13.4 and the bottom graph is constructed from a new set of realizations. both graphs match the general shape of the density, with some bumps and valleys that are particular for the corresponding set of realizations. in chapters 15 and 17 we shall return to histograms and treat them more elaborately."
1371,1,['histogram'], Consequences of the law of large numbers,seg_181,quick exercise 13.3 the height of the bar at x = 2 in the first histogram is 0.26. how many of the 500 realizations were between 1.75 and 2.25?
1372,0,[], Solutions to the quick exercises,seg_183,13.1 the answers you have found should be in the neighborhood of the following exact values:
1373,1,['distribution'], Solutions to the quick exercises,seg_183,13.2 because y has an exp(1) distribution µ = 1 and var(y ) = σ2 = 1; we find for k ≥ 1:
1374,0,[], Solutions to the quick exercises,seg_183,using this formula and (13.1) we obtain the following numbers:
1375,1,[], Solutions to the quick exercises,seg_183,"13.3 the value of ȳn for this bar equals its area 0.26 · 0.5 = 0.13. the bar represents 13% of the values, or 0.13 · 500 = 65 realizations."
1376,1,"['distribution', 'table', 'distributions']", Exercises,seg_185,"13.1 verify the “µ±a few σ” rule as you did in quick exercise 13.2 for the following distributions: u(−1, 1), u(−a, a), n(0, 1), n(µ, σ2), par(3), geo(1/2). construct a table as in the answer to the quick exercise and enter a line for each distribution."
1377,1,"['model', 'independent', 'variables', 'errors']", Exercises,seg_185,"13.2 an accountant wants to simplify his bookkeeping by rounding amounts to the nearest integer, for example, rounding 99.53 and 100.46 both to 100. what is the cumulative effect of this if there are, say, 100 amounts? to study this we model the rounding errors by 100 independent u(−0.5, 0.5) random variables x1, x2, . . . , x100."
1378,1,"['expectation', 'variance']", Exercises,seg_185,a. compute the expectation and the variance of the xi.
1379,1,"['probability', 'inequality']", Exercises,seg_185,b. use chebyshev’s inequality to compute an upper bound for the probability
1380,1,['error'], Exercises,seg_185,p(|x1 + x2 + · · · + x100| > 10) that the cumulative rounding error x1 + x2 + · · · + x100 exceeds 10.
1381,1,"['error', 'mean']", Exercises,seg_185,13.3 consider the situation of the previous exercise. a manager wants to know what happens to the mean absolute error n
1382,1,['law of large numbers'], Exercises,seg_185,"n =1 |xi| as n becomes large. what can you say about this, applying the law of large numbers?"
1383,1,"['indicator random variable', 'model', 'random variables', 'random', 'independent', 'variables', 'distribution', 'random variable', 'variable', 'event', 'indicator']", Exercises,seg_185,"13.4 of the voters in florida, a proportion p will vote for candidate g, and a proportion 1− p will vote for candidate b. in an election poll a number of voters are asked for whom they will vote. let xi be the indicator random variable for the event “the ith person interviewed will vote for g.” a model for the election poll is that the people to be interviewed are selected in such a way that the indicator random variables x1, x2,. . . are independent and have a ber(p) distribution."
1384,1,['inequality'], Exercises,seg_185,"a. suppose we use x̄n to predict p. according to chebyshev’s inequality, how"
1385,1,['probability'], Exercises,seg_185,"large should n be (how many people should be interviewed) such that the probability that x̄n is within 0.2 of the “true” p is at least 0.9? hint: solve this first for p = 1/2, and use that p(1 − p) ≤ 1/4 for all 0 ≤ p ≤ 1."
1386,1,['probability'], Exercises,seg_185,"c. answer the question from part a, but now the probability should be at"
1387,1,['probability'], Exercises,seg_185,"find an n (as small as you can) such that the probability that you predict correctly is at least 0.9, if in fact p = 0.6."
1388,1,"['sample', 'model', 'random variables', 'measurement', 'variables', 'samples', 'random', 'measurement error', 'error']", Exercises,seg_185,"13.5 you are trying to determine the melting point of a new material, of which you have a large number of samples. for each sample that you measure you find a value close to the actual melting point c but corrupted with a measurement error. we model this with random variables:"
1389,1,"['random variables', 'independent', 'variables', 'samples', 'measurements', 'random', 'random error', 'average', 'error', 'inequality']", Exercises,seg_185,"where mi is the measured value in degree kelvin, and ui is the occurring random error. it is known that e[ui] = 0 and var(ui) = 3, for each i, and that we may consider the random variables m1, m2, . . . independent. according to chebyshev’s inequality, how many samples do you need to measure to be 90% sure that the average of the measurements is within half a degree of c?"
1390,1,['probability'], Exercises,seg_185,"13.6 the casino la bella fortuna is for sale and you think you might want to buy it, but you want to know how much money you are going to make. all the present owner can tell you is that the roulette game red or black is played about 1000 times a night, 365 days a year. each time it is played you have probability 19/37 of winning the player’s bet of 1 and probability 18/37 of having to pay the player 1."
1391,1,['law of large numbers'], Exercises,seg_185,"explain in detail why the law of large numbers can be used to determine the income of the casino, and determine how much it is."
1392,1,"['random variables', 'independent and identically distributed random variables', 'independent', 'variables', 'random', 'function', 'distributions']", Exercises,seg_185,"13.7 let x1, x2, . . . be a sequence of independent and identically distributed random variables with distributions function f . define fn as follows: for any a"
1393,1,"['random variables', 'variables', 'law of large numbers', 'expectation', 'random', 'indicator', 'variance']", Exercises,seg_185,consider a fixed and introduce the appropriate indicator random variables (as in section 13.4). compute their expectation and variance and show that the law of large numbers tells us that
1394,1,"['density function', 'histogram', 'probability density function', 'probability', 'function']", Exercises,seg_185,"13.8 in section 13.4 we described how the probability density function could be recovered from a sequence x1, x2, x3, . . . . we consider the gam(2, 1) probability density discussed in the main text and a histogram bar"
1395,1,['estimate'], Exercises,seg_185,"−2 around the point a = 2. then f(a) = f(2) = 2e = 0.27 and the estimate for f(2) is ȳn/2h, where ȳn as in (13.3)."
1396,1,"['deviation', 'standard deviation', 'standard']", Exercises,seg_185,a. express the standard deviation of ȳn/2h in terms of n and h.
1397,1,"['probability', 'estimate']", Exercises,seg_185,"equality) so that the estimate is within 20% of the “true value”, with probability 80%?"
1398,1,"['independent', 'random']", Exercises,seg_185,"13.9 let x1, x2, . . . be an independent sequence of u(−1, 1) random"
1399,0,['n'], Exercises,seg_185,variables and let tn = n
1400,0,[], Exercises,seg_185,a. explain how this could be true.
1401,1,"['random variables', 'independent', 'variables', 'random']", Exercises,seg_185,"13.10 let mn be the maximum of n independent u(0, 1) random variables."
1402,1,"['law of large numbers', 'inequality']", Exercises,seg_185,shev’s inequality or the law of large numbers?
1403,1,"['probabilities', 'random variable', 'variable', 'random']", Exercises,seg_185,"13.11 for some t > 1, let x be a random variable taking the values 0 and t, with probabilities"
1404,1,['probability'], Exercises,seg_185,then e[x ] = 1 and var(x) = t− 1. consider the probability p(|x − 1| > a).
1405,1,"['probability', 'inequality']", Exercises,seg_185,chebyshev’s inequality gives an upper bound for this probability of 9/64. the difference is 9/64 − 1/10 ≈ 0.04. we will say that for t = 10 the chebyshev gap for x at a = 8 is 0.04.
1406,0,[], Exercises,seg_185,"c. can you find a gap smaller than 0.01, smaller than 0.001, smaller than"
1407,1,['inequality'], Exercises,seg_185,"d. do you think one could improve chebyshev’s inequality, i.e., find an upper"
1408,1,['probabilities'], Exercises,seg_185,bound closer to the true probabilities?
1409,1,"['random variables', 'independent', 'variables', 'law of large numbers', 'independent random variables', 'random']", Exercises,seg_185,"13.12 (a more general law of large numbers). let x1, x2, . . . be a sequence of independent random variables, with e[xi] = µi and var(xi) = σi2, for i = 1, 2, . . . . suppose that 0 < σi2 ≤ m , for all i. let a be an arbitrary positive number."
1410,1,['inequality'], Exercises,seg_185,a. apply chebyshev’s inequality to show that
1411,0,[], Exercises,seg_185,b. conclude from a that
1412,1,"['law of large numbers', 'case']", Exercises,seg_185,check that the law of large numbers is a special case of this result.
1413,1,"['random variables', 'probabilities', 'independent', 'variables', 'law of large numbers', 'distribution', 'normal', 'central limit theorem', 'variance', 'random', 'average', 'limit', 'normal distribution']", The central limit theorem,seg_187,"the central limit theorem is a refinement of the law of large numbers. for a large number of independent identically distributed random variables x1, . . . , xn, with finite variance, the average x̄n approximately has a normal distribution, no matter what the distribution of the xi is. in the first section we discuss the proper normalization of x̄n to obtain a normal distribution in the limit. in the second section we will use the central limit theorem to approximate probabilities of averages and sums of random variables."
1414,1,"['probability', 'random', 'function', 'independent random variables', 'distributions', 'density function', 'probability density function', 'distribution', 'expected value', 'expectation', 'convergence', 'variance', 'bimodal', 'random variables', 'independent', 'variables', 'law of large numbers', 'average']", Standardizing averages,seg_189,"in the previous chapter we saw that the law of large numbers guarantees the convergence to µ of the average x̄n of n independent random variables x1, . . . , xn, all having the same expectation µ and variance σ2. this convergence was illustrated by figure 13.1. closer examination of this figure suggests another phenomenon: for the two distributions considered (i.e., the gam(2, 1) distribution and a bimodal distribution), the probability density function of x̄n seems to become symmetrical and bell shaped around the expected value µ as n becomes larger and larger. however, the bell collapses into a single spike at µ. nevertheless, by a proper normalization it is possible to stabilize the bell shape, as we will see."
1415,1,"['density function', 'random variables', 'probability density function', 'variables', 'distribution', 'expectation', 'variance', 'random', 'function', 'probability', 'average']", Standardizing averages,seg_189,"in order to let the distribution of x̄n settle down it seems to be a good idea to stabilize the expectation and variance. since e[x̄n] = µ for all n, only the variance needs some special attention. in figure 14.1 we depict the probability density function of the centered average x̄n−µ of gam(2, 1) random variables, multiplied by three different powers of n. in the left column we display the density of n 4"
1416,1,['factor'], Standardizing averages,seg_189,in the right column the density of n(x̄n − µ). these figures suggest that √n is the right factor to stabilize the bell shape.
1417,1,"['average', 'variance']", Standardizing averages,seg_189,"indeed, according to the rule for the variance of an average (see page 182), we have var(x̄n) = σ2/n, and therefore for any number c:"
1418,1,"['random', 'variance']", Standardizing averages,seg_189,"to stabilize the variance we therefore must choose c = √n. in fact, by choosing c = √n/σ, one standardizes the averages, i.e., the resulting random variable zn, defined by"
1419,1,"['random variables', 'variables', 'expected value', 'random', 'variance']", Standardizing averages,seg_189,has expected value 0 and variance 1. what more can we say about the distribution of the random variables zn?
1420,1,"['states', 'case', 'probability', 'random', 'function', 'probability density functions', 'functions', 'density function', 'probability density function', 'gamma', 'density functions', 'distribution', 'expectation', 'variance', 'bimodal', 'random variables', 'independent', 'variables']", Standardizing averages,seg_189,"in case x1, x2, . . . are independent n(µ, σ2) distributed random variables, we know from section 11.2 and the rule on expectation and variance under change of units (see page 98), that zn has an n(0, 1) distribution for all n. for the gamma and bimodal random variables from section 13.1 we depicted the probability density function of zn in figure 14.2. for both examples we see that the probability density functions of the zn seem to converge to the probability density function of the n(0, 1) distribution, indicated by the dotted line. the following amazing result states that this behavior generally occurs no matter what distribution we start with."
1421,1,"['random variables', 'independent', 'variables', 'expected value', 'central limit theorem', 'random', 'variance', 'limit']", Standardizing averages,seg_189,"the central limit theorem. let x1, x2, . . . be any sequence of independent identically distributed random variables with finite positive variance. let µ be the expected value and σ2 the variance of each of the xi. for n ≥ 1, let zn be defined by"
1422,0,[], Standardizing averages,seg_189,then for any number a
1423,1,"['standard normal', 'distribution function', 'distribution', 'normal', 'standard', 'function', 'standard normal distribution', 'normal distribution']", Standardizing averages,seg_189,"where φ is the distribution function of the n(0, 1) distribution. in words: the distribution function of zn converges to the distribution function φ of the standard normal distribution."
1424,1,"['average', 'standardized']", Standardizing averages,seg_189,which is a more direct way to see that zn is the average x̄n standardized.
1425,1,['standardized'], Standardizing averages,seg_189,one can also write zn as a standardized sum
1426,1,"['random variables', 'probabilities', 'independent', 'variables', 'random']", Standardizing averages,seg_189,in the next section we will see that this last representation of zn is very helpful when one wants to approximate probabilities of sums of independent identically distributed random variables.
1427,1,"['densities', 'random variables', 'variables', 'distribution', 'normal', 'probability', 'random']", Standardizing averages,seg_189,"it follows that x̄n approximately has an n(µ, σ2/n) distribution; see the change-of-units rule for normal random variables on page 106. this explains the symmetrical bell shape of the probability densities in figure 13.1."
1428,1,"['random variables', 'probabilities', 'independent', 'variables', 'distribution', 'probability distribution', 'central limit theorem', 'probability', 'random', 'average', 'limit']", Applications of the central limit theorem,seg_191,"the central limit theorem provides a tool to approximate the probability distribution of the average or the sum of independent identically distributed random variables. this plays an important role in applications, for instance, see sections 23.4, 24.1, 26.2, and 27.2. here we will illustrate the use of the central limit theorem to approximate probabilities of averages and sums of random variables in three examples. the first example deals with an average; the other two concern sums of random variables."
1429,0,[], Applications of the central limit theorem,seg_191,did we have bad luck?
1430,1,"['random variables', 'independent', 'simulation', 'variables', 'approximation', 'expected value', 'central limit theorem', 'simulated', 'probability', 'random', 'limit']", Applications of the central limit theorem,seg_191,"in the example in section 13.3 averages of independent gam(2, 1) distributed random variables were simulated for n = 1, . . . , 500. in figure 13.2 the realization of x̄n for n = 400 is 1.99, which is almost exactly equal to the expected value 2. for n = 500 the simulation was 2.06, a little bit farther away. did we have bad luck, or is a value 2.06 or higher not unusual? to answer this question we want to compute p(x̄n ≥ 2.06). we will find an approximation of this probability using the central limit theorem."
1431,1,"['random variables', 'variables', 'random']", Applications of the central limit theorem,seg_191,"since the xi are gam(2, 1) random variables, µ = e[xi] = 2 and σ2 = var(xi) = 2. we find for n = 500 that"
1432,1,"['limit', 'central limit theorem']", Applications of the central limit theorem,seg_191,it now follows from the central limit theorem that
1433,1,['probability'], Applications of the central limit theorem,seg_191,"this is close to the exact answer 0.1710881, which was obtained using the probability density of x̄n as given in section 13.1."
1434,1,"['average', 'probability']", Applications of the central limit theorem,seg_191,"thus we see that there is about a 17% probability that the average x̄500 is at least 0.06 above 2. since 17% is quite large, we conclude that the value 2.06 is not unusual. in other words, we did not have bad luck; n = 500 is simply not large enough to be that close. would 2.06 be unusual if n = 5000?"
1435,1,"['limit', 'central limit theorem']", Applications of the central limit theorem,seg_191,"quick exercise 14.1 show that p(x̄5000 ≥ 2.06) ≈ 0.0013, using the central limit theorem."
1436,0,[], Applications of the central limit theorem,seg_191,rounding amounts to the nearest integer
1437,1,"['probability', 'inequality']", Applications of the central limit theorem,seg_191,"in exercise 13.2 an accountant wanted to simplify his bookkeeping by rounding amounts to the nearest integer, and you were asked to use chebyshev’s inequality to compute an upper bound for the probability"
1438,1,"['approximation', 'distribution', 'central limit theorem', 'error', 'limit']", Applications of the central limit theorem,seg_191,"that the cumulative rounding error x1 + x2 + · · · + x100 exceeds 10. this upper bound equals 1/12. in order to know the exact value of p one has to determine the distribution of the sum x1+ · · ·+x100. this is difficult, but the central limit theorem is a handy tool to get an approximation of p. clearly,"
1439,1,['probability'], Applications of the central limit theorem,seg_191,"standardizing as in (14.1), for the second probability we write, with n = 100"
1440,1,"['random variables', 'variables', 'random']", Applications of the central limit theorem,seg_191,"the xi are u(−0.5, 0.5) random variables, µ = e[xi] = 0, and σ2 = var(xi) = 1/12, so that"
1441,1,"['limit', 'central limit theorem']", Applications of the central limit theorem,seg_191,it follows from the central limit theorem that
1442,1,"['approximation', 'distribution', 'binomial', 'binomial distribution']", Applications of the central limit theorem,seg_191,normal approximation of the binomial distribution
1443,1,['probability'], Applications of the central limit theorem,seg_191,"in section 4.3 we considered the (fictitious) situation that you attend, completely unprepared, a multiple-choice exam consisting of 10 questions. we saw that the probability you will pass equals"
1444,1,['independent'], Applications of the central limit theorem,seg_191,where x—being the sum of 10 independent ber(1
1445,1,['random'], Applications of the central limit theorem,seg_191,"4 ) random variables—has a bin(10, 1"
1446,1,"['random variables', 'independent', 'variables', 'approximation', 'distribution', 'random variable', 'variable', 'central limit theorem', 'random', 'limit']", Applications of the central limit theorem,seg_191,"4 ) distribution. as we saw in chapter 4 it is rather easy, but tedious, to calculate p(x ≥ 6). although n is small, we investigate what the central limit theorem will yield as an approximation of p(x ≥ 6). recall that a random variable with a bin(n, p) distribution can be written as the sum of n independent ber(p) distributed random variables r1, . . . , rn. substituting n = 10, µ = p = 1/4, and σ2 = p(1 − p) = 3/16, it follows from the central limit theorem that"
1447,1,['approximation'], Applications of the central limit theorem,seg_191,"the number 0.0052 is quite a poor approximation for the true value 0.0197. note however, that we could also argue that"
1448,1,['approximation'], Applications of the central limit theorem,seg_191,"which gives an approximation that is too large! a better approach lies somewhere in the middle, as the following quick exercise illustrates."
1449,1,"['limit', 'central limit theorem']", Applications of the central limit theorem,seg_191,quick exercise 14.2 apply the central limit theorem to find 0.0143 as an approximation to p(x ≥ 5 1
1450,0,[], Applications of the central limit theorem,seg_191,"1), this also provides"
1451,1,['approximation'], Applications of the central limit theorem,seg_191,an approximation of p(x ≥ 6).
1452,0,['n'], Applications of the central limit theorem,seg_191,how large should n be?
1453,1,"['approximation', 'discrete', 'distribution', 'asymmetric', 'normal', 'bimodal', 'discrete distribution', 'central limit theorem', 'continuous', 'convergence', 'limit', 'normal distribution']", Applications of the central limit theorem,seg_191,"in view of the previous examples one might raise the question of how large n should be to have a good approximation when using the central limit theorem. in other words, how fast is the convergence to the normal distribution? this is a difficult question to answer in general. for instance, in the third example one might initially be tempted to think that the approximation was quite poor, but after taking the fact into account that we approximate a discrete distribution by a continuous one we obtain a considerable improvement of the approximation, as was illustrated in quick exercise 14.2. for another example, see figure 14.2. here we see that the convergence is slightly faster for the bimodal distribution than for the gam(2, 1) distribution, which is due to the fact that the gam(2, 1) is rather asymmetric."
1454,1,"['approximation', 'asymmetric', 'bimodal', 'discrete']", Applications of the central limit theorem,seg_191,"in general the approximation might be poor when n is small, when the distribution of the xi is asymmetric, bimodal, or discrete, or when the value a in"
1455,1,['distribution'], Applications of the central limit theorem,seg_191,is far from the center of the distribution of the xi.
1456,1,"['limit', 'central limit theorem']", Solutions to the quick exercises,seg_193,"14.1 in the same way we approximated p(x̄n ≥ 2.06) using the central limit theorem, we have that"
1457,1,"['central limit theorem', 'mean', 'probability', 'limit']", Solutions to the quick exercises,seg_193,"which is approximately equal to 1−φ(3) = 0.0013, thanks to the central limit theorem. because we think that 0.13% is a small probability, to find 2.06 as a value for x̄5000 would mean that you really had bad luck!"
1458,1,"['limit', 'probability', 'central limit theorem']", Solutions to the quick exercises,seg_193,"we have seen that using the central limit theorem to approximate p(x ≥ 6) gives an underestimate of this probability, while using the central limit theorem to p(x > 5) gives an overestimation. since 52"
1459,0,[], Solutions to the quick exercises,seg_193,approximation will be better.
1460,1,"['random variables', 'independent', 'variables', 'expected value', 'central limit theorem', 'random', 'variance', 'limit']", Exercises,seg_195,"14.1 let x1, x2, . . . , x144 be independent identically distributed random variables, each with expected value µ = e[xi] = 2, and variance σ2 = var(xi) = 4. approximate p(x1 + x2 + · · · + x144 > 144), using the central limit theorem."
1461,1,"['density function', 'random variables', 'probability density function', 'independent', 'variables', 'probability', 'random', 'function']", Exercises,seg_195,"14.2 let x1, x2, . . . , x625 be independent identically distributed random variables, with probability density function f given by"
1462,1,"['limit', 'central limit theorem']", Exercises,seg_195,use the central limit theorem to approximate p(x1 + x2 + · · · + x625 < 170).
1463,1,"['central limit theorem', 'limit', 'probability', 'inequality']", Exercises,seg_195,14.3 in exercise 13.4 a you were asked to use chebyshev’s inequality to determine how large n should be (how many people should be interviewed) so that the probability that x̄n is within 0.2 of the “true” p is at least 0.9. here p is the proportion of the voters in florida who will vote for g (and 1 − p is the proportion of the voters who will vote for b). how large should n at least be according to the central limit theorem?
1464,1,"['model', 'normal approximation', 'random variables', 'independent', 'variables', 'approximation', 'normal', 'probability', 'random']", Exercises,seg_195,"14.4 in the single-server queue model from section 6.4, ti is the time between the arrival of the (i − 1)th and ith customers. furthermore, one of the model assumptions is that the ti are independent, exp(0.5) distributed random variables. in section 11.2 we saw that the probability p(t1 + · · · + t30 ≤ 60) of the 30th customer arriving within an hour at the well is equal to 0.542. find the normal approximation of this probability."
1465,1,"['random variable', 'variable', 'random']", Exercises,seg_195,"14.5 let x be a bin(n, p) distributed random variable. show that the"
1466,1,['variable'], Exercises,seg_195,random variable x − np
1467,1,"['distribution', 'standard normal', 'normal', 'standard']", Exercises,seg_195,has a distribution that is approximately standard normal.
1468,1,"['random variable', 'variable', 'random']", Exercises,seg_195,"14.6 again, as in the previous exercise, let x be a bin(n, p) distributed random variable."
1469,1,"['approximation', 'limit', 'central limit theorem']", Exercises,seg_195,and p = 1/4. use the central limit theorem to give an approximation of p(x ≤ 25) and p(x < 26).
1470,1,"['approximation', 'probability']", Exercises,seg_195,limit theorem to give an approximation of this probability.
1471,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random', 'variance', 'inequality']", Exercises,seg_195,"14.7 let x1, x2, . . . , xn be n independent random variables, each with expected value µ and finite positive variance σ2. use chebyshev’s inequality to show that for any a > 0 one has"
1472,0,[], Exercises,seg_195,use this fact to explain the occurrence of a single spike in the left column of figure 14.1.
1473,1,"['random variables', 'independent', 'variables', 'random variable', 'variable', 'random']", Exercises,seg_195,"14.8 let x1, x2, . . . be a sequence of independent n(0, 1) distributed random variables. for n = 1, 2, . . . , let yn be the random variable, defined by"
1474,0,[], Exercises,seg_195,b. one can show—using integration by parts—that e[xi4] = 3. deduce from
1475,1,"['limit', 'central limit theorem']", Exercises,seg_195,c. use the central limit theorem to approximate p(y100 > 110).
1476,1,"['expected value', 'random variable', 'variable', 'random', 'variance']", Exercises,seg_195,"14.9 a factory produces links for heavy metal chains. the research lab of the factory models the length (in cm) of a link by the random variable x , with expected value e[x ] = 5 and variance var(x) = 0.04. the length of a link is defined in such a way that the length of a chain is equal to the sum of"
1477,1,[], Exercises,seg_195,"the lengths of its links. the factory sells chains of 50 meters; to be on the safe side 1002 links are used for such chains. the factory guarantees that the chain is not shorter than 50 meters. if by chance a chain is too short, the customer is reimbursed, and a new chain is given for free."
1478,1,"['probability', 'estimate']", Exercises,seg_195,a. give an estimate of the probability that for a chain of at least 50 meters
1479,1,['percentage'], Exercises,seg_195,more than 1002 links are needed. for what percentage of the chains does the factory have to reimburse clients and provide free chains?
1480,1,[], Exercises,seg_195,b. the sales department of the factory notices that it has to hand out a
1481,1,['expectation'], Exercises,seg_195,"lot of free chains and asks the research lab what is wrong. after further investigations the research lab reports to the sales department that the expectation value 5 is incorrect, and that the correct value is 4.99 (cm). do you think that it was necessary to report such a minor change of this value?"
1482,1,"['sample', 'measurements', 'average', 'inequality']", Exercises,seg_195,14.10 chebyshev’s inequality was used in exercise 13.5 to determine how many times n one needs to measure a sample to be 90% sure that the average of the measurements is within half a degree of the actual melting point c of a new material.
1483,1,"['normal approximation', 'approximation', 'normal']", Exercises,seg_195,a. use the normal approximation to find a less conservative value for n.
1484,1,"['random errors', 'case', 'errors', 'measurements', 'normal', 'random']", Exercises,seg_195,b. only in case the random errors ui in the measurements have a normal
1485,1,['cases'], Exercises,seg_195,"distribution the value of n from a is “exact,” in all other cases an approximation. explain this."
1486,1,"['model', 'experiment', 'dataset', 'observations', 'set', 'associated', 'probability', 'random']", Exploratory data analysis graphical summaries,seg_197,"in the previous chapters we focused on probability models to describe random phenomena. confronted with a new phenomenon, we want to learn about the randomness that is associated with it. it is common to conduct an experiment for this purpose and record observations concerning the phenomenon. the set of observations is called a dataset. by exploring the dataset we can gain insight into what probability model suits the phenomenon."
1487,1,"['histogram', 'estimates', 'measurements', 'function', 'graphical', 'dataset', 'kernel', 'bivariate dataset', 'scatterplot', 'data', 'kernel density estimates', 'bivariate']", Exploratory data analysis graphical summaries,seg_197,"frequently you will have to deal with a dataset that contains so many elements that it is necessary to condense the data for easy visual comprehension of general characteristics. in this chapter we present several graphical methods to do so. to graphically represent univariate datasets, consisting of repeated measurements of one particular quantity, we discuss the classical histogram, the more recently introduced kernel density estimates and the empirical distribution function. to represent a bivariate dataset, which consists of repeated measurements of two quantities, we use the scatterplot."
1488,1,"['data', 'table']", Example the Old Faithful data,seg_199,"the old faithful geyser at yellowstone national park, wyoming, usa, was observed from august 1st to august 15th, 1985. during that time, data were collected on the duration of eruptions. there were 272 eruptions observed, of which the recorded durations are listed in table 15.1. the data are given in seconds."
1489,1,"['dataset', 'data', 'information', 'vary']", Example the Old Faithful data,seg_199,"the variety in the lengths of the eruptions indicates that randomness is involved. by exploring the dataset we might learn about this randomness. for instance: we like to know which durations are more likely to occur than others; is there something like “the typical duration of an eruption”; do the durations vary symmetrically around the center of the dataset; and so on. in order to retrieve this type of information, just listing the observed durations does not help us very much. somehow we must summarize the observed data. we could"
1490,1,"['dataset', 'mean', 'data', 'information']", Example the Old Faithful data,seg_199,"start by computing the mean of the data, which is 209.3 for the old faithful data. however, this is a poor summary of the dataset, because there is a lot more information in the observed durations. how do we get hold of this?"
1491,1,"['table', 'dataset', 'data', 'information']", Example the Old Faithful data,seg_199,"just staring at the dataset for a while tells us very little. to see something, we have to rearrange the data somehow. the first thing we could do is order the data. the result is shown in table 15.2. putting the elements in order already provides more information. for instance, it is now immediately clear that all elements lie between 96 and 306."
1492,1,['dataset'], Example the Old Faithful data,seg_199,quick exercise 15.1 which two elements of the old faithful dataset split the dataset in three groups of equal size?
1493,1,['data'], Example the Old Faithful data,seg_199,"a closer look at the ordered data shows that the two middle elements (the 136th and 137th elements in ascending order) are equal to 240, which is much closer to the maximum value 306 than to the minimum value 96. this seems to"
1494,1,"['plot', 'dataset', 'data', 'symmetry', 'asymmetric']", Example the Old Faithful data,seg_199,"indicate that the dataset is somewhat asymmetric, but even from the ordered dataset we cannot get a clear picture of this asymmetry. also, geologists believe that there are two different kinds of eruptions that play a role. hence one would expect two separate values around which the elements of the dataset would accumulate, corresponding to the typical durations of the two types of eruptions. again it is not clear, not even from the ordered dataset, what these two typical values are. it would be better to have a plot of the dataset that reflects symmetry or asymmetry of the data and from which we can easily see where the elements accumulate. in the following sections we will discuss two such methods."
1495,1,"['data', 'histogram', 'method']", Histograms,seg_201,"the classical method to graphically represent data is the histogram, which probably dates from the mortality studies of john graunt in 1662 (see west-"
1496,1,"['histogram', 'dataset', 'data', 'pearson', 'tables']", Histograms,seg_201,"ergaard [39], p.22). the term histogram appears to have been used first by karl pearson ([22]). figure 15.1 displays a histogram of the old faithful data. the picture immediately reveals the asymmetry of the dataset and the fact that the elements accumulate somewhere near 120 and 270, which was not clear from tables 15.1 and 15.2."
1497,1,"['histogram', 'dataset']", Histograms,seg_201,the construction of the histogram is as follows. let us denote a generic (univariate) dataset of size n by
1498,1,"['curve', 'histogram']", Histograms,seg_201,and suppose we want to construct a histogram. we use the version of the histogram that is scaled in such a way that the total area under the curve is equal to one.1
1499,1,"['bins', 'range', 'data', 'intervals']", Histograms,seg_201,first we divide the range of the data into intervals. these intervals are called bins and are denoted by
1500,1,"['bin', 'bin width', 'bins', 'histogram', 'interval', 'dataset']", Histograms,seg_201,"the length of an interval bi is denoted by |bi| and is called the bin width. the bins do not necessarily have the same width. in figure 15.1 we have eight bins of equal bin width. we want the area under the histogram on each bin bi to reflect the number of elements in bi. since the total area 1 under the histogram then corresponds to the total number of elements n in the dataset, the area under the histogram on a bin bi is equal to the proportion of elements"
1501,1,"['bin', 'histogram']", Histograms,seg_201,the height of the histogram on bin bi must then be equal to
1502,0,[], Histograms,seg_201,the number of xj in bi .
1503,1,"['bin', 'bins', 'table']", Histograms,seg_201,"quick exercise 15.2 use table 15.2 to count how many elements fall into each of the bins (90, 120], (120, 150], . . . , (300, 330] in figure 15.1 and compute the height on each bin."
1504,1,"['bin', 'bin width']", Histograms,seg_201,choice of the bin width
1505,1,"['bins', 'histogram', 'case']", Histograms,seg_201,consider a histogram with bins of equal width. in that case the bins are of the form
1506,1,"['bin', 'bin width', 'bins', 'risk', 'histogram', 'dataset', 'bin widths', 'table', 'data', 'information', 'histograms']", Histograms,seg_201,"where r is some reference point smaller than the minimum of the dataset, and b denotes the bin width. in figure 15.2, three histograms of the old faithful data of table 15.2 are displayed with bin widths equal to 2, 30, and 90, respectively. clearly, the choice of the bin width b, or the corresponding choice of the number of bins m, will determine what the resulting histogram will look like. choosing the bin width too small will result in a chaotic figure with many isolated peaks. choosing the bin width too large will result in a figure without much detail, at the risk of losing information about general characteristics. in figure 15.2, bin width b = 2 is somewhat too small. bin width b = 90 is clearly too large and produces a histogram that no longer captures the fact that the data show two separate modes near 120 and 270."
1507,1,"['bin', 'bin width', 'trial', 'error']", Histograms,seg_201,"how does one go about choosing the bin width? in practice, this might boil down to picking the bin width by trial and error, continuing until the figure looks reasonable. mathematical research, however, has provided some guidelines for a data-based choice for b or m. formulas that may effectively be"
1508,1,"['deviation', 'sample', 'sample standard deviation', 'standard', 'standard deviation']", Histograms,seg_201,"−1/3 used are m = 1 + 3.3 log10(n) (see [34]) or b = 3.49 sn (see [29]; see also remark 15.1), where s is the sample standard deviation (see section 16.2 for the definition of the sample standard deviation)."
1509,1,"['histogram', 'data']", Histograms,seg_201,quick exercise 15.3 if we construct a histogram for the old faithful data
1510,1,"['bin', 'bin width', 'bins', 'data']", Histograms,seg_201,"−1/3 with equal bin width b = 3.49 sn , how may bins will we need to cover the data if s = 68.48?"
1511,1,"['bin', 'bin width', 'bins', 'plot', 'histogram', 'method', 'discrete']", Histograms,seg_201,the main advantage of the histogram is that it is simple. its disadvantage is the discrete character of the plot. in figure 15.1 it is still somewhat unclear which two values correspond to the typical durations of the two types of eruptions. another well-known artifact is that changing the bin width slightly or keeping the bin width fixed and shifting the bins slightly may result in a figure of a different nature. a method that produces a smoother figure and is less sensitive to these kinds of changes will be discussed in the next section.
1512,1,"['plot', 'method', 'estimate', 'estimation', 'kernel', 'kernel density estimate', 'density estimation', 'data', 'kernel density estimation']", Kernel density estimates,seg_203,"we can graphically represent data in a more variegated plot by a so-called kernel density estimate. the basic ideas of kernel density estimation first appeared in the early 1950s. rosenblatt [25] and parzen [21] provided the stimulus for further research on this topic. although the method was introduced in the middle of the last century, until recently it remained unpopular as a tool for practitioners because of its computationally intensive nature."
1513,1,"['estimate', 'dataset', 'kernel', 'kernel density estimate', 'data']", Kernel density estimates,seg_203,"figure 15.3 displays a kernel density estimate of the old faithful data. again the picture immediately reveals the asymmetry of the dataset, but it is much"
1514,1,['histogram'], Kernel density estimates,seg_203,smoother than the histogram in figure 15.1. note that it is now easier to detect the two typical values around which the elements accumulate.
1515,1,"['plot', 'dataset', 'kernel', 'kernels', 'bandwidth', 'parameter', 'function']", Kernel density estimates,seg_203,"the idea behind the construction of the plot is to “put a pile of sand” around each element of the dataset. at places where the elements accumulate, the sand will pile up. the actual plot is constructed by choosing a kernel k and a bandwidth h. the kernel k reflects the shape of the piles of sand, whereas the bandwidth is a tuning parameter that determines how wide the piles of sand will be. formally, a kernel k is a function k : r → r. figure 15.4 displays several well-known kernels. a kernel k typically satisfies the following conditions:"
1516,1,['probability'], Kernel density estimates,seg_203,"(k1) k is a probability density, i.e., k(u) ≥ 0 and ∫−"
1517,1,['symmetric'], Kernel density estimates,seg_203,"(k2) k is symmetric around zero, i.e., k(u) = k(−u);"
1518,1,"['kernel', 'kernel ']", Kernel density estimates,seg_203,examples are the epanechnikov kernel :
1519,1,['kernel'], Kernel density estimates,seg_203,"and k(u) = 0 elsewhere, and the triweight kernel"
1520,1,"['kernel', 'normal', 'kernels', 'condition']", Kernel density estimates,seg_203,"and k(u) = 0 elsewhere. sometimes one uses kernels that do not satisfy condition (k3), for example, the normal kernel"
1521,1,"['estimate', 'dataset', 'kernel', 'kernel density estimate']", Kernel density estimates,seg_203,"let us denote a kernel density estimate by fn,h, and suppose that we want to construct fn,h for a dataset x1, x2, . . . , xn. in figure 15.5 the construction is"
1522,1,"['dataset', 'kernel', 'bandwidth']", Kernel density estimates,seg_203,"illustrated for a dataset containing five elements, where we use the epanechnikov kernel and bandwidth h = 0.5. first we scale the kernel k (solid line)"
1523,1,['function'], Kernel density estimates,seg_203,into the function 1 t
1524,1,"['functions', 'curve', 'interval', 'dataset', 'kernel', 'results']", Kernel density estimates,seg_203,"the scaled kernel (dotted line) is of the same type as the original kernel, with area 1 under the curve but is positive on the interval [−h, h] instead of [−1, 1] and higher (lower) when h is smaller (larger) than 1. next, we put a scaled kernel around each element xi in the dataset. this results in functions of the type"
1525,1,"['curve', 'transformed', 'interval', 'estimate', 'kernel', 'symmetric', 'kernel density estimate', 'kernels']", Kernel density estimates,seg_203,"these shifted kernels (dotted lines) have the same shape as the transformed kernel, all with area 1 under the curve, but they are now symmetric around xi and positive on the interval [xi − h, xi + h]. we see that the graphs of the shifted kernels will overlap whenever xi and xj are close to each other, so that things will pile up more at places where more elements accumulate. the kernel density estimate fn,h is constructed by summing the scaled kernels and dividing them by n, in order to obtain area 1 under the curve:"
1526,1,"['bin', 'contrast', 'histogram', 'condition', 'observations', 'probability']", Kernel density estimates,seg_203,"when computing fn,h(t), we assign higher weights to observations xi closer to t, in contrast to the histogram where we simply count the number of observations in the bin that contains t. note that as a consequence of condition (k1), fn,h itself is a probability density:"
1527,1,"['kernel', 'kernel density estimate', 'estimate']", Kernel density estimates,seg_203,"quick exercise 15.4 check that the total area under the kernel density estimate is equal to one, i.e., show that ∫−"
1528,1,['method'], Kernel density estimates,seg_203,"note that computing fn,h is very computationally intensive. its common use nowadays is therefore a typical product of the recent developments in computer hardware, despite the fact that the method was introduced much earlier."
1529,1,['bandwidth'], Kernel density estimates,seg_203,choice of the bandwidth
1530,1,"['bin', 'bin width', 'curve', 'kernel density estimates', 'risk', 'estimate', 'estimates', 'kernel', 'bandwidths', 'kernel density estimate', 'data', 'bandwidth', 'histograms']", Kernel density estimates,seg_203,"the bandwidth h plays the same role for kernel density estimates as the bin width b does for histograms. in figure 15.6 three kernel density estimates of the old faithful data are plotted with the triweight kernel and bandwidths 1.8, 18, and 180. it is clear that the choice of the bandwidth h determines largely what the resulting kernel density estimate will look like. choosing the bandwidth too small will produce a curve with many isolated peaks. choosing the bandwidth too large will produce a very smooth curve, at the risk of smoothing away important features of the data. in figure 15.6 bandwidth"
1531,1,"['estimate', 'kernel', 'kernel density estimate', 'data', 'bandwidth']", Kernel density estimates,seg_203,h = 1.8 is somewhat too small. bandwidth h = 180 is clearly too large and produces an oversmoothed kernel density estimate that no longer captures the fact that the data show two separate modes.
1532,1,"['bandwidth', 'histograms', 'trial', 'error']", Kernel density estimates,seg_203,"how does one go about choosing the bandwidth? similar to histograms, in practice one could do this by trial and error and continue until one obtains a reasonable picture. recent research, however, has provided some guidelines for a data-based choice of h. a formula that may effectively be used is h ="
1533,1,"['deviation', 'sample', 'sample standard deviation', 'standard', 'standard deviation']", Kernel density estimates,seg_203,"−1/5 1.06 sn , where s denotes the sample standard deviation (see, for instance, [31]; see also remark 15.2)."
1534,1,"['kernel', 'kernel density estimate', 'estimate']", Kernel density estimates,seg_203,quick exercise 15.5 if we construct a kernel density estimate for the old
1535,1,"['interval', 'bandwidth', 'data']", Kernel density estimates,seg_203,"−1/5 faithful data with bandwidth h = 1.06sn , then on what interval is fn,h strictly positive if s = 68.48?"
1536,1,['kernel'], Kernel density estimates,seg_203,choice of the kernel
1537,1,"['kernel density estimates', 'estimate', 'estimates', 'kernel', 'table', 'kernel density estimate', 'data', 'set', 'normal', 'kernels', 'bandwidth']", Kernel density estimates,seg_203,"to construct a kernel density estimate, one has to choose a kernel k and a bandwidth h. the choice of kernel is less important. in figure 15.7 we have plotted two kernel density estimates for the old faithful data of table 15.1: one is constructed with the triweight kernel (solid line), and one with the epanechnikov kernel (dotted line), both with the same bandwidth h = 24. as one can see, the graphs are very similar. if one wants to compare with the normal kernel, one should set the bandwidth of the normal kernel at about h/4. this has to do with the fact that the normal kernel is much more spread out than the two kernels mentioned here, which are zero outside [−1, 1]."
1538,1,['kernels'], Kernel density estimates,seg_203,boundary kernels
1539,1,"['model', 'parameters', 'estimate', 'results', 'failure', 'data']", Kernel density estimates,seg_203,"in order to estimate the parameters of a software reliability model, failure data are collected. usually the most desirable type of failure data results when the"
1540,1,"['interval', 'estimate', 'failures', 'kernel', 'table', 'kernel density estimate', 'failure', 'data', 'control']", Kernel density estimates,seg_203,"failure times are recorded, or equivalently, the length of an interval between successive failures. the data in table 15.3 are observed interfailure times in cpu seconds for a certain control software system. on the left in figure 15.8 a kernel density estimate of the observed interfailure times is plotted. note that to the left of the origin, fn,h is positive. this is absurd, since it suggests that there are negative interfailure times."
1541,1,"['interval', 'estimate', 'dataset', 'kernel', 'symmetric', 'kernel density estimate', 'case']", Kernel density estimates,seg_203,"this phenomenon is a consequence of the fact that one uses a symmetric kernel. in that case, the resulting kernel density estimate will always be positive on the interval [xi−h, xi+h] for every element xi in the dataset. hence, obser-"
1542,1,"['histogram', 'kernel density estimates', 'interval', 'estimate', 'estimates', 'kernel', 'symmetric', 'kernel density estimate', 'adjusted']", Kernel density estimates,seg_203,"vations close to zero will cause the kernel density estimate fn,h to be positive to the left of zero. it is possible to improve the kernel density estimate in a neighborhood of zero by means of a so-called boundary kernel. without going into detail about the construction of such an improvement, we will only show the result of this. on the right in figure 15.8 the histogram of the interfailure times is plotted together with the kernel density estimate constructed with a symmetric kernel (dotted line) and with the boundary kernel density estimate (solid line). the boundary kernel density estimate is 0 to the left of the origin and is adjusted on the interval [0, h). on the interval [h,∞) both kernel density estimates are the same."
1543,1,"['plot', 'cumulative distribution function', 'dataset', 'distribution function', 'empirical cumulative distribution function', 'data', 'distribution', 'function']", The empirical distribution function,seg_205,another way to graphically represent a dataset is to plot the data in a cumulative manner. this can be done using the empirical cumulative distribution function of the data. it is denoted by fn and is defined at a point x as the proportion of elements in the dataset that are less than or equal to x:
1544,1,['dataset'], The empirical distribution function,seg_205,number of elements in the dataset ≤ x fn(x) = . n
1545,1,['dataset'], The empirical distribution function,seg_205,"to illustrate the construction of fn, consider the dataset consisting of the elements"
1546,1,"['distribution function', 'empirical distribution function', 'distribution', 'function']", The empirical distribution function,seg_205,"the corresponding empirical distribution function is displayed in figure 15.9. for x < 1, there are no elements less than or equal to x, so that fn(x) = 0. for 1 ≤ x < 3, only the element 1 is less than or equal to x, so that fn(x) = 1/5. for 3 ≤ x < 4, the elements 1 and 3 are less than or equal to x, so that fn(x) = 2/5, and so on."
1547,1,['dataset'], The empirical distribution function,seg_205,"in general, the graph of fn has the form of a staircase, with fn(x) = 0 for all x smaller than the minimum of the dataset and fn(x) = 1 for all x greater than the maximum of the dataset. between the minimum and maximum, fn has a jump of size 1/n at each element of the dataset and is constant between successive elements. in figure 15.9, the marks • and ◦ are added to the graph to emphasize the fact that, for instance, the value of fn(x) at x = 3 is 0.4, not 0.2. usually, we leave these out, and one might also connect the horizontal segments by vertical lines."
1548,1,"['functions', 'empirical distribution functions', 'dataset', 'data', 'distribution']", The empirical distribution function,seg_205,in figure 15.10 the empirical distribution functions are plotted for the old faithful data and the software reliability data. the fact that the old faithful data accumulate in the neighborhood of 120 and 270 is reflected in the graph of fn by the fact that it is steeper at these places: the jumps of fn succeed each other faster. in regions where the elements of the dataset are more stretched
1549,1,"['distribution function', 'empirical distribution function', 'data', 'distribution', 'function']", The empirical distribution function,seg_205,"out, the graph of fn is flatter. similar behavior can be seen for the software reliability data in the neighborhood of zero. the elements accumulate more close to zero, less as we move to the right. this is reflected by the empirical distribution function, which is very steep near zero and flattens out if we move to the right."
1550,1,"['bin', 'bins', 'graphical', 'histogram', 'distribution function', 'relative frequency', 'empirical distribution function', 'data', 'distribution', 'frequency', 'function']", The empirical distribution function,seg_205,"the graph of the empirical distribution function for the old faithful data agrees with the histogram in figure 15.1 whose height is the largest on the bins (90, 120] and (240, 270]. in fact, there is a one-to-one relation between the two graphical summaries of the data: the area under the histogram on a single bin is equal to the relative frequency of elements that lie in that bin, which is also equal to the increase of fn on that bin. for instance, the area under the histogram on bin (240, 270] for the old faithful data is equal to 30 · 0.0092 ="
1551,1,"['dataset', 'distribution function', 'empirical distribution function', 'distribution', 'function']", The empirical distribution function,seg_205,"quick exercise 15.6 suppose that for a dataset consisting of 300 elements, the value of the empirical distribution function in the point 1.5 is equal to 0.7. how many elements in the dataset are strictly greater than 1.5?"
1552,1,"['dataset', 'variables', 'observations', 'case']", Scatterplot,seg_207,"in some situations one wants to investigate the relationship between two or more variables. in the case of two variables x and y, the dataset consists of pairs of observations :"
1553,1,"['plot', 'contrast', 'dataset', 'variables', 'bivariate dataset', 'observations', 'scatterplot', 'data', 'variable', 'bivariate']", Scatterplot,seg_207,"we call such a dataset a bivariate dataset in contrast to the univariate dataset, which consists of observations of one particular quantity. we often like to investigate whether the value of variable y depends on the value of the variable x, and if so, whether we can describe the relation between the two variables. a first step is to take a look at the data, i.e., to plot the points (xi, yi) for i = 1, 2 . . . , n. such a plot is called a scatterplot."
1554,1,"['table', 'observations', 'data', 'mean', 'process']", Scatterplot,seg_207,"during a study about “dry” and “wet” drilling in rock, six holes were drilled, three corresponding to each process. in a dry hole one forces compressed air down the drill rods to flush the cutting and the drive hammer, whereas in a wet hole one forces water. as the hole gets deeper, one has to add a rod of 5 feet length to the drill. in each hole the time was recorded to advance 5 feet to a total depth of 400 feet. the data in table 15.4 are in 1/100 minute and are derived from the original data in [23]. the original data consisted of drill times for each of the six holes and contained missing observations and observations that were known to be too large. the data in table 15.4 are the mean drill times of the bona fide observations at each depth for dry and wet drilling."
1555,1,"['plot', 'mean']", Scatterplot,seg_207,"one of the questions of interest is whether drill time depends on depth. to investigate this, we plot the mean drill time against depth. figure 15.11 displays"
1556,1,"['range', 'scatterplots', 'vary']", Scatterplot,seg_207,"the resulting scatterplots for the dry and wet holes. the scatterplots seem to indicate that in the beginning the drill time hardly depends on depth, at least up to, let’s say, 250 feet. at greater depth, the drill time seems to vary over a larger range and increases somewhat with depth. a possible explanation for this is that the drill moved from softer to harder material. this was suggested by the fact that the drill hit an ore lens at about 250 feet and that the natural place such ore lenses occur is between two different materials (see [23] for details)."
1557,1,['scatterplots'], Scatterplot,seg_207,a more important question is whether one can drill holes faster using dry drilling or wet drilling. the scatterplots seem to suggest that dry drilling might be faster. we will come back to this later.
1558,0,[], Scatterplot,seg_207,predicting janka hardness of australian timber
1559,1,"['standard', 'dataset', 'table', 'bivariate dataset', 'bivariate', 'test']", Scatterplot,seg_207,"the janka hardness test is a standard test to measure the hardness of wood. it measures the force required to push a steel ball with a diameter of 11.28 millimeters (0.444 inch) into the wood to a depth of half the ball’s diameter. to measure janka hardness directly is difficult. however, it is related to the density of the wood, which is comparatively easy to measure. in table 15.5 a bivariate dataset is given of density (x) and janka hardness (y) of 36 australian eucalypt hardwoods."
1560,1,"['linear', 'dataset', 'variables', 'bivariate dataset', 'scatterplot', 'case', 'bivariate']", Scatterplot,seg_207,"in order to get an impression of the relationship between hardness and density, we made a scatterplot of the bivariate dataset, which is displayed in figure 15.12. it consists of all points (xi, yi) for i = 1, 2, . . . , 36. the scatterplot might provide suggestions for the formula that describes the relationship between the variables x and y. in this case, a linear relationship between the two variables does not seem unreasonable. later (chapter 22) we will discuss"
1561,1,['linear'], Scatterplot,seg_207,how one can establish such a linear relationship by means of the observed pairs.
1562,1,['prediction'], Scatterplot,seg_207,quick exercise 15.7 suppose we have a eucalypt hardwood tree with density 65. what would your prediction be for the corresponding janka hardness?
1563,1,"['table', 'data', 'dataset']", Solutions to the quick exercises,seg_209,"15.1 there are 272 elements in the dataset. the 91st and 182nd elements of the ordered data divide the dataset in three groups, each consisting of 90 elements. from a closer look at table 15.2 we find that these two elements are 145 and 260."
1564,1,"['bin', 'bins', 'table', 'observations', 'number of observations']", Solutions to the quick exercises,seg_209,"15.2 in table 15.2 one can easily count the number of observations in each of the bins (90, 120], . . . , (300, 330]. the heights on each bin can be computed by dividing the number of observations in each bin by 272 ·30 = 8160. we get the following:"
1565,1,['bin'], Solutions to the quick exercises,seg_209,bin count height bin count height
1566,1,"['bins', 'interval', 'dataset', 'table']", Solutions to the quick exercises,seg_209,"15.3 from table 15.2 we see that we must cover an interval of length of at least 306 − 96 = 210 with bins of width b = 3.49 · 68.48 · 272−1/3 = 36.89. since 210/36.89 = 5.69, we need at least six bins to cover the whole dataset."
1567,1,[], Solutions to the quick exercises,seg_209,"15.4 by means of formula (15.1), we can write"
1568,1,['variables'], Solutions to the quick exercises,seg_209,"for any i = 1, . . . , n, we find by change of integration variables t = hu + xi"
1569,1,['condition'], Solutions to the quick exercises,seg_209,where we also use condition (k1). this directly yields
1570,1,"['table', 'estimate', 'kernel', 'kernel density estimate', 'bandwidth']", Solutions to the quick exercises,seg_209,"15.5 the kernel density estimate will be strictly positive between the minimum minus h and the maximum plus h. the bandwidth equals h = 1.06 · 68.48 · 272−1/5 = 23.66. from table 15.2, we see that this will be between 96 − 23.66 = 72.34 and 306 + 23.66 = 329.66."
1571,0,[], Solutions to the quick exercises,seg_209,15.6 by definition the number of elements less than or equal to 1.5 is f300(1.5) · 300 = 210. hence 90 elements are strictly greater than 1.5.
1572,1,['predicted'], Solutions to the quick exercises,seg_209,"15.7 just by drawing a straight line that seems to fit the datapoints well, the authors predicted a janka hardness of about 2700."
1573,1,"['bin', 'bin width', 'histogram', 'data', 'information']", Exercises,seg_211,"15.1 in [33] stephen stigler discusses data from the edinburgh medical and surgical journal (1817). these concern the chest circumference of 5732 scottish soldiers, measured in inches. the following information is given about the histogram with bin width 1, the first bin starting at 32.5."
1574,1,['bin'], Exercises,seg_211,bin count bin count
1575,1,"['bin', 'histogram']", Exercises,seg_211,a. compute the height of the histogram on each bin.
1576,1,"['histogram', 'dataset']", Exercises,seg_211,b. make a sketch of the histogram. would you view the dataset as being
1577,1,['skewed'], Exercises,seg_211,symmetric or skewed?
1578,1,['space shuttle challenger'], Exercises,seg_211,15.2 recall the example of the space shuttle challenger in section 1.4. the following list contains the launch temperatures in degrees fahrenheit during previous takeoffs.
1579,1,"['bin', 'bin width', 'histogram']", Exercises,seg_211,"a. compute the heights of a histogram with bin width 5, the first bin starting"
1580,1,['space shuttle challenger'], Exercises,seg_211,"b. on january 28, 1986, during the launch of the space shuttle challenger,"
1581,1,['dataset'], Exercises,seg_211,"the temperature was 31 degrees fahrenheit. given the dataset of launch temperatures of previous takeoffs, would you consider 31 as a representative launch temperature?"
1582,1,"['dataset', 'table', 'data', 'intervals']", Exercises,seg_211,"15.3 in an article in biometrika, an example is discussed about mine disasters during the period from march 15, 1851, to march, 22, 1962. a dataset has been obtained of 190 recorded time intervals (in days) between successive coal mine disasters involving ten or more men killed. the ordered data are listed in table 15.6."
1583,1,"['bin', 'bins', 'histogram']", Exercises,seg_211,"a. compute the height on each bin of the histogram with bins [0, 250],"
1584,1,"['histogram', 'dataset']", Exercises,seg_211,b. make a sketch of the histogram. would you view the dataset as being
1585,1,['skewed'], Exercises,seg_211,symmetric or skewed?
1586,1,"['data', 'table']", Exercises,seg_211,15.4 the ordered software data (see also table 15.3) are given in the following list.
1587,1,"['bin', 'bins', 'histogram']", Exercises,seg_211,"a. compute the heights on each bin of the histogram with bins [0, 500],"
1588,1,"['distribution function', 'empirical distribution function', 'distribution', 'function']", Exercises,seg_211,b. compute the value of the empirical distribution function in the endpoints
1589,1,['bins'], Exercises,seg_211,of the bins.
1590,1,"['bin', 'histogram']", Exercises,seg_211,"c. check that the area under the histogram on bin (1000, 1500] is equal to"
1591,1,"['bin', 'distribution function', 'empirical distribution function', 'distribution', 'function']", Exercises,seg_211,"the increase fn(1500) − fn(1000) of the empirical distribution function on this bin. actually, this is true for each single bin (see exercise 15.11)."
1592,1,"['bins', 'histogram', 'distribution function', 'empirical distribution function', 'distribution', 'function']", Exercises,seg_211,"15.5 suppose we construct a histogram with bins [0,1], (1,3], (3,5], (5,8], (8,11], (11,14], and (14,18]. given are the values of the empirical distribution function at the boundaries of the bins:"
1593,1,"['bin', 'histogram']", Exercises,seg_211,compute the height of the histogram on each bin.
1594,1,"['histogram', 'information']", Exercises,seg_211,15.6 given is the following information about a histogram:
1595,1,"['distribution function', 'empirical distribution function', 'distribution', 'function']", Exercises,seg_211,compute the value of the empirical distribution function in the point t = 7.
1596,1,"['bin', 'histogram', 'distribution function', 'empirical distribution function', 'data', 'distribution', 'function']", Exercises,seg_211,15.7 in exercise 15.2 a histogram was constructed for the challenger data. on which bin does the empirical distribution function have the largest increase?
1597,1,['function'], Exercises,seg_211,15.8 define a function k by
1598,1,"['kernel', 'function']", Exercises,seg_211,and k(u) = 0 elsewhere. check whether k satisfies the conditions (k1)–(k3) for a kernel function.
1599,1,['scatterplot'], Exercises,seg_211,"15.9 on the basis of the duration of an eruption of the old faithful geyser, park rangers try to predict the waiting time to the next eruption. in figure 15.13 a scatterplot is displayed of the duration and the time to the next eruption in seconds."
1600,1,['scatterplot'], Exercises,seg_211,a. does the scatterplot give reason to believe that the duration of an eruption
1601,0,[], Exercises,seg_211,influences the time to the next eruption?
1602,0,[], Exercises,seg_211,b. suppose you have just observed an eruption that lasted 250 seconds. what
1603,0,[], Exercises,seg_211,would you predict for the time to the next eruption?
1604,1,['dataset'], Exercises,seg_211,"c. the dataset of durations shows two modes, i.e., there are two places where"
1605,1,"['histogram', 'data', 'dataset']", Exercises,seg_211,"the data accumulate (see, for instance, the histogram in figure 15.1). how many modes does the dataset of waiting times show?"
1606,1,"['dataset', 'distribution function', 'empirical distribution function', 'distribution', 'function']", Exercises,seg_211,15.10 figure 15.14 displays the graph of an empirical distribution function of a dataset consisting of 200 elements. how many modes does the dataset show?
1607,1,"['bin', 'histogram', 'dataset', 'distribution function', 'empirical distribution function', 'distribution', 'function']", Exercises,seg_211,"15.11 given is a histogram and the empirical distribution function fn of the same dataset. show that the height of the histogram on a bin (a, b] is"
1608,1,"['kernel', 'probability', 'estimate']", Exercises,seg_211,"15.12 let fn,h be a kernel estimate. as mentioned in section 15.3, fn,h itself is a probability density."
1609,1,['expectation'], Exercises,seg_211,a. show that the corresponding expectation is equal to
1610,0,[], Exercises,seg_211,hint: you might consult the solution to quick exercise 15.4.
1611,1,"['second moment', 'moment']", Exercises,seg_211,"b. show that the second moment corresponding to fn,h satisfies"
1612,1,"['empirical quantiles', 'probability', 'random', 'numerical', 'sample', 'graphical', 'sample mean', 'dataset', 'probability distributions', 'quantiles', 'mean', 'distributions', 'center of a dataset', 'random variables', 'variability', 'variables', 'boxplot']", Exploratory data analysis numerical summaries,seg_213,"the classical way to describe important features of a dataset is to give several numerical summaries. we discuss numerical summaries for the center of a dataset and for the amount of variability among the elements of a dataset, and then we introduce the notion of quantiles for a dataset. to distinguish these quantities from corresponding notions for probability distributions of random variables, we will often add the word sample or empirical ; for instance, we will speak of the sample mean and empirical quantiles. we end this chapter with the boxplot, which combines some of the numerical summaries in a graphical display."
1613,1,"['center of a dataset', 'method', 'dataset']", The center of a dataset,seg_215,the best-known method to identify the center of a dataset is to compute the
1614,1,['mean'], The center of a dataset,seg_215,sample mean x1 + x2 + · · · + xn
1615,1,"['sample', 'sample mean', 'dataset', 'measurements', 'mean']", The center of a dataset,seg_215,"for the sake of notational convenience we will sometimes drop the subscript n and write x̄ instead of x̄n. the following dataset consists of hourly temperatures in degrees fahrenheit (rounded to the nearest integer), recorded at wick in northern scotland from 5 p.m. december 31, 1960, to 3 a.m. january 1, 1961. the sample mean of the 11 measurements is equal to 44.7."
1616,1,"['sample median', 'sample', 'center of a dataset', 'median', 'dataset']", The center of a dataset,seg_215,"another way to identify the center of a dataset is by means of the sample median, which we will denote by med(x1, x2, . . . , xn) or briefly medn. the sample median is defined as the middle element of the dataset when it is put in ascending order. when n is odd, it is clear what this means. when n is even,"
1617,1,"['sample median', 'sample', 'median', 'data', 'average']", The center of a dataset,seg_215,we take the average of the two middle elements. for the wick temperature data the sample median is equal to 42.
1618,1,"['sample median', 'sample', 'sample mean', 'median', 'dataset', 'mean']", The center of a dataset,seg_215,quick exercise 16.1 compute the sample mean and sample median of the dataset
1619,1,"['outliers', 'sample', 'sample mean', 'dataset', 'observations', 'data', 'distribution', 'probability distribution', 'expectation', 'mean', 'probability']", The center of a dataset,seg_215,"both methods have pros and cons. the sample mean is the natural analogue for a dataset of what the expectation is for a probability distribution. however, it is very sensitive to outliers, by which we mean observations in the dataset that deviate a lot from the bulk of the data."
1620,1,"['sample median', 'sample', 'outliers', 'sample mean', 'median', 'data', 'measurements', 'mean', 'sensitivity', 'average']", The center of a dataset,seg_215,"to illustrate the sensitivity of the sample mean, consider the wick temperature data displayed in figure 16.1. the values 58 and 58 recorded at midnight and 1 a.m. are clearly far from the bulk of the data and give grounds for concern whether they are genuine (58 degrees fahrenheit seems very warm at midnight for new year’s in northern scotland). to investigate their effect on the sample mean we compute the average of the data, leaving out these measurements, which gives 41.8 (instead of 44.7). the sample median of the data is equal to 41 (instead of 42) when leaving out the measurements with value 58. the median is more robust in the sense that it is hardly affected by a few outliers."
1621,1,"['sample', 'measurement', 'sample mean', 'case', 'data', 'outlier', 'measurements', 'mean', 'sensitivity']", The center of a dataset,seg_215,"it should be emphasized that this discussion is only meant to illustrate the sensitivity of the sample mean and by no means is intended to suggest we leave out measurements that deviate a lot from the bulk of the data! it is important to be aware of the presence of an outlier. in that case, one could try to find out whether there is perhaps something suspicious about this measurement. this might lead to assigning a smaller weight to such a measurement or even to"
1622,1,"['measurement', 'dataset']", The center of a dataset,seg_215,"removing it from the dataset. however, sometimes it is possible to reconstruct the exact circumstances and correct the measurement. for instance, after further inquiry in the temperature example it turned out that at midnight the meteorological office changed its recording unit from degrees fahrenheit to 1/10th degree celsius (so 58 and 41 should read 5.8◦c and 4.1◦c). the corrected values in degrees fahrenheit (to the nearest integer) are"
1623,1,"['sample median', 'sample', 'sample mean', 'median', 'data', 'mean']", The center of a dataset,seg_215,for the corrected data the sample mean is 41.5 and the sample median is 42.
1624,1,['dataset'], The center of a dataset,seg_215,quick exercise 16.2 consider the same dataset as in quick exercise 16.1. suppose that someone misreads the dataset as
1625,1,"['sample median', 'sample', 'sample mean', 'median', 'mean']", The center of a dataset,seg_215,compute the sample mean and sample median and compare these values with the ones you found in quick exercise 16.1.
1626,1,"['sample', 'variability', 'dataset', 'sample variance', 'variance']", The amount of variability of a dataset,seg_217,"to quantify the amount of variability among the elements of a dataset, one often uses the sample variance defined by"
1627,1,"['deviation', 'sample', 'factor', 'variance', 'sample variance', 'average']", The amount of variability of a dataset,seg_217,"up to a scaling factor this is equal to the average squared deviation from x̄n. at first sight, it seems more natural to define the sample variance by"
1628,1,"['deviation', 'sample', 'dataset', 'factor', 'sample standard deviation', 'standard', 'standard deviation']", The amount of variability of a dataset,seg_217,"why we choose the factor 1/(n−1) instead of 1/n will be explained later (see chapter 19). because s2n is in different units from the elements of the dataset, one often prefers the sample standard deviation"
1629,1,['dataset'], The amount of variability of a dataset,seg_217,which is measured in the same units as the elements of the dataset itself.
1630,1,"['outliers', 'sample', 'deviation', 'sample mean', 'sample standard deviation', 'data', 'measurements', 'mean', 'standard', 'standard deviation']", The amount of variability of a dataset,seg_217,"just as the sample mean, the sample standard deviation is very sensitive to outliers. for the (uncorrected) wick temperature data the sample standard deviation is 6.62, or 0.97 if we leave out the two measurements with value 58."
1631,1,"['sample median', 'sample', 'median of absolute deviations', 'deviation', 'variability', 'median', 'data', 'deviations', 'standard', 'standard deviation']", The amount of variability of a dataset,seg_217,"for the corrected data the standard deviation is 1.44. a more robust measure of variability is the median of absolute deviations or mad, which is defined as follows. consider the absolute deviation of every element xi with respect to the sample median:"
1632,1,"['deviations', 'median']", The amount of variability of a dataset,seg_217,the mad is obtained by taking the median of all these absolute deviations
1633,1,"['deviation', 'sample', 'dataset', 'sample standard deviation', 'standard', 'standard deviation']", The amount of variability of a dataset,seg_217,quick exercise 16.3 compute the sample standard deviation for the dataset of quick exercise 16.1 for which it is given that the values of xi − x̄n are:
1634,1,['dataset'], The amount of variability of a dataset,seg_217,also compute the mad for this dataset.
1635,1,"['sample median', 'sample', 'outliers', 'median', 'observations', 'data', 'measurements']", The amount of variability of a dataset,seg_217,"just as the sample median, the mad is hardly affected by outliers. for the (uncorrected) wick temperature data the mad is 1 and equal to 0 if we leave out the two measurements with value 58 (the value 0 seems a bit strange, but is a consequence of the fact that the observations are given in degrees fahrenheit rounded to the nearest integer). for the corrected data the mad is 1."
1636,1,"['deviation', 'sample', 'dataset', 'sample standard deviation', 'standard', 'standard deviation']", The amount of variability of a dataset,seg_217,quick exercise 16.4 compute the sample standard deviation for the misread dataset of quick exercise 16.2 for which it is given that the values of xi − x̄n are:
1637,1,['dataset'], The amount of variability of a dataset,seg_217,also compute the mad for this dataset and compare both values with the ones you found in quick exercise 16.3.
1638,1,"['sample median', 'sample', 'quantile', 'empirical quantile', 'percentile', 'median', 'empirical percentile', 'dataset', 'empirical quantiles', 'statistics', 'quantiles', 'order statistics']", Empirical quantiles quartiles and the IQR,seg_219,"the sample median divides the dataset in two more or less equal parts: about half of the elements are less than the median and about half of the elements are greater than the median. more generally, we can divide the dataset in two parts in such a way that a proportion p is less than a certain number and a proportion 1 − p is greater than this number. such a number is called the 100p empirical percentile or the pth empirical quantile and is denoted by qn(p). for a suitable introduction of empirical quantiles we need the notion of order statistics."
1639,1,"['dataset', 'statistics', 'order statistics']", Empirical quantiles quartiles and the IQR,seg_219,"the order statistics consist of the same elements as in the original dataset x1, x2, . . . , xn, but in ascending order. denote by x(k) the kth element in the ordered list. then"
1640,1,"['statistics', 'data', 'order statistics']", Empirical quantiles quartiles and the IQR,seg_219,"are called the order statistics of x1, x2, . . . , xn. the order statistics of the wick temperature data are"
1641,1,"['dataset', 'table', 'statistics', 'order statistics']", Empirical quantiles quartiles and the IQR,seg_219,"note that by putting the elements in order, it is possible that successive order statistics are the same, for instance, x(1) = · · · = x(5) = 41. another example is table 15.2, which lists the order statistics of the old faithful dataset."
1642,1,"['quantile', 'empirical quantile', 'dataset', 'empirical quantiles', 'integer part', 'quantiles', 'statistic', 'order statistic']", Empirical quantiles quartiles and the IQR,seg_219,"to compute empirical quantiles one linearly interpolates between order statistics of the dataset. let 0 < p < 1, and suppose we want to compute the pth empirical quantile for a dataset x1, x2, . . . , xn. the following computation is based on requiring that the ith order statistic is the i/(n + 1) quantile. if we denote the integer part of a by a , then the computation of qn(p) runs as follows:"
1643,1,"['quantile', 'distribution function', 'empirical distribution function', 'data', 'distribution', 'function']", Empirical quantiles quartiles and the IQR,seg_219,with k = p(n + 1) and α = p(n + 1) − k. on the left in figure 16.2 the relation between the pth quantile and the empirical distribution function is illustrated for the old faithful data.
1644,1,"['empirical percentile', 'percentile', 'data']", Empirical quantiles quartiles and the IQR,seg_219,quick exercise 16.5 compute the 55th empirical percentile for the wick temperature data.
1645,1,['quartiles'], Empirical quantiles quartiles and the IQR,seg_219,lower and upper quartiles
1646,1,"['range', 'distribution function', 'function', 'sample median', 'sample', 'quartiles', 'dataset', 'empirical percentile', 'data', 'percentiles', 'quartile', 'skewness', 'interquartile range', 'percentile', 'median', 'empirical distribution function', 'empirical percentiles', 'distribution', 'upper quartile']", Empirical quantiles quartiles and the IQR,seg_219,"instead of identifying only the center of the dataset, tukey [35] suggested to give a five-number summary of the dataset: the minimum, the maximum, the sample median, and the 25th and 75th empirical percentiles. the 25th empirical percentile qn(0.25) is called the lower quartile and the 75th empirical percentile qn(0.75) is called the upper quartile. together with the median, the lower and upper quartiles divide the dataset in four more or less equal parts consisting of about one quarter of the number of elements. the relation of the two quartiles and the median with the empirical distribution function is illustrated for the old faithful data on the right of figure 16.2. the distance between the lower quartile and the median, relative to the distance between the upper quartile and the median, gives some indication on the skewness of the dataset. the distance between the upper and lower quartiles is called the interquartile range, or iqr:"
1647,1,"['dataset', 'variability', 'range', 'data']", Empirical quantiles quartiles and the IQR,seg_219,the iqr specifies the range of the middle half of the dataset. it could also serve as a robust measure of the amount of variability among the elements of the dataset. for the old faithful data the five-number summary is
1648,1,"['quartile', 'median', 'upper quartile']", Empirical quantiles quartiles and the IQR,seg_219,minimum lower quartile median upper quartile maximum 96 129.25 240 267.75 306
1649,1,['data'], Empirical quantiles quartiles and the IQR,seg_219,quick exercise 16.6 compute the five-number summary for the (uncorrected) wick temperature data.
1650,1,"['sample median', 'sample', 'plot', 'whisker', 'median', 'observations', 'whiskers', 'data', 'observation', 'boxplot', 'quartile', 'outlier', 'upper quartile']", The boxandwhisker plot,seg_221,"tukey [35] also proposed visualizing the five-number summary discussed in the previous section by a so-called box-and-whisker plot, briefly boxplot. figure 16.3 displays a boxplot. the data are now on the vertical axis, where we left out the numbers on the axis in order to explain the construction of the figure. the horizontal width of the box is irrelevant. in the vertical direction the box extends from the lower to the upper quartile, so that the height of the box is precisely the iqr. the horizontal line inside the box corresponds to the sample median. up from the upper quartile we measure out a distance of 1.5 times the iqr and draw a so-called whisker up to the largest observation that lies within this distance, where we put a horizontal line. similarly, down from the lower quartile we measure out a distance of 1.5 times the iqr and draw a whisker to the smallest observation that lies within this distance, where we also put a horizontal line. all other observations beyond the whiskers are marked by ◦. such an observation is called an outlier."
1651,1,"['sample median', 'sample', 'median', 'dataset', 'observations', 'whiskers', 'skewed', 'data', 'boxplot', 'quartile', 'boxplots', 'upper quartile', 'skewness']", The boxandwhisker plot,seg_221,"in figure 16.4 the boxplots of the old faithful data and of the software reliability data (see also chapter 15) are displayed. the skewness of the software reliability data produces a boxplot with whiskers of very different length and with several observations beyond the upper quartile plus 1.5 times the iqr. the boxplot of the old faithful data illustrates one of the shortcomings of the boxplot; it does not capture the fact that the data show two separate peaks. however, the position of the sample median inside the box does suggest that the dataset is skewed."
1652,1,"['outliers', 'extreme outliers', 'whiskers', 'data', 'boxplot', 'measurements']", The boxandwhisker plot,seg_221,"quick exercise 16.7 suppose we want to construct a boxplot of the (uncorrected) wick temperature data. what is the height of the box, the length of both whiskers, and which measurements fall outside the box and whiskers? would you consider the two values 58 extreme outliers?"
1653,1,['boxplots'], The boxandwhisker plot,seg_221,using boxplots to compare several datasets
1654,1,"['histogram', 'range', 'graphical', 'estimate', 'dataset', 'kernel', 'data', 'information', 'boxplots', 'sets', 'symmetry', 'skewness', 'kernel density estimate', 'table', 'boxplot', 'average']", The boxandwhisker plot,seg_221,"although the boxplot provides some information about the structure of the data, such as center, range, skewness or symmetry, it is a poor graphical display of the dataset. graphical summaries such as the histogram and kernel density estimate are more informative displays of a single dataset. boxplots become useful if we want to compare several sets of data in a simple graphical display. in figure 16.5 boxplots are displayed of the average drill time for dry and wet drilling up to a depth of 250 feet for the drill data discussed in section 15.5 (see also table 15.4). it is clear that the boxplot corresponding to dry drilling differs from that corresponding to wet drilling. however, the question is whether this difference can still be attributed to chance or is caused by the drilling technique used. we will return to this type of question in chapter 25."
1655,1,['average'], Solutions to the quick exercises,seg_223,16.1 the average is
1656,1,['median'], Solutions to the quick exercises,seg_223,"the median is the middle element of 3.0, 3.2, 4.2, 4.6, and 5.0, which gives medn = 4.2."
1657,1,['average'], Solutions to the quick exercises,seg_223,16.2 the average is
1658,1,"['outliers', 'average', 'median']", Solutions to the quick exercises,seg_223,"which differs 14.4 from the average we found in quick exercise 16.1. the median is the middle element of 3.2, 4.2, 4.6, 30, and 50. this gives medn = 4.6, which only differs 0.4 from the median we found in quick exercise 16.1. as one can see, the median is hardly affected by the two outliers."
1659,1,"['sample', 'variance', 'sample variance']", Solutions to the quick exercises,seg_223,16.3 the sample variance is
1660,1,"['deviation', 'sample', 'median', 'sample standard deviation', 'deviations', 'standard', 'standard deviation']", Solutions to the quick exercises,seg_223,"so that the sample standard deviation is sn = √0.76 = 0.872. the median is 4.2, so that the absolute deviations from the median are given by"
1661,1,['median'], Solutions to the quick exercises,seg_223,"the mad is the median of these numbers, which is 0.8."
1662,1,"['sample', 'variance', 'sample variance']", Solutions to the quick exercises,seg_223,16.4 the sample variance is
1663,1,"['deviation', 'sample', 'median', 'sample standard deviation', 'deviations', 'standard', 'standard deviation']", Solutions to the quick exercises,seg_223,"so that the sample standard deviation is sn = √439.06 = 20.95, which is a difference of 20.19 from the value we found in quick exercise 16.3. the median is 4.6, so that the absolute deviations from the median are given by"
1664,1,"['outliers', 'median']", Solutions to the quick exercises,seg_223,"the mad is the median of these numbers, which is 1.4. just as the median, the mad is hardly affected by the two outliers."
1665,1,"['statistics', 'data', 'order statistics']", Solutions to the quick exercises,seg_223,16.6 from the order statistics of the wick temperature data
1666,1,"['quartile', 'median', 'data', 'upper quartile']", Solutions to the quick exercises,seg_223,"it can be seen immediately that minimum, maximum, and median are given by 41, 58, and 42. for the lower quartile we have k = 0.25·12 = 3, so that α = 0 and qn(0.25) = x(3) = 41. for the upper quartile we have k = 0.75 · 12 = 9, so that again α = 0 and qn(0.75) = x(9) = 43. hence for the wick temperature data the five-number summary is"
1667,1,"['quartile', 'median', 'upper quartile']", Solutions to the quick exercises,seg_223,minimum lower quartile median upper quartile maximum 41 41 42 43 58
1668,1,"['outliers', 'whisker', 'extreme outliers', 'range', 'whiskers', 'data', 'observation', 'quartile', 'measurements']", Solutions to the quick exercises,seg_223,"16.7 from the five-number summary for the wick temperature data (see quick exercise 16.6), it follows immediately that the height of the box is the iqr: 43 − 41 = 2. if we measure out a distance of 1.5 times 2 down from the lower quartile 41, we see that the smallest observation within this range is 41, which means that the lower whisker has length zero. similarly, the upper whisker has length zero. the two measurements with value 58 are outside the box and whiskers. the two values 58 are clearly far away from the bulk of the data and should be considered extreme outliers."
1669,1,"['statistics', 'data', 'order statistics']", Exercises,seg_225,16.1 use the order statistics of the software data as given in exercise 15.4 to answer the following questions.
1670,1,"['sample median', 'sample', 'median']", Exercises,seg_225,a. compute the sample median.
1671,1,['quartiles'], Exercises,seg_225,b. compute the lower and upper quartiles and the iqr.
1672,1,"['empirical percentile', 'percentile']", Exercises,seg_225,c. compute the 37th empirical percentile.
1673,1,"['quartiles', 'median', 'data']", Exercises,seg_225,16.2 compute for the old faithful data the distance of the lower and upper quartiles to the median and explain the difference.
1674,1,"['table', 'statistics', 'space shuttle challenger', 'order statistics']", Exercises,seg_225,"16.3 recall the example about the space shuttle challenger in section 1.4. the following table lists the order statistics of launch temperatures during take-offs in degrees fahrenheit, including the launch temperature on january 28, 1986."
1675,1,"['sample median', 'sample', 'quartiles', 'median']", Exercises,seg_225,a. find the sample median and the lower and upper quartiles.
1676,1,"['boxplot', 'dataset']", Exercises,seg_225,b. sketch the boxplot of this dataset.
1677,0,[], Exercises,seg_225,"c. on january 28, 1986, the launch temperature was 31 degrees fahrenheit."
1678,1,['data'], Exercises,seg_225,comment on the value 31 with respect to the other data points.
1679,1,"['sample median', 'sample', 'sample mean', 'median', 'data', 'mean', 'transform']", Exercises,seg_225,16.4 the sample mean and sample median of the uncorrected wick temperature data (in degrees fahrenheit) are 44.7 and 42. we transform the data from degrees fahrenheit (xi) to degrees celsius (yi) by means of the formula
1680,1,['dataset'], Exercises,seg_225,which gives the following dataset
1681,1,['dataset'], Exercises,seg_225,"c. suppose we have a dataset x1, x2, . . . , xn and construct y1, y2, . . . , yn"
1682,1,"['sample median', 'sample', 'sample mean', 'median', 'mean']", Exercises,seg_225,"where yi = axi + b with a and b being real numbers. do similar relations hold for the sample mean and sample median? if so, state them."
1683,1,"['deviation', 'sample', 'sample standard deviation', 'data', 'standard', 'standard deviation']", Exercises,seg_225,16.5 consider the uncorrected wick temperature data in degrees fahrenheit (xi) and the corresponding temperatures in degrees celsius (yi) as given in exercise 16.4. the sample standard deviation and the mad for the wick data are 6.62 and 1.
1684,1,"['sample', 'sample standard deviations', 'standard deviations', 'deviations', 'standard']", Exercises,seg_225,"a. let sf and sc denote the sample standard deviations of x1, x2, . . . , xn"
1685,1,[], Exercises,seg_225,"b. let madf and madc denote the mad of x1, x2, . . . , xn and y1, y2, . . . , yn"
1686,0,[], Exercises,seg_225,respectively. is it also true that madc = 5 9madf ?
1687,1,['dataset'], Exercises,seg_225,"c. suppose we have a dataset x1, x2, . . . , xn and construct y1, y2, . . . , yn"
1688,1,"['deviation', 'sample', 'sample standard deviation', 'standard', 'standard deviation']", Exercises,seg_225,"where yi = axi + b with a and b being real numbers. do similar relations hold for the sample standard deviation and the mad? if so, state them."
1689,1,[], Exercises,seg_225,"16.6 consider two datasets: 1, 5, 9 and 2, 4, 6, 8."
1690,1,"['sample', 'sample means']", Exercises,seg_225,a. denote the sample means of the two datasets by x̄ and ȳ. is it true that the
1691,1,"['sample', 'sample mean', 'dataset', 'mean']", Exercises,seg_225,average (x̄+ ȳ)/2 of x̄ and ȳ is equal to the sample mean of the combined dataset with 7 elements?
1692,1,"['sample', 'sample mean', 'mean']", Exercises,seg_225,b. suppose we have two other datasets: one of size n with sample mean
1693,1,"['sample', 'sample mean', 'dataset', 'mean', 'average']", Exercises,seg_225,"x̄n and another dataset of size m with sample mean ȳm. is it always true that the average (x̄n + ȳm)/2 of x̄n and ȳm is equal to the sample mean of the combined dataset with n + m elements? if no, then provide a counterexample. if yes, then explain this."
1694,1,"['sample', 'sample mean', 'dataset', 'mean']", Exercises,seg_225,"c. if m = n, is (x̄n+ȳm)/2 equal to the sample mean of the combined dataset"
1695,1,[], Exercises,seg_225,16.7 consider the two datasets from exercise 16.6.
1696,1,"['sample', 'sample medians', 'medians']", Exercises,seg_225,a. denote the sample medians of the two datasets by medx and medy. is it
1697,1,"['sample median', 'sample', 'median', 'dataset', 'sample medians', 'medians']", Exercises,seg_225,true that the sample median (medx +medy)/2 of the two sample medians is equal to the sample median of the combined dataset with 7 elements?
1698,1,"['sample median', 'sample', 'median']", Exercises,seg_225,b. suppose we have two other datasets: one of size n with sample median
1699,1,"['sample median', 'sample', 'median', 'dataset', 'sample medians', 'medians']", Exercises,seg_225,"medx and another dataset of size m with sample median medy. is it always true that the sample median (medx + medy)/2 of the two sample medians is equal to the sample median of the combined dataset with n+m elements? if no, then provide a counterexample. if yes, then explain this."
1700,1,['dataset'], Exercises,seg_225,16.8 compute the mad for the combined dataset of 7 elements from exercise 16.6.
1701,1,['dataset'], Exercises,seg_225,"16.9 consider a dataset x1, x2, . . . , xn with xi = 0. we construct a second"
1702,1,['dataset'], Exercises,seg_225,"a. suppose dataset x1, x2, . . . , xn consists of −6, 1, 15. is it true that ȳ3 ="
1703,1,"['sample median', 'sample', 'outliers', 'method', 'median', 'extreme outliers', 'sample mean', 'dataset', 'mean', 'sensitivity']", Exercises,seg_225,"16.10 a method to investigate the sensitivity of the sample mean and the sample median to extreme outliers is to replace one or more elements in a given dataset by a number y and investigate the effect when y goes to infinity. to illustrate this, consider the dataset from quick exercise 16.1:"
1704,1,"['sample median', 'sample', 'sample mean', 'median', 'mean']", Exercises,seg_225,with sample mean 4 and sample median 4.2.
1705,0,[], Exercises,seg_225,a. we replace the element 3.2 by some real number y. what happens with
1706,1,"['sample median', 'sample', 'sample mean', 'median', 'dataset', 'mean']", Exercises,seg_225,the sample mean and the sample median of this new dataset as y → ∞?
1707,0,[], Exercises,seg_225,b. we replace a number of elements by some real number y. how many
1708,1,"['sample median', 'sample', 'median', 'dataset']", Exercises,seg_225,elements do we need to replace so that the sample median of the new dataset goes to infinity as y → ∞?
1709,1,['dataset'], Exercises,seg_225,c. suppose we have another dataset of size n. how many elements do we
1710,1,"['sample median', 'sample', 'sample mean', 'median', 'dataset', 'mean']", Exercises,seg_225,"need to replace by some real number y, so that the sample mean of the new dataset goes to infinity as y → ∞? and how many elements do we need to replace, so that the sample median of the new dataset goes to infinity?"
1711,1,"['outliers', 'sample median', 'sample', 'sample mean', 'dataset', 'mean', 'standard', 'standard deviation', 'median', 'deviation', 'extreme outliers', 'sample standard deviation', 'sensitivity']", Exercises,seg_225,"16.11 just as in exercise 16.10 we investigate the sensitivity of the sample standard deviation and the mad to extreme outliers, by considering the same dataset with sample standard deviation 0.872 and mad equal to 0.8. answer the same three questions for the sample standard deviation and the mad instead of the sample mean and sample median."
1712,1,"['sample median', 'sample', 'sample mean', 'median', 'dataset', 'mean']", Exercises,seg_225,16.12 compute the sample mean and sample median for the dataset
1713,1,['case'], Exercises,seg_225,in case n is odd and in case n is even. you may use the fact that
1714,1,"['deviation', 'sample', 'dataset', 'sample standard deviation', 'standard', 'standard deviation']", Exercises,seg_225,16.13 compute the sample standard deviation and mad for the dataset
1715,0,[], Exercises,seg_225,you may use the fact that
1716,1,"['sample median', 'sample', 'percentile', 'median', 'empirical percentile']", Exercises,seg_225,16.14 check that the 50th empirical percentile is the sample median.
1717,1,"['deviation', 'sample', 'standard', 'standard deviation']", Exercises,seg_225,16.15 the following rule is useful for the computation of the sample variance (and standard deviation). show that
1718,1,"['variance', 'mean', 'estimate']", Exercises,seg_225,"16.16 recall exercise 15.12, where we computed the mean and second moment corresponding to a density estimate fn,h. show that the variance corresponding to fn,h satisfies:"
1719,1,"['quantile', 'empirical quantile', 'dataset', 'statistic', 'order statistic']", Exercises,seg_225,"16.17 suppose we have a dataset x1, x2, . . . , xn. check that if p = i/(n + 1) the pth empirical quantile is the ith order statistic."
1720,1,"['simple linear regression', 'statistics', 'measurements', 'sample statistics', 'probability', 'random', 'paired', 'sample', 'linear', 'dataset', 'statistical model', 'statistical', 'linear regression model', 'model', 'regression model', 'regression', 'distribution', 'linear regression', 'random variables', 'variables', 'probability distribution']", Basic statistical models,seg_227,"in this chapter we introduce a common statistical model. it corresponds to the situation where the elements of the dataset are repeated measurements of the same quantity and where different measurements do not influence each other. next, we discuss the probability distribution of the random variables that model the measurements and illustrate how sample statistics can help to select a suitable statistical model. finally, we discuss the simple linear regression model that corresponds to the situation where the elements of the dataset are paired measurements."
1721,1,"['measurements', 'probability', 'random', 'experiment', 'probability distributions', 'random variable', 'distributions', 'model', 'experimental', 'outcome', 'random variables', 'measurement', 'table', 'variables', 'variable', 'variation', 'measuring', 'realization']", Random samples and statistical models,seg_229,"in chapter 1 we briefly discussed michelson’s experiment conducted between june 5 and july 2 in 1879, in which 100 measurements were obtained on the speed of light. the values are given in table 17.1 and represent the speed of light in air in km/sec minus 299 000. the variation among the 100 values suggests that measuring the speed of light is subject to random influences. as we have seen before, we describe random phenomena by means of a probability model, i.e., we interpret the outcome of an experiment as a realization of some random variable. hence the first measurement is modeled by a random variable x1 and the value 850 is interpreted as the realization of x1. similarly, the second measurement is modeled by a random variable x2 and the value 740 is interpreted as the realization of x2. since both measurements are obtained under the same experimental conditions, it is justified to assume that the probability distributions of x1 and x2 are the same. more generally, the 100 measurements are modeled by random variables"
1722,1,"['table', 'distribution', 'probability distribution', 'probability']", Random samples and statistical models,seg_229,"with the same probability distribution, and the values in table 17.1 are interpreted as realizations of x1, x2, . . . , x100. moreover, because we believe that"
1723,1,"['sample', 'random variables', 'variables', 'random sample', 'independence', 'measurements', 'random']", Random samples and statistical models,seg_229,"michelson took great care not to have the measurements influence each other, the random variables x1, x2, . . . , x100 are assumed to be mutually independent (see also remark 3.1 about physical and stochastic independence). such a collection of random variables is called a random sample or briefly, sample."
1724,1,"['sample', 'random', 'independent', 'mutually independent', 'distribution', 'probability distribution', 'probability', 'random sample']", Random samples and statistical models,seg_229,"random sample. a random sample is a collection of random variables x1, x2, . . . , xn, that have the same probability distribution and are mutually independent."
1725,1,"['sample', 'distribution function', 'random sample', 'distribution', 'variable', 'random variable', 'random', 'function']", Random samples and statistical models,seg_229,"if f is the distribution function of each random variable xi in a random sample, we speak of a random sample from f . similarly, we speak of a random sample from a density f , a random sample from an n(µ, σ2) distribution, etc."
1726,1,"['sample', 'random', 'random sample', 'variance']", Random samples and statistical models,seg_229,"quick exercise 17.1 suppose we have a random sample x1, x2 from a distribution with variance 1. compute the variance of x1 + x2."
1727,1,"['interval', 'probability', 'random sample', 'random', 'process', 'sample', 'rate', 'exponential distribution', 'data', 'intervals', 'exponential', 'model', 'disjoint', 'poisson process', 'failure', 'distribution', 'poisson', 'independent', 'failures', 'realization']", Random samples and statistical models,seg_229,"properties that are inherent to the random phenomenon under study may provide additional knowledge about the distribution of the sample. recall the software data discussed in chapter 15. the data are observed lengths in cpu seconds between successive failures that occur during the execution of a certain real-time command. typically, in a situation like this, in a small time interval, either 0 or 1 failure occurs. moreover, failures occur with small probability and in disjoint time intervals failures occur independent of each other. in addition, let us assume that the rate at which the failures occur is constant over time. according to chapter 12, this justifies the choice of a poisson process to model the series of failures. from the properties of the poisson process we know that the interfailure times are independent and have the same exponential distribution. hence we model the software data as the realization of a random sample from an exponential distribution."
1728,1,"['sample', 'model', 'continuous distribution', 'random sample', 'data', 'distribution', 'cases', 'random', 'continuous', 'realization']", Random samples and statistical models,seg_229,"in some cases we may not be able to specify the type of distribution. take, for instance, the old faithful data consisting of observed durations of eruptions of the old faithful geyser. due to lack of specific geological knowledge about the subsurface and the mechanism that governs the eruptions, we prefer not to assume a particular type of distribution. however, we do model the durations as the realization of a random sample from a continuous distribution on (0,∞)."
1729,1,"['sample', 'model', 'experimental', 'experiment', 'dataset', 'random sample', 'distribution', 'probability distribution', 'measurements', 'probability', 'random', 'realization']", Random samples and statistical models,seg_229,in each of the three examples the dataset was obtained from repeated measurements performed under the same experimental conditions. the basic statistical model for such a dataset is to consider the measurements as a random sample and to interpret the dataset as the realization of the random sample. knowledge about the phenomenon under study and the nature of the experiment may lead to partial specification of the probability distribution of each xi in the sample. this should be included in the model.
1730,1,"['sample', 'model', 'dataset', 'random sample', 'distribution', 'probability distribution', 'measurements', 'probability', 'random', 'realization']", Random samples and statistical models,seg_229,"statistical model for repeated measurements. a dataset consisting of values x1, x2, . . . , xn of repeated measurements of the same quantity is modeled as the realization of a random sample x1, x2, . . . , xn. the model may include a partial specification of the probability distribution of each xi."
1731,1,"['case', 'probability', 'sample', 'exponential distribution', 'approximation', 'exponential', 'parameter', 'statistical model', 'statistical', 'distributions', 'model', 'distribution', 'exponential distributions', 'continuous', 'probability distribution', 'continuous distributions']", Random samples and statistical models,seg_229,"the probability distribution of each xi is called the model distribution. usually it refers to a collection of distributions: in the old faithful example to the collection of all continuous distributions on (0,∞), in the software example to the collection of all exponential distributions. in the latter case the parameter of the exponential distribution is called the model parameter. the unique distribution from which the sample actually originates is assumed to be one particular member of this collection and is called the “true” distribution. similarly, in the software example, the parameter corresponding to the “true” exponential distribution is called the “true” parameter. the word true is put between quotation marks because it does not refer to something in the real world, but only to a distribution (or parameter) in the statistical model, which is merely an approximation of the real situation."
1732,1,"['distribution', 'model', 'dataset']", Random samples and statistical models,seg_229,quick exercise 17.2 we obtain a dataset of ten elements by tossing a coin ten times and recording the result of each toss. what is an appropriate statistical model and corresponding model distribution for this dataset?
1733,1,"['sample', 'model', 'dataset', 'random sample', 'case', 'independence', 'random', 'statistical model', 'statistical', 'realization', 'distributions']", Random samples and statistical models,seg_229,"of course there are situations where the assumption of independence or identical distributions is unrealistic. in that case a different statistical model would be more appropriate. however, we will restrict ourselves mainly to the case where the dataset can be modeled as the realization of a random sample."
1734,1,"['model', 'dataset', 'distribution', 'statistical model', 'statistical']", Random samples and statistical models,seg_229,"once we have formulated a statistical model for our dataset, we can use the dataset to infer knowledge about the model distribution. important questions about the corresponding model distribution are"
1735,1,"['distribution', 'model']", Random samples and statistical models,seg_229,ĺ which feature of the model distribution represents the quantity of interest
1736,1,['dataset'], Random samples and statistical models,seg_229,and how do we use our dataset to determine a value for this?
1737,1,"['distribution', 'model', 'dataset']", Random samples and statistical models,seg_229,ĺ which model distribution fits a particular dataset best?
1738,1,"['sample', 'curve', 'continuous distribution', 'continuous', 'estimate', 'dataset', 'distribution function', 'random sample', 'data', 'distribution', 'probability', 'random', 'function', 'realization']", Random samples and statistical models,seg_229,"these questions can be diverse, and answering them may be difficult. for instance, the old faithful data are modeled as a realization of a random sample from a continuous distribution. suppose we are interested in a complete characterization of the “true” distribution, such as the distribution function f or the probability density f . since there are no further specifications about the type of distribution, our problem would be to estimate the complete curve of f or f on the basis of our dataset."
1739,1,"['sample', 'random', 'exponential distribution', 'case', 'data', 'distribution', 'exponential', 'parameter', 'random sample', 'realization']", Random samples and statistical models,seg_229,"on the other hand, the software data are modeled as the realization of a random sample from an exponential distribution. in that case f and f are completely characterized by a single parameter λ:"
1740,1,"['parameter', 'dataset']", Random samples and statistical models,seg_229,"even if we are interested in the curves of f and f , our problem would reduce to estimating a single parameter on the basis of our dataset."
1741,1,"['model', 'measurement', 'experiment', 'distribution', 'cases']", Random samples and statistical models,seg_229,"in other cases we may not be interested in the distribution as a whole, but only in a specific feature of the model distribution that represents the quantity of interest. for instance, in a physical experiment, such as the one performed by michelson, one usually thinks of each measurement as"
1742,1,"['measurement error', 'measurement', 'error']", Random samples and statistical models,seg_229,measurement = quantity of interest + measurement error.
1743,1,"['case', 'measurements', 'random sample', 'random', 'sample', 'estimate', 'dataset', 'random variable', 'error', 'model', 'distribution', 'expectation', 'variance', 'measurement', 'variable', 'measurement error']", Random samples and statistical models,seg_229,"the quantity of interest, in this case the speed of light, is thought of as being some (unknown) constant and the measurement error is some random fluctuation. in the absence of systematic error, the measurement error can be modeled by a random variable with zero expectation and finite variance. in that case the measurements are modeled by a random sample from a distribution with some unknown expectation and finite variance. the speed of light is represented by the expectation of the model distribution. our problem would be to estimate the expectation of the model distribution on the basis of our dataset."
1744,1,"['graphical', 'dataset', 'distribution', 'expectation', 'statistical', 'numerical']", Random samples and statistical models,seg_229,"in the remaining chapters, we will develop several statistical methods to infer knowledge about the “true” distribution or about a specific feature of it, by means of a dataset. in the remainder of this chapter we will investigate how the graphical and numerical summaries of our dataset can serve as a first indication of what an appropriate choice would be for this distribution or for a specific feature, such as its expectation."
1745,1,[], Distribution features and sample statistics,seg_231,"in chapters 15 and 16 we have discussed several empirical summaries of datasets. they are examples of numbers, curves, and other objects that are a"
1746,1,"['random samples', 'dataset', 'samples', 'random', 'realization']", Distribution features and sample statistics,seg_231,"of the dataset x1, x2, . . . , xn only. since datasets are modeled as realizations of random samples x1, x2, . . . , xn, an object h(x1, x2, . . . , xn) is a realization of the corresponding random object"
1747,1,"['sample', 'random', 'statistic', 'sample statistic', 'random sample']", Distribution features and sample statistics,seg_231,"such an object, which depends on the random sample x1, x2, . . . , xn only, is called a sample statistic."
1748,1,"['sample', 'model', 'dataset', 'statistics', 'distribution', 'statistic', 'sample statistic', 'sample statistics', 'statistical model', 'statistical']", Distribution features and sample statistics,seg_231,"if a statistical model adequately describes the dataset at hand, then the sample statistics corresponding to the empirical summaries should somehow reflect corresponding features of the model distribution. we have already seen a mathematical justification for this in chapter 13 for the sample statistic"
1749,1,"['sample', 'law of large numbers', 'distribution', 'probability distribution', 'probability']", Distribution features and sample statistics,seg_231,"based on a sample x1, x2, . . . , xn from a probability distribution with expectation µ. according to the law of large numbers,"
1750,1,"['sample size', 'sample', 'sample mean', 'dataset', 'random sample', 'statistics', 'distribution', 'normal', 'expectation', 'mean', 'random', 'sample statistics', 'normal distribution']", Distribution features and sample statistics,seg_231,"for every ε > 0. this means that for large sample size n, the sample mean of most realizations of the random sample is close to the expectation of the corresponding distribution. in fact, all sample statistics discussed in chapters 15 and 16 are close to corresponding distribution features. to illustrate this we generate an artificial dataset from a normal distribution with parameters µ = 5 and σ = 2, using a technique similar to the one described in section 6.2. next, we compare the sample statistics with corresponding features of this distribution."
1751,1,"['distribution', 'function', 'distribution function', 'empirical distribution function']", Distribution features and sample statistics,seg_231,the empirical distribution function
1752,1,"['sample', 'distribution function', 'random sample', 'distribution', 'random', 'function']", Distribution features and sample statistics,seg_231,"let x1, x2, . . . , xn be a random sample from distribution function f , and let"
1753,1,"['sample', 'distribution function', 'law of large numbers', 'empirical distribution function', 'distribution', 'function']", Distribution features and sample statistics,seg_231,"be the empirical distribution function of the sample. another application of the law of large numbers (see exercise 13.7) yields that for every ε > 0,"
1754,1,"['sample', 'distribution function', 'random sample', 'empirical distribution function', 'distribution', 'random', 'function']", Distribution features and sample statistics,seg_231,this means that for most realizations of the random sample the empirical distribution function fn is close to f :
1755,1,"['dataset', 'distribution function', 'empirical distribution function', 'distribution', 'normal', 'function']", Distribution features and sample statistics,seg_231,hence the empirical distribution function of the normal dataset should resemble the distribution function
1756,1,"['sample size', 'sample', 'functions', 'empirical distribution functions', 'distribution function', 'distribution', 'function']", Distribution features and sample statistics,seg_231,"of the n(5, 4) distribution, and the fit should become better as the sample size n increases. an illustration of this can be found in figure 17.1. we displayed the empirical distribution functions of datasets generated from an n(5, 4) distribution together with the “true” distribution function f (dotted lines), for sample sizes n = 20 (left) and n = 200 (right)."
1757,1,"['histogram', 'estimate', 'kernel', 'kernel density estimate']", Distribution features and sample statistics,seg_231,the histogram and the kernel density estimate
1758,1,"['sample', 'continuous distribution', 'random', 'law of large numbers', 'distribution', 'probability', 'random sample', 'continuous']", Distribution features and sample statistics,seg_231,"suppose the random sample x1, x2, . . . , xn is generated from a continuous distribution with probability density f . in section 13.4 we have seen yet another consequence of the law of large numbers:"
1759,1,"['bin', 'sample', 'histogram', 'random', 'random sample']", Distribution features and sample statistics,seg_231,"when (x− h, x+ h] is a bin of a histogram of the random sample, this means that the height of the histogram approximates the value of f at the midpoint of the bin:"
1760,1,['histogram'], Distribution features and sample statistics,seg_231,"height of the histogram on (x − h, x + h] ≈ f(x)."
1761,1,"['sample', 'random', 'estimate', 'kernel', 'kernel density estimate', 'probability', 'random sample']", Distribution features and sample statistics,seg_231,"similarly, the kernel density estimate of a random sample approximates the corresponding probability density f :"
1762,1,"['histogram', 'estimate', 'dataset', 'kernel', 'kernel density estimate', 'normal', 'probability']", Distribution features and sample statistics,seg_231,so the histogram and kernel density estimate of the normal dataset should resemble the graph of the probability density
1763,1,"['histogram', 'estimate', 'dataset', 'kernel', 'kernel density estimate', 'distribution']", Distribution features and sample statistics,seg_231,"of the n(5, 4) distribution. this is illustrated in figure 17.2, where we displayed a histogram and a kernel density estimate of our dataset consisting of 200 values generated from the n(5, 4) distribution. it should be noted that with a smaller dataset the similarity can be much worse. this is demonstrated in figure 17.3, which is based on the dataset consisting of 20 values generated from the same distribution."
1764,1,"['sample median', 'sample', 'sample mean', 'median', 'empirical quantiles', 'quantiles', 'mean']", Distribution features and sample statistics,seg_231,"the sample mean, the sample median, and empirical quantiles"
1765,1,"['dataset', 'law of large numbers', 'distribution', 'expectation']", Distribution features and sample statistics,seg_231,"as we saw in section 5.5, the expectation of an n(µ, σ2) distribution is µ; so the n(5, 4) distribution has expectation 5. according to the law of large numbers: x̄n ≈ µ. this is illustrated by our dataset of 200 values generated from the n(5, 4) distribution for which we find"
1766,1,"['sample median', 'sample', 'median']", Distribution features and sample statistics,seg_231,for the sample median we find
1767,1,"['sample median', 'sample', 'quantile', 'random', 'empirical quantile', 'median', 'random sample']", Distribution features and sample statistics,seg_231,"this illustrates the fact that the sample median of a random sample from f approximates the median q0.5 = f inv(0.5). in fact, we have the following general property for the pth empirical quantile:"
1768,1,"['sample median', 'sample', 'sample mean', 'median', 'dataset', 'case', 'distribution', 'expectation', 'normal', 'mean']", Distribution features and sample statistics,seg_231,"in the special case of the n(µ, σ2) distribution, the expectation and the median coincide, which explains why the sample mean and sample median of the normal dataset are so close to each other."
1769,1,"['deviation', 'sample', 'standard', 'standard deviation', 'sample variance', 'variance']", Distribution features and sample statistics,seg_231,"the sample variance and standard deviation, and the mad"
1770,1,"['deviation', 'law of large numbers', 'distribution', 'standard', 'standard deviation', 'variance']", Distribution features and sample statistics,seg_231,"as we saw in section 5.5, the standard deviation and variance of an n(µ, σ2) distribution are σ and σ2; so for the n(5, 4) distribution these are 2 and 4. another consequence of the law of large numbers is that"
1771,1,"['normal', 'dataset']", Distribution features and sample statistics,seg_231,"this is illustrated by our normal dataset of size 200, for which we find"
1772,1,"['deviation', 'sample', 'sample variance', 'sample standard deviation', 'standard', 'standard deviation', 'variance']", Distribution features and sample statistics,seg_231,for the sample variance and sample standard deviation.
1773,1,"['deviation', 'dataset', 'distribution', 'standard', 'standard deviation']", Distribution features and sample statistics,seg_231,"for the mad of the dataset we find 1.334, which clearly differs from the standard deviation 2 of the n(5, 4) distribution. the reason is that"
1774,1,"['median', 'standard normal', 'symmetric', 'distribution function', 'distribution', 'normal', 'standard', 'function', 'standard normal distribution', 'normal distribution']", Distribution features and sample statistics,seg_231,"for any distribution that is symmetric around its median f inv(0.5). for the n(5, 4) distribution f inv(0.75) − f inv(0.5) = 2φinv(0.75) = 1.3490, where φ denotes the distribution function of the standard normal distribution (see exercise 17.10)."
1775,1,['frequencies'], Distribution features and sample statistics,seg_231,relative frequencies
1776,1,"['histogram', 'estimates', 'probability mass function', 'case', 'discrete', 'statistic', 'sample statistic', 'probability', 'random sample', 'random', 'function', 'discrete distribution', 'sample', 'mass function', 'kernel', 'distributions', 'kernel density estimates', 'frequencies', 'relative frequencies', 'distribution', 'continuous', 'probabilities', 'law of large numbers', 'continuous distributions']", Distribution features and sample statistics,seg_231,"for continuous distributions the histogram and kernel density estimates of a random sample approximate the corresponding probability density f . for discrete distributions we would like to have a sample statistic that approximates the probability mass function. in section 13.4 we saw that, as a consequence of the law of large numbers, relative frequencies based on a random sample approximate corresponding probabilities. as a special case, for a random sample x1, x2, . . . , xn from a discrete distribution with probability mass function p,"
1777,1,"['sample', 'table', 'mass function', 'probability mass function', 'relative frequency', 'statistics', 'distribution', 'frequency', 'probability', 'sample statistics', 'function']", Distribution features and sample statistics,seg_231,n this means that the relative frequency of a’s in the sample approximates the value of the probability mass function at a. table 17.2 lists the sample statistics and the corresponding distribution features they approximate.
1778,1,"['sample', 'estimate', 'dataset', 'random sample', 'statistics', 'case', 'distribution', 'probability distribution', 'sample statistics', 'probability', 'random', 'realization']", Estimating features of the true distribution,seg_233,"in the previous section we generated a dataset of 200 elements from a probability distribution, and we have seen that certain features of this distribution are approximated by corresponding sample statistics. in practice, the situation is reversed. in that case we have a dataset of n elements that is modeled as the realization of a random sample with a probability distribution that is unknown to us. our goal is to use our dataset to estimate a certain feature of this distribution that represents the quantity of interest. in this section we will discuss a few examples."
1779,1,['data'], Estimating features of the true distribution,seg_233,the old faithful data
1780,1,"['distribution function', 'continuous probability distribution', 'probability', 'random sample', 'random', 'function', 'sample', 'estimate', 'dataset', 'kernel', 'data', 'distributions', 'density function', 'model', 'probability density function', 'kernel density estimate', 'empirical distribution function', 'distribution', 'continuous', 'probability distribution', 'realization']", Estimating features of the true distribution,seg_233,"we stick to the assumptions of section 17.1: by lack of knowledge on this phenomenon we prefer not to specify a particular parametric type of distribution, and we model the old faithful data as the realization of a random sample of size 272 from a continuous probability distribution. from the previous section we know that the kernel density estimate and the empirical distribution function of the dataset approximate the probability density f and the distribution function f of this distribution. in figure 17.4 a kernel density estimate (left) and the empirical distribution function (right) are displayed. indeed, neither graph resembles the probability density function or distribution function of any of the familiar parametric distributions. instead of viewing both graphs"
1781,1,"['model', 'graphical', 'estimate', 'estimates', 'kernel', 'distribution function', 'kernel density estimate', 'empirical distribution function', 'data', 'distribution', 'probability', 'function']", Estimating features of the true distribution,seg_233,"only as graphical summaries of the data, we can also use both curves as estimates for f and f . we estimate the model probability density f by means of the kernel density estimate and the model distribution function f by means of the empirical distribution function. since neither estimate assumes a particular parametric model, they are called nonparametric estimates."
1782,1,['data'], Estimating features of the true distribution,seg_233,the software data
1783,1,"['plot', 'sample', 'model', 'random', 'exponential distribution', 'histogram', 'estimate', 'kernel', 'kernel density estimate', 'data', 'distribution', 'exponential', 'random sample', 'realization']", Estimating features of the true distribution,seg_233,"next consider the software reliability data. as motivated in section 17.1, we model interfailure times as the realization of a random sample from an exponential distribution. to see whether an exponential distribution is indeed a reasonable model, we plot a histogram and a kernel density estimate using a boundary kernel in figure 17.5."
1784,1,"['estimates', 'range', 'distribution function', 'function', 'sample', 'exponential distribution', 'estimate', 'dataset', 'kernel', 'data', 'exponential', 'parameter', 'model', 'kernel density estimate', 'empirical distribution function', 'distribution', 'estimated', 'law of large numbers']", Estimating features of the true distribution,seg_233,"both seem to corroborate the assumption of an exponential distribution. accepting this, we are left with estimating the parameter λ. because for the exponential distribution e[x ] = 1/λ, the law of large numbers suggests 1/x̄ as an estimate for λ. for our dataset x̄ = 656.88, which yields 1/x̄ = 0.0015. in figure 17.6 we compare the estimated exponential density (left) and distribution function (right) with the corresponding nonparametric estimates. note that the nonparametric estimates do not assume an exponential model for the data. but, if an exponential distribution were the right model, the kernel density estimate and empirical distribution function should resemble the estimated exponential density and distribution function. at first sight the fit seems reasonable, although near zero the data accumulate more than one might perhaps expect for a sample of size 135 from an exponential distribution, and the other way around at the other end of the data range. the question is whether this phenomenon can be attributed to chance or is caused by the fact that the exponential model is the wrong model. we will return to this type of question in chapter 25 (see also chapter 18)."
1785,1,['data'], Estimating features of the true distribution,seg_233,michelson data
1786,1,"['sample', 'sample mean', 'estimate', 'estimation', 'law of large numbers', 'case', 'data', 'distribution', 'expectation', 'mean']", Estimating features of the true distribution,seg_233,"consider the michelson data on the speed of light. in this case we are not particularly interested in estimation of the “true” distribution, but solely in the expectation of this distribution, which represents the speed of light. the law of large numbers suggests to estimate the expectation by the sample mean x̄, which equals 852.4."
1787,0,[], The linear regression model,seg_235,"recall the example about predicting janka hardness of wood from the density of the wood in section 15.5. the idea is, of course, that janka hardness is related to the density: the higher the density of the wood, the higher the value of janka hardness. this suggests a relationship of the type"
1788,0,[], The linear regression model,seg_235,hardness = g(density of timber)
1789,1,"['model', 'regression model', 'dataset', 'table', 'bivariate dataset', 'scatterplot', 'data', 'regression', 'function', 'bivariate']", The linear regression model,seg_235,"for some increasing function g. this is supported by the scatterplot of the data in figure 17.7. a closer look at the bivariate dataset in table 15.5 suggests that randomness is also involved. for instance, for the value 51.5 of the density, different corresponding values of janka hardness were observed. one way to model such a situation is by means of a regression model :"
1790,1,['random'], The linear regression model,seg_235,hardness = g(density of timber) + random fluctuation.
1791,1,"['function', 'scatterplot']", The linear regression model,seg_235,the important question now is what sort of function g fits well to the points in the scatterplot?
1792,1,['data'], The linear regression model,seg_235,"in general, this may be a difficult question to answer. we may have so little knowledge about the phenomenon under study, and the data points may be"
1793,1,"['scatterplot', 'data', 'random', 'function']", The linear regression model,seg_235,"scattered in such a way, that there is no reason to assume a specific type of function for g. however, for the janka hardness data it makes sense to assume that g is increasing, but this still leaves us with many possibilities. looking at the scatterplot, at first sight it does not seem unreasonable to assume that g is a straight line, i.e., janka hardness depends linearly on the density of timber. the fact that the points are not exactly on a straight line is then modeled by a random fluctuation with respect to the straight line:"
1794,1,['random'], The linear regression model,seg_235,hardness = α + β · (density of timber) + random fluctuation.
1795,1,"['simple linear regression', 'linear', 'model', 'regression model', 'regression', 'linear regression', 'linear regression model']", The linear regression model,seg_235,this is a loose description of a simple linear regression model. a more complete description is given below.
1796,1,"['simple linear regression', 'model', 'linear', 'random variables', 'regression model', 'dataset', 'variables', 'bivariate dataset', 'regression', 'linear regression', 'random', 'bivariate', 'linear regression model']", The linear regression model,seg_235,"simple linear regression model. in a simple linear regression model for a bivariate dataset (x1, y1), (x2, y2), . . . , (xn, yn), we assume that x1, x2, . . . , xn are nonrandom and that y1, y2, . . . , yn are realizations of random variables y1, y2, . . . , yn satisfying"
1797,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random']", The linear regression model,seg_235,"where u1, . . . , un are independent random variables with e[ui] = 0 and var(ui) = σ2."
1798,1,"['dependent', 'measurements', 'random', 'independent and dependent variables', 'parameters', 'regression', 'regression line', 'response', 'random variables', 'independent', 'variables', 'intercept', 'dependent variables', 'variable', 'explanatory variable', 'explanatory', 'response variable', 'slope']", The linear regression model,seg_235,"the line y = α + βx is called the regression line. the parameters α and β represent the intercept and slope of the regression line. usually, the x-variable is called the explanatory variable and the y-variable is called the response variable. one also refers to x and y as independent and dependent variables. the random variables u1, u2, . . . , un are assumed to be independent when the different measurements do not influence each other. they are assumed to have"
1799,1,"['sample', 'variability', 'random sample', 'regression', 'independence', 'expectation', 'random', 'variance', 'regression line', 'distributions']", The linear regression model,seg_235,"expectation zero, because the random fluctuation is considered to be around the regression line y = α + βx. finally, because each random fluctuation is supposed to have the same amount of variability, we assume that all ui have the same variance. note that by the propagation of independence rule in section 9.4, independence of the ui implies independence of yi. however, y1, y2, . . . , yn do not form a random sample. indeed, the yi have different distributions because every yi has a different expectation"
1800,1,"['simple linear regression', 'linear', 'model', 'regression model', 'regression', 'linear regression', 'variance', 'linear regression model']", The linear regression model,seg_235,quick exercise 17.3 consider the simple linear regression model as defined earlier. compute the variance of yi.
1801,1,"['estimated', 'parameters', 'estimate', 'scatterplot', 'data']", The linear regression model,seg_235,the parameters α and β are unknown and our task will be to estimate them on the basis of the data. we will come back to this in chapter 22. in figure 17.8 the scatterplot for the janka hardness data is displayed with the estimated
1802,0,[], The linear regression model,seg_235,"taking a closer look at figure 17.8, you might wonder whether"
1803,1,"['simple linear regression', 'linear', 'model', 'regression', 'linear regression', 'multiple linear regression']", The linear regression model,seg_235,would be a more appropriate model. by trying to answer this question we enter the area of multiple linear regression. we will not pursue this topic; we restrict ourselves to simple linear regression.
1804,1,"['sample', 'random variables', 'independent', 'variables', 'random sample', 'independent random variables', 'random', 'variance']", Solutions to the quick exercises,seg_237,"17.1 because x1, x2 form a random sample, they are independent. using the rule about the variance of the sum of independent random variables, this means that var(x1 + x2) = var(x1) + var(x2) = 1 + 1 = 2."
1805,1,"['case', 'probability', 'random sample', 'random', 'sample', 'random variable', 'parameter', 'bernoulli distribution', 'bernoulli random variable', 'model', 'distribution', 'outcomes', 'bernoulli', 'variable', 'tails', 'realization']", Solutions to the quick exercises,seg_237,"17.2 the result of each toss of a coin can be modeled by a bernoulli random variable taking values 1 (heads) and 0 (tails). in the case when it is known that we are tossing a fair coin, heads and tails occur with equal probability. since it is reasonable to assume that the tosses do not influence each other, the outcomes of the ten tosses are modeled as the realization of a random sample x1, . . . , x10 from a bernoulli distribution with parameter p = 1/2. in this case the model distribution is completely specified and coincides with the “true” distribution: a ber(1"
1806,1,['distribution'], Solutions to the quick exercises,seg_237,2 ) distribution.
1807,1,"['sample', 'model', 'random', 'bernoulli distribution', 'case', 'distribution', 'bernoulli', 'parameter', 'random sample', 'outcomes', 'realization']", Solutions to the quick exercises,seg_237,"in the case when we are dealing with a possibly unfair coin, the outcomes of the ten tosses are still modeled as the realization of a random sample x1, . . . , x10 from a bernoulli distribution, but we cannot specify the value of the parameter p. the model distribution is a bernoulli distribution. the “true” distribution is a bernoulli distribution with one particular value for p, unknown to us."
1808,1,"['estimates', 'dataset', 'kernel', 'exponential', 'histograms', 'distributions', 'functions', 'normal distributions', 'kernel density estimates', 'empirical distribution functions', 'distribution', 'exponential distributions', 'normal']", Exercises,seg_239,"17.1 figure 17.9 displays several histograms, kernel density estimates, and empirical distribution functions. it is known that all figures correspond to datasets of size 200 that are generated from normal distributions n(0, 1), n(0, 9), and n(3, 1), and from exponential distributions exp(1) and exp(1/3). report for each figure from which distribution the dataset has been generated."
1809,1,"['dataset', 'distribution', 'boxplot', 'boxplots']", Exercises,seg_239,17.2 figure 17.10 displays several boxplots. it is known that all figures correspond to datasets of size 200 that are generated from the same five distributions as in exercise 17.1. report for each boxplot from which distribution the dataset has been generated.
1810,1,"['table', 'dataset']", Exercises,seg_239,"17.3 at a london underground station, the number of women was counted in each of 100 queues of length 10. in this way a dataset x1, x2, . . . , x100 was obtained, where xi denotes the observed number of women in the ith queue. the dataset is summarized in the following table and lists the number of queues with 0 women, 1 woman, 2 women, etc."
1811,1,['frequency'], Exercises,seg_239,count 0 1 2 3 4 5 6 7 8 9 10 frequency 1 3 4 23 25 19 18 5 1 1 0
1812,1,"['sample', 'model', 'random', 'dataset', 'random sample', 'statistical model', 'statistical', 'realization']", Exercises,seg_239,"in the statistical model for this dataset, we assume that the observed counts are a realization of a random sample x1, x2, . . . , x100."
1813,0,[], Exercises,seg_239,a. assume that people line up in such a way that a man or woman in a
1814,1,"['distribution', 'model', 'independent', 'probability']", Exercises,seg_239,"certain position is independent of the other positions, and that in each position one has a woman with equal probability. what is an appropriate choice for the model distribution?"
1815,1,"['model', 'table', 'estimate']", Exercises,seg_239,b. use the table to find an estimate for the parameter(s) of the model dis-
1816,0,[], Exercises,seg_239,tribution chosen in part a.
1817,1,"['table', 'data', 'dataset']", Exercises,seg_239,"17.4 during the second world war, london was hit by numerous flying bombs. the following data are from an area in south london of 36 square kilometers. the area was divided into 576 squares with sides of length 1/4 kilometer. for each of the 576 squares the number of hits was recorded. in this way we obtain a dataset x1, x2, . . . , x576, where xi denotes the number of hits in the ith square. the data are summarized in the following table which lists the number of squares with no hits, 1 hit, 2 hits, etc."
1818,1,"['poisson', 'case', 'data', 'distribution', 'random', 'poisson distribution']", Exercises,seg_239,an interesting question is whether london was hit in a completely random manner. in that case a poisson distribution should fit the data.
1819,1,"['sample', 'model', 'random', 'dataset', 'random sample', 'realization']", Exercises,seg_239,a. if we model the dataset as the realization of a random sample from a
1820,1,"['distribution', 'parameter', 'estimate']", Exercises,seg_239,"poisson distribution with parameter µ, then what would you choose as an estimate for µ?"
1821,1,"['poisson', 'distribution', 'poisson distribution']", Exercises,seg_239,b. check the fit with a poisson distribution by comparing some of the ob-
1822,1,"['poisson', 'estimated', 'probabilities', 'frequencies', 'relative frequencies', 'distribution', 'poisson distribution']", Exercises,seg_239,"served relative frequencies of 0’s, 1’s, 2’s, etc., with the corresponding probabilities for the poisson distribution with µ estimated as in part a."
1823,1,"['data', 'random variable', 'variable', 'tables', 'random', 'geometric']", Exercises,seg_239,"17.5 we return to the example concerning the number of menstrual cycles up to pregnancy, where the number of cycles was modeled by a geometric random variable (see section 4.4). the original data concerned 100 smoking and 486 nonsmoking women. for 7 smokers and 12 nonsmokers, the exact number of cycles up to pregnancy was unknown. in the following tables we only"
1824,1,['dataset'], Exercises,seg_239,"incorporated the 93 smokers and 474 nonsmokers, for which the exact number of cycles was observed. another analysis, based on the complete dataset, is done in section 21.1."
1825,1,['dataset'], Exercises,seg_239,"a. consider the dataset x1, x2, . . . , x93 corresponding to the smoking women,"
1826,1,"['data', 'table']", Exercises,seg_239,where xi denotes the number of cycles for the ith smoking woman. the data are summarized in the following table.
1827,1,['frequency'], Exercises,seg_239,cycles 1 2 3 4 5 6 7 8 9 10 11 12 frequency 29 16 17 4 3 9 4 5 1 1 1 3
1828,1,"['sample', 'model', 'random', 'estimate', 'dataset', 'geometric distribution', 'table', 'distribution', 'parameter', 'random sample', 'geometric', 'realization']", Exercises,seg_239,"the table lists the number of women that had to wait 1 cycle, 2 cycles, etc. if we model the dataset as the realization of a random sample from a geometric distribution with parameter p, then what would you choose as an estimate for p?"
1829,1,"['parameter', 'estimate']", Exercises,seg_239,"b. also estimate the parameter p for the 474 nonsmoking women, which"
1830,1,"['sample', 'random', 'dataset', 'geometric distribution', 'distribution', 'random sample', 'geometric', 'realization']", Exercises,seg_239,"is also modeled as the realization of a random sample from a geometric distribution. the dataset y1, y2, . . . , y474, where yj denotes the number of cycles for the jth nonsmoking woman, is summarized here:"
1831,1,['frequency'], Exercises,seg_239,cycles 1 2 3 4 5 6 7 8 9 10 11 12 frequency 198 107 55 38 18 22 7 9 5 3 6 6
1832,1,"['estimates', 'probability']", Exercises,seg_239,c. compare the estimates of the probability of becoming pregnant in three
1833,0,[], Exercises,seg_239,or fewer cycles for smoking and nonsmoking women.
1834,1,"['sample', 'histogram', 'random sample', 'data', 'distribution', 'normal', 'random', 'realization', 'normal distribution']", Exercises,seg_239,"17.6 recall exercise 15.1 about the chest circumference of 5732 scottish soldiers, where we constructed the histogram displayed in figure 17.11. the histogram suggests modeling the data as the realization of a random sample from a normal distribution."
1835,1,[], Exercises,seg_239,a. suppose that for the dataset∑xi = 228377.2 and∑x2i = 9124064. what
1836,1,"['parameters', 'estimates', 'distribution']", Exercises,seg_239,"would you choose as estimates for the parameters µ and σ of the n(µ, σ2) distribution? hint: you may want to use the relation from exercise 16.15."
1837,1,"['probability', 'estimate']", Exercises,seg_239,b. give an estimate for the probability that a scottish soldier has a chest
1838,0,[], Exercises,seg_239,circumference between 38.5 and 42.5 inches.
1839,1,"['poisson', 'rate', 'poisson process', 'histogram', 'distribution function', 'empirical distribution function', 'intervals', 'distribution', 'probability', 'function', 'process']", Exercises,seg_239,17.7 recall exercise 15.3 about time intervals between successive coal mine disasters. let us assume that the rate at which the disasters occur is constant over time and that on a single day a disaster takes place with small probability independently of what happens on other days. according to chapter 12 this suggests modeling the series of disasters with a poisson process. figure 17.12 displays a histogram and empirical distribution function of the observed time intervals.
1840,1,"['model', 'dataset', 'intervals', 'statistical model', 'statistical']", Exercises,seg_239,a. in the statistical model for this dataset we model the 190 time intervals
1841,1,"['sample', 'model', 'random', 'distribution', 'random sample', 'realization']", Exercises,seg_239,as the realization of a random sample. what would you choose for the model distribution?
1842,1,"['intervals', 'estimate']", Exercises,seg_239,b. the sum of the observed time intervals is 40 549 days. give an estimate
1843,1,['distribution'], Exercises,seg_239,for the parameter(s) of the distribution chosen in part a.
1844,1,"['failure', 'data']", Exercises,seg_239,17.8 the following data represent the number of revolutions to failure (in millions) of 22 deep-groove ball-bearings.
1845,1,"['sample', 'weibull', 'dataset', 'distribution function', 'random sample', 'distribution', 'random', 'function', 'weibull distribution', 'realization']", Exercises,seg_239,"lieblein and zelen propose modeling the dataset as a realization of a random sample from a weibull distribution, which has distribution function"
1846,1,"['distribution', 'random variable', 'variable', 'random', 'weibull distribution', 'weibull']", Exercises,seg_239,a. suppose that x is a random variable with a weibull distribution. check
1847,1,"['exponential distribution', 'distribution', 'random variable', 'variable', 'exponential', 'parameter', 'random']", Exercises,seg_239,that the random variable y = xα has an exponential distribution with parameter λα and conclude that e[xα] = 1/λα.
1848,1,"['data', 'table']", Exercises,seg_239,b. use part a to explain how one can use the data in the table to find
1849,1,"['estimated', 'parameter', 'estimate']", Exercises,seg_239,"an estimate for the parameter λ, if it is given that the parameter α is estimated by 2.102."
1850,1,"['table', 'scatterplot', 'data', 'estimate']", Exercises,seg_239,"17.9 the volume (i.e., the effective wood production in cubic meters), height (in meters), and diameter (in meters) (measured at 1.37 meter above the ground) are recorded for 31 black cherry trees in the allegheny national forest in pennsylvania. the data are listed in table 17.3. they were collected to find an estimate for the volume of a tree (and therefore for the timber yield), given its height and diameter. for each tree the volume y and the value of x = d2h are recorded, where d and h are the diameter and height of the tree. the resulting points (x1, y1), . . . , (x31, y31) are displayed in the scatterplot in figure 17.13."
1851,1,"['linear', 'model', 'regression model', 'data', 'regression', 'intercept', 'linear regression', 'linear regression model']", Exercises,seg_239,we model the data by the following linear regression model (without intercept)
1852,1,['linear'], Exercises,seg_239,a. what physical reasons justify the linear relationship between y and d2h?
1853,0,[], Exercises,seg_239,hint: how does the volume of a cylinder relate to its diameter and height?
1854,1,"['slope', 'estimate']", Exercises,seg_239,b. we want to find an estimate for the slope β of the line y = βx. two
1855,1,"['average', 'slope']", Exercises,seg_239,"natural candidates are the average slope z̄n, where zi = yi/xi, and the"
1856,1,[], Exercises,seg_239,slope of the averages ȳ/x̄. in chapter 22 we will encounter the so-called
1857,1,['estimate'], Exercises,seg_239,least squares estimate: n
1858,1,"['estimates', 'table', 'data']", Exercises,seg_239,"compute all three estimates for the data in table 17.3. you need at least 5 digits accuracy, and you may use that ∑ xi = 87.456, ∑ yi = 26.486, ∑ yi/xi = 9.369, ∑ xiyi = 95.498, and ∑x2i = 314.644."
1859,1,"['continuous', 'median', 'distribution function', 'distribution', 'random variable', 'variable', 'random', 'function']", Exercises,seg_239,17.10 let x be a random variable with (continuous) distribution function f . let m = q0.5 = f inv(0.5) be the median of f and define the random variable
1860,1,"['distribution', 'function', 'distribution function']", Exercises,seg_239,"a. show that y has distribution function g, defined by"
1861,1,['median'], Exercises,seg_239,b. the mad of f is the median of g. show that if the density f correspond-
1862,1,"['symmetric', 'median']", Exercises,seg_239,"ing to f is symmetric around its median m, then"
1863,1,['distribution'], Exercises,seg_239,"c. use b to conclude that the mad of an n(µ, σ2) distribution is equal to"
1864,1,"['standard normal', 'distribution function', 'distribution', 'normal', 'standard', 'function', 'standard normal distribution', 'normal distribution']", Exercises,seg_239,"σφinv(3/4), where φ is the distribution function of a standard normal distribution. recall that the distribution function f of an n(µ, σ2) can"
1865,1,['distribution'], Exercises,seg_239,"you might check that, as stated in section 17.2, the mad of the n(5, 4) distribution is equal to 2φinv(3/4) = 1.3490."
1866,1,['distribution'], Exercises,seg_239,17.11 in this exercise we compute the mad of the exp(λ) distribution.
1867,1,"['distribution', 'median']", Exercises,seg_239,"a. let x have an exp(λ) distribution, with median m = (ln 2)/λ. show that"
1868,1,"['distribution', 'function', 'distribution function']", Exercises,seg_239,y = |x − m| has distribution function
1869,1,['distribution'], Exercises,seg_239,b. argue that the mad of the exp(λ) distribution is a solution of the equa-
1870,1,['distribution'], Exercises,seg_239,c. compute the mad of the exp(λ) distribution.
1871,1,"['distribution function', 'statistics', 'statistic', 'sample statistic', 'sample statistics', 'probability', 'function', 'sample', 'sample mean', 'estimate', 'simulation', 'approximation', 'mean', 'statistical', 'limit', 'sample size', 'model', 'distribution', 'expectation', 'method', 'normal']", The bootstrap,seg_241,"in the forthcoming chapters we will develop statistical methods to infer knowledge about the model distribution and encounter several sample statistics to do this. in the previous chapter we have seen examples of sample statistics that can be used to estimate different model features, for instance, the empirical distribution function to estimate the model distribution function f , and the sample mean to estimate the expectation µ corresponding to f . one of the things we would like to know is how close a sample statistic is to the model feature it is supposed to estimate. for instance, what is the probability that the sample mean and µ differ more than a given tolerance ε? for this we need to know the distribution of x̄n − µ. more generally, it is important to know how a sample statistic is distributed in relation to the corresponding model feature. for the distribution of the sample mean we saw a normal limit approximation in chapter 14. in this chapter we discuss a simulation procedure that approximates the distribution of the sample mean for finite sample size. moreover, the method is more generally applicable to sample statistics other than the sample mean."
1872,1,"['distribution function', 'random sample', 'random', 'function', 'sample', 'sample mean', 'estimate', 'dataset', 'data', 'random variable', 'mean', 'distribution', 'expectation', 'variable', 'realization']", The bootstrap principle,seg_243,"consider the old faithful data introduced in chapter 15, which we modeled as the realization of a random sample of size n = 272 from some distribution function f . the sample mean x̄n of the observed durations equals 209.3. what does this say about the expectation µ of f? as we saw in chapter 17, the value 209.3 is a natural estimate for µ, but to conclude that µ is equal to 209.3 is unwise. the reason is that, if we would observe a new dataset of durations, we will obtain a different sample mean as an estimate for µ. this should not come as a surprise. since the dataset x1, x2, . . . , xn is just one possible realization of the random sample x1, x2, . . . , xn, the observed sample mean is just one possible realization of the random variable"
1873,1,"['sample', 'sample mean', 'dataset', 'random sample', 'variation', 'distribution', 'random variable', 'probability distribution', 'variable', 'mean', 'probability', 'random', 'vary', 'realization']", The bootstrap principle,seg_243,"a new dataset is another realization of the random sample, and the corresponding sample mean is another realization of the random variable x̄n. hence, to infer something about µ, one should take into account how realizations of x̄n vary. this variation is described by the probability distribution of x̄n."
1874,1,"['distribution function', 'probability', 'random sample', 'random', 'function', 'sample', 'sample mean', 'estimate', 'bootstrap', 'dataset', 'random variable', 'mean', 'distribution', 'variable', 'probability distribution']", The bootstrap principle,seg_243,"in principle1 it is possible to determine the distribution function of x̄n from the distribution function f of the random sample x1, x2, . . . , xn. however, f is unknown. nevertheless, in chapter 17 we saw that the observed dataset reflects most features of the “true” probability distribution. hence the natural thing to do is to compute an estimate f̂ for the distribution function f and then to consider a random sample from f̂ and the corresponding sample mean as substitutes for the random sample x1, x2, . . . , xn from f and the random variable x̄n. a random sample from f̂ is called a bootstrap random sample, or briefly bootstrap sample, and is denoted by"
1875,1,"['sample', 'sample mean', 'random sample', 'variable', 'random variable', 'mean', 'bootstrapped', 'random', 'average']", The bootstrap principle,seg_243,"to distinguish it from the random sample x1, x2, . . . , xn from the “true” f . the corresponding average is called the bootstrapped sample mean, and this random variable is denoted by"
1876,1,"['distribution', 'random variable', 'variable', 'random']", The bootstrap principle,seg_243,to distinguish it from the random variable x̄n. the idea is now to use the distribution of x̄n
1877,1,['distribution'], The bootstrap principle,seg_243,∗ to approximate the distribution of x̄n.
1878,1,"['sample', 'sample mean', 'bootstrap', 'distribution', 'probability distribution', 'statistic', 'sample statistic', 'mean', 'probability', 'bootstrapped']", The bootstrap principle,seg_243,"the preceding procedure is called the bootstrap principle for the sample mean. clearly, it can be applied to any sample statistic h(x1, x2, . . . , xn) by approximating its probability distribution by that of the corresponding bootstrapped sample statistic h(x1∗, x2∗, . . . , xn"
1879,1,"['sample', 'estimate', 'dataset', 'distribution function', 'random sample', 'distribution', 'probability', 'random', 'function']", The bootstrap principle,seg_243,"bootstrap principle. use the dataset x1, x2, . . . , xn to compute an estimate f̂ for the “true” distribution function f . replace the random sample x1, x2, . . . , xn from f by a random sample x1∗, x2∗, . . . , xn ∗ from f̂ , and approximate the probability distribution of h(x1, x2, . . . , xn) by that of h(x1∗, x2∗, . . . , xn"
1880,1,"['sample', 'sample mean', 'distribution', 'mean']", The bootstrap principle,seg_243,"returning to the sample mean, the first question that comes to mind is, of course, how well does the distribution of x̄n"
1881,1,['distribution'], The bootstrap principle,seg_243,∗ approximate the distribution
1882,1,"['sample', 'distribution', 'statistic', 'sample statistic', 'bootstrapped']", The bootstrap principle,seg_243,"of x̄n? or more generally, how well does the distribution of a bootstrapped sample statistic h(x1∗, x2∗, . . . , xn"
1883,1,['distribution'], The bootstrap principle,seg_243,∗) approximate the distribution of the sam-
1884,1,"['sample', 'sample mean', 'bootstrap', 'approximation', 'distribution', 'statistic', 'mean']", The bootstrap principle,seg_243,"ple statistic of interest h(x1, x2, . . . , xn)? applied in such a straightforward manner, the bootstrap approximation for the distribution of x̄n by that of x̄n ∗ may not be so good (see remark 18.1). the bootstrap approximation will improve if we approximate the distribution of the centered sample mean:"
1885,1,"['random variable', 'variable', 'expectation', 'bootstrapped', 'random']", The bootstrap principle,seg_243,where µ is the expectation corresponding to f . the bootstrapped version would be the random variable
1886,1,"['sample median', 'sample', 'bootstrap', 'median', 'distribution', 'statistic', 'expectation', 'sample statistic']", The bootstrap principle,seg_243,where µ∗ is the expectation corresponding to f̂ . often the bootstrap approximation of the distribution of a sample statistic will improve if we somehow normalize the sample statistic by relating it to a corresponding feature of the “true” distribution. an example is the centered sample median
1887,1,"['sample', 'median', 'sample variance', 'variance']", The bootstrap principle,seg_243,where we subtract the median f inv(0.5) of f . another example is the normalized sample variance
1888,1,['variance'], The bootstrap principle,seg_243,where we divide by the variance σ2 of f .
1889,1,"['distribution', 'bootstrap']", The bootstrap principle,seg_243,"quick exercise 18.1 describe how the bootstrap principle should be applied to approximate the distribution of med(x1, x2, . . . , xn) − f inv(0.5)."
1890,1,"['sample', 'model', 'random', 'bootstrap', 'estimate', 'dataset', 'data', 'distribution', 'parameter', 'random sample', 'realization']", The bootstrap principle,seg_243,"the question that remains is what to take as an estimate f̂ for f . this will depend on how well f can be specified. for the old faithful data we cannot say anything about the type of distribution. however, for the software data it seems reasonable to model the dataset as a realization of a random sample from an exp(λ) distribution and then we only have to estimate the parameter λ. different assumptions about f give rise to different bootstrap procedures. we will discuss two of them in the next sections."
1891,1,"['sample', 'estimate', 'dataset', 'distribution function', 'random sample', 'empirical distribution function', 'distribution', 'random', 'function', 'realization']", The empirical bootstrap,seg_245,"suppose we consider our dataset x1, x2, . . . , xn as a realization of a random sample from a distribution function f . when we cannot make any assumptions about the type of f , we can always estimate f by the empirical distribution function of the dataset:"
1892,1,"['sample', 'sample mean', 'estimate', 'bootstrap', 'distribution function', 'random sample', 'empirical distribution function', 'distribution', 'empirical bootstrap', 'mean', 'random', 'function']", The empirical bootstrap,seg_245,"since we estimate f by the empirical distribution function, the corresponding bootstrap principle is called the empirical bootstrap. applying this principle to the centered sample mean, the random sample x1, x2, . . . , xn from f is replaced by a bootstrap random sample x1∗, x2∗, . . . , xn"
1893,1,"['sample', 'sample mean', 'bootstrap', 'approximation', 'distribution', 'empirical bootstrap', 'expectation', 'mean']", The empirical bootstrap,seg_245,"the expectation corresponding to fn. the question is, of course, how good this approximation is. a mathematical theorem tells us that the empirical bootstrap works for the centered sample mean, i.e., the distribution of x̄n−µ is well approximated by that of x̄n"
1894,1,"['sample', 'normalized', 'bootstrap', 'statistics', 'empirical bootstrap', 'sample statistics']", The empirical bootstrap,seg_245,"∗−µ∗ (see remark 18.2). on the other hand, there are (normalized) sample statistics for which the empirical bootstrap fails,"
1895,1,"['sample', 'random', 'distribution', 'random sample']", The empirical bootstrap,seg_245,"based on a random sample x1, x2, . . . , xn from a u(0, θ) distribution (see exercise 18.12)."
1896,1,"['distribution function', 'discrete', 'probability', 'random', 'function', 'bootstrap', 'dataset', 'random variable', 'empirical distribution function', 'distribution', 'expectation', 'discrete random variable', 'random variables', 'variables', 'variable']", The empirical bootstrap,seg_245,"let us continue with approximating the distribution of x̄n − µ by that of x̄n ∗−µ∗. first note that the empirical distribution function fn of the original dataset is the distribution function of a discrete random variable that attains the values x1, x2, . . . , xn, each with probability 1/n. this means that each of the bootstrap random variables xi∗ has expectation"
1897,1,"['distribution', 'bootstrap', 'empirical bootstrap']", The empirical bootstrap,seg_245,"therefore, applying the empirical bootstrap to x̄n − µ means approximating its distribution by that of x̄n"
1898,1,"['distribution', 'probability distribution', 'probability']", The empirical bootstrap,seg_245,∗ − x̄n. in principle it would be possible to determine the probability distribution of x̄n
1899,1,"['random variable', 'variable', 'random']", The empirical bootstrap,seg_245,"∗− x̄n. indeed, the random variable x̄n"
1900,1,"['random variables', 'variables', 'distribution', 'probability', 'random']", The empirical bootstrap,seg_245,"is based on the random variables xi∗, whose distribution we know precisely: it takes values x1, x2, . . . , xn with equal probability 1/n. hence we could determine the possible values of x̄n"
1901,1,['probabilities'], The empirical bootstrap,seg_245,∗ − x̄n and the corresponding probabilities.
1902,1,['approximation'], The empirical bootstrap,seg_245,"for small n this can be done (see exercise 18.5), but for large n this becomes cumbersome. therefore we invoke a second approximation."
1903,1,"['sample', 'random', 'simulation', 'statistics', 'variation', 'distribution', 'sample statistics', 'random sample', 'vary', 'realization']", The empirical bootstrap,seg_245,"recall the jury example in section 6.3, where we investigated the variation of two different rules that a jury might use to assign grades. in terms of the present chapter, the jury example deals with a random sample from a u(−0.5, 0.5) distribution and two different sample statistics t and m , corresponding to the two rules. to investigate the distribution of t and m , a simulation was carried out with one thousand runs, where in every run we generated a realization of a random sample from the u(−0.5, 0.5) distribution and computed the corresponding realization of t and m . the one thousand realizations give a good impression of how t and m vary around the deserved score (see figure 6.4)."
1904,1,['distribution'], The empirical bootstrap,seg_245,returning to the distribution of x̄n
1905,1,"['sample', 'random', 'bootstrap', 'random sample', 'realization']", The empirical bootstrap,seg_245,"∗− x̄n, the analogue would be to repeatedly generate a realization of the bootstrap random sample from fn and every time compute the corresponding realization of x̄n"
1906,1,['distribution'], The empirical bootstrap,seg_245,∗ − x̄n. the resulting realizations would give a good impression about the distribution of x̄n
1907,1,['realization'], The empirical bootstrap,seg_245,∗− x̄n. a realization
1908,1,"['sample', 'random', 'bootstrap', 'dataset', 'random sample']", The empirical bootstrap,seg_245,of the bootstrap random sample is called a bootstrap dataset and is denoted by
1909,1,"['sample', 'simulation', 'sample mean', 'dataset', 'mean']", The empirical bootstrap,seg_245,"to distinguish it from the original dataset x1, x2, . . . , xn. for the centered sample mean the simulation procedure is as follows."
1910,1,"['bootstrap', 'estimate', 'dataset', 'simulation', 'distribution function', 'empirical distribution function', 'distribution', 'expectation', 'function']", The empirical bootstrap,seg_245,"empirical bootstrap simulation (for x̄n−µ). given a dataset x1, x2, . . . , xn, determine its empirical distribution function fn as an estimate of f , and compute the expectation"
1911,1,"['sample', 'bootstrap', 'sample mean', 'dataset', 'mean']", The empirical bootstrap,seg_245,"corresponding to fn. 1. generate a bootstrap dataset x∗1, x∗2, . . . , x∗n from fn. 2. compute the centered sample mean for the bootstrap dataset:"
1912,0,[], The empirical bootstrap,seg_245,repeat steps 1 and 2 many times.
1913,1,"['probability', 'dataset']", The empirical bootstrap,seg_245,"note that generating a value x∗i from fn is equivalent to choosing one of the elements x1, x2, . . . , xn of the original dataset with equal probability 1/n."
1914,1,"['sample', 'simulation', 'sample mean', 'bootstrap', 'empirical bootstrap', 'statistic', 'sample statistic', 'mean']", The empirical bootstrap,seg_245,"the empirical bootstrap simulation is described for the centered sample mean, but clearly a similar simulation procedure can be formulated for any (normalized) sample statistic."
1915,1,"['sample median', 'sample', 'simulation', 'bootstrap', 'median', 'empirical bootstrap']", The empirical bootstrap,seg_245,"quick exercise 18.2 describe the empirical bootstrap simulation for the centered sample median med(x1, x2, . . . , xn) − f inv(0.5)."
1916,1,"['sample', 'histogram', 'simulation', 'sample mean', 'estimate', 'bootstrap', 'kernel', 'sample means', 'kernel density estimate', 'data', 'empirical bootstrap', 'mean']", The empirical bootstrap,seg_245,for the old faithful data we carried out the empirical bootstrap simulation for the centered sample mean with one thousand repetitions. in figure 18.1 a histogram (left) and kernel density estimate (right) are displayed of one thousand centered bootstrap sample means
1917,1,"['random variable', 'variable', 'random']", The empirical bootstrap,seg_245,since these are realizations of the random variable x̄n
1918,1,['distribution'], The empirical bootstrap,seg_245,"∗ − x̄n, we know from section 17.2 that they reflect the distribution of x̄n"
1919,1,"['sample', 'bootstrap', 'sample means', 'distribution']", The empirical bootstrap,seg_245,"∗ − x̄n approximates that of x̄n − µ, the centered bootstrap sample means also reflect the distribution of x̄n−µ. this leads to the following application."
1920,1,"['empirical bootstrap', 'bootstrap']", The empirical bootstrap,seg_245,an application of the empirical bootstrap
1921,1,"['sample', 'random', 'estimate', 'data', 'expectation', 'measurements', 'random sample', 'average', 'realization']", The empirical bootstrap,seg_245,"let us return to our example about the old faithful data, which are modeled as a realization of a random sample from some f . suppose we estimate the expectation µ corresponding to f by x̄n = 209.3. can we say how far away 209.3 is from the “true” expectation µ? to be honest, the answer is no. . . (oops). in a situation like this, the measurements and their corresponding average are subject to randomness, so that we cannot say anything with absolute certainty about how far away the average will be from µ. one of the things we can say is how likely it is that the average is within a given distance from µ."
1922,1,"['sample', 'sample mean', 'dataset', 'mean', 'probability', 'average']", The empirical bootstrap,seg_245,"to get an impression of how close the average of a dataset of n = 272 observed durations of the old faithful geyser is to µ, we want to compute the probability that the sample mean deviates more than 5 from µ:"
1923,1,"['distribution', 'random variable', 'variable', 'probability', 'random']", The empirical bootstrap,seg_245,"direct computation of this probability is impossible, because we do not know the distribution of the random variable x̄n−µ. however, since the distribution of x̄n"
1924,1,"['distribution', 'probability']", The empirical bootstrap,seg_245,"∗ − x̄n approximates the distribution of x̄n − µ, we can approximate the probability as follows"
1925,1,"['probability', 'data']", The empirical bootstrap,seg_245,"where we have also used that for the old faithful data, x̄n = 209.3. as we mentioned before, in principle it is possible to compute the last probability exactly. since this is too cumbersome, we approximate p(|x̄n"
1926,1,"['sample', 'simulation', 'bootstrap', 'sample means', 'empirical bootstrap']", The empirical bootstrap,seg_245,by means of the one thousand centered bootstrap sample means obtained from the empirical bootstrap simulation:
1927,1,"['table', 'estimate']", The empirical bootstrap,seg_245,"in view of table 17.2, a natural estimate for p(|x̄n"
1928,1,"['sample', 'absolute value', 'bootstrap', 'sample means', 'relative frequency', 'frequency']", The empirical bootstrap,seg_245,∗ − 209.3| > 5) is the relative frequency of centered bootstrap sample means that are greater than 5 in absolute value:
1929,1,"['sample', 'bootstrap', 'sample means', 'approximation']", The empirical bootstrap,seg_245,"for the centered bootstrap sample means of figure 18.1, this relative frequency is 0.227. hence, we obtain the following bootstrap approximation"
1930,1,"['bootstrap procedure', 'approximation', 'bootstrap']", The empirical bootstrap,seg_245,it should be emphasized that the second approximation can be made arbitrarily accurate by increasing the number of repetitions in the bootstrap procedure.
1931,1,"['sample', 'parameters', 'estimate', 'dataset', 'distribution function', 'random sample', 'case', 'distribution', 'parameter', 'random', 'function', 'realization']", The parametric bootstrap,seg_247,"suppose we consider our dataset as a realization of a random sample from a distribution of a specific parametric type. in that case the distribution function is completely determined by a parameter or vector of parameters θ: f = fθ. then we do not have to estimate the whole distribution function f , but it suffices to estimate the parameter(vector) θ by θ̂ and estimate f by"
1932,1,"['bootstrap', 'parametric bootstrap']", The parametric bootstrap,seg_247,the corresponding bootstrap principle is called the parametric bootstrap.
1933,1,"['sample', 'sample mean', 'bootstrap', 'distribution function', 'random sample', 'distribution', 'expectation', 'mean', 'parametric bootstrap', 'random', 'function']", The parametric bootstrap,seg_247,"let us investigate what this would mean for the centered sample mean. first we should realize that the expectation of fθ is also determined by θ: µ = µθ. the parametric bootstrap for the centered sample mean now amounts to the following. the random sample x1, x2, . . . , xn from the “true” distribution function fθ is replaced by a bootstrap random sample x1∗, x2∗, . . . , xn"
1934,1,"['distribution', 'probability distribution', 'probability']", The parametric bootstrap,seg_247,"fθ̂, and the probability distribution of x̄n − µθ is approximated by that of x̄n"
1935,1,['expectation'], The parametric bootstrap,seg_247,denotes the expectation corresponding to fθ̂.
1936,1,"['empirical bootstrap', 'approximation', 'bootstrap', 'parametric bootstrap']", The parametric bootstrap,seg_247,"often the parametric bootstrap approximation is better than the empirical bootstrap approximation, as illustrated in the next quick exercise."
1937,1,"['sample', 'random', 'bootstrap', 'estimate', 'dataset', 'distribution', 'random sample', 'realization']", The parametric bootstrap,seg_247,"quick exercise 18.3 suppose the dataset x1, x2, . . . , xn is a realization of a random sample x1, x2, . . . , xn from an n(µ, 1) distribution. estimate µ by x̄n and consider a bootstrap random sample x1∗, x2∗, . . . , xn"
1938,1,"['probability distributions', 'distribution', 'probability', 'distributions']", The parametric bootstrap,seg_247,distribution. check that the probability distributions of x̄n − µ and x̄n
1939,1,['distribution'], The parametric bootstrap,seg_247,"are the same: an n(0, 1/n) distribution."
1940,1,['distribution'], The parametric bootstrap,seg_247,"once more, in principle it is possible to determine the distribution of x̄n"
1941,1,"['sample', 'contrast', 'simulation', 'sample mean', 'cases', 'mean']", The parametric bootstrap,seg_247,"exactly. however, in contrast with the situation considered in the previous quick exercise, in some cases this is still cumbersome. again a simulation procedure may help us out. for the centered sample mean the procedure is as follows."
1942,1,"['simulation', 'bootstrap']", The parametric bootstrap,seg_247,parametric bootstrap simulation (for x̄n − µ). given a
1943,1,"['sample', 'sample mean', 'estimate', 'bootstrap', 'dataset', 'expectation', 'mean']", The parametric bootstrap,seg_247,"dataset x1, x2, . . . , xn, compute an estimate θ̂ for θ. determine fθ̂ as an estimate for fθ, and compute the expectation µ∗ = µθ̂ corresponding to fθ̂. 1. generate a bootstrap dataset x∗1, x∗2, . . . , x∗n from fθ̂. 2. compute the centered sample mean for the bootstrap dataset:"
1944,0,[], The parametric bootstrap,seg_247,repeat steps 1 and 2 many times.
1945,1,"['model', 'exponential distribution', 'bootstrap', 'simulation', 'data', 'distribution', 'exponential', 'parametric bootstrap']", The parametric bootstrap,seg_247,as an application we will use the parametric bootstrap simulation to investigate whether the exponential distribution is a reasonable model for the software data.
1946,1,"['exponential', 'data']", The parametric bootstrap,seg_247,are the software data exponential?
1947,1,"['model', 'estimated', 'exponential distribution', 'dataset', 'distribution function', 'empirical distribution function', 'data', 'distribution', 'exponential', 'function']", The parametric bootstrap,seg_247,"consider fitting an exponential distribution to the software data, as discussed in section 17.3. at first sight, figure 17.6 shows a reasonable fit with the exponential distribution. one way to quantify the difference between the dataset and the exponential model is to compute the maximum distance between the empirical distribution function fn of the dataset and the exponential distribution function fλ̂ estimated from the dataset:"
1948,1,"['estimated', 'dataset']", The parametric bootstrap,seg_247,where λ̂ = 1/x̄n is estimated from the dataset. the quantity tks is called the kolmogorov-smirnov distance between fn and fλ̂.
1949,1,"['functions', 'model', 'estimated', 'dataset', 'distribution function', 'empirical distribution function', 'distribution', 'exponential', 'function']", The parametric bootstrap,seg_247,"the idea behind the use of this distance is the following. if f denotes the “true” distribution function, then according to section 17.2 the empirical distribution function fn will resemble f whether f equals the distribution function fλ of some exp(λ) distribution or not. on the other hand, if the “true” distribution function is fλ, then the estimated exponential distribution function fλ̂ will resemble fλ, because λ̂ = 1/x̄n is close to the “true” λ. therefore, if f = fλ, then both fn and fλ̂ will be close to the same distribution function, so that tks is small; if f is different from fλ, then fn and fλ̂ are close to two different distribution functions, so that tks is large. the value tks is always between 0 and 1, and the further away this value is from 0, the more it is an indication that the exponential model is inappropriate. for the software dataset we find λ̂ = 1/x̄n = 0.0015 and tks = 0.176. does this speak against the believed exponential model?"
1950,1,"['distribution function', 'case', 'statistic', 'sample statistic', 'random sample', 'random', 'function', 'sample', 'estimate', 'dataset', 'data', 'random variable', 'exponential', 'empirical distribution function', 'distribution', 'variable', 'realization']", The parametric bootstrap,seg_247,"one way to investigate this is to find out whether, in the case when the data are truly a realization of an exponential random sample from fλ, the value 0.176 is unusually large. to answer this question we consider the sample statistic that corresponds to tks. the estimate λ̂ = 1/x̄n is replaced by the random variable λ̂ = 1/x̄n, and the empirical distribution function of the dataset is replaced by the empirical distribution function of the random sample x1, x2, . . . , xn (again denoted by fn):"
1951,1,"['sample', 'statistic', 'sample statistic', 'realization']", The parametric bootstrap,seg_247,"in this way, tks is a realization of the sample statistic"
1952,1,"['sample', 'random', 'bootstrap', 'estimate', 'dataset', 'distribution', 'probability distribution', 'probability', 'random sample', 'parametric bootstrap', 'parameter']", The parametric bootstrap,seg_247,"to find out whether 0.176 is an exceptionally large value for the random variable tks, we must determine the probability distribution of tks. however, this is impossible because the parameter λ of the exp(λ) distribution is unknown. we will approximate the distribution of tks by a parametric bootstrap. we use the dataset to estimate λ by λ̂ = 1/x̄n = 0.0015 and replace the random sample x1, x2, . . . , xn from fλ by a bootstrap random sample x1∗, x2∗, . . . , xn"
1953,1,['distribution'], The parametric bootstrap,seg_247,from fλ̂. next we approximate the distribution of tks by that of its bootstrapped version
1954,1,"['bootstrap', 'distribution function', 'empirical distribution function', 'distribution', 'random', 'function']", The parametric bootstrap,seg_247,where fn∗ is the empirical distribution function of the bootstrap random sam-
1955,0,[], The parametric bootstrap,seg_247,ple: number of x∗ less than or equal to a
1956,1,"['sample', 'random', 'bootstrap', 'random sample', 'average']", The parametric bootstrap,seg_247,"∗, with x̄n ∗ being the average of the bootstrap random sample."
1957,1,"['sample', 'bootstrap', 'distribution', 'probability distribution', 'statistic', 'sample statistic', 'probability', 'parametric bootstrap', 'bootstrapped']", The parametric bootstrap,seg_247,"the bootstrapped sample statistic tk∗s is too complicated to determine its probability distribution, and hence we perform a parametric bootstrap simulation:"
1958,1,"['exponential', 'bootstrap', 'dataset']", The parametric bootstrap,seg_247,"1. we generate a bootstrap dataset x∗1, x∗2, . . . , x∗135 from an exponential dis-"
1959,1,['parameter'], The parametric bootstrap,seg_247,tribution with parameter λ̂ = 0.0015.
1960,1,['bootstrapped'], The parametric bootstrap,seg_247,2. we compute the bootstrapped ks distance
1961,1,"['estimated', 'exponential distribution', 'bootstrap', 'dataset', 'distribution function', 'empirical distribution function', 'distribution', 'exponential', 'function']", The parametric bootstrap,seg_247,"where fn∗ denotes the empirical distribution function of the bootstrap dataset and fλ̂∗ denotes the estimated exponential distribution function,"
1962,1,"['bootstrap', 'dataset']", The parametric bootstrap,seg_247,where λ̂∗ = 1/x̄n∗ is computed from the bootstrap dataset.
1963,1,"['histogram', 'bootstrapped', 'process', 'rate', 'exponential distribution', 'estimate', 'kernel', 'results', 'data', 'exponential', 'model', 'poisson process', 'kernel density estimate', 'distribution', 'poisson', 'failures']", The parametric bootstrap,seg_247,"we repeat steps 1 and 2 one thousand times, which results in one thousand values of the bootstrapped ks distance. in figure 18.2 we have displayed a histogram and kernel density estimate of the one thousand bootstrapped ks distances. it is clear that if the software data would come from an exponential distribution, the value 0.176 of the ks distance would be very unlikely! this strongly suggests that the exponential distribution is not the right model for the software data. the reason for this is that the poisson process is the wrong model for the series of failures. a closer inspection shows that the rate at which failures occur over time is not constant, as was assumed in chapter 17, but decreases."
1964,1,"['sample', 'random', 'estimate', 'dataset', 'random sample']", Solutions to the quick exercises,seg_249,"18.1 you could have written something like the following: “use the dataset x1, x2, . . . , xn to compute an estimate f̂ for f . replace the random sample x1, x2, . . . , xn from f by a random sample x1∗, x2∗, . . . , xn"
1965,1,"['distribution', 'probability distribution', 'probability']", Solutions to the quick exercises,seg_249,approximate the probability distribution of
1966,1,['median'], Solutions to the quick exercises,seg_249,"∗) − f̂ inv(0.5), where f̂ inv(0.5) is the median"
1967,1,"['median', 'estimate', 'dataset', 'distribution function', 'empirical distribution function', 'distribution', 'function']", Solutions to the quick exercises,seg_249,"18.2 you could have written something like the following: “given a dataset x1, x2, . . . , xn, determine its empirical distribution function fn as an estimate of f , and the median f inv(0.5) of fn."
1968,1,"['bootstrap', 'dataset']", Solutions to the quick exercises,seg_249,"1. generate a bootstrap dataset x∗1, x∗2, . . . , x∗n from fn."
1969,1,"['sample median', 'sample', 'bootstrap', 'median', 'dataset']", Solutions to the quick exercises,seg_249,2. compute the sample median for the bootstrap dataset:
1970,1,"['sample median', 'sample', 'median']", Solutions to the quick exercises,seg_249,"where med∗n = sample median of x∗1, x∗2, . . . , x∗n."
1971,0,[], Solutions to the quick exercises,seg_249,repeat steps 1 and 2 many times.”
1972,1,"['sample median', 'sample', 'median', 'dataset']", Solutions to the quick exercises,seg_249,"note that if n is odd, then f inv(0.5) equals the sample median of the original dataset, but this is not necessarily so for n even."
1973,1,"['random variables', 'independent', 'variables', 'distribution', 'normal', 'random', 'average', 'normal distribution']", Solutions to the quick exercises,seg_249,"18.3 according to remark 11.2 about the sum of independent normal random variables, the sum of n independent n(µ, 1) distributed random variables has an n(nµ, n) distribution. hence by the change-of-units rule for the normal distribution (see page 106), it follows that x̄n has an n(µ, 1/n) distribution, and that x̄n − µ has an n(0, 1/n) distribution. similarly, the average x̄n"
1974,1,"['random variables', 'independent', 'bootstrap', 'variables', 'distribution', 'random']", Solutions to the quick exercises,seg_249,"n independent n(x̄n, 1) distributed bootstrap random variables has a normal distribution n(x̄n, 1/n) distribution, and therefore x̄n"
1975,1,['distribution'], Solutions to the quick exercises,seg_249,"n(0, 1/n) distribution."
1976,1,"['bootstrap', 'dataset', 'distribution function', 'empirical distribution function', 'distribution', 'function']", Exercises,seg_251,"18.1 we generate a bootstrap dataset x∗1, x∗2, . . . , x∗6 from the empirical distribution function of the dataset"
1977,1,"['bootstrap', 'with replacement', 'replacement', 'probability']", Exercises,seg_251,"i.e., we draw (with replacement) six values from these numbers with equal probability 1/6. how many different bootstrap datasets are possible? are they all equally likely to occur?"
1978,1,"['bootstrap', 'dataset', 'function']", Exercises,seg_251,"18.2 we generate a bootstrap dataset x∗1, x∗2, x∗3, x∗4 from the empirical distribution function of the dataset"
1979,1,"['sample', 'bootstrap', 'sample mean', 'mean', 'probability']", Exercises,seg_251,a. compute the probability that the bootstrap sample mean is equal to 1.
1980,1,"['probability', 'bootstrap', 'dataset']", Exercises,seg_251,b. compute the probability that the maximum of the bootstrap dataset is
1981,1,"['bootstrap', 'probability']", Exercises,seg_251,c. compute the probability that exactly two elements in the bootstrap sam-
1982,1,"['bootstrap', 'dataset', 'distribution function', 'empirical distribution function', 'distribution', 'function']", Exercises,seg_251,"18.3 we generate a bootstrap dataset x∗1, x∗2, . . . , x∗10 from the empirical distribution function of the dataset"
1983,1,"['probability', 'bootstrap', 'dataset']", Exercises,seg_251,a. compute the probability that the bootstrap dataset has exactly three
1984,1,"['probability', 'bootstrap', 'dataset']", Exercises,seg_251,b. compute the probability that the bootstrap dataset has at most two ele-
1985,1,"['probability', 'bootstrap', 'dataset']", Exercises,seg_251,c. compute the probability that the bootstrap dataset has exactly two ele-
1986,0,[], Exercises,seg_251,ments less than or equal to 0.38 and all other elements greater than 0.42.
1987,1,['dataset'], Exercises,seg_251,"18.4 consider the dataset from exercise 18.3, with maximum 0.46."
1988,1,"['sample', 'random', 'bootstrap', 'random sample']", Exercises,seg_251,"a. we generate a bootstrap random sample x1∗, x2∗, . . . , x1∗0 from the empir-"
1989,1,"['dataset', 'distribution function', 'distribution', 'function']", Exercises,seg_251,"ical distribution function of the dataset. compute p(m1∗0 < 0.46), where m1∗0 = max{x1∗, x2∗, . . . , x1∗0}."
1990,1,['dataset'], Exercises,seg_251,"b. the same question as in a, but now for a dataset with distinct elements"
1991,1,"['sample', 'random', 'bootstrap', 'random sample']", Exercises,seg_251,"x1, x2, . . . , xn and maximum mn. compute p(mn∗ < mn), where mn∗ is the maximum of a bootstrap random sample x1∗, x2∗, . . . , xn"
1992,1,"['dataset', 'distribution function', 'empirical distribution function', 'distribution', 'function']", Exercises,seg_251,from the empirical distribution function of the dataset.
1993,1,['dataset'], Exercises,seg_251,18.5 suppose we have a dataset
1994,1,"['distribution function', 'bootstrapped', 'random sample', 'random', 'function', 'sample', 'sample mean', 'estimate', 'bootstrap', 'random variable', 'mean', 'empirical distribution function', 'distribution', 'probabilities', 'variable', 'realization']", Exercises,seg_251,"which is the realization of a random sample from a distribution function f . if we estimate f by the empirical distribution function, then according to the bootstrap principle applied to the centered sample mean x̄3 − µ, we must replace this random variable by its bootstrapped version x̄3∗ − x̄3. determine the possible values for the bootstrap random variable x̄3∗ − x̄3 and the corresponding probabilities."
1995,1,"['sample', 'dataset', 'distribution function', 'random sample', 'distribution', 'random', 'function', 'realization']", Exercises,seg_251,"18.6 suppose that the dataset x1, x2, . . . , xn is a realization of a random sample from an exp(λ) distribution with distribution function fλ, and that x̄n = 5."
1996,1,"['distribution', 'median']", Exercises,seg_251,a. check that the median of the exp(λ) distribution is mλ = (ln 2)/λ (see
1997,1,"['bootstrap', 'parametric bootstrap', 'estimate']", Exercises,seg_251,b. suppose we estimate λ by 1/x̄n. describe the parametric bootstrap sim-
1998,1,"['sample', 'sample mean', 'bootstrap', 'dataset', 'random sample', 'distribution', 'empirical bootstrap', 'simulations', 'expectation', 'mean', 'bootstrapped', 'random']", Exercises,seg_251,"18.7 to give an example in which the bootstrapped centered sample mean in the parametric and empirical bootstrap simulations may be different, consider the following situation. suppose that the dataset x1, x2, . . . , xn is a realization of a random sample from a u(0, θ) distribution with expectation"
1999,1,['estimate'], Exercises,seg_251,µ = θ/2. we estimate θ by n + 1
2000,1,"['sample', 'bootstrap', 'sample mean', 'mean', 'parametric bootstrap']", Exercises,seg_251,"where mn = max{x1, x2, . . . , xn}. describe the parametric bootstrap simulation for the centered sample mean x̄n − µ."
2001,1,"['distribution function', 'bootstrapped', 'random sample', 'random', 'function', 'sample', 'sample mean', 'bootstrap', 'simulation', 'data', 'mean', 'median', 'distribution', 'empirical bootstrap', 'simulations', 'expectation', 'average', 'realization']", Exercises,seg_251,"18.8 here is an example in which the bootstrapped centered sample mean in the parametric and empirical bootstrap simulations are the same. consider the software data with average x̄n = 656.8815 and median mn = 290, modeled as a realization of a random sample x1, x2, . . . , xn from a distribution function f with expectation µ. by means of bootstrap simulation we like to get an impression of the distribution of x̄n − µ."
2002,1,['distribution'], Exercises,seg_251,a. suppose that we assume nothing about the distribution of the interfailure
2003,1,"['simulation', 'bootstrap']", Exercises,seg_251,times. describe the appropriate bootstrap simulation procedure with one thousand repetitions.
2004,1,"['distribution', 'function', 'distribution function']", Exercises,seg_251,b. suppose we assume that f is the distribution function of an exp(λ) distri-
2005,1,"['estimated', 'simulation', 'bootstrap']", Exercises,seg_251,"bution, where λ is estimated by 1/x̄n = 0.0015. describe the appropriate bootstrap simulation procedure with one thousand repetitions."
2006,1,"['distribution', 'function', 'distribution function']", Exercises,seg_251,c. suppose we assume that f is the distribution function of an exp(λ) dis-
2007,1,"['estimated', 'simulation', 'bootstrap', 'parameter']", Exercises,seg_251,"tribution, and that (as suggested by exercise 18.6a) the parameter λ is estimated by (ln 2)/mn = 0.0024. describe the appropriate bootstrap simulation procedure with one thousand repetitions."
2008,1,"['histogram', 'probability', 'random', 'random sample', 'sample', 'sample mean', 'estimate', 'bootstrap', 'dataset', 'data', 'mean', 'standard', 'standard deviation', 'distribution', 'deviation', 'sample standard deviation', 'average', 'realization']", Exercises,seg_251,"18.9 consider the dataset from exercises 15.1 and 17.6 consisting of measured chest circumferences of scottish soldiers with average x̄n = 39.85 and sample standard deviation sn = 2.09. the histogram in figure 17.11 suggests modeling the data as the realization of a random sample x1, x2, . . . , xn from an n(µ, σ2) distribution. we estimate µ by the sample mean and we are interested in the probability that the sample mean deviates more than 1 from µ: p(|x̄n − µ| > 1). describe how one can use the bootstrap principle to approximate this probability, i.e., describe the distribution of the bootstrap random sample x1∗, x2∗, . . . , xn"
2009,1,"['simulation', 'probability']", Exercises,seg_251,∗ and compute p(|x̄n ∗ − µ∗| > 1). note that one does not need a simulation to approximate this latter probability.
2010,1,"['sample', 'sample mean', 'estimate', 'random sample', 'data', 'distribution', 'expectation', 'mean', 'probability', 'random', 'average', 'realization']", Exercises,seg_251,"18.10 consider the software data, with average x̄n = 656.8815, modeled as a realization of a random sample x1, x2, . . . , xn from a distribution function f . we estimate the expectation µ of f by the sample mean and we are interested in the probability that the sample mean deviates more than ten from µ: p(|x̄n − µ| > 10)."
2011,1,['distribution'], Exercises,seg_251,a. suppose we assume nothing about the distribution of the interfailure
2012,1,"['simulation', 'bootstrap', 'approximation', 'results', 'probability']", Exercises,seg_251,"times. describe how one can obtain a bootstrap approximation for the probability, i.e., describe the appropriate bootstrap simulation procedure with one thousand repetitions and how the results of this simulation can be used to approximate the probability."
2013,1,"['distribution', 'function', 'distribution function']", Exercises,seg_251,b. suppose we assume that f is the distribution function of an exp(λ) dis-
2014,1,"['approximation', 'bootstrap', 'probability']", Exercises,seg_251,tribution. describe how one can obtain a bootstrap approximation for the probability.
2015,1,"['estimated', 'parameters', 'dataset', 'distribution function', 'empirical distribution function', 'distribution', 'normal', 'function', 'normal distribution']", Exercises,seg_251,"18.11 consider the dataset of measured chest circumferences of 5732 scottish soldiers (see exercises 15.1, 17.6, and 18.9). the kolmogorov-smirnov distance between the empirical distribution function and the distribution function fx̄n,sn of the normal distribution with estimated parameters µ̂ = x̄n = 39.85 and σ̂ = sn = 2.09 is equal to"
2016,1,"['deviation', 'sample', 'simulation', 'sample mean', 'bootstrap', 'dataset', 'sample standard deviation', 'normality', 'mean', 'standard', 'standard deviation']", Exercises,seg_251,where x̄n and sn denote sample mean and sample standard deviation of the dataset. suppose we want to perform a bootstrap simulation with one thousand repetitions for the ks distance to investigate to which degree the value 0.0987 agrees with the assumed normality of the dataset. describe the appropriate bootstrap simulation that must be carried out.
2017,1,"['sample', 'random', 'bootstrap', 'dataset', 'distribution', 'empirical bootstrap', 'random sample', 'realization']", Exercises,seg_251,"18.12 to give an example where the empirical bootstrap fails, consider the following situation. suppose our dataset x1, x2, . . . , xn is a realization of a random sample x1, x2, . . . , xn from a u(0, θ) distribution. consider the nor-"
2018,1,"['sample', 'statistic', 'sample statistic']", Exercises,seg_251,malized sample statistic mn
2019,1,"['sample', 'functions', 'bootstrap', 'dataset', 'distribution function', 'random sample', 'empirical distribution function', 'distribution', 'random', 'function']", Exercises,seg_251,"strap random sample from the empirical distribution function of our dataset, and let mn∗ be the corresponding bootstrap maximum. we are going to compare the distribution functions of tn and its bootstrap counterpart"
2020,1,"['distribution', 'function', 'distribution function']", Exercises,seg_251,"b. let gn(t) = p(tn ≤ t) be the distribution function of tn, and similarly let"
2021,1,"['bootstrap', 'distribution function', 'distribution', 'statistic', 'function']", Exercises,seg_251,g∗n(t) = p(tn∗ ≤ t) be the distribution function of the bootstrap statistic tn∗. conclude from part a that the maximum distance between g∗n and gn can be bounded from below as follows:
2022,0,['n'], Exercises,seg_251,"c. use part b to argue that for all n, the maximum distance between g∗n"
2023,1,"['sample', 'functions', 'bootstrap', 'distribution']", Exercises,seg_251,hint: you may use that e−x ≥ 1 − x for all x. we conclude that even for very large sample sizes the maximum distance between the distribution functions of tn and its bootstrap counterpart tn∗ is at least 0.632.
2024,1,"['contrast', 'bootstrap', 'estimate', 'distribution', 'empirical bootstrap', 'parameter', 'parametric bootstrap']", Exercises,seg_251,"18.13 (exercise 18.12 continued). in contrast to the empirical bootstrap, the parametric bootstrap for tn does work. suppose we estimate the parameter θ of the u(0, θ) distribution by"
2025,1,"['sample', 'bootstrap', 'distribution function', 'random sample', 'distribution', 'random', 'function']", Exercises,seg_251,"∗ be a bootstrap random sample from a u(0, θ̂) distribution, and let mn∗ be the corresponding bootstrap maximum. again, we are going to compare the distribution function gn of tn = 1−mn/θ with the"
2026,1,"['function', 'bootstrap']", Exercises,seg_251,distribution function g∗n of its bootstrap counterpart tn∗ = 1 − mn∗/θ̂.
2027,1,"['distribution', 'function', 'distribution function']", Exercises,seg_251,"a. check that the distribution function fθ of a u(0, θ) distribution is given"
2028,1,"['distribution', 'function', 'distribution function']", Exercises,seg_251,b. check that the distribution function of tn is
2029,1,"['distribution', 'function', 'distribution function']", Exercises,seg_251,hint: rewrite p(tn ≤ t) as 1 − p(mn ≤ θ(1 − t)) and use the rule on page 109 about the distribution function of the maximum.
2030,1,"['distribution', 'function', 'distribution function']", Exercises,seg_251,c. show that tn∗ has the same distribution function:
2031,1,"['contrast', 'bootstrap', 'empirical bootstrap', 'parametric bootstrap']", Exercises,seg_251,"this means that, in contrast to the empirical bootstrap (see exercise 18.12), the parametric bootstrap works perfectly in this situation."
2032,1,"['estimators', 'probability', 'random sample', 'random', 'function', 'estimator', 'sample', 'estimate', 'dataset', 'mean', 'model', 'parameters', 'unbiased estimators', 'distribution', 'expectation', 'variance', 'unbiased', 'probability distribution', 'realization']", Unbiased estimators,seg_253,in chapter 17 we saw that a dataset can be modeled as a realization of a random sample from a probability distribution and that quantities of interest correspond to features of the model distribution. one of our tasks is to use the dataset to estimate a quantity of interest. we shall mainly deal with the situation where it is modeled as one of the parameters of the model distribution or as a certain function of the parameters. we will first discuss what we mean exactly by an estimator and then introduce the notion of unbiasedness as a desirable property for estimators. we end the chapter by providing unbiased estimators for the expectation and variance of a model distribution.
2033,1,"['poisson', 'intensity', 'poisson process', 'percentage', 'distribution', 'random variable', 'variable', 'mean', 'parameter', 'random', 'poisson distribution', 'process']", Estimators,seg_255,"consider the arrivals of packages at a network server. one is interested in the intensity at which packages arrive on a generic day and in the percentage of minutes during which no packages arrive. if the arrivals occur completely at random in time, the arrival process can be modeled by a poisson process. this would mean that the number of arrivals during one minute is modeled by a random variable having a poisson distribution with (unknown) parameter µ. the intensity of the arrivals is then modeled by the parameter µ itself, and the percentage of minutes during which no packages arrive is modeled by the"
2034,1,"['dataset', 'estimate', 'probability', 'process']", Estimators,seg_255,"−µ probability of zero arrivals: e . suppose one observes the arrival process for a while and gathers a dataset x1, x2, . . . , xn, where xi represents the number of arrivals in the ith minute. our task will be to estimate, based on the dataset,"
2035,1,"['function', 'parameter']", Estimators,seg_255,−µ the parameter µ and a function of the parameter: e .
2036,1,"['sample', 'model', 'parameters', 'dataset', 'random sample', 'distribution', 'probability distribution', 'probability', 'random', 'realization']", Estimators,seg_255,"this example is typical for the general situation in which our dataset is modeled as a realization of a random sample x1, x2, . . . , xn from a probability distribution that is completely determined by one or more parameters. the parameters that determine the model distribution are called the model parameters. we focus on the situation where the quantity of interest corresponds"
2037,1,"['model', 'intensity', 'percentage', 'parameters', 'distribution', 'parameter of interest', 'parameter', 'function']", Estimators,seg_255,"to a feature of the model distribution that can be described by the model parameters themselves or by some function of the model parameters. this distribution feature is referred to as the parameter of interest. in discussing this general setup we shall denote the parameter of interest by the greek letter θ. so, for instance, in our network server example, µ is the model parameter. when we are interested in the arrival intensity, the role of θ is played by the parameter µ itself, and when we are interested in the percentage of"
2038,1,"['method', 'estimate', 'dataset', 'parameter of interest', 'parameter']", Estimators,seg_255,"whatever method we use to estimate the parameter of interest θ, the result depends only on our dataset."
2039,1,"['dataset', 'estimate', 'function']", Estimators,seg_255,"estimate. an estimate is a value t that only depends on the dataset x1, x2, . . . , xn, i.e., t is some function of the dataset only:"
2040,1,"['curve', 'estimate', 'estimates', 'dataset', 'table', 'parameter']", Estimators,seg_255,"this description of estimate is a bit formal. the idea is, of course, that the value t, computed from our dataset x1, x2, . . . , xn, gives some indication of the “true” value of the parameter θ. we have already met several estimates in chapter 17; see, for instance, table 17.2. this table illustrates that the value of an estimate can be anything: a single number, a vector of numbers, even a complete curve."
2041,1,"['random sample', 'random', 'sample', 'sample mean', 'estimate', 'dataset', 'mean', 'parameter', 'sample variance', 'model', 'distribution', 'expectation', 'variance', 'intensity', 'law of large numbers', 'realization']", Estimators,seg_255,"let us return to our network server example in which our dataset x1, x2, . . . , xn is modeled as a realization of a random sample from a pois(µ) distribution. the intensity at which packages arrive is then represented by the parameter µ. since the parameter µ is the expectation of the model distribution, the law of large numbers suggests the sample mean x̄n as a natural estimate for µ. on the other hand, the parameter µ also represents the variance of the model distribution, so that by a similar reasoning another natural estimate is the sample variance s2n."
2042,1,"['percentage', 'estimate', 'dataset', 'relative frequency', 'frequency', 'probability']", Estimators,seg_255,"the percentage of idle minutes is modeled by the probability of zero arrivals. similar to the reasoning in section 13.4, a natural estimate is the relative frequency of zeros in the dataset:"
2043,0,[], Estimators,seg_255,number of xi equal to zero .
2044,1,['probability'], Estimators,seg_255,"on the other hand, the probability of zero arrivals can be expressed as a"
2045,1,"['model', 'estimate', 'parameter', 'function']", Estimators,seg_255,"−µ function of the model parameter: e . hence, if we estimate µ by x̄n, we could also estimate e−µ by e−x̄n ."
2046,1,"['estimate', 'relative frequency', 'frequency', 'probability']", Estimators,seg_255,quick exercise 19.1 suppose we estimate the probability of zero arrivals −µ e by the relative frequency of xi equal to zero. deduce an estimate for µ from this.
2047,1,"['estimates', 'parameter of interest', 'parameter']", Estimators,seg_255,the preceding examples illustrate that one can often think of several estimates for the parameter of interest. this raises questions like
2048,1,['estimate'], Estimators,seg_255,ĺ when is one estimate better than another?
2049,1,['estimate'], Estimators,seg_255,ĺ does there exist a best possible estimate?
2050,1,"['sample', 'random variables', 'estimate', 'variables', 'estimates', 'dataset', 'random sample', 'variable', 'random variable', 'parameter', 'random', 'realization']", Estimators,seg_255,"for instance, can we say which of the values x̄n or s2n computed from the dataset is closer to the “true” parameter µ? the answer is no. the measurements and the corresponding estimates are subject to randomness, so that we cannot say anything with certainty about which of the two is closer to µ. one of the things we can say for each of them is how likely it is that they are within a given distance from µ. to this end, we consider the random variables that correspond to the estimates. because our dataset x1, x2, . . . , xn is modeled as a realization of a random sample x1, x2, . . . , xn, the estimate t is a realization of a random variable t ."
2051,1,"['estimate', 'dataset', 'random variable', 'variable', 'random', 'realization']", Estimators,seg_255,"estimator. let t = h(x1, x2, . . . , xn) be an estimate based on the dataset x1, x2, . . . , xn. then t is a realization of the random variable"
2052,1,"['random variable', 'variable', 'random', 'estimator']", Estimators,seg_255,the random variable t is called an estimator.
2053,1,"['sample', 'method', 'estimate', 'dataset', 'estimation', 'statistics', 'estimators', 'cases', 'sample statistics', 'estimator']", Estimators,seg_255,"the word estimator refers to the method or device for estimation. this is distinguished from estimate, which refers to the actual value computed from a dataset. note that estimators are special cases of sample statistics. in the remainder of this chapter we will discuss the notion of unbiasedness that describes to some extent the behavior of estimators."
2054,1,"['sample', 'random', 'dataset', 'estimators', 'distribution', 'probability', 'random sample', 'realization']", Investigating the behavior of an estimator,seg_257,"let us continue with our network server example. suppose we have observed the network for 30 minutes and we have recorded the number of arrivals in each minute. the dataset is modeled as a realization of a random sample x1, x2, . . . , xn of size n = 30 from a pois(µ) distribution. let us concentrate on estimating the probability p0 of zero arrivals, which is an unknown number between 0 and 1. as motivated in the previous section, we have the following possible estimators:"
2055,1,['estimator'], Investigating the behavior of an estimator,seg_257,"1 2 our first estimator s can only attain the values 0, 30 , 30 , . . . , 1, so that in general it cannot give the exact value of p0. similarly for our second estima-"
2056,1,"['observations', 'estimator', 'estimators']", Investigating the behavior of an estimator,seg_257,"cannot expect our estimators always to give the exact value of p0 on basis of 30 observations. well, then what can we expect from a reasonable estimator?"
2057,1,"['estimation', 'observations', 'case', 'estimators', 'process']", Investigating the behavior of an estimator,seg_257,"to get an idea of the behavior of both estimators, we pretend we know µ and we simulate the estimation process in the case of n = 30 observations."
2058,1,"['poisson', 'estimate', 'estimators', 'distribution', 'frequency', 'parameter', 'vary', 'poisson distribution', 'estimator']", Investigating the behavior of an estimator,seg_257,"−µ let us choose µ = ln 10, so that p0 = e = 0.1. we draw 30 values from a poisson distribution with parameter µ = ln 10 and compute the value of estimators s and t . we repeat this 500 times, so that we have 500 values for each estimator. in figure 19.1 a frequency histogram1 of these values for estimator s is displayed on the left and for estimator t on the right. clearly, the values of both estimators vary around the value 0.1, which they are supposed to estimate."
2059,1,"['average', 'estimator']", The sampling distribution and unbiasedness,seg_259,"we have just seen that the values generated for estimator s fluctuate around p0 = 0.1. although the value of this estimator is not always equal to 0.1, it is desirable that on average, s is on target, i.e., e[s] = 0.1. moreover, it is desirable that this property holds no matter what the actual value of p0 is, i.e.,"
2060,1,"['distribution', 'probability distribution', 'estimator', 'probability']", The sampling distribution and unbiasedness,seg_259,"irrespective of the value 0 < p0 < 1. in order to find out whether this is true, we need the probability distribution of the estimator s. of course this"
2061,1,"['sample', 'random sample', 'estimators', 'sampling', 'variable', 'random variable', 'distribution', 'random', 'sampling distribution']", The sampling distribution and unbiasedness,seg_259,"is simply the distribution of a random variable, but because estimators are constructed from a random sample x1, x2, . . . , xn, we speak of the sampling distribution."
2062,1,"['sample', 'random sample', 'sampling', 'distribution', 'random', 'estimator', 'sampling distribution']", The sampling distribution and unbiasedness,seg_259,"the sampling distribution. let t = h(x1, x2, . . . , xn) be an estimator based on a random sample x1, x2, . . . , xn. the probability distribution of t is called the sampling distribution of t ."
2063,1,"['sampling', 'sampling distribution', 'distribution']", The sampling distribution and unbiasedness,seg_259,the sampling distribution of s can be found as follows. write
2064,1,"['discrete', 'probability', 'random', 'successes', 'trials', 'random variable', 'success', 'distribution', 'sampling distribution', 'discrete random variable', 'independent', 'probabilities', 'sampling', 'variable', 'probability of success']", The sampling distribution and unbiasedness,seg_259,"where y is the number of xi equal to zero. if for each i we label xi = 0 as a success, then y is equal to the number of successes in n independent trials with p0 as the probability of success. similar to section 4.3, it follows that y has a bin(n, p0) distribution. hence the sampling distribution of s is that of a bin(n, p0) distributed random variable divided by n. this means that s is a discrete random variable that attains the values k/n, where k = 0, 1, . . . , n, with probabilities given by"
2065,1,"['mass function', 'probability mass function', 'case', 'distribution', 'probability', 'function']", The sampling distribution and unbiasedness,seg_259,"the probability mass function of s for the case n = 30 and p0 = 0.1 is displayed in figure 19.2. since s = y/n and y has a bin(n, p0) distribution,"
2066,1,['estimator'], The sampling distribution and unbiasedness,seg_259,"so, indeed, the estimator s for p0 has the property e[s] = p0. this property reflects the fact that estimator s has no systematic tendency to produce"
2067,1,"['estimates', 'estimators', 'unbiased']", The sampling distribution and unbiasedness,seg_259,"estimates that are larger than p0, and no systematic tendency to produce estimates that are smaller than p0. this is a desirable property for estimators, and estimators that have this property are called unbiased."
2068,1,"['unbiased estimator', 'estimator', 'parameter', 'unbiased']", The sampling distribution and unbiasedness,seg_259,"definition. an estimator t is called an unbiased estimator for the parameter θ, if e [t ] = θ"
2069,1,"['bias', 'biased']", The sampling distribution and unbiasedness,seg_259,"irrespective of the value of θ. the difference e[t ] − θ is called the bias of t ; if this difference is nonzero, then t is called biased."
2070,1,"['sampling', 'distribution', 'probability', 'estimator', 'sampling distribution']", The sampling distribution and unbiasedness,seg_259,let us return to our second estimator for the probability of zero arrivals in the network server example: t = e−x̄n . the sampling distribution can be obtained as follows. write
2071,1,"['discrete random variable', 'random variables', 'independent', 'variables', 'discrete', 'distribution', 'random variable', 'variable', 'random']", The sampling distribution and unbiasedness,seg_259,"where z = x1 +x2 + · · ·+xn. from exercise 12.9 we know that the random variable z, being the sum of n independent pois(µ) random variables, has a pois(nµ) distribution. this means that t is a discrete random variable"
2072,1,"['mass function', 'probability mass function', 'probability', 'function']", The sampling distribution and unbiasedness,seg_259,"−k/n attaining values e , where k = 0, 1, . . . and the probability mass function of t is given by"
2073,1,"['unbiased estimator', 'histogram', 'mass function', 'probability mass function', 'case', 'probability', 'function', 'estimator', 'unbiased', 'inequality']", The sampling distribution and unbiasedness,seg_259,"the probability mass function of t for the case n = 30 and p0 = 0.1 is displayed in figure 19.3. from the histogram in figure 19.1 as well as from the probability mass function in figure 19.3, you may get the impression that t is also an unbiased estimator. however, this not the case, which follows immediately from an application of jensen’s inequality:"
2074,1,"['model', 'distribution', 'expectation', 'parameter', 'function', 'inequality']", The sampling distribution and unbiasedness,seg_259,"−x where we have a strict inequality because the function g(x) = e is strictly convex (g′′(x) = e−x > 0). recall that the parameter µ equals the expectation of the pois(µ) model distribution, so that according to section 13.1 we have e[x̄n] = µ. we find that"
2075,1,"['bias', 'estimator']", The sampling distribution and unbiasedness,seg_259,which means that the estimator t for p0 has positive bias. in fact we can compute e[t ] exactly (see exercise 19.9):
2076,1,"['sample size', 'sample', 'bias', 'case', 'expectation', 'function']", The sampling distribution and unbiasedness,seg_259,"as n goes to infinity. hence, although t has positive bias, the bias decreases to zero as the sample size becomes larger. in figure 19.4 the expectation of t is displayed as a function of the sample size n for the case µ = ln(10). for n = 30 the difference between e[t ] and p0 = 0.1 equals 0.0038."
2077,1,"['bias', 'estimate', 'relative frequency', 'biased', 'biased estimator', 'frequency', 'estimator']", The sampling distribution and unbiasedness,seg_259,"−µ quick exercise 19.2 if we estimate p0 = e by the relative frequency of zeros s = y/n, then we could estimate µ by u = − ln(s). argue that u is a biased estimator for µ. is the bias positive or negative?"
2078,1,"['estimation', 'biased', 'parameter', 'estimator']", The sampling distribution and unbiasedness,seg_259,we conclude this section by returning to the estimation of the parameter µ. apart from the (biased) estimator in quick exercise 19.2 we also considered
2079,1,"['sample', 'sample mean', 'mean', 'sample variance', 'variance']", The sampling distribution and unbiasedness,seg_259,the sample mean x̄n and sample variance sn
2080,1,['estimators'], The sampling distribution and unbiasedness,seg_259,2 as possible estimators for µ.
2081,1,"['unbiased estimators', 'estimators', 'parameter', 'unbiased']", The sampling distribution and unbiasedness,seg_259,these are both unbiased estimators for the parameter µ. this is a direct consequence of a more general property of x̄n and sn
2082,0,[], The sampling distribution and unbiasedness,seg_259,"2 , which is discussed in the next section."
2083,1,"['model', 'unbiased estimators', 'estimators', 'distribution', 'expectation', 'variance', 'unbiased']", Unbiased estimators for expectation and variance,seg_261,"sometimes the quantity of interest can be described by the expectation or variance of the model distribution, and is it irrelevant whether this distribution is of a parametric type. in this section we propose unbiased estimators for these distribution features."
2084,1,"['sample', 'random sample', 'estimators', 'distribution', 'expectation', 'random', 'variance']", Unbiased estimators for expectation and variance,seg_261,"unbiased estimators for expectation and variance. suppose x1, x2, . . . , xn is a random sample from a distribution with finite expectation µ and finite variance σ2. then"
2085,1,"['unbiased estimator', 'estimator', 'unbiased']", Unbiased estimators for expectation and variance,seg_261,is an unbiased estimator for µ and
2086,1,"['unbiased estimator', 'estimator', 'unbiased']", Unbiased estimators for expectation and variance,seg_261,is an unbiased estimator for σ2.
2087,0,[], Unbiased estimators for expectation and variance,seg_261,the second statement says e[sn
2088,1,[], Unbiased estimators for expectation and variance,seg_261,"2] = σ2. to see this, use linearity of expecta-"
2089,0,['n'], Unbiased estimators for expectation and variance,seg_261,tions to write n
2090,1,"['random variable', 'variable', 'random']", Unbiased estimators for expectation and variance,seg_261,"since e[x̄n] = µ, we have e[xi − x̄n] = e[xi]−e[x̄n] = 0. now note that for any random variable y with e[y ] = 0, we have"
2091,0,[], Unbiased estimators for expectation and variance,seg_261,note that we can write
2092,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random', 'variances']", Unbiased estimators for expectation and variance,seg_261,then from the rules concerning variances of sums of independent random variables we find that
2093,0,[], Unbiased estimators for expectation and variance,seg_261,this explains why we divide by n− 1 in the formula for sn
2094,1,['case'], Unbiased estimators for expectation and variance,seg_261,2 ; only in this case
2095,1,"['unbiased estimator', 'bias', 'estimates', 'variance', 'estimator', 'unbiased']", Unbiased estimators for expectation and variance,seg_261,"sn 2 is an unbiased estimator for the “true” variance σ2. if we would divide by n instead of n− 1, we would obtain an estimator with negative bias; it would systematically produce too-small estimates for σ2."
2096,1,['estimator'], Unbiased estimators for expectation and variance,seg_261,quick exercise 19.3 consider the following estimator for σ2:
2097,1,"['bias', 'estimator']", Unbiased estimators for expectation and variance,seg_261,"compute the bias e[vn2]− σ2 for this estimator, where you can keep computations simple by realizing that vn2 = (n − 1)sn"
2098,0,[], Unbiased estimators for expectation and variance,seg_261,unbiasedness does not always carry over
2099,0,[], Unbiased estimators for expectation and variance,seg_261,we have seen that sn
2100,1,"['unbiased estimator', 'case', 'function', 'variance', 'estimator', 'unbiased', 'inequality']", Unbiased estimators for expectation and variance,seg_261,"2 is an unbiased estimator for the “true” variance σ2. a natural question is whether sn is again an unbiased estimator for σ. this is not the case. since the function g(x) = x2 is strictly convex, jensen’s inequality yields that"
2101,1,"['unbiased estimator', 'biased', 'estimator', 'unbiased']", Unbiased estimators for expectation and variance,seg_261,"which implies that e[sn] < σ. another example is the network arrivals, in which x̄n is an unbiased estimator for µ, whereas e−x̄n is positively biased"
2102,1,"['unbiased estimator', 'estimator', 'parameter', 'unbiased']", Unbiased estimators for expectation and variance,seg_261,"−µ with respect to e . these examples illustrate a general fact: unbiasedness does not always carry over, i.e., if t is an unbiased estimator for a parameter θ, then g(t ) does not have to be an unbiased estimator for g(θ)."
2103,1,"['expectations', 'case', 'unbiased']", Unbiased estimators for expectation and variance,seg_261,"however, there is one special case in which unbiasedness does carry over, namely if g(t ) = at + b. indeed, if t is unbiased for θ: e [t ] = θ, then by the change-of-units rule for expectations,"
2104,1,['unbiased'], Unbiased estimators for expectation and variance,seg_261,which means that at + b is unbiased for aθ + b.
2105,1,['probability'], Solutions to the quick exercises,seg_263,19.1 write y for the number of xi equal to zero. denote the probability of
2106,1,"['estimate', 'relative frequency', 'frequency']", Solutions to the quick exercises,seg_263,"−µ zero by p0, so that p0 = e . this means that µ = − ln(p0). hence if we estimate p0 by the relative frequency y/n, we can estimate µ by − ln(y/n)."
2107,1,"['function', 'inequality']", Solutions to the quick exercises,seg_263,"19.2 the function g(x) = − ln(x) is strictly convex, since g′′(x) = 1/x2 > 0. hence by jensen’s inequality"
2108,1,['bias'], Solutions to the quick exercises,seg_263,"−µ since we have seen that e[s] = p0 = e , it follows that e[u ] > − ln(e[s]) = − ln(e−µ) = µ. this means that u has positive bias."
2109,1,['bias'], Solutions to the quick exercises,seg_263,we conclude that the bias of vn2 equals e[vn2]− σ2 = −σ2/n < 0.
2110,1,"['sample', 'random', 'interval', 'dataset', 'uniform distribution', 'distribution', 'random sample', 'realization']", Exercises,seg_265,"19.1 suppose our dataset is a realization of a random sample x1, x2, . . . , xn from a uniform distribution on the interval [−θ, θ], where θ is unknown."
2111,1,"['unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_265,is an unbiased estimator for θ2.
2112,1,"['unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_265,"b. is √t also an unbiased estimator for θ? if not, argue whether it has"
2113,1,['bias'], Exercises,seg_265,positive or negative bias.
2114,1,"['random variables', 'variables', 'random']", Exercises,seg_265,"19.2 suppose the random variables x1, x2, . . . , xn have the same expectation µ."
2115,1,"['unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_265,2x1 + 1 3x2 + 1 6x3 an unbiased estimator for µ?
2116,1,[], Exercises,seg_265,"b. under what conditions on constants a1, a2, . . . , an is"
2117,1,"['unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_265,an unbiased estimator for µ?
2118,1,"['random variables', 'variables', 'random']", Exercises,seg_265,"19.3 suppose the random variables x1, x2, . . . , xn have the same expectation µ. for which constants a and b is"
2119,1,"['unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_265,an unbiased estimator for µ?
2120,1,"['sample', 'random', 'dataset', 'table', 'law of large numbers', 'distribution', 'random sample', 'estimator', 'realization']", Exercises,seg_265,"19.4 recall exercise 17.5 about the number of cycles to pregnancy. suppose the dataset corresponding to the table in exercise 17.5 a is modeled as a realization of a random sample x1, x2, . . . , xn from a geo(p) distribution, where 0 < p < 1 is unknown. motivated by the law of large numbers, a natural estimator for p is"
2121,1,"['biased estimator', 'estimator', 'biased']", Exercises,seg_265,a. check that t is a biased estimator for p and find out whether it has
2122,1,['bias'], Exercises,seg_265,positive or negative bias.
2123,1,"['estimation', 'probability']", Exercises,seg_265,b. in exercise 17.5 we discussed the estimation of the probability that a
2124,1,"['frequency', 'relative frequency', 'probability']", Exercises,seg_265,woman becomes pregnant within three or fewer cycles. one possible estimator for this probability is the relative frequency of women that became pregnant within three cycles
2125,1,"['probability', 'unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_265,show that s is an unbiased estimator for this probability.
2126,1,"['sample', 'dataset', 'random sample', 'distribution', 'expectation', 'random', 'estimator', 'realization']", Exercises,seg_265,"19.5 suppose a dataset is modeled as a realization of a random sample x1, x2, . . . , xn from an exp(λ) distribution, where λ > 0 is unknown. let µ denote the corresponding expectation and let mn denote the minimum of x1, x2, . . . , xn. recall from exercise 8.18 that mn has an exp(nλ) distribution. find out for which constant c the estimator"
2127,1,"['unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_265,is an unbiased estimator for µ.
2128,1,['dataset'], Exercises,seg_265,19.6 consider the following dataset of lifetimes of ball bearings in hours.
2129,1,"['sample', 'dataset', 'random sample', 'random variable', 'variable', 'random', 'realization']", Exercises,seg_265,"one is interested in estimating the minimum lifetime of this type of ball bearing. the dataset is modeled as a realization of a random sample x1, . . . , xn. each random variable xi is represented as"
2130,1,"['model', 'unbiased estimator', 'distribution', 'parameter', 'estimator', 'unbiased']", Exercises,seg_265,where yi has an exp(λ) distribution and δ > 0 is an unknown parameter that is supposed to model the minimum lifetime. the objective is to construct an unbiased estimator for δ. it is known that
2131,1,"['unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_265,is an unbiased estimator for 1/λ.
2132,1,"['unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_265,b. construct an unbiased estimator for δ.
2133,1,"['dataset', 'estimate']", Exercises,seg_265,c. use the dataset to compute an estimate for the minimum lifetime δ. you
2134,1,"['average', 'data']", Exercises,seg_265,may use that the average lifetime of the data is 8563.5.
2135,1,['probabilities'], Exercises,seg_265,"19.7 leaves are divided into four different types: starchy-green, sugary-white, starchy-white, and sugary-green. according to genetic theory, the types occur with probabilities 1"
2136,1,"['distribution', 'random variable', 'variable', 'random']", Exercises,seg_265,"4 (θ + 2), 1 4θ, 1 4 (1 − θ), and 1 4 (1 − θ), respectively, where 0 < θ < 1. suppose one has n leaves. then the number of starchy-green leaves is modeled by a random variable n1 with a bin(n, p1) distribution, where p1 = 1"
2137,1,"['distribution', 'random variable', 'variable', 'random']", Exercises,seg_265,"4 (θ + 2), and the number of sugary-white leaves is modeled by a random variable n2 with a bin(n, p2) distribution, where p2 = 1"
2138,1,['table'], Exercises,seg_265,4θ. the following table lists the counts for the progeny of self-fertilized heterozygotes among 3839 leaves.
2139,0,[], Exercises,seg_265,starchy-green 1997 sugary-white 32 starchy-white 906 sugary-green 904
2140,1,['estimators'], Exercises,seg_265,consider the following two estimators for θ:
2141,1,"['unbiased estimators', 'estimators', 'unbiased']", Exercises,seg_265,a. check that both t1 and t2 are unbiased estimators for θ.
2142,1,['estimators'], Exercises,seg_265,b. compute the value of both estimators for θ.
2143,1,"['linear', 'model', 'regression model', 'intercept', 'regression', 'linear regression', 'linear regression model']", Exercises,seg_265,"19.8 recall the black cherry trees example from exercise 17.9, modeled by a linear regression model without intercept"
2144,1,"['random variables', 'independent', 'variables', 'estimators', 'independent random variables', 'parameter', 'random']", Exercises,seg_265,"where u1, u2, . . . , un are independent random variables with e[ui] = 0 and var(ui) = σ2. we discussed three estimators for the parameter β:"
2145,1,"['unbiased', 'estimators']", Exercises,seg_265,show that all three estimators are unbiased for β.
2146,1,"['sample', 'estimate', 'dataset', 'random sample', 'distribution', 'probability', 'random']", Exercises,seg_265,"19.9 consider the network example where the dataset is modeled as a realization of a random sample x1, x2, . . . , xn from a pois(µ) distribution. we estimate the probability of zero arrivals e−µ by means of t = e−x̄n . check that"
2147,1,['distribution'], Exercises,seg_265,"−z/n hint: write t = e , where z = x1 + x2 + · · · + xn has a pois(nµ) distribution."
2148,1,"['mse', 'bias', 'unbiased estimators', 'estimators', 'parameter of interest', 'mean', 'parameter', 'mean squared error', 'variance', 'estimator', 'error', 'unbiased']", Efficiency and mean squared error,seg_267,"in the previous chapter we introduced the notion of unbiasedness as a desirable property of an estimator. if several unbiased estimators for the same parameter of interest exist, we need a criterion for comparison of these estimators. a natural criterion is some measure of spread of the estimators around the parameter of interest. for unbiased estimators we will use variance. for arbitrary estimators we introduce the notion of mean squared error (mse), which combines variance and bias."
2149,1,"['set', 'estimate']", Estimating the number of German tanks,seg_269,"in this section we come back to the problem of estimating german war production as discussed in section 1.5. we consider serial numbers on tanks, recoded to numbers running from 1 to some unknown largest number n . given is a subset of n numbers of this set. the objective is to estimate the total number of tanks n on the basis of the observed serial numbers."
2150,1,"['without replacement', 'dependent', 'estimators', 'probability', 'random sample', 'random', 'sample', 'dataset', 'replacement', 'unbiased estimators', 'unbiased', 'random variables', 'variables', 'realization']", Estimating the number of German tanks,seg_269,"denote the observed distinct serial numbers by x1, x2, . . . , xn. this dataset can be modeled as a realization of random variables x1, x2, . . . , xn representing n draws without replacement from the numbers 1, 2, . . . , n with equal probability. note that in this example our dataset is not a realization of a random sample, because the random variables x1, x2, . . . , xn are dependent. we propose two unbiased estimators. the first one is based on the sample"
2151,1,['sample'], Estimating the number of German tanks,seg_269,and the second one is based on the sample maximum
2152,1,"['sample', 'sample mean', 'mean', 'estimator']", Estimating the number of German tanks,seg_269,an estimator based on the sample mean
2153,1,"['sample', 'unbiased estimator', 'random variables', 'sample mean', 'variables', 'dependent', 'expectation', 'mean', 'random', 'estimator', 'unbiased']", Estimating the number of German tanks,seg_269,"to construct an unbiased estimator for n based on the sample mean, we start by computing the expectation of x̄n. the linearity-of-expectations rule also applies to dependent random variables, so that"
2154,1,"['distribution', 'marginal', 'marginal distribution']", Estimating the number of German tanks,seg_269,in section 9.3 we saw that the marginal distribution of each xi is the same:
2155,1,['expectation'], Estimating the number of German tanks,seg_269,therefore the expectation of each xi is given by
2156,0,[], Estimating the number of German tanks,seg_269,this directly implies that
2157,1,"['unbiased estimator', 'estimator', 'unbiased']", Estimating the number of German tanks,seg_269,"is an unbiased estimator for n , since the change-of-units rule yields that"
2158,0,[], Estimating the number of German tanks,seg_269,quick exercise 20.1 suppose we have observed tanks with (recoded) serial numbers
2159,1,['estimator'], Estimating the number of German tanks,seg_269,compute the value of the estimator t1 for the total number of tanks.
2160,1,"['sample', 'estimator']", Estimating the number of German tanks,seg_269,an estimator based on the sample maximum
2161,1,"['unbiased estimator', 'without replacement', 'replacement', 'distribution', 'expectation', 'binomial', 'probability', 'binomial distribution', 'estimator', 'unbiased']", Estimating the number of German tanks,seg_269,"to construct an unbiased estimator for n based on the maximum, we first compute the expectation of mn. we start by computing the probability that mn = k, where k takes the values n, . . . , n . similar to the combinatorics used in section 4.3 to derive the binomial distribution, the number of ways to draw n numbers without replacement from 1, 2, . . . , n is (n"
2162,0,['n'], Estimating the number of German tanks,seg_269,n). hence each
2163,1,['probability'], Estimating the number of German tanks,seg_269,combination has probability 1/(n
2164,1,['expectation'], Estimating the number of German tanks,seg_269,thus the expectation of mn is given by
2165,0,[], Estimating the number of German tanks,seg_269,how to continue the computation of e[mn]? we use a trick: we start by rearranging
2166,0,['n'], Estimating the number of German tanks,seg_269,this holds for any n and any n ≤ n . in particular we could replace n by n + 1 and n by n + 1:
2167,1,"['variable', 'summation']", Estimating the number of German tanks,seg_269,"changing the summation variable to k = j − 1, we obtain"
2168,0,[], Estimating the number of German tanks,seg_269,"this is exactly what we need to finish the computation of e[mn]. substituting (20.2) in what we obtained earlier, we find"
2169,0,['n'], Estimating the number of German tanks,seg_269,quick exercise 20.2 choosing n = n in this formula yields e[mn ] = n . can you argue that this is the right answer without doing any computations?
2170,0,[], Estimating the number of German tanks,seg_269,with the formula for e[mn] we can derive immediately that
2171,1,"['unbiased estimator', 'estimator', 'unbiased']", Estimating the number of German tanks,seg_269,"is an unbiased estimator for n , since by the change-of-units rule,"
2172,1,['estimator'], Estimating the number of German tanks,seg_269,quick exercise 20.3 compute the value of estimator t2 for the total number of tanks on basis of the observed numbers from quick exercise 20.1.
2173,1,"['histogram', 'simulation', 'without replacement', 'replacement', 'estimators', 'parameter of interest', 'probability', 'vary', 'parameter', 'histograms', 'unbiased', 'distributions']", Variance of an estimator,seg_271,"in the previous section we saw that we can construct two completely different estimators for the total number of tanks n that are both unbiased. the obvious question is: which of the two is better? to answer this question, we investigate how both estimators vary around the parameter of interest n . although we could in principle compute the distributions of t1 and t2, we carry out a small simulation study instead. take n = 1000 and n = 10 fixed. we draw 10 numbers, without replacement, from 1, 2, . . . , 1000 and compute the value of the estimators t1 and t2. we repeat this two thousand times, so that we have 2000 values for both estimators. in figure 20.1 we have displayed the histogram of the 2000 values for t1 on the left and the histogram of the 2000 values for t2 on the right. from the histograms, which reflect the probability"
2174,1,"['estimates', 'estimators', 'random', 'vary', 'estimator', 'random variable', 'parameter', 'histograms', 'distributions', 'functions', 'parameter of interest', 'variance', 'unbiased', 'variable', 'variation']", Variance of an estimator,seg_271,"mass functions of both estimators, we see that the distributions of t1 and t2 are of completely different types. as can be expected from the fact that both estimators are unbiased, the values vary around the parameter of interest n = 1000. the most important difference between the histograms is that the variation in the values of t2 is less than the variation in the values of t1. this suggests that estimator t2 estimates the total number of tanks more efficiently than estimator t1, in the sense that it produces estimates that are more concentrated around the parameter of interest n than estimates produced by t1. recall that the variance measures the spread of a random variable. hence the previous discussion motivates the use of the variance of an estimator to evaluate its performance."
2175,1,"['efficient', 'unbiased estimators', 'estimators', 'parameter', 'estimator', 'unbiased']", Variance of an estimator,seg_271,"efficiency. let t1 and t2 be two unbiased estimators for the same parameter θ. then estimator t2 is called more efficient than estimator t1 if var(t2) < var(t1), irrespective of the value of θ."
2176,0,[], Variance of an estimator,seg_271,let us compare t1 and t2 using this criterion. for t1 we have
2177,1,"['random variables', 'independent', 'variables', 'distribution', 'random']", Variance of an estimator,seg_271,"although the xi are not independent, it is true that all pairs (xi, xj) with i = j have the same distribution (this follows in the same way in which we showed on page 122 that all xi have the same distribution). with the variance-of-the-sum rule for n random variables (see exercise 10.17), we find that"
2178,0,[], Variance of an estimator,seg_271,"in exercises 9.18 and 10.18, we computed that"
2179,0,[], Variance of an estimator,seg_271,we find therefore that
2180,1,['variance'], Variance of an estimator,seg_271,obtaining the variance of t2 is a little more work. one can compute the variance of mn in a way that is very similar to the way we obtained e[mn]. the result is (see remark 20.1 for details)
2181,1,['variance'], Variance of an estimator,seg_271,"with the expression for the variance of mn, we derive"
2182,1,"['efficient', 'case', 'estimators', 'variances']", Variance of an estimator,seg_271,"we see that var(t2) < var(t1) for all n and n ≥ 2. hence t2 is always more efficient than t1, except when n = 1. in this case the variances are equal, simply because the estimators are the same—they both equal x1."
2183,1,"['efficiency', 'case', 'relative efficiency']", Variance of an estimator,seg_271,"the quotient var(t1) /var(t2), is called the relative efficiency of t2 with respect to t1. in our case the relative efficiency of t2 with respect to t1 equals"
2184,1,"['sample size', 'sample']", Variance of an estimator,seg_271,"surprisingly, this quotient does not depend on n , and we see clearly the advantage of t2 over t1 as the sample size n gets larger."
2185,1,['sample'], Variance of an estimator,seg_271,"quick exercise 20.4 let n = 5, and let the sample be"
2186,1,['estimator'], Variance of an estimator,seg_271,compute the value of the estimator t1 for n . do you notice anything strange?
2187,1,['samples'], Variance of an estimator,seg_271,the self-contradictory behavior of t1 in quick exercise 20.4 is not rare: this phenomenon will occur for up to 50% of the samples if n and n are large. this gives another reason to prefer t2 over t1.
2188,1,"['estimated', 'unbiased estimators', 'estimators', 'parameter', 'variance', 'estimator', 'unbiased']", Mean squared error,seg_273,"in the last section we compared two unbiased estimators by considering their spread around the value to be estimated, where the spread was measured by the variance. although unbiasedness is a desirable property, the performance of an estimator should mainly be judged by the way it spreads around the parameter θ to be estimated. this leads to the following definition."
2189,1,"['mean', 'parameter', 'mean squared error', 'estimator', 'error']", Mean squared error,seg_273,definition. let t be an estimator for a parameter θ. the mean squared error of t is the number mse(t ) = e[(t − θ)2] .
2190,1,['estimator'], Mean squared error,seg_273,"according to this criterion, an estimator t1 performs better than an estimator t2 if mse(t1) < mse(t2). note that"
2191,1,"['bias', 'mse', 'efficient', 'errors', 'estimators', 'parameter of interest', 'mean', 'parameter', 'efficiency', 'variance', 'estimator', 'unbiased']", Mean squared error,seg_273,"so the mse of t turns out to be the variance of t plus the square of the bias of t . in particular, when t is unbiased, the mse of t is just the variance of t . this means that we already used mean squared errors to compare the estimators t1 and t2 in the previous section. we extend the notion of efficiency by saying that estimator t2 is more efficient than estimator t1 (for the same parameter of interest), if the mse of t2 is smaller than the mse of t1."
2192,1,['efficiency'], Mean squared error,seg_273,unbiasedness and efficiency
2193,1,"['unbiased estimator', 'biased', 'biased estimator', 'variance', 'estimator', 'unbiased']", Mean squared error,seg_273,a biased estimator with a small variance may be more useful than an unbiased estimator with a large variance. we illustrate this with the network server
2194,1,"['estimators', 'probability', 'estimate']", Mean squared error,seg_273,example from section 19.2. recall that our goal was to estimate the probability p0 = e−µ of zero arrivals (of packages) in a minute. we did have two promising candidates as estimators:
2195,1,"['efficient', 'random samples', 'biased', 'distribution', 'samples', 'simulations', 'random', 'histograms', 'estimator']", Mean squared error,seg_273,"in figure 20.2 we depict histograms of one thousand simulations of the values of s and t computed for random samples of size n = 25 from a pois(µ) distribution, where µ = 2. considering the way the values of the (biased!) estimator t are more concentrated around the true value e−µ = e−2 = 0.1353, we would be inclined to prefer t over s. this choice is strongly supported by the fact that t is more efficient than s: mse(t ) is always smaller than mse(s), as illustrated in figure 20.3."
2196,1,"['sample', 'estimate']", Solutions to the quick exercises,seg_275,"20.4 since 45 is in the sample, n has to be at least 45. adding the numbers yields 7 + 3 + 10 + 15 + 45 = 80. so t1 = 2x̄n − 1 = 2 · 16 − 1 = 31. what is strange about this is that the estimate for n is far smaller than the number 45 in the sample!"
2197,1,"['sample', 'efficient', 'sample mean', 'estimate', 'relative efficiency', 'random sample', 'distribution', 'expectation', 'mean', 'random', 'efficiency', 'variance', 'estimator']", Exercises,seg_277,"20.1 given is a random sample x1, x2, . . . , xn from a distribution with finite variance σ2. we estimate the expectation of the distribution with the sample mean x̄n. argue that the larger our sample, the more efficient our estimator. what is the relative efficiency var(x̄n) /var(x̄2n) of x̄2n with respect to x̄n?"
2198,1,"['parameter', 'estimators']", Exercises,seg_277,20.2 given are two estimators s and t for a parameter θ. furthermore it is known that var(s) = 40 and var(t ) = 4.
2199,1,['estimator'], Exercises,seg_277,a. suppose that we know that e[s] = θ and e[t ] = θ + 3. which estimator
2200,0,[], Exercises,seg_277,"would you prefer, and why?"
2201,1,['estimator'], Exercises,seg_277,"number a. for each a, which estimator would you prefer, and why?"
2202,1,"['sample', 'random', 'estimate', 'mean', 'random sample']", Exercises,seg_277,"20.3 suppose we have a random sample x1, . . . , xn from an exp(λ) distribution. suppose we want to estimate the mean 1/λ. according to section 19.4"
2203,1,['estimator'], Exercises,seg_277,the estimator 1
2204,1,"['distribution', 'unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_277,"is an unbiased estimator of 1/λ. let mn be the minimum of x1, x2, . . . , xn. recall from exercise 8.18 that mn has an exp(nλ) distribution. in exercise 19.5 you have determined that"
2205,1,"['unbiased estimator', 'estimators', 'mean', 'estimator', 'unbiased']", Exercises,seg_277,is another unbiased estimator for 1/λ. which of the estimators t1 and t2 would you choose for estimating the mean 1/λ? substantiate your answer.
2206,1,"['sample', 'unbiased estimator', 'estimate', 'without replacement', 'replacement', 'distribution', 'variable', 'random variable', 'symmetry', 'parameter', 'random', 'estimator', 'unbiased']", Exercises,seg_277,"20.4 consider the situation of this chapter, where we have to estimate the parameter n from a sample x1, . . . , xn drawn without replacement from the numbers {1, . . . , n}. to keep it simple, we consider n = 2. let m = m2 be the maximum of x1 and x2. we have found that t2 = 3m/2 − 1 is a good unbiased estimator for n . we want to construct a new unbiased estimator t3 based on the minimum l of x1 and x2. in the following you may use that the random variable l has the same distribution as the random variable n + 1 − m (this follows from symmetry considerations)."
2207,1,"['unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_277,a. show that t3 = 3l − 1 is an unbiased estimator for n .
2208,0,[], Exercises,seg_277,has been computed in remark 20.1.)
2209,1,"['efficiency', 'relative efficiency']", Exercises,seg_277,c. what is the relative efficiency of t2 with respect to t3?
2210,1,"['relative efficiency', 'unbiased estimators', 'joint probability distribution', 'case', 'estimators', 'distribution', 'information', 'probability distribution', 'joint', 'probability', 'efficiency', 'variance', 'estimator', 'unbiased', 'joint probability']", Exercises,seg_277,"20.5 someone is proposing two unbiased estimators u and v , with the same variance var(u) = var(v ). it therefore appears that we would not prefer one estimator over the other. however, we could go for a third estimator, namely w = (u + v )/2. note that w is unbiased. to judge the quality of w we want to compute its variance. lacking information on the joint probability distribution of u and v , this is impossible. however, we should prefer w in any case! to see this, show by means of the variance-of-the-sum rule that the relative efficiency of u with respect to w is equal to"
2211,1,"['coefficient', 'correlation coefficient', 'correlation']", Exercises,seg_277,"here ρ(u, v ) is the correlation coefficient. why does this result imply that we should use w instead of u (or v )?"
2212,1,"['random variables', 'independent', 'variables', 'uncertainty', 'independent random variables', 'measurements', 'random', 'expectations']", Exercises,seg_277,"20.6 a geodesic engineer measures the three unknown angles α1, α2, and α3 of a triangle. he models the uncertainty in the measurements by considering them as realizations of three independent random variables t1, t2, and t3 with expectations"
2213,1,"['variance', 'estimators']", Exercises,seg_277,"and all three with the same variance σ2. in order to make use of the fact that the three angles must add to π, he also considers new estimators u1, u2, and u3 defined by"
2214,1,['measurements'], Exercises,seg_277,(note that the “deviation” π − t1 − t2 − t3 is evenly divided over the three measurements and that u1 + u2 + u3 = π.)
2215,1,"['efficiency', 'estimate']", Exercises,seg_277,b. what does he gain in efficiency when he uses u1 instead of t1 to estimate
2216,1,['estimator'], Exercises,seg_277,c. what kind of estimator would you choose for α1 if it is known that the
2217,1,['probabilities'], Exercises,seg_277,"20.7 (exercise 19.7 continued.) leaves are divided into four different types: starchy-green, sugary-white, starchy-white, and sugary-green. according to genetic theory, the types occur with probabilities 1"
2218,1,"['distribution', 'random variable', 'variable', 'random']", Exercises,seg_277,"4 (θ + 2), 1 4θ, 1 4 (1 − θ), and 1 4 (1 − θ), respectively, where 0 < θ < 1. suppose one has n leaves. then the number of starchy-green leaves is modeled by a random variable n1 with a bin(n, p1) distribution, where p1 = 1"
2219,1,"['distribution', 'random variable', 'variable', 'random']", Exercises,seg_277,"4 (θ +2), and the number of sugary-white leaves is modeled by a random variable n2 with a bin(n, p2) distribution, where p2 = 1"
2220,1,['estimators'], Exercises,seg_277,4θ. consider the following two estimators for θ:
2221,1,"['unbiased estimators', 'estimators', 'estimator', 'unbiased']", Exercises,seg_277,in exercise 19.7 you showed that both t1 and t2 are unbiased estimators for θ. which estimator would you prefer? motivate your answer.
2222,1,"['sample', 'independent', 'random samples', 'sample means', 'estimators', 'distribution', 'samples', 'mean', 'random', 'estimator']", Exercises,seg_277,20.8 let x̄n and ȳm be the sample means of two independent random samples of size n (resp. m) from the same distribution with mean µ. we combine these two estimators to a new estimator t by putting
2223,0,[], Exercises,seg_277,where r is some number between 0 and 1.
2224,1,"['unbiased estimator', 'estimator', 'mean', 'unbiased']", Exercises,seg_277,a. show that t is an unbiased estimator for the mean µ.
2225,1,['efficient'], Exercises,seg_277,b. show that t is most efficient when r = n/(n + m).
2226,1,"['sample', 'random', 'estimators', 'distribution', 'random sample']", Exercises,seg_277,"20.9 given is a random sample x1, x2, . . . , xn from a ber(p) distribution. one considers the estimators"
2227,1,"['unbiased estimators', 'estimators', 'unbiased']", Exercises,seg_277,a. are t1 and t2 unbiased estimators for p?
2228,1,[], Exercises,seg_277,"1 mse(t1) = p(1 − p), mse(t2) = pn − 2pn+1 + p2. n"
2229,1,"['estimator', 'efficient']", Exercises,seg_277,c. which estimator is more efficient when n = 2?
2230,1,"['sample', 'random', 'estimate', 'expectation', 'random sample']", Exercises,seg_277,"20.10 suppose we have a random sample x1, . . . , xn from an exp(λ) distribution. we want to estimate the expectation 1/λ. according to section 19.4,"
2231,1,"['unbiased estimator', 'estimator', 'unbiased', 'estimators']", Exercises,seg_277,is an unbiased estimator of 1/λ. let us consider more generally estimators t of the form
2232,1,"['mse', 'estimators']", Exercises,seg_277,where c is a real number. we are interested in the mse of these estimators and would like to know whether there are choices for c that yield a smaller mse than the choice c = 1/n.
2233,1,[], Exercises,seg_277,a. compute mse(t ) for each c.
2234,1,"['mse', 'estimator']", Exercises,seg_277,b. for which c does the estimator perform best in the mse sense? compare
2235,1,"['unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_277,this to the unbiased estimator x̄n that one obtains for c = 1/n.
2236,1,"['linear', 'model', 'regression model', 'intercept', 'regression', 'linear regression', 'linear regression model']", Exercises,seg_277,20.11 in exercise 17.9 we modeled diameters of black cherry trees with the linear regression model (without intercept)
2237,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random']", Exercises,seg_277,"for i = 1, 2, . . . , n. as usual, the ui here are independent random variables with e[ui]=0, and var(ui) = σ2."
2238,1,"['least squares', 'estimators', 'average', 'estimator', 'slope']", Exercises,seg_277,"we considered three estimators for the slope β of the line y = βx: the socalled least squares estimator t1 (which will be considered in chapter 22), the average slope estimator t2, and the slope of the averages estimator t3. these estimators are defined by:"
2239,1,"['mse', 'unbiased', 'estimators']", Exercises,seg_277,in exercise 19.8 it was shown that all three estimators are unbiased. compute the mse of all three estimators.
2240,1,"['cauchy', 'efficient', 'inequality']", Exercises,seg_277,"remark: it can be shown that t1 is always more efficient than t3, which in turn is more efficient than t2. to prove the first inequality one uses a famous inequality called the cauchy schwartz inequality; for the second inequality one uses jensen’s inequality (can you see how?)."
2241,1,"['distribution', 'probability', 'without replacement', 'replacement']", Exercises,seg_277,"20.12 let x1, x2, . . . , xn represent n draws without replacement from the numbers 1, 2, . . . , n with equal probability. the goal of this exercise is to compute the distribution of mn in a way other than by the combinatorial analysis we did in this chapter."
2242,1,"['sample', 'maximum likelihood estimators', 'probabilities', 'sample mean', 'parameters', 'frequencies', 'likelihood', 'estimators', 'relative frequencies', 'maximum likelihood', 'expectation', 'mean']", Maximum likelihood,seg_279,"in previous chapters we could easily construct estimators for various parameters of interest because these parameters had a natural sample analogue: expectation versus sample mean, probabilities versus relative frequencies, etc. however, in some situations such an analogue does not exist. in this chapter, a general principle to construct estimators is introduced, the so-called maximum likelihood principle. maximum likelihood estimators have certain attractive properties that are discussed in the last section."
2243,1,"['table', 'geometric distribution', 'data', 'distribution', 'variable', 'parameter', 'geometric']", Why a general principle,seg_281,in section 4.4 we modeled the number of cycles up to pregnancy by a random variable x with a geometric distribution with (unknown) parameter p. weinberg and gladen studied the effect of smoking on the number of cycles and obtained the data in table 21.1 for 100 smokers and 486 nonsmokers.
2244,1,"['cases', 'parameter', 'probability']", Why a general principle,seg_281,"is the parameter p, which equals the probability of becoming pregnant after one cycle, different for smokers and nonsmokers? let us try to find out by estimating p in the two cases."
2245,1,"['law of large numbers', 'estimate']", Why a general principle,seg_281,"what would be reasonable ways to estimate p? since p = p(x = 1), the law of large numbers (see section 13.3) motivates use of"
2246,1,"['unbiased estimator', 'estimate', 'estimates', 'table', 'case', 'data', 'estimators', 'cases', 'information', 'average', 'estimator', 'unbiased']", Why a general principle,seg_281,"as an estimator for p. this yields estimates p = 29/100 = 0.29 for smokers and p = 198/486 = 0.41 for nonsmokers. we know from section 19.4 that s is an unbiased estimator for p. however, one cannot escape the feeling that s is a “bad” estimator: s does not use all the information in the table, i.e., the way the women are distributed over the numbers 2, 3, . . . of observed numbers of cycles is not used. one would like to have an estimator that incorporates all the available information. due to the way the data are given, this seems to be difficult. for instance, estimators based on the average cannot be evaluated, because 7 smokers and 12 nonsmokers had an unknown number of cycles up to pregnancy (larger than 12). if one simply ignores the last column in table 21.1 as we did in exercise 17.5, the average can be computed and yields 1/x̄93 = 0.2809 as an estimate of p for smokers and 1/x̄474 = 0.3688 for nonsmokers. however, because we discard seven values larger than 12 in case of the smokers and twelve values larger than 12 in case of the nonsmokers, we overestimate p in both cases."
2247,1,"['estimate', 'likelihood', 'estimators', 'parameter of interest', 'maximum likelihood', 'parameter']", Why a general principle,seg_281,"in the next section we introduce a general principle to find an estimate for a parameter of interest, the maximum likelihood principle. this principle yields good estimators and will solve problems such as those stated earlier."
2248,1,"['test', 'percentage']", The maximum likelihood principle,seg_283,"suppose a dealer of computer chips is offered on the black market two batches of 10 000 chips each. according to the seller, in one batch about 50% of the chips are defective, while this percentage is about 10% in the other batch. our dealer is only interested in this last batch. unfortunately the seller cannot tell the two batches apart. to help him to make up his mind, the seller offers our dealer one batch, from which he is allowed to select and test 10 chips. after selecting 10 chips arbitrarily, it turns out that only the second one is defective. our dealer at once decides to buy this batch. is this a wise decision?"
2249,1,"['maximum likelihood', 'likelihood']", The maximum likelihood principle,seg_283,"with the batch where 50% of the chips are defective it is more likely that defective chips will appear, whereas with the other batch one would expect hardly any defective chip. clearly, our dealer chooses the batch for which it is most likely that only one chip is defective. this is also the guiding idea behind the maximum likelihood principle."
2250,1,"['dataset', 'likelihood', 'data', 'maximum likelihood']", The maximum likelihood principle,seg_283,"the maximum likelihood principle. given a dataset, choose the parameter(s) of interest in such a way that the data are most likely."
2251,1,"['random variables', 'independent', 'variables', 'case', 'data', 'probability', 'random']", The maximum likelihood principle,seg_283,"set ri = 1 in case the ith tested chip was defective and ri = 0 in case it was operational, where i = 1, . . . , 10. then r1, . . . , r10 are ten independent ber(p) distributed random variables, where p is the probability that a randomly selected chip is defective. the probability that the observed data occur is equal to"
2252,0,[], The maximum likelihood principle,seg_283,for the batch where about 10% of the chips are defective we find that
2253,0,[], The maximum likelihood principle,seg_283,whereas for the other batch
2254,1,"['probability', 'data']", The maximum likelihood principle,seg_283,"so the probability for the batch with only 10% defective chips is about 40 times larger than the probability for the other batch. given the data, our dealer made a sound decision."
2255,0,[], The maximum likelihood principle,seg_283,quick exercise 21.1 which batch should the dealer choose if only the first three chips are defective?
2256,0,[], The maximum likelihood principle,seg_283,"returning to the example of the number of cycles up to pregnancy, denoting xi as the number of cycles up to pregnancy of the ith smoker, recall that"
2257,1,['success'], The maximum likelihood principle,seg_283,p(xi > 12) = p(no success in cycle 1 to 12) = (1 − p)12;
2258,1,"['sample', 'model', 'table', 'geometric distribution', 'random sample', 'data', 'distribution', 'probability', 'random', 'function', 'geometric']", The maximum likelihood principle,seg_283,"cf. quick exercise 4.6. from table 21.1 we see that there are 29 smokers for which xi = 1, that there are 16 for which xi = 2, etc. since we model the data as a random sample from a geometric distribution, the probability of the data—as a function of p—is given by"
2259,1,"['maximum likelihood', 'likelihood']", The maximum likelihood principle,seg_283,"here c is the number of ways we can assign 29 ones, 16 twos, . . . , 3 twelves, and 7 numbers larger than 12 to 100 smokers.1 according to the maximum likelihood principle we now choose p, with 0 ≤ p ≤ 1, in such a way, that l(p)"
2260,1,['function'], The maximum likelihood principle,seg_283,"is maximal. since c does not depend on p, we do not need to know the value of c explicitly to find for which p the function l(p) is maximal."
2261,0,[], The maximum likelihood principle,seg_283,differentiating l(p) with respect to p yields that
2262,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", The maximum likelihood principle,seg_283,"now l′(p) = 0 if p = 0, p = 1, or p = 93/415 = 0.224, and l(p) attains its unique maximum in this last point (check this!). we say that 93/415 = 0.224 is the maximum likelihood estimate of p for the smokers. note that this estimate is quite a lot smaller than the estimate 0.29 for the smokers we found in the previous section, and the estimate 0.2809 you obtained in exercise 17.5."
2263,1,"['probability', 'data']", The maximum likelihood principle,seg_283,quick exercise 21.2 check that for the nonsmokers the probability of the data is given by
2264,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", The maximum likelihood principle,seg_283,compute the maximum likelihood estimate for p.
2265,1,"['sample', 'random', 'dataset', 'distribution', 'parameter', 'random sample', 'dependence', 'realization']", Likelihood and loglikelihood,seg_285,"suppose we have a dataset x1, x2, . . . , xn, modeled as a realization of a random sample from a distribution characterized by a parameter θ. to stress the dependence of the distribution on θ, we write"
2266,1,"['sample', 'mass function', 'probability mass function', 'case', 'discrete', 'distribution', 'probability', 'function', 'discrete distribution']", Likelihood and loglikelihood,seg_285,for the probability mass function in case we have a sample from a discrete distribution and
2267,1,"['sample', 'density function', 'continuous distribution', 'probability density function', 'continuous', 'distribution', 'probability', 'function']", Likelihood and loglikelihood,seg_285,for the probability density function when we have a sample from a continuous distribution.
2268,1,"['sample', 'estimate', 'dataset', 'likelihood', 'random sample', 'discrete', 'distribution', 'maximum likelihood', 'random', 'function', 'discrete distribution', 'realization']", Likelihood and loglikelihood,seg_285,"for a dataset x1, x2, . . . , xn modeled as the realization of a random sample x1, . . . , xn from a discrete distribution, the maximum likelihood principle now tells us to estimate θ by that value, for which the function l(θ), given by"
2269,1,"['maximum likelihood estimate', 'estimate', 'likelihood', 'maximum likelihood', 'likelihood function', 'function']", Likelihood and loglikelihood,seg_285,"is maximal. this value is called the maximum likelihood estimate of θ. the function l(θ) is called the likelihood function. this is a function of θ, determined by the numbers x1, x2, . . . , xn."
2270,1,"['sample', 'density function', 'continuous distribution', 'probability density function', 'independent', 'continuous', 'likelihood', 'case', 'discrete', 'distribution', 'likelihood function', 'probability', 'function']", Likelihood and loglikelihood,seg_285,"in case the sample is from a continuous distribution we clearly need to define the likelihood function l(θ) in a way different from the discrete case (if we would define l(θ) as in the discrete case, one always would have that l(θ) = 0). for a reasonable definition of the likelihood function we have the following motivation. let fθ be the probability density function of x , and let ε > 0 be some fixed, small number. it is sensible to choose θ in such a way, that the probability p(x1 − ε ≤ x1 ≤ x1 + ε, . . . , xn − ε ≤ xn ≤ xn + ε) is maximal. since the xi are independent, we find that"
2271,0,[], Likelihood and loglikelihood,seg_285,where in the last step we used that (see also equation (5.1))
2272,1,"['dataset', 'likelihood', 'likelihood function', 'function']", Likelihood and loglikelihood,seg_285,"note that the right-hand side of (21.1) is maximal whenever the function fθ(x1)fθ(x2) · · · fθ(xn) is maximal, irrespective of the value of ε. in view of this, given a dataset x1, x2, . . . , xn, the likelihood function l(θ) is defined by"
2273,1,"['continuous', 'case']", Likelihood and loglikelihood,seg_285,in the continuous case.
2274,1,"['estimate', 'estimates', 'likelihood', 'random variable', 'variable', 'maximum likelihood', 'likelihood function', 'random', 'function']", Likelihood and loglikelihood,seg_285,"maximum likelihood estimates. the maximum likelihood estimate of θ is the value t = h(x1, x2, . . . , xn) that maximizes the likelihood function l(θ). the corresponding random variable"
2275,1,"['maximum likelihood estimator', 'maximum likelihood', 'estimator', 'likelihood']", Likelihood and loglikelihood,seg_285,is called the maximum likelihood estimator for θ.
2276,1,"['sample', 'density function', 'probability density function', 'dataset', 'random sample', 'distribution', 'probability', 'random', 'function']", Likelihood and loglikelihood,seg_285,"as an example, suppose we have a dataset x1, x2, . . . , xn modeled as a realization of a random sample from an exp(λ) distribution, with probability density function given by fλ(x) = 0 if x < 0 and"
2277,1,['likelihood'], Likelihood and loglikelihood,seg_285,then the likelihood is given by
2278,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", Likelihood and loglikelihood,seg_285,"to obtain the maximum likelihood estimate of λ it is enough to find the maximum of l(λ). to do so, we determine the derivative of l(λ):"
2279,1,"['likelihood', 'maximum likelihood', 'maximum likelihood estimator', 'likelihood function', 'function', 'estimator']", Likelihood and loglikelihood,seg_285,"i.e., if λ = 1/x̄n. check that for this value of λ the likelihood function l(λ) attains a maximum! so the maximum likelihood estimator for λ is 1/x̄n."
2280,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", Likelihood and loglikelihood,seg_285,"in the example of the number of cycles up to pregnancy of smoking women, we have seen that l(p) = c ·p93 ·(1−p)322. the maximum likelihood estimate of p was found by differentiating l(p). differentiating is not always possible, as the following example shows."
2281,1,"['distribution', 'uniform distribution']", Likelihood and loglikelihood,seg_285,estimating the upper endpoint of a uniform distribution
2282,1,"['sample', 'density function', 'probability density function', 'dataset', 'random sample', 'distribution', 'probability', 'random', 'function', 'realization']", Likelihood and loglikelihood,seg_285,"suppose the dataset x1 = 0.98, x2 = 1.57, and x3 = 0.31 is the realization of a random sample from a u(0, θ) distribution with θ > 0 unknown. the probability density function of each xi is now given by fθ(x) = 0 if x is not"
2283,1,['likelihood'], Likelihood and loglikelihood,seg_285,"the likelihood l(θ) is zero if θ is smaller than at least one of the xi, and equals 1/θ3 if θ is greater than or equal to each of the three xi, i.e.,"
2284,1,"['function', 'likelihood', 'likelihood function']", Likelihood and loglikelihood,seg_285,"figure 21.1 depicts this likelihood function. one glance at this figure is enough to realize that l(θ) attains its maximum at max (x1, x2, x3) = 1.57."
2285,1,"['dataset', 'likelihood', 'maximum likelihood', 'maximum likelihood estimator', 'estimator']", Likelihood and loglikelihood,seg_285,"in general, given a dataset x1, x2, . . . , xn originating from a u(0, θ) distribution, we see that l(θ) = 0 if θ is smaller than at least one of the xi and that l(θ) = 1/θn if θ is greater than or equal to the largest of the xi. we conclude that the maximum likelihood estimator of θ is given by max {x1, x2, . . . , xn}."
2286,1,"['likelihood', 'product rule', 'loglikelihood function', 'parameter', 'likelihood function', 'function', 'process']", Likelihood and loglikelihood,seg_285,"in the preceding example it was easy to find the value of the parameter for which the likelihood is maximal. usually one can find the maximum by differentiating the likelihood function l(θ). the calculation of the derivative of l(θ) may be tedious, because l(θ) is a product of terms, all involving θ (see also quick exercise 21.3). to differentiate l(θ) we have to apply the product rule from calculus. considering the logarithm of l(θ) changes the product of the terms involving θ into a sum of logarithms of these terms, which makes the process of differentiating easier. moreover, because the logarithm is an increasing function, the likelihood function l(θ) and the loglikelihood function (θ), defined by"
2287,1,"['extreme values', 'likelihood', 'loglikelihood function', 'likelihood function', 'function']", Likelihood and loglikelihood,seg_285,"attain their extreme values for the same values of θ. in particular, l(θ) is maximal if and only if (θ) is maximal. this is illustrated in figure 21.2 by the likelihood function l(p) = cp93(1 − p)322 and the loglikelihood function (p) = ln(c) + 93 ln(p) + 322 ln(1 − p) for the smokers."
2288,1,"['sample', 'dataset', 'likelihood', 'random sample', 'distribution', 'loglikelihood function', 'likelihood function', 'random', 'function']", Likelihood and loglikelihood,seg_285,"in the situation that we have a dataset x1, x2, . . . , xn modeled as a realization of a random sample from an exp(λ) distribution, we found as likelihood function l(λ) = λn · e−λ(x1+x2+···+xn). therefore, the loglikelihood function is given by"
2289,1,"['maximum likelihood estimate', 'estimate', 'likelihood', 'maximum likelihood', 'loglikelihood function', 'function']", Likelihood and loglikelihood,seg_285,"quick exercise 21.3 in this example, use the loglikelihood function (λ) to show that the maximum likelihood estimate of λ equals 1/x̄n."
2290,1,"['parameters', 'distribution', 'normal', 'normal distribution']", Likelihood and loglikelihood,seg_285,estimating the parameters of the normal distribution
2291,1,"['estimates', 'case', 'likelihood function', 'random sample', 'random', 'function', 'sample', 'dataset', 'distribution', 'maximum likelihood estimates', 'variables', 'likelihood', 'maximum likelihood', 'realization']", Likelihood and loglikelihood,seg_285,"suppose that the dataset x1, x2, . . . , xn is a realization of a random sample from an n(µ, σ2) distribution, with µ and σ unknown. what are the maximum likelihood estimates for µ and σ? in this case θ is the vector (µ, σ), and therefore the likelihood function is a function of two variables:"
2292,1,"['density function', 'probability density function', 'probability', 'function']", Likelihood and loglikelihood,seg_285,"where each fµ,σ(x) is the n(µ, σ2) probability density function:"
2293,0,[], Likelihood and loglikelihood,seg_285,the partial derivatives of are
2294,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", Likelihood and loglikelihood,seg_285,"it is not hard to show that for these values of µ and σ the likelihood function l(µ, σ) attains a maximum. we find that x̄n is the maximum likelihood estimate for µ and that"
2295,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", Likelihood and loglikelihood,seg_285,is the maximum likelihood estimate for σ.
2296,1,"['maximum likelihood estimators', 'likelihood', 'estimators', 'maximum likelihood']", Properties of maximum likelihood estimators,seg_287,"apart from the fact that the maximum likelihood principle provides a general principle to construct estimators, one can also show that maximum likelihood estimators have several desirable properties."
2297,0,[], Properties of maximum likelihood estimators,seg_287,"in the previous example, we saw that"
2298,1,"['likelihood', 'maximum likelihood', 'maximum likelihood estimator', 'parameter', 'estimator']", Properties of maximum likelihood estimators,seg_287,"is the maximum likelihood estimator for the parameter σ of an n(µ, σ2) distribution. does this imply that dn"
2299,1,"['likelihood', 'case', 'maximum likelihood', 'maximum likelihood estimator', 'parameter', 'function', 'estimator']", Properties of maximum likelihood estimators,seg_287,"2 is the maximum likelihood estimator for σ2? this is indeed the case! in general one can show that if t is the maximum likelihood estimator of a parameter θ and g(θ) is an invertible function of θ, then g(t ) is the maximum likelihood estimator for g(θ)."
2300,1,[], Properties of maximum likelihood estimators,seg_287,asymptotic unbiasedness
2301,1,"['likelihood', 'biased', 'maximum likelihood', 'maximum likelihood estimator', 'estimator']", Properties of maximum likelihood estimators,seg_287,"the maximum likelihood estimator t may be biased. for example, because dn"
2302,1,"['maximum likelihood estimator', 'maximum likelihood', 'estimator', 'likelihood']", Properties of maximum likelihood estimators,seg_287,"2 , for the previously mentioned maximum likelihood estimator dn"
2303,0,[], Properties of maximum likelihood estimators,seg_287,we see that dn
2304,1,"['biased', 'expected value', 'biased estimator', 'estimator']", Properties of maximum likelihood estimators,seg_287,"2 is a biased estimator for σ2, but also that as n goes to infinity, the expected value of dn"
2305,1,"['estimators', 'random', 'estimator', 'dataset', 'maximum likelihood estimator', 'mean', 'parameter', 'distribution', 'unbiased', 'maximum likelihood estimators', 'random variables', 'variables', 'likelihood', 'maximum likelihood']", Properties of maximum likelihood estimators,seg_287,"2 converges to σ2. this holds more generally. under mild conditions on the distribution of the random variables xi under consideration (see, e.g., [36]), one can show that asymptotically (that is, as the size n of the dataset goes to infinity) maximum likelihood estimators are unbiased. by this we mean that if tn = h(x1, x2, . . . , xn) is the maximum likelihood estimator for a parameter θ, then"
2306,1,"['minimum variance', 'variance']", Properties of maximum likelihood estimators,seg_287,asymptotic minimum variance
2307,1,"['unbiased estimator', 'unbiased estimators', 'likelihood', 'estimators', 'maximum likelihood', 'maximum likelihood estimator', 'parameter', 'variance', 'estimator', 'unbiased']", Properties of maximum likelihood estimators,seg_287,"the variance of an unbiased estimator for a parameter θ is always larger than or equal to a certain positive number, known as the cramér-rao lower bound (see remark 20.2). again under mild conditions one can show that maximum likelihood estimators have asymptotically the smallest variance among unbiased estimators. that is, asymptotically the variance of the maximum likelihood estimator for a parameter θ attains the cramér-rao lower bound."
2308,1,"['probability', 'case', 'data']", Solutions to the quick exercises,seg_289,"21.1 in the case that only the first three chips are defective, the probability that the observed data occur is equal to"
2309,0,[], Solutions to the quick exercises,seg_289,for the batch where about 10% of the chips are defective we find that
2310,1,['probability'], Solutions to the quick exercises,seg_289,whereas for the other batch this probability is equal to ( 2
2311,1,['probability'], Solutions to the quick exercises,seg_289,"so the probability for the batch with about 50% defective chips is about 2 times larger than the probability for the other batch. in view of this, it would be reasonable to choose the other batch, not the tested one."
2312,1,['table'], Solutions to the quick exercises,seg_289,21.2 from table 21.1 we derive
2313,0,[], Solutions to the quick exercises,seg_289,"here the constant is the number of ways we can assign 198 ones, 107 twos, . . . , 6 twelves, and 12 numbers larger than 12 to 486 nonsmokers. differentiating l(p) with respect to p yields that"
2314,1,"['function', 'loglikelihood function']", Solutions to the quick exercises,seg_289,21.3 the loglikelihood function l(λ) has derivative
2315,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", Solutions to the quick exercises,seg_289,one finds that ′(λ) = 0 if and only if λ = 1/x̄n and that this is a maximum. the maximum likelihood estimate for λ is therefore 1/x̄n.
2316,1,['experiment'], Exercises,seg_291,"21.1 consider the following situation. suppose we have two fair dice, d1 with 5 red sides and 1 white side and d2 with 1 red side and 5 white sides. we pick one of the dice randomly, and throw it repeatedly until red comes up for the first time. with the same die this experiment is repeated two more times. suppose the following happens:"
2317,1,['experiment'], Exercises,seg_291,first experiment: first red appears in 3rd throw second experiment: first red appears in 5th throw third experiment: first red appears in 4th throw.
2318,1,"['probabilities', 'probability']", Exercises,seg_291,"show that for die d1 this happens with probability 5.7424 · 10−8, and for die d2 the probability with which this happens is 8.9725 · 10−4. given these probabilities, which die do you think we picked?"
2319,1,"['data', 'experiment']", Exercises,seg_291,21.2 we throw an unfair coin repeatedly until heads comes up for the first time. we repeat this experiment three times (with the same coin) and obtain the following data:
2320,1,['experiment'], Exercises,seg_291,first experiment: heads first comes up in 3rd throw second experiment: heads first comes up in 5th throw third experiment: heads first comes up in 4th throw.
2321,1,"['maximum likelihood estimate', 'estimate', 'likelihood', 'maximum likelihood', 'probability']", Exercises,seg_291,let p be the probability that heads comes up in a throw with this coin. determine the maximum likelihood estimate p̂ of p.
2322,1,"['poisson', 'distribution', 'parameter', 'poisson distribution']", Exercises,seg_291,21.3 in exercise 17.4 we modeled the hits of london by flying bombs by a poisson distribution with parameter µ.
2323,1,"['maximum likelihood estimate', 'estimate', 'likelihood', 'data', 'maximum likelihood']", Exercises,seg_291,a. use the data from exercise 17.4 to find the maximum likelihood estimate
2324,1,['data'], Exercises,seg_291,b. suppose the summarized data from exercise 17.4 got corrupted in the
2325,0,[], Exercises,seg_291,number of squares 440 93 35 7 0 0 1
2326,1,"['maximum likelihood estimate', 'estimate', 'likelihood', 'data', 'maximum likelihood']", Exercises,seg_291,"using this new data, what is the maximum likelihood estimate of µ?"
2327,1,"['sample', 'random', 'estimate', 'dataset', 'distribution', 'probability', 'random sample', 'realization']", Exercises,seg_291,"21.4 in section 19.1, we considered the arrivals of packages at a network server, where we modeled the number of arrivals per minute by a pois(µ) distribution. let x1, x2, . . . , xn be a realization of a random sample from a pois(µ) distribution. we saw on page 286 that a natural estimate of the probability of zeros in the dataset is given by"
2328,0,[], Exercises,seg_291,number of xi equal to zero .
2329,1,['likelihood'], Exercises,seg_291,a. show that the likelihood l(µ) is given by
2330,0,[], Exercises,seg_291,b. determine the loglikelihood (µ) and the formula of the maximum likeli-
2331,1,"['maximum likelihood estimate', 'estimate', 'likelihood', 'maximum likelihood', 'probability']", Exercises,seg_291,hood estimate for µ. −µ c. what is the maximum likelihood estimate for the probability e of zero
2332,1,"['sample', 'dataset', 'random sample', 'distribution', 'normal', 'random', 'realization', 'normal distribution']", Exercises,seg_291,"21.5 suppose that x1, x2, . . . , xn is a dataset, which is a realization of a random sample from a normal distribution."
2333,1,"['distribution', 'probability', 'normal', 'normal distribution']", Exercises,seg_291,a. let the probability density of this normal distribution be given by
2334,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", Exercises,seg_291,determine the maximum likelihood estimate for µ.
2335,1,"['distribution', 'normal', 'normal distribution']", Exercises,seg_291,b. now suppose that the density of this normal distribution is given by
2336,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", Exercises,seg_291,determine the maximum likelihood estimate for σ.
2337,1,"['sample', 'random', 'dataset', 'distribution', 'probability', 'random sample', 'realization']", Exercises,seg_291,"21.6 let x1, x2, . . . , xn be a dataset that is a realization of a random sample from a distribution with probability density fδ(x) given by"
2338,1,['likelihood'], Exercises,seg_291,a. draw the likelihood l(δ).
2339,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", Exercises,seg_291,b. determine the maximum likelihood estimate for δ.
2340,1,"['sample', 'density function', 'probability density function', 'continuous distribution', 'continuous', 'dataset', 'distribution', 'probability', 'rayleigh distribution', 'function', 'realization']", Exercises,seg_291,"21.7 suppose that x1, x2, . . . , xn is a dataset, which is a realization of a random sample from a rayleigh distribution, which is a continuous distribution with probability density function given by"
2341,1,"['maximum likelihood estimate', 'estimate', 'likelihood', 'case', 'maximum likelihood']", Exercises,seg_291,in this case what is the maximum likelihood estimate for θ?
2342,1,['table'], Exercises,seg_291,"21.8 (exercises 19.7 and 20.7 continued) a certain type of plant can be divided into four types: starchy-green, starchy-white, sugary-green, and sugarywhite. the following table lists the counts of the various types among 3839 leaves."
2343,0,[], Exercises,seg_291,starchy-green 1997 sugary-white 32 starchy-white 906 sugary-green 904
2344,0,[], Exercises,seg_291,"⎧1 if the observed leave is of type starchy-green ⎪⎪⎪ 2 if the observed leave is of type sugary-white x = ⎪⎨3 if the observed leave is of type starchy-white ⎪ ⎩4 if the observed leave is of type sugary-green,"
2345,1,"['mass function', 'probability mass function', 'probability', 'function']", Exercises,seg_291,the probability mass function p of x is given by
2346,1,"['estimated', 'maximum likelihood estimate', 'estimate', 'likelihood', 'maximum likelihood', 'parameter']", Exercises,seg_291,"and p(a) = 0 for all other a. here 0 < θ < 1 is an unknown parameter, which was estimated in exercise 19.7. we want to find a maximum likelihood estimate of θ."
2347,1,"['likelihood', 'data']", Exercises,seg_291,a. use the data to find the likelihood l(θ) and the loglikelihood (θ).
2348,1,"['maximum likelihood estimate', 'estimate', 'likelihood', 'data', 'maximum likelihood']", Exercises,seg_291,b. what is the maximum likelihood estimate of θ using the data from the
2349,1,['table'], Exercises,seg_291,preceding table?
2350,0,['n'], Exercises,seg_291,c. suppose that we have the counts of n different leaves: n1 of type starchy-
2351,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", Exercises,seg_291,"green, n2 of type sugary-white, n3 of type starchy-white, and n4 of type sugary-green (so n = n1 + n2 + n3 + n4). determine the general formula for the maximum likelihood estimate of θ."
2352,1,"['sample', 'random', 'estimates', 'dataset', 'likelihood', 'distribution', 'maximum likelihood', 'random sample', 'realization', 'maximum likelihood estimates']", Exercises,seg_291,"21.9 let x1, x2, . . . , xn be a dataset that is a realization of a random sample from a u(α, β) distribution (with α and β unknown, α < β). determine the maximum likelihood estimates for α and β."
2353,1,"['sample', 'random', 'maximum likelihood estimate', 'estimate', 'dataset', 'likelihood', 'distribution', 'maximum likelihood', 'random sample', 'realization']", Exercises,seg_291,"21.10 let x1, x2, . . . , xn be a dataset, which is a realization of a random sample from a par(α) distribution. what is the maximum likelihood estimate for α?"
2354,1,"['random variables', 'variables', 'estimates', 'likelihood', 'maximum likelihood', 'random', 'maximum likelihood estimates']", Exercises,seg_291,"21.11 in exercise 4.13 we considered the situation where we have a box containing an unknown number—say n—of identical bolts. in order to get an idea of the size of n we introduced three random variables x , y , and z. here we will use x and y , and in the next exercise z, to find maximum likelihood estimates of n ."
2355,1,"['dataset', 'random', 'realization']", Exercises,seg_291,"a. suppose that x1, x2, . . . , xn is a dataset, which is a realization of a random"
2356,1,"['maximum likelihood estimate', 'estimate', 'likelihood', 'distribution', 'maximum likelihood']", Exercises,seg_291,sample from a geo(1/n) distribution. determine the maximum likelihood estimate for n .
2357,1,"['dataset', 'random', 'realization']", Exercises,seg_291,"b. suppose that y1, y2, . . . , yn is a dataset, which is a realization of a random"
2358,1,"['maximum likelihood estimate', 'estimate', 'uniform distribution', 'likelihood', 'discrete', 'distribution', 'maximum likelihood', 'discrete uniform distribution']", Exercises,seg_291,"sample from a discrete uniform distribution on 1, 2, . . . , n . determine the maximum likelihood estimate for n ."
2359,1,"['sample', 'parameters', 'likelihood', 'distribution', 'hypergeometric', 'hypergeometric distribution']", Exercises,seg_291,"21.12 (exercise 21.11 continued.) suppose that m bolts in the box were marked and then r bolts were selected from the box; z is the number of marked bolts in the sample. (recall that it was shown in exercise 4.13 c that z has a hypergeometric distribution, with parameters m, n , and r.) suppose that k bolts in the sample were marked. show that the likelihood l(n) is given by"
2360,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", Exercises,seg_291,"l(n) = . (n r ) next show that l(n) increases for n < mr/k and decreases for n > mr/k, and conclude that mr/k is the maximum likelihood estimate for n ."
2361,1,"['poisson', 'model', 'rate', 'poisson process', 'process']", Exercises,seg_291,"21.13 often one can model the times that customers arrive at a shop rather well by a poisson process with (unknown) rate λ (customers/hour). on a certain day, one of the attendants noticed that between noon and 12.45 p.m."
2362,1,"['maximum likelihood estimate', 'estimate', 'observations', 'likelihood', 'maximum likelihood']", Exercises,seg_291,"two customers arrived, and another attendant noticed that on the same day one customer arrived between 12.15 and 1 p.m. use the observations of the attendants to determine the maximum likelihood estimate of λ."
2363,1,"['maximum likelihood estimate', 'estimate', 'likelihood', 'maximum likelihood', 'random']", Exercises,seg_291,"21.14 a very inexperienced archer shoots n times an arrow at a disc of (unknown) radius θ. the disc is hit every time, but at completely random places. let r1, r2, . . . , rn be the distances of the various hits to the center of the disc. determine the maximum likelihood estimate for θ."
2364,1,"['failure', 'space shuttle challenger', 'probability of failure', 'probability']", Exercises,seg_291,"21.15 on january 28, 1986, the main fuel tank of the space shuttle challenger exploded shortly after takeoff. essential in this accident was the leakage of some of the six o-rings of the challenger. in section 1.4 the probability of failure of an o-ring is given by"
2365,1,"['failures', 'table', 'likelihood', 'data']", Exercises,seg_291,"where t is the temperature at launch in degrees fahrenheit. in table 21.2 the temperature t (in ◦f, rounded to the nearest integer) and the number of failures n for 23 missions are given, ordered according to increasing temperatures. (see also figure 1.3, where these data are graphically depicted.) give the likelihood l(a, b) and the loglikelihood (a, b)."
2366,1,"['statistics', 'probability theory', 'random variable', 'variable', 'probability', 'random', 'realization']", Exercises,seg_291,"21.16 in the 18th century georges-louis leclerc, comte de buffon (1707– 1788) found an amusing way to approximate the number π using probability theory and statistics. buffon had the following idea: take a needle and a large sheet of paper, and draw horizontal lines that are a needle-length apart. throw the needle a number of times (say n times) on the sheet, and count how often it hits one of the horizontal lines. say this number is sn, then sn is the realization of a bin(n, p) distributed random variable sn. here p is the probability that the needle hits one of the horizontal lines. in exercise 9.20 you found that"
2367,1,"['maximum likelihood estimator', 'maximum likelihood', 'estimator', 'likelihood']", Exercises,seg_291,is the maximum likelihood estimator for π.
2368,1,"['simple linear regression', 'estimates', 'case', 'normally distributed', 'method of least squares', 'linear', 'estimate', 'linear regression model', 'model', 'regression model', 'parameters', 'regression', 'distribution', 'linear regression', 'response', 'maximum likelihood estimates', 'estimated', 'method', 'likelihood', 'least squares', 'errors', 'variable', 'maximum likelihood', 'response variable']", The method of least squares,seg_293,"the maximum likelihood principle provides a way to estimate parameters. the applicability of the method is quite general but not universal. for example, in the simple linear regression model, introduced in section 17.4, we need to know the distribution of the response variable in order to find the maximum likelihood estimates for the parameters involved. in this chapter we will see how these parameters can be estimated using the method of least squares. furthermore, the relation between least squares and maximum likelihood will be investigated in the case of normally distributed errors."
2369,1,"['simple linear regression', 'model', 'linear', 'random variables', 'regression model', 'dataset', 'variables', 'bivariate dataset', 'regression', 'linear regression', 'random', 'bivariate', 'linear regression model']", Least squares estimation and regression,seg_295,"recall from section 17.4 the simple linear regression model for a bivariate dataset (x1, y1), (x2, y2), . . . , (xn, yn). in this model x1, x2, . . . , xn are nonrandom and y1, y2, . . . , yn are realizations of random variables y1, y2, . . . , yn satisfying"
2370,1,"['model', 'random variables', 'parameters', 'independent', 'variables', 'estimates', 'independent random variables', 'expectation', 'random', 'variance']", Least squares estimation and regression,seg_295,"where u1, u2, . . . , un are independent random variables with zero expectation and variance σ2. how can one obtain estimates for the parameters α, β, and σ2 in this model?"
2371,1,"['method', 'estimates', 'parameters', 'sum of squared', 'likelihood', 'data', 'regression', 'distribution', 'sum of squares', 'maximum likelihood', 'least squares', 'method of least squares', 'regression line', 'maximum likelihood estimates']", Least squares estimation and regression,seg_295,"note that we cannot find maximum likelihood estimates for these parameters, simply because we have no further knowledge about the distribution of the ui (and consequently of the yi). we want to choose α and β in such a way that we obtain a line that fits the data best. a classical approach to do this is to consider the sum of squared distances between the observed values yi and the values α+βxi on the regression line y = α+βx. see figure 22.1, where these distances are indicated. the method of least squares prescribes to choose α and β such that the sum of squares"
2372,1,"['sum is the squared', 'estimates', 'least squares', 'set', 'least squares estimates']", Least squares estimation and regression,seg_295,"is minimal. the ith term in the sum is the squared distance in the vertical direction from (xi, yi) to the line y = α + βx. to find these so-called least squares estimates , we differentiate s(α, β) with respect to α and β, and we set the derivatives equal to 0:"
2373,0,[], Least squares estimation and regression,seg_295,this is equivalent to
2374,1,"['data', 'table']", Least squares estimation and regression,seg_295,"for example, for the timber data from table 15.5 we would obtain"
2375,1,"['estimated', 'dataset', 'scatterplot', 'regression', 'estimated regression line', 'regression line']", Least squares estimation and regression,seg_295,"these are two equations with two unknowns α and β. solving for α and β yields the solutions α̂ = −1160.5 and β̂ = 57.51. in figure 22.2 a scatterplot of the timber dataset, together with the estimated regression line y = −1160.5+ 57.51x, is depicted."
2376,1,['estimate'], Least squares estimation and regression,seg_295,quick exercise 22.1 suppose you are given a piece of australian timber with density 65. what would you choose as an estimate for the janka hardness?
2377,0,['n'], Least squares estimation and regression,seg_295,"n =1, we find the following formulas for the"
2378,1,"['intercept', 'slope']", Least squares estimation and regression,seg_295,estimates α̂ (the intercept) and β̂ (the slope):
2379,1,"['unbiased', 'estimators']", Least squares estimation and regression,seg_295,least squares estimators are unbiased
2380,1,"['estimates', 'least squares', 'estimators', 'least squares estimators', 'least squares estimates']", Least squares estimation and regression,seg_295,we denote the least squares estimates by α̂ and β̂. it is quite common to also denote the least squares estimators by α̂ and β̂:
2381,1,"['unbiased estimator', 'estimator', 'unbiased']", Least squares estimation and regression,seg_295,"in exercise 22.12 it is shown that β̂ is an unbiased estimator for β. using this and the fact that e[yi] = α + βxi (see page 258), we find for α̂:"
2382,1,"['unbiased estimator', 'estimator', 'unbiased']", Least squares estimation and regression,seg_295,we see that α̂ is an unbiased estimator for α.
2383,1,"['unbiased estimator', 'estimator', 'unbiased']", Least squares estimation and regression,seg_295,an unbiased estimator for σ2
2384,1,"['simple linear regression', 'model', 'linear', 'random variables', 'independent', 'regression model', 'variables', 'regression', 'linear regression', 'random', 'variance', 'linear regression model']", Least squares estimation and regression,seg_295,"in the simple linear regression model the assumptions imply that the random variables yi are independent with variance σ2. unfortunately, one cannot ap-"
2385,1,['estimator'], Least squares estimation and regression,seg_295,ply the usual estimator (1/(n − 1))∑i
2386,1,"['expectations', 'variance', 'estimator']", Least squares estimation and regression,seg_295,"n =1 (yi − ȳi)2 for the variance of the yi (see section 19.4), because different yi have different expectations. what would be a reasonable estimator for σ2? the following quick exercise suggests a candidate."
2387,1,"['random variables', 'independent', 'variables', 'expected value', 'independent random variables', 'random', 'variance']", Least squares estimation and regression,seg_295,"quick exercise 22.3 let u1, u2, . . . , un be independent random variables, each with expected value zero and variance σ2. show that"
2388,1,"['unbiased estimator', 'estimator', 'unbiased']", Least squares estimation and regression,seg_295,is an unbiased estimator for σ2.
2389,1,"['unbiased estimator', 'estimator', 'unbiased', 'estimate']", Least squares estimation and regression,seg_295,"at first sight one might be tempted to think that the unbiased estimator t from this quick exercise is a useful tool to estimate σ2. unfortunately, we only observe the xi and yi, not the ui. however, from the fact that ui = yi−α−βxi, it seems reasonable to try"
2390,1,"['expected value', 'random variable', 'variable', 'random', 'estimator']", Least squares estimation and regression,seg_295,as an estimator for σ2. tedious calculations show that the expected value of this random variable equals n−2σ2. but then we can easily turn it into an
2391,1,"['unbiased estimator', 'estimator', 'unbiased']", Least squares estimation and regression,seg_295,n unbiased estimator for σ2.
2392,1,"['simple linear regression', 'linear', 'model', 'unbiased estimator', 'regression model', 'regression', 'random variable', 'variable', 'linear regression', 'random', 'estimator', 'unbiased', 'linear regression model']", Least squares estimation and regression,seg_295,an unbiased estimator for σ2. in the simple linear regression model the random variable
2393,1,"['unbiased estimator', 'estimator', 'unbiased']", Least squares estimation and regression,seg_295,is an unbiased estimator for σ2.
2394,1,"['simple linear regression', 'residuals', 'linear', 'dataset', 'bivariate dataset', 'scatterplot', 'linear regression model', 'model', 'regression model', 'regression', 'linear regression', 'regression line', 'estimated', 'residual', 'bivariate', 'estimated regression line']", Residuals,seg_297,a way to explore whether the simple linear regression model is appropriate to model a given bivariate dataset is to inspect a scatterplot of the so-called residuals ri against the xi. the ith residual ri is defined as the vertical distance between the ith point and the estimated regression line:
2395,1,"['linear', 'model', 'linear model', 'residuals', 'scatterplot', 'case', 'data', 'random']", Residuals,seg_297,"when a linear model is appropriate, the scatterplot of the residuals ri against the xi should show truly random fluctuations around zero, in the sense that it should not exhibit any trend or pattern. this seems to be the case in figure 22.3, which shows the residuals for the black cherry tree data from exercise 17.9."
2396,1,['residuals'], Residuals,seg_297,"n =1 ri = 0, i.e., that the sum of the residuals is zero."
2397,1,"['case', 'dataset']", Residuals,seg_297,in figure 22.4 we depicted ri versus xi for the timber dataset. in this case a slight parabolic pattern can be observed. figures 22.2 and 22.4 suggest that
2398,1,"['model', 'dataset']", Residuals,seg_297,for the timber dataset a better model might be
2399,1,"['model', 'residuals']", Residuals,seg_297,in this new model the residuals are
2400,1,"['estimates', 'least squares', 'least squares estimates']", Residuals,seg_297,"where α̂, β̂, and γ̂ are the least squares estimates obtained by minimizing"
2401,1,"['residuals', 'heteroscedasticity']", Residuals,seg_297,"in figure 22.5 we depicted ri versus xi. the residuals display no trend or pattern, except that they “fan out”—an example of a phenomenon called heteroscedasticity."
2402,1,"['heteroscedasticity', 'case', 'data', 'transformations', 'expected value', 'least squares', 'homoscedasticity', 'variance']", Residuals,seg_297,"the assumption of equal variance of the ui (and therefore of the yi) is called homoscedasticity. in case the variance of yi depends on the value of xi, we speak of heteroscedasticity. for instance, heteroscedasticity occurs when yi with a large expected value have a larger variance than those with small expected values. this produces a “fanning out” effect, which can be observed in figure 22.5. this figure strongly suggests that the timber data are heteroscedastic. possible ways out of this problem are a technique called weighted least squares or the use of variance-stabilizing transformations."
2403,1,"['independent', 'method', 'estimates', 'likelihood', 'case', 'least squares', 'distribution', 'maximum likelihood', 'method of least squares', 'maximum likelihood estimates']", Relation with maximum likelihood,seg_299,"to apply the method of least squares no assumption is needed about the type of distribution of the ui. in case the type of distribution of the ui is known, the maximum likelihood principle can be applied. consider, for instance, the classical situation where the ui are independent with an n(0, σ2) distribution. what are the maximum likelihood estimates for α and β?"
2404,1,"['histogram', 'residuals', 'case', 'normal distribution', 'random sample', 'random', 'sample', 'linear', 'dataset', 'bivariate dataset', 'data', 'model', 'linear model', 'distribution', 'independent', 'normal', 'bivariate', 'realization']", Relation with maximum likelihood,seg_299,"in this case the yi are independent, and yi has an n(α+βxi, σ2) distribution. under these assumptions and assuming that the linear model is appropriate to model a given bivariate dataset, the ri should look like the realization of a random sample from a normal distribution. as an example a histogram of the residuals ri of the cherry tree data of exercise 17.9 is depicted in figure 22.6."
2405,1,"['normality', 'data']", Relation with maximum likelihood,seg_299,the data do not exhibit strong evidence against the assumption of normality.
2406,1,"['distribution', 'probability']", Relation with maximum likelihood,seg_299,"when yi has an n(α + βxi, σ2) distribution, the probability density of yi is given by"
2407,0,[], Relation with maximum likelihood,seg_299,the loglikelihood is:
2408,1,"['independent', 'method', 'likelihood', 'case', 'least squares', 'estimators', 'distribution', 'least squares method', 'maximum likelihood']", Relation with maximum likelihood,seg_299,"n =1(yi − α − βxi)2 is minimal. hence, in case the ui are independent with an n(0, σ2) distribution, the maximum likelihood principle and the least squares method yield the same estimators."
2409,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", Relation with maximum likelihood,seg_299,"to find the maximum likelihood estimate of σ we differentiate (α, β, σ) with respect to σ:"
2410,1,"['invariance principle', 'likelihood', 'invariance', 'maximum likelihood', 'maximum likelihood estimator', 'estimator']", Relation with maximum likelihood,seg_299,it follows (from the invariance principle on page 321) that the maximum likelihood estimator of σ2 is given by
2411,1,['estimator'], Relation with maximum likelihood,seg_299,which is the estimator from (22.3).
2412,1,"['estimated', 'prediction', 'regression', 'estimated regression line', 'regression line']", Solutions to the quick exercises,seg_301,22.1 we can use the estimated regression line y = −1160.5+57.51x to predict the janka hardness. for density x = 65 we find as a prediction for the janka hardness y = 2577.65.
2413,1,"['estimated', 'regression', 'estimated regression line', 'regression line']", Solutions to the quick exercises,seg_301,"22.2 rewriting α̂ = ȳn − β̂, it follows that ȳn = α̂ + β̂x̄n, which means that (x̄n, ȳn) is a point on the estimated regression line y = α̂ + β̂x."
2414,1,['residuals'], Solutions to the quick exercises,seg_301,"22.4 since ri = yi − (α̂ + β̂xi) for i = 1, 2, . . . , n, it follows that the sum of the residuals equals"
2415,1,"['dataset', 'bivariate dataset', 'bivariate']", Exercises,seg_303,22.1 consider the following bivariate dataset:
2416,1,"['parameters', 'estimates', 'least squares', 'least squares estimates']", Exercises,seg_303,a. determine the least squares estimates α̂ and β̂ of the parameters of the
2417,1,['residuals'], Exercises,seg_303,"b. determine the residuals r1, r2, and r3 and check that they add up to 0."
2418,1,"['estimated', 'scatterplot', 'data', 'regression']", Exercises,seg_303,c. draw in one figure the scatterplot of the data and the estimated regression
2419,1,"['estimates', 'dataset']", Exercises,seg_303,22.2 adding one point may dramatically change the estimates of α and β. suppose one extra datapoint is added to the dataset of the previous exercise and that we have as dataset:
2420,1,"['leverage point', 'estimate', 'estimates', 'least squares', 'least squares estimate', 'leverage']", Exercises,seg_303,"determine the least squares estimate of β̂. a point such as (0, 0), which dramatically changes the estimates for α and β, is called a leverage point."
2421,1,"['dataset', 'bivariate dataset', 'bivariate']", Exercises,seg_303,22.3 suppose we have the following bivariate dataset:
2422,1,"['parameters', 'estimates', 'least squares', 'least squares estimates']", Exercises,seg_303,a. determine the least squares estimates α̂ and β̂ of the parameters of the
2423,1,"['estimated', 'scatterplot', 'data', 'regression']", Exercises,seg_303,b. draw in one figure the scatterplot of the data and the estimated regression
2424,1,"['dataset', 'bivariate dataset', 'bivariate']", Exercises,seg_303,"22.4 we are given a bivariate dataset (x1, y1), (x2, y2), . . . , (x100, y100). for this bivariate dataset it is known that ∑ xi = 231.7, ∑x2i = 2400.8, ∑ yi ="
2425,1,"['parameters', 'estimates', 'least squares', 'regression', 'least squares estimates', 'regression line']", Exercises,seg_303,"321, and ∑xiyi = 5189. what are the least squares estimates α̂ and β̂ of the parameters of the regression line y = α + βx?"
2426,1,"['model', 'intercept', 'dataset']", Exercises,seg_303,22.5 for the timber dataset it seems reasonable to leave out the intercept α (“no hardness without density”). the model then becomes
2427,1,"['estimator', 'least squares']", Exercises,seg_303,show that the least squares estimator β̂ of β is now given by
2428,1,['sum of squares'], Exercises,seg_303,by minimizing the appropriate sum of squares.
2429,1,"['model', 'regression model', 'estimate', 'intercept', 'regression']", Exercises,seg_303,"22.6 (quick exercise 22.1 and exercise 22.5 continued). suppose we are given a piece of australian timber with density 65. what would you choose as an estimate for the janka hardness, based on the regression model with no intercept? recall that ∑ xiyi = 2790525 and ∑x2i = 81750.02 (see also section 22.1)."
2430,1,['dataset'], Exercises,seg_303,22.7 consider the dataset
2431,1,['variables'], Exercises,seg_303,"where x1, x2, . . . , xn are nonrandom and y1, y2, . . . , yn are realizations of random variables y1, y2, . . . , yn, satisfying"
2432,1,"['model', 'random variables', 'parameters', 'independent', 'variables', 'estimates', 'least squares estimates', 'least squares', 'independent random variables', 'expectation', 'random', 'variance']", Exercises,seg_303,"here u1, u2, . . . , un are independent random variables with zero expectation and variance σ2. what are the least squares estimates for the parameters α and β in this model?"
2433,1,"['model', 'regression model', 'residual', 'regression', 'sum of squares', 'residual sum of squares']", Exercises,seg_303,22.8 which simple regression model has the larger residual sum of squares
2434,1,"['model', 'intercept']", Exercises,seg_303,"n =1 ri2, the model with intercept or the one without?"
2435,1,"['model', 'random variable', 'variable', 'random', 'slope']", Exercises,seg_303,"22.9 for some datasets it seems reasonable to leave out the slope β. for example, in the jury example from section 6.3 it was assumed that the score that juror i assigns when the performance deserves a score g is yi = g + zi, where zi is a random variable with values around zero. in general, when the slope β is left out, the model becomes"
2436,1,"['estimator', 'least squares']", Exercises,seg_303,show that ȳn is the least squares estimator α̂ of α.
2437,1,"['sum is the squared', 'method', 'dataset', 'bivariate dataset', 'sum of squared', 'residuals', 'regression', 'least squares', 'bivariate', 'method of least squares', 'regression line']", Exercises,seg_303,"22.10 in the method of least squares we choose α and β in such a way that the sum of squared residuals s(α, β) is minimal. since the ith term in this sum is the squared vertical distance from (xi, yi) to the regression line y = α + βx, one might also wonder whether it is a good idea to replace this squared distance simply by the distance. so, given a bivariate dataset"
2438,0,[], Exercises,seg_303,choose α and β in such a way that the sum
2439,1,"['dataset', 'bivariate dataset', 'bivariate']", Exercises,seg_303,is minimal. we will investigate this by a simple example. consider the following bivariate dataset:
2440,1,"['estimates', 'least squares', 'least squares estimates']", Exercises,seg_303,"a. determine the least squares estimates α̂ and β̂, and draw in one figure"
2441,1,"['estimated', 'scatterplot', 'data', 'regression', 'estimated regression line', 'regression line']", Exercises,seg_303,"the scatterplot of the data and the estimated regression line y = α̂ + β̂x. finally, determine a(α̂, β̂)."
2442,1,"['random variables', 'dataset', 'variables', 'random']", Exercises,seg_303,"22.11 consider the dataset (x1, y1), (x2, y2), . . . , (xn, yn), where the xi are nonrandom and the yi are realizations of random variables y1, y2, . . . , yn satisfying"
2443,1,"['random variables', 'independent', 'dataset', 'variables', 'scatterplot', 'independent random variables', 'random', 'variance']", Exercises,seg_303,"where u1, u2, . . . , un are independent random variables with zero expectation and variance σ2. visual inspection of the scatterplot of our dataset in"
2444,1,['model'], Exercises,seg_303,figure 22.7 suggests that we should model the yi by
2445,1,"['least squares estimators', 'least squares', 'estimators']", Exercises,seg_303,a. show that the least squares estimators β̂ and γ̂ satisfy
2446,1,"['linear', 'estimators']", Exercises,seg_303,"b. infer from a—for instance, by using linear algebra—that the estimators"
2447,1,"['unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_303,22.12 the least square estimator β̂ from (22.1) is an unbiased estimator for β. you can show this in four steps.
2448,0,[], Exercises,seg_303,a. first show that
2449,0,[], Exercises,seg_303,c. simplify this last expression to find
2450,1,"['unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_303,"d. finally, conclude that β̂ is an unbiased estimator for β."
2451,1,"['model', 'level', 'confidence intervals', 'estimate', 'interval', 'range', 'confident', 'data', 'intervals', 'distribution', 'expected value', 'mean', 'parameter', 'confidence']", Confidence intervals for the mean,seg_305,"sometimes, a range of plausible values for an unknown parameter is preferred to a single estimate. we shall discuss how to turn data into what are called confidence intervals and show that this can be done in such a manner that definite statements can be made about how confident we are that the true parameter value is in the reported interval. this level of confidence is something you can choose. we start this chapter with the general principle of confidence intervals. we continue with confidence intervals for the mean, the common way to refer to confidence intervals made for the expected value of the model distribution. depending on the situation, one of the four methods presented will apply."
2452,1,"['mse', 'statistics', 'estimators', 'measurements', 'sample statistics', 'estimator', 'sample', 'bias', 'sample mean', 'experiment', 'estimate', 'results', 'data', 'mean', 'parameter', 'sample variance', 'confidence', 'distribution', 'variance', 'point estimate', 'realization']", General principle,seg_307,"in previous chapters we have encountered sample statistics as estimators for distribution features. this started somewhat informally in chapter 17, where it was claimed, for example, that the sample mean and the sample variance are usually close to µ and σ2 of the underlying distribution. bias and mse of estimators, discussed in chapters 19 and 20, are used to judge the quality of estimators. if we have at our disposal an estimator t for an unknown parameter θ, we use its realization t as our estimate for θ. for example, when collecting data on the speed of light, as michelson did (see section 13.1), the unknown speed of light would be the parameter θ, our estimator t could be the sample mean, and michelson’s data then yield an estimate t for θ of 299 852.4 km/sec. we call this number a point estimate: if we are required to select one number, this is it. had the measurements started a day earlier, however, the whole experiment would in essence be the same, but the results might have been different. hence, we cannot say that the estimate equals the speed of light but rather that it is close to the true speed of light. for example, we could say something like: “we have great confidence that the true speed of"
2453,1,"['confident', 'interval']", General principle,seg_307,light is somewhere between . . . and . . . .” in addition to providing an interval of plausible values for θ we would want to add a specific statement about how confident we are that the true θ is among them.
2454,1,"['estimators', 'estimator', 'inequality', 'moment', 'standard', 'standard deviation', 'confidence', 'sampling distributions', 'distributions', 'parameters', 'unbiased', 'deviation', 'confidence statements', 'sampling']", General principle,seg_307,"in this chapter we shall present methods to make confidence statements about unknown parameters, based on knowledge of the sampling distributions of corresponding estimators. to illustrate the main idea, suppose the estimator t is unbiased for the speed of light θ. for the moment, also suppose that t has standard deviation σt = 100 km/sec (we shall drop this unrealistic assumption shortly). then, applying formula (13.1), which was derived from chebyshev’s inequality (see section 13.2), we find"
2455,1,"['estimator', 'probability']", General principle,seg_307,"in words this reads: with probability at least 75%, the estimator t is within 2σt = 200 of the true speed of light θ. we could rephrase this as"
2456,1,['probability'], General principle,seg_307,"t ∈ (θ − 200, θ + 200) with probability at least 75%."
2457,0,[], General principle,seg_307,"however, if i am near the city of paris, then the city of paris is near me: the statement “t is within 200 of θ” is the same as “θ is within 200 of t ,” and we could equally well rephrase (23.1) as"
2458,1,['probability'], General principle,seg_307,"θ ∈ (t − 200, t + 200) with probability at least 75%."
2459,1,"['interval', 'estimate', 'random variable', 'variable', 'interval estimator', 'random interval', 'interval estimate', 'probability', 'random', 'estimator', 'realization']", General principle,seg_307,"note that of the last two equations the first is a statement about a random variable t being in a fixed interval, whereas in the second equation the interval is random and the statement is about the probability that the random interval covers the fixed but unknown θ. the interval (t − 200, t + 200) is sometimes called an interval estimator, and its realization is an interval estimate."
2460,1,"['data', 'realization']", General principle,seg_307,"evaluating t for the michelson data we find as its realization t = 299 852.4, and this yields the statement"
2461,1,"['interval', 'random variable', 'variable', 'probability', 'random', 'realization']", General principle,seg_307,"because we substituted the realization for the random variable, we cannot claim that (23.2) holds with probability at least 75%: either the true speed of light θ belongs to the interval or it does not; the statement we make is either true or false, we just do not know which. however, because the procedure guarantees a probability of at least 75% of getting a “right” statement, we say:"
2462,1,['confidence'], General principle,seg_307,"θ ∈ (299 652.4, 300 052.4) with confidence at least 75%. (23.3)"
2463,1,"['confidence interval', 'deviation', 'interval', 'information', 'sampling', 'distribution', 'standard', 'standard deviation', 'confidence', 'estimator', 'sampling distribution', 'unbiased']", General principle,seg_307,"the construction of this confidence interval only involved an unbiased estimator and knowledge of its standard deviation. when more information on the sampling distribution of the estimator is available, more refined statements can be made, as we shall see shortly."
2464,1,"['confidence', 'interval', 'confidence interval', 'inequality']", General principle,seg_307,"quick exercise 23.1 repeat the preceding derivation, starting from the statement p(|t − θ| < 3σt ) ≥ 8/9 (check that this follows from chebyshev’s inequality). what is the resulting confidence interval for the speed of light, and what is the corresponding confidence?"
2465,1,"['confidence intervals', 'intervals', 'confidence']", General principle,seg_307,many confidence intervals are of the form1
2466,1,"['confidence intervals', 'intervals', 'confidence']", General principle,seg_307,"we just encountered, where c is a number near 2 or 3. the corresponding confidence is often much higher than in the preceding example. because there are many other ways confidence intervals can (or have to) be constructed, the general definition looks a bit different."
2467,1,"['statistics', 'sample statistics', 'random', 'sample', 'dataset', 'intervals', 'parameter', 'parameter of interest', 'random variables', 'variables', 'realization']", General principle,seg_307,"confidence intervals. suppose a dataset x1, . . . , xn is given, modeled as realization of random variables x1, . . . , xn. let θ be the parameter of interest, and γ a number between 0 and 1. if there exist sample statistics ln = g(x1, . . . , xn) and un = h(x1, . . . , xn) such that p(ln < θ < un) = γ"
2468,1,"['interval', 'level', 'confidence', 'confidence level', 'confidence interval']", General principle,seg_307,"where ln = g(x1, . . . , xn) and un = h(x1, . . . , xn), is called a 100γ% confidence interval for θ. the number γ is called the confidence level."
2469,1,"['sample', 'statistics', 'sample statistics']", General principle,seg_307,"sometimes sample statistics ln and un as required in the definition do not exist, but one can find ln and un that satisfy"
2470,1,"['interval', 'level', 'confidence', 'confidence level', 'confidence interval']", General principle,seg_307,"the resulting confidence interval (ln, un) is called a conservative 100γ% confidence interval for θ: the actual confidence level might be higher. for example, the interval in (23.2) is a conservative 75% confidence interval."
2471,1,['interval'], General principle,seg_307,quick exercise 23.2 why is the interval in (23.2) a conservative 75% confidence interval?
2472,1,"['simulation', 'interval', 'probability', 'confidence', 'confidence interval']", General principle,seg_307,"there is no way of knowing whether an individual confidence interval is correct, in the sense that it indeed does cover θ. the procedure guarantees that each time we make a confidence interval we have probability γ of covering θ. what this means in practice can easily be illustrated with an example, using simulation:"
2473,1,"['confidence interval', 'method', 'interval', 'data', 'distribution', 'expectation', 'normal', 'confidence', 'normal distribution']", General principle,seg_307,"generate x1, . . . , x20 from an n(0, 1) distribution. next, pretend that it is known that the data are from a normal distribution but that both µ and σ are unknown. construct the 90% confidence interval for the expectation µ using the method described in the next section, which says to use (ln, un) with"
2474,1,"['deviation', 'sample', 'sample mean', 'interval', 'case', 'mean', 'standard', 'standard deviation', 'confidence', 'confidence interval']", General principle,seg_307,"where x̄20 and s20 are the sample mean and standard deviation. finally, check whether the “true µ,” in this case 0, is in the confidence interval."
2475,1,"['interval', 'confidence intervals', 'data', 'intervals', 'set', 'confidence', 'confidence interval']", General principle,seg_307,"we repeated the whole procedure 50 times, making 50 confidence intervals for µ. each confidence interval is based on a fresh independently generated set of data. the 50 intervals are plotted in figure 23.1 as horizontal line"
2476,1,['intervals'], General principle,seg_307,"segments, and at µ (0!) a vertical line is drawn. we count 46 “hits”: only four intervals do not contain the true µ."
2477,1,"['confidence intervals', 'intervals', 'level', 'confidence', 'confidence level']", General principle,seg_307,quick exercise 23.3 suppose you were to make 40 confidence intervals with confidence level 95%. about how many of them should you expect to be “wrong”? should you be surprised if 10 of them are wrong?
2478,1,"['sample', 'confidence intervals', 'bootstrap', 'data', 'intervals', 'distribution', 'samples', 'normal', 'expectation', 'mean', 'central limit theorem', 'confidence', 'variance', 'limit', 'normal distribution']", General principle,seg_307,"in the remainder of this chapter we consider confidence intervals for the mean: confidence intervals for the unknown expectation µ of the distribution from which the sample originates. we start with the situation where it is known that the data originate from a normal distribution, first with known variance, then with unknown variance. then we drop the normal assumption, first use the bootstrap, and finally show how, for very large samples, confidence intervals based on the central limit theorem are made."
2479,1,"['sample', 'confidence intervals', 'data', 'intervals', 'distribution', 'parameter of interest', 'parameter', 'confidence', 'variance', 'realization']", Normal data,seg_309,"suppose the data can be seen as the realization of a sample x1, . . . , xn from an n(µ, σ2) distribution and µ is the (unknown) parameter of interest. if the variance σ2 is known, confidence intervals are easily derived. before we do this, some preparation has to be done."
2480,1,"['critical values', 'tail probability', 'standard normal', 'right tail probability', 'distribution', 'normal', 'probability', 'standard', 'standard normal distribution', 'tail', 'critical value', 'normal distribution']", Normal data,seg_309,"we shall need so-called critical values for the standard normal distribution. the critical value zp of an n(0, 1) distribution is the number that has right tail probability p. it is defined by"
2481,1,"['quantile', 'random', 'table', 'standard normal', 'distribution', 'random variable', 'variable', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Normal data,seg_309,"where z is an n(0, 1) random variable. for example, from table b.1 we read p(z ≥ 1.96) = 0.025, so z0.025 = 1.96. in fact, zp is the (1 − p)th quantile of the standard normal distribution:"
2482,1,"['standard normal', 'symmetry', 'normal', 'standard']", Normal data,seg_309,"by the symmetry of the standard normal density, p(z ≤ −zp) = p(z ≥ zp) = p, so p(z ≥ −zp) = 1 − p and therefore"
2483,1,['table'], Normal data,seg_309,quick exercise 23.4 determine z0.01 and z0.95 from table b.1.
2484,1,"['sample', 'random sample', 'distribution', 'normal', 'random', 'normal distribution']", Normal data,seg_309,"if x1, . . . , xn is a random sample from an n(µ, σ2) distribution, then x̄n has an n(µ, σ2/n) distribution, and from the properties of the normal distribution (see page 106), we know that"
2485,1,['distribution'], Normal data,seg_309,"x̄n − µ has an n(0, 1) distribution. σ/√n"
2486,1,"['random variable', 'variable', 'random']", Normal data,seg_309,"if cl and cu are chosen such that p(cl < z < cu) = γ for an n(0, 1) distributed random variable z, then"
2487,0,[], Normal data,seg_309,we have found that
2488,1,"['interval', 'confidence', 'probability', 'confidence interval']", Normal data,seg_309,"satisfy the confidence interval definition: the interval (ln, un) covers µ with probability γ. therefore"
2489,1,"['interval', 'confidence', 'confidence interval']", Normal data,seg_309,"is a 100γ% confidence interval for µ. a common choice is to divide α = 1− γ evenly between the tails,2 that is, solve cl and cu from"
2490,1,"['confidence', 'interval', 'confidence interval']", Normal data,seg_309,"so that cu = zα/2 and cl = z1−α/2 = −zα/2. summarizing, the 100(1 − α)% confidence interval for µ is:"
2491,1,"['confidence', 'interval', 'confidence interval']", Normal data,seg_309,"for example, if α = 0.05, we use z0.025 = 1.96 and the 95% confidence interval"
2492,0,[], Normal data,seg_309,example: gross calorific content of coal
2493,1,"['deviation', 'measurement', 'method', 'table', 'errors', 'standardization', 'normal', 'measurements', 'standard', 'standard deviation', 'numerical']", Normal data,seg_309,"when a shipment of coal is traded, a number of its properties should be known accurately, because the value of the shipment is determined by them. an important example is the so-called gross calorific value, which characterizes the heat content and is a numerical value in megajoules per kilogram (mj/kg). the international organization of standardization (iso) issues standard procedures for the determination of these properties. for the gross calorific value, there is a method known as iso 1928. when the procedure is carried out properly, resulting measurement errors are known to be approximately normal, with a standard deviation of about 0.1 mj/kg. laboratories that operate according to standard procedures receive iso certificates. in table 23.1, a number of such iso 1928 measurements is given for a shipment of osterfeld coal coded 262de27."
2494,1,"['confidence interval', 'confidence', 'interval', 'data']", Normal data,seg_309,"we want to combine these values into a confidence statement about the “true” gross calorific content of osterfeld 262de27. from the data, we compute x̄n = 23.788. using the given σ = 0.1 and α = 0.05, we find the 95% confidence interval"
2495,0,[], Normal data,seg_309,"when σ is unknown, the fact that"
2496,1,"['confidence interval', 'random', 'interval', 'standard normal', 'distribution', 'random variable', 'variable', 'normal', 'standard', 'standard normal distribution', 'confidence', 'estimator', 'normal distribution']", Normal data,seg_309,"has a standard normal distribution has become useless, as it involves this unknown σ, which would subsequently appear in the confidence interval. however, if we substitute the estimator sn for σ, the resulting random variable"
2497,1,['distribution'], Normal data,seg_309,"has a distribution that only depends on n and not on µ or σ. moreover, its density can be given explicitly."
2498,1,"['continuous random variable', 'random variable', 'variable', 'probability', 'random', 'continuous', 'parameter']", Normal data,seg_309,"definition. a continuous random variable has a t-distribution with parameter m, where m ≥ 1 is an integer, if its probability density is given by"
2499,1,"['distribution', 'degrees of freedom']", Normal data,seg_309,+1) / (γ (m 2 )√mπ). this distribution is denoted by t(m) and is referred to as the t-distribution with m degrees of freedom.
2500,1,"['function', 'standard normal distribution', 'symmetric', 'standard', 'limit', 'gamma function', 'functions', 'densities', 'gamma', 'standard normal', 'distribution', 'cauchy', 'cauchy distribution', 'normal', 'tails', 'normal distribution']", Normal data,seg_309,"the normalizing constant km is given in terms of the gamma function, which was defined on page 157. for m = 1, it evaluates to k1 = 1/π, and the resulting density is that of the standard cauchy distribution (see page 161). if x has a t(m) distribution, then e[x ] = 0 for m ≥ 2 and var(x) = m/(m − 2) for m ≥ 3. densities of t-distributions look like that of the standard normal distribution: they are also symmetric around 0 and bell-shaped. as m goes to infinity the limit of the t(m) density is the standard normal density. the distinguishing feature is that densities of t-distributions have heavier tails: f(x) goes to zero as x goes to +∞ or −∞, but more slowly than the density φ(x) of the standard normal distribution. these properties are illustrated in figure 23.3, which shows the densities and distribution functions of the t(1), t(2), and t(5) distribution as well as those of the standard normal."
2501,1,"['distribution', 'critical values', 'critical value']", Normal data,seg_309,"we will also need critical values for the t(m) distribution: the critical value tm,p is the number satisfying"
2502,1,"['critical values', 'random', 'standard normal', 'symmetric', 'distribution', 'random variable', 'variable', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Normal data,seg_309,"where t is a t(m) distributed random variable. because the t-distribution is symmetric around zero, using the same reasoning as for the critical values of the standard normal distribution, we find:"
2503,1,['table'], Normal data,seg_309,"for example, in table b.2 we read t10,0.01 = 2.764, and from this we deduce that t10,0.99 = −2.764."
2504,1,['table'], Normal data,seg_309,"quick exercise 23.5 determine t3,0.01 and t35,0.9975 from table b.2."
2505,1,['distribution'], Normal data,seg_309,we now return to the distribution of
2506,1,"['confidence', 'interval', 'confidence interval']", Normal data,seg_309,and construct a confidence interval for µ.
2507,1,"['sample', 'studentized mean', 'random sample', 'distribution', 'normal', 'mean', 'random']", Normal data,seg_309,"the studentized mean of a normal random sample. for a random sample x1, . . . , xn from an n(µ, σ2) distribution, the stu-"
2508,1,['mean'], Normal data,seg_309,dentized mean x̄n − µ
2509,1,['distribution'], Normal data,seg_309,"has a t(n − 1) distribution, regardless of the values of µ and σ."
2510,1,['critical values'], Normal data,seg_309,"from this fact and using critical values of the t-distribution, we derive that"
2511,1,"['confidence', 'interval', 'confidence interval']", Normal data,seg_309,and in the same way as when σ is known it now follows that a 100(1 − α)% confidence interval for µ is given by:
2512,1,"['data', 'table', 'method', 'estimate']", Normal data,seg_309,"returning to the coal example, there was another shipment, of daw mill 258gb41 coal, where there were actually some doubts whether the stated accuracy of the iso 1928 method was attained. we therefore prefer to consider σ unknown and estimate it from the data, which are given in table 23.2."
2513,1,"['confidence', 'interval', 'confidence interval']", Normal data,seg_309,"doing this, we find x̄n = 31.012 and sn = 0.1294. because n = 22, for a 95% confidence interval we use t21,0.025 = 2.080 and obtain"
2514,1,"['sample size', 'sample', 'method', 'estimate', 'interval', 'samples', 'confidence', 'critical value', 'confidence interval']", Normal data,seg_309,"note that this confidence interval is (50%!) wider than the one we made for the osterfeld coal, with almost the same sample size. there are two reasons for this; one is that σ = 0.1 is replaced by the (larger) estimate sn = 0.1294, and the second is that the critical value z0.025 = 1.96 is replaced by the larger t21,0.025 = 2.080. the differences in the method and the ingredients seem minor, but they matter, especially for small samples."
2515,1,"['levels', 'confidence intervals', 'interval', 'data', 'intervals', 'cases', 'confidence', 'limit', 'confidence interval', 'model', 'distribution', 'central limit theorem', 'level', 'percent', 'method', 'deviations', 'normality', 'normal', 'confidence level', 'normal distribution']", Bootstrap confidence intervals,seg_311,"it is not uncommon that the methods of the previous section are used even when the normal distribution is not a good model for the data. in some cases this is not a big problem: with small deviations from normality the actual confidence level of a constructed confidence interval may deviate only a few percent from the intended confidence level. for large datasets the central limit theorem in fact ensures that this method provides confidence intervals with approximately correct confidence levels, as we shall see in the next section."
2516,1,"['confidence interval', 'sample', 'interval', 'bootstrap', 'dataset', 'random sample', 'data', 'distribution', 'normality', 'expectation', 'random', 'confidence', 'realization']", Bootstrap confidence intervals,seg_311,"if we doubt the normality of the data and we do not have a large sample, usually the best thing to do is to bootstrap. suppose we have a dataset x1, . . . , xn, modeled as a realization of a random sample from some distribution f , and we want to construct a confidence interval for its (unknown) expectation µ."
2517,0,[], Bootstrap confidence intervals,seg_311,in the previous section we saw that it suffices to find numbers cl and cu such
2518,1,"['confidence', 'interval', 'confidence interval']", Bootstrap confidence intervals,seg_311,the 100(1− α)% confidence interval would then be
2519,1,"['deviation', 'sample', 'sample mean', 'sample standard deviation', 'distribution', 'mean', 'standard', 'standard deviation']", Bootstrap confidence intervals,seg_311,"where, of course, x̄n and sn are the sample mean and the sample standard deviation. to find cl and cu we need to know the distribution of the studentized"
2520,1,"['bootstrap', 'data', 'estimate']", Bootstrap confidence intervals,seg_311,"we apply the bootstrap principle. from the data x1, . . . , xn we determine an estimate f̂ of f . let x1∗, . . . , xn"
2521,1,"['sample', 'random sample', 'random']", Bootstrap confidence intervals,seg_311,"∗ be a random sample from f̂ , with µ∗ = e[xi∗], and consider"
2522,1,"['distribution', 'approximation']", Bootstrap confidence intervals,seg_311,"the distribution of t ∗ is now used as an approximation to the distribution of t . if we use f̂ = fn, we get the following."
2523,1,"['studentized mean', 'bootstrap', 'estimate', 'dataset', 'simulation', 'distribution function', 'empirical distribution function', 'distribution', 'expectation', 'mean', 'function']", Bootstrap confidence intervals,seg_311,"empirical bootstrap simulation for the studentized mean. given a dataset x1, x2, . . . , xn, determine its empirical distribution function fn as an estimate of f . the expectation corresponding to fn is µ∗ = x̄n. 1. generate a bootstrap dataset x∗1, x∗2, . . . , x∗n from fn. 2. compute the studentized mean for the bootstrap dataset:"
2524,1,"['sample', 'sample mean', 'mean', 'standard']", Bootstrap confidence intervals,seg_311,"where x̄∗n and s∗n are the sample mean and sample standard deviation of x∗1, x∗2, . . . , x∗n. repeat steps 1 and 2 many times."
2525,1,"['bootstrap', 'experiment']", Bootstrap confidence intervals,seg_311,from the bootstrap experiment we can determine c∗l and c∗u such that
2526,1,"['critical values', 'estimated', 'bootstrap', 'distribution']", Bootstrap confidence intervals,seg_311,"by the bootstrap principle we may transfer this statement about the distribution of t ∗ to the distribution of t . that is, we may use these estimated critical values as bootstrap approximations to cl and cu:"
2527,0,[], Bootstrap confidence intervals,seg_311,"therefore, we call"
2528,1,"['confidence', 'interval', 'bootstrap', 'confidence interval']", Bootstrap confidence intervals,seg_311,a 100(1 − α)% bootstrap confidence interval for µ.
2529,1,['data'], Bootstrap confidence intervals,seg_311,example: the software data
2530,1,"['histogram', 'estimates', 'distribution function', 'statistic', 'sample statistic', 'random sample', 'random', 'function', 'sample', 'bootstrap', 'dataset', 'data', 'empirical distribution function', 'distribution', 'normal', 'normal distribution']", Bootstrap confidence intervals,seg_311,"recall the software data, a dataset of interfailure times (see section 17.3). from the nature of the data—failure times are positive numbers—and the histogram (figure 17.5), we know that they should not be modeled as a realization of a random sample from a normal distribution. from the data we know x̄n = 656.88, sn = 1037.3, and n = 135. we generate one thousand bootstrap datasets, and for each dataset we compute t∗ as in step 2 of the procedure. the histogram and empirical distribution function made from these one thousand values are estimates of the density and the distribution function, respectively, of the bootstrap sample statistic t ∗; see figure 23.4."
2531,1,"['quantile', 'interval', 'bootstrap', 'distribution function', 'empirical distribution function', 'distribution', 'statistic', 'function', 'confidence', 'confidence interval', 'order statistic']", Bootstrap confidence intervals,seg_311,"we want to make a 90% bootstrap confidence interval, so we need c∗l and c∗u, or the 0.05th and 0.95th quantile from the empirical distribution function in figure 23.4. the 50th order statistic of the one thousand t∗ values is −2.107. this means that 50 out of the one thousand values, or 5%, are smaller than or equal to this value, and so c∗l = −2.107. similarly, from the 951st order statistic, 1.389, we obtain3 cu∗ = 1.389. inserting these values, we find the following 90% bootstrap confidence interval for µ:"
2532,1,"['interval', 'bootstrap', 'results', 'statistic', 'level', 'confidence', 'confidence level', 'confidence interval', 'order statistic']", Bootstrap confidence intervals,seg_311,"quick exercise 23.6 the 25th and 976th order statistic from the preceding bootstrap results are −2.443 and 1.713, respectively. use these numbers to construct a confidence interval for µ. what is the corresponding confidence level?"
2533,1,['bootstrap'], Bootstrap confidence intervals,seg_311,why the bootstrap may be better
2534,1,"['studentized mean', 'critical values', 'bootstrap', 'approximation', 'data', 'distribution', 'normality', 'normal', 'mean']", Bootstrap confidence intervals,seg_311,"the reason to use the bootstrap is that it should lead to a more accurate approximation of the distribution of the studentized mean than the t(n − 1) distribution that follows from assuming normality. if, in the previous example, we would think we had normal data, we would use critical values from the t(134) distribution: t134,0.05 = 1.656. the result would be"
2535,1,"['histogram', 'interval', 'sample', 'sample mean', 'bootstrap', 'data', 'intervals', 'mean', 'skewness', 'confidence', 'confidence interval', 'long tail', 'skewed', 'distribution', 'percentage', 'method', 'confidence statements', 'normal', 'tail']", Bootstrap confidence intervals,seg_311,"comparing the intervals, we see that here the bootstrap interval is a little larger and, as opposed to the t-interval, not centered around the sample mean but skewed to the right side. this is one of the features of the bootstrap: if the distribution from which the data originate is skewed, this is reflected in the confidence interval. looking at the histogram of the software data (figure 17.5), we see that is it skewed to the right: it has a long tail on the right, but not on the left, so the same most likely holds for the distribution from which these data originate. the skewness is reflected in the confidence interval, which extends more to the right of x̄n than to the left. in some sense, the bootstrap adapts to the shape of the distribution, and in this way it leads to more accurate confidence statements than using the method for normal data. what we mean by this is that, for example, with the normal method only 90% of the 95% confidence statements would actually cover the true value, whereas for the bootstrap intervals this percentage would be close(r) to 95%."
2536,1,"['studentized mean', 'states', 'distribution', 'mean', 'central limit theorem', 'limit']", Large samples,seg_313,"a variant of the central limit theorem states that as n goes to infinity, the distribution of the studentized mean"
2537,1,"['sample', 'random', 'confidence intervals', 'standard normal', 'intervals', 'distribution', 'normal', 'standard', 'standard normal distribution', 'confidence', 'normal distribution']", Large samples,seg_313,"approaches the standard normal distribution. this fact is the basis for socalled large sample confidence intervals. suppose x1, . . . , xn is a random"
2538,1,"['distribution', 'expectation']", Large samples,seg_313,"sample from some distribution f with expectation µ. if n is large enough, we may use"
2539,1,"['sample', 'random', 'distribution', 'expectation', 'random sample', 'realization']", Large samples,seg_313,"this implies that if x1, . . . , xn can be seen as a realization of a random sample from some unknown distribution with expectation µ and if n is large enough, then"
2540,1,"['confidence', 'interval', 'confidence interval']", Large samples,seg_313,is an approximate 100(1− α)% confidence interval for µ.
2541,1,"['levels', 'confidence intervals', 'sample', 'simulation', 'experiment', 'results', 'coverage probabilities', 'intervals', 'exponential', 'confidence', 'limit', 'distributions', 'skewed', 'distribution', 'pareto', 'central limit theorem', 'percent', 'estimated', 'probabilities', 'method', 'table', 'pareto distribution']", Large samples,seg_313,"just as earlier with the central limit theorem, a key question is “how big should n be?” again, there is no easy answer. to give you some idea, we have listed in table 23.3 the results of a small simulation experiment. for each of the distributions, sample sizes, and confidence levels listed, we constructed 10 000 confidence intervals with the large sample method; the numbers listed in the table are the confidence levels as estimated from the simulation, the coverage probabilities. the chosen pareto distribution is very skewed, and this shows; the coverage probabilities for the exponential are just a few percent off."
2542,1,"['independent', 'simulation', 'dataset', 'case', 'cases']", Large samples,seg_313,"in the case of simulation one can often quite easily generate a very large number of independent repetitions, and then this question poses no problem. in other cases there may be nothing better to do than hope that the dataset is large enough. we give an example where (we believe!) this is definitely the case."
2543,1,"['table', 'dataset', 'observations', 'intervals']", Large samples,seg_313,"in an article published in 1910 ([28]), rutherford and geiger reported their observations on the radioactive decay of the element polonium. using a small disk coated with polonium they counted the number of emitted alpha-particles during 2608 intervals of 7.5 seconds each. the dataset consists of the counted number of alpha-particles for each of the 2608 intervals and can be summarized as in table 23.4."
2544,1,"['deviation', 'sample', 'interval', 'table', 'sample standard deviation', 'results', 'data', 'standard', 'standard deviation', 'confidence', 'average', 'confidence interval']", Large samples,seg_313,"the total number of counted alpha-particles is 10 097, the average number per interval is therefore 3.8715. the sample standard deviation can also be computed from the table; it is 1.9225. so we know of the actual data x1, x2, . . . , x2608 (where the counts xi are between 0 and 14) that x̄n = 3.8715 and sn = 1.9225. we construct a 98% confidence interval for the expected number of particles per interval. as z0.01 = 2.33 this results in"
2545,1,['probability'], Solutions to the quick exercises,seg_315,"23.1 from the probability statement, we derive, using σt = 100 and 8/9 = 0.889:"
2546,1,['probability'], Solutions to the quick exercises,seg_315,"θ ∈ (t − 300, t + 300) with probability at least 88%."
2547,1,['confidence'], Solutions to the quick exercises,seg_315,"θ ∈ (299 552.4, 300 152.4) with confidence at least 88%."
2548,1,"['case', 'distribution', 'probability', 'distributions', 'inequality']", Solutions to the quick exercises,seg_315,"23.2 chebyshev’s inequality only gives an upper bound. the actual value of p(|t − θ| < 2σt ) could be higher than 3/4, depending on the distribution of t . for example, in quick exercise 13.2 we saw that in case of an exponential distribution this probability is 0.865. for other distributions, even higher values are attained; see exercise 13.1."
2549,1,"['deviation', 'confidence intervals', 'standard deviations', 'outcome', 'intervals', 'distribution', 'deviations', 'expectation', 'probability', 'standard', 'standard deviation', 'confidence']", Solutions to the quick exercises,seg_315,"23.3 for each of the confidence intervals we have a 5% probability that it is wrong. therefore, the number of wrong confidence intervals has a bin(40, 0.05) distribution, and we would expect about 40 · 0.05 = 2 to be wrong. the standard deviation of this distribution is √40 · 0.05 · 0.95 = 1.38. the outcome “10 confidence intervals wrong” is (10− 2)/1.38 = 5.8 standard deviations from the expectation and would be a surprising outcome indeed. (the probability of 10 or more wrong is 0.00002.)"
2550,1,"['distribution', 'tail', 'table']", Solutions to the quick exercises,seg_315,"23.4 we need to solve p(z ≥ a) = 0.01. in table b.1 we find p(z ≥ 2.33) = 0.0099 ≈ 0.01, so z0.01 ≈ 2.33. for z0.95 we need to solve p(z ≥ a) = 0.95, and because this is in the left tail of the distribution, we use z0.95 = −z0.05. in the table we read p(z ≥ 1.64) = 0.0505 and p(z ≥ 1.65) = 0.0495, from which we conclude z0.05 ≈ (1.64 + 1.65)/2 = 1.645 and z0.95 ≈ −1.645."
2551,1,['table'], Solutions to the quick exercises,seg_315,"23.5 in table b.1 we find p(t3 ≥ 4.541) = 0.01, so t3,0.01 = 4.541. for t35,0.9975, we need to use t35,0.9975 = −t35,0.0025. in the table we find t30,0.0025 = 3.030 and t40,0.0025 = 2.971, and by interpolation t35,0.0025 ≈ (3.030 + 2.971)/2 = 3.0005. hence, t35,0.9975 ≈ −3.000."
2552,1,"['interval', 'estimates', 'bootstrap', 'statistics', 'order statistics', 'confidence', 'confidence interval']", Solutions to the quick exercises,seg_315,"23.6 the order statistics are estimates for c∗0.025 and c∗0.975, respectively. so the corresponding α is 0.05, and the 95% bootstrap confidence interval for µ is:"
2553,1,"['sample', 'interval', 'distribution', 'confidence', 'confidence interval']", Exercises,seg_317,"23.1 a bottling machine is known to fill wine bottles with amounts that follow an n(µ, σ2) distribution, with σ = 5 (ml). in a sample of 16 bottles, x̄ = 743 (ml) was found. construct a 95% confidence interval for µ."
2554,1,"['interval', 'random', 'random sample', 'sample', 'dataset', 'standard', 'standard deviation', 'confidence', 'confidence interval', 'expectation', 'deviation', 'sample standard deviation', 'normal', 'average', 'realization']", Exercises,seg_317,"23.2 you are given a dataset that may be considered a realization of a normal random sample. the size of the dataset is 34, the average is 3.54, and the sample standard deviation is 0.13. construct a 98% confidence interval for the unknown expectation µ."
2555,1,"['interval', 'normal distribution', 'random', 'random sample', 'sample', 'standard', 'standard deviation', 'confidence', 'confidence interval', 'parameters', 'distribution', 'deviation', 'sample standard deviation', 'normal', 'average', 'realization']", Exercises,seg_317,"23.3 you have ordered 10 bags of cement, which are supposed to weigh 94 kg each. the average weight of the 10 bags is 93.5 kg. assuming that the 10 weights can be viewed as a realization of a random sample from a normal distribution with unknown parameters, construct a 95% confidence interval for the expected weight of a bag. the sample standard deviation of the 10 weights is 0.75."
2556,1,"['sample', 'random', 'percentage', 'interval', 'factors', 'association', 'random sample', 'distribution', 'confidence', 'standard', 'test', 'average', 'confidence interval']", Exercises,seg_317,"23.4 a new type of car tire is launched by a tire manufacturer. the automobile association performs a durability test on a random sample of 18 of these tires. for each tire the durability is expressed as a percentage: a score of 100 (%) means that the tire lasted exactly as long as the average standard tire, an accepted comparison standard. from the multitude of factors that influence the durability of individual tires the assumption is warranted that the durability of an arbitrary tire follows an n(µ, σ2) distribution. the parameters µ and σ2 characterize the tire type, and µ could be called the durability index for this type of tire. the automobile association found for the tested tires: x̄18 = 195.3 and s18 = 16.7. construct a 95% confidence interval for µ."
2557,1,"['results', 'table']", Exercises,seg_317,"23.5 during the 2002 winter olympic games in salt lake city a newspaper article mentioned the alleged advantage speed-skaters have in the 1500m race if they start in the outer lane. in the men’s 1500m, there were 24 races, but in race 13 (really!) someone fell and did not finish. the results in seconds of the remaining 23 races are listed in table 23.5. you should know that who races against whom, in which race, and who starts in the outer lane are all determined by a fair lottery."
2558,1,['factors'], Exercises,seg_317,a. as a consequence of the lottery and the fact that many different factors
2559,1,"['distribution', 'normal', 'normal distribution']", Exercises,seg_317,"contribute to the actual time difference “inner lane minus outer lane” the assumption of a normal distribution for the difference is warranted. the numbers in the last column can be seen as realizations from an n(δ, σ2)"
2560,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_317,"distribution, where δ is the expected outer lane advantage. construct a 95% confidence interval for δ. n.b. n = 23, not 24!"
2561,1,"['confidence', 'interval', 'bootstrap', 'confidence interval']", Exercises,seg_317,b. you decide to make a bootstrap confidence interval instead. describe the
2562,1,"['bootstrap', 'experiment']", Exercises,seg_317,appropriate bootstrap experiment.
2563,1,"['bootstrap', 'experiment']", Exercises,seg_317,c. the bootstrap experiment was performed with one thousand repetitions.
2564,1,"['interval', 'bootstrap', 'table', 'results', 'confidence', 'outcomes', 'confidence interval']", Exercises,seg_317,"part of the bootstrap outcomes are listed in the following table. from the ordered list of results, numbers 21 to 60 and 941 to 980 are given. use these to construct a 95% bootstrap confidence interval for δ."
2565,1,"['sample', 'dataset', 'statistics', 'distribution', 'sample statistics', 'realization']", Exercises,seg_317,"23.6 a dataset x1, x2, . . . , xn is given, modeled as realization of a sample x1, x2, . . . , xn from an n(µ, 1) distribution. suppose there are sample statistics ln = g(x1, . . . , xn) and un = h(x1, . . . , xn) such that"
2566,1,"['confidence interval', 'confidence', 'interval', 'data']", Exercises,seg_317,"for every value of µ. suppose that the corresponding 95% confidence interval derived from the data is (ln, un) = (−2, 5)."
2567,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_317,b. write the 95% confidence interval for θ in terms of ln and un.
2568,1,['confidence'], Exercises,seg_317,"c. suppose θ = 1 − µ. again, find l̃n and ũn, as well as the confidence"
2569,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_317,d. suppose θ = µ2. can you construct a confidence interval for θ?
2570,1,"['interval', 'distribution', 'random variable', 'variable', 'parameter', 'random', 'confidence', 'confidence interval']", Exercises,seg_317,"23.7 a 95% confidence interval for the parameter µ of a pois(µ) distribution is given: (2, 3). let x be a random variable with this distribution."
2571,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_317,−µ construct a 95% confidence interval for p(x = 0) = e .
2572,1,"['confidence interval', 'sample', 'deviation', 'independent', 'interval', 'distribution', 'normal', 'standard', 'standard deviation', 'confidence', 'average', 'normal distribution']", Exercises,seg_317,"23.8 suppose that in exercise 23.1 the content of the bottles has to be determined by weighing. it is known that the wine bottles involved weigh on average 250 grams, with a standard deviation of 15 grams, and the weights follow a normal distribution. for a sample of 16 bottles, an average weight of 998 grams was found. you may assume that 1 ml of wine weighs 1 gram, and that the filling amount is independent of the bottle weight. construct a 95% confidence interval for the expected amount of wine per bottle, µ."
2573,1,"['interval', 'bootstrap', 'table', 'data', 'confidence', 'confidence interval']", Exercises,seg_317,23.9 consider the alpha-particle counts discussed in section 23.4; the data are given in table 23.4. we want to bootstrap in order to make a bootstrap confidence interval for the expected number of particles in a 7.5-second interval.
2574,1,"['simulation', 'bootstrap']", Exercises,seg_317,a. describe in detail how you would perform the bootstrap simulation.
2575,1,"['bootstrap', 'experiment']", Exercises,seg_317,b. the bootstrap experiment was performed with one thousand repetitions.
2576,1,"['interval', 'bootstrap', 'table', 'confidence', 'confidence interval']", Exercises,seg_317,part of the (ordered) bootstrap t∗’s are given in the following table. construct the 95% bootstrap confidence interval for the expected number of particles in a 7.5-second interval.
2577,0,[], Exercises,seg_317,c. answer this without doing any calculations: if we made the 98% boot-
2578,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_317,"strap confidence interval, would it be smaller or larger than the interval constructed in section 23.4?"
2579,1,"['studentized mean', 'interval', 'distribution', 'mean', 'parameter', 'confidence', 'confidence interval']", Exercises,seg_317,"23.10 in a report you encounter a 95% confidence interval (1.6, 7.8) for the parameter µ of an n(µ, σ2) distribution. the interval is based on 16 observations, constructed according to the studentized mean procedure."
2580,1,"['mean', 'dataset']", Exercises,seg_317,a. what is the mean of the (unknown) dataset?
2581,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_317,b. you prefer to have a 99% confidence interval for µ. construct it.
2582,1,"['interval', 'distribution', 'expectation', 'confidence', 'confidence interval']", Exercises,seg_317,23.11 a 95% confidence interval for the unknown expectation of some distribution contains the number 0.
2583,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_317,"a. we construct the corresponding 98% confidence interval, using the same"
2584,0,[], Exercises,seg_317,data. will it contain the number 0?
2585,1,"['confidence', 'interval', 'bootstrap', 'confidence interval']", Exercises,seg_317,b. the confidence interval in fact is a bootstrap confidence interval. we re-
2586,1,"['interval', 'experiment', 'bootstrap', 'results', 'data', 'confidence', 'confidence interval']", Exercises,seg_317,peat the bootstrap experiment (using the same data) and construct a new 95% confidence interval based on the results. will it contain the number 0?
2587,1,"['data', 'dataset']", Exercises,seg_317,"c. we collect new data, resulting in a dataset of the same size. with this data,"
2588,1,"['interval', 'confidence', 'expectation', 'confidence interval']", Exercises,seg_317,we construct a 95% confidence interval for the unknown expectation. will the interval contain 0?
2589,1,"['sample', 'random', 'sample standard deviations', 'standard deviations', 'random sample', 'distribution', 'deviations', 'standard']", Exercises,seg_317,"23.12 let z1, . . . , zn be a random sample from an n(0, 1) distribution. define xi = µ+σzi for i = 1, . . . , n and σ > 0. let z̄, x̄ denote the sample averages and sz and sx the sample standard deviations, of the zi and xi, respectively."
2590,1,"['sample', 'random', 'distribution', 'random sample']", Exercises,seg_317,"a. show that x1, . . . , xn is a random sample from an n(µ, σ2) distribution."
2591,0,[], Exercises,seg_317,c. verify that
2592,1,"['distribution', 'studentized mean', 'mean']", Exercises,seg_317,and explain why this shows that the distribution of the studentized mean does not depend on µ and σ.
2593,1,"['sample size', 'sample', 'confidence intervals', 'method', 'interval', 'intervals', 'distribution', 'binomial', 'parameter', 'expectations', 'binomial distribution', 'confidence', 'confidence interval']", More on confidence intervals,seg_319,"while in chapter 23 we were solely concerned with confidence intervals for expectations, in this chapter we treat a variety of topics. first, we focus on confidence intervals for the parameter p of the binomial distribution. then, based on an example, we briefly discuss a general method to construct confidence intervals. one-sided confidence intervals, or upper and lower confidence bounds, are discussed next. at the end of the chapter we investigate the question of how to determine the sample size when a confidence interval of a certain width is desired."
2594,1,"['sample', 'estimate', 'distribution', 'random variable', 'variable', 'population', 'random', 'estimator']", The probability of success,seg_321,"a common situation is that we observe a random variable x with a bin(n, p) distribution and use x to estimate p. for example, if we want to estimate the proportion of voters that support candidate g in an election, we take a sample from the voter population and determine the proportion in the sample that supports g. if n individuals are selected at random from the population, where a proportion p supports candidate g, the number of supporters x in the sample is modeled by a bin(n, p) distribution; we count the supporters of candidate g as “successes.” usually, the sample proportion x/n is taken as an estimator for p."
2595,1,"['sample', 'confidence intervals', 'interval', 'statistics', 'intervals', 'confidence', 'confidence interval']", The probability of success,seg_321,"if we want to make a confidence interval for p, based on the number of successes x in the sample, we need to find statistics l and u (see the definition of confidence intervals on page 343) such that"
2596,1,"['confidence intervals', 'method', 'intervals', 'confidence']", The probability of success,seg_321,"where l and u are to be based on x only. in general, this problem does not have a solution. however, the method for large n described next, sometimes called “the wilson method” (see [40]), yields confidence intervals with"
2597,1,"['level', 'confidence', 'confidence level']", The probability of success,seg_321,"confidence level approximately 100(1 − α)%. (how close the true confidence level is to 100(1−α)% depends on the (unknown) p, though it is known that for p near 0 and 1 it is too low. for some details and an alternative for this situation, see remark 24.1.)"
2598,1,"['normal approximation', 'approximation', 'distribution', 'normal', 'binomial', 'central limit theorem', 'binomial distribution', 'limit']", The probability of success,seg_321,"recall the normal approximation to the binomial distribution, a consequence of the central limit theorem (see page 201 and exercise 14.5): for large n, the distribution of x is approximately normal and"
2599,1,"['standard normal', 'normal', 'standard']", The probability of success,seg_321,"is approximately standard normal. by dividing by n in both the numerator and the denominator, we see that this equals:"
2600,0,['n'], The probability of success,seg_321,"therefore, for large n"
2601,1,['event'], The probability of success,seg_321,note that the event x − p
2602,0,[], The probability of success,seg_321,is the same as
2603,1,"['interval', 'data', 'confidence', 'confidence interval', 'inequality']", The probability of success,seg_321,"to derive expressions for l and u we can rewrite the inequality in this statement to obtain the form l < p < u , but the resulting formulas are rather awkward. to obtain the confidence interval, we instead substitute the data values directly and then solve for p, which yields the desired result."
2604,1,"['sample', 'interval', 'confidence', 'population', 'realization', 'confidence interval', 'inequality']", The probability of success,seg_321,"suppose, in a sample of 125 voters, 78 support one candidate. what is the 95% confidence interval for the population proportion p supporting that candidate? the realization of x is x = 78 and n = 125. we substitute this, together with zα/2 = z0.025 = 1.96, in the last inequality:"
2605,0,[], The probability of success,seg_321,"or, working out squares and products and grouping terms:"
2606,1,"['interval', 'results', 'coefficient', 'confidence', 'confidence interval', 'inequality']", The probability of success,seg_321,"this quadratic form describes a parabola, which is depicted in figure 24.1. also, for other values of n and x there always results a quadratic inequality like this, with a positive coefficient for p2 and a similar picture. for the confidence interval we need to find the values where the parabola intersects the horizontal axis. the solutions we find are:"
2607,1,"['confidence', 'interval', 'confidence interval']", The probability of success,seg_321,"hence, l = 0.54 and u = 0.70, so the resulting confidence interval is (0.54, 0.70)."
2608,1,"['sample', 'interval', 'confidence', 'confidence interval']", The probability of success,seg_321,quick exercise 24.1 suppose in another election we find 80 supporters in a sample of 200. suppose we use α = 0.0456 for which zα/2 = 2. construct the corresponding confidence interval for p.
2609,1,"['confidence intervals', 'method', 'intervals', 'cases', 'confidence']", Is there a general method,seg_323,"we have now seen a number of examples of confidence intervals, and while it should be clear to you that in each of these cases the resulting intervals are valid confidence intervals, you may wonder how we go about finding confidence intervals in new situations. one could ask: is there a general method? we first consider an example."
2610,1,"['confidence', 'interval', 'confidence interval']", Is there a general method,seg_323,a confidence interval for the minimum lifetime
2611,1,"['sample', 'model', 'exponential distribution', 'random sample', 'distribution', 'random variable', 'variable', 'exponential', 'probability', 'random', 'parameter']", Is there a general method,seg_323,"suppose we have a random sample x1, . . . , xn from a shifted exponential distribution, that is, xi = δ +yi, where y1, . . . , yn are a random sample from an exp(1) distribution. this type of random variable is sometimes used to model lifetimes; a minimum lifetime is guaranteed, but otherwise the lifetime has an exponential distribution. the unknown parameter δ represents the minimum lifetime, and the probability density of the xi is positive only for values greater than δ."
2612,1,"['likelihood', 'information', 'maximum likelihood', 'maximum likelihood estimator', 'estimator']", Is there a general method,seg_323,"to derive information about δ it is natural to use the smallest observed value t = min{x1, . . . , xn}. this is also the maximum likelihood estimator for δ; see exercise 21.6. writing"
2613,1,"['distribution', 'function', 'distribution function']", Is there a general method,seg_323,"and observing that m = min{y1, . . . , yn} has an exp(n) distribution (see exercise 8.18), we find for the distribution function of t : ft (a) = 0 for a < δ and"
2614,0,[], Is there a general method,seg_323,"next, we solve"
2615,0,[], Is there a general method,seg_323,using (24.1) we find the following equations:
2616,0,[], Is there a general method,seg_323,"both cl and cu are values larger than δ, because the logarithms are negative. we have found that, whatever the value of δ:"
2617,0,[], Is there a general method,seg_323,"by rearranging the inequalities, we see this is equivalent to"
2618,1,"['confidence', 'interval', 'confidence interval']", Is there a general method,seg_323,and therefore a 100(1 − α)% confidence interval for δ is given by
2619,1,"['interval', 'dataset', 'distribution', 'confidence', 'confidence interval']", Is there a general method,seg_323,"quick exercise 24.2 suppose you have a dataset of size 15 from a shifted exp(1) distribution, whose minimum value is 23.5. what is the 99% confidence interval for δ?"
2620,1,"['sample', 'confidence intervals', 'exponential distribution', 'interval', 'intervals', 'distribution', 'statistic', 'sample statistic', 'exponential', 'parameter', 'function', 'confidence', 'confidence interval']", Is there a general method,seg_323,"looking back at the example, we see that the confidence interval could be constructed because we know that t −δ = m has an exponential distribution. there are many more examples of this type: some function g(t, θ) of a sample statistic t and the unknown parameter θ has a known distribution. however, this still does not cover all the ways to construct confidence intervals (see also the following remark)."
2621,1,"['interval', 'confident', 'data', 'confidence', 'confidence interval']", Onesided confidence intervals,seg_325,"suppose you are in charge of a power plant that generates and sells electricity, and you are about to buy a shipment of coal, say a shipment of the daw mill coal identified as 258gb41 earlier. you plan to buy the shipment if you are confident that the gross calorific content exceeds 31.00 mj/kg. at the end of section 23.2 we obtained for the gross calorific content the 95% confidence interval (30.946, 31.067): based on the data we are 95% confident that the gross calorific content is higher than 30.946 and lower than 31.067."
2622,1,"['confidence', 'confident']", Onesided confidence intervals,seg_325,"in the present situation, however, we are only interested in the lower bound: we would prefer a confidence statement of the type “we are 95% confident that the gross calorific content exceeds 31.00.” modifying equation (23.4) we"
2623,0,[], Onesided confidence intervals,seg_325,which is equivalent to
2624,0,[], Onesided confidence intervals,seg_325,we conclude that sn
2625,1,"['interval', 'confidence', 'results', 'confidence interval']", Onesided confidence intervals,seg_325,"is a 100(1 − α)% one-sided confidence interval for µ. for the daw mill coal, using α = 0.05, with t21,0.05 = 1.721 this results in:"
2626,1,"['confidence', 'uncertainty', 'interval']", Onesided confidence intervals,seg_325,"we see that because “all uncertainty may be put on one side,” the lower bound in the one-sided interval is higher than that in the two-sided one, though still below 31.00. other situations may require a confidence upper bound. for example, if the calorific value is below a certain number you can try to negotiate a lower the price."
2627,1,"['sample', 'confidence intervals', 'intervals', 'statistic', 'sample statistic', 'confidence']", Onesided confidence intervals,seg_325,the definition of confidence intervals (page 343) can be extended to include one-sided confidence intervals as well. if we have a sample statistic ln such that
2628,1,"['parameter of interest', 'parameter']", Onesided confidence intervals,seg_325,"for every value of the parameter of interest θ, then"
2629,1,"['confidence', 'interval', 'confidence bound', 'confidence interval']", Onesided confidence intervals,seg_325,"is called a 100γ% one-sided confidence interval for θ. the number ln is sometimes called a 100γ% lower confidence bound for θ. similary, un with p(θ < un) = γ for every value of θ, yields the one-sided confidence interval (−∞, un), and un is called a 100γ% upper confidence bound."
2630,1,"['confidence', 'confidence bound']", Onesided confidence intervals,seg_325,quick exercise 24.3 determine the 99% upper confidence bound for the gross calorific value of the daw mill coal.
2631,1,"['sample size', 'sample', 'interval', 'data', 'set', 'measurements', 'confidence', 'confidence interval']", Determining the sample size,seg_327,"the narrower the confidence interval the better (why?). as a general principle, we know that more accurate statements can be made if we have more measurements. sometimes, an accuracy requirement is set, even before data are collected, and the corresponding sample size is to be determined. we provide an example of how to do this and note that this generally can be done, but the actual computation varies with the type of confidence interval."
2632,1,"['interval', 'measurements', 'confidence', 'test', 'confidence interval']", Determining the sample size,seg_327,"consider the question of the calorific content of coal once more. we have a shipment of coal to test and we want to obtain a 95% confidence interval, but it should not be wider than 0.05 mj/kg, i.e., the lower and upper bound should not differ more than 0.05. how many measurements do we need?"
2633,1,"['deviation', 'method', 'interval', 'case', 'normally distributed', 'measurements', 'standard', 'level', 'standard deviation', 'confidence', 'confidence level', 'confidence interval']", Determining the sample size,seg_327,"we answer this question for the case when iso method 1928 is used, whence we may assume that measurements are normally distributed with standard deviation σ = 0.1. when the desired confidence level is 1 − α, the width of the confidence interval will be"
2634,1,[], Determining the sample size,seg_327,requiring that this is at most w means finding the smallest n that satisfies
2635,1,['measurements'], Determining the sample size,seg_327,"that is, we should perform at least 62 measurements."
2636,1,"['sample size', 'sample', 'deviation', 'method', 'estimate', 'interval', 'case', 'data', 'standard', 'standard deviation', 'confidence', 'confidence interval']", Determining the sample size,seg_327,"in case σ is unknown, we somehow have to estimate it, and then the method can only give an indication of the required sample size. the standard deviation as we (afterwards) estimate it from the data may turn out to be quite different, and the obtained confidence interval may be smaller or larger than intended."
2637,1,"['sample size', 'sample', 'interval', 'confidence', 'confidence interval']", Determining the sample size,seg_327,quick exercise 24.4 what is the required sample size if we want the 99% confidence interval to be 0.05 mj/kg wide?
2638,0,[], Solutions to the quick exercises,seg_329,the solutions are:
2639,1,"['confidence', 'interval', 'confidence interval']", Solutions to the quick exercises,seg_329,"so the confidence interval is (0.33, 0.47)."
2640,1,"['confidence', 'confidence bound']", Solutions to the quick exercises,seg_329,24.3 the upper confidence bound is given by
2641,1,"['level', 'confidence', 'confidence level']", Solutions to the quick exercises,seg_329,"24.4 the confidence level changes to 99%, so we use z0.005 = 2.576 instead of 1.96 in the computation:"
2642,1,['measurements'], Solutions to the quick exercises,seg_329,so we need at least 107 measurements.
2643,1,"['independent', 'interval', 'experiment', 'success', 'confidence', 'probability', 'experiments', 'confidence interval']", Exercises,seg_331,"24.1 of a series of 100 (independent and identical) chemical experiments, 70 were concluded succesfully. construct a 90% confidence interval for the success probability of this type of experiment."
2644,0,[], Exercises,seg_331,"24.2 in january 2002 the euro was introduced and soon after stories started to circulate that some of the euro coins would not be fair coins, because the “national side” of some coins would be too heavy or too light (see, for example, the new scientist of january 4, 2002, but also national newspapers of that date)."
2645,1,['tails'], Exercises,seg_331,"a. a french 1 euro coin was tossed six times, resulting in 1 heads and 5 tails."
2646,1,"['interval', 'method', 'wilson method', 'confidence', 'confidence interval']", Exercises,seg_331,"is it reasonable to use the wilson method, introduced in section 24.1, to construct a confidence interval for p?"
2647,1,['tails'], Exercises,seg_331,b. a belgian 1 euro coin was tossed 250 times: 140 heads and 110 tails.
2648,1,"['interval', 'confidence', 'probability', 'confidence interval']", Exercises,seg_331,construct a 95% confidence interval for the probability of getting heads with this coin.
2649,1,"['sample size', 'sample', 'interval', 'confidence', 'confidence interval']", Exercises,seg_331,"24.3 in exercise 23.1, what sample size is needed if we want a 99% confidence interval for µ at most 1 ml wide?"
2650,1,"['deviation', 'sample', 'sample standard deviation', 'standard', 'standard deviation', 'average']", Exercises,seg_331,"24.4 recall exercise 23.3 and the 10 bags of cement that should each weigh 94 kg. the average weight was 93.5 kg, with sample standard deviation 0.75."
2651,1,"['sample', 'data']", Exercises,seg_331,"a. based on these data, how many bags would you need to sample to make"
2652,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_331,a 90% confidence interval that is 0.1 kg wide?
2653,0,[], Exercises,seg_331,b. suppose you actually do measure the required number of bags and con-
2654,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_331,struct a new confidence interval. is it guaranteed to be at most 0.1 kg wide?
2655,1,"['sample size', 'sample', 'interval', 'distribution', 'probability', 'confidence', 'confidence interval']", Exercises,seg_331,"24.5 suppose we want to make a 95% confidence interval for the probability of getting heads with a dutch 1 euro coin, and it should be at most 0.01 wide. to determine the required sample size, we note that the probability of getting heads is about 0.5. furthermore, if x has a bin(n, p) distribution, with n large and p ≈ 0.5, then"
2656,1,"['standard normal', 'normal', 'standard']", Exercises,seg_331,x − np is approximately standard normal.
2657,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_331,a. use this statement to derive that the width of the 95% confidence interval
2658,0,[], Exercises,seg_331,for p is approximately z0.025 .
2659,0,['n'], Exercises,seg_331,use this width to determine how large n should be.
2660,0,[], Exercises,seg_331,"b. the coin is thrown the number of times just computed, resulting in 19 477"
2661,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_331,times heads. construct the 95% confidence interval and check whether the required accuracy is attained.
2662,1,"['sample', 'random', 'concentration', 'data', 'samples', 'normal', 'random sample', 'realization']", Exercises,seg_331,"24.6 environmentalists have taken 16 samples from the wastewater of a chemical plant and measured the concentration of a certain carcinogenic substance. they found x̄16 = 2.24 (ppm) and s216 = 1.12, and want to use these data in a lawsuit against the plant. it may be assumed that the data are a realization of a normal random sample."
2663,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_331,a. construct the 97.5% one-sided confidence interval that the environmen-
2664,1,['concentration'], Exercises,seg_331,talists made to convince the judge that the concentration exceeds legal limits.
2665,1,['data'], Exercises,seg_331,b. the plant management uses the same data to construct a 97.5% one-
2666,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_331,sided confidence interval to show that concentrations are not too high. construct this interval as well.
2667,1,"['poisson', 'independent', 'interval', 'observations', 'data', 'distribution', 'normal', 'central limit theorem', 'poisson distribution', 'average', 'limit', 'normal distribution']", Exercises,seg_331,"24.7 consider once more the rutherford-geiger data as given in section 23.4. knowing that the number of α-particle emissions during an interval has a poisson distribution, we may see the data as observations from a pois(µ) distribution. the central limit theorem tells us that the average x̄n of a large number of independent pois(µ) approximately has a normal distribution and"
2668,1,['distribution'], Exercises,seg_331,"has a distribution that is approximately n(0, 1)."
2669,1,"['sample', 'interval', 'confidence', 'confidence interval']", Exercises,seg_331,a. show that the large sample 95% confidence interval contains those values
2670,1,"['sample', 'interval', 'confidence', 'confidence interval']", Exercises,seg_331,b. use the result from a to construct the large sample 95% confidence interval
2671,1,['data'], Exercises,seg_331,based on the rutherford-geiger data.
2672,0,[], Exercises,seg_331,c. compare the result with that of exercise 23.9 b. is this surprising?
2673,1,['results'], Exercises,seg_331,"24.8 recall exercise 23.5 about the 1500m speed-skating results in the 2002 winter olympic games. if there were no outer lane advantage, the number"
2674,1,['distribution'], Exercises,seg_331,"out of the 23 completed races won by skaters starting in the outer lane would have a bin(23, p) distribution with p = 1/2, because of the lane assignment by lottery."
2675,0,[], Exercises,seg_331,"a. of the 23 races, 15 were won by the skater starting in the outer lane. use"
2676,1,"['method', 'interval', 'information', 'wilson method', 'central limit theorem', 'confidence', 'limit', 'confidence interval']", Exercises,seg_331,"this information to construct a 95% confidence interval for p by means of the wilson method. if you think that n = 23 is probably too small to use a method based on the central limit theorem, we agree. we should be careful with conclusions we draw from this confidence interval."
2677,0,[], Exercises,seg_331,b. the question posed earlier “is there an outer lane advantage?” implies that
2678,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_331,a one-sided confidence interval is more suitable. construct the appropriate 95% one-sided confidence interval for p by first constructing a 90% twosided confidence interval.
2679,1,"['sample', 'random', 'dataset', 'distribution', 'random sample', 'realization']", Exercises,seg_331,"24.9 suppose we have a dataset x1, . . . , x12 that may be modeled as the realization of a random sample x1, . . . , x12 from a u(0, θ) distribution, with θ unknown. let m = max{x1, . . . , x12}."
2680,1,"['confidence', 'realization']", Exercises,seg_331,c. suppose the realization of m is m = 3. construct the 90% confidence
2681,1,"['level', 'confidence', 'interval', 'confidence interval']", Exercises,seg_331,d. derive the general expression for a confidence interval of level 1−α based
2682,1,['sample'], Exercises,seg_331,on a sample of size n.
2683,1,"['sample', 'random', 'dataset', 'distribution', 'random sample', 'realization']", Exercises,seg_331,"24.10 suppose we have a dataset x1, . . . , xn that may be modeled as the realization of a random sample x1, . . . , xn from an exp(λ) distribution, where λ is unknown. let sn = x1 + · · · + xn."
2684,1,['distribution'], Exercises,seg_331,"a. check that λsn has a gam(n, 1) distribution."
2685,1,"['distribution', 'quantiles']", Exercises,seg_331,"b. the following quantiles of the gam(20, 1) distribution are given: q0.05 ="
2686,1,"['confidence', 'interval', 'confidence interval']", Exercises,seg_331,13.25 and q0.95 = 27.88. use these to construct a 90% confidence interval for λ when n = 20.
2687,1,"['confidence intervals', 'estimates', 'probability', 'process', 'numerical', 'experiment', 'intervals', 'hypothesis testing', 'confidence', 'statistical', 'model', 'distribution', 'hypothesis', 'hypotheses']", Testing hypotheses essentials,seg_333,"the statistical methods that we have discussed until now have been developed to infer knowledge about certain features of the model distribution that represent our quantities of interest. these inferences often take the form of numerical estimates, as either single numbers or confidence intervals. however, sometimes the conclusion to be drawn is not expressed numerically, but is concerned with choosing between two conflicting theories, or hypotheses. for instance, one has to assess whether the lifetime of a certain type of ball bearing deviates or does not deviate from the lifetime guaranteed by the manufacturer of the bearings; an engineer wants to know whether dry drilling is faster or the same as wet drilling; a gynecologist wants to find out whether smoking affects or does not affect the probability of getting pregnant; the allied forces want to know whether the german war production is equal to or smaller than what allied intelligence agencies reported. the process of formulating the possible conclusions one can draw from an experiment and choosing between two alternatives is known as hypothesis testing. in this chapter we start to explore this statistical methodology."
2688,1,"['hypothesis testing', 'hypothesis']", Null hypothesis and test statistic,seg_335,"we will introduce the basic concepts of hypothesis testing with an example. let us return to the analysis of german war equipment. during world war ii the allied forces received reports by the allied intelligence agencies on german war production. the numbers of produced tires, tanks, and other equipment, as claimed in these reports, were a lot higher than indicated by the observed serial numbers. the objective was to decide whether the actual produced quantities were smaller than the ones reported."
2689,0,[], Null hypothesis and test statistic,seg_335,for simplicity suppose that we have observed tanks with (recoded) serial numbers
2690,1,"['alternative hypothesis', 'data', 'trial', 'null hypothesis', 'hypothesis']", Null hypothesis and test statistic,seg_335,"furthermore, suppose that the allied intelligence agencies report a production of 350 tanks.1 this is a lot more than we would surmise from the observed data. we want to choose between the proposition that the total number of tanks is 350 and the proposition that the total number is smaller than 350. the two competing propositions are called null hypothesis, denoted by h0, and alternative hypothesis, denoted by h1. the way we go about choosing between h0 and h1 is conceptually similar to the way a jury deliberates in a court trial. the null hypothesis corresponds to the position of the defendant: just as he is presumed to be innocent until proven guilty, so is the null hypothesis presumed to be true until the data provide convincing evidence against it. the alternative hypothesis corresponds to the charges brought against the defendant."
2691,1,"['model', 'random variables', 'variables', 'without replacement', 'replacement', 'hypotheses', 'parameter', 'random', 'statistical model', 'statistical', 'realization']", Null hypothesis and test statistic,seg_335,"to decide whether h0 is false we use a statistical model. as argued in chapter 20 the (recoded) serial numbers are modeled as a realization of random variables x1, x2, . . . , x5 representing five draws without replacement from the numbers 1, 2, . . . , n . the parameter n represents the total number of tanks. the two hypotheses in question are"
2692,1,"['alternative hypothesis', 'test', 'null hypothesis', 'hypothesis']", Null hypothesis and test statistic,seg_335,"if we reject the null hypothesis we will accept h1; we speak of rejecting h0 in favor of h1. usually, the alternative hypothesis represents the theory or belief that we would like to accept if we do reject h0. this means that we must carefully choose h1 in relation with our interests in the problem at hand. in our example we are particularly interested in whether the number of tanks is less than 350; so we test the null hypothesis against h1 : n < 350. if we would be interested in whether the number of tanks differs from 350, or is greater than 350, we would test against h1 : n = 350 or h1 : n > 350."
2693,1,"['sample', 'random sample', 'data', 'distribution', 'expectation', 'random', 'test', 'null hypothesis', 'realization', 'hypothesis']", Null hypothesis and test statistic,seg_335,"quick exercise 25.1 in the drilling example from sections 15.5 and 16.4 the data on drill times for dry drilling are modeled as a realization of a random sample from a distribution with expectation µ1, and similarly the data for wet drilling correspond to a distribution with expectation µ2. we want to know whether dry drilling is faster than wet drilling. to this end we test the null hypothesis h0 : µ1 = µ2 (the drill time is the same for both methods). what would you choose for h1?"
2694,1,"['test statistic', 'statistic', 'test']", Null hypothesis and test statistic,seg_335,"the next step is to select a criterion based on x1, x2, . . . , x5 that provides an indication about whether h0 is false. such a criterion involves a test statistic."
2695,1,"['sample', 'random variables', 'dataset', 'variables', 'test statistic', 'statistic', 'sample statistic', 'numerical', 'random', 'test', 'realization']", Null hypothesis and test statistic,seg_335,"test statistic. suppose the dataset is modeled as the realization of random variables x1, x2, . . . , xn. a test statistic is any sample statistic t = h(x1, x2, . . . , xn), whose numerical value is used to decide whether we reject h0."
2696,1,"['test statistic', 'statistic', 'test']", Null hypothesis and test statistic,seg_335,in the tank example we use the test statistic
2697,1,"['test statistic', 'statistic', 'test']", Null hypothesis and test statistic,seg_335,"having chosen a test statistic t , we investigate what sort of values t can attain. these values can be viewed on a credibility scale for h0, and we must determine which of these values provide evidence in favor of h0, and which provide evidence in favor of h1. first of all note that if we find a value of t larger than 350, we immediately know that h0 as well as h1 is false. if this happens, we actually should be considering another testing problem, but for the current problem of testing h0 : n = 350 against h1 : n < 350 such values are irrelevant. hence the possible values of t that are of interest to us are the integers from 5 to 350."
2698,1,['expectation'], Null hypothesis and test statistic,seg_335,"if h0 is true, then what is a typical value for t and what is not? remember from section 20.1 that, because n = 5, the expectation of t is e[t ] = 5"
2699,1,['distribution'], Null hypothesis and test statistic,seg_335,6 (n+1). this means that the distribution of t is centered around 5
2700,1,"['null hypothesis', 'hypothesis']", Null hypothesis and test statistic,seg_335,"6 values of t that deviate a lot from 292.5 are evidence against h0. values that are much greater than 292.5 are evidence against h0 but provide even stronger evidence against h1. for such values we will not reject h0 in favor of h1. also values a little smaller than 292.5 are grounds not to reject h0, because we are committed to giving h0 the benefit of the doubt. on the other hand, values of t very close to 5 should be considered as strong evidence against the null hypothesis and are in favor of h1, hence they lead to a decision to reject h0. this is summarized in figure 25.1."
2701,1,"['test statistic', 'statistic', 'test']", Null hypothesis and test statistic,seg_335,"quick exercise 25.2 another possible test statistic would be x̄5. if we use its values as a credibility scale for h0, then what are the possible values of x̄5, which values of x̄5 are in favor of h1 : n < 350, and which values are in favor of h0 : n = 350?"
2702,1,['data'], Null hypothesis and test statistic,seg_335,for the data we find
2703,1,"['test statistic', 'statistic', 'test', 'realization']", Null hypothesis and test statistic,seg_335,as the realization of the test statistic. how do we use this to decide on h0?
2704,0,[], Tail probabilities,seg_337,"as we have just seen, if h0 is true, then typical values of t are in the neighbor-"
2705,1,"['null hypothesis', 'hypothesis']", Tail probabilities,seg_337,"6 left, the stronger evidence it provides in favor of h1. the value 61 is in the left region of figure 25.1. can we now reject h0 and conclude that n is smaller than 350, or can the fact that we observe 61 as maximum be attributed to chance? in courtroom terminology: can we reach the conclusion that the null hypothesis is false beyond reasonable doubt? one way to investigate this is to examine how likely it is that one would observe a value of t that provides even stronger evidence against h0 than 61, in the situation that n = 350. if this is very unlikely, then 61 already bears strong evidence against h0."
2706,1,"['test statistic', 'without replacement', 'replacement', 'statistic', 'test']", Tail probabilities,seg_337,"values of t that provide stronger evidence against h0 than 61 are to the left of 61. therefore we compute p(t ≤ 61). in the situation that n = 350, the test statistic t is the maximum of 5 numbers drawn without replacement from 1, 2, . . . , 350. we find that"
2707,1,"['null hypothesis', 'case', 'probability', 'hypothesis']", Tail probabilities,seg_337,"this probability is so small that we view the value 61 as strong evidence against the null hypothesis. indeed, if the null hypothesis would be true, then values of t that would provide the same or even stronger evidence against h0 than 61 are very unlikely to occur, i.e., they occur with probability 0.00014! in other words, the observed value 61 is exceptionally small in case h0 is true."
2708,1,"['alternative hypothesis', 'events', 'probability', 'null hypothesis', 'hypothesis']", Tail probabilities,seg_337,"at this point we can do two things: either we believe that h0 is true and that something very unlikely has happened, or we believe that events with such a small probability do not happen in practice, so that t ≤ 61 could only have occurred because h0 is false. we choose to believe that things happening with probability 0.00014 are so exceptional that we reject the null hypothesis h0 : n = 350 in favor of the alternative hypothesis h1 : n < 350. in courtroom terminology: we say that a value of t smaller than or equal to 61 implies that the null hypothesis is false beyond reasonable doubt."
2709,1,"['tail', 'tail probability', 'probability']", Tail probabilities,seg_337,"in our example, the more a value of t is to the left, the stronger evidence it provides against h0. for this reason we computed the left tail probability"
2710,1,"['tail probability', 'test statistic', 'case', 'data', 'right tail probability', 'cases', 'statistic', 'probability', 'tail', 'test']", Tail probabilities,seg_337,"p(t ≤ 61). in other situations, the direction in which values of t provide stronger evidence against h0 may be to the right of the observed value t, in which case one would compute a right tail probability p(t ≥ t). in both cases the tail probability expresses how likely it is to obtain a value of the test statistic t at least as extreme as the value t observed for the data. such a probability is called a p-value. in a way, the size of the p-value reflects how much evidence the observed value t provides against h0. the smaller the p-value, the stronger evidence the observed value t bears against h0."
2711,1,"['tail probability', 'case', 'cases', 'probability', 'tail']", Tail probabilities,seg_337,"the phrase “at least as extreme as the observed value t” refers to a particular direction, namely the direction in which values of t provide stronger evidence against h0 and in favor of h1. in our example, this was to the left of 61, and the p-value corresponding to 61 was p(t ≤ 61) = 0.00014. in this case it is clear what is meant by “at least as extreme as t” and which tail probability corresponds to the p-value. however, in some testing problems one can deviate from h0 in both directions. in such cases it may not be clear what values of t are at least as extreme as the observed value, and it may be unclear how the p-value should be computed. one approach to a solution in this case is to simply compute the one-tailed p-value that corresponds to the direction in which t deviates from h0."
2712,1,['test'], Tail probabilities,seg_337,"quick exercise 25.3 suppose that the allied intelligence agencies had reported a production of 80 tanks, so that we would test h0 : n = 80 against h1 : n < 80. compute the p-value corresponding to 61. would you conclude h0 is false beyond reasonable doubt?"
2713,1,"['case', 'expected value', 'hypothesis', 'null hypothesis']", Type I and type II errors,seg_339,suppose that the maximum is 200 instead of 61. this is also to the left of the expected value 292.5 of t . is it far enough to the left to reject the null hypothesis? in this case the p-value is equal to
2714,1,"['cases', 'probability', 'null hypothesis', 'hypothesis']", Type I and type II errors,seg_339,"this means that if the total number of produced tanks is 350, then in 5.96% of all cases we would observe a value of t that is at least as extreme as the value 200. before we decide whether 0.0596 is small enough to reject the null hypothesis let us explore in more detail what the preceding probability stands for."
2715,1,['data'], Type I and type II errors,seg_339,it is important to distinguish between (1) the true state of nature: h0 is true or h1 is true and (2) our decision: we reject or do not reject h0 on the basis of the data. in our example the possibilities for the true state of nature are:
2716,0,[], Type I and type II errors,seg_339,"ĺ h1 is true, i.e., the number of tanks produced is less than 350."
2717,0,[], Type I and type II errors,seg_339,we do not know in which situation we are. there are two possible decisions:
2718,0,[], Type I and type II errors,seg_339,"this leads to four possible situations, which are summarized in figure 25.2."
2719,1,"['alternative hypothesis', 'type i and type ii errors', 'data', 'errors', 'type ii', 'null hypothesis', 'type ii errors', 'hypothesis']", Type I and type II errors,seg_339,"there are two situations in which the decision made on the basis of the data is wrong. the null hypothesis h0 may be true, whereas the data lead to rejection of h0. on the other hand, the alternative hypothesis h1 may be true, whereas we do not reject h0 on the basis of the data. these wrong decisions are called type i and type ii errors."
2720,1,"['type ii error', 'errors', 'type ii', 'type i error', 'error']", Type I and type II errors,seg_339,type i and ii errors. a type i error occurs if we falsely reject h0. a type ii error occurs if we falsely do not reject h0.
2721,1,"['type ii error', 'type i error', 'error', 'type ii']", Type I and type II errors,seg_339,"in courtroom terminology, a type i error corresponds to convicting an innocent defendant, whereas a type ii error corresponds to acquitting a criminal."
2722,1,"['risk', 'mean', 'probability', 'type i error', 'error']", Type I and type II errors,seg_339,"if h0 : n = 350 is true, then the decision to reject h0 is a type i error. we will never know whether we make a type i error. however, given a particular decision rule, we can say something about the probability of committing a type i error. suppose the decision rule would be “reject h0 : n = 350 whenever t ≤ 200.” with this decision rule the probability of committing a type i error is p(t ≤ 200) = 0.0596. if we are willing to run the risk of committing a type i error with probability 0.0596, we could adopt this decision rule. this would also mean that on the basis of an observed maximum of 200 we would reject h0 in favor of h1 : n < 350."
2723,1,"['null hypothesis', 'probability', 'type i error', 'error', 'hypothesis']", Type I and type II errors,seg_339,"quick exercise 25.4 suppose we adopt the following decision rule about the null hypothesis: “reject h0 : n = 350 whenever t ≤ 250.” using this decision rule, what is the probability of committing a type i error?"
2724,1,"['level', 'risk', 'probability']", Type I and type II errors,seg_339,"the question remains what amount of risk one is willing to take to falsely reject h0, or in courtroom terminology: how small should the p-value be to reach a conclusion that is “beyond reasonable doubt”? in many situations, as a rule of thumb 0.05 is used as the level where reasonable doubt begins. something happening with probability less than or equal to 0.05 is then viewed as being too exceptional. however, there is no general rule that specifies how small the p-value must be to reject h0. there is no way to argue that this probability should be below 0.10 or 0.18 or 0.009—or anything else."
2725,1,"['dataset', 'test statistic', 'information', 'statistic', 'hypothesis', 'level', 'test', 'null hypothesis']", Type I and type II errors,seg_339,a possible solution is to solely report the p-value corresponding to the observed value of the test statistic. this is objective and does not have the arbitrariness of a preselected level such as 0.05. an investigator who reports the p-value conveys the maximum amount of information contained in the dataset and permits all decision makers to choose their own level and make their own decision about the null hypothesis. this is especially important when there is no justifiable reason for preselecting a particular value for such a level.
2726,0,[], Solutions to the quick exercises,seg_341,"25.1 one is interested in whether dry drilling is faster than wet drilling. hence if we reject h0 : µ1 = µ2, we would like to conclude that the drill time is smaller for dry drilling than for wet drilling. since µ1 and µ2 represent the drill time for dry and wet drilling, we should choose h1 : µ1 < µ2."
2727,1,"['unbiased estimator', 'estimator', 'unbiased']", Solutions to the quick exercises,seg_341,"25.2 the value of x̄5 is at least 3 and if we find a value of x̄5 that is larger than 348, then at least one of the five numbers must be greater than 350, so that we immediately know that h0 as well as h1 is false. hence the possible values of x̄5 that are relevant for our testing problem are between 3 and 348. we know from section 20.1 that 2x̄5 − 1 is an unbiased estimator for n , no matter what the value of n is. this implies that values of x̄5 itself are centered around (n + 1)/2. hence values close to 351/2=175.5 are in favor of h0, whereas values close to 3 are in favor of h1. values close to 348 are against h0, but also against h1. see figure 25.3."
2728,1,[], Solutions to the quick exercises,seg_341,25.3 the p-value corresponding to 61 is now equal to
2729,0,[], Solutions to the quick exercises,seg_341,"if h0 is true, then in 24.75% of the time one will observe a value t less than or equal to 61. such values are not exceptionally small for t under h0, and therefore the evidence that the value 61 bears against h0 is pretty weak. we cannot reject h0 beyond reasonable doubt."
2730,1,"['type i error', 'error', 'probability', 'associated']", Solutions to the quick exercises,seg_341,25.4 the type i error associated with the decision rule occurs if n = 350 (h0 is true) and t ≤ 250 (reject h0). the probability that this happens is
2731,1,"['alternative hypothesis', 'sample', 'tests', 'random sample', 'variation', 'distribution', 'random', 'variance', 'null hypothesis', 'hypothesis']", Exercises,seg_343,"25.1 in a study about train delays in the netherlands one was interested in whether arrival delays of trains exhibit more variation during rush hours than during quiet hours. the observed arrival delays during rush hours are modeled as realizations of a random sample from a distribution with variance σ12, and similarly the observed arrival delays during quiet hours correspond to a distribution with variance σ22. one tests the null hypothesis h0 : σ1 = σ2. what do you choose as the alternative hypothesis?"
2732,1,"['poisson', 'alternative hypothesis', 'poisson random variable', 'random variable', 'variable', 'parameter', 'random', 'test', 'average', 'hypothesis']", Exercises,seg_343,"25.2 on average, the number of babies born in cleveland, ohio, in the month of september is 1472. on january 26, 1977, the city was immobilized by a blizzard. nine months later, in september 1977, the recorded number of births was 1718. can the increase of 246 be attributed to chance? to investigate this, the number of births in the month of september is modeled by a poisson random variable with parameter µ, and we test h0 : µ = 1472. what would you choose as the alternative hypothesis?"
2733,1,"['alternative hypothesis', 'parameters', 'scatterplot', 'regression', 'regression line', 'null hypothesis', 'hypothesis']", Exercises,seg_343,25.3 recall exercise 17.9 about black cherry trees. the scatterplot of y (volume) versus x = d2h (squared diameter times height) seems to indicate that the regression line y = α + βx runs through the origin. one wants to investigate whether this is true by means of a testing problem. formulate a null hypothesis and alternative hypothesis in terms of (one of) the parameters α and β.
2734,1,"['random samples', 'geometric distribution', 'distribution', 'samples', 'parameter', 'random', 'geometric', 'distributions']", Exercises,seg_343,"25.4 consider the example from section 4.4 about the number of cycles up to pregnancy of smoking and nonsmoking women. suppose the observed number of cycles are modeled as realizations of random samples from geometric distributions. let p1 be the parameter of the geometric distribution corresponding to smoking women and p2 be the parameter for the nonsmoking women. we are interested in whether p1 is different from p2, and we investigate this by testing h0 : p1 = p2 against h1 : p1 = p2."
2735,1,"['test', 'data']", Exercises,seg_343,"a. if the data are as given in exercise 17.5, what would you choose as a test"
2736,1,"['test statistic', 'statistic', 'test']", Exercises,seg_343,"b. what would you choose as a test statistic, if you were given the extra"
2737,1,['table'], Exercises,seg_343,knowledge as in table 21.1?
2738,0,[], Exercises,seg_343,c. suppose we are interested in whether smoking women are less likely to get
2739,1,"['alternative hypothesis', 'case', 'hypothesis']", Exercises,seg_343,pregnant than nonsmoking women. what is the appropriate alternative hypothesis in this case?
2740,1,"['sample', 'random', 'dataset', 'uniform distribution', 'distribution', 'random sample', 'test', 'realization']", Exercises,seg_343,"25.5 suppose a dataset is a realization of a random sample x1, x2, . . . , xn from a uniform distribution on [0, θ], for some (unknown) θ > 0. we test h0 : θ = 5 versus h1 : θ = 5."
2741,1,"['test statistic', 'statistic', 'test']", Exercises,seg_343,"a. we take t1 = max{x1, x2, . . . , xn} as our test statistic. specify what"
2742,0,[], Exercises,seg_343,"the (relevant) possible values are for t and which are in favor of h0 and which are in favor of h1. for instance, make a picture like figure 25.1."
2743,1,"['test statistic', 'statistic', 'test']", Exercises,seg_343,"b. same as a, but now for test statistic t2 = |2x̄n − 5|."
2744,1,"['tail probability', 'test statistic', 'case', 'right tail probability', 'sampling', 'distribution', 'statistic', 'probability', 'continuous', 'tail', 'test', 'sampling distribution', 'null hypothesis', 'hypothesis']", Exercises,seg_343,"25.6 to test a certain null hypothesis h0 one uses a test statistic t with a continuous sampling distribution. one agrees that h0 is rejected if one observes a value t of the test statistic for which (under h0) the right tail probability p(t ≥ t) is smaller than or equal to 0.05. given below are different values t and a corresponding left or right tail probability (under h0). specify for each case what the p-value is, if possible, and whether we should reject h0."
2745,1,"['poisson', 'poisson random variable', 'random variable', 'variable', 'parameter', 'random', 'test']", Exercises,seg_343,"25.7 (exercise 25.2 continued). the number of births in september is modeled by a poisson random variable t with parameter µ, which represents the expected number of births. suppose that one uses t to test the null hypothesis h0 : µ = 1472 and that one decides to reject h0 on the basis of observing the value t = 1718."
2746,0,[], Exercises,seg_343,a. in which direction do values of t provide evidence against h0 (and in
2747,1,[], Exercises,seg_343,"b. compute the p-value corresponding to t = 1718, where you may use the"
2748,1,['distribution'], Exercises,seg_343,"fact that the distribution of t can be approximated by an n(µ, µ) distribution."
2749,1,"['statistic', 'random', 'random sample', 'standard normal distribution', 'null hypothesis', 'sample', 'dataset', 'standard', 'standard normal', 'test statistic', 'distribution', 'test', 'hypothesis', 'normal', 'normal distribution']", Exercises,seg_343,25.8 suppose we want to test the null hypothesis that our dataset is a realization of a random sample from a standard normal distribution. as test statistic we use the kolmogorov-smirnov distance between the empirical distribution
2750,1,"['distribution function', 'data', 'distribution', 'standard', 'function']", Exercises,seg_343,function fn of the data and the distribution function φ of the standard normal:
2751,1,"['null hypothesis', 'hypothesis']", Exercises,seg_343,what are the possible values of t and in which direction do values of t deviate from the null hypothesis?
2752,1,"['estimated', 'exponential distribution', 'distribution function', 'empirical distribution function', 'data', 'distribution', 'exponential', 'function']", Exercises,seg_343,"25.9 recall the example from section 18.3, where we investigated whether the software data are exponential by means of the kolmogorov-smirnov distance between the empirical distribution function fn of the data and the estimated exponential distribution function:"
2753,1,"['bootstrap', 'data', 'simulated', 'parametric bootstrap']", Exercises,seg_343,for the data we found tks = 0.176. by means of a new parametric bootstrap we simulated 100 000 realizations of tks and found that all of them are smaller than 0.176. what can you say about the p-value corresponding to 0.176?
2754,1,"['statistic', 'measurements', 'normal distribution', 'random', 'random sample', 'null hypothesis', 'sample', 'dataset', 'data', 'standard', 'standard deviation', 'test statistic', 'distribution', 'expectation', 'test', 'hypothesis', 'deviation', 'table', 'normal', 'realization']", Exercises,seg_343,"25.10 consider the coal data from table 23.1, where 23 gross calorific value measurements are listed for osterfeld coal coded 262de27. we modeled this dataset as a realization of a random sample from a normal distribution with expectation µ unknown and standard deviation 0.1 mj/kg. we are planning to buy a shipment if the gross calorific value exceeds 23.75 mj/kg. in order to decide whether this is sensible, we test the null hypothesis h0 : µ = 23.75 with test statistic x̄n."
2755,1,"['alternative hypothesis', 'hypothesis']", Exercises,seg_343,a. what would you choose as the alternative hypothesis?
2756,1,['dataset'], Exercises,seg_343,"b. for the dataset x̄n is 23.788. compute the corresponding p-value, using"
2757,1,"['distribution', 'null hypothesis', 'hypothesis']", Exercises,seg_343,"that x̄n has an n(23.75, (0.1)2/23) distribution under the null hypothesis."
2758,1,"['test statistic', 'distribution', 'statistic', 'probability', 'random', 'test', 'type i error', 'error', 'realization']", Exercises,seg_343,"25.11 one is given a number t, which is the realization of a random variable t with an n(µ, 1) distribution. to test h0 : µ = 0 against h1 : µ = 0, one uses t as the test statistic. one decides to reject h0 in favor of h1 if |t| ≥ 2. compute the probability of committing a type i error."
2759,1,"['alternative hypothesis', 'type ii error', 'error', 'test statistic', 'null hypothesis', 'significance level', 'statistic', 'hypotheses', 'probability', 'type ii', 'level', 'significance', 'test', 'type i error', 'critical region', 'hypothesis']", Testing hypotheses elaboration,seg_345,"in the previous chapter we introduced the setup for testing a null hypothesis against an alternative hypothesis using a test statistic t . the notions of type i error and type ii error were introduced. a type i error occurs when we falsely reject h0 on the basis of the observed value of t , whereas a type ii error occurs when we falsely do not reject h0. the decision to reject h0 or not was based on the size of the p-value. in this chapter we continue the introduction of basic concepts of testing hypotheses, such as significance level and critical region, and investigate the probability of committing a type ii error."
2760,1,"['sample', 'test statistic', 'random sample', 'statistic', 'set', 'measurements', 'random', 'level', 'test', 'average', 'limit']", Significance level,seg_347,"as mentioned in the previous chapter, there is no general rule that specifies a level below which the p-value is considered exceptionally small. however, there are situations where this level is set a priori, and the question is: which values of the test statistic should then lead to rejection of h0? to illustrate this, consider the following example. the speed limit on freeways in the netherlands is 120 kilometers per hour. a device next to freeway a2 between amsterdam and utrecht measures the speed of passing vehicles. suppose that the device is designed in such a way that it conducts three measurements of the speed of a passing vehicle, modeled by a random sample x1, x2, x3. on the basis of the value of the average x̄3, the driver is either fined for speeding or not. for what values of x̄3 should we fine the driver, if we allow that 5% of the drivers are fined unjustly?"
2761,1,['measurement'], Significance level,seg_347,let us rephrase things in terms of a testing problem. each measurement can be thought of as
2762,1,"['measurement error', 'measurement', 'error']", Significance level,seg_347,measurement = true speed + measurement error.
2763,1,"['measurement', 'moment', 'random variable', 'variable', 'mean', 'measuring', 'random', 'measurement error', 'error']", Significance level,seg_347,"suppose for the moment that the measuring device is carefully calibrated, so that the measurement error is modeled by a random variable with mean zero"
2764,1,"['case', 'measurements', 'random sample', 'random', 'sample', 'random variable', 'parameter', 'error', 'distribution', 'variance', 'measurement', 'variable', 'normal', 'measurement error', 'experiments', 'normal distribution']", Significance level,seg_347,"and known variance σ2, say σ2 = 4. moreover, in physical experiments such as this one, the measurement error is often modeled by a random variable with a normal distribution. in that case, the measurements x1, x2, x3 are modeled by a random sample from an n(µ, 4) distribution, where the parameter µ represents the true speed of the passing vehicle. our testing problem can now be formulated as testing"
2765,1,"['test statistic', 'statistic', 'test']", Significance level,seg_347,with test statistic x1 + x2 + x3
2766,1,"['random variables', 'independent', 'variables', 'distribution', 'normal', 'random']", Significance level,seg_347,"since sums of independent normal random variables again have a normal distribution (see remark 11.2), it follows that x̄3 has an n(µ, 4/3) distribution. in particular, the distribution of t = x̄3 is centered around µ no matter what the value of µ is. values of t close to 120 are therefore in favor of h0. values of t that are far from 120 are considered as strong evidence against h0. values much larger than 120 suggest that µ > 120 and are therefore in favor of h1. values much smaller than 120 suggest that µ < 120. they also constitute evidence against h0, but even stronger evidence against h1. thus we reject h0 in favor of h1 only for values of t larger than 120. see also figure 26.1."
2767,1,"['set', 'type i error', 'probability', 'level', 'significance', 'significance level', 'error']", Significance level,seg_347,"rejection of h0 in favor of h1 corresponds to fining the driver for speeding. unjustly fining a driver corresponds to falsely rejecting h0, i.e., committing a type i error. since we allow 5% of the drivers to be fined unjustly, we are dealing with a testing problem where the probability of committing a type i error is set a priori at 0.05. the question is: for which values of t should we reject h0? the decision rule for rejecting h0 should be such that the corresponding probability of committing a type i error is 0.05. the value 0.05 is called the significance level."
2768,1,"['type i error', 'probability', 'level', 'significance', 'significance level', 'error']", Significance level,seg_347,"significance level. the significance level is the largest acceptable probability of committing a type i error and is denoted by α, where 0 < α < 1."
2769,1,"['level', 'test']", Significance level,seg_347,"we speak of “performing the test at level α,” as well as “rejecting h0 in favor of h1 at level α.” in our example we are testing h0 : µ = 120 against h1 : µ > 120 at level 0.05."
2770,1,['level'], Significance level,seg_347,"quick exercise 26.1 suppose that in the freeway example h0 : µ = 120 is rejected in favor of h1 : µ > 120 at level α = 0.05. will it necessarily be rejected at level α = 0.01? on the other hand, suppose that h0 : µ = 120 is rejected in favor of h1 : µ > 120 at level α = 0.01. will it necessarily be rejected at level α = 0.05?"
2771,1,"['distribution', 'random variable', 'variable', 'normal', 'probability', 'random', 'level', 'average', 'type i error', 'error', 'normal distribution']", Significance level,seg_347,"let us continue with our example and determine for which values of t = x̄3 we should reject h0 at level α = 0.05 in favor of h1 : µ > 120. suppose we decide to fine each driver whose recorded average speed is 121 or more, i.e., we reject h0 whenever t ≥ 121. then how large is the probability of a type i error p(t ≥ 121)? when h0 : µ = 120 is true, then t = x̄3 has an n(120, 4/3) distribution, so that by the change-of-units rule for the normal distribution (see page 106), the random variable"
2772,1,['distribution'], Significance level,seg_347,"has an n(0, 1) distribution. this implies that"
2773,1,"['table', 'case', 'type i error', 'probability', 'level', 'significance', 'average', 'significance level', 'error']", Significance level,seg_347,"from table b.1, we find p(z ≥ 0.87) = 0.1922, which means that the probability of a type i error is greater than the significance level α = 0.05. since this level was defined as the largest acceptable probability of a type i error, we do not reject h0. similarly, if we decide to reject h0 whenever we record an average of 122 or more, the probability of a type i error equals 0.0416 (check this). this is smaller than α = 0.05, so in that case we reject h0. the boundary case is the value c that satisfies p(t ≥ c) = 0.05. to find c, we must"
2774,1,['table'], Significance level,seg_347,"from table b.2 we have that z0.05 = t∞,0.05 = 1.645, so that we find"
2775,0,[], Significance level,seg_347,which leads to 2
2776,1,"['set', 'level', 'significance', 'average', 'significance level']", Significance level,seg_347,"hence, if we set the significance level α at 0.05, we should reject h0 : µ = 120 in favor of h1 : µ > 120 whenever t ≥ 121.9. for our freeway example this means that if the average recorded speed of a passing vehicle is greater than or equal to 121.9, then the driver is fined for speeding. with this decision rule, at most 5% of the drivers get fined unjustly."
2777,1,"['level', 'significance', 'significance level']", Significance level,seg_347,"in connection with p-values: the significance level is the level below which the p-value is sufficiently small to reject h0. indeed, for any observed value t ≥ 121.9 we reject h0, and the p-value for such a t is at most 0.05:"
2778,0,[], Significance level,seg_347,we will see more about this relation in the next section.
2779,1,"['critical region', 'test statistic', 'case', 'set', 'statistic', 'level', 'significance', 'test', 'significance level', 'critical value']", Critical region and critical values,seg_349,"in the freeway example the significance level 0.05 corresponds to the decision rule “reject h0 : µ = 120 in favor h1 : µ > 120 whenever t ≥ 121.9.” the set k = [121.9, ∞) consisting of values of the test statistic t for which we reject h0 is called critical region. the value 121.9, which is the boundary case between rejecting and not rejecting h0, is called the critical value."
2780,1,"['critical values', 'critical region', 'test statistic', 'statistic', 'set', 'level', 'significance', 'test', 'significance level']", Critical region and critical values,seg_349,critical region and critical values. suppose we test h0 against h1 at significance level α by means of a test statistic t . the set k ⊂ r that corresponds to all values of t for which we reject h0 in favor of h1 is called the critical region. values on the boundary of the critical region are called critical values.
2781,1,"['test statistic', 'statistic', 'probability', 'level', 'significance', 'test', 'significance level', 'critical region']", Critical region and critical values,seg_349,the precise shape of the critical region depends on both the chosen significance level α and the test statistic t that is used. but it will always be such that the probability that t ∈ k satisfies
2782,1,['case'], Critical region and critical values,seg_349,p(t ∈ k) ≤ α in the case that h0 is true.
2783,1,"['probabilities', 'probability']", Critical region and critical values,seg_349,"at this point it becomes important to emphasize whether probabilities are computed under the assumption that h0 is true. with a slight abuse of notation, we briefly write p(t ∈ k | h0) for the probability."
2784,1,[], Critical region and critical values,seg_349,relation with p-values
2785,1,"['tail probability', 'right tail probability', 'probability', 'tail', 'average', 'critical region']", Critical region and critical values,seg_349,"if we record average speed t = 124, then this value falls in the critical region k = [121.9, ∞), so that h0 : µ = 120 is rejected in favor h1 : µ > 120. on the other hand we can also compute the p-value corresponding to the observed value 124. since values of t to the right provide stronger evidence against h0, the p-value is the following right tail probability"
2786,1,"['level', 'significance', 'significance level']", Critical region and critical values,seg_349,which is smaller than the significance level 0.05. this is no coincidence.
2787,1,"['level', 'test statistic', 'statistic', 'test']", Critical region and critical values,seg_349,"in general, suppose that we perform a test at level α using test statistic t and that we have observed t as the value of our test statistic. then"
2788,1,[], Critical region and critical values,seg_349,t ∈ k ⇔ the p-value corresponding to t is less than or equal to α.
2789,1,"['observed significance level', 'tail probability', 'significance', 'case', 'right tail probability', 'probability', 'level', 'tail', 'significance level']", Critical region and critical values,seg_349,"figure 26.2 illustrates this for a testing problem where values of t to the right provide evidence against h0 and in favor of h1. in that case, the p-value corresponds to the right tail probability p(t ≥ t | h0). the shaded area to the right of cα corresponds to α = p(t ≥ cα | h0), whereas the more intensely shaded area to the right of t represents the p-value. we see that deciding whether to reject h0 at a given significance level α can be done by comparing either t with cα or the p-value with α. for this reason the p-value is sometimes called the observed significance level."
2790,1,"['critical values', 'dataset', 'critical region', 'test statistic', 'statistic', 'level', 'test', 'critical value']", Critical region and critical values,seg_349,"the concepts of critical value and p-value have their own merit. the critical region and the corresponding critical values specify exactly what values of t lead to rejection of h0 at a given level α. this can be done even without obtaining a dataset and computing the value t of the test statistic. the pvalue, on the other hand, represents the strength of the evidence the observed value t bears against h0. but it does not specify all values of t that lead to rejection of h0 at a given level α."
2791,1,"['tail probability', 'case', 'set', 'tail', 'probability', 'level', 'significance', 'average', 'significance level', 'critical region']", Critical region and critical values,seg_349,"quick exercise 26.2 in our freeway example, we have already computed the relevant tail probability to decide whether a person with recorded average speed t = 124 gets fined if we set the significance level at 0.05. suppose the significance level is set at α = 0.01 (we allow 1% of the drivers to get fined unjustly). determine whether a person with recorded average speed t = 124 gets fined (h0 : µ = 120 is rejected). furthermore, determine the critical region in this case."
2792,1,"['distribution', 'critical region', 'discrete']", Critical region and critical values,seg_349,"sometimes the critical region k can be constructed such that p(t ∈ k | h0) is exactly equal to α, as in the freeway example. however, when the distribution of t is discrete, this is not always possible. this is illustrated by the next example."
2793,1,"['distribution', 'test', 'probability']", Critical region and critical values,seg_349,"after the introduction of the euro, polish mathematicians claimed that the belgian 1 euro coin is not a fair coin (see, for instance, the new scientist, january 4, 2002). suppose we put a 1 euro coin to the test. we will throw it ten times and record x , the number of heads. then x has a bin(10, p) distribution, where p denotes the probability of heads. we like to find out whether p differs from 1/2. therefore we test"
2794,1,"['test statistic', 'set', 'statistic', 'level', 'significance', 'test', 'significance level']", Critical region and critical values,seg_349,"we use x as the test statistic. when we set the significance level α at 0.05, for what values of x will we reject h0 and conclude that the coin is not fair? let us first find out what values of x are in favor of h1. if h0 : p = 1/2 is"
2795,0,[], Critical region and critical values,seg_349,"2 values close to 10 suggest that p > 1/2 and values close to 0 suggest that p < 1/2. hence, both values close to 0 and values close to 10 are in favor of h1 : p = 1/2."
2796,1,"['critical region', 'set']", Critical region and critical values,seg_349,"this means that we will reject h0 in favor of h1 whenever x ≤ cl or x ≥ cu. therefore, the critical region is the set"
2797,1,"['critical values', 'right critical values', 'critical region']", Critical region and critical values,seg_349,the boundary values cl and cu are called left and right critical values. they must be chosen such that the critical region k is as large as possible and still satisfies
2798,1,['probability'], Critical region and critical values,seg_349,"1) denotes the probability p(x ≥ cu) computed with x having a bin(10, 1"
2799,1,['distribution'], Critical region and critical values,seg_349,"2 ) distribution. since we have no preference for rejecting h0 for values close to 0 or close to 10, we divide 0.05 over the two sides, and we choose cl as large as possible and cu as small as possible such that"
2800,1,"['tail', 'tail probabilities', 'probabilities']", Critical region and critical values,seg_349,"the left tail probabilities of the bin(10, 1"
2801,1,['distribution'], Critical region and critical values,seg_349,"2 ) distribution are listed in table 26.1. we immediately see that cl = 1 is the largest value such that p(x ≤ cl | p = 1/2) ≤ 0.025. similarly, cu = 9 is the smallest value such that p(x ≥ cu | p = 1/2) ≤ 0.025. indeed, when x has a bin(10, 1"
2802,1,['distribution'], Critical region and critical values,seg_349,"2 ) distribution,"
2803,1,"['error', 'set', 'level', 'test', 'type i error', 'critical region']", Critical region and critical values,seg_349,"hence, if we test h0 : p = 1/2 against h1 : p = 1/2 at level α = 0.05, the critical region is the set k = {0, 1, 9, 10}. the corresponding type i error is"
2804,1,"['level', 'significance', 'significance level', 'critical region']", Critical region and critical values,seg_349,which is smaller than the significance level. you may perform ten throws with your favorite coin and see whether the number of heads falls in the critical region.
2805,1,"['probabilities', 'test statistic', 'statistic', 'tail probabilities', 'level', 'tail', 'test', 'critical region']", Critical region and critical values,seg_349,quick exercise 26.3 recall the tank example where we tested h0 : n = 350 against h1 : n < 350 by means of the test statistic t = maxxi. suppose that we perform the test at level 0.05. deduce the critical region k corresponding to level 0.05 from the left tail probabilities given here:
2806,1,[], Critical region and critical values,seg_349,one- and two-tailed p-values
2807,1,[], Critical region and critical values,seg_349,"in the euro coin example, we deviate from h0 : p = 1/2 in two directions: values of x both far to the right and far to the left of 5 are evidence against h0. suppose that in ten throws with the 1 euro coin we recorded x heads. what would the p-value be corresponding to x? the problem is that the direction in which values of x are at least as extreme as the observed value x depends on whether x lies to the right or to the left of 5."
2808,1,"['critical values', 'tail probability', 'right tail probability', 'tail', 'probability', 'level', 'significance', 'significance level']", Critical region and critical values,seg_349,"at this point there are two natural solutions. one may report the appropriate left or right tail probability, which corresponds to the direction in which x deviates from h0. for instance, if x lies to the right of 5, we compute p(x ≥ x | h0). this is called a one-tailed p-value. the disadvantage of onetailed p-values is that they are somewhat misleading about how strong the evidence of the observed value x bears against h0. in view of the relation between rejection on the basis of critical values or on the basis of a p-value, the one-tailed p-value should be compared to α/2. on the other hand, since people are inclined to compare p-values with the significance level α itself, one could also double the one-tailed p-value and compare this with α. this double-tail probability is called a two-tailed p-value. it doesn’t make much of a difference, as long as one also reports whether the reported p-value is one-tailed or two-tailed."
2809,1,[], Critical region and critical values,seg_349,"let us illustrate things by means of the findings by the polish mathematicians. they performed 250 throws with a belgian 1 euro coin and recorded heads 140 times (see also exercise 24.2). the question is whether this provides strong enough evidence against h0 : p = 1/2. the observed value 140 is to the right of 125, the value we would expect if h0 is true. hence the one-tailed p-value is p(x ≥ 140), where now x has a bin(250, 1"
2810,1,"['normal approximation', 'approximation', 'distribution', 'normal']", Critical region and critical values,seg_349,"2 ) distribution. by means of the normal approximation (see page 201), we find"
2811,1,['statistical'], Critical region and critical values,seg_349,"therefore the two-tailed p-value is approximately 0.0574, which does not provide very strong evidence against h0. in fact, the exact two-tailed p-value, computed by means of statistical software, is 0.066, which is even larger."
2812,1,"['probabilities', 'case']", Critical region and critical values,seg_349,"quick exercise 26.4 in a dutch newspaper (de telegraaf, january 3, 2002) it was reported that the polish mathematicians recorded heads 150 times. what are the oneand two-tailed probabilities is this case? do they now have a case?"
2813,1,"['control', 'type i error', 'probability', 'level', 'significance', 'average', 'significance level', 'error']", Type II error,seg_351,"as we have just seen, by setting a significance level α, we are able to control the probability of committing a type i error; it will at most be α. for instance, let us return to the freeway example and suppose that we adopt the decision rule to fine the driver for speeding if her average observed speed is at least 121.9, i.e.,"
2814,1,"['type ii error', 'percentage', 'probability', 'type ii', 'average', 'type i error', 'error']", Type II error,seg_351,"from section 26.1 we know that with this decision rule, the probability of a type i error is 0.05. what is the probability of committing a type ii error? this corresponds to the percentage of drivers whose true speed is above 120 but who do not get fined because their recorded average speed is below 121.9."
2815,1,"['type ii error', 'distribution', 'type ii', 'probability', 'error']", Type II error,seg_351,"for instance, suppose that a car passes at true speed µ = 125. a type ii error occurs when t < 121.9, and since t = x̄3 has an n(125, 4/3) distribution, the probability that this happens is"
2816,1,"['type ii error', 'case', 'type ii', 'probability', 'error']", Type II error,seg_351,"this looks promising, but now consider a vehicle passing at true speed µ = 123. the probability of committing a type ii error in this case is"
2817,1,"['curve', 'error', 'distribution', 'probability', 'type i error', 'null hypothesis', 'hypothesis']", Type II error,seg_351,"hence 17.11% of all drivers that pass at speed µ = 123 will not get fined. in figure 26.3 the last situation is illustrated. the curve on the left represents the probability density of the n(120, 4/3) distribution, which is the distribution of t under the null hypothesis. the shaded area on the right of 121.9 represents the probability of committing a type i error"
2818,1,"['type ii error', 'curve', 'distribution', 'probability', 'type ii', 'error']", Type II error,seg_351,"the curve on the right is the probability density of the n(123, 4/3) distribution, which is the distribution of t under the alternative µ = 123. the shaded area on the left of 121.9 represents the probability of a type ii error"
2819,1,"['type ii error', 'type ii', 'error', 'probability']", Type II error,seg_351,"shifting µ further to the right will result in a smaller probability of a type ii error. however, shifting µ toward the value 120 leads to a larger probability of a type ii error. in fact it can be arbitrarily close to 0.95."
2820,1,"['alternative hypothesis', 'type ii error', 'contrast', 'probability', 'type ii', 'type i error', 'error', 'hypothesis']", Type II error,seg_351,"the previous example illustrates that the probability of committing a type ii error depends on the actual value of µ in the alternative hypothesis h1 : µ > 120. the closer µ is to 120, the higher the probability of a type ii error will be. in contrast with the probability of a type i error, which is always at most α, the probability of a type ii error may be arbitrarily close to 1− α. this is illustrated in the next quick exercise."
2821,1,"['type ii error', 'type ii', 'error', 'probability']", Type II error,seg_351,quick exercise 26.5 what is the probability of a type ii error in the freeway example if µ = 120.1?
2822,1,"['level', 'critical value']", Relation with confidence intervals,seg_353,"when testing h0 : µ = 120 against h1 : µ > 120 at level 0.05 in the freeway example, the critical value was obtained by the formula"
2823,1,"['distribution', 'confidence', 'case', 'confidence bound']", Relation with confidence intervals,seg_353,"on the other hand, using that x̄3 has an n(µ, 4/3) distribution, a 95% lower confidence bound for µ in this case can be derived from"
2824,1,"['dataset', 'hypotheses', 'confidence', 'statistical']", Relation with confidence intervals,seg_353,"although, at first sight, testing hypotheses and constructing confidence intervals seem to be two separate statistical procedures, they are in fact intimately related. in the freeway example, observe that for a given dataset x1, x2, x3,"
2825,1,['level'], Relation with confidence intervals,seg_353,we reject h0 : µ = 120 in favor of h1 : µ > 120 at level 0.05 2 ⇔ x̄3 ≥ 120 + 1.645 · √3
2826,1,"['confidence', 'interval', 'confidence interval']", Relation with confidence intervals,seg_353,2 ⇔ x̄3 − 1.645 · ≥ 120 √3 ⇔ 120 is not in the 95% one-sided confidence interval for µ.
2827,1,"['test', 'parameter']", Relation with confidence intervals,seg_353,"this is not a coincidence. in general, the following applies. suppose that for some parameter θ we test h0 : θ = θ0. then"
2828,1,['level'], Relation with confidence intervals,seg_353,we reject h0 : θ = θ0 in favor of h1 : θ > θ0 at level α
2829,0,[], Relation with confidence intervals,seg_353,if and only if
2830,1,"['confidence', 'interval', 'confidence interval']", Relation with confidence intervals,seg_353,θ0 is not in the 100(1− α)% one-sided confidence interval for θ.
2831,1,"['confidence intervals', 'intervals', 'confidence']", Relation with confidence intervals,seg_353,"the same relation holds for testing against h1 : θ < θ0, and a similar relation holds between testing against h1 : θ = θ0 and two-sided confidence intervals:"
2832,1,['level'], Relation with confidence intervals,seg_353,we reject h0 : θ = θ0 in favor of h1 : θ0 = θ0 at level α
2833,0,[], Relation with confidence intervals,seg_353,if and only if
2834,1,['confidence'], Relation with confidence intervals,seg_353,θ0 is not in the 100(1− α)% two-sided confidence region for θ.
2835,1,"['set', 'parameter', 'level', 'confidence', 'null hypothesis', 'hypothesis']", Relation with confidence intervals,seg_353,"in fact, one could use these facts to define the 100(1−α)% confidence region for a parameter θ as the set of values θ0 for which the null hypothesis h0 : θ = θ0 is not rejected at level α."
2836,1,"['studentized mean', 'interval', 'statistic', 'random', 'null hypothesis', 'sample median', 'sample', 'random variable', 'mean', 'parameter', 'confidence', 'confidence interval', 'median', 'test statistic', 'distribution', 'test', 'hypothesis', 'variable']", Relation with confidence intervals,seg_353,"it should be emphasized that these relations only hold if the random variable that is used to construct the confidence interval relates appropriately to the test statistic. for instance, the preceding relations do not hold if on the one hand, we construct a confidence interval for the parameter µ of an n(µ, σ2) distribution by means of the studentized mean (x̄n−µ)/(sn/√n), and on the other hand, use the sample median medn to test a null hypothesis for µ."
2837,1,"['information', 'mean', 'type i error', 'probability', 'level', 'significance', 'significance level', 'error']", Solutions to the quick exercises,seg_355,"26.1 in the first situation, we reject at significance level α = 0.05, which means that the probability of committing a type i error is at most 0.05. this does not necessarily mean that this probability will also be less than or equal to 0.01. therefore with this information we cannot know whether we also reject at level α = 0.01. in the reversed situation, if we reject at level α = 0.01, then the probability of committing a type i error is at most 0.01, and is therefore also smaller than 0.05. this means that we also reject at level α = 0.05."
2838,1,"['level', 'significance', 'significance level']", Solutions to the quick exercises,seg_355,"26.2 to decide whether we should reject h0 : µ = 120 at level 0.01, we could compute p(t ≥ 124 | h0) and compare this with 0.01. we have already seen that p(t ≥ 124 | h0) = 0.0003. this is (much) smaller than the significance level α = 0.01, so we should reject."
2839,1,['critical region'], Solutions to the quick exercises,seg_355,"the critical region is k = [c,∞), where we must solve c from"
2840,1,[], Solutions to the quick exercises,seg_355,"since z0.01 = 2.326, this means that c = 120 + 2.326 · (2/√3) = 122.7."
2841,1,"['critical region', 'table']", Solutions to the quick exercises,seg_355,"26.3 the critical region is of the form k = {5, 6, . . . , c}, where the critical value c is the largest value, for which p(t ≤ c | h0) is still less than or equal to 0.05. from the table we immediately see that c = 193 and that p(t ∈ k | h0) = p(t ≤ 193 | h0) = 0.0498, which is not equal to 0.05."
2842,1,"['normal approximation', 'approximation', 'normal']", Solutions to the quick exercises,seg_355,"26.4 by means of the normal approximation, for the one-tailed p-value we find"
2843,1,['case'], Solutions to the quick exercises,seg_355,"the two-tailed p-value is 0.0016. this is a lot smaller than the two-tailed pvalue 0.0574, corresponding to 140 heads. it seems that with 150 heads the mathematicians would have a case; the belgian euro coin would then appear not to be fair."
2844,1,"['type ii error', 'type ii', 'error', 'probability']", Solutions to the quick exercises,seg_355,26.5 the probability of a type ii error is
2845,1,"['alternative hypothesis', 'type ii error', 'experienced', 'error', 'probabilities', 'method', 'estimate', 'table', 'results', 'hypothesis testing', 'type ii', 'type i error', 'null hypothesis', 'hypothesis']", Exercises,seg_357,"26.1 polygraphs that are used in criminal investigations are supposed to indicate whether a person is lying or telling the truth. however the procedure is not infallible, as is illustrated by the following example. an experienced polygraph examiner was asked to make an overall judgment for each of a total 280 records, of which 140 were from guilty suspects and 140 from innocent suspects. the results are listed in table 26.2. we view each judgment as a problem of hypothesis testing, with the null hypothesis corresponding to “suspect is innocent” and the alternative hypothesis to “suspect is guilty.” estimate the probabilities of a type i error and a type ii error that apply to this polygraph method on the basis of table 26.2."
2846,1,"['type ii error', 'type ii', 'probability', 'error']", Exercises,seg_357,26.2 consider the testing problem in exercise 25.11. compute the probability of committing a type ii error if the true value of µ is 1.
2847,1,"['distribution', 'uniform distribution', 'test', 'interval']", Exercises,seg_357,"26.3 one generates a number x from a uniform distribution on the interval [0, θ]. one decides to test h0 : θ = 2 against h1 : θ = 2 by rejecting h0 if x ≤ 0.1 or x ≥ 1.9."
2848,1,"['type i error', 'error', 'probability']", Exercises,seg_357,a. compute the probability of committing a type i error.
2849,1,"['type ii error', 'type ii', 'error', 'probability']", Exercises,seg_357,b. compute the probability of committing a type ii error if the true value
2850,1,['hypothesis'], Exercises,seg_357,"26.4 to investigate the hypothesis that a horse’s chances of winning an eighthorse race on a circular track are affected by its position in the starting lineup,"
2851,1,"['model', 'test statistic', 'distribution', 'random variable', 'variable', 'statistic', 'random', 'level', 'test', 'hypothesis']", Exercises,seg_357,"the starting position of each of 144 winners was recorded ([30]). it turned out that 29 of these winners had starting position one (closest to the rail on the inside track). we model the number of winners with starting position one by a random variable t with a bin(144, p) distribution. we test the hypothesis h0 : p = 1/8 against h1 : p > 1/8 at level α = 0.01 with t as test statistic."
2852,1,"['test', 'critical value', 'right critical value']", Exercises,seg_357,"a. argue whether the test procedure involves a right critical value, a left"
2853,0,[], Exercises,seg_357,"critical value, or both."
2854,1,"['normal approximation', 'approximation', 'normal']", Exercises,seg_357,b. use the normal approximation to compute the critical value(s) correspond-
2855,1,"['null hypothesis', 'critical region', 'hypothesis']", Exercises,seg_357,"ing to α = 0.01, determine the critical region, and report your conclusion about the null hypothesis."
2856,1,"['normal approximation', 'confidence intervals', 'statistic', 'random', 'null hypothesis', 'approximation', 'results', 'intervals', 'random variable', 'confidence', 'test statistic', 'distribution', 'test', 'hypothesis', 'table', 'variable', 'normal']", Exercises,seg_357,"26.5 recall exercises 23.5 and 24.8 about the 1500m speed-skating results in the 2002 winter olympic games. the number of races won by skaters starting in the outer lane is modeled by a random variable x with a bin(23, p) distribution. the question of whether there is an outer lane advantage was investigated in exercise 24.8 by means of constructing confidence intervals using the normal approximation. in this exercise we examine this question by testing the null hypothesis h0 : p = 1/2 against h1 : p > 1/2 using x as the test statistic. the distribution of x under h0 is given in table 26.3. out of 23 completed races, 15 were won by skaters starting in the outer lane."
2857,1,[], Exercises,seg_357,a. compute the p-value corresponding to x = 15 and report your conclusion
2858,1,"['interval', 'level', 'confidence', 'test', 'confidence interval']", Exercises,seg_357,if we perform the test at level 0.05. does your conclusion agree with the confidence interval you found for p in exercise 24.8b?
2859,1,"['level', 'significance', 'significance level', 'critical region']", Exercises,seg_357,b. determine the critical region corresponding to significance level α = 0.05.
2860,1,"['type i error', 'error', 'probability']", Exercises,seg_357,c. compute the probability of committing a type i error if we base our
2861,1,['critical region'], Exercises,seg_357,decision rule on the critical region determined in b.
2862,1,"['normal approximation', 'approximation', 'normal', 'probability']", Exercises,seg_357,d. use the normal approximation to determine the probability of committing
2863,1,"['type ii error', 'error', 'case', 'type ii', 'critical region']", Exercises,seg_357,"a type ii error for the case p = 0.6, if we base our decision rule on the critical region determined in b."
2864,1,"['level', 'test statistic', 'statistic', 'test']", Exercises,seg_357,26.6 consider exercises 25.2 and 25.7. one decides to test h0 : µ = 1472 against h1 : µ > 1472 at level α = 0.05 on the basis of the recorded value 1718 of the test statistic t .
2865,1,"['test', 'critical value', 'right critical value']", Exercises,seg_357,"a. argue whether the test procedure involves a right critical value, a left"
2866,0,[], Exercises,seg_357,"critical value, or both."
2867,1,['distribution'], Exercises,seg_357,"b. use the fact that the distribution of t can be approximated by an n(µ, µ)"
2868,1,"['critical region', 'null hypothesis', 'hypothesis']", Exercises,seg_357,"distribution to determine the critical value(s) and the critical region, and report your conclusion about the null hypothesis."
2869,1,"['sample', 'random', 'interval', 'uniform distribution', 'distribution', 'random sample', 'level', 'significance', 'test', 'level of significance', 'critical region']", Exercises,seg_357,"26.7 a random sample x1, x2 is drawn from a uniform distribution on the interval [0, θ]. we wish to test h0 : θ = 1 against h1 : θ < 1 by rejecting if x1 + x2 ≤ c. find the value of c and the critical region that correspond to a level of significance 0.05. hint: use exercise 11.5."
2870,1,"['alternative hypothesis', 'test statistic', 'statistic', 'test', 'critical region', 'hypothesis']", Exercises,seg_357,26.8 this exercise is meant to illustrate that the shape of the critical region is not necessarily similar to the type of alternative hypothesis. the type of alternative hypothesis and the test statistic used determine the shape of the critical region.
2871,1,"['sample', 'random', 'statistics', 'test statistics', 'random sample', 'test']", Exercises,seg_357,"suppose that x1, x2, . . . , xn form a random sample from an exp(λ) distribution, and we test h0 : λ = 1 with test statistics t = x̄n and t ′ = e−x̄n ."
2872,1,"['test', 'null hypothesis', 'hypothesis']", Exercises,seg_357,a. suppose we test the null hypothesis against h1 : λ > 1. determine for
2873,1,"['left critical value', 'test', 'critical value', 'right critical value']", Exercises,seg_357,"both test procedures whether they involve a right critical value, a left critical value, or both."
2874,1,['test'], Exercises,seg_357,"b. same question as in part a, but now test against h1 : λ = 1."
2875,1,"['sample', 'random', 'statistics', 'distribution', 'test statistics', 'random sample', 'test']", Exercises,seg_357,"26.9 similar to exercise 26.8, but with a random sample x1, x2, . . . , xn from an n(µ, 1) distribution. we test h0 : µ = 0 with test statistics t = (x̄n)2 and t ′ = 1/x̄n."
2876,1,"['test', 'null hypothesis', 'hypothesis']", Exercises,seg_357,a. suppose that we test the null hypothesis against h1 : µ = 0. determine
2877,1,"['test', 'critical region']", Exercises,seg_357,the shape of the critical region for both test procedures.
2878,1,['test'], Exercises,seg_357,"b. same question as in part a, but now test against h1 : µ > 0."
2879,1,"['simple linear regression', 'linear', 'statistical', 'linear regression model', 'model', 'regression model', 'regression', 'distribution', 'expectation', 'linear regression', 'statistical test', 'test', 'intercept', 'normality', 'normal', 'slope']", The ttest,seg_359,"in many applications the quantity of interest can be represented by the expectation of the model distribution. in some of these applications one wants to know whether this expectation deviates from some a priori specified value. this can be investigated by means of a statistical test, known as the t-test. we consider this test both under the assumption that the model distribution is normal and without the assumption of normality. furthermore, we discuss a similar test for the slope and the intercept in a simple linear regression model."
2880,1,"['sample', 'model', 'dataset', 'table', 'random sample', 'distribution', 'expected value', 'probability distribution', 'set', 'probability', 'random', 'parameter', 'average', 'realization']", Monitoring the production of ball bearings,seg_361,"a production line in a large industrial corporation are set to produce a specific type of steel ball bearing with a diameter of 1 millimeter. in order to check the performance of the production lines, a number of ball bearings are picked at the end of the day and their diameters are measured. suppose we observe 20 diameters of ball bearings from the production lines, which are listed in table 27.1. the average diameter is x̄20 = 1.03 millimeter. this clearly deviates from the target value 1, but the question is whether the difference can be attributed to chance or whether it is large enough to conclude that the production line is producing ball bearings with a wrong diameter. to answer this question, we model the dataset as a realization of a random sample x1, x2, . . . , x20 from a probability distribution with expected value µ. the parameter µ represents the diameter of ball bearings produced by the produc-"
2881,1,"['test', 'null hypothesis', 'hypothesis']", Monitoring the production of ball bearings,seg_361,"tion lines. in order to investigate whether this diameter deviates from 1, we test the null hypothesis h0 : µ = 1 against h1 : µ = 1."
2882,1,"['sample', 'test statistic', 'random sample', 'law of large numbers', 'data', 'distribution', 'statistic', 'expectation', 'random', 'test', 'null hypothesis', 'realization', 'hypothesis']", Monitoring the production of ball bearings,seg_361,"this example illustrates a situation that often occurs: the data x1, x2, . . . , xn are a realization of a random sample x1, x2, . . . , xn from a distribution with expectation µ, and we want to test whether µ equals an a priori specified value, say µ0. according to the law of large numbers, x̄n is close to µ for large n. this suggests a test statistic based on x̄n − µ0; realizations of x̄n − µ0 close to zero are in favor of the null hypothesis. does x̄n − µ0 suffice as a test statistic?"
2883,1,"['deviation', 'standard deviations', 'deviations', 'standard', 'standard deviation', 'null hypothesis', 'hypothesis']", Monitoring the production of ball bearings,seg_361,"in our example, x̄n −µ0 = 1.03− 1 = 0.03. should we interpret this as small? first, note that under the null hypothesis e[x̄n − µ0] = µ − µ0 = 0. now, if x̄n − µ0 would have standard deviation 1, then the value 0.03 is within one standard deviation of e[x̄n − µ0]. the “µ ± a few σ” rule on page 185 then suggests that the value 0.03 is not exceptional; it must be seen as a small deviation. on the other hand, if x̄n − µ0 has standard deviation 0.001, then the value 0.03 is 30 standard deviations away from e[x̄n − µ0]. according to the “µ ± a few σ” rule this is very exceptional; the value 0.03 must be seen as a large deviation. the next quick exercise provides a concrete example."
2884,1,"['case', 'random variable', 'variable', 'normal', 'expectation', 'probability', 'random', 'normal random variable', 'variance']", Monitoring the production of ball bearings,seg_361,"quick exercise 27.1 suppose that x̄n is a normal random variable with expectation 1 and variance 1. determine p(x̄n − 1 ≥ 0.03). find the same probability, but for the case where the variance is (0.01)2."
2885,1,['variation'], Monitoring the production of ball bearings,seg_361,this discussion illustrates that we must standardize x̄n − µ0 to incorporate its variation. recall that
2886,1,"['deviation', 'sample', 'test statistic', 'sample standard deviation', 'statistic', 'standard', 'standard deviation', 'test', 'variance', 'null hypothesis', 'hypothesis']", Monitoring the production of ball bearings,seg_361,"where σ2 is the variance of each xi. hence, standardizing x̄n − µ0 means that we should divide by σ/√n. since σ is unknown, we substitute the sample standard deviation sn for σ. this leads to the following test statistic for the null hypothesis h0 : µ = µ0:"
2887,0,[], Monitoring the production of ball bearings,seg_361,values of t close to zero are in favor of h0 : µ = µ0. large positive values of t suggest that µ > µ0 and large negative values suggest that µ < µ0; both are evidence against h0.
2888,1,['data'], Monitoring the production of ball bearings,seg_361,"for the ball bearing data one finds that sn = 0.0372, so that"
2889,0,[], Monitoring the production of ball bearings,seg_361,"this is clearly different from zero, but the question is whether this difference is large enough to reject h0 : µ = 1. to answer this question, we need to know"
2890,1,"['test statistic', 'distribution', 'probability distribution', 'statistic', 'probability', 'test', 'null hypothesis', 'hypothesis']", Monitoring the production of ball bearings,seg_361,"the probability distribution of t under the null hypothesis. note that under the null hypothesis h0 : µ = µ0, the test statistic"
2891,1,"['studentized mean', 'mean']", Monitoring the production of ball bearings,seg_361,is the studentized mean (see also chapter 23)
2892,1,"['studentized mean', 'distribution', 'probability distribution', 'mean', 'probability', 'null hypothesis', 'hypothesis']", Monitoring the production of ball bearings,seg_361,"hence, under the null hypothesis, the probability distribution of t is the same as that of the studentized mean."
2893,1,"['case', 'statistic', 'random sample', 'random', 'null hypothesis', 'sample', 'bootstrap', 'dataset', 'limit', 'test statistic', 'distribution', 'central limit theorem', 'test', 'hypothesis', 'normality', 'realization']", The onesample ttest,seg_363,"the classical assumption is that the dataset is a realization of a random sample from an n(µ, σ2) distribution. in that case our test statistic t turns out to have a t-distribution under the null hypothesis, as we will see later. for this reason, the test for the null hypothesis h0 : µ = µ0 is called the (one-sample) t-test. without the assumption of normality, we will use the bootstrap to approximate the distribution of t . for large sample sizes, this distribution can be approximated by means of the central limit theorem. we start with the first case."
2894,1,['data'], The onesample ttest,seg_363,normal data
2895,1,"['sample', 'studentized mean', 'dataset', 'test statistic', 'random sample', 'distribution', 'statistic', 'mean', 'random', 'level', 'test', 'null hypothesis', 'realization', 'hypothesis']", The onesample ttest,seg_363,"suppose that the dataset x1, x2, . . . , xn is a realization of a random sample x1, x2, . . . , xn from an n(µ, σ2) distribution. then, according to the rule on page 349, the studentized mean has a t(n − 1) distribution. an immediate consequence is that, under the null hypothesis h0 : µ = µ0, also our test statistic t has a t(n − 1) distribution. therefore, if we test h0 : µ = µ0 against h1 : µ = µ0 at level α, then we must reject the null hypothesis in favor of h1 : µ = µ0, if"
2896,1,"['table', 'data', 'level', 'test', 'null hypothesis', 'hypothesis']", The onesample ttest,seg_363,"similar decision rules apply to alternatives h1 : µ > µ0 and h1 : µ < µ0. suppose that in the ball bearing example we test h0 : µ = 1 against h1 : µ = 1 at level α = 0.05. from table b.2 we find t19,0.025 = 2.093. hence, we must reject if t ≤ −2.093 or t ≥ 2.093. for the ball bearing data we found t = 3.607, which means we reject the null hypothesis at level α = 0.05."
2897,1,"['tail probability', 'probability', 'tail']", The onesample ttest,seg_363,"alternatively, one might report the one-tailed p-value corresponding to the observed value t and compare this with α/2. the one-tailed p-value is either a right or a left tail probability, which must be computed by means"
2898,1,"['tail probability', 'table', 'right tail probability', 'data', 'distribution', 'probability', 'tail', 'statistical', 'null hypothesis', 'hypothesis']", The onesample ttest,seg_363,"of the t(n − 1) distribution. in our ball bearing example the one-tailed pvalue is the right tail probability p(t ≥ 3.607). from table b.2 we see that this probability is between 0.0005 and 0.0010, which is smaller than α/2 = 0.025 (to be precise, by means of a statistical software package we found p(t ≥ 3.607) = 0.00094). the data provide strong enough evidence against the null hypothesis, so that it seems sensible to adjust the settings of the production line."
2899,1,"['deviation', 'table', 'data', 'normality', 'measurements', 'standard', 'level', 'standard deviation', 'average']", The onesample ttest,seg_363,"quick exercise 27.2 suppose that the data in table 27.1 are from two separate production lines. the first ten measurements have average 1.0194 and standard deviation 0.0290, whereas the last ten measurements have average 1.0406 and standard deviation 0.0428. perform the t-test h0 : µ = 1 against h1 : µ = 1 at level α = 0.01 for both datasets separately, assuming normality."
2900,1,['data'], The onesample ttest,seg_363,nonnormal data
2901,0,[], The onesample ttest,seg_363,"draw a rectangle with height h and width w (let us agree that w > h), and within this rectangle draw a square with sides of length h (see figure 27.1). this creates another (smaller) rectangle with horizontal and vertical sides of"
2902,0,[], The onesample ttest,seg_363,"lengths w−h and h. a large rectangle with a vertical-to-horizontal ratio that is equal to the horizontal-to-vertical ratio for the small rectangle, i.e.,"
2903,0,[], The onesample ttest,seg_363,"was called a “golden rectangle” by the ancient greeks, who often used these in their architecture. after solving for h/w, we obtain that the height-to-width"
2904,1,"['data', 'table']", The onesample ttest,seg_363,ratio h/w is equal to the “golden number” (√5 − 1)/2 ≈ 0.618. the data in table 27.2 represent corresponding h/w ratios for rectangles used by shoshoni indians to decorate their leather goods. is it reasonable to assume that they were also using golden rectangles? we examine this by means of a t-test.
2905,1,"['sample', 'random sample', 'distribution', 'expectation', 'parameter', 'random', 'test', 'realization']", The onesample ttest,seg_363,"the observed ratios are modeled as a realization of a random sample from a distribution with expectation µ, where the parameter µ represents the true esthetic preference for height-to-width ratios of the shoshoni indians. we want to test"
2906,1,"['test statistic', 'statistic', 'test']", The onesample ttest,seg_363,"for the shoshoni ratios, x̄n = 0.6605 and sn = 0.0925, so that the value of the test statistic is"
2907,1,"['model', 'test statistic', 'data', 'distribution', 'statistic', 'normal', 'test', 'null hypothesis', 'normal distribution', 'hypothesis']", The onesample ttest,seg_363,"closer examination of the data indicates that the normal distribution is not the right model. for instance, by definition the height-to-width ratios h/w are always between 0 and 1. because some of the data points are also close to right boundary 1, the normal distribution is inappropriate. if we cannot assume a normal model distribution, we can no longer conclude that our test statistic has a t(n − 1) distribution under the null hypothesis."
2908,1,"['model', 'studentized mean', 'simulation', 'bootstrap', 'dataset', 'data', 'distribution', 'empirical bootstrap', 'mean']", The onesample ttest,seg_363,"since there is no reason to assume any other particular type of distribution to model the data, we approximate the distribution of t under the null hypothesis. recall that this distribution is the same as that of the studentized mean (see the end of section 27.1). to approximate its distribution, we use the empirical bootstrap simulation for the studentized mean, as described on page 351. we generate 10 000 bootstrap datasets and for each bootstrap dataset x∗1, x∗2, . . . , x∗n, we compute"
2909,1,"['critical values', 'level', 'bootstrap', 'estimate', 'kernel', 'kernel density estimate', 'distribution function', 'empirical distribution function', 'distribution', 'function', 'test']", The onesample ttest,seg_363,"in figure 27.2 the kernel density estimate and empirical distribution function are displayed for 10 000 bootstrap values t∗. suppose we test h0 : µ = 0.618 against h1 : µ = 0.618 at level α = 0.05. in the same way as in section 23.3, we find the following bootstrap approximations for the critical values:"
2910,1,"['tail probability', 'bootstrap', 'test statistic', 'approximation', 'right tail probability', 'data', 'statistic', 'hypothesis', 'probability', 'level', 'tail', 'test', 'null hypothesis']", The onesample ttest,seg_363,"since for the shoshoni data the value 2.055 of the test statistic is greater than 1.644, we reject the null hypothesis at level 0.05. alternatively, we can also compute a bootstrap approximation of the one-tailed p-value corresponding to 2.055, which is the right tail probability p(t ≥ 2.055). the bootstrap approximation for this probability is:"
2911,0,[], The onesample ttest,seg_363,"hence p(t ≥ 2.055) ≈ 0.0067, which is smaller than α/2 = 0.025. the value 2.055 should be considered as exceptionally large, and we reject the null hypothesis. the esthetic preference for height-to-width ratios of the shoshoni indians differs from that of the ancient greeks."
2912,1,['samples'], The onesample ttest,seg_363,large samples
2913,1,"['studentized mean', 'statistic', 'standard normal distribution', 'null hypothesis', 'sample', 'data', 'mean', 'standard', 'model', 'standard normal', 'test statistic', 'distribution', 'expectation', 'level', 'test', 'hypothesis', 'normal', 'normal distribution']", The onesample ttest,seg_363,"for large sample sizes the distribution of the studentized mean can be approximated by a standard normal distribution (see section 23.4). this means that for large sample sizes the distribution of the t-test statistic under the null hypothesis can also be approximated by a standard normal distribution. to illustrate this, recall the old faithful data. park rangers in yellowstone national park inform the public about the behavior of the geyser, such as the expected time between successive eruptions and the length of the duration of an eruption. suppose they claim that the expected length of an eruption is 4 minutes (240 seconds). does this seem likely on the basis of the data from section 15.1? we investigate this by testing h0 : µ = 240 against h1 : µ = 240 at level α = 0.001, where µ is the expectation of the model distribution. the value of the test statistic is"
2914,1,"['table', 'data', 'distribution', 'hypothesis', 'level', 'statistical', 'null hypothesis']", The onesample ttest,seg_363,"the one-tailed p-value p(t ≤ −7.39) can be approximated by p(z ≤ −7.39), where z has an n(0, 1) distribution. from table b.1 we see that this probability is smaller than p(z ≤ −3.49) = 0.0002. this is smaller than α/2 = 0.0005, so we reject the null hypothesis at level 0.001. in fact the p-value is much smaller: a statistical software package gives p(z ≤ −7.39) = 7.5 · 10−14. the data provide overwhelming evidence against h0 : µ = 240, so that we conclude that the expected length of an eruption is different from 4 minutes."
2915,1,"['normal approximation', 'approximation', 'normal', 'test', 'critical region']", The onesample ttest,seg_363,"quick exercise 27.3 compute the critical region k for the test, using the normal approximation, and check that t = −7.39 falls in k."
2916,1,"['tail probability', 'probability', 'hypothesis', 'tail', 'test', 'null hypothesis']", The onesample ttest,seg_363,"in fact, if we would test h0 : µ = 240 against h1 : µ < 240, the p-value corresponding to t = −7.39 is the left tail probability p(t ≤ −7.39). this probability is very small, so that we also reject the null hypothesis in favor of this alternative and conclude that the expected length of an eruption is smaller than 4 minutes."
2917,1,"['rate', 'percentage', 'table', 'scatterplot', 'case', 'data', 'rates', 'measurements', 'paired', 'concentration']", The ttest in a regression setting,seg_365,"is calcium in your drinking water good for your health? in england and wales, an investigation of environmental causes of disease was conducted. the annual mortality rate (percentage of deaths) and the calcium concentration in the drinking water supply were recorded for 61 large towns. the data in table 27.3 represent the annual mortality rate averaged over the years 1958–1964, and the calcium concentration in parts per million. in figure 27.3 the 61 paired measurements are displayed in a scatterplot. the scatterplot shows a slight downward trend, which suggests that higher concentrations of calcium lead to lower mortality rates. the question is whether this is really the case or if the slight downward trend should be attributed to chance."
2918,1,"['simple linear regression', 'dependent', 'normally distributed', 'linear', 'rate', 'dependent variable', 'data', 'linear regression model', 'model', 'regression model', 'regression', 'linear regression', 'concentration', 'errors', 'variable']", The ttest in a regression setting,seg_365,"to investigate this question we model the mortality data by means of a simple linear regression model with normally distributed errors, with the mortality rate as the dependent variable y and the calcium concentration as the independent variable x:"
2919,1,"['sample', 'rate', 'random sample', 'distribution', 'parameter', 'random', 'test', 'null hypothesis', 'concentration', 'hypothesis']", The ttest in a regression setting,seg_365,"where u1, u2, . . . , u61 is a random sample from an n(0, σ2) distribution. the parameter β represents the change of the mortality rate if we increase the calcium concentration by one unit. we test the null hypothesis h0 : β = 0 (calcium has no effect on the mortality rate) against h1 : β < 0 (higher concentration of calcium reduces the mortality rate)."
2920,1,['dataset'], The ttest in a regression setting,seg_365,"this example illustrates the general situation, where the dataset"
2921,1,"['simple linear regression', 'statistics', 'normally distributed', 'test statistics', 'statistic', 'null hypothesis', 'linear', 'linear regression model', 'model', 'regression model', 'test statistic', 'regression', 'linear regression', 'test', 'hypothesis', 'errors', 'hypotheses']", The ttest in a regression setting,seg_365,"is modeled by a simple linear regression model, and one wants to test a null hypothesis of the form h0 : α = α0 or h0 : β = β0. similar to the one-sample t-test we will construct a test statistic for each of these null hypotheses. with normally distributed errors, these test statistics have a t-distribution under the null hypothesis. for this reason, for both null hypotheses the test is called a t-test."
2922,1,['slope'], The ttest in a regression setting,seg_365,the t-test for the slope
2923,1,"['test statistic', 'statistic', 'test', 'null hypothesis', 'hypothesis']", The ttest in a regression setting,seg_365,"for the null hypothesis h0 : β = β0, we use as test statistic"
2924,1,"['estimator', 'least squares']", The ttest in a regression setting,seg_365,where β̂ is the least squares estimator for β (see chapter 22) and
2925,0,['n'], The ttest in a regression setting,seg_365,"in this expression, n"
2926,1,['estimator'], The ttest in a regression setting,seg_365,is the estimator for σ2 as introduced on page 332. it can be shown that
2927,1,"['deviation', 'estimator', 'standard', 'test statistic', 'variable', 'random variable', 'statistic', 'random', 'standard deviation', 'test', 'variance', 'null hypothesis', 'hypothesis']", The ttest in a regression setting,seg_365,"so that the random variable sb2 is an estimator for the variance of β̂ − β0. hence, similar to the test statistic for the one-sample t-test, the test statistic tb compares the estimator β̂ with the value β0 and standardizes by dividing by an estimator for the standard deviation of β̂ − β0. values of tb close to zero are in favor of the null hypothesis h0 : β = β0. large positive values of tb suggest that β > β0, whereas large negative values of tb suggest that β < β0."
2928,1,"['random samples', 'case', 'errors', 'normally distributed', 'distribution', 'samples', 'normal', 'random', 'test', 'null hypothesis', 'hypothesis']", The ttest in a regression setting,seg_365,"recall that in the case of normal random samples the one-sample t-test statistic has a t(n − 1) distribution under the null hypothesis. for the same reason, it is also a fact that in the case of normally distributed errors the test statistic tb has a t(n − 2) distribution under the null hypothesis h0 : β = β0."
2929,1,"['test', 'data']", The ttest in a regression setting,seg_365,"in our mortality example we want to test h0 : β = 0 against h0 : β < 0. for the data we find β̂ = −3.2261 and sb = 0.4847, so that the value of tb is"
2930,1,"['table', 'left critical value', 'level', 'test', 'critical value']", The ttest in a regression setting,seg_365,"if we test at level α = 0.05, then we must compare this value with the left critical value −t59,0.05. this value is not in table b.2, but we have that"
2931,1,"['table', 'rates', 'data', 'hypothesis', 'probability', 'level', 'statistical', 'null hypothesis']", The ttest in a regression setting,seg_365,"this means that tb is much smaller than −t59,0.05, so that we reject the null hypothesis at level 0.05. how much evidence the value tb = −6.656 bears against the null hypothesis is expressed by the one-tailed p-value p(tb ≤ −6.656). from table b.2 we can only see that this probability is smaller than 0.0005. by means of a statistical package we find p(tb ≤ −6.656) = 5.2 · 10−9. the data provide overwhelming evidence against the null hypothesis. we conclude that higher concentrations of calcium correspond to lower mortality rates."
2932,1,"['table', 'test statistic', 'data', 'statistic', 'level', 'test', 'null hypothesis', 'hypothesis']", The ttest in a regression setting,seg_365,"quick exercise 27.4 the data in table 27.3 can be separated into measurements for towns at least as far north as derby and towns south of derby. for the data corresponding to 35 towns at least as far north as derby, one finds β̂ = −1.9313 and sb = 0.8479. test h0 : β = 0 against h0 : β < 0 at level 0.01, i.e., compute the value of the test statistic and report your conclusion about the null hypothesis."
2933,1,['intercept'], The ttest in a regression setting,seg_365,the t-test for the intercept
2934,1,"['test statistic', 'statistic', 'test', 'null hypothesis', 'hypothesis']", The ttest in a regression setting,seg_365,we test the null hypothesis h0 : α = α0 with test statistic
2935,1,"['estimator', 'least squares']", The ttest in a regression setting,seg_365,where α̂ is the least squares estimator for α and
2936,1,"['estimator', 'random variable', 'variable', 'random', 'variance']", The ttest in a regression setting,seg_365,with σ̂2 defined as before. the random variable sa2 is an estimator for the variance
2937,1,"['deviation', 'test statistic', 'case', 'errors', 'distribution', 'statistic', 'normal', 'standard', 'standard deviation', 'test', 'estimator', 'null hypothesis', 'hypothesis']", The ttest in a regression setting,seg_365,"again, we compare the estimator α̂ with the value α0 and standardize by dividing by an estimator for the standard deviation of α̂ − α0. values of ta close to zero are in favor of the null hypothesis h0 : α = α0. large positive values of ta suggest that α > α0, whereas large negative values of ta suggest that α < α0. like tb, in the case of normal errors, the test statistic ta has a t(n − 2) distribution under the null hypothesis h0 : α = α0."
2938,1,"['simple linear regression', 'linear', 'model', 'regression model', 'linear model', 'scatterplot', 'data', 'regression', 'intercept', 'variable', 'linear regression', 'linear regression model']", The ttest in a regression setting,seg_365,"as an illustration, recall exercise 17.9 where we modeled the volume y of black cherry trees by means of a linear model without intercept, with independent variable x = d2h, where d and h are the diameter and height of the trees. the scatterplot of the pairs (x1, y1), (x2, y2), . . . , (x31, y31) is displayed in figure 27.4. as mentioned in exercise 17.9, there are physical reasons to leave out the intercept. we want to investigate whether this is confirmed by the data. to this end, we model the data by a simple linear regression model with intercept"
2939,1,"['sample', 'random', 'distribution', 'random sample', 'level', 'test']", The ttest in a regression setting,seg_365,"where u1, u2, . . . , u31 are a random sample from an n(0, σ2) distribution, and we test h0 : α = 0 against h1 : α = 0 at level 0.10. the value of the test"
2940,1,"['model', 'null hypothesis', 'data', 'intercept', 'left critical value', 'statistical', 'critical value', 'hypothesis']", The ttest in a regression setting,seg_365,"ta = = −0.3089, 0.9636 and the left critical value is −t29,0.05 = −1.699. this means we cannot reject the null hypothesis. the data do not provide sufficient evidence against h0 : α = 0, which is confirmed by the one-tailed p-value p(ta ≤ −0.3089) = 0.3798 (computed by means of a statistical package). we conclude that the intercept does not contribute significantly to the model."
2941,1,"['distribution', 'case', 'table']", Solutions to the quick exercises,seg_367,"27.1 if y has an n(1, 1) distribution, then y − 1 has an n(0, 1) distribution. therefore, from table b.1: p(y − 1 ≥ 0.03) = 0.4880. if y has an n(1, (0.01)2) distribution, then (y − 1)/0.01 has an n(0, 1) distribution. in that case,"
2942,1,"['test statistic', 'statistic', 'measurements', 'test']", Solutions to the quick exercises,seg_367,27.2 for the first and last ten measurements the values of the test statistic
2943,1,"['null hypothesis', 'critical value', 'hypothesis']", Solutions to the quick exercises,seg_367,"the critical value t9,0.025 = 2.262, which means we reject the null hypothesis for the second production line, but not for the first production line."
2944,1,"['table', 'critical region', 'distribution', 'left critical value', 'symmetry', 'normal', 'right critical value', 'critical value', 'normal distribution']", Solutions to the quick exercises,seg_367,"27.3 the critical region is of the form k = (−∞, cl] ∪ [cu,∞). the right critical value cu is approximated by z0.0005 = t∞,0.0005 = 3.291, which can be found in table b.2. by symmetry of the normal distribution, the left critical value cl is approximated by −z0.0005 = −3.291. clearly, t = −7.39 < −3.291, so that it falls in k."
2945,1,"['test statistic', 'statistic', 'test']", Solutions to the quick exercises,seg_367,27.4 the value of the test statistic is
2946,1,"['table', 'left critical value', 'level', 'critical value']", Solutions to the quick exercises,seg_367,"the left critical value is equal to −t33,0.01, which is not in table b.2, but we see that −t33,0.01 < −t40,0.01 = −2.423. this means that −t33,0.01 < tb, so that we cannot reject h0 : β = 0 against h0 : β < 0 at level 0.01."
2947,1,"['sample', 'sample mean', 'dataset', 'mean', 'level', 'sample variance', 'significance', 'variance', 'significance level', 'null hypothesis', 'hypothesis']", Exercises,seg_369,27.1 we perform a t -test for the null hypothesis h0 : µ = 10 by means of a dataset consisting of n = 16 elements with sample mean 11 and sample variance 4. we use significance level 0.05.
2948,1,"['null hypothesis', 'hypothesis']", Exercises,seg_369,a. should we reject the null hypothesis in favor of h1 : µ = 10?
2949,1,['test'], Exercises,seg_369,b. what if we test against h1 : µ > 10?
2950,1,"['process', 'table', 'variable', 'set', 'level', 'test']", Exercises,seg_369,27.2 the cleveland casting plant is a large highly automated producer of gray and nodular iron automotive castings for ford motor company. one process variable of interest to cleveland casting is the pouring temperature of molten iron. the pouring temperatures (in degrees fahrenheit) of ten crankshafts are given in table 27.4. the target setting for the pouring temperature is set at 2550 degrees. one wants to conduct a test at level α = 0.01 to determine whether the pouring temperature differs from the target setting.
2951,1,"['alternative hypothesis', 'null hypothesis', 'hypothesis']", Exercises,seg_369,a. formulate the appropriate null hypothesis and alternative hypothesis.
2952,1,"['test statistic', 'statistic', 'test']", Exercises,seg_369,b. compute the value of the test statistic and report your conclusion. you
2953,1,"['sample', 'model', 'distribution', 'normal', 'sample variance', 'variance']", Exercises,seg_369,may assume a normal model distribution and use that the sample variance is 517.34.
2954,1,"['null hypothesis', 'sample', 'sample mean', 'results', 'data', 'mean', 'standard', 'standard deviation', 'tests', 'failure', 'distribution', 'expectation', 'test', 'hypothesis', 'deviation', 'table', 'sample standard deviation', 'normal', 'normal distribution']", Exercises,seg_369,27.3 table 27.5 lists the results of tensile adhesion tests on 22 u-700 alloy specimens. the data are loads at failure in mpa. the sample mean is 13.71 and the sample standard deviation is 3.55. you may assume that the data originated from a normal distribution with expectation µ. one is interested in whether the load at failure exceeds 10 mpa. we investigate this by means of a t -test for the null hypothesis h0 : µ = 10.
2955,1,"['alternative hypothesis', 'hypothesis']", Exercises,seg_369,a. what do you choose as the alternative hypothesis?
2956,1,"['test statistic', 'statistic', 'test']", Exercises,seg_369,"b. compute the value of the test statistic and report your conclusion, when"
2957,1,"['level', 'test']", Exercises,seg_369,performing the test at level 0.05.
2958,1,"['statistic', 'measurements', 'random sample', 'random', 'significance', 'null hypothesis', 'sample', 'sample mean', 'dataset', 'data', 'mean', 'sample variance', 'test statistic', 'distribution', 'level', 'test', 'variance', 'hypothesis', 'table', 'significance level', 'critical value', 'realization']", Exercises,seg_369,"27.4 consider the coal data from table 23.2, where 22 gross calorific value measurements are listed for daw mill coal coded 258gb41. we modeled this dataset as a realization of a random sample from an n(µ, σ2) distribution with µ and σ unknown. we are planning to buy a shipment if the gross calorific value exceeds 31.00 mj/kg. the sample mean and sample variance of the data are x̄n = 31.012 and sn = 0.1294. perform a t -test for the null hypothesis h0 : µ = 31.00 against h1 : µ > 31.00 using significance level 0.01, i.e., compute the value of the test statistic, the critical value of the test, and report your conclusion."
2959,1,"['deviation', 'sample', 'sample mean', 'coefficient', 'sample standard deviation', 'mean', 'standard', 'standard deviation', 'tests', 'test', 'coefficients']", Exercises,seg_369,"27.5 in the november 1988 issue of science a study was reported on the inbreeding of tropical swarm-founding wasps. each member of a sample of 197 wasps was captured, frozen, and subjected to a series of genetic tests, from which an inbreeding coefficient was determined. the sample mean and the sample standard deviation of the coefficients are x̄197 = 0.044 and s197 = 0.884. if a species does not have the tendency to inbreed, their true inbreeding coefficient is 0. determine by means of a test whether the inbreeding coefficient for this species of wasp exceeds 0."
2960,1,"['alternative hypothesis', 'null hypothesis', 'hypothesis']", Exercises,seg_369,a. formulate the appropriate null hypothesis and alternative hypothesis and
2961,1,"['test statistic', 'statistic', 'test']", Exercises,seg_369,compute the value of the test statistic.
2962,1,"['test statistic', 'statistic', 'test']", Exercises,seg_369,b. compute the p-value corresponding to the value of the test statistic and
2963,1,"['null hypothesis', 'hypothesis']", Exercises,seg_369,report your conclusion about the null hypothesis.
2964,1,"['simple linear regression', 'dependent', 'normally distributed', 'linear', 'dependent variable', 'data', 'linear regression model', 'model', 'regression model', 'regression', 'linear regression', 'independent', 'table', 'independent variable', 'errors', 'variable']", Exercises,seg_369,"27.6 the stopping distance of an automobile is related to its speed. the data in table 27.6 give the stopping distance in feet and speed in miles per hour of an automobile. the data are modeled by means of simple linear regression model with normally distributed errors, with the square root of the stopping distance as dependent variable y and the speed as independent variable x:"
2965,1,['dataset'], Exercises,seg_369,for the dataset we find
2966,1,"['alternative hypothesis', 'test statistic', 'data', 'intercept', 'statistic', 'level', 'test', 'hypothesis']", Exercises,seg_369,"one would expect that the intercept can be taken equal to 0, since zero speed would yield zero stopping distance. investigate whether this is confirmed by the data by performing the appropriate test at level 0.10. formulate the proper null and alternative hypothesis, compute the value of the test statistic, and report your conclusion."
2967,1,"['model', 'regression model', 'table', 'data', 'regression', 'errors', 'variable', 'set', 'normally distributed', 'response', 'response variable', 'average']", Exercises,seg_369,"27.7 in a study about the effect of wall insulation, the weekly gas consumption (in 1000 cubic feet) and the average outside temperature (in degrees celsius) was measured of a certain house in southeast england, for 26 weeks before and 30 weeks after cavity-wall insulation had been installed. the house thermostat was set at 20 degrees throughout. the data are listed in table 27.7. we model the data before insulation by means of a simple linear regression model with normally distributed errors and gas consumption as response variable. a similar model was used for the data after insulation. given are"
2968,1,['data'], Exercises,seg_369,a. use the data before insulation to investigate whether smaller outside tem-
2969,1,"['alternative hypothesis', 'test statistic', 'statistic', 'level', 'significance', 'test', 'significance level', 'hypothesis']", Exercises,seg_369,"peratures lead to higher gas consumption. formulate the proper null and alternative hypothesis, compute the value of the test statistic, and report your conclusion, using significance level 0.05."
2970,1,['data'], Exercises,seg_369,b. do the same for the data after insulation.
2971,1,"['model', 'variances', 'observations', 'normal', 'expectations', 'test', 'distributions']", Comparing two samples,seg_371,"many applications are concerned with two groups of observations of the same kind that originate from two possibly different model distributions, and the question is whether these distributions have different expectations. we describe a test for equality of expectations, where we consider normal and nonnormal model distributions and equal and unequal variances of the model distributions."
2972,1,"['mean', 'scatterplots']", Is dry drilling faster than wet drilling,seg_373,"recall the drilling example from sections 15.5 and 16.4. the question was whether dry drilling is faster than wet drilling. the scatterplots in figure 15.11 seem to suggest that up to a depth of 250 feet the drill time does not depend on depth. therefore, for a first investigation of a possible difference between dry and wet drilling we only consider the (mean) drill times up to this depth. a more thorough study can be found in [23]."
2973,1,"['boxplot', 'boxplots']", Is dry drilling faster than wet drilling,seg_373,"the boxplots of the drill times for both types of drilling are displayed in figure 28.1. clearly, the boxplot for dry drilling is positioned lower than the"
2974,1,"['model', 'functions', 'parameters', 'random samples', 'distribution', 'expected value', 'samples', 'random', 'test']", Is dry drilling faster than wet drilling,seg_373,"one for wet drilling. however, the question is whether this difference can be attributed to chance or if it is large enough to conclude that the dry drill time is shorter than the wet drill time. to answer this question, we model the datasets of dry and wet drill times as realizations of random samples from two distribution functions f and g, one with expected value µ1 and the other with expected value µ2. the parameters µ1 and µ2 represent the drill times of dry drilling and wet drilling, respectively. we test h0 : µ1 = µ2 against h1 : µ1 < µ2."
2975,1,[], Is dry drilling faster than wet drilling,seg_373,this example illustrates a general situation where we compare two datasets
2976,1,"['random', 'independent', 'random samples', 'samples', 'realization']", Is dry drilling faster than wet drilling,seg_373,which are the realization of independent random samples
2977,1,"['variance', 'expectations', 'test', 'distributions']", Is dry drilling faster than wet drilling,seg_373,"from two distributions, and we want to test whether the expectations of both distributions are the same. both the variance σx"
2978,1,['variance'], Is dry drilling faster than wet drilling,seg_373,2 of the xi and the variance
2979,0,[], Is dry drilling faster than wet drilling,seg_373,2 of the yj are unknown.
2980,1,"['test statistic', 'statistic', 'variance', 'test', 'estimator', 'null hypothesis', 'hypothesis']", Is dry drilling faster than wet drilling,seg_373,"note that the null hypothesis is equivalent to the statement µ1 − µ2 = 0. for this reason, similar to chapter 27, the test statistic for the null hypothesis h0 : µ1 = µ2 is based on an estimator x̄n − ȳm for the difference µ1 − µ2. as before, we standardize x̄n − ȳm by an estimator for its variance"
2981,1,"['sample', 'sample variances', 'variances']", Is dry drilling faster than wet drilling,seg_373,recall that the sample variances sx
2982,1,"['unbiased estimators', 'estimators', 'unbiased']", Is dry drilling faster than wet drilling,seg_373,"2 and sy 2 of the xi and yj , are unbiased estimators for σx"
2983,1,['combination'], Is dry drilling faster than wet drilling,seg_373,2 and σy 2 . we will use a combination of sx
2984,1,"['standardization', 'cases', 'estimator', 'variances']", Is dry drilling faster than wet drilling,seg_373,struct an estimator for var(x̄n − ȳm). the actual standardization of x̄n−ȳm depends on whether the variances of the xi and yj are the same. we distinguish between the two cases σx
2985,1,"['case', 'variances']", Is dry drilling faster than wet drilling,seg_373,2 = σy 2 and σx 2 = σy 2 . in the next section we consider the case of equal variances.
2986,1,['boxplots'], Is dry drilling faster than wet drilling,seg_373,"quick exercise 28.1 looking at the boxplots in figure 28.1, does the assumption σx"
2987,0,[], Is dry drilling faster than wet drilling,seg_373,2 = σy 2 seem reasonable to you? can you think of a way to quantify your belief?
2988,1,"['samples', 'variance', 'distributions']", Two samples with equal variances,seg_375,suppose that the samples originate from distributions with the same (but unknown) variance:
2989,1,"['sample', 'sample variances', 'case', 'variances']", Two samples with equal variances,seg_375,in this case we can pool the sample variances sx
2990,0,[], Two samples with equal variances,seg_375,2 and sy 2 by constructing
2991,1,"['linear', 'linear combination', 'combination']", Two samples with equal variances,seg_375,a linear combination asx
2992,1,"['unbiased estimator', 'weighted average', 'average', 'estimator', 'unbiased']", Two samples with equal variances,seg_375,2 + bsy 2 that is an unbiased estimator for σ2. one particular choice is the weighted average
2993,1,"['linear', 'normally distributed', 'linear combinations', 'samples', 'combinations', 'variance', 'unbiased']", Two samples with equal variances,seg_375,it has the property that for normally distributed samples it has the smallest variance among all unbiased linear combinations of sx
2994,1,"['sample', 'estimate']", Two samples with equal variances,seg_375,"cise 28.5). moreover, the weights depend on the sample sizes. this is appropriate, since if one sample is much larger than the other, the estimate of σ2 from that sample is more reliable and should receive greater weight."
2995,0,[], Two samples with equal variances,seg_375,we find that the pooled-variance:
2996,1,"['unbiased estimator', 'estimator', 'unbiased']", Two samples with equal variances,seg_375,is an unbiased estimator for
2997,1,"['test statistic', 'statistic', 'test', 'null hypothesis', 'hypothesis']", Two samples with equal variances,seg_375,this leads to the following test statistic for the null hypothesis h0 : µ1 = µ2:
2998,1,"['deviation', 'standard', 'standard deviation', 'estimator', 'null hypothesis', 'hypothesis']", Two samples with equal variances,seg_375,"as before, we compare the estimator x̄n − ȳm with 0 (the value of µ1 − µ2 under the null hypothesis), and we standardize by dividing by the estimator sp for the standard deviation of x̄n − ȳm. values of tp close to zero are in favor of the null hypothesis h0 : µ1 = µ2. large positive values of tp suggest that µ1 > µ2, whereas large negative values suggest that µ1 < µ2."
2999,1,"['studentized mean', 'test statistic', 'distribution', 'statistic', 'mean', 'test', 'null hypothesis', 'studentized mean difference', 'hypothesis']", Two samples with equal variances,seg_375,"the next step is to determine the distribution of tp. note that under the null hypothesis h0 : µ1 = µ2, the test statistic tp is the pooled studentized mean difference"
3000,1,"['studentized mean', 'data', 'distribution', 'probability distribution', 'normal', 'mean', 'probability', 'null hypothesis', 'studentized mean difference', 'hypothesis']", Two samples with equal variances,seg_375,"hence, under the null hypothesis, the probability distribution of tp is the same as that of the pooled studentized mean difference. to determine its distribution, we distinguish between normal and nonnormal data."
3001,1,['samples'], Two samples with equal variances,seg_375,normal samples
3002,1,"['sample', 'normal distributions', 'studentized mean', 'independent', 'distribution', 'samples', 'normal', 'mean', 'distributions']", Two samples with equal variances,seg_375,"in the same way as the studentized mean of a single normal sample has a t(n − 1) distribution (see page 349), it is also a fact that if two independent samples originate from normal distributions, i.e.,"
3003,1,"['sample', 'random sample', 'random']", Two samples with equal variances,seg_375,"x1, x2, . . . , xn random sample from n(µ1, σ2)"
3004,1,"['sample', 'random sample', 'random']", Two samples with equal variances,seg_375,"y1, y2, . . . , ym random sample from n(µ2, σ2),"
3005,1,"['studentized mean', 'test statistic', 'distribution', 'statistic', 'mean', 'test', 'null hypothesis', 'studentized mean difference', 'hypothesis']", Two samples with equal variances,seg_375,"then the pooled studentized mean difference has a t(n + m − 2) distribution. hence, under the null hypothesis, the test statistic tp has a t(n + m − 2)"
3006,1,"['test', 'null hypothesis', 'hypothesis']", Two samples with equal variances,seg_375,"distribution. for this reason, a test for the null hypothesis h0 : µ1 = µ2 is called a two-sample t-test."
3007,1,"['model', 'normal distributions', 'variances', 'random samples', 'data', 'samples', 'normal', 'random', 'level', 'test', 'distributions']", Two samples with equal variances,seg_375,"suppose that in our drilling example we model our datasets as realizations of random samples of sizes n = m = 50 from two normal distributions with equal variances, and we test h0 : µ1 = µ2 against h1 : µ1 < µ2 at level 0.05. for the data we find x̄50 = 727.78, ȳ50 = 873.02, and sp = 13.62, so that"
3008,1,"['tail probability', 'table', 'null hypothesis', 'data', 'left critical value', 'probability', 'level', 'tail', 'statistical', 'critical value', 'hypothesis']", Two samples with equal variances,seg_375,"we compare this with the left critical value −t98,0.05. this value is not in table b.2, but −1.676 = −t50,0.05 < −t98,0.05. this means that tp < −t98,0.05, so that we reject h0 : µ1 = µ2 in favor of h1 : µ1 < µ2 at level 0.05. the pvalue corresponding to tp = −10.66 is the left tail probability p(t ≤ −10.66). from table b.2 we can only see that this is smaller than 0.0005 (a statistical software package gives p(t ≤ −10.66) = 2.25 ·10−18). the data provide overwhelming evidence against the null hypothesis, so that we conclude that dry drilling is faster than wet drilling."
3009,1,"['level', 'critical values', 'test']", Two samples with equal variances,seg_375,"quick exercise 28.2 suppose that in the ball bearing example of quick exercise 27.2, we test h0 : µ1 = µ2 against h1 : µ1 = µ2, where µ1 and µ2 represent the diameters of a ball bearing from the first and second production line. what are the critical values corresponding to level α = 0.01?"
3010,1,['samples'], Two samples with equal variances,seg_375,nonnormal samples
3011,1,"['model', 'test statistic', 'distribution', 'statistic', 'normal', 'mean', 'test', 'null hypothesis', 'hypothesis']", Two samples with equal variances,seg_375,"similar to the one-sample t -test, if we cannot assume normal model distributions, then we can no longer conclude that our test statistic has a t(n + m − 2) distribution under the null hypothesis. recall that under the null hypothesis, the distribution of our test statistic is the same as that of the pooled studentized mean difference (see page 417)."
3012,1,"['studentized mean', 'simulation', 'bootstrap', 'distribution', 'empirical bootstrap', 'mean', 'studentized mean difference']", Two samples with equal variances,seg_375,"to approximate its distribution, we use the empirical bootstrap simulation for the pooled studentized mean difference"
3013,1,"['functions', 'estimates', 'expectations']", Two samples with equal variances,seg_375,"given datasets x1, x2, . . . , xn and y1, y2, . . . , ym, determine their empirical distribution functions fn and gm as estimates for f and g. the expectations corresponding to fn and gm are µ∗1 = x̄n and µ∗2 = ȳm. then repeat the following two steps many times:"
3014,1,"['bootstrap', 'dataset']", Two samples with equal variances,seg_375,"1. generate a bootstrap dataset x∗1, x∗2, . . . , x∗n from fn and a bootstrap"
3015,1,"['studentized mean', 'bootstrap', 'data', 'mean', 'studentized mean difference']", Two samples with equal variances,seg_375,2. compute the pooled studentized mean difference for the bootstrap data:
3016,1,"['sample', 'bootstrap', 'sample means']", Two samples with equal variances,seg_375,"where x̄∗n and ȳm ∗ are the sample means of the bootstrap datasets, and"
3017,1,"['sample', 'sample variances', 'bootstrap', 'variances']", Two samples with equal variances,seg_375,with (sx∗ )2 and (sy∗ )2 the sample variances of the bootstrap datasets.
3018,1,"['functions', 'model', 'bootstrap', 'empirical distribution functions', 'distribution', 'expectations', 'distributions']", Two samples with equal variances,seg_375,"the reason that in each iteration we subtract x̄n − ȳm is that µ1 − µ2 is the difference of the expectations of the two model distributions. therefore, according to the bootstrap principle we should replace this by the difference x̄n − ȳm of the expectations corresponding to the two empirical distribution functions."
3019,1,"['histogram', 'level', 'bootstrap', 'simulation', 'approximation', 'null hypothesis', 'distribution function', 'empirical distribution function', 'data', 'distribution', 'left critical value', 'function', 'test', 'critical value', 'hypothesis']", Two samples with equal variances,seg_375,"we carried out this bootstrap simulation for the drill times. the result of this simulation can be seen in figure 28.2, where a histogram and the empirical distribution function are displayed for one thousand bootstrap values of t∗p. suppose that we test h0 : µ1 = µ2 against h1 : µ1 < µ2 at level 0.05. the bootstrap approximation for the left critical value is c∗l = −1.659. the value of tp = −10.66, computed from the data, is much smaller. hence, also on the basis of the bootstrap simulation we reject the null hypothesis and conclude that the dry drill time is shorter than the wet drill time."
3020,1,['experiments'], Two samples with unequal variances,seg_377,"during an investigation about weather modification, a series of experiments was conducted in southern florida from 1968 to 1972. these experiments were designed to investigate the use of massive silver-iodide seeding. it was"
3021,1,"['table', 'random', 'experiments', 'test']", Two samples with unequal variances,seg_377,"hypothesized that under specified conditions, this leads to invigorated cumulus growth and prolonged lifetimes, thereby causing increased precipitation. in these experiments, 52 isolated cumulus clouds were observed, of which 26 were selected at random and injected with silver-iodide smoke. rainfall amounts (in acre-feet) were recorded for all clouds. they are listed in table 28.1. to investigate whether seeding leads to increased rainfall, we test h0 : µ1 = µ2 against h1 : µ1 < µ2, where µ1 and µ2 represent the rainfall for unseeded and seeded clouds."
3022,1,"['statistic', 'estimator', 'sample', 'unbiased estimator', 'sample variances', 'standardized', 'boxplots', 'test statistic', 'test', 'unbiased', 'variances']", Two samples with unequal variances,seg_377,"in figure 28.3 the boxplots of both datasets are displayed. from this we see that the assumption of equal variances may not be realistic. indeed, this is confirmed by the values s2x = 77 521 and s2y = 423 524 of the sample variances of the datasets. this means that we need to test h0 : µ1 = µ2 without the assumption of equal variances. as before, the test statistic will be a standardized version of x̄n − ȳm, but sp2 is no longer an unbiased estimator"
3023,1,['estimate'], Two samples with unequal variances,seg_377,"however, if we estimate σx"
3024,1,"['nonpooled variance', 'nonpooled', 'variance']", Two samples with unequal variances,seg_377,"2 and σy 2 by sx 2 and sy 2 , then the nonpooled variance"
3025,1,"['unbiased estimator', 'test statistic', 'statistic', 'test', 'estimator', 'unbiased']", Two samples with unequal variances,seg_377,is an unbiased estimator for var(x̄n − ȳm). this leads to test statistic
3026,1,"['deviation', 'standard', 'standard deviation', 'estimator', 'null hypothesis', 'hypothesis']", Two samples with unequal variances,seg_377,"again, we compare the estimator x̄n − ȳm with zero and standardize by dividing by an estimator for the standard deviation of x̄n − ȳm. values of td close to zero are in favor of the null hypothesis h0 : µ1 = µ2."
3027,0,[], Two samples with unequal variances,seg_377,quick exercise 28.3 consider the ball bearing example from quick exercise 27.2. compute the value of td for this example.
3028,1,"['test statistic', 'statistic', 'test', 'null hypothesis', 'hypothesis']", Two samples with unequal variances,seg_377,"under the null hypothesis h0 : µ1 = µ2, the test statistic"
3029,1,"['studentized mean', 'nonpooled', 'mean', 'studentized mean difference']", Two samples with unequal variances,seg_377,is equal to the nonpooled studentized mean difference
3030,1,"['studentized mean', 'null hypothesis', 'case', 'distribution', 'samples', 'normal', 'mean', 'nonpooled', 'studentized mean difference', 'hypothesis']", Two samples with unequal variances,seg_377,"therefore, the distribution of td under the null hypothesis is the same as that of the nonpooled studentized mean difference. unfortunately, its distribution is not a t-distribution, not even in the case of normal samples. this means that we have to approximate this distribution."
3031,1,"['studentized mean', 'simulation', 'bootstrap', 'empirical bootstrap', 'mean', 'nonpooled', 'studentized mean difference']", Two samples with unequal variances,seg_377,"similar to the previous section, we use the empirical bootstrap simulation for the nonpooled studentized mean difference. the only difference with the procedure outlined in the previous section is that now in each iteration we compute the nonpooled studentized mean difference for the bootstrap datasets:"
3032,1,"['sample', 'bootstrap', 'sample means']", Two samples with unequal variances,seg_377,"∗ are the sample means of the bootstrap datasets, and"
3033,1,"['sample', 'sample variances', 'bootstrap', 'variances']", Two samples with unequal variances,seg_377,with (sx∗ )2 and (sy∗ )2 the sample variances of the bootstrap datasets.
3034,1,"['histogram', 'level', 'bootstrap', 'simulation', 'approximation', 'distribution function', 'empirical distribution function', 'data', 'distribution', 'left critical value', 'function', 'critical value']", Two samples with unequal variances,seg_377,"we carried out this bootstrap simulation for the cloud seeding data. the result of this simulation can be seen in figure 28.4, where a histogram and the empirical distribution function are displayed for one thousand values t∗d. the bootstrap approximation for the left critical value corresponding to level 0.05 is c∗l = −1.405. for the data we find the value"
3035,1,"['null hypothesis', 'hypothesis']", Two samples with unequal variances,seg_377,"this is smaller than c∗l , so we reject the null hypothesis. although the evidence against the null hypothesis is not overwhelming, there is some indication that seeding clouds leads to more rainfall."
3036,1,"['studentized mean', 'mean', 'central limit theorem', 'limit', 'studentized mean difference', 'distributions']", Large samples,seg_379,"variants of the central limit theorem state that as n and m both tend to infinity, the distributions of the pooled studentized mean difference"
3037,1,"['studentized mean', 'nonpooled', 'mean', 'studentized mean difference']", Large samples,seg_379,and the nonpooled studentized mean difference
3038,1,"['standard normal', 'statistics', 'distribution', 'test statistics', 'normal', 'standard', 'standard normal distribution', 'test', 'null hypothesis', 'normal distribution', 'hypothesis']", Large samples,seg_379,both approach the standard normal distribution. this fact can be used to approximate the distribution of the test statistics tp and td under the null hypothesis by a standard normal distribution.
3039,1,"['table', 'data', 'test', 'average']", Large samples,seg_379,"we illustrate this by means of the following example. to investigate whether a restricted diet promotes longevity, two groups of randomly selected rats were put on the different diets. one group of n = 106 rats was put on a restricted diet, the other group of m = 89 rats on an ad libitum diet (i.e., unrestricted eating). the data in table 28.2 represent the remaining lifetime in days of two groups of rats after they were put on the different diets. the average lifetimes are x̄n = 968.75 and ȳm = 684.01 days. to investigate whether a restricted diet promotes longevity, we test h0 : µ1 = µ2 against h1 : µ1 > µ2, where µ1 and µ2 represent the lifetime of a rat on a restricted diet and on an ad libitum diet, respectively."
3040,1,['variances'], Large samples,seg_379,"if we may assume equal variances, we compute"
3041,1,"['level', 'critical value', 'right critical value']", Large samples,seg_379,"this value is larger than the right critical value z0.0005 = 3.291, which means that we would reject h0 : µ1 = µ2 in favor of h1 : µ1 > µ2 at level α = 0.0005."
3042,1,"['tail probability', 'table', 'right tail probability', 'distribution', 'probability', 'tail', 'statistical']", Large samples,seg_379,"the p-value is the right tail probability p(tp ≥ 8.66), which we approximate by p(z ≥ 8.66), where z has an n(0, 1) distribution. from table b.1 we see that this probability is smaller than p(z ≥ 3.49) = 0.0002. by means of a statistical package we find p(z ≥ 8.66) = 2.4 · 10−16."
3043,1,"['test', 'variances']", Large samples,seg_379,"if we repeat the test without the assumption of equal variances, we compute"
3044,1,"['case', 'data', 'hypothesis', 'statistical', 'null hypothesis']", Large samples,seg_379,"which also leads to rejection of the null hypothesis. in this case, the p-value p(td ≥ 9.16) ≈ p(z ≥ 9.16) is even smaller since 9.16 > 8.66 (a statistical package gives p(z ≥ 9.16) = 2.6 · 10−18). the data provide overwhelming evidence against the null hypothesis, and we conclude that a restricted diet promotes longevity."
3045,1,['boxplots'], Solutions to the quick exercises,seg_381,"28.1 just by looking at the boxplots, the authors believe that the assumption σx"
3046,1,"['sample', 'sample variances', 'dataset', 'case', 'samples', 'boxplots', 'normal', 'standard', 'test', 'vary', 'variances']", Solutions to the quick exercises,seg_381,"2 is reasonable. the lengths of the boxplots and their iqrs are almost the same. however, the boxplots do not reveal how the elements of the dataset vary around the center. one way of quantifying our belief would be to compare the sample variances of the datasets. one possibility is to compare the ratio of both sample variances; a ratio close to one would support our belief of equal variances (in case of normal samples, this is a standard test called the f -test)."
3047,1,"['left critical value', 'right critical value', 'critical value', 'case']", Solutions to the quick exercises,seg_381,"28.2 in this case we have a right and left critical value. from quick exercise 27.2 we know that n = m = 10, so that the right critical value is t18,0.005 = 2.878 and the left critical value is −t18,0.005 = −2.878."
3048,1,"['sample', 'sample mean', 'table', 'range', 'data', 'mean', 'sample variance', 'variance']", Exercises,seg_383,"28.1 the data in table 28.3 represent salaries (in pounds sterling) in 72 randomly selected advertisements in the the guardian (april 6, 1992). when a range was given in the advertisement, the midpoint of the range is reproduced in the table. the data are salaries corresponding to two kinds of occupations (n = m = 72): (1) creative, media, and marketing and (2) education. the sample mean and sample variance of the two datasets are, respectively:"
3049,1,"['normal distributions', 'normal', 'expectations', 'distributions']", Exercises,seg_383,"suppose that the datasets are modeled as realizations of normal distributions with expectations µ1 and µ2, which represent the salaries for occupations (1) and (2)."
3050,1,"['test', 'null hypothesis', 'hypothesis']", Exercises,seg_383,a. test the null hypothesis that the salary for both occupations is the same
3051,1,"['test statistic', 'null and alternative hypotheses', 'statistic', 'hypotheses', 'level', 'test', 'alternative hypotheses', 'variances']", Exercises,seg_383,"at level α = 0.05 under the assumption of equal variances. formulate the proper null and alternative hypotheses, compute the value of the test statistic, and report your conclusion."
3052,1,['variances'], Exercises,seg_383,b. do the same without the assumption of equal variances.
3053,1,"['empirical bootstrap', 'simulation', 'bootstrap']", Exercises,seg_383,"c. as a comparison, one carries out an empirical bootstrap simulation for the"
3054,1,"['studentized mean', 'critical values', 'bootstrap', 'results', 'mean', 'studentized mean difference']", Exercises,seg_383,nonpooled studentized mean difference. the bootstrap approximations for the critical values are c∗l = −2.004 and c∗u = 2.133. report your conclusion about the salaries on the basis of the bootstrap results.
3055,1,"['data', 'table']", Exercises,seg_383,"28.2 the data in table 28.4 represent the duration of pregnancy for 1669 women who gave birth in a maternity hospital in newcastle-upon-tyne, england, in 1954."
3056,1,"['sample', 'sample variances', 'sample means', 'variances']", Exercises,seg_383,"the durations are measured in complete weeks from the beginning of the last menstrual period until delivery. the pregnancies are divided into those where an admission was booked for medical reasons, those booked for social reasons (such as poor housing), and unbooked emergency admissions. for the three groups the sample means and sample variances are"
3057,1,['observations'], Exercises,seg_383,"medical: 775 observations with x̄ = 39.08 and s2 = 7.77, emergency: 261 observations with x̄ = 37.59 and s2 = 25.33, social: 633 observations with x̄ = 39.60 and s2 = 4.95."
3058,1,"['random samples', 'statistic', 'random', 'null hypothesis', 'samples', 'expectations', 'distributions', 'normal distributions', 'combination', 'test statistic', 'test', 'hypothesis', 'normal', 'variances']", Exercises,seg_383,"suppose we view the datasets as realizations of random samples from normal distributions with expectations µ1, µ2, and µ3 and variances σ12, σ22, and σ32, where µi represents the duration of pregnancy for the women from the ith group. we want to investigate whether the duration differs for the different groups. for each combination of two groups test the null hypothesis of equality of µi. compute the values of the test statistic and report your conclusions."
3059,1,"['sample', 'table', 'sample means', 'results']", Exercises,seg_383,"28.3 in a seven-day study on the effect of ozone, a group of 23 rats was kept in an ozone-free environment and a group of 22 rats in an ozone-rich environment. from each member in both groups the increase in weight (in grams) was recorded. the results are given in table 28.5. the interest is in whether ozone affects the increase of weight. we investigate this by testing h0 : µ1 = µ2 against h1 : µ1 = µ2, where µ1 and µ2 denote the increases of weight for a rat in the ozone-free and ozone-rich groups. the sample means"
3060,1,"['deviation', 'standard', 'pooled standard deviation', 'standard deviation', 'nonpooled']", Exercises,seg_383,"the pooled standard deviation is sp = 4.58, and the nonpooled standard deviation is sd = 4.64."
3061,1,"['level', 'test', 'normal', 'data']", Exercises,seg_383,a. perform the test at level 0.05 under the assumption of normal data with
3062,1,"['test statistic', 'statistic', 'test', 'variances']", Exercises,seg_383,"equal variances, i.e., compute the test statistic and report your conclusion."
3063,1,"['simulation', 'bootstrap', 'test statistic', 'statistic', 'test']", Exercises,seg_383,b. one also carries out a bootstrap simulation for the test statistic used in
3064,1,"['critical values', 'simulation', 'bootstrap']", Exercises,seg_383,"a, and finds critical values c∗l = −1.912 and c∗u = 1.959. what is your conclusion on the basis of the bootstrap simulation?"
3065,1,"['level', 'test']", Exercises,seg_383,c. also perform the test at level 0.05 without the assumption of equal vari-
3066,1,"['normal approximation', 'test statistic', 'approximation', 'distribution', 'statistic', 'normal', 'test', 'null hypothesis', 'hypothesis']", Exercises,seg_383,"ances, where you may use the normal approximation for the distribution of the test statistic under the null hypothesis."
3067,1,"['simulation', 'bootstrap', 'test statistic', 'statistic', 'tail', 'test']", Exercises,seg_383,d. a bootstrap simulation for the test statistic in c yields that the right tail
3068,1,"['simulation', 'bootstrap', 'test statistic', 'case', 'statistic', 'test']", Exercises,seg_383,probability corresponding to the observed value of the test statistic in this case is 0.014. what is your conclusion on the basis of the bootstrap simulation?
3069,1,"['random variables', 'variables', 'case', 'random']", Exercises,seg_383,"28.4 show that in the case when n = m, the random variables tp and td are the same."
3070,1,"['normal distributions', 'independent', 'variances', 'normal', 'random', 'distributions']", Exercises,seg_383,"28.5 let x1, x2, . . . , xn and y1, y2, . . . , ym be independent random samples from normal distributions with variances σ2. it can be shown that"
3071,1,"['linear', 'linear combinations', 'combinations']", Exercises,seg_383,consider linear combinations asx
3072,1,"['unbiased estimators', 'estimators', 'unbiased']", Exercises,seg_383,2 + bsy 2 that are unbiased estimators for σ2.
3073,1,"['independent', 'variances', 'random samples', 'samples', 'random', 'distributions']", Exercises,seg_383,"28.6 let x1, x2, . . . , xn and y1, y2, . . . , ym be independent random samples from distributions with (possibly unequal) variances σx"
3074,1,"['variance', 'biased', 'pooled variance']", Exercises,seg_383,"b. show that the pooled variance sp2, as defined on page 417, is a biased"
3075,1,"['nonpooled variance', 'nonpooled', 'variance']", Exercises,seg_383,"c. show that the nonpooled variance sd2, as defined on page 420, is the only"
3076,1,['estimator'], Exercises,seg_383,unbiased estimator for var(x̄n − ȳm) of the form asx
3077,1,"['unbiased estimator', 'estimator', 'unbiased']", Exercises,seg_383,"2 = σy 2 = σ2. show that sd2, as defined on page 417, is an unbiased estimator for var(x̄n − ȳm) = σ2(1/n + 1/m)."
3078,1,"['unbiased estimator', 'estimator', 'case', 'unbiased']", Exercises,seg_383,e. is sd2 also an unbiased estimator for var(x̄n − ȳm) in the case σx
3079,1,['distributions'],A Summary of distributions,seg_385,discrete distributions
3080,1,"['distribution', 'bernoulli', 'bernoulli distribution']",A Summary of distributions,seg_385,"1. bernoulli distribution: ber(p), where 0 ≤ p ≤ 1."
3081,1,"['distribution', 'binomial distribution', 'binomial']",A Summary of distributions,seg_385,"2. binomial distribution: bin(n, p), where 0 ≤ p ≤ 1."
3082,1,"['distribution', 'geometric distribution', 'geometric']",A Summary of distributions,seg_385,"3. geometric distribution: geo(p), where 0 < p ≤ 1."
3083,1,"['poisson', 'distribution', 'poisson distribution']",A Summary of distributions,seg_385,"4. poisson distribution: pois(µ), where µ > 0."
3084,1,['distributions'],A Summary of distributions,seg_385,continuous distributions
3085,1,"['distribution', 'cauchy', 'cauchy distribution']",A Summary of distributions,seg_385,"1. cauchy distribution: cau(α, β), where −∞ < α < ∞ and β > 0."
3086,1,"['distribution', 'exponential', 'exponential distribution']",A Summary of distributions,seg_385,"2. exponential distribution: exp(λ), where λ > 0."
3087,1,"['distribution', 'gamma distribution', 'gamma']",A Summary of distributions,seg_385,"3. gamma distribution: gam(α, λ), where α > 0 and λ > 0."
3088,1,"['distribution', 'normal', 'normal distribution']",A Summary of distributions,seg_385,"4. normal distribution: n(µ, σ2), where −∞ < µ < ∞ and σ > 0."
3089,1,"['distribution', 'pareto', 'pareto distribution']",A Summary of distributions,seg_385,"5. pareto distribution: par(α), where α > 0."
3090,1,"['distribution', 'uniform distribution']",A Summary of distributions,seg_385,"6. uniform distribution: u(a, b), where a < b."
3091,1,['set'],List of symbols,seg_395,"∅ empty set, page 14"
3092,1,"['level', 'significance', 'significance level']",List of symbols,seg_395,"α significance level, page 384"
3093,1,"['complement', 'event']",List of symbols,seg_395,"ac complement of the event a, page 14"
3094,1,['intersection'],List of symbols,seg_395,"a ∩ b intersection of a and b, page 14"
3095,1,['union'],List of symbols,seg_395,"a ∪ b union of a and b, page 14"
3096,1,"['distribution', 'bernoulli', 'parameter', 'bernoulli distribution']",List of symbols,seg_395,"ber(p) bernoulli distribution with parameter p, page 45"
3097,1,"['parameters', 'distribution', 'binomial', 'binomial distribution']",List of symbols,seg_395,"bin(n, p) binomial distribution with parameters n and p, page 48"
3098,1,"['right critical values', 'critical values']",List of symbols,seg_395,"cl, cu left and right critical values, page 388"
3099,1,"['parameters', 'cauchy', 'cauchy distribution', 'distribution']",List of symbols,seg_395,"cau(α, β) cauchy distribution with parameters α en β, page 161"
3100,1,['covariance'],List of symbols,seg_395,"cov(x, y ) covariance between x and y , page 139"
3101,1,"['random variable', 'variable', 'expectation', 'random']",List of symbols,seg_395,"e[x ] expectation of the random variable x , page 90, 91"
3102,1,"['distribution', 'exponential', 'exponential distribution', 'parameter']",List of symbols,seg_395,"exp(λ) exponential distribution with parameter λ, page 62"
3103,1,"['standard normal', 'distribution function', 'distribution', 'normal', 'standard', 'function', 'standard normal distribution', 'normal distribution']",List of symbols,seg_395,"φ distribution function of the standard normal distribution, page 65"
3104,1,"['standard normal', 'distribution', 'normal', 'probability', 'standard', 'standard normal distribution', 'normal distribution']",List of symbols,seg_395,"φ probability density of the standard normal distribution, page 65"
3105,1,"['density function', 'probability density function', 'probability', 'function']",List of symbols,seg_395,"f probability density function, page 57"
3106,1,"['density function', 'probability density function', 'joint probability density function', 'joint', 'probability', 'function', 'joint probability']",List of symbols,seg_395,"f joint probability density function, page 119"
3107,1,"['distribution', 'function', 'distribution function']",List of symbols,seg_395,"f distribution function, page 44"
3108,1,"['distribution function', 'distribution', 'joint', 'function']",List of symbols,seg_395,"f joint distribution function, page 118"
3109,1,"['distribution', 'function', 'distribution function']",List of symbols,seg_395,"f inv inverse function of distribution function f , page 73"
3110,1,"['distribution function', 'empirical distribution function', 'distribution', 'function']",List of symbols,seg_395,"fn empirical distribution function, page 219"
3111,1,"['kernel', 'kernel density estimate', 'estimate']",List of symbols,seg_395,"fn,h kernel density estimate, page 213"
3112,1,"['parameters', 'gamma', 'distribution', 'gamma distribution']",List of symbols,seg_395,"gam(α, λ) gamma distribution with parameters α en λ, page 157"
3113,1,"['distribution', 'geometric distribution', 'geometric', 'parameter']",List of symbols,seg_395,"geo(p) geometric distribution with parameter p, page 49"
3114,1,"['alternative hypothesis', 'null hypothesis', 'hypothesis']",List of symbols,seg_395,"h0, h1 null hypothesis and alternative hypothesis, page 374"
3115,1,"['function', 'likelihood', 'likelihood function']",List of symbols,seg_395,"l(θ) likelihood function, page 317"
3116,1,"['function', 'loglikelihood function']",List of symbols,seg_395,"(θ) loglikelihood function, page 319"
3117,1,"['sample median', 'sample', 'median', 'dataset']",List of symbols,seg_395,"medn sample median of a dataset, page 231"
3118,1,['factorial'],List of symbols,seg_395,"n! n factorial, page 14"
3119,1,"['parameters', 'distribution', 'normal', 'normal distribution']",List of symbols,seg_395,"n(µ, σ2) normal distribution with parameters µ and σ2, page 64"
3120,1,"['sample', 'sample space']",List of symbols,seg_395,"ω sample space, page 13"
3121,1,"['distribution', 'pareto', 'parameter', 'pareto distribution']",List of symbols,seg_395,"par(α) pareto distribution with parameter α, page 63"
3122,1,"['poisson', 'distribution', 'parameter', 'poisson distribution']",List of symbols,seg_395,"pois(µ) poisson distribution with parameter µ, page 170"
3123,1,"['probability', 'conditional', 'conditional probability']",List of symbols,seg_395,"p(a |c) conditional probability of a given c, page 26"
3124,1,"['probability', 'probability of the event', 'event']",List of symbols,seg_395,"p(a) probability of the event a, page 16"
3125,1,"['quantile', 'empirical quantile']",List of symbols,seg_395,"qn(p) pth empirical quantile, page 234"
3126,1,"['quantile', 'percentile']",List of symbols,seg_395,"qp pth quantile or 100pth percentile, page 66"
3127,1,"['coefficient', 'correlation coefficient', 'correlation']",List of symbols,seg_395,"ρ(x, y ) correlation coefficient between x and y , page 142"
3128,1,"['sample', 'dataset', 'sample variance', 'variance']",List of symbols,seg_395,"s2n sample variance of a dataset, page 233"
3129,1,"['sample', 'random', 'random sample', 'sample variance', 'variance']",List of symbols,seg_395,"sn 2 sample variance of random sample, page 292"
3130,1,['degrees of freedom'],List of symbols,seg_395,"t(m) t-distribution with m degrees of freedom, page 348"
3131,1,"['distribution', 'critical value']",List of symbols,seg_395,"tm,p critical value of the t(m) distribution, page 348"
3132,1,"['distribution', 'uniform distribution', 'parameters']",List of symbols,seg_395,"u(α, β) uniform distribution with parameters α and β, page 60"
3133,1,"['random variable', 'variable', 'random', 'variance']",List of symbols,seg_395,"var(x) variance of the random variable x , page 96"
3134,1,"['sample', 'sample mean', 'dataset', 'mean']",List of symbols,seg_395,"x̄n sample mean of a dataset, page 231"
3135,1,"['random variables', 'variables', 'random', 'average']",List of symbols,seg_395,"x̄n average of the random variables x1, . . . , xn, page 182"
3136,1,"['distribution', 'critical value']",List of symbols,seg_395,"zp critical value of the n(0, 1) distribution, page 345"
