,Relevance,Tags,Heading,Seg,Sentence
0,1,"['data', 'sets', 'data sets']", Basics,seg_1,the purpose of this chapter is to get you started using r. it is assumed that you have a working installation of the software and of the iswr package that contains the data sets for this book. instructions for obtaining and installing the software are given in appendix a.
1,1,['independent'], Basics,seg_1,"the text that follows describes r version 2.6.2. as of this writing, that is the latest version of r. as far as possible, i present the issues in a way that is independent of the operating system in use and assume that the reader has the elementary operational knowledge to select from menus, move windows around, etc. i do, however, make exceptions where i am aware of specific difficulties with a particular platform or specific features of it."
2,0,[], First steps,seg_3,this section gives an introduction to the r computing environment and walks you through its most basic features.
3,1,['method'], First steps,seg_3,"starting r is straightforward, but the method will depend on your computing platform. you will be able to launch it from a system menu, by double-clicking an icon, or by entering the command “r” at the system command line. this will either produce a console window or cause r to start up as an interactive program in the current terminal window. in"
4,1,"['model', 'graphical', 'case']", First steps,seg_3,"either case, r works fundamentally by the question-and-answer model: you enter a line with a command and press enter (←↩). then the program does something, prints the result if relevant, and asks for more input. when r is ready for input, it prints out its prompt, a “>”. it is possible to use r as a text-only application, and also in batch mode, but for the purposes of this chapter, i assume that you are sitting at a graphical workstation."
5,0,[], First steps,seg_3,"all the examples in this book should run if you type them in exactly as printed, provided that you have the iswr package not only installed but also loaded into your current search path. this is done by entering"
6,0,[], First steps,seg_3,at the command prompt. you do not need to understand what the command does at this point. it is explained in section 2.1.5.
7,0,[], First steps,seg_3,"for a first impression of what r can do, try typing the following:"
8,1,"['graphics', 'plots', 'normal', 'random']", First steps,seg_3,this command draws 1000 numbers at random from the normal distribution (rnorm = random normal) and plots them in a pop-up graphics window. the result on a windows machine can be seen in figure 1.1.
9,0,[], First steps,seg_3,"of course, you are not expected at this point to guess that you would obtain this result in that particular way. the example is chosen because it shows several components of the user interface in action. before the style"
10,0,[], First steps,seg_3,"of commands will fall naturally, it is necessary to introduce some concepts and conventions through simpler examples."
11,1,['graphics'], First steps,seg_3,"under windows, the graphics window will have taken the keyboard focus at this point. click on the console to make it accept further commands."
12,0,[], An overgrown calculator,seg_5,one of the simplest possible tasks in r is to enter an arithmetic expression and receive a result. (the second line is the answer from the machine.)
13,1,['standard'], An overgrown calculator,seg_5,"so the machine knows that 2 plus 2 makes 4. of course, it also knows how to do other standard calculations. for instance, here is how to compute −2 e :"
14,1,"['case', 'distribution', 'normal', 'random', 'random numbers', 'normal distribution']", An overgrown calculator,seg_5,"the [1] in front of the result is part of r’s way of printing numbers and vectors. it is not useful here, but it becomes so when the result is a longer vector. the number in brackets is the index of the first number on that line. consider the case of generating 15 random numbers from a normal distribution:"
15,1,"['random number', 'random']", An overgrown calculator,seg_5,"here, for example, the [6] indicates that 0.07976977 is the sixth element in the vector. (for typographical reasons, the examples in this book are made with a shortened line width. if you try it on your own machine, you will see the values printed with six numbers per line rather than five. the numbers themselves will also be different since random number generation is involved.)"
16,1,"['variables', 'results']", Assignments,seg_7,"even on a calculator, you will quickly need some way to store intermediate results, so that you do not have to key them in over and over again. r, like other computer languages, has symbolic variables, that is names that"
17,1,['variable'], Assignments,seg_7,"can be used to represent values. to assign the value 2 to the variable x, you can enter"
18,1,"['consequences', 'variable']", Assignments,seg_7,"the two characters <- should be read as a single symbol: an arrow pointing to the variable to which the value is assigned. this is known as the assignment operator. spacing around operators is generally disregarded by r, but notice that adding a space in the middle of a <- changes the meaning to “less than” followed by “minus” (conversely, omitting the space when comparing a variable to a negative number has unexpected consequences!)."
19,0,[], Assignments,seg_7,"there is no immediately visible result, but from now on, x has the value 2 and can be used in subsequent arithmetic expressions."
20,1,"['variable name', 'variables', 'variable']", Assignments,seg_7,"names of variables can be chosen quite freely in r. they can be built from letters, digits, and the period (dot) symbol. there is, however, the limitation that the name must not start with a digit or a period followed by a digit. names that start with a period are special and should be avoided. a typical variable name could be height.1yr, which might be used to describe the height of a child at the age of 1 year. names are case-sensitive: wt and wt do not refer to the same variable."
21,1,"['functions', 'cases', 'variable', 'standard', 'variable names']", Assignments,seg_7,"some names are already used by the system. this can cause some confusion if you use them for other purposes. the worst cases are the single-letter names c, q, t, c, d, f, i, and t, but there are also diff, df, and pt, for example. most of these are functions and do not usually cause trouble when used as variable names. however, f and t are the standard abbreviations for false and true and no longer work as such if you redefine them."
22,1,"['statistics', 'data', 'variable']", Vectorized arithmetic,seg_9,"you cannot do much statistics on single numbers! rather, you will look at data from a group of patients, for example. one strength of r is that it can handle entire data vectors as single objects. a data vector is simply an array of numbers, and a vector variable can be constructed like this:"
23,1,['normal'], Vectorized arithmetic,seg_9,the construct c(...) is used to define vectors. the numbers are made up but might represent the weights (in kg) of a group of normal men.
24,1,"['data', 'method']", Vectorized arithmetic,seg_9,"this is neither the only way to enter data vectors into r nor is it generally the preferred method, but short vectors are used for many other purposes, and the c(...) construct is used extensively. in section 2.4, we discuss alternative techniques for reading data. for now, we stick to a single method."
25,0,[], Vectorized arithmetic,seg_9,"you can do calculations with vectors just like ordinary numbers, as long as they are of the same length. suppose that we also have the heights that correspond to the weights above. the body mass index (bmi) is defined for each person as the weight in kilograms divided by the square of the height in meters. this could be calculated as follows:"
26,0,[], Vectorized arithmetic,seg_9,"notice that the operation is carried out elementwise (that is, the first value of bmi is 60/1.752 and so forth) and that the ^ operator is used for raising a value to a power. (on some keyboards, ^ is a “dead key” and you will have to press the spacebar afterwards to make it show.)"
27,1,['cases'], Vectorized arithmetic,seg_9,"it is in fact possible to perform arithmetic operations on vectors of different length. we already used that when we calculated the height^2 part above since 2 has length 1. in such cases, the shorter vector is recycled. this is mostly used with vectors of length 1 (scalars) but sometimes also in other cases where a repeating pattern is desired. a warning is issued if the longer vector is not a multiple of the shorter in length."
28,1,"['deviation', 'variable', 'mean', 'standard', 'standard deviation', 'statistical']", Vectorized arithmetic,seg_9,"these conventions for vectorized calculations make it very easy to specify typical statistical calculations. consider, for instance, the calculation of the mean and standard deviation of the weight variable."
29,1,['mean'], Vectorized arithmetic,seg_9,"first, calculate the mean, x̄ = ∑ xi/n:"
30,1,"['variable', 'mean']", Vectorized arithmetic,seg_9,then save the mean in a variable xbar and proceed with the calculation
31,1,"['deviations', 'mean']", Vectorized arithmetic,seg_9,of sd = √(∑(xi − x̄)2)/(n− 1). we do this in steps to see the individual components. the deviations from the mean are
32,1,['deviations'], Vectorized arithmetic,seg_9,"notice how xbar, which has length 1, is recycled and subtracted from each element of weight. the squared deviations will be"
33,0,[], Vectorized arithmetic,seg_9,"since this command is quite similar to the one before it, it is convenient to enter it by editing the previous command. on most systems running r, the previous command can be recalled with the up-arrow key."
34,1,"['sum of squared', 'deviations']", Vectorized arithmetic,seg_9,the sum of squared deviations is similarly obtained with
35,1,"['deviation', 'standard deviation', 'standard']", Vectorized arithmetic,seg_9,and all in all the standard deviation becomes
36,1,"['statistical', 'results']", Vectorized arithmetic,seg_9,"of course, since r is a statistical program, such calculations are already built into the program, and you get the same results just by entering"
37,1,"['t test', 'data', 'distribution', 'statistical', 'normal', 'mean', 'function', 'test', 'normal distribution']", Standard procedures,seg_11,"as a slightly more complicated example of what r can do, consider the following: the rule of thumb is that the bmi for a normal-weight individual should be between 20 and 25, and we want to know if our data deviate systematically from that. you might use a one-sample t test to assess whether the six persons’ bmi can be assumed to have mean 22.5 given that they come from a normal distribution. to this end, you can use the function t.test. (you might not know the theory of the t test yet. the example is included here mainly to give some indication of what “real” statistical output looks like. a thorough description of t.test is given in chapter 5.)"
38,1,['mean'], Standard procedures,seg_11,"the argument mu=22.5 attaches a value to the formal argument mu, which represents the greek letter µ conventionally used for the theoretical mean. if this is not given, t.test would use the default mu=0, which is not of interest here."
39,1,"['interval', 'data', 'information', 'mean', 'hypothesis', 'probability', 'test']", Standard procedures,seg_11,"for a test like this, we get a more extensive printout than in the earlier examples. the details of the output are explained in chapter 5, but you might focus on the p-value which is used for testing the hypothesis that the mean is 22.5. the p-value is not small, indicating that it is not at all unlikely to get data like those observed if the mean were in fact 22.5. (loosely speaking; actually p is the probability of obtaining a t value bigger than 0.3449 or less than−0.3449.) however, you might also look at the 95% confidence interval for the true mean. this interval is quite wide, indicating that we really have very little information about the true mean."
40,1,"['model', 'graphical', 'graphics', 'plots', 'data', 'standard', 'control']", Graphics,seg_13,one of the most important aspects of the presentation and analysis of data is the generation of proper graphics. r — like s before it — has a model for constructing plots that allows simple production of standard plots as well as fine control over the graphical components.
41,1,['plot'], Graphics,seg_13,"if you want to investigate the relation between weight and height, the first idea is to plot one versus the other. this is done by"
42,1,"['parameters', 'set', 'plotting']", Graphics,seg_13,"you will often want to modify the drawing in various ways. to that end, there are a wealth of plotting parameters that you can set. as an example, let us try changing the plotting symbol using the keyword pch (“plotting character”) like this:"
43,1,['plot'], Graphics,seg_13,"this gives the plot in figure 1.3, with the points now marked with little triangles."
44,1,['normal'], Graphics,seg_13,"the idea behind the bmi calculation is that this value should be independent of the person’s height, thus giving you a single number as an indication of whether someone is overweight and by how much. since a normal bmi should be about 22.5, you would expect that weight ≈"
45,1,['curve'], Graphics,seg_13,"22.5 × height2. accordingly, you can superimpose a curve of expected weights at bmi 22.5 on the figure:"
46,1,"['function', 'plot']", Graphics,seg_13,"yielding figure 1.4. the function lines will add (x, y) values joined by straight lines to an existing plot."
47,1,"['plot', 'linear', 'curve', 'nonlinear curve', 'nonlinear', 'data', 'distribution', 'variable']", Graphics,seg_13,"the reason for defining a new variable (hh) with heights rather than using the original height vector is twofold. first, the relation between height and weight is a quadratic one and hence nonlinear, although it can be difficult to see on the plot. since we are approximating a nonlinear curve with a piecewise linear one, it will be better to use points that are spread evenly along the x-axis than to rely on the distribution of the original data. sec-"
48,0,[], Graphics,seg_13,"ond, since the values of height are not sorted, the line segments would not connect neighbouring points but would run back and forth between distant points."
49,0,[], R language essentials,seg_15,"this section outlines the basic aspects of the r language. it is necessary to do this in a slightly superficial manner, with some of the finer points glossed over. the emphasis is on items that are useful to know in interactive usage as opposed to actual programming, although a brief section on programming is included."
50,1,['interaction'], Expressions and objects,seg_17,the basic interaction mode in r is one of expression evaluation. the user enters an expression; the system evaluates it and prints the result. some expressions are evaluated not for their result but for side effects such as
51,1,['graphics'], Expressions and objects,seg_17,"putting up a graphics window or writing to a file. all r expressions return a value (possibly null), but sometimes it is “invisible” and not printed."
52,1,"['function', 'variable']", Expressions and objects,seg_17,"expressions typically involve variable references, operators such as +, and function calls, as well as some other items that have not been introduced yet."
53,1,['variable'], Expressions and objects,seg_17,"expressions work on objects. this is an abstract term for anything that can be assigned to a variable. r contains several different types of objects. so far, we have almost exclusively seen numeric vectors, but several other types are introduced in this chapter."
54,0,[], Expressions and objects,seg_17,"although objects can be discussed abstractly, it would make a rather boring read without some indication of how to generate them and what to do with them. conversely, much of the expression syntax makes little sense without knowledge of the objects on which it is intended to work. therefore, the subsequent sections alternate between introducing new objects and introducing new language elements."
55,1,"['plot', 'variables', 'function']", Functions and arguments,seg_19,"at this point, you have obtained an impression of the way r works, and we have already used some of the special terminology when talking about the plot function, etc. that is exactly the point: many things in r are done using function calls, commands that look like an application of a mathematical function of one or several variables; for example, log(x) or plot(height, weight)."
56,1,"['plot', 'set', 'function']", Functions and arguments,seg_19,"the format is that a function name is followed by a set of parentheses containing one or more arguments. for instance, in plot(height,weight) the function name is plot and the arguments are height and weight. these are the actual arguments, which apply only to the current call. a function also has formal arguments, which get connected to actual arguments in the call."
57,1,"['cases', 'standard']", Functions and arguments,seg_19,"when you write plot(height, weight), r assumes that the first argument corresponds to the x-variable and the second one to the y-variable. this is known as positional matching. this becomes unwieldy if a function has a large number of arguments since you have to supply every one of them and remember their position in the sequence. fortunately, r has methods to avoid this: most arguments have sensible defaults and can be omitted in the standard cases, and there are nonpositional ways of specifying them when you need to depart from the default settings."
58,1,"['plot', 'function']", Functions and arguments,seg_19,"the plot function is in fact an example of a function that has a large selection of arguments in order to be able to modify symbols, line widths, titles, axis type, and so forth. we used the alternative form of specifying arguments when setting the plot symbol to triangles with plot(height, weight, pch=2)."
59,1,"['plot', 'function', 'plotting']", Functions and arguments,seg_19,"the pch=2 form is known as a named actual argument, whose name can be matched against the formal arguments of the function and thereby allow keyword matching of arguments. the keyword pch was used to say that the argument is a specification of the plotting character. this type of function argument can be specified in arbitrary order. thus, you can write plot(y=weight,x=height) and get the same plot as with plot(x=height,y=weight)."
60,0,[], Functions and arguments,seg_19,the two kinds of argument specification — positional and named — can be mixed in the same call.
61,1,"['function', 'error', 'results']", Functions and arguments,seg_19,"even if there are no arguments to a function call, you have to write, for example, ls() for displaying the contents of the workspace. a common error is to leave off the parentheses, which instead results in the display of a piece of r code since ls entered by itself indicates that you want to see the definition of the function rather than execute it."
62,1,"['plot', 'method', 'set', 'function']", Functions and arguments,seg_19,"the formal arguments of a function are part of the function definition. the set of formal arguments to a function, for instance plot.default (which is the function that gets called when you pass plot an x argument for which no special plot method exists), may be seen with"
63,1,"['associated', 'function']", Functions and arguments,seg_19,"notice that most of the arguments have defaults, meaning that if you do not specify (say) the type argument, the function will behave as if you had passed type=""p"". the null defaults for many of the arguments really serve as indicators that the argument is unspecified, allowing special behaviour to be defined inside the function. for instance, if they are not specified, the xlab and ylab arguments are constructed from the actual arguments passed as x and y. (there are some very fine points associated with this, but we do not go further into the topic.)"
64,1,"['functions', 'function']", Functions and arguments,seg_19,"the triple-dot (...) argument indicates that this function will accept additional arguments of unspecified name and number. these are often meant to be passed on to other functions, although some functions treat it specially. for instance, in data.frame and c, the names of the ...-arguments become the names of the elements of the result."
65,0,[], Vectors,seg_21,"we have already seen numeric vectors. there are two further types, character vectors and logical vectors."
66,0,[], Vectors,seg_21,"a character vector is a vector of text strings, whose elements are specified and printed in quotes:"
67,0,[], Vectors,seg_21,"it does not matter whether you use singleor double-quote symbols, as long as the left quote is the same as the right quote:"
68,0,[], Vectors,seg_21,"however, you should avoid the acute accent key (´), which is present on some keyboards. double quotes are used throughout this book to prevent mistakes. logical vectors can take the value true or false (or na; see below). in input, you may use the convenient abbreviations t and f (if you"
69,1,['function'], Vectors,seg_21,are careful not to redefine them). logical vectors are constructed using the c function just like the other vector types:
70,1,['function'], Vectors,seg_21,"actually, you will not often have to specify logical vectors in the manner above. it is much more common to use single logical values to turn an option on or off in a function call. vectors of more than one value most often result from relational expressions:"
71,1,['conditional'], Vectors,seg_21,we return to relational expressions and logical operations in the context of conditional selection in section 1.2.12.
72,1,['moment'], Quoting and escape sequences,seg_23,"quoted character strings require some special considerations: how, for instance, do you put a quote symbol inside a string? and what about special characters such as newlines? this is done using escape sequences. we shall look at those in a moment, but first it will be useful to observe the following."
73,1,"['variable', 'variable name']", Quoting and escape sequences,seg_23,"there is a distinction between a text string and the way it is printed. when, for instance, you give the string ""huey"", it is a string of four characters, not six. the quotes are not actually part of the string, they are just there so that the system can tell the difference between a string and a variable name."
74,1,['function'], Quoting and escape sequences,seg_23,"if you print a character vector, it usually comes out with quotes added to each element. there is a way to avoid this, namely to use the cat function. for instance,"
75,0,[], Quoting and escape sequences,seg_23,"this prints the strings without quotes, just separated by a space character. there is no newline following the string, so the prompt (>) for the next line of input follows directly at the end of the line. (notice that when the character vector is printed by cat there is no way of telling the difference from the single string ""huey dewey louie"".)"
76,0,[], Quoting and escape sequences,seg_23,"to get the system prompt onto the next line, you must include a newline character"
77,0,['n'], Quoting and escape sequences,seg_23,"here, \n is an example of an escape sequence. it actually represents a single character, the linefeed (lf), but is represented as two. the backslash (\) is known as the escape character. in a similar vein, you can insert quote characters with \"", as in"
78,1,['control'], Quoting and escape sequences,seg_23,"there are also ways to insert other control characters and special glyphs, but it would lead us too far astray to discuss it in full detail. one important thing, though: what about the escape character itself? this, too, must be escaped, so to put a backslash in a string, you must double it. this is important to know when specifying file paths on windows, see also section 2.4.1."
79,1,"['missing values', 'experiment', 'data', 'associated', 'statistical']", Missing values,seg_25,"in practical data analysis, a data point is frequently unavailable (the patient did not show up, an experiment failed, etc.). statistical software needs ways to deal with this. r allows vectors to contain a special na value. this value is carried through in computations so that operations on na yield na as the result. there are some special issues associated with the handling of missing values; we deal with them as we encounter them (see “missing values” in the index)."
80,1,['functions'], Functions that create vectors,seg_27,"here we introduce three functions, c, seq, and rep, that are used to create vectors in various situations."
81,1,['function'], Functions that create vectors,seg_27,"the first of these, c, has already been introduced. it is short for “con- catenate”, joining items end to end, which is exactly what the function does:"
82,0,[], Functions that create vectors,seg_27,you can also concatenate vectors of more than one element as in
83,0,[], Functions that create vectors,seg_27,"however, you do not need to use c to create vectors of length 1. people sometimes type, for example, c(1), but it is the same as plain 1."
84,0,[], Functions that create vectors,seg_27,it is also possible to assign names to the elements. this modifies the way the vector is printed and is often used for display purposes.
85,1,['case'], Functions that create vectors,seg_27,"(in this case, it does of course make sense to use c even for single-element vectors.)"
86,1,['set'], Functions that create vectors,seg_27,the names can be extracted or set using names:
87,0,[], Functions that create vectors,seg_27,"all elements of a vector have the same type. if you concatenate vectors of different types, they will be converted to the least “restrictive” type:"
88,1,['representations'], Functions that create vectors,seg_27,"that is, logical values may be converted to 0/1 or ""false""/""true"" and numbers converted to their printed representations."
89,1,['function'], Functions that create vectors,seg_27,"the second function, seq (“sequence”), is used for equidistant series of numbers. writing"
90,0,[], Functions that create vectors,seg_27,"yields, as shown, the integers from 4 to 9. if you want a sequence in jumps of 2, write"
91,1,"['curve', 'graphics']", Functions that create vectors,seg_27,"this kind of vector is frequently needed, particularly for graphics. for example, we previously used c(1.65,1.70,1.75,1.80,1.85,1.90) to define the x-coordinates for a curve, something that could also have been"
92,0,[], Functions that create vectors,seg_27,"written seq(1.65,1.90,0.05) (the advantage of using seqmight have been more obvious if the heights had been in steps of 1 cm rather than 5 cm!)."
93,1,['case'], Functions that create vectors,seg_27,the case with step size equal to 1 can also be written using a special syntax:
94,0,[], Functions that create vectors,seg_27,"the above is exactly the same as seq(4,9), only easier to read."
95,1,['function'], Functions that create vectors,seg_27,"the third function, rep (“replicate”), is used to generate repeated values. it is used in two variants, depending on whether the second argument is a vector or a single number:"
96,1,"['function', 'observations']", Functions that create vectors,seg_27,"the first of the function calls above repeats the entire vector oops three times. the second call has the number 3 replaced by a vector with the three values (1, 2, 3); these values correspond to the elements of the oops vector, indicating that 7 should be repeated once, 9 twice, and 13 three times. the rep function is often used for things such as group codes: if it is known that the first 10 observations are men and the last 15 are women, you can use"
97,1,['observation'], Functions that create vectors,seg_27,to form a vector that for each observation indicates whether it is from a man or a woman.
98,1,"['replications', 'case']", Functions that create vectors,seg_27,"the special case where there are equally many replications of each value can be obtained using the each argument. e.g., rep(1:2,each=10) is the same as rep(1:2,c(10,10))."
99,1,['statistics'], Matrices and arrays,seg_29,"a matrix in mathematics is just a two-dimensional array of numbers. matrices are used for many purposes in theoretical and practical statistics, but it is not assumed that the reader is familiar with matrix algebra, so many special operations on matrices, including matrix multiplication, are skipped. (the document “an introduction to r”, which comes with"
100,1,['tables'], Matrices and arrays,seg_29,"the installation, outlines these items quite well.) however, matrices and also higher-dimensional arrays do get used for simpler purposes as well, mainly to hold tables, so an elementary description is in order."
101,0,[], Matrices and arrays,seg_29,"in r, the matrix notion is extended to elements of any type, so you could have, for instance, a matrix of character strings. matrices and arrays are represented as vectors with dimensions:"
102,1,"['function', 'sets']", Matrices and arrays,seg_29,"the dim assignment function sets or changes the dimension attribute of x, causing r to treat the vector of 12 numbers as a 3× 4 matrix. notice that the storage is column-major; that is, the elements of the first column are followed by those of the second, etc."
103,1,['function'], Matrices and arrays,seg_29,a convenient way to create matrices is to use the matrix function:
104,0,[], Matrices and arrays,seg_29,notice how the byrow=t switch causes the matrix to be filled in a rowwise fashion rather than columnwise.
105,1,"['function', 'functions']", Matrices and arrays,seg_29,"useful functions that operate on matrices include rownames, colnames, and the transposition function t (notice the lowercase t as opposed to uppercase t for true), which turns rows into columns and vice versa:"
106,1,['variable'], Matrices and arrays,seg_29,"the character vector letters is a built-in variable that contains the capital letters a–z. similar useful vectors are letters, month.name, and month.abb with lowercase letters, month names, and abbreviated month names."
107,1,['functions'], Matrices and arrays,seg_29,"you can “glue” vectors together, columnwise or rowwise, using the cbind and rbind functions."
108,1,"['table', 'variables', 'data', 'set', 'data set']", Matrices and arrays,seg_29,"we return to table operations in section 4.5, which discusses tabulation of variables in a data set."
109,1,"['categorical', 'variables', 'data', 'statistical', 'categorical variables']", Factors,seg_31,"it is common in statistical data to have categorical variables, indicating some subdivision of data, such as social class, primary diagnosis, tumor stage, tanner stage of puberty, etc. typically, these are input using a numeric code."
110,1,"['factors', 'variables', 'data', 'categories']", Factors,seg_31,such variables should be specified as factors in r. this is a data structure that (among other things) makes it possible to assign meaningful names to the categories.
111,1,"['categorical', 'variables', 'numerical']", Factors,seg_31,there are analyses where it is essential for r to be able to distinguish between categorical codes and variables whose values have a direct numerical meaning (see chapter 7).
112,1,"['levels', 'factor', 'set']", Factors,seg_31,"the terminology is that a factor has a set of levels — say four levels for concreteness. internally, a four-level factor consists of two items: (a) a vector of integers between 1 and 4 and (b) a character vector of length 4 containing strings describing what the four levels are. let us look at an example:"
113,1,"['level', 'levels', 'categorical', 'factor', 'variable', 'categorical variable', 'function']", Factors,seg_31,"the first command creates a numeric vector pain, encoding the pain levels of five patients. we wish to treat this as a categorical variable, so we create a factor fpain from it using the function factor. this is called with one argument in addition to pain, namely levels=0:3, which indicates that the input coding uses the values 0–3. the latter can in principle be left out since r by default uses the values in pain, suitably sorted, but it is a good habit to retain it; see below. the effect of the final line is that the level names are changed to the four specified character strings."
114,0,[], Factors,seg_31,the result should be apparent from the following:
115,1,"['levels', 'factor', 'function', 'numerical']", Factors,seg_31,the function as.numeric extracts the numerical coding as numbers 1–4 and levels extracts the names of the levels. notice that the original input coding in terms of numbers 0–3 has disappeared; the internal representation of a factor always uses numbers starting at 1.
116,1,"['ordinal', 'levels', 'factors', 'variables', 'factor', 'contrasts', 'nominal', 'function']", Factors,seg_31,"r also allows you to create a special kind of factor in which the levels are ordered. this is done using the ordered function, which works similarly to factor. these are potentially useful in that they distinguish nominal and ordinal variables from each other (and arguably text.pain above ought to have been an ordered factor). unfortunately, r defaults to treating the levels as if they were equidistant in the modelling code (by generating polynomial contrasts), so it may be better to ignore ordered factors at this stage."
117,0,[], Lists,seg_33,it is sometimes useful to combine a collection of objects into a larger composite object. this can be done using lists.
118,1,['function'], Lists,seg_33,you can construct a list from its components with the function list.
119,1,"['set', 'data']", Lists,seg_33,"as an example, consider a set of data from altman (1991, p. 183) concerning preand postmenstrual energy intake in a group of women. we can place these data in two vectors as follows:"
120,1,"['cases', 'normal']", Lists,seg_33,"notice how input lines can be broken and continue on the next line. if you press the enter key while an expression is syntactically incomplete, r will assume that the expression continues on the next line and will change its normal > prompt to the continuation prompt +. this often happens inadvertently due to a forgotten parenthesis or a similar problem; in such cases, either complete the expression on the next line or press esc (windows and macintosh) or ctrl-c (unix). the “stop” button can also be used under windows."
121,0,[], Lists,seg_33,"to combine these individual vectors into a list, you can say"
122,0,[], Lists,seg_33,the components of the list are named according to the argument names used in list. named components may be extracted like this:
123,1,"['functions', 'results']", Lists,seg_33,many of r’s built-in functions compute more than a single vector of values and return their results in the form of a list.
124,1,"['experimental', 'factors', 'data', 'set', 'experimental unit', 'statistical']", Data frames,seg_35,"a data frame corresponds to what other statistical packages call a “data matrix” or a “data set”. it is a list of vectors and/or factors of the same length that are related “across” such that data in the same position come from the same experimental unit (subject, animal, etc.). in addition, it has a unique set of row names."
125,1,"['variables', 'data frames', 'data']", Data frames,seg_35,you can create data frames from preexisting variables:
126,1,"['data', 'paired']", Data frames,seg_35,"notice that these data are paired, that is, the same woman has an intake of 5260 kj premenstrually and 3910 kj postmenstrually."
127,1,['variables'], Data frames,seg_35,"as with lists, components (i.e., individual variables) can be accessed using the $ notation:"
128,0,[], Indexing,seg_37,"if you need a particular element in a vector, for instance the premenstrual energy intake for woman no. 5, you can do"
129,1,['data'], Indexing,seg_37,"the brackets are used for selection of data, also known as indexing or subsetting. this also works on the left-hand side of an assignment (so that you can say, for instance, intake.pre[5] <- 6390) if you want to modify elements of a vector."
130,1,['data'], Indexing,seg_37,"if you want a subvector consisting of data for more than one woman, for instance nos. 3, 5, and 7, you can index with a vector:"
131,1,['mean'], Indexing,seg_37,"note that it is necessary to use the c(...)-construction to define the vector consisting of the three numbers 3, 5, and 7. intake.pre[3,5,7] would mean something completely different. it would specify indexing into a three-dimensional array."
132,1,"['variables', 'variable']", Indexing,seg_37,"of course, indexing with a vector also works if the index vector is stored in a variable. this is useful when you need to index several variables in the same way."
133,0,[], Indexing,seg_37,"it is also worth noting that to get a sequence of elements, for instance the first five, you can use the a:b notation:"
134,1,['observations'], Indexing,seg_37,"a neat feature of r is the possibility of negative indexing. you can get all observations except nos. 3, 5, and 7 by writing"
135,0,[], Indexing,seg_37,it is not possible to mix positive and negative indices. that would be highly ambiguous.
136,1,['data'], Conditional selection,seg_39,"we saw in section 1.2.11 how to extract data using one or several indices. in practice, you often need to extract data that satisfy certain criteria, such as data from the males or the prepubertal or those with chronic diseases, etc. this can be done simply by inserting a relational expression instead of the index,"
137,0,[], Conditional selection,seg_39,yielding the postmenstrual energy intake for the four women who had an energy intake above 7000 kj premenstrually.
138,1,"['variables', 'variable']", Conditional selection,seg_39,"of course, this kind of expression makes sense only if the variables that go into the relational expression have the same length as the variable being indexed."
139,1,['function'], Conditional selection,seg_39,"the comparison operators available are < (less than), > (greater than), == (equal to), <= (less than or equal to), >= (greater than or equal to), and != (not equal to). notice that a double equal sign is used for testing equality. this is to avoid confusion with the = symbol used to match keywords with function arguments. also, the != operator is new to some; the ! symbol indicates negation. the same operators are used in the c programming language."
140,1,['logical operators'], Conditional selection,seg_39,"to combine several expressions, you can use the logical operators & (logical “and”), | (logical “or”), and ! (logical “not”). for instance, we find the postmenstrual intake for women with a premenstrual intake between 7000 and 8000 kj with"
141,1,['control'], Conditional selection,seg_39,"there are also && and ||, which are used for flow control in r programming. however, their use is beyond what we discuss here."
142,0,[], Conditional selection,seg_39,it may be worth taking a closer look at what actually happens when you use a logical expression as an index. the result of the logical expression is a logical vector as described in section 1.2.3:
143,0,[], Conditional selection,seg_39,"indexing with a logical vector implies that you pick out the values where the logical vector is true, so in the preceding example we got the 8th and 9th values in intake.post."
144,1,"['set', 'missing values']", Conditional selection,seg_39,"if missing values (na; see section 1.2.5) appear in an indexing vector, then r will create the corresponding elements in the result but set the values to na."
145,1,"['functions', 'logical operators']", Conditional selection,seg_39,"in addition to the relational and logical operators, there are a series of functions that return a logical value. a particularly important one is is.na(x), which is used to find out which elements of x are recorded as missing (na)."
146,0,[], Conditional selection,seg_39,notice that there is a real need for is.na because you cannot make comparisons of the form x==na. that simply gives na as the result for any value of x. the result of a comparison with an unknown value is unknown!
147,1,"['variables', 'data']", Indexing of data frames,seg_41,"we have already seen how it is possible to extract variables from a data frame by typing, for example, d$intake.post. however, it is also possible to use a notation that uses the matrix-like structure directly:"
148,1,['measurement'], Indexing of data frames,seg_41,"gives fifth row, first column (that is, the “pre” measurement for woman no. 5), and"
149,1,"['measurements', 'data']", Indexing of data frames,seg_41,"gives all measurements for woman no. 5. notice that the comma in d[5,] is required; without the comma, for example d[2], you get the data frame"
150,0,[], Indexing of data frames,seg_41,"consisting of the second column of d (that is, more like d[,2], which is the column itself)."
151,1,"['cases', 'data']", Indexing of data frames,seg_41,"other indexing techniques also apply. in particular, it can be useful to extract all data for cases that satisfy some criterion, such as women with a premenstrual intake above 7000 kj:"
152,1,['data'], Indexing of data frames,seg_41,here we extracted the rows of the data frame where intake.pre>7000. notice that the row names are those of the original data frame.
153,0,[], Indexing of data frames,seg_41,"if you want to understand the details of this, it may be a little easier if it is divided into smaller steps. it could also have been done like this:"
154,1,['data'], Indexing of data frames,seg_41,"what happens is that sel (select) becomes a logical vector with the value true for to the four women consuming more than 7000 kj premenstrually. indexing as d[sel,] yields data from the rows where sel is true and from all columns because of the empty field after the comma."
155,1,"['data', 'cases', 'set', 'data set']", Indexing of data frames,seg_41,"it is often convenient to look at the first few cases in a data set. this can be done with indexing, like this:"
156,1,['function'], Indexing of data frames,seg_41,"this is such a frequent occurrence that a convenience function called head exists. by default, it shows the first six lines."
157,1,['tail'], Indexing of data frames,seg_41,"similarly, tail shows the last part."
158,1,"['factor', 'grouped data', 'data', 'set', 'data set']", Grouped data and data frames,seg_43,"the natural way of storing grouped data in a data frame is to have the data themselves in one vector and parallel to that have a factor telling which data are from which group. consider, for instance, the following data set on energy expenditure for lean and obese women."
159,1,['data'], Grouped data and data frames,seg_43,"this is a convenient format since it generalizes easily to data classified by multiple criteria. however, sometimes it is desirable to have data in a separate vector for each group. fortunately, it is easy to extract these from the data frame:"
160,1,['function'], Grouped data and data frames,seg_43,"alternatively, you can use the split function, which generates a list of vectors according to a grouping."
161,1,['functions'], Implicit loops,seg_45,"the looping constructs of r are described in section 2.3.1. for the purposes of this book, you can largely ignore their existence. however, there is a group of r functions that it will be useful for you to know about."
162,1,"['functions', 'results', 'data', 'variable', 'set', 'mean', 'function']", Implicit loops,seg_45,"a common application of loops is to apply a function to each element of a set of values or vectors and collect the results in a single structure. in r this is abstracted by the functions lapply and sapply. the former always returns a list (hence the ‘l’), whereas the latter tries to simplify (hence the ‘s’) the result to a vector or a matrix if possible. so, to compute the mean of each variable in a data frame of numeric vectors, you can do the following:"
163,1,"['functions', 'missing values', 'case', 'mean', 'function']", Implicit loops,seg_45,"notice how both forms attach meaningful names to the result, which is another good reason to prefer to use these functions rather than explicit loops. the second argument to lapply/sapply is the function that should be applied, here mean. any further arguments are passed on to the function; in this case we pass na.rm=t to request that missing values be removed (see section 4.1)."
164,1,"['simulation', 'results', 'replicate', 'case']", Implicit loops,seg_45,"sometimes you just want to repeat something a number of times but still collect the results as a vector. obviously, this makes sense only when the repeated computations actually give different results, the common case being simulation studies. this can be done using sapply, but there is a simplified version called replicate, in which you just have to give a count and the expression to evaluate:"
165,1,['function'], Implicit loops,seg_45,"a similar function, apply, allows you to apply a function to the rows or columns of a matrix (or over indices of a multidimensional array in general) as in"
166,1,"['function', 'case']", Implicit loops,seg_45,the second argument is the index (or vector of indices) that defines what the function is applied to; in this case we get the columnwise minima.
167,1,"['factors', 'table', 'factor', 'case', 'tables', 'function']", Implicit loops,seg_45,"also, the function tapply allows you to create tables (hence the ‘t’) of the value of a function on subgroups defined by its second argument, which can be a factor or a list of factors. in the latter case a cross-classified table is generated. (the grouping can also be defined by ordinary vectors. they will be converted to factors internally.)"
168,1,"['data', 'set', 'data set', 'function']", Sorting,seg_47,it is trivial to sort a vector. just use the sort function. (we use the builtin data set intake here; it contains the same data that were used in section 1.2.9.)
169,0,[], Sorting,seg_47,(intake$pre could not be used for this example since it is sorted already!)
170,1,['variables'], Sorting,seg_47,"however, sorting a single vector is not always what is required. often you need to sort a series of variables according to the values of some other variables — blood pressures sorted by sex and age, for instance. for this"
171,1,['variable'], Sorting,seg_47,"purpose, there is a construction that may look somewhat abstract at first but is really very powerful. you first compute an ordering of a variable."
172,0,[], Sorting,seg_47,"the result is the numbers 1 to 11 (or whatever the length of the vector is), sorted according to the size of the argument to order (here intake$post). interpreting the result of order is a bit tricky — it should be read as follows: you sort intake$post by placing its values in the order no. 3, no. 1, no. 2, no. 6, etc."
173,1,['variables'], Sorting,seg_47,"the point is that, by indexing with this vector, other variables can be sorted by the same criterion. note that indexing with a vector containing the numbers from 1 to the number of elements exactly once corresponds to a reordering of the elements."
174,0,[], Sorting,seg_47,what has happened here is that intake$post has been sorted — just as in sort(intake$post) — while intake$pre has been sorted by the size of the corresponding intake$post.
175,1,['data'], Sorting,seg_47,it is of course also possible to sort the entire data frame intake
176,1,['variable'], Sorting,seg_47,"sorting by several criteria is done simply by having several arguments to order; for instance, order(sex,age)will give a main division into men and women, and within each sex an ordering by age. the second variable is used when the order cannot be decided from the first variable. sorting in reverse order can be handled by, for example, changing the sign of the variable."
177,1,['function'], Exercises,seg_49,1.1 how would you check whether two vectors are the same if they may contain missing (na) values? (use of the identical function is considered cheating!)
178,1,"['factor', 'levels']", Exercises,seg_49,"1.2 if x is a factor with n levels and y is a length n vector, what happens if you compute y[x]?"
179,1,"['set', 'data set', 'data']", Exercises,seg_49,1.3 write the logical expression to use to extract girls between 7 and 14 years of age in the juul data set.
180,1,"['levels of a factor', 'factor', 'levels']", Exercises,seg_49,1.4 what happens if you change the levels of a factor (with levels) and give the same value to two or more levels?
181,1,"['replicate', 'exponential distribution', 'distribution', 'exponential', 'mean', 'random', 'random numbers']", Exercises,seg_49,"1.5 on p. 27, replicate was used to simulate the distribution of the mean of 20 random numbers from the exponential distribution by repeating the operation 10 times. how would you do the same thing with sapply?"
182,1,"['graphical', 'parameters', 'data']", The R environment,seg_51,"this chapter collects some practical aspects of working with r. it describes issues regarding the structure of the workspace, graphical devices and their parameters, and elementary programming, and includes a fairly extensive, although far from complete, discussion of data entry."
183,1,"['variables', 'function']", The workspace,seg_55,"all variables created in r are stored in a common workspace. to see which variables are defined in the workspace, you can use the function ls (list). it should look as follows if you have run all the examples in the preceding chapter:"
184,0,[], The workspace,seg_55,remember that you cannot omit the parentheses in ls().
185,0,[], The workspace,seg_55,"if at some point things begin to look messy, you can delete some of the objects. this is done using rm (remove), so that"
186,1,['variables'], The workspace,seg_55,deletes the variables height and weight.
187,1,['variables'], The workspace,seg_55,"the entire workspace can be cleared using rm(list=ls()) and also via the “remove all objects” or “clear workspace” menu entries in the windows and macintosh guis. this does not remove variables whose name begins with a dot because they are not listed by ls() — you would need ls(all=t) for that, but it could be dangerous because such names are used for system purposes."
188,0,[], The workspace,seg_55,"if you are acquainted with the unix operating system, for which the s language, which preceded r, was originally written, then you will know that the commands for listing and removing files in unix are called precisely ls and rm."
189,0,[], The workspace,seg_55,it is possible to save the workspace to a file at any time. if you just write
190,0,[], The workspace,seg_55,"then it will be saved to a file called .rdata in your working directory. the windows version also has this on the file menu. when you exit r, you will be asked whether to save the workspace image; if you accept, the same thing will happen. it is also possible to specify an alternative filename (within quotes). you can also save selected objects with save. the .rdata file is loaded by default when r is started in its directory. other save files can be loaded into your current workspace using load."
191,1,"['statistics', 'standard']", Textual output,seg_57,"it is important to note that the workspace consists only of r objects, not of any of the output that you have generated during a session. if you want to save your output, use “save to file” from the file menu in windows or use standard cut-and-paste facilities. you can also use ess (emacs speaks statistics), which works on all platforms. it is a “mode” for the emacs editor where you can run your entire session in an emacs buffer. you can get ess and installation instructions for it from cran (see appendix a)."
192,0,[], Textual output,seg_57,"an alternative way of diverting output to a file is to use the sink function. this is largely a relic from the days of the 80× 25 computer terminal, where cut-and-paste techniques were not available, but it can still be use-"
193,0,[], Textual output,seg_57,"ful at times. in particular, it can be used in batch processing. the way it works is as follows:"
194,1,['normal'], Textual output,seg_57,"no output appears! this is because the output goes into the file myfile in the current directory. the system will remain in a state where commands are processed, but the output (apparently) goes into the drain until the normal state of affairs is reestablished by"
195,1,['sets'], Textual output,seg_57,"the current working directory can be obtained by getwd() and changed by setwd(mydir), where mydir is a character string. the initial working directory is system-dependent; for instance, the windows gui sets it to the user’s home directory, and command line versions use the directory from which you start r."
196,1,"['level', 'cases', 'memory']", Scripting,seg_59,"beyond a certain level of complexity, you will not want to work with r on a line-by-line basis. for instance, if you have entered an 8× 8 matrix over eight lines and realize that you made a mistake, you will find yourself using the up-arrow key 64 times to reenter it! in such cases, it is better to work with r scripts, collections of lines of r code stored either in a file or in computer memory somehow."
197,1,"['function', 'set']", Scripting,seg_59,"one option is to use the source function, which is sort of the opposite of sink. it takes the input (i.e., the commands from a file) and runs them. notice, though, that the entire file is syntax-checked before anything is executed. it is often useful to set echo=t in the call so that commands are printed along with the output."
198,0,[], Scripting,seg_59,"another option is more interactive in nature. you can work with a script editor window, which allows you to submit one or more lines of the script to a running r, which will then behave as if the same lines had been entered at the prompt. the windows and macintosh versions of r have simple scripting windows built-in, and a number of text editors also have features for sending commands to r; popular choices on windows include tinn-r and winedt. this is also available as part of ess (see the preceding section)."
199,0,[], Scripting,seg_59,"the history of commands entered in a session can be saved and reloaded using the savehistory and loadhistory commands, which are also mapped to menu entries in windows. saved histories can be useful as a"
200,1,['function'], Scripting,seg_59,starting point for writing scripts; notice also that the history() function will show the last commands entered at the console (up to a maximum of 25 lines by default).
201,1,['statistical'], Getting help,seg_61,"r can do a lot more than what a typical beginner can be expected to need or even understand. this book is written so that most of the code you are likely to need in relation to the statistical procedures is described in the text, and the compendium in appendix c is designed to provide a basic overview. however, it is obviously not possible to cover everything."
202,0,[], Getting help,seg_61,r also comes with extensive online help in text form as well as in the form of a series of html files that can be read using a web browser such as netscape or internet explorer. the help pages can be accessed via “help” in the menu bar on windows and by entering help.start() on any platform. you will find that the pages are of a technical nature. precision and conciseness here take precedence over readability and pedagogy (something one learns to appreciate after exposure to the opposite).
203,1,['function'], Getting help,seg_61,"from the command line, you can always enter help(aggregate) to get help on the aggregate function or use the prefix form ?aggregate. if the html viewer is running, then the help page is shown there. otherwise it is shown as text either through a pager to the terminal window or in a separate window."
204,1,"['correlation', 'correlation coefficient', 'function', 'coefficient']", Getting help,seg_61,"notice that the html version of the help system features a very useful “search engine and keywords” and that the apropos function allows you to get a list of command names that contain a given pattern. the function help.search is similar but uses fuzzy matching and searches deeper into the help pages, so that it will be able to locate, for example, kendall’s correlation coefficient in cor.test if you use help.search(""kendal"")."
205,1,"['set', 'distributions']", Getting help,seg_61,"also available with the r distributions is a set of documents in various formats. of particular interest is “an introduction to r”, originally based on a set of notes for s-plus by bill venables and david smith and modified for r by various people. it contains an introduction to the r language and environment in a rather more language-centric fashion than this book. on the windows platform, you can choose to install pdf documents as part of the installation procedure so that — provided the adobe acrobat reader program is also installed — it can be accessed via the help menu. an html version (without pictures) can be accessed via the browser interface on all platforms."
206,0,[], Packages,seg_63,"an r installation contains one or more libraries of packages. some of these packages are part of the basic installation. others can be downloaded from cran (see appendix a), which currently hosts over 1000 packages for various purposes. you can even create your own packages."
207,1,['set'], Packages,seg_63,"a library is generally just a folder on your disk. a system library is created when r is installed. in some installations, users may be prohibited from modifying the system library. it is possible to set up private user libraries; see help("".library"") for details."
208,1,"['functions', 'data', 'sets', 'data sets']", Packages,seg_63,"a package can contain functions written in the r language, dynamically loaded libraries of compiled code (written in c or fortran mostly), and data sets. it generally implements functionality that most users will probably not need to have loaded all the time. a package is loaded into r using the library command, so to load the survival package you should enter"
209,0,[], Packages,seg_63,"the loaded packages are not considered part of the user workspace. if you terminate your r session and start a new session with the saved workspace, then you will have to load the packages again. for the same reason, it is rarely necessary to remove a package that you have loaded, but it can be done if desired with"
210,1,"['memory', 'data', 'distribution', 'sets', 'standard', 'data sets']", Builtin data,seg_65,"many packages, both inside and outside the standard r distribution, come with built-in data sets. such data sets can be rather large, so it is not a good idea to keep them all in computer memory at all times. a mechanism for on-demand loading is required. in many packages, this works via a mechanism called lazy loading, which allows the system to “pretend” that the data are in memory, but in fact they are not loaded until they are referenced for the first time."
211,1,"['function', 'data']", Builtin data,seg_65,"with this mechanism, data are “just there”. for example, if you type “thue- sen”, the data frame of that name is displayed. some packages still require explicit calls to the data function. most often, this loads a data frame with the name that its argument specifies; data(thuesen) will, for instance, load the thuesen data frame."
212,1,"['data', 'associated']", Builtin data,seg_65,"what data does is to go through the data directories associated with each package (see section 2.1.5) and look for files whose basename matches the given name. depending on the file extension, several things can then happen. files with a .tab extension are read using read.table (section 2.4), whereas files with a .r extension are executed as source files (and could, in general, do anything!), to give two common examples."
213,1,['data'], Builtin data,seg_65,"if there is a subdirectory of the current directory called data, then it is searched as well. this can be quite a handy way of organizing your personal projects."
214,1,"['variables', 'data frames', 'data']", attach and detach,seg_67,the notation for accessing variables in data frames gets rather heavy if you repeatedly have to write longish commands like
215,1,"['variables', 'data']", attach and detach,seg_67,"fortunately, you can make r look for objects among the variables in a given data frame, for example thuesen. you write"
216,1,['data'], attach and detach,seg_67,and then thuesen’s data are available without the clumsy $-notation:
217,1,['data'], attach and detach,seg_67,what happens is that the data frame thuesen is placed in the system’s search path. you can view the search path with search:
218,1,"['functions', 'wilcoxon test', 'data', 'sets', 'set', 'statistical', 'data sets', 'standard', 'test']", attach and detach,seg_67,"notice that thuesen is placed as no. 2 in the search path. .globalenv is the workspace and package:base is the system library where all standard functions are defined. autoloads is not described here. package:stats and onwards contains the basic statistical routines such as the wilcoxon test, and the other packages similarly contain various functions and data sets. (the package system is modular, and you can run r with a minimal set of packages for specific uses.) finally, package:iswr contains the data sets used for this book."
219,1,"['factors', 'variables', 'case', 'data']", attach and detach,seg_67,"there may be several objects of the same name in different parts of the search path. in that case, r chooses the first one (that is, it searches first in .globalenv, then in thuesen, and so forth). for this reason, you need to be a little careful with “loose” objects that are defined in the workspace outside a data frame since they will be used before any vectors and factors of the same name in an attached data frame. for the same reason, it is not a good idea to give a data frame the same name as one of the variables inside it. note also that changing a data frame after attaching it will not affect the variables available since attach involves a (virtual) copy operation of the data frame."
220,1,"['data frames', 'data']", attach and detach,seg_67,"it is not possible to attach data frames in front of .globalenv or following package:base. however, it is possible to attach more than one data frame. new data frames are inserted into position 2 by default, and everything except .globalenv moves one step to the right. it is, however, possible to specify that a data frame should be searched before .globalenv by using constructions of the form"
221,1,"['method', 'variable', 'function']", attach and detach,seg_67,"in some contexts, r uses a slightly different method when looking for objects. if looking for a variable of a specific type (usually a function), r will skip those of other types. this is what saves you from the worst consequences of accidentally naming a variable (say) c, even though there is a system function of the same name."
222,1,['data'], attach and detach,seg_67,"you can remove a data frame from the search path with detach. if no arguments are given, the data frame in position 2 is removed, which is generally what is desired. .globalenv and package:base cannot be detach’ed."
223,1,"['functions', 'transformed', 'variables', 'data', 'data frames', 'variable']", subset transform and within,seg_69,"you can attach a data frame to avoid the cumbersome indexing of every variable inside of it. however, this is less helpful for selecting subsets of data and for creating new data frames with transformed variables. a couple of functions exist to make these operations easier. they are used as follows:"
224,1,"['variables', 'data']", subset transform and within,seg_69,notice that the variables used in the expressions for new variables or for subsetting are evaluated with variables taken from the data frame.
225,1,"['missing values', 'observations']", subset transform and within,seg_69,"subset also works on single vectors. this is nearly the same as indexing with a logical vector (such as short.velocity[blood.glucose<7]), except that observations with missing values in the selection criterion are excluded."
226,1,"['variables', 'data']", subset transform and within,seg_69,subset also has a select argument which can be used to extract variables from the data frame. we shall return to this in section 10.3.1.
227,1,"['variables', 'function', 'transform']", subset transform and within,seg_69,"the transform function has a couple of drawbacks, the most serious of which is probably that it does not allow chained calculations where some of the new variables depend on the others. the = signs in the syntax are not assignments, but indicate names, which are assigned to the computed vectors in the last step."
228,1,"['function', 'transform']", subset transform and within,seg_69,"an alternative to transform is the within function, which can be used like this:"
229,1,"['variables', 'results', 'data', 'function']", subset transform and within,seg_69,"notice that the second argument is an arbitrary expression (here a compound expression, see p. 45). the function is similar to with, but instead of just returning the computed value, it collects all new and modified variables into a modified data frame, which is then returned. as shown, variables containing intermediate results can be discarded with rm. (it is particularly important to do this if the contents are incompatible with the data frame.)"
230,1,"['plot', 'curve', 'graphics', 'statistical graphics', 'statistical']", The graphics subsystem,seg_71,"in section 1.1.5, we saw how to generate a simple plot and superimpose a curve on it. it is quite common in statistical graphics for you to want to create a plot that is slightly different from the default: sometimes you will want to add annotation, sometimes you want the axes to be different — labels instead of numbers, irregular placement of tick marks, etc. all these things can be obtained in r. the methods for doing them may feel slightly unusual at first, but offers a very flexible and powerful approach."
231,1,"['plot', 'results', 'plots']", The graphics subsystem,seg_71,"in this section, we look deeper into the structure of a typical plot and give some indication of how you can work with plots to achieve your desired results. beware, though, that this is a large and complex area and it is not within the scope of this book to cover it completely. in fact, we completely ignore important newer tools in the grid and lattice packages."
232,1,"['plot', 'model', 'graphics', 'data', 'plotting']", Plot layout,seg_73,"in the graphics model that r uses, there is (for a single plot) a figure region containing a central plotting region surrounded by margins. coordinates inside the plotting region are specified in data units (the kind generally used to label the axes). coordinates in the margins are specified in lines of text as you move in a direction perpendicular to a side of the plotting region but in data units as you move along the side. this is useful since you generally want to put text in the margins of a plot."
233,1,"['plot', 'standard']", Plot layout,seg_73,"a standard x–y plot has an x and a y title label generated from the expressions being plotted. you may, however, override these labels and also"
234,1,['plot'], Plot layout,seg_73,"add two further titles, a main title above the plot and a subtitle at the very bottom, in the plot call."
235,1,"['plot', 'plotting']", Plot layout,seg_73,"inside the plotting region, you can place points and lines that are either specified in the plot call or added later with points and lines. you can also place a text with"
236,1,['plots'], Plot layout,seg_73,"here, the abline call is just to show how the text is centered on the point (0.6, 0.6). (normally, abline plots the line y = a + bx when given a and b as arguments, but it can also be used to draw horizontal and vertical lines as shown.)"
237,1,['function'], Plot layout,seg_73,the margin coordinates are used by the mtext function. they can be demonstrated as follows:
238,1,['plotting'], Plot layout,seg_73,"the for loop (see section 2.3.1) places the numbers −1 to 4 on corresponding lines in each of the four margins at an off-center position of 0.7 measured in user coordinates. the subsequent call places a label on each side, giving the side number. the argument font=2means that a boldface font is used. notice in figure 2.1 that not all the margins are wide enough to hold all the numbers and that it is possible to use negative line numbers to place text within the plotting region."
239,1,"['plot', 'plots', 'case', 'standard']", Building a plot from pieces,seg_75,"high-level plots are composed of elements, each of which can also be drawn separately. the separate drawing commands often allow finer control of the element, so a standard strategy to achieve a given effect is first to draw the plot without that element and add the element subsequently. as an extreme case, the following command will plot absolutely nothing:"
240,1,"['plot', 'set']", Building a plot from pieces,seg_75,"here type=""n"" causes the points not to be drawn. axes=f suppresses the axes and the box around the plot, and the x and y title labels are set to empty strings."
241,1,"['plot', 'data', 'sets', 'mean', 'plotting']", Building a plot from pieces,seg_75,"however, the fact that nothing is plotted does not mean that nothing happened. the command sets up the plotting region and coordinate systems just as if it had actually plotted the data. to add the plot elements, evaluate the following:"
242,1,"['plot', 'set']", Building a plot from pieces,seg_75,notice how the second axis call specifies an alternative set of tick marks (and labels). this is a common technique used to create special axes on a plot and might also be used to create nonequidistant axes as well as axes with nonnumeric labelling.
243,1,"['plot', 'data']", Building a plot from pieces,seg_75,"plotting with type=""n"" is sometimes a useful technique because it has the side effect of dimensioning the plot area. for instance, to create a plot with different colours for different groups, you could first plot all data with type=""n"", ensuring that the plot region is large enough, and then"
244,1,['case'], Building a plot from pieces,seg_75,add the points for each group using points. (passing a vector argument for col is more expedient in this particular case.)
245,1,"['plot', 'function', 'control']", Using par,seg_77,"the par function allows incredibly fine control over the details of a plot, although it can be quite confusing to the beginner (and even to experienced users at times). the best strategy for learning it may well be simply to try and pick up a few useful tricks at a time and once in a while try to solve a particular problem by poring over the help page."
246,1,"['functions', 'parameters', 'set', 'parameter']", Using par,seg_77,"some of the parameters, but not all, can also be set via arguments to plotting functions, which also have some arguments that cannot be set by par. when a parameter can be set by both methods, the difference is generally that if something is set via par, then it stays set subsequently."
247,1,"['plot', 'parameters', 'control']", Using par,seg_77,"the par settings allow you to control line width and type, character size and font, colour, style of axis calculation, size of the plot and figure regions, clipping, etc. it is possible to divide a figure into several subfigures by using the mfrow and mfcol parameters."
248,1,"['plot', 'plots', 'set', 'plotting']", Using par,seg_77,"for instance, the default margin sizes are just over 5, 4, 4, and 2 lines. you might set par(mar=c(4,4,2,2)+0.1) before plotting. this shaves one line off the bottom margin and two lines off the top margin of the plot, which will reduce the amount of unused whitespace when there is no main title or subtitle. if you look carefully, you will in fact notice that figure 2.1 has a somewhat smaller plotting region than the other plots in this book. this is because the other plots have been made with reduced margins for typesetting reasons."
249,1,"['graphics', 'parameters', 'plots']", Using par,seg_77,"however, it is quite pointless to describe the graphics parameters completely at this point. instead, we return to them as they are used for specific plots."
250,1,"['plot', 'histogram', 'information', 'normal', 'histograms']", Combining plots,seg_79,"some special considerations arise when you wish to put several elements together in the same plot. consider overlaying a histogram with a normal density (see sections 4.2 and 4.4.1 for information on histograms and section 3.5.1 for density). the following is close, but only nearly good enough (figure not shown)."
251,1,"['plot', 'density function', 'densities', 'curve', 'histogram', 'set', 'normal', 'function']", Combining plots,seg_79,"the freq=f argument to hist ensures that the histogram is in terms of densities rather than absolute counts. the curve function graphs an expression (in terms of x) and its add=t allows it to overplot an existing plot. so things are generally set up correctly, but sometimes the top of the density function gets chopped off. the reason is of course that the height of the normal density played no role in the setting of the y-axis for the histogram. it will not help to reverse the order and draw the curve first and add the histogram because then the highest bars might get clipped."
252,1,['plot'], Combining plots,seg_79,the solution is first to get hold of the magnitude of the y values for both plot elements and make the plot big enough to hold both (figure 2.2):
253,1,"['plot', 'range', 'normal']", Combining plots,seg_79,"when called with plot=f, hist will not plot anything, but it will return a structure containing the bar heights on the density scale. this and the fact that the maximum of dnorm(x) is dnorm(0) allows us to calculate a range covering both the bars and the normal density. the zero in"
254,1,"['function', 'range']", Combining plots,seg_79,"the range call ensures that the bottom of the bars will be in range, too. the range of y values is then passed to the hist function via the ylim argument."
255,1,"['deviation', 'functions', 'data', 'set', 'mean', 'standard', 'function', 'standard deviation', 'statistical']", R programming,seg_81,"it is possible to write your own r functions. in fact, this is a major aspect and attraction of working with the system in the long run. this book largely avoids the issue in favour of covering a larger set of basic statistical procedures that can be executed from the command line. however, to give you a feel for what can be done, consider the following function, which wraps the code from the example of section 2.2.4 so that you can just say hist.with.normal(rnorm(200)). it has been slightly extended so that it now uses the empirical mean and standard deviation of the data instead of just 0 and 1."
256,0,[], R programming,seg_81,"notice the use of a default argument for xlab. if xlab is not specified, then it is obtained from this expression, which evaluates to a character form of the expression given for x; that is, if you pass rnorm(100) for x, then the x label becomes “rnorm(100)”. notice also the use of a ... argument, which collects any additional arguments and passes them on to hist in the two calls."
257,1,['functions'], R programming,seg_81,"you can learn more about programming in r by studying the built-in functions, starting with simple ones like log10 or weighted.mean."
258,1,"['method', 'conditional']", Flow control,seg_83,"until now, we have seen components of the r language that cause evaluation of single expressions. however, r is a true programming language that allows conditional execution and looping constructs as well. consider, for instance, the following code. (the code implements a version of newton’s method for calculating the square root of y.)"
259,1,"['test', 'condition']", Flow control,seg_83,"notice the while(condition) expression construction, which says that the expression should be evaluated as long as the condition is true. the test occurs at the top of the loop, so the expression might never be evaluated."
260,1,"['test', 'algorithm', 'variation']", Flow control,seg_83,a variation of the same algorithm with the test at the bottom of the loop can be written with a repeat construction:
261,1,"['control', 'compound', 'conditional']", Flow control,seg_83,"this also illustrates three other flow control structures: (a) a compound expression, several expressions held together between curly braces; (b) an if construction for conditional execution; and (c) a break expression, which causes the enclosing loop to exit."
262,1,['condition'], Flow control,seg_83,"incidentally, the loop could allow for y being a vector simply by changing the termination condition to"
263,0,[], Flow control,seg_83,"this would iterate excessively for some elements, but the vectorized arithmetic would likely more than make up for that."
264,1,"['interval', 'plots', 'set', 'power curves']", Flow control,seg_83,"however, the most frequently used looping construct is for, which loops over a fixed set of values as in the following example, which plots a set of power curves on the unit interval."
265,1,['variable'], Flow control,seg_83,"notice the loop variable j, which in turn takes the values of the given sequence when used in the lines call."
266,1,"['data', 'method']", Classes and generic functions,seg_85,"object-oriented programming is about creating coherent systems of data and methods that work upon them. one purpose is to simplify programs by accommodating the fact that you will have conceptually similar methods for different types of data, even though the implementations will have to be different. a prototype example is the print method: it makes sense to print many kinds of data objects, but the print layout will depend on what the data object is. you will generally have a class of data objects and a print method for that class. there are several object-oriented languages implementing these ideas in different ways."
267,1,"['hypothesis test', 'hypothesis', 'tests', 'test']", Classes and generic functions,seg_85,"most of the basic parts of r use the same object system as s version 3. an alternative object system similar to that of s version 4 has been developed in recent years. the new system has several advantages over the old one, but we shall restrict attention to the latter. the s3 object system is a simple system in which an object has a class attribute, which is simply a character vector. one example of this is that all the return values of the classical tests such as t.test have class ""htest"", indicating that they are the result of a hypothesis test. when these objects are printed, it is done by print.htest, which creates the nice layout (see chapter 5 for examples). however, from a programmatic viewpoint, these objects are just lists, and you can, for instance, extract the p-value by writing"
268,1,['function'], Classes and generic functions,seg_85,"the function print is a generic function, one that acts differently depending on its argument. these generally look like this:"
269,1,"['function', 'control']", Classes and generic functions,seg_85,"what usemethod(""print"") means is that r should pass control to a function named according to the object class (print.htest for objects of class ""htest"", etc.) or, if this is not found, to print.default. to see all the methods available for print, type methods(print) (there are 138 of them in r 2.6.2, so the output is not shown here)."
270,1,"['data', 'sets', 'data sets']", Data entry,seg_87,data sets do not have to be very large before it becomes impractical to type them in with c(...). most of the examples in this book use data sets in-
271,1,['data'], Data entry,seg_87,"cluded in the iswr package, made available to you by library(iswr). however, as soon as you wish to apply the methods to your own data, you will have to deal with data file formats and the specification thereof."
272,1,"['bias', 'data']", Data entry,seg_87,"in this section we discuss how to read data files and how to use the data editor module in r. the text has some bias toward windows systems, mainly because of some special issues that need to be mentioned for that platform."
273,1,"['variables', 'data', 'function']", Reading from a text file,seg_89,"the most convenient way of reading data into r is via the function called read.table. it requires that data be in “ascii format”; that is, a “flat file” as created with windows’ notepad or any plain-text editor. the result of read.table is a data frame, and it expects to find data in a corresponding layout where each line in the file contains all data from one subject (or rat or . . . ) in a specific order, separated by blanks or, optionally, some other separator. the first line of the file can contain a header giving the names of the variables, a practice that is highly recommended."
274,1,"['data', 'regression', 'correlation', 'sets', 'data sets']", Reading from a text file,seg_89,"table 11.6 in altman (1991) contains an example on ventricular circumferential shortening velocity versus fasting blood glucose by thuesen et al. we used those data to illustrate subsetting and use them again in the chapter on correlation and regression. they are among the built-in data sets in the iswr package and available as the data frame thuesen, but the point here is to show how to read them from a plain-text file."
275,1,['data'], Reading from a text file,seg_89,"assume that the data are contained in the file thuesen.txt, which looks as follows:"
276,1,"['data', 'standard']", Reading from a text file,seg_89,"to enter the data into the file, you could start up windows’ notepad or any other plain-text editor, such as those discussed in section 2.1.3. unix/linux users should just use a standard editor, such as emacs or vi. if you must, you can even use a word processing program with a little care."
277,1,"['missing value', 'data']", Reading from a text file,seg_89,you should simply type in the data as shown. notice that the columns are separated by an arbitrary number of blanks and that na represents a missing value.
278,1,"['normal', 'data']", Reading from a text file,seg_89,"at the end, you should save the data to a text file. notice that word processors require special actions in order to save as text. their normal save format is difficult to read from other programs."
279,1,['data'], Reading from a text file,seg_89,"assuming further that the file is in the iswr folder on the n: drive, the data can be read using"
280,1,['variables'], Reading from a text file,seg_89,"notice header=t specifying that the first line is a header containing the names of variables contained in the file. also note that you use forward slashes (/), not backslashes (\), in the filename, even on a windows system."
281,0,[], Reading from a text file,seg_89,the reason for avoiding backslashes in windows filenames is that the symbol is used as an escape character (see section 1.2.4) and therefore needs to be doubled. you could have used n:\\iswr\\thuesen.txt.
282,1,"['variable', 'data']", Reading from a text file,seg_89,"the result is a data frame, which is assigned to the variable thuesen2 and looks as follows:"
283,1,"['factors', 'variables', 'factor', 'case', 'data', 'set', 'data set', 'function']", Reading from a text file,seg_89,"to read in factor variables (see section 1.2.8), the easiest way may be to encode them using a textual representation. the read.table function autodetects whether a vector is text or numeric and converts it to a factor in the former case (but makes no attempt to recognize numerically coded factors). for instance, the secretin built-in data set is read from a file that begins like this:"
284,1,"['case', 'data']", Reading from a text file,seg_89,"this file can be read directly by read.table with no arguments other than the filename. it will recognize the case where the first line is one item shorter than the rest and will interpret that layout to imply that the first line contains a header and the first value on all subsequent lines is a row label — that is, exactly the layout generated when printing a data frame."
285,1,"['level', 'factors']", Reading from a text file,seg_89,"reading factors like this may be convenient, but there is a drawback: the level order is alphabetic, so for instance"
286,1,"['factor', 'levels']", Reading from a text file,seg_89,"if this is not what you want, then you may have to manipulate the factor levels; see section 10.1.2."
287,0,[], Reading from a text file,seg_89,a technical note: the files referenced above are contained in the iswr package in the subdirectory (folder) rawdata. exactly where the file is located on your system will depend on where the iswr package was installed. you can find this out as follows:
288,1,['function'], Further details on readtable,seg_91,the read.table function is a very flexible tool that is controlled by many options. we shall not attempt a full description here but just give some indication of what it can do.
289,1,['control'], Further details on readtable,seg_91,we have already seen the use of header=t. a couple of other options control the detailed format of the input file:
290,0,[], Further details on readtable,seg_91,field separator. this can be specified using sep. notice that when this is
291,1,"['missing values', 'missing value', 'data']", Further details on readtable,seg_91,"used, as opposed to the default use of whitespace, there must be exactly one separator between data fields. two consecutive separators will imply that there is a missing value in between. conversely, it is necessary to use specific codes to represent missing values in the default format and also to use some form of quoting for strings that contain embedded spaces."
292,1,['missing values'], Further details on readtable,seg_91,na strings. you can specify which strings represent missing values via
293,0,[], Further details on readtable,seg_91,"na.strings. there can be several different strings, although not different strings for different columns. for print files from the sas program, you would use na.strings="".""."
294,0,[], Further details on readtable,seg_91,"quotes and comments. by default, r-style quotes can be used to delimit"
295,0,[], Further details on readtable,seg_91,"character strings, and parts of files following the comment character # are ignored. these features can be modified or removed via the quote and comment.char arguments."
296,1,['error'], Further details on readtable,seg_91,unequal field count. it is normally considered an error if not all lines con-
297,1,"['case', 'data', 'vary']", Further details on readtable,seg_91,"tain the same number of values (the first line can be one item short, as described above for the secretin data). the fill and flush arguments can be used in case lines vary in length."
298,1,['adjusted'], Further details on readtable,seg_91,"applications such as spreadsheets and databases produce text files in formats that require multiple options to be adjusted. for such purposes, there exist “precooked” variants of read.table. two of these are intended to handle csv files and are called read.csv and read.csv2. the former assumes that fields are separated by a comma, and the latter assumes that they are separated by semicolons but use a comma as the decimal point (this format is often generated in european locales). both formats have header=t as the default. further variants are read.delim and read.delim2 for reading delimited files (by default, tab-delimited files)."
299,1,['factors'], Further details on readtable,seg_91,"it can be desirable to override the default conversion mechanisms in read.table. by default, nonnumeric input is converted to factors, but it does not always make sense. for instance, names and addresses typically should not be converted. this can be modified either for all columns using stringsasfactors or on a per-item basis using as.is."
300,1,"['treatment', 'data', 'standard']", Further details on readtable,seg_91,"automatic conversion is often convenient, but it is inefficient in terms of computer time and storage; in order to read a numeric column, read.table first reads it as character data, checks whether all elements can be converted to numeric, and only then performs the conversion. the colclasses argument allows you to bypass the mechanism by explicitly specifying which columns are of which class (the standard classes ""character"", ""numeric"", etc., get special treatment). you can also skip unwanted columns by specifying ""null"" as the class."
301,1,"['data', 'data frames', 'sets', 'data sets']", The data editor,seg_93,r lets you edit data frames using a spreadsheet-like interface. the interface is a bit rough but quite useful for small data sets.
302,1,"['function', 'data']", The data editor,seg_93,"to edit a data frame, you can use the edit function:"
303,1,"['factor', 'data', 'variable', 'set', 'data set']", The data editor,seg_93,"this brings up a spreadsheet-like editor with a column for each variable in the data frame. the airquality data set is built into r; see help(airquality) for its contents. inside the editor, you can move around with the mouse or the cursor keys and edit the current cell by typing in data. the type of variable can be switched between real (numeric) and character (factor) by clicking on the column header, and the name of"
304,1,"['variable', 'data']", The data editor,seg_93,the variable can be changed similarly. note that there is (as of r 2.6.2) no way to delete rows and columns and that new data can be entered only at the end.
305,1,['data'], The data editor,seg_93,"when you close the data editor, the edited data frame is assigned to aq. the original airquality is left intact. alternatively, if you do not mind overwriting the original data frame, you can use"
306,0,[], The data editor,seg_93,this is equivalent to aq <- edit(aq).
307,1,['data'], The data editor,seg_93,"to enter data into a blank data frame, use"
308,1,"['function', 'data', 'case']", The data editor,seg_93,"an alternative would be dd <- edit(data.frame()), which works fine except that beginners tend to reexecute the command when they need to edit dd, which of course destroys all data. it is necessary in either case to start with an empty data frame since by default edit expects you to want to edit a user-defined function and would bring up a text editor if you started it as edit()."
309,1,"['statistical', 'data']", Interfacing to other programs,seg_95,"sometimes you will want to move data between r and other statistical packages or spreadsheets. a simple fallback approach is to request that the package in question export data as a text file of some sort and use read.table, read.csv, read.csv2, read.delim, or read.delim2, as previously described."
310,1,['distributions'], Interfacing to other programs,seg_95,"the foreign package is one of the packages labelled “recommended” and should therefore be available with binary distributions of r. it contains routines to read files in several formats, including those from spss (.sav format), sas (export libraries), epi-info (.rec), stata, systat, minitab, and some s-plus version 3 dump files."
311,1,"['data', 'sets', 'data sets']", Interfacing to other programs,seg_95,unix/linux users sometimes find themselves with data sets written on windows machines. the foreign package will work there as well for those formats that it supports. notice that ordinary sas data sets are not among the supported formats. these have to be converted to export libraries on the originating system. data that have been entered into microsoft excel spreadsheets are most conveniently extracted using a compatible application such as ooo (openoffice.org).
312,0,[], Interfacing to other programs,seg_95,"an expedient technique is to read from the system clipboard. say, highlight a rectangular region in a spreadsheet, press ctrl-c (if on windows), and inside r use"
313,1,"['loss', 'data']", Interfacing to other programs,seg_95,"this does require a little caution, though. it may result in loss of accuracy since you only transfer the data as they appear on the screen. this is mostly a concern if you have data to many significant digits."
314,1,"['set', 'data']", Interfacing to other programs,seg_95,"for data stored in databases, there exist a number of interface packages on cran. of particular interest on windows and with some unix databases is the rodbc package because you can set up odbc (“open database connectivity”) connections to data stored by common applications, including excel and access. some unix databases (e.g., postgresql) also allow odbc connections."
315,1,"['data', 'information']", Interfacing to other programs,seg_95,"for up-to-date information on these matters, consult the “r data import/export” manual that comes with the system."
316,1,['function'], Exercises,seg_97,"2.1 describe how to insert a value between two elements of a vector at a given position by using the append function (use the help system to find out). without append, how would you do it?"
317,1,"['data', 'set', 'data set']", Exercises,seg_97,"2.2 write the built-in data set thuesen to a tab-separated text file with write.table. view it with a text editor (depending on your system). change the na value to . (period), and read the changed file back into r with a suitable command. also try importing the data into other applications of your choice and exporting them to a new file after editing. you may have to remove row names to make this work."
318,1,"['functions', 'statistics', 'data', 'sampling', 'distribution', 'probability', 'random', 'experiments', 'statistical', 'vary', 'random sampling', 'distributions']", Probability and distributions,seg_99,"the concepts of randomness and probability are central to statistics. it is an empirical fact that most experiments and investigations are not perfectly reproducible. the degree of irreproducibility may vary: some experiments in physics may yield data that are accurate to many decimal places, whereas data on biological systems are typically much less reliable. however, the view of data as something coming from a statistical distribution is vital to understanding statistical methods. in this section, we outline the basic ideas of probability and the functions that r has for random sampling and handling of theoretical distributions."
319,1,"['sample', 'random', 'probability theory', 'symmetry', 'probability', 'random sample']", Random sampling,seg_101,"much of the earliest work in probability theory was about games and gambling issues, based on symmetry considerations. the basic notion then is that of a random sample: dealing from a well-shuffled pack of cards or picking numbered balls from a well-stirred urn."
320,1,"['sample', 'set', 'random', 'function']", Random sampling,seg_101,"in r, you can simulate these situations with the sample function. if you want to pick five numbers at random from the set 1:40, then you can write"
321,1,"['sample size', 'sample']", Random sampling,seg_101,"the first argument (x) is a vector of values to be sampled and the second (size) is the sample size. actually, sample(40,5) would suffice since a single number is interpreted to represent the length of a sequence of integers."
322,1,"['sample', 'with replacement', 'replacement', 'sampling', 'samples']", Random sampling,seg_101,"notice that the default behaviour of sample is sampling without replacement. that is, the samples will not contain the same number twice, and size obviously cannot be bigger than the length of the vector to be sampled. if you want sampling with replacement, then you need to add the argument replace=true."
323,1,"['with replacement', 'replacement']", Random sampling,seg_101,"sampling with replacement is suitable for modelling coin tosses or throws of a die. so, for instance, to simulate 10 coin tosses we could write"
324,1,"['sample', 'probabilities', 'symmetric', 'successful', 'data', 'cases', 'success', 'probability', 'random', 'outcome', 'tails', 'outcomes', 'event']", Random sampling,seg_101,"in fair coin-tossing, the probability of heads should equal the probability of tails, but the idea of a random event is not restricted to symmetric cases. it could be equally well applied to other cases, such as the successful outcome of a surgical procedure. hopefully, there would be a better than 50% chance of this. you can simulate data with nonequal probabilities for the outcomes (say, a 90% chance of success) by using the prob argument to sample, as in"
325,1,"['sample', 'distribution', 'binomial', 'binomial distribution']", Random sampling,seg_101,"this may not be the best way to generate such a sample, though. see the later discussion of the binomial distribution."
326,1,"['sample', 'without replacement', 'case', 'replacement', 'sampling', 'probability', 'function']", Probability calculations and combinatorics,seg_103,"let us return to the case of sampling without replacement, specifically sample(1:40, 5). the probability of obtaining a given number as the first one of the sample should be 1/40, the next one 1/39, and so forth. the probability of a given sample should then be 1/(40× 39× 38× 37× 36). in r, use the prod function, which calculates the product of a vector of numbers"
327,1,"['case', 'factorial', 'cases', 'set', 'probability']", Probability calculations and combinatorics,seg_103,"however, notice that this is the probability of getting given numbers in a given order. if this were a lotto-like game, then you would rather be interested in the probability of guessing a given set of five numbers correctly. thus you need also to include the cases that give the same numbers in a different order. since obviously the probability of each such case is going to be the same, all we need to do is to figure out how many such cases there are and multiply by that. there are five possibilities for the first number, and for each of these there are four possibilities for the second, and so forth; that is, the number is 5× 4× 3× 2× 1. this number is also written as 5! (5 factorial). so the probability of a “winning lotto coupon” would be"
328,1,"['sets', 'set', 'probability']", Probability calculations and combinatorics,seg_103,"there is another way of arriving at the same result. notice that since the actual set of numbers is immaterial, all sets of five numbers must have the same probability. so all we need to do is to calculate the number of ways to choose 5 numbers out of 40. this is denoted"
329,1,"['function', 'probability']", Probability calculations and combinatorics,seg_103,"in r, the choose function can be used to calculate this number, and the probability is thus"
330,1,"['case', 'random', 'experiment', 'successes', 'random variable', 'success', 'continuous random variables', 'failure', 'continuous', 'replications', 'random variables', 'independent', 'failures', 'variables', 'variable']", Discrete distributions,seg_105,"when looking at independent replications of a binary experiment, you would not usually be interested in whether each case is a success or a failure but rather in the total number of successes (or failures). obviously, this number is random since it depends on the individual random outcomes, and it is consequently called a random variable. in this case it is a discrete-valued random variable that can take values 0, 1, . . . , n, where n is the number of replications. continuous random variables are encountered later."
331,1,"['cumulative distribution function', 'probabilities', 'distribution function', 'case', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'function']", Discrete distributions,seg_105,"a random variable x has a probability distribution that can be described using point probabilities f (x) = p(x = x) or the cumulative distribution function f(x) = p(x ≤ x). in the case at hand, the distribution can be worked out as having the point probabilities"
332,1,"['distribution', 'binomial distribution', 'binomial']", Discrete distributions,seg_105,"this is known as the binomial distribution, and the (n"
333,1,"['probabilities', 'outcome', 'successful', 'trial', 'distribution', 'binomial', 'probability', 'parameter', 'binomial distribution', 'coefficients']", Discrete distributions,seg_105,x) are known as binomial coefficients. the parameter p is the probability of a successful outcome in an individual trial. a graph of the point probabilities of the binomial distribution appears in figure 3.2 ahead.
334,1,"['functions', 'continuous', 'continuous distributions', 'distribution', 'binomial', 'binomial distribution', 'distributions']", Discrete distributions,seg_105,we delay describing the r functions related to the binomial distribution until we have discussed continuous distributions so that we can present the conventions in a unified manner.
335,1,"['failures', 'geometric distribution', 'distribution', 'success', 'binomial', 'probability', 'geometric', 'distributions']", Discrete distributions,seg_105,"many other distributions can be derived from simple probability models. for instance, the geometric distribution is similar to the binomial distribution but records the number of failures that occur before the first success."
336,1,"['precision', 'random variation', 'data', 'variation', 'deviations', 'measurements', 'random', 'continuous']", Continuous distributions,seg_107,"some data arise from measurements on an essentially continuous scale, for instance temperature, concentrations, etc. in practice, they will be recorded to a finite precision, but it is useful to disregard this in the modelling. such measurements will usually have a component of random variation, which makes them less than perfectly reproducible. however, these random fluctuations will tend to follow patterns; typically they will cluster around a central value, with large deviations being more rare than smaller ones."
337,1,"['model', 'cumulative distribution function', 'random variables', 'continuous', 'variables', 'distribution function', 'data', 'distribution', 'probability', 'random', 'function']", Continuous distributions,seg_107,"in order to model continuous data, we need to define random variables that can obtain the value of any real number. because there are infinitely many numbers infinitely close, the probability of any particular value will be zero, so there is no such thing as a point probability as for discretevalued random variables. instead we have the concept of a density. this is the infinitesimal probability of hitting a small region around x divided by the size of the region. the cumulative distribution function can be defined as before, and we have the relation"
338,1,"['standard', 'statistical', 'distributions']", Continuous distributions,seg_107,there are a number of standard distributions that come up in statistical theory and are available in r. it makes little sense to describe them in detail here except for a couple of examples.
339,1,"['distribution', 'uniform distribution', 'interval']", Continuous distributions,seg_107,"the uniform distribution has a constant density over a specified interval (by default [0, 1])."
340,1,"['distribution', 'normal', 'gaussian distribution', 'normal distribution']", Continuous distributions,seg_107,the normal distribution (also known as the gaussian distribution) has density
341,1,"['binomial distribution', 'sample', 'mean', 'standard', 'standard deviation', 'statistical', 'error', 'distribution', 'binomial', 'deviation', 'normal', 'variation', 'normal distribution']", Continuous distributions,seg_107,"depending on its mean µ and standard deviation σ. the normal distribution has a characteristic bell shape (figure 3.1), and modifying µ and σ simply translates and widens the distribution. it is a standard building block in statistical models, where it is commonly used to describe error variation. it also comes up as an approximating distribution in several contexts; for instance, the binomial distribution for large sample sizes can be well approximated by a suitably scaled normal distribution."
342,1,"['model', 'model building', 'statistical tests', 'distribution', 'statistical', 'normal', 'binomial', 'tables', 'standard', 'binomial distribution', 'tests', 'distributions']", The builtin distributions in R,seg_109,"the standard distributions that turn up in connection with model building and statistical tests have been built into r, and it can therefore completely replace traditional statistical tables. here we look only at the normal distribution and the binomial distribution, but other distributions follow exactly the same pattern."
343,1,"['distribution', 'statistical']", The builtin distributions in R,seg_109,four fundamental items can be calculated for a statistical distribution:
344,1,['probability'], The builtin distributions in R,seg_109,• density or point probability
345,1,"['distribution', 'function', 'distribution function', 'probability']", The builtin distributions in R,seg_109,"• cumulated probability, distribution function"
346,1,['quantiles'], The builtin distributions in R,seg_109,• quantiles
347,0,[], The builtin distributions in R,seg_109,• pseudo-random numbers
348,1,"['quantile', 'distribution', 'normal', 'probability', 'random', 'function', 'normal distribution', 'distributions']", The builtin distributions in R,seg_109,"for all distributions implemented in r, there is a function for each of the four items listed above. for example, for the normal distribution, these are named dnorm, pnorm, qnorm, and rnorm (density, probability, quantile, and random, respectively)."
349,1,"['curve', 'continuous distribution', 'interval', 'distribution', 'probability', 'continuous']", Densities,seg_111,the density for a continuous distribution is a measure of the relative probability of “getting a value close to x”. the probability of getting a value in a particular interval is the area under the corresponding part of the curve.
350,1,"['discrete', 'discrete distributions', 'probability', 'distributions']", Densities,seg_111,"for discrete distributions, the term “density” is used for the point probability — the probability of getting exactly the value x. technically, this is correct: it is a density with respect to counting measure."
351,1,"['density function', 'curve', 'distribution', 'normal', 'function', 'normal distribution']", Densities,seg_111,"the density function is likely the one of the four function types that is least used in practice, but if for instance it is desired to draw the well-known bell curve of the normal distribution, then it can be done like this:"
352,1,"['plot', 'function', 'plotting']", Densities,seg_111,"the function seq (see p. 15) is used to generate equidistant values, here from −4 to 4 in steps of 0.1; that is, (−4.0,−3.9,−3.8, . . . , 3.9, 4.0). the use of type=""l"" as an argument to plot causes the function to draw lines between the points rather than plotting the points themselves."
353,1,"['plot', 'curve']", Densities,seg_111,an alternative way of creating the plot is to use curve as follows:
354,0,[], Densities,seg_111,"this is often a more convenient way of making graphs, but it does require that the y-values can be expressed as a simple functional expression in x."
355,1,"['variables', 'discrete', 'discrete distributions', 'distribution', 'binomial', 'binomial distribution', 'pin diagram', 'distributions']", Densities,seg_111,"for discrete distributions, where variables can take on only distinct values, it is preferable to draw a pin diagram, here for the binomial distribution with n = 50 and p = 0.33 (figure 3.2):"
356,1,"['standard normal', 'distribution', 'trials', 'normal', 'mean', 'parameter', 'standard', 'standard normal distribution', 'normal distribution']", Densities,seg_111,"notice that there are three arguments to the “d-function” this time. in addition to x, you have to specify the number of trials n and the probability parameter p. the distribution drawn corresponds to, for example, the number of 5s or 6s in 50 throws of a symmetrical die. actually, dnorm also takes more than one argument, namely the mean and standard deviation, but they have default values of 0 and 1, respectively, since most often it is the standard normal distribution that is requested."
357,0,[], Densities,seg_111,"the form 0:50 is a short version of seq(0,50,1): the whole numbers from 0 to 50 (see p. 15). it is type=""h"" (as in histogram-like) that causes the pins to be drawn."
358,1,"['functions', 'cumulative distribution function', 'distribution function', 'distribution', 'probability', 'function']", Cumulative distribution functions,seg_113,the cumulative distribution function describes the probability of “hitting” x or less in a given distribution. the corresponding r functions begin with a ‘p’ (for probability) by convention.
359,1,"['plot', 'functions', 'densities', 'deviation', 'distribution', 'normal', 'mean', 'standard', 'standard deviation', 'normal distribution']", Cumulative distribution functions,seg_113,"just as you can plot densities, you can of course also plot cumulative distribution functions, but that is usually not very informative. more often, actual numbers are desired. say that it is known that some biochemical measure in healthy individuals is well described by a normal distribution with a mean of 132 and a standard deviation of 13. then, if a patient has a value of 160, there is"
360,1,"['deviation', 'distribution', 'normal', 'mean', 'probability', 'standard', 'function', 'standard deviation', 'population', 'normal distribution']", Cumulative distribution functions,seg_113,"or only about 1.5% of the general population, that has that value or higher. the function pnorm returns the probability of getting a value smaller than its first argument in a normal distribution with the given mean and standard deviation."
361,1,"['probability', 'statistical tests', 'tests', 'statistical', 'distribution', 'outcome', 'test', 'tail probability', 'treatment', 'sign test', 'normal', 'tail', 'normal distribution']", Cumulative distribution functions,seg_113,"another typical application occurs in connection with statistical tests. consider a simple sign test: twenty patients are given two treatments each (blindly and in randomized order) and then asked whether treatment a or b worked better. it turned out that 16 patients liked a better. the question is then whether this can be taken as sufficient evidence that a actually is the better treatment or whether the outcome might as well have happened by chance even if the treatments were equally good. if there was no difference between the two treatments, then we would expect the number of people favouring treatment a to be binomially distributed with p = 0.5 and n = 20. how (im)probable would it then be to obtain what we have observed? as in the normal distribution, we need a tail probability, and the immediate guess might be to look at"
362,1,"['tail', 'error', 'probability']", Cumulative distribution functions,seg_113,"and subtract it from 1 to get the upper tail — but this would be an error! what we need is the probability of the observed or more extreme, and pbinom is giving the probability of 16 or less. we need to use “15 or less” instead."
363,1,"['treatment', 'results', 'case', 'probability', 'test']", Cumulative distribution functions,seg_113,"if you want a two-tailed test because you have no prior idea about which treatment is better, then you will have to add the probability of obtaining equally extreme results in the opposite direction. in the present case, that"
364,1,"['total probability', 'probability']", Cumulative distribution functions,seg_113,"means the probability that four or fewer people prefer a, giving a total probability of"
365,1,['probability'], Cumulative distribution functions,seg_113,(which is obviously exactly twice the one-tailed probability).
366,0,[], Cumulative distribution functions,seg_113,"as can be seen from the last command, it is not strictly necessary to use the size and prob keywords as long as the arguments are given in the right order (positional matching; see section 1.2.2)."
367,1,"['observation', 'binomial', 'function', 'test']", Cumulative distribution functions,seg_113,"it is quite confusing to keep track of whether or not the observation itself needs to be counted. fortunately, the function binom.test keeps track of such formalities and performs the correct binomial test. this is further discussed in chapter 8."
368,1,"['quantile', 'median', 'distribution', 'probability', 'function']", Quantiles,seg_115,the quantile function is the inverse of the cumulative distribution function. the p-quantile is the value with the property that there is probability p of getting a value less than or equal to it. the median is by definition the 50% quantile.
369,1,"['functions', 'case']", Quantiles,seg_115,some details concerning the definition in the case of discontinuous distributions are glossed over here. you can fairly easily deduce the behaviour by experimenting with the r functions.
370,1,"['probabilities', 'table', 'test statistic', 'statistical', 'set', 'statistic', 'level', 'test', 'distributions']", Quantiles,seg_115,"tables of statistical distributions are almost always given in terms of quantiles. for a fixed set of probabilities, the table shows the boundary that a test statistic must cross in order to be considered significant at that level. this is purely for operational reasons; it is almost superfluous when you have the option of computing p exactly."
371,1,"['interval', 'quantiles', 'intervals', 'confidence', 'experiments', 'confidence interval']", Quantiles,seg_115,theoretical quantiles are commonly used for the calculation of confidence intervals and for power calculations in connection with designing and dimensioning experiments (see chapter 9). a simple example of a confidence interval can be given here (see also chapter 5).
372,1,"['deviation', 'interval', 'observations', 'normally distributed', 'mean', 'standard', 'standard deviation', 'confidence', 'average', 'confidence interval']", Quantiles,seg_115,"if we have n normally distributed observations with the same mean µ and standard deviation σ, then it is known that the average x̄ is normally distributed around µ with standard deviation σ/√n. a 95% confidence interval for µ can be obtained as"
373,1,"['quantile', 'distribution', 'normal', 'average', 'normal distribution']", Quantiles,seg_115,"where n0.025 is the 2.5% quantile in the normal distribution. if σ = 12 and we have measured n = 5 persons and found an average of x̄ = 83, then"
374,1,"['standard error', 'error', 'mean', 'standard']", Quantiles,seg_115,we can compute the relevant quantities as (“sem” means standard error of the mean)
375,1,"['confidence intervals', 'interval', 'case', 't distribution', 'data', 'distribution', 'intervals', 'control', 'confidence', 'process', 'confidence interval']", Quantiles,seg_115,and thus find a 95% confidence interval for µ going from 72.48 to 93.52. (notice that this is based on the assumption that σ is known. this is sometimes reasonable in process control applications. the more common case of estimating σ from the data leads to confidence intervals based on the t distribution and is discussed in chapter 5.)
376,1,"['quantile', 'cumulative distribution function', 'symmetric', 'distribution function', 'distribution', 'normal', 'standard', 'function', 'confidence', 'normal distribution']", Quantiles,seg_115,"since it is known that the normal distribution is symmetric, so that n0.025 = −n0.975, it is common to write the formula for the confidence interval as x̄±σ/√n×n0.975. the quantile itself is often written φ−1(0.975), where φ is standard notation for the cumulative distribution function of the normal distribution (pnorm)."
377,1,"['plots', 'quantiles', 'data', 'distribution', 'set']", Quantiles,seg_115,"another application of quantiles is in connection with q–q plots (see section 4.2.3), which can be used to assess whether a set of data can reasonably be assumed to come from a given distribution."
378,1,"['results', 'random numbers', 'random']", Random numbers,seg_117,"to many people, it sounds like a contradiction in terms to generate random numbers on a computer since its results are supposed to be predictable and reproducible. what is in fact possible is to generate sequences of “pseudo-random” numbers, which for practical purposes behave as if they were drawn randomly."
379,1,"['statistics', 'data', 'sets', 'set', 'data sets', 'simulated', 'random', 'random numbers']", Random numbers,seg_117,"here random numbers are used to give the reader a feeling for the way in which randomness affects the quantities that can be calculated from a set of data. in professional statistics, they are used to create simulated data sets in order to study the accuracy of mathematical approximations and the effect of assumptions being violated."
380,1,"['functions', 'random', 'random numbers', 'distributions']", Random numbers,seg_117,"the use of the functions that generate random numbers is straightforward. the first argument specifies the number of random numbers to compute, and the subsequent arguments are similar to those for other functions related to the same distributions. for instance,"
381,1,"['degrees of freedom', 'normally distributed', 'probability', 'binomial distribution', 'uniform distribution', 'successes', 'events', 'mean', 'standard', 'standard deviation', 'distribution', 'binomial', 'deviation', 'variable']", Exercises,seg_119,3.1 calculate the probability for each of the following events: (a) a standard normally distributed variable is larger than 3. (b) a normally distributed variable with mean 35 and standard deviation 6 is larger than 42. (c) getting 10 out of 10 successes in a binomial distribution with probability 0.8. (d) x < 0.9 when x has the standard uniform distribution. (e) x > 6.5 in a χ2 distribution with 2 degrees of freedom.
382,1,"['deviation', 'quartiles', 'interval', 'distribution', 'normal', 'mean', 'standard', 'standard deviation', 'normal distribution']", Exercises,seg_119,"3.2 a rule of thumb is that 5% of the normal distribution lies outside an interval approximately ±2s about the mean. to what extent is this true? where are the limits corresponding to 1%, 0.5%, and 0.1%? what is the position of the quartiles measured in standard deviation units?"
383,1,"['method', 'frequency', 'probability', 'tests']", Exercises,seg_119,"3.3 for a disease known to have a postoperative complication frequency of 20%, a surgeon suggests a new procedure. he tests it on 10 patients and there are no complications. what is the probability of operating on 10 patients successfully with the traditional method?"
384,1,"['sample', 'simulated']", Exercises,seg_119,3.4 simulated coin-tossing can be done using rbinom instead of sample. how exactly would you do that?
385,1,"['graphics', 'statistics', 'data', 'set', 'data set', 'statistical']", Descriptive statistics and graphics,seg_121,"before going into the actual statistical modelling and analysis of a data set, it is often useful to make some simple characterizations of the data in terms of summary statistics and graphics."
386,1,"['deviation', 'median', 'statistics', 'mean', 'standard', 'standard deviation', 'variance']", Summary statistics for a single group,seg_123,"it is easy to calculate simple summary statistics with r. here is how to calculate the mean, standard deviation, variance, and median."
387,1,"['observations', 'results', 'data', 'normally distributed', 'random', 'random numbers']", Summary statistics for a single group,seg_123,"notice that the example starts with the generation of an artificial data vector x of 50 normally distributed observations. it is used in examples throughout this section. when reproducing the examples, you will not get exactly the same results since your random numbers will differ."
388,1,"['function', 'quantile', 'quantiles']", Summary statistics for a single group,seg_123,empirical quantiles may be obtained with the function quantile like this:
389,1,"['deviation', 'quartiles', 'range', 'interquartile range', 'quantiles', 'percentiles', 'standard', 'standard deviation']", Summary statistics for a single group,seg_123,"as you see, by default you get the minimum, the maximum, and the three quartiles — the 0.25, 0.50, and 0.75 quantiles — so named because they correspond to a division into four parts. similarly, we have deciles for 0.1, 0.2, . . . , 0.9, and centiles or percentiles. the difference between the first and third quartiles is called the interquartile range (iqr) and is sometimes used as a robust alternative to the standard deviation."
390,1,"['percentage', 'quantiles']", Summary statistics for a single group,seg_123,"it is also possible to obtain other quantiles; this is done by adding an argument containing the desired percentage points. this, for example, is how to get the deciles:"
391,1,"['sample', 'quantile', 'linear', 'empirical quantiles', 'observations', 'quantiles', 'observation', 'quartile', 'first quartile']", Summary statistics for a single group,seg_123,be aware that there are several possible definitions of empirical quantiles. the one r uses by default is based on a sum polygon where the ith ranking observation is the (i− 1)/(n− 1) quantile and intermediate quantiles are obtained by linear interpolation. it sometimes confuses students that in a sample of 10 there will be 3 observations below the first quartile with this definition. other definitions are available via the type argument to quantile.
392,1,"['missing values', 'data']", Summary statistics for a single group,seg_123,"if there are missing values in data, things become a bit more complicated. for illustration, we use the following example."
393,1,"['variables', 'factor', 'data', 'set', 'data set']", Summary statistics for a single group,seg_123,"the data set juul contains variables from an investigation performed by anders juul (rigshospitalet, department for growth and reproduction) concerning serum igf-i (insulin-like growth factor) in a group of healthy humans, primarily schoolchildren. the data set is contained in the iswr package and contains a number of variables, of which we only use igf1 (serum igf-i) for now, but later in the chapter we also use tanner (tanner stage of puberty, a classification into five groups based on appearance"
394,0,[], Summary statistics for a single group,seg_123,"of primary and secondary sexual characteristics), sex, and menarche (indicating whether or not a girl has had her first period)."
395,1,['mean'], Summary statistics for a single group,seg_123,attempting to calculate the mean of igf1 reveals a problem.
396,1,"['missing values', 'mean']", Summary statistics for a single group,seg_123,"r will not skip missing values unless explicitly requested to do so. the mean of a vector with an unknown value is unknown. however, you can give the na.rm argument (not available, remove) to request that missing values be removed:"
397,1,"['function', 'measurements']", Summary statistics for a single group,seg_123,"there is one slightly annoying exception: the length function will not understand na.rm, so we cannot use it to count the number of nonmissing measurements of igf1. however, you can use"
398,0,[], Summary statistics for a single group,seg_123,"the construction above uses the fact that if logical values are used in arithmetic, then true is converted to 1 and false to 0."
399,1,"['variable', 'function', 'numeric variable']", Summary statistics for a single group,seg_123,a nice summary display of a numeric variable is obtained from the summary function:
400,1,"['quartiles', 'quantiles']", Summary statistics for a single group,seg_123,the 1st qu. and 3rd qu. refer to the empirical quartiles (0.25 and 0.75 quantiles).
401,1,['data'], Summary statistics for a single group,seg_123,"in fact, it is possible to summarize an entire data frame with"
402,1,"['categorical', 'variables', 'data', 'set', 'data set']", Summary statistics for a single group,seg_123,"the data set has menarche, sex, and tanner coded as numeric variables even though they are clearly categorical. this can be mended as follows:"
403,1,"['variables', 'factor', 'data']", Summary statistics for a single group,seg_123,notice how the display changes for the factor variables. note also that juul was detached and reattached after the modification. this is because modifying a data frame does not affect any attached version. it was not strictly necessary to do it here because summary works directly on the data frame whether attached or not.
404,1,"['level', 'factors', 'variables', 'data', 'function', 'transform']", Summary statistics for a single group,seg_123,"in the above, the variables sex, menarche, and tanner were converted to factors with suitable level names (in the raw data these are represented using numeric codes). the converted variables were put back into the data frame juul, replacing the original sex, tanner, and menarche variables. we might also have used the transform function (or within):"
405,1,"['distribution', 'histogram', 'observations']", Histograms,seg_127,"you can get a reasonable impression of the shape of a distribution by drawing a histogram; that is, a count of how many observations fall within specified divisions (“bins”) of the x-axis (figure 4.1)."
406,1,"['histogram', 'interval', 'data', 'algorithm', 'control', 'rates']", Histograms,seg_127,"by specifying breaks=n in the hist call, you get approximately n bars in the histogram since the algorithm tries to create “pretty” cutpoints. you can have full control over the interval divisions by specifying breaks as a vector rather than as a number. altman (1991, pp. 25–26) contains an example of accident rates by age group. these are given as a count in age groups 0–4, 5–9, 10–15, 16, 17, 18–19, 20–24, 25–59, and 60–79 years of age. the data can be entered as follows:"
407,0,[], Histograms,seg_127,histogram of age.acc
408,1,"['set', 'interval', 'table']", Histograms,seg_127,"here the first three lines generate pseudo-data from the table in the book. for each interval, the relevant number of “observations” is generated with an age set to the midpoint of the interval; that is, 28 2.5-year-olds, 46 7.5- year-olds, etc. then a vector brk of cutpoints is defined (note that the extremes need to be included) and used as the breaks argument to hist, yielding figure 4.2."
409,1,"['density function', 'densities', 'histogram', 'interval', 'observations', 'change of scale', 'data', 'set', 'function']", Histograms,seg_127,"notice that you automatically got the “correct” histogram where the area of a column is proportional to the number. the y-axis is in density units (that is, proportion of data per x unit), so that the total area of the histogram will be 1. if, for some reason, you want the (misleading) histogram where the column height is the raw number in each interval, then it can be specified using freq=t. for equidistant breakpoints, that is the default (because then you can see how many observations have gone into each column), but you can set freq=f to get densities displayed. this is really just a change of scale on the y-axis, but it has the advantage that it becomes possible to overlay the histogram with a corresponding theoretical density function."
410,1,"['cumulative distribution function', 'distribution function', 'empirical cumulative distribution function', 'data', 'observation', 'distribution', 'simulated', 'function']", Empirical cumulative distribution,seg_129,"the empirical cumulative distribution function is defined as the fraction of data smaller than or equal to x. that is, if x is the kth smallest observation, then the proportion k/n of the data is smaller than or equal to x (7/10 if x is no. 7 of 10). the empirical cumulative distribution function can be plotted as follows (see figure 4.3) where x is the simulated data vector from section 4.1:"
411,1,"['plot', 'step function', 'parameter', 'function', 'plotting']", Empirical cumulative distribution,seg_129,"the plotting parameter type=""s"" gives a step function where (x, y) is the left end of the steps and ylim is a vector of two elements specifying the extremes of the y-coordinates on the plot. recall that c(...) is used to create vectors."
412,1,"['distribution', 'function', 'step function']", Empirical cumulative distribution,seg_129,some more elaborate displays of empirical cumulative distribution functions are available via the ecdf function. this is also more precise regarding the mathematical definition of the step function.
413,1,['plot'], Empirical cumulative distribution,seg_129,normal q−q plot
414,1,"['distribution function', 'empirical cumulative distribution function', 'observation', 'normally distributed', 'function', 'standard normal distribution', 'cumulative distribution function', 'data', 'mean', 'standard', 'standard deviation', 'standard normal', 'distribution', 'expected value', 'plot', 'deviation', 'normal', 'normal distribution']", QQ plots,seg_131,"one purpose of calculating the empirical cumulative distribution function (c.d.f.) is to see whether data can be assumed normally distributed. for a better assessment, you might plot the kth smallest observation against the expected value of the kth smallest observation out of n in a standard normal distribution. the point is that in this way you would expect to obtain a straight line if data come from a normal distribution with any mean and standard deviation."
415,1,"['plot', 'qqnorm', 'function']", QQ plots,seg_131,"creating such a plot is slightly complicated. fortunately, there is a builtin function for doing it, qqnorm. the result of using it can be seen in figure 4.4. you only have to write"
416,1,"['plot', 'curve', 'quantile', 'plots', 'distribution', 'tails']", QQ plots,seg_131,"as the title of the plot indicates, plots of this kind are also called “q–q plots” (quantile versus quantile). notice that x and y are interchanged relative to the empirical c.d.f. — the observed values are now drawn along the y-axis. you should notice that with this convention the distribution has heavy tails if the outer parts of the curve are steeper than the middle part."
417,1,"['plot', 'empirical quantiles', 'quantiles', 'data', 'variable']", QQ plots,seg_131,"some readers will have been taught “probability plots”, which are similar but have the axes interchanged. it can be argued that the way r draws the plot is the better one since the theoretical quantiles are known in advance, while the empirical quantiles depend on data. you would normally choose to draw fixed values horizontally and variable values vertically."
418,1,"['distribution', 'boxplots']", Boxplots,seg_133,"a “boxplot”, or more descriptively a “box-and-whiskers plot”, is a graphical summary of a distribution. figure 4.5 shows boxplots for igm and its logarithm; see the example on page 23 in altman (1991)."
419,1,"['quartiles', 'median', 'observations', 'observation', 'boxplot']", Boxplots,seg_133,"here is how a boxplot is drawn in r. the box in the middle indicates “hinges” (nearly quartiles; see the help page for boxplot.stats) and median. the lines (“whiskers”) show the largest or smallest observation that falls within a distance of 1.5 times the box size from the nearest hinge. if any observations fall farther away, the additional points are considered “extreme” values and are shown separately."
420,0,[], Boxplots,seg_133,the practicalities are these:
421,1,"['plot', 'plots', 'parameter']", Boxplots,seg_133,"a layout with two plots side by side is specified using the mfrow graphical parameter. it should be read as “multif rame, rowwise, 1× 2 layout”. individual plots are organized in one row and two columns. as you might guess, there is also an mfcol parameter to plot columnwise. in a 2× 2 layout, the difference is whether plot no. 2 is drawn in the top right or bottom left corner."
422,1,"['plots', 'parameter']", Boxplots,seg_133,"notice that it is necessary to reset the layout parameter to c(1,1) at the end unless you also want two plots side by side subsequently."
423,1,"['standard deviations', 'table', 'statistics', 'grouped data', 'data', 'deviations', 'standard', 'concentration']", Summary statistics by groups,seg_135,"when dealing with grouped data, you will often want to have various summary statistics computed within groups; for example, a table of means and standard deviations. to this end, you can use tapply (see section 1.2.15). here is an example concerning the folate concentration in red blood cells according to three types of ventilation during anesthesia (alt-"
424,0,[], Summary statistics by groups,seg_135,"man, 1991, p. 208). we return to this example in section 7.1, which also contains the explanation of the category names."
425,1,"['standard deviations', 'observations', 'deviations', 'variable', 'mean', 'standard', 'number of observations']", Summary statistics by groups,seg_135,"the tapply call takes the folate variable, splits it according to ventilation, and computes the mean for each group. in the same way, standard deviations and the number of observations in the groups can be computed."
426,0,[], Summary statistics by groups,seg_135,try something like this for a nicer display:
427,1,"['missing values', 'mean', 'data']", Summary statistics by groups,seg_135,"for the juul data, we might want the mean igf1 by tanner group, but of course we run into the problem of missing values again:"
428,1,"['missing values', 'mean', 'parameter']", Summary statistics by groups,seg_135,we need to get tapply to pass na.rm=t as a parameter to mean to make it exclude the missing values. this is achieved simply by passing it as an additional argument to tapply.
429,1,"['functions', 'variables', 'results', 'data', 'variations']", Summary statistics by groups,seg_135,"the functions aggregate and by are variations on the same topic. the former is very much like tapply, except that it works on an entire data frame and presents its results as a data frame. this is useful for presenting many variables at once; e.g.,"
430,1,"['variables', 'case', 'data', 'function']", Summary statistics by groups,seg_135,"notice that the grouping argument in this case must be a list, even when it is one-dimensional, and that the names of the list elements get used as column names in the output. notice also that since the function is applied to all columns of the data frame, you may have to choose a subset of columns, in this case the numeric variables."
431,1,"['data frames', 'variable', 'data']", Summary statistics by groups,seg_135,"the indexing variable is not necessarily part of the data frame that is being aggregated, and there is no attempt at “smart evaluation” as there is in subset, so you have to spell out juul$sex. you can also use the fact that data frames are list-like and say"
432,1,['data'], Summary statistics by groups,seg_135,(the “trick” being that indexing a data frame with single brackets yields a data frame as the result).
433,1,"['function', 'data']", Summary statistics by groups,seg_135,"the by function is again similar, but different. the difference is that the function now takes an entire (sub-) data frame as its argument, so that you can for instance summarize the juul data by sex as follows:"
434,1,"['variable', 'method', 'standard']", Summary statistics by groups,seg_135,"the result of the call to by is actually a list of objects that has has been wrapped as an object of class ""by"" and printed using a print method for that class. you can assign the result to a variable and access the result for each subgroup using standard list indexing."
435,1,['statistical'], Summary statistics by groups,seg_135,the same technique can also be used to generate more elaborate statistical analyses for each group.
436,1,"['functions', 'graphical', 'plots', 'grouped data', 'data']", Graphics for grouped data,seg_137,"in dealing with grouped data, it is important to be able not only to create plots for each group but also to compare the plots between groups. in this section we review some general graphical techniques that allow us to display similar plots for several groups on the same page. some functions have specific features for displaying data from more than one group."
437,1,"['bin', 'histogram', 'data', 'variable']", Histograms,seg_139,"we have already seen in section 4.2.1 how to obtain a histogram simply by typing hist(x), where x is the variable containing the data. r will then choose a number of groups so that a reasonable number of data points fall in each bin while at the same time ensuring that the cutpoints are “pretty” numbers on the x-axis."
438,1,"['data', 'intervals', 'set']", Histograms,seg_139,"it is also mentioned there that an alternative number of intervals can be set via the argument breaks, although you do not always get exactly the number you asked for since r reserves the right to choose “pretty” column boundaries. for instance, multiples of 0.5 mj are chosen in the following example using the energy data introduced in section 1.2.14 on the 24-hour energy expenditure for two groups of women."
439,1,['data'], Histograms,seg_139,"in this example, some further techniques of general use are illustrated. the end result is seen in figure 4.6, but first we must fetch the data:"
440,1,"['factor', 'data']", Histograms,seg_139,notice how we separate the expend vector in the energy data frame into two vectors according to the value of the factor stature.
441,1,['plotting'], Histograms,seg_139,now we do the actual plotting:
442,1,"['plot', 'plots', 'set', 'histograms']", Histograms,seg_139,"we set par(mfrow=c(2,1)) to get the two histograms in the same plot. in the hist commands themselves, we used the breaks argument as already mentioned and col, whose effect should be rather obvious. we also used xlim and ylim to get the same x and y axes in the two plots. however, it is a coincidence that the columns have the same width."
443,0,[], Histograms,seg_139,histogram of expend.lean
444,0,[], Histograms,seg_139,histogram of expend.obese
445,1,"['functions', 'plots']", Histograms,seg_139,"as a practical remark, when working with plots like the above, where more than a single line of code is required, it gets cumbersome to use command recall in the r console window every time something needs to be changed. a better idea may be to start up a script window or a plain-text editor and cut and paste entire blocks of code from there (see section 2.1.3). you might also take it as an incentive to start writing simple functions."
446,1,"['factor', 'data', 'boxplot', 'boxplots', 'set']", Parallel boxplots,seg_141,"you might want a set of boxplots from several groups in the same frame. boxplot can handle this both when data are given in the form of separate vectors from each group and when data are in one long vector and a parallel vector or factor defines the grouping. to illustrate the latter, we use the energy data introduced in section 1.2.14."
447,0,[], Parallel boxplots,seg_141,figure 4.7 is created as follows:
448,1,"['plot', 'case']", Parallel boxplots,seg_141,"we could also have based the plot on the separate vectors expend.lean and expend.obese. in that case, a syntax is used that specifies the vectors as two separate arguments:"
449,1,"['plot', 'data']", Parallel boxplots,seg_141,"the plot is not shown here, but the only difference lies in the labelling of the x-axis. there is also a third form, where data are given as a single argument that is a list of vectors."
450,1,"['plot', 'variable']", Parallel boxplots,seg_141,the bottom plot has been made using the complete expend vector and the grouping variable fstature.
451,1,['model'], Parallel boxplots,seg_141,notation of the type y ~ x should be read “y described using x”. this is the first example we see of a model formula. we see many more examples of model formulas later on.
452,1,"['boxplots', 'data']", Stripcharts,seg_143,the boxplots made in the preceding section show a “laurel & hardy” effect that is not really well founded in the data. the cause is that the in-
453,1,"['plot', 'quartiles', 'range', 'stripchart', 'stripcharts', 'data', 'boxplot', 'function']", Stripcharts,seg_143,"terquartile range is quite a bit larger in one group than in the other, making the boxplot appear “fatter”. with groups as small as these, the quartiles will be quite inaccurately determined, and it may therefore be more desirable to plot the raw data. if you were to do this by hand, you might draw a dot diagram where every number is marked with a dot on a number line. r’s automated variant of this is the function stripchart. four variants of stripcharts are shown in figure 4.8."
454,1,['plots'], Stripcharts,seg_143,the four plots were created as follows:
455,1,"['plot', 'plots']", Stripcharts,seg_143,"notice that a little par magic was used to reduce the spacing between the four plots. the mex setting reduces the interline distance, and mar reduces the number of lines that surround the plot region. this can be done for these plots since they have neither main title, subtitle, nor x and y labels."
456,1,['variable'], Stripcharts,seg_143,all the original values of the changed settings can be stored in a variable (here opar) and reestablished with par(opar).
457,1,"['plot', 'method', 'stripchart', 'set', 'standard']", Stripcharts,seg_143,"the first plot is a standard stripchart, where the points are simply plotted on a line. the problem with this is that some points can become invisible because they are overplotted. this is why there is a method argument, which can be set to either ""stack"" or ""jitter""."
458,1,"['plot', 'method', 'data']", Stripcharts,seg_143,"the former method stacks points with identical values, but it only does so if data are completely identical, so in the upper right plot, it is only the two replicates of 7.48 that get stacked, whereas 8.08, 8.09, and 8.11 are still plotted in almost the same spot."
459,1,"['plot', 'standard', 'method', 'data', 'set', 'random']", Stripcharts,seg_143,"the “jitter” method offsets all points a random amount vertically. the standard jittering on plot no. 3 (bottom left) is a bit large; it may be preferable to make it clearer that data are placed along a horizontal line. for that purpose, you can set jitter lower than the default of 0.1, which is done in the fourth plot."
460,1,"['boxplot', 'data']", Stripcharts,seg_143,in this example we have not bothered to specify data in several forms as we did for boxplot but used expend~stature throughout. we could also have written
461,0,[], Stripcharts,seg_143,"but stripchart(expend.lean, expend.obese) cannot be used."
462,1,"['frequencies', 'data', 'relative frequencies', 'tables']", Tables,seg_145,categorical data are usually described in the form of tables. this section outlines how you can create tables from your data and calculate relative frequencies.
463,1,"['tables', 'table']", Generating tables,seg_147,"we deal mainly with two-way tables. in the first example, we enter a table directly, as is required for tables taken from a book or a journal article."
464,1,['table'], Generating tables,seg_147,"a two-way table can be entered as a matrix object (section 1.2.7). altman (1991, p. 242) contains an example on caffeine consumption by marital status among women giving birth. that table may be input as follows:"
465,1,"['function', 'table']", Generating tables,seg_147,"the matrix function needs an argument containing the table values as a single vector and also the number of rows in the argument nrow. by default, the values are entered columnwise; if rowwise entry is desired, then you need to specify byrow=t."
466,0,[], Generating tables,seg_147,"you might also give the number of columns instead of rows using ncol. if exactly one of ncol and nrow is given, r will compute the other one so that it fits the number of values. if both ncol and nrow are given and it does not fit the number of values, the values will be “recycled”, which in some (other!) circumstances can be useful."
467,0,[], Generating tables,seg_147,"to get readable printouts, you can add row and column names to the matrices."
468,1,['tables'], Generating tables,seg_147,"furthermore, you can name the row and column names as follows. this is particularly useful if you are generating many tables with similar classification criteria."
469,1,"['table', 'tables', 'function']", Generating tables,seg_147,"actually, i glossed over something. tables are not completely equivalent to matrices. there is a ""table"" class for which special methods exist, and you can convert to that class using as.table(caff.marital). the table function below returns an object of class ""table""."
470,1,"['tables', 'case', 'data', 'table']", Generating tables,seg_147,"for most elementary purposes, you can use matrices where two-dimensio- nal tables are expected. one important case where you do need as.table is when converting a table to a data frame of counts:"
471,1,"['levels', 'factor', 'case', 'set', 'function', 'data', 'functions', 'factors', 'categorical', 'data set', 'categorical data', 'table', 'variables', 'tables']", Generating tables,seg_147,"in practice, the more frequent case is that you have a data frame with variables for each person in a data set. in that case, you should do the tabulation with table, xtabs, or ftable. these functions will generally work for tabulating numeric vectors as well as factor variables, but the latter will have their levels used for row and column names automatically. hence, it is recommended to convert numerically coded categorical data into factors. the table function is the oldest and most basic of the three. the two others offer formula-based interfaces and better printing of multiway tables."
472,1,"['variables', 'data', 'set', 'data set', 'tables']", Generating tables,seg_147,"the data set juul was introduced on p. 68. here we look at some other variables in that data set, namely sex and menarche; the latter indicates whether or not a girl has had her first period. we can generate some simple tables as follows:"
473,1,"['data', 'table']", Generating tables,seg_147,"of course, the table of menarche versus sex is just a check on internal consistency of the data. the table of menarche versus tanner stage of puberty is more interesting."
474,1,"['functions', 'table', 'data', 'tables', 'function']", Generating tables,seg_147,"there are also tables with more than two sides, but not many simple statistical functions use them. briefly, to tabulate such data, just write, for example, table(factor1,factor2,factor3). to input a table of cell counts, use the array function (an analogue of matrix)."
475,1,"['model', 'table', 'variables', 'function']", Generating tables,seg_147,the xtabs function is quite similar to table except that it uses a model formula interface. this most often uses a one-sided formula where you just list the classification variables separated by +.
476,1,"['variables', 'data']", Generating tables,seg_147,notice how the interface allows you to refer to variables in a data frame without attaching it. the empty left-hand side can be replaced by a vector of counts in order to handle pretabulated data.
477,1,"['tables', 'table']", Generating tables,seg_147,"the formatting of multiway tables from table or xtabs is not really nice; e.g.,"
478,1,"['function', 'tables']", Generating tables,seg_147,"as you add dimensions, you get more of these two-sided subtables and it becomes rather easy to lose track. this is where ftable comes in. this function creates “flat” tables; e.g., like this:"
479,1,"['functions', 'table', 'variables', 'data']", Generating tables,seg_147,"that is, variables on the left-hand side tabulate across the page and those on the right tabulate downwards. ftable works on raw data as shown, but its data argument can also be a table as generated by one of the other functions."
480,1,"['function', 'table']", Generating tables,seg_147,"like any matrix, a table can be transposed with the t function:"
481,1,['tables'], Generating tables,seg_147,"for multiway tables, exchanging indices (generalized transposition) is done by aperm."
482,1,"['table', 'factor', 'marginal', 'tables', 'function']", Marginal tables and relative frequency,seg_149,"it is often desired to compute marginal tables; that is, the sums of the counts along one or the other dimension of a table. due to missing values, this might not coincide with just tabulating a single factor. this is done fairly easily using the apply function (section 1.2.15), but there is also a simplified version called margin.table, described below."
483,1,['table'], Marginal tables and relative frequency,seg_149,"first, we need to generate the table itself:"
484,0,[], Marginal tables and relative frequency,seg_149,(tanner.sex is an arbitrarily chosen name for the crosstable.)
485,1,"['marginal', 'tables']", Marginal tables and relative frequency,seg_149,then we compute the marginal tables:
486,1,"['column totals', 'row and column totals', 'marginal']", Marginal tables and relative frequency,seg_149,"the second argument to margin.table is the number of the marginal index: 1 and 2 give row and column totals, respectively."
487,1,"['column totals', 'table', 'frequencies', 'relative frequencies', 'tables']", Marginal tables and relative frequency,seg_149,relative frequencies in a table are generally expressed as proportions of the row or column totals. tables of relative frequencies can be constructed using prop.table as follows:
488,1,"['percentages', 'table']", Marginal tables and relative frequency,seg_149,"note that the rows (1st index) sum to 1. if a table of percentages is desired, just multiply the entire table by 100."
489,1,['table'], Marginal tables and relative frequency,seg_149,"prop.table cannot be used to express the numbers relative to the grand total of the table, but you can of course always write"
490,1,"['functions', 'tables']", Marginal tables and relative frequency,seg_149,the functions margin.table and prop.table also work on multiway tables — the margin argument can be a vector if the relevant margin has two or more dimensions.
491,1,"['table', 'percentages']", Graphical display of tables,seg_151,"for presentation purposes, it may be desirable to display a graph rather than a table of counts or percentages. in this section, the main methods for doing this are described."
492,1,"['function', 'barplot']", Barplots,seg_153,"barplots are made using barplot. this function takes an argument, which can be a vector or a matrix. the simplest variant goes as follows (figure 4.9):"
493,1,['plot'], Barplots,seg_153,"without the col=""white"" argument, the plot comes out in colour, but this is not suitable for a black and white book illustration."
494,1,"['barplot', 'table']", Barplots,seg_153,"if the argument is a matrix, then barplot creates by default a “stacked barplot”, where the columns are partitioned according to the contributions from different rows of the table. if you want to place the row contributions beside each other instead, you can use the argument beside=t. a series of variants is found in figure 4.10, which is constructed as follows:"
495,1,"['plots', 'data', 'information', 'set', 'data set', 'function']", Barplots,seg_153,"in the last three plots, we switched rows and columns with the transposition function t. in the very last one, the columns are expressed as proportions of the total number in the group. thus, information is lost on the relative sizes of the marital status groups, but the group of previously married women (recall that the data set deals with women giving birth)"
496,1,['profile'], Barplots,seg_153,is so small that it otherwise becomes almost impossible to compare their caffeine consumption profile with those of the other groups.
497,1,['plots'], Barplots,seg_153,"as usual, there are a multitude of ways to “prettify” the plots. here is one possibility (figure 4.11):"
498,1,"['plot', 'locator', 'function', 'control']", Barplots,seg_153,"notice that the legend overlaps the top of one of the columns. r is not designed to be able to find a clear area in which to place the legend. however, you can get full control of the legend’s position if you insert it explicitly with the legend function. for that purpose, it will be helpful to use locator(), which allows you to click a mouse button over the plot and have the coordinates returned. see p. 209 for more about this."
499,1,"['dotcharts', 'table']", Dotcharts,seg_155,"the cleveland dotcharts, named after william s. cleveland (1994), can be employed to study a table from both sides at the same time. they contain"
500,1,"['barplots', 'information']", Dotcharts,seg_155,the same information as barplots with beside=t but give quite a different visual impression. we content ourselves with a single example here (figure 4.12):
501,0,[], Dotcharts,seg_155,"(the line colour was changed from the default ""gray"" because it tends to be hard to see in print.)"
502,1,"['table', 'barplot', 'statisticians', 'data', 'information']", Piecharts,seg_157,"piecharts are traditionally frowned upon by statisticians because they are so often used to make trivial data look impressive and are difficult to decode for the human mind. they very rarely contain information that would not have been at least as effectively conveyed in a barplot. once in a while they are useful, though, and it is no problem to get r to draw them. here is a way to represent the table of caffeine consumption versus marital status (figure 4.13; see section 4.4.3 for an explanation of the “par magic” used to reduce the space between the subplots):"
503,1,['sets'], Piecharts,seg_157,the col argument sets the colour of the pie slices.
504,1,['distribution'], Piecharts,seg_157,there are more possibilities with piechart. the help page for pie contains an illustrative example concerning the distribution of pie sales (!) by pie type.
505,1,"['plot', 'plots', 'vary']", Exercises,seg_159,"4.1 explore the possibilities for different kinds of line and point plots. vary the plot symbol, line type, line width, and colour."
506,1,"['plot', 'plotting']", Exercises,seg_159,"4.2 if you make a plot like plot(rnorm(10),type=""o"") with overplotted lines and points, the lines will be visible inside the plotting symbols. how can this be avoided?"
507,1,"['plot', 'plots', 'qqnorm', 'plotting']", Exercises,seg_159,"4.3 how can you overlay two qqnorm plots in the same plotting area? what goes wrong if you try to generate the plot using type=""l"", and how do you avoid that?"
508,1,"['plot', 'histogram', 'biased', 'data', 'replacement', 'set', 'data set']", Exercises,seg_159,"4.4 plot a histogram for the react data set. since these data are highly discretized, the histogram will be biased. why? you may want to try truehist from the mass package as a replacement."
509,1,"['plot', 'sample', 'curve', 'distribution', 'random', 'function', 'random numbers']", Exercises,seg_159,"4.5 generate a sample vector z of five random numbers from the uniform distribution, and plot quantile(z,x) as a function of x (use curve, for instance)."
510,1,['functions'], One and twosample tests,seg_161,most of the rest of this book describes applications of r for actual statistical analysis. the focus to some extent shifts from explanation of the syntax to description of the output and specific arguments to the relevant functions.
511,1,"['tests', 'data', 'statistical tests', 'continuous', 'statistical']", One and twosample tests,seg_161,some of the most basic statistical tests deal with comparing continuous data either between two groups or against an a priori stipulated value. this is the topic for this chapter.
512,1,"['sample', 'functions', 'paired data', 'wilcoxon tests', 'data', 'tests', 'paired']", One and twosample tests,seg_161,"two functions are introduced here, namely t.test and wilcox.test for t tests and wilcoxon tests, respectively. both can be applied to oneand two-sample problems as well as paired data. notice that the “two- sample wilcoxon test” is the same as the one called the “mann–whitney test” in many textbooks."
513,1,"['case', 'normal distribution', 'random', 'null hypothesis', 'estimate', 'data', 'mean', 'standard', 'tests', 'parameters', 'distribution', 'test', 'variance', 'hypothesis', 'random variables', 'independent', 'variables', 'normal']", Onesample t test,seg_163,"the t tests are based on an assumption that data come from the normal distribution. in the one-sample case we thus have data x1, . . . , xn assumed to be independent realizations of random variables with distribution n(µ, σ2), which denotes the normal distribution with mean µ and variance σ2, and we wish to test the null hypothesis that µ = µ0. we can estimate the parameters µ and σ by the empirical mean x̄ and standard"
514,0,[], Onesample t test,seg_163,"deviation s, although we must realize that we could never pinpoint their values exactly."
515,1,"['standard', 'standard error', 'variation', 'mean', 'variance', 'random', 'average', 'error']", Onesample t test,seg_163,"the key concept is that of the standard error of the mean, or sem. this describes the variation of the average of n random values with mean µ and variance σ2. this value is"
516,1,"['sample', 'experiment', 'data', 'normally distributed', 'distribution', 'mean', 'probability', 'standard', 'average']", Onesample t test,seg_163,"and means that if you were to repeat the entire experiment several times and calculate an average for each experiment, then these averages would follow a distribution that is narrower than that of the original distribution. the crucial point is that even based on a single sample, it is possible to calculate an empirical sem as s/√n using the empirical standard deviation of the sample. this value will tell us how far the observed mean may reasonably have strayed from its true value. for normally distributed data, the rule of thumb is that there is a 95% probability of staying within µ± 2σ, so we would expect that if µ0 were the true mean, then x̄ should be within 2 sems of it. formally, you calculate"
517,1,"['interval', 'acceptance region', 'case', 'probability', 'level', 'significance', 'significance level']", Onesample t test,seg_163,"and see whether this falls within an acceptance region outside which t should fall with probability equal to a specified significance level. this is often chosen as 5%, in which case the acceptance region is almost, but not exactly, the interval from −2 to 2."
518,1,"['degrees of freedom', 'acceptance region', 'quantiles', 't distribution', 'distribution', 'samples', 'deviations', 'normal', 'normal distribution']", Onesample t test,seg_163,"in small samples, it is necessary to correct for the fact that an empirical sem is used and that the distribution of t therefore has somewhat “heavier tails” than the n(0, 1): large deviations happen more frequently than in the normal distribution since they can result from normalizing with an sem that is too small. the correct values for the acceptance region can be looked up as quantiles in the t distribution with f = n− 1 degrees of freedom."
519,1,"['acceptance region', 'probability', 'hypothesis', 'level', 'significance', 'significance level', 'null hypothesis']", Onesample t test,seg_163,"if t falls outside the acceptance region, then we reject the null hypothesis at the chosen significance level. alternatively (and equivalently), you can calculate the p-value, which is the probability of obtaining a value as numerically large as or larger than the observed t and reject the hypothesis if the p-value is less than the significance level."
520,1,"['information', 'distribution', 'tail', 'cases', 'rejection region', 'level', 'significance', 'test', 'significance level', 'hypothesis']", Onesample t test,seg_163,"sometimes you have prior information on the direction of an effect; for instance, that all plausible mechanisms that would cause µ not to equal µ0 would tend to make it bigger. in those cases, you may choose to reject the hypothesis only if t falls in the upper tail of the distribution. this is known as testing against a one-sided alternative. since removing the lower tail from the rejection region effectively halves the significance level, a one-sided test at a given level will have a smaller cutoff point. similarly, p-values"
521,1,"['probability', 'tests', 'test']", Onesample t test,seg_163,"are calculated as the probability of a larger value than observed rather than a numerically larger one, effectively halving the p-value as long as the observed effect is in the stipulated direction. one-sided tests should be used with some care, preferably only when there is a clear statement of the intent to use them in the study protocol. switching to a one-sided test to make an otherwise nonsignificant result significant could easily be regarded as dishonest."
522,1,['data'], Onesample t test,seg_163,"here is an example concerning daily energy intake in kj for 11 women (altman, 1991, p. 183). first, the values are placed in a data vector:"
523,1,"['statistics', 'data', 'set', 'data set']", Onesample t test,seg_163,"let us first look at some simple summary statistics, even though these are hardly necessary for such a small data set:"
524,1,"['data', 'distribution', 'normal', 'mean', 'test', 'normal distribution']", Onesample t test,seg_163,"you might wish to investigate whether the women’s energy intake deviates systematically from a recommended value of 7725 kj. assuming that data come from a normal distribution, the object is to test whether this distribution might have mean µ = 7725. this is done with t.test as follows:"
525,0,[], Onesample t test,seg_163,this is an example of the exact same type as used in the introductory section 1.1.4. the description of the output is quite superficial there. here it is explained more thoroughly.
526,1,"['tests', 'statistical tests', 'standard', 'statistical']", Onesample t test,seg_163,"the layout is common to many of the standard statistical tests, and a “dissection” is given in the following:"
527,1,"['function', 'test']", Onesample t test,seg_163,"this should be self-explanatory. it is simply a description of the test that we have asked for. notice that, by looking at the format of the function call, t.test has automatically found out that a one-sample test is desired."
528,1,"['function', 'data']", Onesample t test,seg_163,"this tells us which data are being tested. of course, this will be obvious unless output has been separated from the command that generated it. this can happen, for example, when using the source function to read commands from an external file."
529,1,"['degrees of freedom', 'table', 'quantiles', 't distribution', 'data', 'distribution', 'statistic', 'mean', 'level', 'significance', 'level of significance', 'hypothesis']", Onesample t test,seg_163,"this is where it begins to get interesting. we get the t statistic, the associated degrees of freedom, and the exact p-value. we do not need to use a table of the t distribution to look up which quantiles the t-value can be found between. you can immediately see that p < 0.05 and thus that (using the customary 5% level of significance) data deviate significantly from the hypothesis that the mean is 7725."
530,1,"['test', 'mean', 'information']", Onesample t test,seg_163,this contains two important pieces of information: (a) the value we wanted to test whether the mean could be equal to (7725 kj) and (b) that the test is two-sided (“not equal to”).
531,1,"['interval', 'acceptance region', 't test', 'data', 'set', 'mean', 'confidence', 'test', 'confidence interval']", Onesample t test,seg_163,"this is a 95% confidence interval for the true mean; that is, the set of (hypothetical) mean values from which the data do not deviate significantly. it is based on inverting the t test by solving for the values of µ0 that cause t to lie within its acceptance region. for a 95% confidence interval, the solution is"
532,1,"['mean', 'estimate']", Onesample t test,seg_163,"this final item is the observed mean; that is, the (point) estimate of the true mean."
533,1,['function'], Onesample t test,seg_163,"the function t.test has a number of optional arguments, three of which are relevant in one-sample problems. we have already seen the use of mu"
534,1,"['confidence intervals', 'interval', 'intervals', 'confidence', 'mean', 'level', 'test', 'confidence level', 'null hypothesis', 'hypothesis']", Onesample t test,seg_163,"to specify the mean value µ under the null hypothesis (default is mu=0). in addition, you can specify that a one-sided test is desired against alternatives greater than µ by using alternative=""greater"" or alternatives less than µ using alternative=""less"". the third item that can be specified is the confidence level used for the confidence intervals; you would write conf.level=0.99 to get a 99% interval."
535,1,['test'], Onesample t test,seg_163,"actually, it is often allowable to abbreviate a longish argument specification; for instance, it is sufficient to write alt=""g"" to get the test against greater alternatives."
536,1,"['statistics', 'data', 'distribution', 'order statistics', 'samples', 'normal', 'tests']", Wilcoxon signedrank test,seg_165,"the t tests are fairly robust against departures from the normal distribution especially in larger samples, but sometimes you wish to avoid making that assumption. to this end, the distribution-free methods are convenient. these are generally obtained by replacing data with corresponding order statistics."
537,1,"['wilcoxon test', 'symmetric', 'test statistic', 'distribution', 'samples', 'statistic', 'normal', 'numerical', 'probability', 'test', 'normal distribution']", Wilcoxon signedrank test,seg_165,"for the one-sample wilcoxon test, the procedure is to subtract the theoretical µ0 and rank the differences according to their numerical value, ignoring the sign, and then calculate the sum of the positive or negative ranks. the point is that, assuming only that the distribution is symmetric around µ0, the test statistic corresponds to selecting each number from 1 to n with probability 1/2 and calculating the sum. the distribution of the test statistic can be calculated exactly, at least in principle. it becomes computationally excessive in large samples, but the distribution is then very well approximated by a normal distribution."
538,1,"['t test', 'test']", Wilcoxon signedrank test,seg_165,practical application of the wilcoxon signed-rank test is done almost exactly like the t test:
539,1,"['estimate', 'confidence limits', 'parameter', 'confidence', 'test']", Wilcoxon signedrank test,seg_165,"there is not quite as much output as from t.test due to the fact that there is no such thing as a parameter estimate in a nonparametric test and therefore no confidence limits, etc., either. it is, however, possible under"
540,1,"['confidence intervals', 'location', 'location measure', 'intervals', 'confidence']", Wilcoxon signedrank test,seg_165,some assumptions to define a location measure and calculate confidence intervals for it. see the help files for wilcox.test for details.
541,1,"['efficient', 'observations', 'sample', 't test', 'biased', 'data', 'samples', 'tests', 'sample size', 'covariate', 'model', 'level', 'test', 'independent']", Wilcoxon signedrank test,seg_165,"the relative merits of distribution-free (or nonparametric) versus parametric methods such as the t test are a contentious issue. if the model assumptions of the parametric test are fulfilled, then it will be somewhat more efficient, on the order of 5% in large samples, although the difference can be larger in small samples. notice, for instance, that unless the sample size is 6 or above, the signed-rank test simply cannot become significant at the 5% level. this is probably not too important, though; what is more important is that the apparent lack of assumptions for these tests sometimes misleads people into using them for data where the observations are not independent or where a comparison is biased by an important covariate."
542,1,"['tests', 'cases', 'normal', 'wilcoxon tests', 'average', 'distributions']", Wilcoxon signedrank test,seg_165,"the wilcoxon tests are susceptible to the problem of ties, where several observations share the same value. in such cases, you simply use the average of the tied ranks; for example, if there are four identical values corresponding to places 6 to 9, they will all be assigned the value 7.5. this is not a problem for the large-sample normal approximations, but the exact small-sample distributions become much more difficult to calculate and wilcox.test cannot do so."
543,1,"['normal approximation', 'test statistic', 'approximation', 'statistic', 'normal', 'test']", Wilcoxon signedrank test,seg_165,"the test statistic v is the sum of the positive ranks. in the example, the p-value is computed from the normal approximation because of the tie at 7515."
544,1,"['function', 'tests']", Wilcoxon signedrank test,seg_165,"the function wilcox.test takes arguments mu and alternative, just like t.test. in addition, it has correct, which turns a continuity correction on or off (the default is “on”, as seen from the output title; correct=f turns it off), and exact, which specifies whether exact tests should be calculated. recall that “on/off” options such as these are specified using logical values that can be either true or false."
545,1,"['t test', 'samples', 'mean', 'distributions', 'test', 'hypothesis']", Twosample t test,seg_167,the two-sample t test is used to test the hypothesis that two samples may be assumed to come from distributions with the same mean.
546,1,"['t test', 'data', 'normal', 'test', 'null hypothesis', 'hypothesis']", Twosample t test,seg_167,"the theory for the two-sample t test is not very different in principle from that of the one-sample test. data are now from two groups, x11, . . . , x1n1 and x21, . . . , x2n2 , which we assume are sampled from the normal distributions n(µ1, σ12) and n(µ2, σ22), and it is desired to test the null hypothesis µ1 = µ2. you then calculate"
547,1,"['standard error', 'error', 'standard']", Twosample t test,seg_167,where the standard error of difference of means is
548,1,"['degrees of freedom', 'standard deviations', 't distribution', 'distribution', 'deviations', 'hypothesis', 'standard', 'variance', 'null hypothesis', 'variances']", Twosample t test,seg_167,"there are two ways of calculating the sedm depending on whether or not you assume that the two groups have the same variance. the “classical” approach is to assume that the variances are identical. with this approach, you first calculate a pooled s based on the standard deviations from the two groups and plug that value into the sem. under the null hypothesis, the t value will follow a t distribution with n1 + n2− 2 degrees of freedom."
549,1,"['degrees of freedom', 'standard deviations', 't distribution', 'distribution', 'deviations', 'standard']", Twosample t test,seg_167,"an alternative procedure due to welch is to calculate the sems from the separate group standard deviations s1 and s2. with this procedure, t is actually not t-distributed, but its distribution may be approximated by a t distribution with a number of degrees of freedom that can be calculated from s1, s2, and the group sizes. this is generally not an integer."
550,1,"['standard deviations', 'results', 'deviations', 'standard']", Twosample t test,seg_167,"the welch procedure is generally considered the safer one. usually, the two procedures give very similar results unless both the group sizes and the standard deviations are very different."
551,1,['data'], Twosample t test,seg_167,we return to the daily energy expenditure data (see section 1.2.14) and consider the problem of comparing energy expenditures between lean and obese women.
552,1,"['model', 'factor', 'data', 'information', 'data frames', 'variable', 'response', 'numeric variable']", Twosample t test,seg_167,"notice that the necessary information is contained in two parallel columns of a data frame. the factor stature contains the group and the numeric variable expend the energy expenditure in mega-joules. r allows data in this format to be analyzed by t.test and wilcox.test using a model formula specification. an older format (still available) requires you to specify data from each group in a separate variable, but the newer format is much more convenient for data that are kept in data frames and is also more flexible if you later want to group the same response data according to other criteria."
553,1,"['level', 't test', 'test']", Twosample t test,seg_167,"the object is to see whether there is a shift in level between the two groups, so we apply a t test as follows:"
554,0,[], Twosample t test,seg_167,notice the use of the tilde (~) operator to specify that expend is described by stature.
555,1,"['interval', 'level', 'confidence', 'test', 'confidence interval']", Twosample t test,seg_167,"the output is not much different from that of the one-sample test. the confidence interval is for the difference in means and does not contain 0, which is in accordance with the p-value indicating a significant difference at the 5% level."
556,1,"['degrees of freedom', 't test', 'results', 'variance', 'test']", Twosample t test,seg_167,"it is welch’s variant of the t test that is calculated by default. this is the test where you do not assume that the variance is the same in the two groups, which (among other things) results in the fractional degrees of freedom."
557,1,"['t test', 'test', 'variances']", Twosample t test,seg_167,"to get the usual (textbook) t test, you must specify that you are willing to assume that the variances are the same. this is done via the optional argument var.equal=t; that is:"
558,1,"['degrees of freedom', 'interval', 'confidence', 'confidence interval']", Twosample t test,seg_167,"notice that the degrees of freedom now has become a whole number, namely 13 + 9− 2 = 20. the p-value has dropped slightly (from 0.14% to 0.08%) and the confidence interval is a little narrower, but overall the changes are slight."
559,1,"['f test', 't test', 'function', 'test', 'variances']", Comparison of variances,seg_169,"even though it is possible in r to perform the two-sample t test without the assumption that the variances are the same, you may still be interested in testing that assumption, and r provides the var.test function for that purpose, implementing an f test on the ratio of the group variances. it is called the same way as t.test:"
560,1,"['confidence interval', 'tests', 'interval', 'homogeneity', 'data', 'distribution', 'sets', 'normal', 'confidence', 'data sets', 'test', 'variance', 'normal distribution', 'variances']", Comparison of variances,seg_169,"the test is not significant, so there is no evidence against the assumption that the variances are identical. however, the confidence interval is very wide. for small data sets such as this one, the assumption of constant variance is largely a matter of belief. it may also be noted that this test is not robust against departures from a normal distribution. the stats package contains several alternative tests for variance homogeneity, each with its own assumptions, benefits, and drawbacks, but we shall not discuss them at length."
561,1,"['paired data', 'independent', 'data', 'test', 'paired']", Comparison of variances,seg_169,notice that the test is based on the assumption that the groups are independent. you should not apply this test to paired data.
562,1,"['wilcoxon test', 't test', 'without replacement', 'replacement', 'data', 'sampling', 'normal', 'test']", Twosample Wilcoxon test,seg_171,"you might prefer a nonparametric test if you doubt the normal distribution assumptions of the t test. the two-sample wilcoxon test is based on replacing the data by their rank (without regard to grouping) and calculating the sum of the ranks in one group, thus reducing the problem to one of sampling n1 values without replacement from the numbers 1 to n1 + n2."
563,0,[], Twosample Wilcoxon test,seg_171,"this is done using wilcox.test, which behaves similarly to t.test:"
564,1,"['test statistic', 'distribution', 'statistic', 'normal', 'test', 'normal distribution']", Twosample Wilcoxon test,seg_171,"the test statistic w is the sum of ranks in the first group minus its theoretical minimum (i.e., it is zero if all the smallest values fall in the first group). some textbooks use a statistic that is the sum of ranks in the smallest group with no minimum correction, which is of course equivalent. notice that, as in the one-sample example, we are having problems with ties and rely on the approximate normal distribution of w."
565,1,"['plot', 'deviation', 'graphical', 'level', 'independent', 'dispersion', 'case', 'data', 'distribution', 'measurements', 'transform', 'transformation', 'tests', 'test', 'average']", The paired t test,seg_173,"paired tests are used when there are two measurements on the same experimental unit. the theory is essentially based on taking differences and thus reducing the problem to that of a one-sample test. notice, though, that it is implicitly assumed that such differences have a distribution that is independent of the level. a useful graphical check is to make a scatterplot of the pairs with the line of identity added or to plot the difference against the average of the pair (sometimes called a bland–altman plot). if there seems to be a tendency for the dispersion to change with the level, then it may be useful to transform the data; frequently the standard deviation is proportional to the level, in which case a logarithmic transformation is useful."
566,1,"['set', 'data set', 'data']", The paired t test,seg_173,"the data on preand postmenstrual energy intake in a group of women are considered several times in chapter 1 (and you may notice that the first column is identical to daily.intake, which was used in section 5.1). there data are entered from the command line, but they are also available as a data set in the iswr package:"
567,0,[], The paired t test,seg_173,"the point is that the same 11 women are measured twice, so it makes sense to look at individual differences:"
568,1,"['t test', 'test', 'paired t test', 'paired']", The paired t test,seg_173,it is immediately seen that they are all negative. all the women have a lower energy intake postmenstrually than premenstrually. the paired t test is obtained as follows:
569,1,"['t test', 'test']", The paired t test,seg_173,there is not much new to say about the output; it is virtually identical to that of a one-sample t test on the elementwise differences.
570,1,"['t test', 'test', 'data', 'paired']", The paired t test,seg_173,"notice that you have to specify paired=t explicitly in the call, indicating that you want a paired test. in the old-style interface for the unpaired t test, the two groups are specified as separate vectors and you would request that analysis by omitting paired=t. if data are actually paired, then it would be seriously inappropriate to analyze them without taking the pairing into account."
571,1,"['results', 't test', 'test', 'data']", The paired t test,seg_173,"even though it might be considered pedagogically dubious to show what you should not do, the following shows the results of an unpaired t test on the same data for comparison:"
572,0,[], The paired t test,seg_173,the number symbol (or “hash”) # introduces a comment in r. the rest of the line is skipped.
573,1,"['level', 'independent', 'interval', 'information', 'loss', 'measurements', 'efficiency', 'confidence', 'confidence interval', 'paired']", The paired t test,seg_173,"it is seen that t has become considerably smaller, although still significant at the 5% level. the confidence interval has become almost four times wider than in the correct paired analysis. both illustrate the loss of efficiency caused by not using the information that the “pre” and “post” measurements are from the same person. alternatively, you could say that it demonstrates the gain in efficiency obtained by planning the experiment with two measurements on the same person, rather than having two independent groups of preand postmenstrual women."
574,1,"['wilcoxon test', 'test', 'paired']", The matchedpairs Wilcoxon test,seg_175,the paired wilcoxon test is the same as a one-sample wilcoxon signedrank test on the differences. the call is completely analogous to t.test:
575,1,"['t test', 'statistic', 'test']", The matchedpairs Wilcoxon test,seg_175,"the result does not show any material difference from that of the t test. the p-value is not quite so extreme, which is not too surprising since the wilcoxon rank sum cannot get any larger than it does when all differences have the same sign, whereas the t statistic can become arbitrarily extreme."
576,1,['data'], The matchedpairs Wilcoxon test,seg_175,"again, we have trouble with tied data invalidating the exact p calculations. this time it is the two identical differences of −1540."
577,1,"['wilcoxon test', 'case', 'probability', 'test']", The matchedpairs Wilcoxon test,seg_175,"in the present case it is actually very easy to calculate the exact p-value for the wilcoxon test. it is the probability of 11 positive differences + the probability of 11 negative ones, 2× (1/2)11 = 1/1024 = 0.00098, so the approximate p-value is almost four times too large."
578,1,"['t test', 'data', 'normally distributed', 'set', 'data set', 'mean', 'test']", Exercises,seg_177,"5.1 do the values of the react data set (notice that this is a single vector, not a data frame) look reasonably normally distributed? does the mean differ significantly from zero according to a t test?"
579,1,"['interval', 't test', 'data', 'set', 'data set', 'confidence', 'test', 'confidence interval']", Exercises,seg_177,"5.2 in the data set vitcap, use a t test to compare the vital capacity for the two groups. calculate a 99% confidence interval for the difference. the result of this comparison may be misleading. why?"
580,1,['data'], Exercises,seg_177,5.3 perform the analyses of the react and vitcap data using nonparametric techniques.
581,1,"['graphical', 't test', 'data', 'set', 'data set', 'test', 'paired t test', 'paired']", Exercises,seg_177,5.4 perform graphical checks of the assumptions for a paired t test in the intake data set.
582,1,"['outliers', 'plot', 'data', 'normality', 'function', 'test']", Exercises,seg_177,5.5 the function shapiro.test computes a test of normality based on the degree of linearity of the q–q plot. apply it to the react data. does it help to remove the outliers?
583,1,"['results', 'trial', 'method']", Exercises,seg_177,"5.6 the crossover trial in ashina can be analyzed for a drug effect in a simple way (how?) if you ignore a potential period effect. however, you can do better. hint: consider the intra-individual differences; if there were only a period effect present, how should the differences behave in the two groups? compare the results of the simple method and the improved method."
584,1,"['degrees of freedom', 'observations', 'case', 't distribution', 'normally distributed', 'exponential distribution', 'experiment', 'data', 'samples', 'sets', 'exponential', 'mean', 'simulated', 'tests', 'data sets', 'distribution', 'replications', 'test']", Exercises,seg_177,"5.7 perform 10 one-sample t tests on simulated normally distributed data sets of 25 observations each. repeat the experiment, but instead simulate samples from a different distribution; try the t distribution with 2 degrees of freedom and the exponential distribution (in the latter case, test for the mean being equal to 1). can you find a way to automate this so that you can have a larger number of replications?"
585,1,"['model', 'prediction intervals', 'prediction', 'plots', 'correlation', 'regression', 'intervals', 'confidence']", Regression and correlation,seg_179,"the main object of this chapter is to show how to perform basic regression analyses, including plots for model checking and display of confidence and prediction intervals. furthermore, we describe the related topic of correlation in both its parametric and nonparametric variants."
586,1,"['linear', 'variables', 'regression analysis', 'regression', 'linear regression', 'function']", Simple linear regression,seg_181,"we consider situations where you want to describe the relation between two variables using linear regression analysis. you may, for instance, be interested in describing short.velocity as a function of blood.glucose. this section deals only with the very basics, whereas several more complicated issues are postponed until chapter 12."
587,1,"['linear', 'model', 'regression model', 'regression', 'linear regression', 'linear regression model']", Simple linear regression,seg_181,the linear regression model is given by
588,1,"['independent', 'intercept', 'regression', 'regression coefficient', 'coefficient', 'slope']", Simple linear regression,seg_181,"in which the ei are assumed independent and n(0, σ2). the nonrandom part of the equation describes the yi as lying on a straight line. the slope of the line (the regression coefficient) is β, the increase per unit change in x. the line intersects the y-axis at the intercept α."
589,1,"['estimated', 'parameters', 'method', 'sum of squared', 'least squares', 'method of least squares']", Simple linear regression,seg_181,"the parameters α, β, and σ2 can be estimated using the method of least squares. find the values of α and β that minimize the sum of squared"
590,1,"['error', 'trial', 'parameters']", Simple linear regression,seg_181,this is not actually done by trial and error. one can find closed-form expressions for the choice of parameters that gives the smallest value of ssres:
591,1,"['deviation', 'estimated', 'residual', 'standard', 'standard deviation', 'variance', 'residual variance']", Simple linear regression,seg_181,"the residual variance is estimated as ssres/(n − 2), and the residual standard deviation is of course the square root of that."
592,1,"['estimates', 'set', 'sample', 'sets', 'intercepts', 'mean', 'parameter', 'standard', 'tests', 'standard errors', 'confidence', 'error', 'parameters', 'distribution', 'slopes', 'standard error', 'errors', 'intercept', 'sampling', 'variation', 'slope']", Simple linear regression,seg_181,"the empirical slope and intercept will deviate somewhat from the true values due to sampling variation. if you were to generate several sets of yi at the same set of xi, you would observe a distribution of empirical slopes and intercepts. just as you could calculate the sem to describe the variability of the empirical mean, it is also possible from a single sample of (xi, yi) to calculate the standard error of the computed estimates, s.e.(α̂) and s.e.(β̂). these standard errors can be used to compute confidence intervals for the parameters and tests for whether a parameter has a specific value."
593,1,"['error', 'estimate', 'standard error', 't test', 'distribution', 'standard', 'test', 'null hypothesis', 'hypothesis']", Simple linear regression,seg_181,"it is usually of prime interest to test the null hypothesis that β = 0 since that would imply that the line was horizontal and thus that the ys have a distribution that is the same, whatever the value of x. you can compute a t test for that hypothesis simply by dividing the estimate by its standard error"
594,1,"['degrees of freedom', 'range', 't distribution', 'data', 'distribution', 'intercept', 'test', 'extrapolation', 'hypothesis']", Simple linear regression,seg_181,"which follows a t distribution on n− 2 degrees of freedom if the true β is zero. a similar test can be calculated for whether the intercept is zero, but you should be aware that it is often a meaningless hypothesis either because there is no natural reason to believe that the line should go through the origin or because it would involve an extrapolation far outside the range of data."
595,1,['data'], Simple linear regression,seg_181,"for the example in this section, we need the data frame thuesen, which we attach with"
596,1,"['linear', 'model', 'linear model', 'regression analysis', 'regression', 'linear regression', 'function']", Simple linear regression,seg_181,"for linear regression analysis, the function lm (linear model) is used:"
597,1,"['model', 'stripcharts', 'boxplots', 'wilcoxon tests', 'tests']", Simple linear regression,seg_181,"the argument to lm is a model formula in which the tilde symbol (~) should be read as “described by”. this was seen several times earlier, both in connection with boxplots and stripcharts and with the t and wilcoxon tests."
598,1,"['model', 'linear', 'dependent', 'regression', 'variable', 'linear regression', 'multiple linear regression', 'function']", Simple linear regression,seg_181,"the lm function handles much more complicated models than simple linear regression. there can be many other things besides a dependent and a descriptive variable in a model formula. a multiple linear regression analysis (which we discuss in chapter 11) of, for example, y on x1, x2, and x3 is specified as y ~ x1 + x2 + x3."
599,1,"['estimated', 'significance', 'intercept', 'tests', 'slope']", Simple linear regression,seg_181,"in its raw form, the output of lm is very brief. all you see is the estimated intercept (α) and the estimated slope (β). the best-fitting straight line is seen to be short.velocity = 1.098 + 0.0220× blood.glucose, but for instance no tests of significance are given."
600,1,"['functions', 'model', 'information', 'extractor functions', 'statistical']", Simple linear regression,seg_181,"the result of lm is a model object. this is a distinctive concept of the s language (of which r is a dialect). whereas other statistical systems focus on generating printed output that can be controlled by setting options, you get instead the result of a model fit encapsulated in an object from which the desired quantities can be obtained using extractor functions. an lm object does in fact contain much more information than you see when it is printed."
601,1,"['function', 'extractor function']", Simple linear regression,seg_181,a basic extractor function is summary:
602,1,['statistical'], Simple linear regression,seg_181,the format above looks more like what other statistical packages would output. the following is a “dissection” of the output:
603,1,"['variable', 'function']", Simple linear regression,seg_181,"as in t.test, etc., the output starts with something that is essentially a repeat of the function call. this is not very interesting when one has just given it as a command to r, but it is useful if the result is saved in a variable that is printed later."
604,1,"['median', 'observations', 'residuals', 'distribution', 'quartile', 'third quartile', 'number of observations', 'average']", Simple linear regression,seg_181,"this gives a superficial view of the distribution of the residuals that may be used as a quick check of the distributional assumptions. the average of the residuals is zero by definition, so the median should not be far from zero, and the minimum and maximum should be roughly equal in absolute value. in the example, it can be noticed that the third quartile is remarkably close to zero, but in view of the small number of observations, this is not really something to worry about."
605,1,"['significance', 'graphical', 'regression coefficient', 'standard', 'coefficient', 'tests', 'standard errors', 'regression', 'level', 'level of significance', 'table', 'errors', 'intercept']", Simple linear regression,seg_181,"here we see the regression coefficient and the intercept again, but this time with accompanying standard errors, t tests, and p-values. the symbols to the right are graphical indicators of the level of significance. the line below the table shows the definition of these indicators; one star means 0.01 < p < 0.05."
606,1,"['tests', 'graphical', 'intercept', 'significance']", Simple linear regression,seg_181,"the graphical indicators have been the target of some controversy. some people like to have the possibility of seeing at a glance whether there is “anything interesting” in an analysis, whereas others feel that the indicators too often correspond to meaningless tests. for instance, the intercept in the analysis above is hardly a meaningful quantity at all, and the threestar significance of it is certainly irrelevant. if you are bothered by the stars, turn them off with options(show.signif.stars=false)."
607,1,"['model', 'residual', 'data', 'residual variation', 'regression', 'set', 'data set', 'parameter', 'variation', 'regression line']", Simple linear regression,seg_181,"this is the residual variation, an expression of the variation of the observations around the regression line, estimating the model parameter σ. the model is not fitted to the entire data set because one value of short.velocity is missing."
608,1,"['simple linear regression', 'linear', 'correlation', 'regression', 'pearson', 'adjusted', 'pearson correlation coefficient', 'linear regression', 'adjusted r2', 'correlation coefficient', 'coefficient', 'variance']", Simple linear regression,seg_181,"the first item above is r2, which in a simple linear regression may be recognized as the squared pearson correlation coefficient (see section 6.4.1); that is, r2 = r2. the other one is the adjusted r2; if you multiply it by 100%, it can be interpreted as “% variance reduction” (this can, in fact, become negative)."
609,1,"['simple linear regression', 'linear', 't test', 'information', 'regression coefficient', 'coefficient', 'model', 'f test', 'regression', 'linear regression', 'test', 'hypothesis', 'regression analysis', 'variable', 'explanatory variable', 'degree of freedom', 'explanatory', 'slope']", Simple linear regression,seg_181,"this is an f test for the hypothesis that the regression coefficient is zero. this test is not really interesting in a simple linear regression analysis since it just duplicates information already given — it becomes more interesting when there is more than one explanatory variable. notice that it gives the exact same result as the t test for a zero slope. in fact, the f test is identical to the square of the t test: 4.414 = (2.101)2. this is true in any model with 1 degree of freedom."
610,1,"['residual', 'prediction', 'plots', 'data', 'residual plots', 'fitted line']", Simple linear regression,seg_181,"we will see later how to draw residual plots and plots of data with confidence and prediction limits. first, we draw just the points and the fitted line. figure 6.1 has been constructed as follows:"
611,1,"['linear', 'model', 'linear model', 'data', 'intercept', 'slope']", Simple linear regression,seg_181,"abline, meaning (a, b)-line, draws lines based on the intercept and slope, a and b, respectively. it can be used with scalar values as in abline(1.1,0.022), but conveniently it can also extract the information from a linear model fitted to data with lm."
612,1,"['functions', 'results', 'regression analysis', 'information', 'regression']", Residuals and fitted values,seg_183,"we have seen how summary can be used to extract information about the results of a regression analysis. two further extraction functions are fitted and resid. they are used as follows. for convenience, we first store the value returned by lm under the name lm.velo (short for “velocity”, but you could of course use any other name)."
613,1,"['function', 'fitted values']", Residuals and fitted values,seg_183,the function fitted returns fitted values — the y-values that you would expect for the given x-values according to the best-fitting straight
614,1,['case'], Residuals and fitted values,seg_183,"line; in the present case, 1.098+0.0220 blood.glucose. the resid-"
615,0,[], Residuals and fitted values,seg_183,* uals shown by resid is the difference between this and the observed short.velocity.
616,1,"['residuals', 'missing value', 'data', 'observation', 'variable', 'response', 'fitted values', 'response variable']", Residuals and fitted values,seg_183,"note that the fitted values and residuals are labelled with the row names of the thuesen data frame. notice in particular that they do not contain observation no. 16, which had a missing value in the response variable."
617,1,"['missing values', 'data']", Residuals and fitted values,seg_183,it is necessary to discuss some awkward aspects that arise when there are missing values in data.
618,1,"['plot', 'fitted line']", Residuals and fitted values,seg_183,"to put the fitted line on the plot, you might, although it is easier to use abline(lm.velo), get the idea of doing it with lines, but"
619,1,"['observations', 'fitted values', 'function', 'error']", Residuals and fitted values,seg_183,"which is true. there are 24 observations but only 23 fitted values because one of the short.velocity values is na. notice, incidentally, that the error occurs within a series of nested function calls, which are being listed along with the error message to reduce confusion."
620,0,[], Residuals and fitted values,seg_183,"what we needed was blood.glucose, but only for those patients whose short.velocity has been recorded."
621,1,"['missing values', 'method', 'variables', 'range', 'data', 'function', 'fitted line']", Residuals and fitted values,seg_183,recall that the is.na function yields a vector that is true wherever the argument is na (missing). one advantage to this method is that the fitted line does not extend beyond the range of data. the technique works but becomes clumsy if there are missing values in several variables:
622,1,"['variables', 'observations', 'data', 'function']", Residuals and fitted values,seg_183,"it becomes easier with the function complete.cases, which can find observations that are nonmissing on several variables or across an entire data frame."
623,1,"['set', 'method']", Residuals and fitted values,seg_183,"we could then attach thuesen[cc,] and work on from there. however, there is a better alternative available: you can use the na.exclude method for na handling. this can be set either as an argument to lm or as an option; that is,"
624,1,"['missing fitted value', 'observation']", Residuals and fitted values,seg_183,"notice how the missing observation, no. 16, now appears in the fitted values with a missing fitted value. it is necessary to recalculate the lm.velo object after changing the option."
625,1,"['plot', 'observations', 'residuals', 'fitted line']", Residuals and fitted values,seg_183,"to create a plot where residuals are displayed by connecting observations to corresponding points on the fitted line, you can do the following. the final result will look like figure 6.2. segments draws line segments; its arguments are the endpoint coordinates in the order (x1, y1, x2, y2)."
626,1,"['plot', 'residuals', 'fitted values']", Residuals and fitted values,seg_183,a simple plot of residuals versus fitted values is obtained as (figure 6.3)
627,1,"['plot', 'residuals', 'distribution', 'normal', 'normal distribution']", Residuals and fitted values,seg_183,and we can get an indication of whether residuals might have come from a normal distribution by checking for a straight line on a q–q plot (see section 4.2.3) as follows (figure 6.4):
628,1,['uncertainty'], Prediction and confidence bands,seg_185,"fitted lines are often presented with uncertainty bands around them. there are two kinds of bands, often referred to as the “narrow” and “wide” limits."
629,1,"['precision', 'confidence bands', 'observations', 'uncertainty', 'mean', 'confidence']", Prediction and confidence bands,seg_185,"the narrow bands, confidence bands, reflect the uncertainty about the line itself, like the sem expresses the precision with which a mean is known. if there are many observations, the bands will be quite narrow, reflecting"
630,1,['plot'], Prediction and confidence bands,seg_185,normal q−q plot
631,1,"['estimated', 'variability', 'standard error', 'predicted', 'standard', 'error', 'slope']", Prediction and confidence bands,seg_185,"a well-determined line. these bands often show a marked curvature since the line is better determined near the center of the point cloud. this is a fact that can be shown mathematically, but you may also understand it intuitively as follows: the predicted value at x̄ will be ȳ, whatever the slope is, and hence the standard error of the fitted value at that point is the sem of the ys. at other values of x, there will also be a contribution from the variability of the estimated slope, having increasing influence as you move away from x̄. technically, you also need to establish that ȳ and β̂ are uncorrelated."
632,1,"['prediction', 'observations', 'prediction bands', 'approximation', 'uncertainty', 'data', 'samples', 'standard', 'confidence', 'curve', 'variance', 'standard deviations', 'confidence bands', 'errors', 'deviations', 'number of observations']", Prediction and confidence bands,seg_185,"the wide bands, prediction bands, include the uncertainty about future observations. these bands should capture the majority of the observed points and will not collapse to a line as the number of observations increases. rather, the limits approach the true line ±2 standard deviations (for 95% limits). in smaller samples, the bands do curve since they include uncertainty about the line itself, but not as markedly as the confidence bands. obviously, these limits rely strongly on the assumption of normally distributed errors with a constant variance, so you should not use such limits unless you believe that the assumption is a reasonable approximation for the data at hand."
633,1,"['prediction', 'confidence bands', 'fitted values', 'function', 'confidence', 'prediction and confidence bands']", Prediction and confidence bands,seg_185,"predicted values, with or without prediction and confidence bands, may be extracted with the function predict. with no arguments, it just gives the fitted values:"
634,1,['predicted'], Prediction and confidence bands,seg_185,"if you add interval=""confidence"" or interval=""prediction"", then you get the vector of predicted values augmented with limits. the arguments can be abbreviated:"
635,1,"['curve', 'evaluating', 'prediction', 'case', 'data', 'confidence limits', 'fitted values', 'mean', 'confidence', 'expected values', 'prediction bands']", Prediction and confidence bands,seg_185,"fit denotes the expected values, here identical to the fitted values (they need not be; read on). lwr and upr are the lower and upper confidence limits for the expected values, respectively, the prediction limits for short.velocity for new persons with these values of blood.glucose. the warning in this case does not really mean that anything is wrong, but there is a pitfall: the limits should not be used for evaluating the observed data to which the line has been fitted. these will tend to lie closer to the line for the extreme x values because those data points are the more influential; that is, the prediction bands curve the wrong way."
636,1,"['confidence intervals', 'prediction', 'plots', 'scatterplot', 'intervals', 'function', 'confidence']", Prediction and confidence bands,seg_185,"the best way to add prediction and confidence intervals to a scatterplot is to use the matlines function, which plots the columns of a matrix against a vector."
637,1,"['plot', 'prediction', 'predicted', 'observation', 'random', 'confidence']", Prediction and confidence bands,seg_185,"there are a few snags to this, however: (a) the blood.glucose values are in random order; we do not want line segments connecting points haphazardly along the confidence curves; (b) the prediction limits, particularly the lower one, extend outside the plot region; and (c) the matlines command needs to be prevented from cycling through line styles and colours. notice that the na.exclude setting (p. 115) prevents us from also having an observation omitted from the predicted values."
638,1,['data'], Prediction and confidence bands,seg_185,the solution is to predict in a new data frame containing suitable x values (here blood.glucose) at which to predict. it is done as follows:
639,1,"['prediction', 'confidence limits', 'data', 'variable', 'confidence', 'predictions']", Prediction and confidence bands,seg_185,"what happens is that we create a new data frame in which the variable blood.glucose contains the values at which we want predictions to be made. pp and pc are then made to contain the result of predict for the new data in pred.frame with prediction limits and confidence limits, respectively."
640,1,"['missing values', 'prediction', 'range', 'scatterplot', 'case', 'standard', 'plotting']", Prediction and confidence bands,seg_185,"for the plotting, we first create a standard scatterplot, except that we ensure that it has enough room for the prediction limits. this is obtained by setting ylim=range(short.velocity, pp, na.rm=t). the function range returns a vector of length 2 containing the minimum and maximum values of its arguments. we need the na.rm=t argument to cause missing values to be skipped for the range computation; notice that short.velocity is included to ensure that points outside the prediction limits are not missed (although in this case there are none). finally, the curves are added, using as x-values the blood.glucose used for the prediction and setting the line types and colours to more sensible values. the final result is seen in figure 6.5."
641,1,"['random variables', 'variables', 'symmetric', 'correlation', 'variable', 'associated', 'correlation coefficient', 'random', 'coefficient']", Correlation,seg_187,"a correlation coefficient is a symmetric, scale-invariant measure of association between two random variables. it ranges from −1 to +1, where the extremes indicate perfect correlation and 0 means no correlation. the sign is negative when large values of one variable are associated with small values of the other and positive if both variables tend to be large or small"
642,1,"['correlation coefficients', 'correlation', 'coefficients']", Correlation,seg_187,"simultaneously. the reader should be warned that there are many incorrect uses of correlation coefficients, particularly when they are used in regression-type settings."
643,1,['correlation'], Correlation,seg_187,this section describes the computation of parametric and nonparametric correlation measures in r.
644,1,"['variables', 'correlation', 'pearson', 'normal', 'variance']", Pearson correlation,seg_189,"the pearson correlation is rooted in the two-dimensional normal distribution where the theoretical correlation describes the contour ellipses for the density. if both variables are scaled to have a variance of 1, then a correlation of zero corresponds to circular contours, whereas the ellipses become narrower and finally collapse into a line segment as the correlation approaches ±1."
645,1,"['coefficient', 'correlation coefficient', 'correlation']", Pearson correlation,seg_189,the empirical correlation coefficient is
646,1,"['pearson', 'linear', 'correlation']", Pearson correlation,seg_189,"it can be shown that |r| will be less than 1 unless there is a perfect linear relation between xi and yi, and for that reason the pearson correlation is sometimes called the “linear correlation”."
647,1,"['correlation', 'regression', 'variable', 'transforming', 'significance', 'test', 'slope']", Pearson correlation,seg_189,"it is possible to test the significance of the correlation by transforming it to a t-distributed variable (the formula is not particularly elucidating so we skip it here), which will be identical with the test obtained from testing the significance of the slope of either the regression of y on x or vice versa."
648,1,"['function', 'correlation']", Pearson correlation,seg_189,"the function cor can be used to compute the correlation between two or more vectors. however, if it is naively applied to the two vectors in thuesen, the following happens:"
649,1,"['functions', 'missing values', 'cases', 'mean', 'statistical']", Pearson correlation,seg_189,"all the elementary statistical functions in r require either that all values be nonmissing or that you explicitly state what should be done with the cases with missing values. for mean, var, sd, and similar one-vector functions, you can give the argument na.rm=t to indicate that missing values should be removed before the computation. for cor, you can write"
650,1,"['functions', 'variables', 'correlation', 'cases', 'measurements']", Pearson correlation,seg_189,"the reason that cor does not use na.rm=t like the other functions is that there are more possibilities than just removing incomplete cases or failing. if more than two variables are in play, it is also possible to use information from all nonmissing pairs of measurements (this might result in a correlation matrix that is not positive definite, though)."
651,1,"['variables', 'data', 'correlations']", Pearson correlation,seg_189,"you can obtain the entire matrix of correlations between all variables in a data frame by saying, for instance,"
652,1,['data'], Pearson correlation,seg_189,"of course, this is more interesting when the data frame contains more than two vectors!"
653,1,['variables'], Pearson correlation,seg_189,"however, the calculations above give no indication of whether the correlation is significantly different from zero. to that end, you need cor.test. it works simply by specifying the two variables:"
654,1,"['model', 'regression model', 'interval', 'table', 'anova table', 'anova', 'regression analysis', 'regression', 'correlation', 'confidence', 'confidence interval']", Pearson correlation,seg_189,"we also get a confidence interval for the true correlation. notice that it is exactly the same p-value as in the regression analysis in section 6.1 and also that based on the anova table for the regression model, which is described in section 7.5."
655,1,"['observations', 'correlation', 'independence', 'null hypothesis', 'coefficient', 'transformations', 'distribution', 'correlation coefficient', 'rank correlation coefficient', 'hypothesis', 'variables', 'normal', 'rank correlation', 'normal distribution']", Spearmans ρ,seg_191,"as with the oneand two-sample problems, you may be interested in nonparametric variants. these have the advantage of not depending on the normal distribution and, indeed, being invariant to monotone transformations of the coordinates. the main disadvantage is that its interpretation is not quite clear. a popular and simple choice is spearman’s rank correlation coefficient ρ. this is obtained quite simply by replacing the observations by their rank and computing the correlation. under the null hypothesis of independence between the two variables, the exact distribution of ρ can be calculated."
656,1,"['correlation', 'correlations', 'function', 'tests', 'test']", Spearmans ρ,seg_191,"unlike group comparisons where there is essentially one function per named test, correlation tests are all grouped into cor.test. there is no special spearman.test function. instead, the test is considered one of several possibilities for testing correlations and is therefore specified via an option to cor.test:"
657,1,"['independence', 'method', 'correlation']", Kendalls τ,seg_193,"the third correlation method that you can choose is kendall’s τ, which is based on counting the number of concordant and discordant pairs. a pair of points is concordant if the difference in the x-coordinate is of the same sign as the difference in the y-coordinate. for a perfect monotone relation, either all pairs will be concordant or all pairs will be discordant. under independence, there should be as many concordant pairs as there are discordant ones."
658,1,"['observations', 'data', 'sets', 'data sets']", Kendalls τ,seg_193,"since there are many pairs of points to check, this is quite a computationally intensive procedure compared with the two others. in small data sets such as the present one, it does not matter at all, though, and the procedure is generally usable up to at least 5000 observations."
659,1,['coefficient'], Kendalls τ,seg_193,"the τ coefficient has the advantage of a more direct interpretation over spearman’s ρ, but apart from that there is little reason to prefer one over the other."
660,1,"['correlations', 'pearson', 'level', 'correlation']", Kendalls τ,seg_193,"notice that neither of the two nonparametric correlations is significant at the 5% level, which the pearson correlation is, albeit only borderline significant."
661,1,"['plot', 'linear', 'model', 'rate', 'regression model', 'data', 'regression', 'set', 'data set', 'linear regression', 'linear regression model']", Exercises,seg_195,"6.1 with the rmr data set, plot metabolic rate versus body weight. fit a linear regression model to the relation. according to the fitted model,"
662,1,"['rate', 'interval', 'predicted', 'confidence', 'confidence interval', 'slope']", Exercises,seg_195,what is the predicted metabolic rate for a body weight of 70 kg? give a 95% confidence interval for the slope of the line.
663,1,"['linear', 'model', 'regression model', 'data', 'regression', 'set', 'data set', 'linear regression', 'concentration', 'linear regression model']", Exercises,seg_195,"6.2 in the juul data set, fit a linear regression model for the square root of the igf-i concentration versus age to the group of subjects over 25 years old."
664,1,"['plot', 'data', 'set', 'data set', 'level']", Exercises,seg_195,"6.3 in the malaria data set, analyze the log-transformed antibody level versus age. make a plot of the relation. do you notice anything peculiar?"
665,1,"['statistics', 'correlation', 'data', 'sets', 'simulated', 'mean', 'standard', 'standard deviation', 'data sets', 'distribution', 'scatterplots', 'deviation', 'normal', 'normal distribution']", Exercises,seg_195,6.4 one can generate simulated data from the two-dimensional normal distribution with a correlation of ρ by the following technique: (a) generate x as a normal variate with mean 0 and standard deviation 1; (b) generate y with mean ρx and standard deviation √1− ρ2. use this to create scatterplots of simulated data with a given correlation. compute the spearman and kendall statistics for some of these data sets.
666,1,"['case', 'observation', 'analysis of variance', 'variance', 'test']", Analysis of variance and the KruskalWallis test,seg_197,"in this section, we consider comparisons among more than two groups parametrically, using analysis of variance, as well as nonparametrically, using the kruskal–wallis test. furthermore, we look at two-way analysis of variance in the case of one observation per cell."
667,1,"['observations', 'observation', 'analysis of variance', 'mean', 'variance', 'grand mean', 'average']", Oneway analysis of variance,seg_199,"we start this section with a brief sketch of the theory underlying the oneway analysis of variance. a little bit of notation is necessary. let xij denote observation no. j in group i, so that x35 is the fifth observation in group 3; x̄i is the mean for group i, and x̄. is the grand mean (average of all observations)."
668,1,['observations'], Oneway analysis of variance,seg_199,we can decompose the observations as
669,1,['model'], Oneway analysis of variance,seg_199,informally corresponding to the model
670,1,"['variance', 'independent', 'error', 'hypothesis']", Oneway analysis of variance,seg_199,in which the hypothesis that all the groups are the same implies that all αi are zero. notice that the error terms eij are assumed to be independent and have the same variance.
671,1,['variation'], Oneway analysis of variance,seg_199,"now consider the sums of squares of the underbraced terms, known as variation within groups"
672,1,['variation'], Oneway analysis of variance,seg_199,and variation between groups
673,0,[], Oneway analysis of variance,seg_199,it is possible to prove that
674,1,"['measurements', 'variation']", Oneway analysis of variance,seg_199,"that is, the total variation is split into a term describing differences between group means and a term describing differences between individual measurements within the groups. one says that the grouping explains part of the total variation, and obviously an informative grouping will explain a large part of the variation."
675,1,"['degrees of freedom', 'observations', 'sum of squares', 'variation', 'number of observations']", Oneway analysis of variance,seg_199,"however, the sums of squares can only be positive, so even a completely irrelevant grouping will always “explain” some part of the variation. the question is how small an amount of explained variation can be before it might as well be due to chance. it turns out that in the absence of any systematic differences between the groups, you should expect the sum of squares to be partitioned according to the degrees of freedom for each term, k− 1 for ssdb and n− k for ssdw , where k is the number of groups and n is the total number of observations."
676,1,"['mean', 'mean squares']", Oneway analysis of variance,seg_199,"accordingly, you can normalize the sums of squares by calculating mean squares:"
677,1,"['estimate', 'estimates', 'pooled variance', 'analysis of variance', 'variance', 'test', 'variances']", Oneway analysis of variance,seg_199,"msw is the pooled variance obtained by combining the individual group variances and thus an estimate of σ2. in the absence of a true group effect, msb will also be an estimate of σ2, but if there is a group effect, then the differences between group means and hence msb will tend to be larger. thus, a test for significant differences between the group means can be performed by comparing two variance estimates. this is why the procedure is called analysis of variance even though the objective is to compare the group means."
678,1,"['random variation', 'variation', 'mean squares', 'mean', 'random', 'test']", Oneway analysis of variance,seg_199,a formal test needs to account for the fact that random variation will cause some difference in the mean squares. you calculate
679,1,"['degrees of freedom', 'f distribution', 'quantile', 'distribution', 'variation', 'level', 'significance', 'test', 'significance level', 'null hypothesis', 'hypothesis']", Oneway analysis of variance,seg_199,"so that f is ideally 1, but some variation around that value is expected. the distribution of f under the null hypothesis is an f distribution with k− 1 and n − k degrees of freedom. you reject the hypothesis of identical means if f is larger than the 95% quantile in that f distribution (if the significance level is 5%). notice that this test is one-sided; a very small f would occur if the group means were very similar, and that will of course not signify a difference between the groups."
680,1,"['functions', 'linear', 'analyses of variance', 't test', 'regression analysis', 'regression', 'function', 'test', 'variance', 'variances']", Oneway analysis of variance,seg_199,"simple analyses of variance can be performed in r using the function lm, which is also used for regression analysis. for more elaborate analyses, there are also the functions aov and lme (linear mixed effects models, from the nlme package). an implementation of welch’s procedure, relaxing the assumption of equal variances and generalizing the unequal-variance t test, is implemented in oneway.test (see section 7.1.2)."
681,1,"['factor', 'data', 'variable', 'set', 'data set']", Oneway analysis of variance,seg_199,"the main example in this section is the “red cell folate” data from altman (1991, p. 208). to use lm, it is necessary to have the data values in one vector and a factor variable (see section 1.2.8) describing the division into groups. the red.cell.folate data set contains a data frame in the proper format."
682,1,"['factors', 'variables', 'data', 'distribution']", Oneway analysis of variance,seg_199,"recall that summary applied to a data frame gives a short summary of the distribution of each of the variables contained in it. the format of the summary is different for numeric vectors and factors, so that provides a check that the variables are defined correctly."
683,1,['mean'], Oneway analysis of variance,seg_199,"the category names for ventilation mean “n2o and o2 for 24 hours”, “n2o and o2 during operation”, and “only o2 for 24 hours”."
684,1,"['grouped data', 'data', 'tables', 'analysis of variance', 'variance']", Oneway analysis of variance,seg_199,"in the following, the analysis of variance is demonstrated first and then a couple of useful techniques for the presentation of grouped data as tables and graphs are shown."
685,1,"['model', 'table', 'factor', 'anova', 'variable', 'analysis of variance', 'variance', 'numeric variable']", Oneway analysis of variance,seg_199,the specification of a one-way analysis of variance is analogous to a regression analysis. the only difference is that the descriptive variable needs to be a factor and not a numeric variable. we calculate a model object using lm and extract the analysis of variance table with anova.
686,0,[], Oneway analysis of variance,seg_199,here we have ssdb and msb in the top line and ssdw and msw in the second line.
687,1,"['residual', 'range', 'factor', 'statistics', 'tables', 'variation', 'statistical', 'anova']", Oneway analysis of variance,seg_199,"in statistics textbooks, the sums of squares are most often labelled “be- tween groups” and “within groups”. like most other statistical software, r uses slightly different labelling. variation between groups is labelled by the name of the grouping factor (ventilation), and variation within groups is labelled residual. anova tables can be used for a wide range of statistical models, and it is convenient to use a format that is less linked to the particular problem of comparing groups."
688,1,"['factor', 'data', 'variable', 'set', 'data set', 'analysis of variance', 'variance', 'error']", Oneway analysis of variance,seg_199,"for a further example, consider the data set juul, introduced in section 4.1. notice that the tanner variable in this data set is a numeric vector and not a factor. for purposes of tabulation, this makes little difference, but it would be a serious error to use it in this form in an analysis of variance:"
689,1,"['linear', 'linear regression', 'data', 'regression']", Oneway analysis of variance,seg_199,this does not describe a grouping of data but a linear regression on the group number! notice the telltale 1 df for the effect of tanner.
690,0,[], Oneway analysis of variance,seg_199,things can be fixed as follows:
691,1,['data'], Oneway analysis of variance,seg_199,"we needed to reattach the juul data frame in order to use the changed definition. an attached data frame is effectively a separate copy of it (although it does not take up extra space as long as the original is unchanged). the df column now has an entry of 4 for tanner, as it should."
692,1,"['test', 'f test']", Pairwise comparisons and multiple testing,seg_201,"if the f test shows that there is a difference between groups, the question quickly arises of where the difference lies. it becomes necessary to compare the individual groups."
693,1,"['standard errors', 'regression coefficients', 'slope', 'errors', 'regression', 'information', 'standard', 'tests', 'regression line', 'coefficients']", Pairwise comparisons and multiple testing,seg_201,"part of this information can be found in the regression coefficients. you can use summary to extract regression coefficients with standard errors and t tests. these coefficients do not have their usual meaning as the slope of a regression line but have a special interpretation, which is described below."
694,1,"['estimates', 'mean', 'intercept']", Pairwise comparisons and multiple testing,seg_201,"the interpretation of the estimates is that the intercept is the mean in the first group (n2o+o2,24h), whereas the two others describe the difference between the relevant group and the first one."
695,1,"['factor', 'observations', 'representations', 'dummy variables', 'linear', 'multiple regression', 'model', 'linear model', 'multiple regression analysis', 'contrasts', 'regression', 'treatment contrasts', 'analysis of variance', 'variance', 'variables', 'treatment', 'regression analysis', 'variable']", Pairwise comparisons and multiple testing,seg_201,"there are multiple ways of representing the effect of a factor variable in linear models (and one-way analysis of variance is the simplest example of a linear model with a factor variable). the representations are in terms of contrasts, the choice of which can be controlled either by global options or as part of the model formula. we do not go deeply into this but just mention that the contrasts used by default are the so-called treatment contrasts, in which the first group is treated as a baseline and the other groups are given relative to that. concretely, the analysis is performed as a multiple regression analysis (see chapter 11) by introducing two dummy variables, which are 1 for observations in the relevant group and 0 elsewhere."
696,1,"['table', 'factor', 'mean', 'tests', 'test']", Pairwise comparisons and multiple testing,seg_201,"among the t tests in the table, you can immediately find a test for the hypothesis that the first two groups have the same true mean (p = 0.0139) and also whether the first and the third might be identical (p = 0.1548). however, a comparison of the last two groups cannot be found. this can be overcome by modifying the factor definition (see the help page for relevel), but that gets tedious when there are more than a few groups."
697,1,"['tests', 'bonferroni correction', 'level', 'probabilities', 'method', 'events', 'probability', 'event', 'significance', 'test', 'significance level', 'multiple testing']", Pairwise comparisons and multiple testing,seg_201,"if we want to compare all groups, we ought to correct for multiple testing. performing many tests will increase the probability of finding one of them to be significant; that is, the p-values tend to be exaggerated. a common adjustment method is the bonferroni correction, which is based on the fact that the probability of observing at least one of n events is less than the sum of the probabilities for each event. thus, by dividing the significance level by the number of tests or, equivalently, multiplying the p-values, we obtain a conservative test where the probability of a significant result is less than or equal to the formal significance level."
698,1,"['multiple comparisons', 'function']", Pairwise comparisons and multiple testing,seg_201,a function called pairwise.t.test computes all possible two-group comparisons. it is also capable of making adjustments for multiple comparisons and works like this:
699,1,"['method', 'table', 'results', 'pairwise comparisons', 'sets', 'adjusted']", Pairwise comparisons and multiple testing,seg_201,"the output is a table of p-values for the pairwise comparisons. here, the p-values have been adjusted by the bonferroni method, where the unadjusted values have been multiplied by the number of comparisons, namely 3. if that results in a value bigger than 1, then the adjustment procedure sets the adjusted p-value to 1."
700,1,"['bonferroni correction', 'method', 'tests']", Pairwise comparisons and multiple testing,seg_201,"the default method for pairwise.t.test is actually not the bonferroni correction but a variant due to holm. in this method, only the smallest p needs to be corrected by the full number of tests, the second smallest is corrected by n − 1, etc., unless that would make it smaller than the previous one, since the order of the p-values should be unaffected by the adjustment."
701,1,"['t test', 'function', 'test', 'anova']", Relaxing the variance assumption,seg_203,"the traditional one-way anova requires an assumption of equal variances for all groups. there is, however, an alternative procedure that does not require that assumption. it is due to welch and similar to the unequal-variances t test. this has been implemented in the oneway.test function:"
702,1,"['variance', 'case']", Relaxing the variance assumption,seg_203,"in this case, the p-value increased to a nonsignificant value, presumably related to the fact that the group that seems to differ from the two others also has the largest variance."
703,1,"['deviation', 'standard', 'pooled standard deviation', 'standard deviation', 'tests']", Relaxing the variance assumption,seg_203,it is also possible to perform the pairwise t tests so that they do not use a common pooled standard deviation. this is controlled by the argument pool.sd.
704,1,"['significance', 'variances']", Relaxing the variance assumption,seg_203,"again, it is seen that the significance disappears as we remove the constraint on the variances."
705,1,"['plot', 'stripchart', 'grouped data', 'data']", Graphical presentation,seg_205,"of course, there are many ways to present grouped data. here we create a somewhat elaborate plot where the raw data are plotted as a stripchart and overlaid with an indication of means and sems (figure 7.1):"
706,1,"['plotting', 'stripchart']", Graphical presentation,seg_205,here we used pch=16 (small plotting dots) in stripchart and put vertical=t to make the “strips” vertical.
707,1,"['plot', 'error bars', 'stripcharts', 'set', 'error']", Graphical presentation,seg_205,"the error bars have been made with arrows, which adds arrows to a plot. we slightly abuse the fact that the angle of the arrowhead is adjustable to create the little crossbars at either end. the first four arguments specify the endpoints, (x1, y1, x2, y2); the angle argument gives the angle between the lines of the arrowhead and shaft, here set to 90◦; and length is the length of the arrowhead (in inches on a printout). finally, code=3 means that the arrow should have a head at both ends. note that the x-coordinates of the stripcharts are simply the group numbers."
708,0,[], Graphical presentation,seg_205,"the indication of averages and the connecting lines are done with lines, where type=""b"" (both) means that both points and lines are printed, leaving gaps in the lines to make room for the symbols. pch=4 is a cross, and cex=2 requests that the symbols be drawn in double size."
709,1,"['plot', 'confidence intervals', 'errors', 'intervals', 'mean', 'standard', 'confidence', 'standard errors', 'distributions']", Graphical presentation,seg_205,"it is debatable whether you should draw the plot using 1 sem as is done here or whether perhaps it is better to draw proper confidence intervals for the means (approximately 2 sem), or maybe even sd instead of sem. the latter point has to do with whether the plot is to be used in a descriptive or an analytical manner. standard errors of the mean are not useful for describing the distributions in the groups; they only say how precisely the mean is determined. on the other hand, sds do not enable the reader to see at a glance which groups are significantly different."
710,0,[], Graphical presentation,seg_205,"in many fields it appears to have become the tradition to use 1 sem “because they are the smallest”; that is, it makes differences look more dramatic. probably, the best thing to do is to follow the traditions in the relevant field and “calibrate your eyeballs” accordingly."
711,1,"['confidence interval', 'interval', 'dependent', 'data', 'distribution', 'set', 'normal', 'data set', 'confidence', 'normal distribution']", Graphical presentation,seg_205,"one word of warning, though: at small group sizes, the rule of thumb that the confidence interval is the mean± 2 sem becomes badly misleading. at a group size of 2, it actually has to be 12.7 sem! that is a correction heavily dependent on data having the normal distribution. if you have such small groups, it may be advisable to use a pooled sd for the entire data set rather than the group-specific sds. this does, of course, require"
712,1,"['deviation', 'standard deviation', 'standard']", Graphical presentation,seg_205,that you can reasonably assume that the true standard deviation actually is the same in all groups.
713,1,"['normal distributions', 'independent', 'variances', 'f test', 'data', 'distribution', 'variable', 'normal', 'test', 'variance', 'distributions']", Bartletts test,seg_207,"testing whether the distribution of a variable has the same variance in all groups can be done using bartlett’s test, although like the f test for comparing two variances, it is rather nonrobust against departures from the assumption of normal distributions. as in var.test, it is assumed that the data are from independent groups. the procedure is performed as follows:"
714,1,"['case', 'data', 'variances']", Bartletts test,seg_207,"that is, in this case, nothing in the data contradicts the assumption of equal variances in the three groups."
715,1,"['test statistic', 'data', 'sampling', 'sum of squares', 'distribution', 'statistic', 'set', 'analysis of variance', 'variance', 'test', 'average', 'hypothesis']", KruskalWallis test,seg_209,"a nonparametric counterpart of a one-way analysis of variance is the kruskal–wallis test. as in the wilcoxon two-sample test (see section 5.5), data are replaced with their ranks without regard to the grouping, only this time the test is based on the between-group sum of squares calculated from the average ranks. again, the distribution of the test statistic can be worked out based on the idea that, under the hypothesis of irrelevant grouping, the problem reduces to a combinatorial one of sampling the within-group ranks from a fixed set of numbers."
716,1,['test'], KruskalWallis test,seg_209,you can make r calculate the kruskal–wallis test as follows:
717,1,"['test', 'variance', 'f test']", KruskalWallis test,seg_209,"it is seen that there is no significant difference using this test. this should not be too surprising in view of the fact that the f test in the one-way analysis of variance was only borderline significant. also, the kruskal–wallis"
718,1,['efficient'], KruskalWallis test,seg_209,"test is less efficient than its parametric counterpart if the assumptions hold, although it does not invariably give a larger p-value."
719,1,"['condition', 'table', 'design', 'data', 'statistical', 'analysis of variance', 'variance']", Twoway analysis of variance,seg_211,"one-way analysis of variance deals with one-way classifications of data. it is also possible to analyze data that are cross-classified according to several criteria. when a cross-classified design is balanced, then you can almost read the entire statistical analysis from a single analysis of variance table, and that table generally consists of items that are simple to compute, which was very important before the computer era. balancedness is a concept that is hard to define exactly; for a two-way classification, a sufficient condition is that the cell counts be equal, but there are other balanced designs."
720,1,"['experimental', 't test', 'case', 'observation', 'measurements', 'experimental unit', 'test', 'paired t test', 'paired']", Twoway analysis of variance,seg_211,here we restrict ourselves to the case of a single observation per cell. this typically arises from having multiple measurements on the same experimental unit and in this sense generalizes the paired t test.
721,1,"['table', 'observations', 'observation', 'analysis of variance', 'variance']", Twoway analysis of variance,seg_211,"let xij denote the observation in row i and column j of the m×n table. this is similar to the notation used for one-way analysis of variance, but notice that there is now a connection between observations with the same j, so that it makes sense to look at both row averages x̄i· and column averages x̄·j."
722,1,['variation'], Twoway analysis of variance,seg_211,"consequently, it now makes sense to look at both variation between rows"
723,1,['variation'], Twoway analysis of variance,seg_211,and variation between columns
724,1,"['residual', 'variation', 'residual variation']", Twoway analysis of variance,seg_211,"subtracting these two from the total variation leaves the residual variation, which works out as"
725,1,"['model', 'level', 'observations', 'statistical model', 'statistical']", Twoway analysis of variance,seg_211,"this corresponds to a statistical model in which it is assumed that the observations are composed of a general level, a row effect, and a column effect plus a noise term:"
726,1,"['model', 'parameters']", Twoway analysis of variance,seg_211,"the parameters of this model are not uniquely defined unless we impose some restriction on the parameters. if we impose αi = 0 and β j = 0,"
727,1,['estimates'], Twoway analysis of variance,seg_211,"∑ ∑ then the estimates of αi, β j, and µ turn out to be x̄i· − x̄.., x̄·j − x̄.., and x̄..."
728,1,"['degrees of freedom', 'residual', 'set', 'mean square', 'mean squares', 'mean', 'tests']", Twoway analysis of variance,seg_211,"dividing the sums of squares by their respective degrees of freedom m− 1 for ssdr, n − 1 for ssdc, and (m − 1)(n − 1) for ssdres, we get a set of mean squares. f tests for no row and column effect can be carried out by dividing the respective mean squares by the residual mean square."
729,1,"['table', 'design', 'sum of squares', 'independence']", Twoway analysis of variance,seg_211,"it is important to notice that this works out so nicely only because of the balanced design. if you have a table with “holes” in it, the analysis is considerably more complicated. the simple formulas for the sum of squares are no longer valid and, in particular, the order independence is lost, so that there is no longer a single ssdc but ones with and without adjusting for row effects."
730,1,"['rate', 'factors', 'data', 'set', 'data set', 'anova']", Twoway analysis of variance,seg_211,"to perform a two-way anova, it is necessary to have data in one vector, with the two classifying factors parallel to it. we consider an example concerning heart rate after administration of enalaprilate (altman, 1991, p. 327). data are found in this form in the heart.rate data set:"
731,1,['data'], Twoway analysis of variance,seg_211,"if you look inside the heart.rate.r file in the data directory of the iswr package, you will see that the actual definition of the data frame is"
732,1,"['level', 'levels', 'factors', 'experimental', 'data', 'function']", Twoway analysis of variance,seg_211,"the gl (generate levels) function is specially designed for generating patterned factors for balanced experimental designs. it has three arguments: the number of levels, the block length (how many times each level should repeat), and the total length of the result. the two patterns in the data frame are thus"
733,1,"['variables', 'variance', 'analysis of variance']", Twoway analysis of variance,seg_211,"once the variables have been defined, the two-way analysis of variance is specified simply by"
734,1,"['missing values', 'model', 'table', 'factor', 'design', 'anova table', 'cases', 'anova']", Twoway analysis of variance,seg_211,"interchanging subj and time in the model formula (hr~time+subj) yields exactly the same analysis except for the order of the rows of the anova table. this is because we are dealing with a balanced design (a complete two-way table with no missing values). in unbalanced cases, the factor order will matter."
735,1,"['plot', 'factor', 'data', 'function']", Graphics for repeated measurements,seg_213,"at least for your own use, it is useful to plot a “spaghettigram” of the data; that is, a plot where data from the same subject are connected with lines. to this end, you can use the function interaction.plot, which graphs the values against one factor while connecting data for the other factor with line segments to form traces."
736,1,"['mean', 'case', 'observation']", Graphics for repeated measurements,seg_213,"in fact there is a fourth argument, which specifies what should be done in case, there is more than one observation per cell. by default, the mean is taken, which is the reason why the y-axis in figure 7.2 reads “mean of hr”."
737,1,['plot'], Graphics for repeated measurements,seg_213,"if you prefer to have the values plotted according to the times of measurement (which are not equidistant in this example), you could instead write (resulting plot not shown)"
738,1,"['normalized', 'test statistic', 'observations', 'case', 'observation', 'sum of squares', 'statistic', 'analysis of variance', 'variance', 'test']", The Friedman test,seg_215,a nonparametric counterpart of two-way analysis of variance exists for the case with one observation per cell. friedman’s test is based on ranking observations within each row assuming that if there is no column effect then all orderings should be equally likely. a test statistic based on the column sum of squares can be calculated and normalized to give a χ2-distributed test statistic.
739,1,"['case', 'distribution', 'sign test', 'binomial', 'binomial distribution', 'test']", The Friedman test,seg_215,"in the case of two columns, the friedman test is equivalent to the sign test, in which one uses the binomial distribution to test for equal probabilities of positive and negative differences within pairs. this is a rather less sensitive test than the wilcoxon signed-rank test discussed in section 5.2."
740,1,['test'], The Friedman test,seg_215,practical application of the friedman test is as follows:
741,1,"['factor', 'model', 'test']", The Friedman test,seg_215,"notice that the blocking factor is specified in a model formula using the vertical bar, which may be read as “time within subj”. it is seen that the test is not quite as strongly significant as the parametric counterpart. this is unsurprising since the latter test is more powerful when its assumptions are met."
742,1,"['linear', 'experimental', 'tables', 'analysis of variance', 'variance']", The ANOVA table in regression analysis,seg_217,"we have seen the use of analysis of variance tables in grouped and crossclassified experimental designs. however, their use is not restricted to these designs but applies to the whole class of linear models (more on this in chapter 12)."
743,1,"['model', 'residual', 'variation', 'residual variation', 'analysis of variance', 'variance']", The ANOVA table in regression analysis,seg_217,the variation between and within groups for a one-way analysis of variance generalizes to model variation and residual variation
744,1,"['linear', 'model', 'linear model', 'intercept', 'fitted values', 'variation']", The ANOVA table in regression analysis,seg_217,which partition the total variation ∑i(yi− ȳ.)2. this applies only when the model contains an intercept; see section 12.2. the role of the group means in the one-way classification is taken over by the fitted values ŷi in the more general linear model.
745,1,"['simple linear regression', 'linear', 'model', 'f test', 'regression', 'regression coefficient', 'linear regression', 'coefficient', 'significance', 'test']", The ANOVA table in regression analysis,seg_217,"an f test for significance of the model is available in direct analogy with section 7.1. in simple linear regression, this test is equivalent to testing that the regression coefficient is zero."
746,1,"['table', 'analyses of variance', 'regression analysis', 'regression', 'analysis of variance', 'function', 'variance', 'anova']", The ANOVA table in regression analysis,seg_217,"the analysis of variance table corresponding to a regression analysis can be extracted with the function anova, just as for oneand two-way analyses of variance. for the thuesen example, it will look like this:"
747,1,"['f test', 't test', 'test', 'slope']", The ANOVA table in regression analysis,seg_217,notice that the f test gives the same p-value as the t test for a zero slope from section 6.1. it is the same f test that gets printed at the end of the summary output:
748,1,"['table', 'anova table', 'standard', 'anova']", The ANOVA table in regression analysis,seg_217,the remaining elements of the three output lines above may also be derived from the anova table. “residual standard error” is the square root
749,1,"['residual', 'regression', 'sum of squares', 'adjusted', 'adjusted r2', 'mean', 'variance', 'regression line', 'residual variance']", The ANOVA table in regression analysis,seg_217,"of “residual mean squares”, namely 0.2167 = √0.04696. r2 is the proportion of the total sum of squares explained by the regression line, 0.1737 = 0.2073/(0.2073 + 0.9861); and, finally, the adjusted r2 is the relative improvement of the residual variance, 0.1343 = (v − 0.04696)/v, where v = (0.2073 + 0.9861)/22 = 0.05425 is the variance of short.velocity if the glucose values are not taken into account."
750,1,"['tests', 'test', 'data']", Exercises,seg_219,"7.1 the zelazo data are in the form of a list of vectors, one for each of the four groups. convert the data to a form suitable for the use of lm, and calculate the relevant test. consider t tests comparing selected subgroups or obtained by combining groups."
751,1,"['data', 'results', 'measurement']", Exercises,seg_219,"7.2 in the lung data, do the three measurement methods give systematically different results? if so, which ones appear to be different?"
752,1,"['tests', 'nonparametric tests', 'data']", Exercises,seg_219,7.3 repeat the previous exercises using the zelazo and lung data with the relevant nonparametric tests.
753,1,"['skewed', 'data', 'transformations', 'variable', 'set', 'data set', 'test', 'variances']", Exercises,seg_219,"7.4 the igf1 variable in the juul data set is arguably skewed and has different variances across tanner groups. try to compensate for this using logarithmic and square-root transformations, and use the welch test. however, the analysis is still problematic — why?"
754,1,"['functions', 'data']", Tabular data,seg_221,"this chapter describes a series of functions designed to analyze tabular data. specifically, we look at the functions prop.test, binom.test, chisq.test, and fisher.test."
755,1,"['sample', 'approximation', 'normal', 'mean', 'binomial', 'probability', 'parameter', 'variance']", Single proportions,seg_223,"tests of single proportions are generally based on the binomial distribution (see section 3.3) with size parameter n and probability parameter p. for large sample sizes, this can be well approximated by a normal distribution with mean np and variance np(1 − p). as a rule of thumb, the approximation is satisfactory when the expected numbers of “successes” and “failures” are both larger than 5."
756,1,"['test', 'hypothesis']", Single proportions,seg_223,"denoting the observed number of “successes” by x, the test for the hypothesis that p = p0 can be based on"
757,1,"['deviation', 'distribution', 'degree of freedom', 'normal', 'mean', 'normal distribution']", Single proportions,seg_223,"which has an approximate normal distribution with mean zero and standard deviation 1, or on u2, which has an approximate χ2 distribution with 1 degree of freedom."
758,1,"['expected value', 'approximation', 'normal', 'normal approximation']", Single proportions,seg_223,"the normal approximation can be somewhat improved by the yates correction, which shrinks the observed value by half a unit towards the expected value when calculating u."
759,1,"['probability', 'test', 'hypothesis']", Single proportions,seg_223,"we consider an example (altman, 1991, p. 230) where 39 of 215 randomly chosen patients are observed to have asthma and one wants to test the hypothesis that the probability of a “random patient” having asthma is 0.15. this can be done using prop.test:"
760,1,"['interval', 'case', 'distribution', 'binomial', 'probability', 'parameter', 'binomial distribution', 'confidence', 'test', 'outcomes', 'confidence interval']", Single proportions,seg_223,"the three arguments to prop.test are the number of positive outcomes, the total number, and the (theoretical) probability parameter that you want to test for. the latter is 0.5 by default, which makes sense for symmetrical problems, but this is not the case here. the amount 15% is a bit synthetic since it is rarely the case that one has a specific a priori value to test for. it is usually more interesting to compute a confidence interval for the probability parameter, such as is given in the last part of the output. notice that we have a slightly unfortunate double usage of the symbol p as the probability parameter of the binomial distribution and as the test probability or p-value."
761,1,"['probabilities', 'distribution', 'binomial', 'probability', 'binomial distribution', 'test']", Single proportions,seg_223,"you can also use binom.test to obtain a test in the binomial distribution. in that way, you get an exact test probability, so it is generally preferable to using prop.test, but prop.test can do more than testing single proportions. the procedure to obtain the p-value is to calculate the point probabilities for all the possible values of x and sum those that are less than or equal to the point probability of the observed x."
762,1,"['tests', 'confidence intervals', 'interval', 'intervals', 'level', 'confidence', 'confidence interval']", Single proportions,seg_223,the “exact” confidence intervals at the 0.05 level are actually constructed from the two one-sided tests at the 0.025 level. finding an exact confidence interval using two-sided tests is not a well-defined problem (see exercise 8.5).
763,1,"['function', 'outcomes']", Two independent proportions,seg_225,"the function prop.test can also be used to compare two or more proportions. for that purpose, the arguments should be given as two vectors, where the first contains the number of positive outcomes and the second the total number for each group."
764,1,"['estimate', 'normally distributed', 'mean', 'variance', 'parameter', 'test', 'hypothesis']", Two independent proportions,seg_225,"the theory is similar to that for a single proportion. consider the difference in the two proportions d = x1/n1 − x2/n2, which will be approximately normally distributed with mean zero and variance vp(d) = (1/n1 + 1/n2) × p(1 − p) if the counts are binomially distributed with the same p parameter. so to test the hypothesis that p1 = p2, plug the common estimate p̂ = (x1 + x2)/(n1 + n2) into the variance formula and"
765,1,"['standard normal', 'normal', 'standard']", Two independent proportions,seg_225,"vp̂(d), which approximately follows a standard normal"
766,0,[], Two independent proportions,seg_225,"distribution, or look at u2, which is approximately χ2(1)-distributed. a yates-type correction is possible, but we skip the details."
767,0,[], Two independent proportions,seg_225,"for illustration, we use an example originally due to lewitt and machin (altman, 1991, p. 232):"
768,1,"['interval', 'approximation', 'confidence', 'test', 'confidence interval']", Two independent proportions,seg_225,"the confidence interval given is for the difference in proportions. the theory behind its calculation is similar to that of the test, but there are some technical complications, and a different approximation is used."
769,1,"['interval', 'tables', 'yates continuity correction', 'continuity correction', 'confidence', 'test', 'confidence interval']", Two independent proportions,seg_225,"you can also perform the test without the yates continuity correction. this is done by adding the argument correct=f. the continuity correction makes the confidence interval somewhat wider than it would otherwise be, but notice that it nevertheless does not contain zero. thus, the confidence interval is contradicting the test, which says that there is no significant difference between the two groups with a two-sided test. the explanation lies in the different approximations, which becomes important for tables as sparse as the present one."
770,1,"['sample', 'table', 'without replacement', 'failure', 'conditional', 'replacement', 'distribution', 'data', 'success', 'hypergeometric', 'test', 'hypergeometric distribution', 'conditional distribution']", Two independent proportions,seg_225,"if you want to be sure that at least the p-value is correct, you can use fisher’s exact test. we illustrate this using the same data as in the preceding section. the test works by making the calculations in the conditional distribution of the 2× 2 table given both the row and column marginals. this can be difficult to envision, but think of it like this: take 13 white balls and 12 black balls (success and failure, respectively), and sample the balls without replacement into two groups of sizes 12 and 13. the number of white balls in the first group obviously defines the whole table, and the point is that its distribution can be found as a purely combinatorial problem. the distribution is known as the hypergeometric distribution."
771,1,"['data', 'function']", Two independent proportions,seg_225,"the relevant function is fisher.test, which requires that data be given in matrix form. this is obtained as follows:"
772,1,"['table', 'observations', 'number of observations', 'outcomes']", Two independent proportions,seg_225,"notice that the second column of the table needs to be the number of negative outcomes, not the total number of observations."
773,1,"['confidence interval', 'odds ratio', 'estimate', 'interval', 'association', 'table', 'conditional', 'intervals', 'distribution', 'fisher', 'confidence', 'test', 'conditional distribution']", Two independent proportions,seg_225,"notice also that the confidence interval is for the odds ratio; that is, for the estimate of (p1/(1− p1))/(p2/(1− p2)). one can show that if the ps are not identical, then the conditional distribution of the table depends only on the odds ratio, so it is the natural measure of association in connection with the fisher test. the exact distribution of the test can be worked out also when the odds ratio differs from 1, but there is the same complication as with binom.test that a two-sided 95% confidence interval must be pasted together from two one-sided 97.5% intervals. this leads to the opposite inconsistency as with prop.test: the test is (barely) significant, but the confidence interval for the odds ratio includes 1."
774,1,"['table', 'data', 'χ2 test', 'standard', 'test']", Two independent proportions,seg_225,"the standard χ2 test (see also section 8.4) in chisq.test works with data in matrix form, like fisher.test does. for a 2× 2 table, the test is exactly equivalent to prop.test."
775,1,"['categories', 'case']", k proportions test for trend,seg_227,"sometimes you want to compare more than two proportions. in that case, the categories are often ordered so that you would expect to find a decreasing or increasing trend in the proportions with the group number."
776,1,['data'], k proportions test for trend,seg_227,"the example used in this section concerns data from a group of women giving birth where it was recorded whether the child was delivered by caesarean section and what shoe size the mother used (altman, 1991, p. 229)."
777,1,['table'], k proportions test for trend,seg_227,the table looks like this:
778,1,"['degrees of freedom', 'test statistic', 'sum of squared', 'distribution', 'deviations', 'statistic', 'normal', 'test']", k proportions test for trend,seg_227,"to compare k > 2 proportions, another test based on the normal approximation is available. it consists of the calculation of a weighted sum of squared deviations between the observed proportions in each group and the overall proportion for all groups. the test statistic has an approximate χ2 distribution with k− 1 degrees of freedom."
779,1,"['case', 'table']", k proportions test for trend,seg_227,"to use prop.test on a table like caesar.shoe, we need to convert it to a vector of “successes” (which in this case is close to being the opposite) and a vector of “trials”. the two vectors can be computed like this:"
780,1,['test'], k proportions test for trend,seg_227,thereafter it is easy to perform the test:
781,1,"['approximation', 'test']", k proportions test for trend,seg_227,"it is seen that the test comes out nonsignificant, but the subdivision is really unreasonably fine in view of the small number of caesarean sections. notice, by the way, the warning about the χ2 approximation being dubious, which is prompted by some cells having an expected count less than 5."
782,1,"['linear', 'regression', 'χ2 test', 'degree of freedom', 'linear regression', 'scores', 'test', 'slope']", k proportions test for trend,seg_227,"you can test for a trend in the proportions using prop.trend.test. it takes three arguments: x, n, and score. the first two of these are exactly as in prop.test, whereas the last one is the score given to the groups, by default simply 1, 2, . . . , k. the basis of the test is essentially a weighted linear regression of the proportions on the group scores, where we test for a zero slope, which becomes a χ2 test on 1 degree of freedom."
783,1,"['linear', 'test']", k proportions test for trend,seg_227,"so if we assume that the effect of shoe size is linear in the group score, then we can see a significant difference. this kind of assumption should not be thought of as something that must hold for the test to be valid. rather, it indicates the rough type of alternative to which the test should be sensitive."
784,1,"['degrees of freedom', 'linear', 'deviations', 'degree of freedom', 'test']", k proportions test for trend,seg_227,the effect of using a trend test can be viewed as an approximate subdivision of the test for equal proportions (χ2 = 9.29) into a contribution from the linear effect (χ2 = 8.02) on 1 degree of freedom and a contribution from deviations from the linear trend (χ2 = 1.27) on 4 degrees of freedom. so you could say that the test for equal proportions is being diluted or wastes degrees of freedom on testing for deviations in a direction we are not really interested in.
785,1,['tables'], r c tables,seg_229,"for the analysis of tables with more than two classes on both sides, you can use chisq.test or fisher.test, although you should note that the latter can be very computationally demanding if the cell counts are large and there are more than two rows or columns. we have already seen chisq.test in a simple example, but with larger tables, some additional features are of interest."
786,1,['table'], r c tables,seg_229,an r× c table looks like this:
787,1,"['column totals', 'probabilities', 'marginal', 'table', 'case', 'sampling', 'distribution', 'statistical', 'cases', 'probability', 'marginal probabilities', 'hypothesis']", r c tables,seg_229,"such a table can arise from several different sampling plans, and the notion of “no relation between rows and columns” is correspondingly different. the total in each row might be fixed in advance, and you would be interested in testing whether the distribution over columns is the same for each row, or vice versa if the column totals were fixed. it might also be the case that only the total number is chosen and the individuals are grouped randomly according to the row and column criteria. in the latter case, you would be interested in testing the hypothesis of statistical independence, that the probability of an individual falling into the ijth cell is the product pi·p·j of the marginal probabilities. however, the analysis of the table turns out to be the same in all cases."
788,0,[], r c tables,seg_229,"if there is no relation between rows and columns, then you would expect to have the following cell values:"
789,1,['row total'], r c tables,seg_229,this can be interpreted as distributing each row total according to the proportions in each column (or vice versa) or as distributing the grand total according to the products of the row and column proportions.
790,1,"['test statistic', 'statistic', 'test']", r c tables,seg_229,the test statistic
791,1,"['distribution', 'expected values', 'table']", r c tables,seg_229,has an approximate χ2 distribution with (r− 1)× (c− 1) degrees of freedom. here the sum is over the entire table and the ij indices have been omitted. o denotes the observed values and e the expected values as described above.
792,1,"['χ2 test', 'test', 'table']", r c tables,seg_229,we consider the table with caffeine consumption and marital status from section 4.5 and compute the χ2 test:
793,1,"['data', 'independence', 'deviations', 'test', 'hypothesis']", r c tables,seg_229,"the test is highly significant, so we can safely conclude that the data contradict the hypothesis of independence. however, you would generally also like to know the nature of the deviations. to that end, you can look at some extra components of the return value of chisq.test."
794,1,['information'], r c tables,seg_229,notice that chisq.test (just like lm) actually returns more information than what is commonly printed:
795,1,"['tables', 'table']", r c tables,seg_229,"these two tables may then be scrutinized to see where the differences lie. it is often useful to look at a table of the contributions from each cell to the total χ2. such a table cannot be directly extracted, but it is easy to calculate:"
796,1,"['deviation', 'data', 'distribution', 'independence']", r c tables,seg_229,"there are some large contributions, particularly from too many “abstain- ing” singles, and the distribution among previously married is shifted in the direction of a larger intake — insofar as they consume caffeine at all. still, it is not easy to find a simple description of the deviation from independence in these data."
797,1,"['data', 'set', 'data set']", r c tables,seg_229,"you can also use chisq.test directly on raw (untabulated) data, here using the juul data set from section 4.5:"
798,1,"['variables', 'independence', 'test']", r c tables,seg_229,it may not really be relevant to test for independence between these particular variables. the definition of tanner stages is gender-dependent by nature.
799,1,['rate'], Exercises,seg_231,"8.1 reconsider the situation of exercise 3.3, where 10 consecutive patients had operations without complications and the expected rate was"
800,1,"['sample', 'distribution', 'statistical', 'binomial', 'statistical significance', 'binomial distribution', 'significance', 'test']", Exercises,seg_231,20%. calculate the relevant one-sided test in the binomial distribution. how large a sample (still with zero complications) would be necessary to obtain statistical significance?
801,1,"['states', 'statistically significant', 'cases']", Exercises,seg_231,"8.2 in 747 cases of “rocky mountain spotted fever” from the western united states, 210 patients died. out of 661 cases from the eastern united states, 122 died. is the difference statistically significant? (see also exercise 13.4.)"
802,1,"['treatment', 'results']", Exercises,seg_231,"8.3 two drugs for the treatment of peptic ulcer were compared (campbell and machin, 1993, p. 72). the results were as follows:"
803,0,[], Exercises,seg_231,healed not healed total pirenzepine 23 7 30 trithiozine 18 13 31 total 41 20 61
804,1,"['interval', 'χ2 test', 'probability', 'confidence', 'test', 'confidence interval']", Exercises,seg_231,compute the χ2 test and fisher’s exact test and discuss the difference. find an approximate 95% confidence interval for the difference in healing probability.
805,1,['results'], Exercises,seg_231,"8.4 (from “mathematics 5” exam, university of copenhagen, summer 1969.) from september 20, 1968, to february 1, 1969, an instructor consumed 254 eggs. every day, he recorded how many eggs broke during boiling so that the white ran out and how many cracked so that the white did not run out. additionally, he recorded whether the eggs were size a or size b. from february 4, 1969, until april 10, 1969, he consumed 130 eggs, but this time he used a “piercer” to create a small hole in the egg to prevent breaking and cracking. the results were as follows:"
806,0,[], Exercises,seg_231,investigate whether or not the piercer seems to have had an effect.
807,1,"['plot', 'interval', 'observations', 'successes', 'trials', 'confidence', 'probability', 'parameter', 'varying', 'confidence interval']", Exercises,seg_231,8.5 make a plot of the two-sided p-value for testing that the probability parameter is x when the observations are 3 successes in 15 trials for x varying from 0 to 1 in steps of 0.001. explain what makes the definition of a two-sided confidence interval difficult.
808,1,"['sample size', 'sample', 'tests', 'data', 'cases', 'statistical', 'statistical test', 'experiments', 'test']", Power and the computation of sample size,seg_233,"a statistical test will not be able to detect a true difference if the sample size is too small compared with the magnitude of the difference. when designing experiments, the experimenter should try to ensure that a sufficient amount of data are collected to be reasonably sure that a difference of a specified size will be detected. r has methods for doing these calculations in the simple cases of comparing means using oneor two-sample t tests and comparing two proportions."
809,1,"['sample size', 'sample']", The principles of power calculations,seg_235,"this section outlines the theory of power calculations and sample-size choice. if you are practically inclined and just need to find the necessary sample size in a particular situation, you can safely skim this section and move quickly to subsequent sections that contain the actual r calls."
810,1,"['rejection regions', 'test statistic', 'hypothesis test', 'rejection region', 'statistic', 'set', 'probability', 'level', 'significance', 'test', 'significance level', 'null hypothesis', 'hypothesis']", The principles of power calculations,seg_235,"the basic idea of a hypothesis test should be clear by now. a test statistic is defined, and its value is used to decide whether or not you can accept the (null) hypothesis. acceptance and rejection regions are set up so that the probability of getting a test statistic that falls into the rejection region is a specified significance level (α) if the null hypothesis is true. in the present context, it is useful to stick to this formulation (as opposed to the use of p-values), as rigid as it might be."
811,1,"['risk', 'data', 'random']", The principles of power calculations,seg_235,"since data are sampled at random, there is always a risk of reaching a wrong conclusion, and things can go wrong in two ways:"
812,1,"['test', 'type i error', 'error', 'hypothesis']", The principles of power calculations,seg_235,"• the hypothesis is correct, but the test rejects it (type i error)."
813,1,"['type ii error', 'type ii', 'test', 'error', 'hypothesis']", The principles of power calculations,seg_235,"• the hypothesis is wrong, but the test accepts it (type ii error)."
814,1,"['type ii error', 'deviation', 'risk', 'statisticians', 'type i error', 'type ii', 'level', 'significance', 'significance level', 'error']", The principles of power calculations,seg_235,"the risk of a type i error is the significance level. the risk of a type ii error will depend on the size and nature of the deviation you are trying to detect. if there is very little difference, then you do not have much of a chance of detecting it. for this reason, some statisticians disapprove of terms like “acceptance region” because you can never prove that there is no difference — you can only fail to prove that there is one."
815,1,"['power of the test', 'probability', 'test', 'hypothesis']", The principles of power calculations,seg_235,"the probability of rejecting a false hypothesis is called the power of the test, and methods exist for calculating or approximating the power in the most important practical situations. it is inconvenient to talk further about these matters in the abstract, so let us move on to some concrete examples."
816,1,"['sample', 'sample mean', 'treatment', 't test', 'case', 'mean', 'test', 'trial', 'paired t test', 'paired']", Power of onesample and paired t tests,seg_237,"consider the case of the comparison of a sample mean to a given value. for example, in a matched trial we wish to test whether the difference between treatment a and treatment b is zero using a paired t test (described in chapter 5)."
817,1,"['degrees of freedom', 't distribution', 'statistic', 'function', 'null hypothesis', 'paired', 'sample', 't test', 'parameter', 'standard', 'standard deviation', 'sample size', 'model', 'test statistic', 'distribution', 'noncentrality parameter', 'noncentral', 'test', 'paired t test', 'hypothesis', 'deviation', 'noncentrality']", Power of onesample and paired t tests,seg_237,"we call the true difference δ. even if the null hypothesis is not true, we can still work out the distribution of the test statistic, provided the other model assumptions hold. it is called the noncentral t distribution and depends on a noncentrality parameter as well as the usual degrees of freedom. for the paired t test, the noncentrality parameter ν is a function of δ, the standard deviation of differences σ, and the sample size n and equals"
818,1,"['standard error', 'error', 'mean', 'standard']", Power of onesample and paired t tests,seg_237,"that is, it is simply the true difference divided by the standard error of the mean."
819,1,"['plot', 'level', 'acceptance region', 't distribution', 'distribution', 'function', 'significance', 'test', 'significance level', 'noncentral']", Power of onesample and paired t tests,seg_237,the cumulative noncentral t distribution is available in r simply by adding an ncp argument to the pt function. figure 9.1 shows a plot of pt with ncp=3 and df=25. a vertical line indicates the upper end of the acceptance region for a two-sided test at the 0.05 significance level. the plot was created as follows:
820,1,"['plot', 'curve', 'acceptance region', 'intersection', 'distribution', 'rejection region', 'probability']", Power of onesample and paired t tests,seg_237,the plot shows the main part of the distribution falling in the rejection region. the probability of getting a value in the acceptance region can be seen from the graph as the intersection between the curve and the vertical line. (almost! see exercise 9.4.) this value is easily calculated as
821,1,"['test', 'case', 'probability', 'power of the test']", Power of onesample and paired t tests,seg_237,"or roughly 0.18. the power of the test is the opposite, the probability of getting a significant result. in this case it is 0.82, and it is of course desirable to have the power as close to 1 as possible."
822,1,"['deviation', 'experiment', 'standard', 'level', 'standard deviation', 'significance', 'significance level']", Power of onesample and paired t tests,seg_237,"notice that the power (traditionally denoted β) depends on four quantities: δ, σ, n, and α. if we fix any three of these, we can adjust the fourth to achieve a given power. this can be used to determine the necessary sample size for an experiment: you need to specify a desired power (β = 0.80 and β = 0.90 are common choices), the significance level (usually given by convention as α = 0.05), a guess of the standard deviation, and δ, which is known as the “minimal relevant difference” (miredif) or “smallest meaningful difference” (smd). this gives an equation that you can solve"
823,0,['n'], Power of onesample and paired t tests,seg_237,"for n. the result will generally be a fractional number, which should of course be rounded up."
824,1,"['sample size', 'sample']", Power of onesample and paired t tests,seg_237,"you can also work on the opposite problem and answer the following question: given a feasible sample size, how large a difference should you reasonably be able to detect?"
825,1,"['deviation', 'case', 'set', 'standard', 'standard deviation']", Power of onesample and paired t tests,seg_237,"sometimes a shortcut is made by expressing δ relative to the standard deviation, in which case you would simply set σ to 1."
826,1,"['case', 'noncentrality', 'parameter', 'noncentrality parameter', 'tests']", Power of twosample t test,seg_239,"procedures for two-sample t tests are essentially the same as for the onesample case, except for the calculation of the noncentrality parameter, which is calculated as"
827,1,"['observations', 'number of observations', 'variance']", Power of twosample t test,seg_239,"it is generally assumed that the variance is the same in the two groups; that is, using the welch procedure is not considered. in sample-size calculations, one usually assumes that the group sizes are the same since that gives the optimal power for a given total number of observations."
828,1,"['deviation', 'standard normal distribution', 'standard normal', 't test', 'distribution', 'cases', 'normal', 'standard', 'standard deviation', 'test', 'normal distribution']", Approximate methods,seg_241,"for hand calculations, the power calculations can be considerably simplified by assuming that the standard deviation is known, so that the t test is replaced by a test in the standard normal distribution. the practical advantage is that the approximate formula for the power is easily inverted to give an explicit formula for n. for the oneand two-sample cases, this works out as"
829,0,[], Approximate methods,seg_241,"two-sample, each group"
830,1,"['quantiles', 'distribution', 'normal', 'tests', 'normal distribution']", Approximate methods,seg_241,"with the φx denoting quantiles on the normal distribution. this is for twosided tests. for one-sided tests, use α instead of α/2."
831,1,['method'], Approximate methods,seg_241,"these formulas are often found in textbooks, and some computer programs implement them rather than the more accurate method described earlier. they do have the advantage of more clearly displaying theoretical properties such as the proportionality of δ and 1/√n for a given power."
832,1,['degrees of freedom'], Approximate methods,seg_241,"however, they become numerically unsatisfactory when the degrees of freedom falls below 20 or so."
833,1,"['sample', 'binomial', 'binomial distributions', 'populations', 'population', 'distributions']", Power of comparisons of proportions,seg_243,"suppose you wish to compare the morbidity between two populations and have to decide the number of persons to sample from each population. that is, you plan to perform a comparison of two binomial distributions as in section 8.2 using prop.test or chisq.test."
834,1,"['probabilities', 'range', 't test', 'distribution', 'normal', 'binomial', 'binomial distribution', 'test', 'hypothesis']", Power of comparisons of proportions,seg_243,"for binomial comparisons, exact power calculations become unwieldy, so we rely on normal approximations to the binomial distribution. the power will depend on the probabilities in both groups, not just their difference. as for the t test, the group sizes are assumed to be equal. the theoretical derivation of the power proceeds along the same lines as before by calculating the distribution of p̂1 − p̂2 when p1 6= p2 and the probability that it falls outside the range of values compatible with the hypothesis p1 = p2. assuming equal numbers in the two groups, this leads to the sample-size formula"
835,1,"['results', 'method', 'table']", Power of comparisons of proportions,seg_243,"since the method is only approximate, the results are not reliable unless the expected number in each of the four cells in the 2× 2 table is greater than 5."
836,1,"['sample size', 'sample', 'deviation', 'distribution', 'standard', 'level', 'standard deviation', 'test']", Twosample problems,seg_245,"the following example is from altman (1991, p. 457) and concerns the influence of milk on growth. two groups are to be given different diets, and their growth will be measured. we wish to compute the sample size that with a power of 90%, using a two-sided test at the 1% level, can find a difference of 0.5 cm in a distribution with a standard deviation of 2 cm. this is done as follows:"
837,1,"['deviation', 'graphical', 'experimental', 'method', 'standard', 'standard deviation', 'experimental units']", Twosample problems,seg_245,"delta stands for the “true difference”, and sd is the standard deviation. as is seen, the calculation may return a fractional number of experimental units. this would, of course, in practice be rounded up to 478. in the original reference, a method employing nomograms (a graphical technique) is used and the value obtained is 450. the difference is probably due to difficulty in reading the value off the nomogram scale. to know which power you would actually obtain with 450 in each group, you would enter"
838,1,"['set', 'function', 'tests']", Twosample problems,seg_245,"the system is that exactly four out of five arguments (power, sig.level, delta, sd, and n) are given, and the function computes the missing one (defaults exist to set sd=1 and sig.level=0.05 — if you wish to have those calculated, explicitly pass them as null). in addition, there are two optional arguments: alternative, which can be used to specify onesided tests; and type, which can be used to specify that you want to handle a one-sample problem. an example of the former is"
839,1,"['tests', 'paired']", Onesample problems and paired tests,seg_247,"one-sample problems are handled by adding type=""one.sample"" in the call to power.t.test. similarly, paired tests are specified with type=""paired""; although these reduce to one-sample tests by forming differences, the printout will be slightly different."
840,1,"['deviation', 'paired data', 'data', 'variation', 'measurements', 'standard', 'standard deviation', 'measuring', 'paired']", Onesample problems and paired tests,seg_247,one pitfall when planning a study with paired data is that the literature sometimes gives the intra-individual variation as “standard deviation of repeated measurements on the same person” or similar. these may be calculated by measuring a number of persons several times and computing a common standard deviation within persons. this needs to be multiplied
841,1,"['deviation', 'paired data', 'data', 'standard', 'level', 'standard deviation', 'test', 'paired']", Onesample problems and paired tests,seg_247,"by√2 to get the standard deviation of differences, which power.t.test requires for paired data. if, for instance, it is known that the standard deviation within persons is about 10, and you want to use a paired test at the 5% level to detect a difference of 10 with a power of 85%, then you should enter"
842,0,[], Onesample problems and paired tests,seg_247,notice that sig.level=0.05 was taken as the default.
843,1,"['sample', 'results', 'distribution', 'normal', 'normal distribution']", Comparison of proportions,seg_249,"to calculate sample sizes and related quantities for comparisons of proportions, you should use power.prop.test. this is based on approximations with the normal distribution, so do not trust the results if any of the expected cell counts drop below 5."
844,1,['probabilities'], Comparison of proportions,seg_249,"the use of power.prop.test is analogous to power.t.test, although delta and sd are replaced by the hypothesized probabilities in the two groups, p1 and p2. currently, it is not possible to specify that one wants to consider a one-sample problem."
845,1,"['level', 'outcome', 'significance', 'significance level']", Comparison of proportions,seg_249,"an example is given in altman (1991, p. 459) in which two groups are administered or not administered nicotine chewing gum and the binary outcome is smoking cessation. the stipulated values are p1 = 0.15 and p2 = 0.30. we want a power of 85%, and the significance level is the traditional 5%. inserting these values yields"
846,1,"['sample size', 'sample', 'deviation', 'treatment', 'randomization', 'standard', 'standard deviation', 'trial']", Exercises,seg_251,9.1 the ashina trial was designed to have 80% power if the true treatment difference was 15% and the standard deviation of differences within a person was 20%. comment on the sample size chosen. (the power calculation was originally done using the approximative formula. the imbalance between the group sizes is due to the use of an open randomization procedure.)
847,1,"['rate', 'success', 'outcome', 'trial']", Exercises,seg_251,"9.2 in a trial comparing a binary outcome between two groups, find the required number of patients to find an increase in the success rate from 60% to 75% with a power of 90%. what happens if we reduce the power requirement to 80%?"
848,1,"['plot', 't distribution', 'distribution', 'central t distribution', 'noncentral']", Exercises,seg_251,"9.3 plot the density of the noncentral t distribution for ncp=3 and df=25 and compare it with the distribution of t + 3, where t has a central t distribution with df=25."
849,1,"['consequences', 'risk', 'rejection region', 'set', 'tests']", Exercises,seg_251,"9.4 in two-sided tests, there is also a risk of falling into the rejection region on the opposite side of the true value. the power calculations in r only take this into account if you set strict=true. discuss the consequences."
850,0,['n'], Exercises,seg_251,9.5 it is occasionally suggested to choose n to “make the true difference significant”. what power would result from choosing n by such a procedure?
851,1,"['set', 'statistical']", Advanced data handling,seg_253,"in the preceding text, we have covered a basic set of elementary statistical procedures. in the chapters that follow, we begin to discuss more elaborate statistical modelling."
852,1,['data'], Advanced data handling,seg_253,this is also a natural point to discuss some data handling techniques that are useful in the practical analysis of data but were too advanced to cover in the first two chapters of the book.
853,1,"['levels', 'quantitative', 'variables', 'factor', 'data']", Recoding variables,seg_255,"this section describes some techniques that are used to construct derived variables: grouping quantitative data, combining and renaming factor levels, and handling date values."
854,1,"['quantitative', 'factor', 'data', 'variable', 'set', 'data set', 'function']", The cut function,seg_257,"you may need to convert a quantitative variable to a grouping factor. for instance, you may wish to present your data in terms of age in 5-year groups, but age is in the data set as a quantitative variable, recorded as whole years or perhaps to a finer resolution. this is what the cut function"
855,0,[], The cut function,seg_257,"is for. the basic principles are quite simple, although there are some fine points to be aware of."
856,1,"['intervals', 'variable', 'set', 'function']", The cut function,seg_257,"the function has two basic arguments: a numeric vector and a vector of breakpoints. the latter defines a set of intervals into which the variable is grouped. you have to specify both ends of all intervals; that is, the total number of break points must be one more than the number of intervals. it is a common mistake to believe that the outer breakpoints can be omitted but the result for a value outside all intervals is set to na. the outer breakpoints can be chosen as -inf and inf, though."
857,1,"['set', 'interval', 'intervals']", The cut function,seg_257,"the intervals are left-open, right-closed by default. that is, they include the breakpoint at the right end of each interval. the lowest breakpoint is not included unless you set include.lowest=true, making the first interval closed at both ends."
858,0,[], The cut function,seg_257,"in (e.g.) epidemiology, you are more likely to want groupings like “40– 49 years of age”. this opposite convention can be obtained by setting right=false."
859,1,"['case', 'interval', 'intervals']", The cut function,seg_257,"of course, as you switch to left-closed, right-open intervals, the issue of losing the extreme interval endpoint shifts to the other end of the scale. in that case, include.lowest actually includes the highest value! in the example below, the difference lies in the inclusion of two subjects who were exactly 16 years old."
860,1,"['quantile', 'data']", The cut function,seg_257,"it is sometimes desired to split data into roughly equal-sized groups. this can be achieved by using breakpoints computed by quantile, which was described in section 4.1. for instance, you could do"
861,1,"['level', 'factors']", The cut function,seg_257,the level names resulting from cut turn out rather ugly at times. fortunately they are easily changed. you can modify each of the factors created above as follows:
862,1,['function'], The cut function,seg_257,"frank harrell’s hmisc package contains the cut2 function, which simplifies some of these matters."
863,1,"['factor', 'set', 'level']", Manipulating factor levels,seg_259,"in section 1.2.8, we used levels(f)<- .... to change the level set of a factor. some related tasks will be discussed in this section."
864,1,['levels'], Manipulating factor levels,seg_259,"first, notice that the conversion from numeric input and renaming of levels can be done in one operation:"
865,1,"['levels', 'case']", Manipulating factor levels,seg_259,"beware the slightly confusing distinction between levels and labels. the latter end up being the levels of the result, whereas the former refers to the coding of the input vector (pain in this case). that is, levels refers to the input and labels to the output."
866,1,"['variables', 'levels']", Manipulating factor levels,seg_259,"if you do not specify a levels argument, the levels will be the sorted, unique values represented in the vector. this is not always desirable when dealing with text variables since the sorting is alphabetical. consider, for instance,"
867,1,"['consequences', 'levels', 'barplots', 'data', 'tables']", Manipulating factor levels,seg_259,"another reason for specifying levels is that the default levels, obviously, do not include values that are not present in data. this may or may not be a problem, but it has consequences for later analyses; for instance, whether tables contain zero entries or whether barplots leave space for the empty columns."
868,1,"['levels', 'factors', 'factor', 'function']", Manipulating factor levels,seg_259,"the factor function works on factors as if they were character vectors, so you can reorder the levels as follows"
869,1,"['level', 'statistical', 'levels']", Manipulating factor levels,seg_259,"another typical task is to combine two or more levels. this is often done when groups would otherwise be too small for valid statistical analysis. say you wish to combine the levels ""medium"" and ""mild"" into a single ""intermediate"" level. for this purpose, the assignment form of levels allows the right-hand side to be a list:"
870,1,['level'], Manipulating factor levels,seg_259,"however, it is often easier just to change the level names and give the same name to several groups:"
871,1,"['control', 'levels', 'method']", Manipulating factor levels,seg_259,"the latter method is not quite as general as the former, though. it gives less control over the final ordering of levels."
872,1,"['data', 'associated']", Working with dates,seg_261,"in epidemiology and survival data, you often deal with time in the form of dates in calendar format. different formats are used in different places of the world, and the files you have to read were not necessarily written in the same region as the one you are currently in. the ""date"" class and associated conversion routines exist to help you deal with the complexity."
873,1,['data'], Working with dates,seg_261,"as an example, consider the estonian stroke study, a preprocessed version of which is contained in the data frame stroke. the raw data files can be found in the rawdata directory of the iswr package and read using the following code:"
874,0,[], Working with dates,seg_261,(you can of course also just substitute the full path to stroke.csv instead of using the system.file construction.)
875,1,"['variables', 'factor', 'data', 'set', 'data set', 'standard', 'function']", Working with dates,seg_261,"in this data set, the two date variables died and dstr (date of stroke) appear as factor variables, which is the standard behaviour of read.table. to convert them to class ""date"", we use the function as.date. this is straightforward but requires some attention to the date format. the format used here is (day, month, year) separated by a period (dot character), with year given as four digits. this is not a standard format, so we need to specify it explicitly."
876,1,['set'], Working with dates,seg_261,"notice the use of “percent-codes” to represent specific parts of the date: %d indicates the day of the month, %m means the month as a number, and %y means that a four-digit year is used (notice the uppercase y). the full set of codes is documented on the help page for strptime."
877,0,[], Working with dates,seg_261,"internally, dates are represented as the number of days before or after a given point in time, known as the epoch. specifically, the epoch is january 1, 1970, although this is an implementation detail that should not be relied upon."
878,0,[], Working with dates,seg_261,"it is possible to perform arithmetic on dates; that is, they behave mostly like numeric vectors:"
879,1,"['variables', 'quantiles', 'numerical']", Working with dates,seg_261,"notice that means and quantiles are displayed in date format (even if they are nonintegers). the count of na values is not displayed for date variables even though the date of death is unknown for quite a few patients; this is a bit unfortunate, but it would conflict with a convention that numerical summaries have the same class as the object that is summarized (so you would get the count displayed as a date!)."
880,1,"['variable', 'numeric variable']", Working with dates,seg_261,"the vector of differences between the two dates is actually an object of class ""difftime"". such objects can have different units — when based on dates, it will always be ""days"", but for other kinds of time variables it can be ""hours"" or ""seconds"". accordingly, it is somewhat bad practice just to treat the vector of differences as a numeric variable. the recommended procedure is to use as.numeric with an explicit units argument."
881,1,['data'], Working with dates,seg_261,"in the data file, na for a death date means that the patient did not die before the end of the study on january 1, 1996. six patients were recorded as having died after this date, but since there may well be unrecorded deaths among the remaining patients, we have to discard these death dates and just record the patients as alive at the end of the study."
882,1,"['indicator', 'transform', 'data']", Working with dates,seg_261,we shall transform the data so that all patients have an end date plus an indicator of what happened at the end date: died or survived.
883,1,['function'], Working with dates,seg_261,"the pmin function calculates the minimum, but unlike the min function, which returns a single number, it does so in parallel across multiple vectors. the na.rm argument allows na values to be ignored, so the result is that wherever died is missing or later than 1996-01-01, the end date becomes 1996-01-01 and the actual date of death otherwise."
884,1,['missing values'], Working with dates,seg_261,"the expression for dead is straightforward, although you should check that missing values are treated correctly. (they are. the & operator handles missingness such that if one argument is false the result is false, even if the other is na.)"
885,1,['observation'], Working with dates,seg_261,"finally, to obtain the observation time for all individuals, we can do"
886,1,['average'], Working with dates,seg_261,"in which we pragmatically convert to “epidemiological years” of average length. (this cannot be done just by setting units=""years"". objects of class ""difftime"" can only have units of ""weeks"" or less.)"
887,1,"['variables', 'transformations', 'function', 'transform']", Working with dates,seg_261,"notice that we performed the transformations in three separate calls to transform. this was not just for the flow of the presentation; each of the last two calls refers to variables that were not defined previously. the transform function does not allow references to variables defined in the same call (we could have used within, though; see section 2.1.8)."
888,0,[], Working with dates,seg_261,"r also has classes that represent time to a granularity finer than 1 day. the ""posixct"" class (calendar time according to the posix standards) is similar to ""date"" except that it counts seconds rather than days, and ""posixlt"" (local time) represents date and time using a structure that consists of fields for various components: year, month, day of month, hours, minutes, seconds, and more. working with such objects involves, by and large, the same issues as for the ""date"" class, although with a couple of extra twists related to time zones and daylight savings time. we shall not go deeper into this area here."
889,1,"['variables', 'data', 'cases', 'set', 'data set', 'transformation']", Recoding multiple variables,seg_263,"in the previous sections, we had some cases where essentially the same transformation had to be applied to several variables. the solution in those cases was simply to repeat the operation, but it can happen that a data set contains many similar variables that all need to be recoded (questionnaire data may, for instance, have dozens of items rated on the same"
890,1,"['cases', 'data frames', 'data']", Recoding multiple variables,seg_263,"five-point scale). in such cases, you can make use of the fact that data frames are fundamentally lists and that lapply and indexing work on them. for instance, in dealing with the raw stroke data, we could have done the date handling as follows:"
891,1,"['factors', 'variables']", Recoding multiple variables,seg_263,"similarly, the four binary variables could be converted to “no/yes” factors in a single operation."
892,1,"['function', 'data']", Conditional calculations,seg_265,"the ifelse function lets you apply different calculations to different parts of data. for illustration, we use a subset of the stroke data discussed in section 10.1.3, but we use the “cooked” version contained in the iswr package."
893,1,['indicator'], Conditional calculations,seg_265,"to compute the time on study and the event/censoring indicator needed for survival models, we can do as follows:"
894,1,"['test', 'condition']", Conditional calculations,seg_265,"the way ifelse works is that it takes three arguments: test, yes, and no. all three are vectors of the same length (if not, they will be made so by recycling). the answer is “stitched together” of pieces of yes and no in the sense that the yes element is selected wherever test is true and the no element where it is false. when the condition is na, so is the result."
895,1,"['cases', 'mean', 'condition']", Conditional calculations,seg_265,"notice that both alternatives are computed (exceptions are made for the cases where the condition is all true or all false). this is not usually a problem in terms of speed, but it does mean that ifelse is not the right tool to use if you want to avoid, for example, taking the logarithm of negative values. also notice that ifelse discards attributes, including the class, so that obstime is not of class ""difftime"" even though both the yes and the no part are. this sometimes makes using ifelse more trouble than it is worth, and it can be preferable simply to use explicit subsetting operations."
896,1,"['variables', 'data', 'data frames', 'measurements']", Combining and restructuring data frames,seg_267,"in this section, we discuss ways of joining data frames either “vertically” (adding records) or “horizontally” (adding variables). we also look at the issue of converting data with repeated measurements of the same variables between the “long” and the “wide” formats."
897,1,"['missing values', 'variables', 'case', 'data', 'data frames', 'set', 'data set', 'statistical']", Appending frames,seg_269,"sometimes data are received from multiple sources and you need to combine them to form one bigger data set. in this subsection, we consider the case where data are combined by “vertical stacking”; that is, you start out with data frames which refer to separate rows of the result — typically different subjects. it is required that the data frames contain the same variables, although not necessarily in the same order (this is unlike some other statistical systems, which will simply insert missing values for variables that are absent in a data set)."
898,1,"['variables', 'case', 'data', 'data frames', 'variable', 'set', 'data set']", Appending frames,seg_269,"to simulate such a situation, suppose that the juul data set had been collected separately for boys and girls. in that case, the data frames might not contain the variable sex, since this is the same for everyone in the same data frame, and variables that only make sense for one gender may also have been omitted for the other group."
899,1,['data'], Appending frames,seg_269,"notice the use of the select argument to subset. the processing of this argument replaces column names by column numbers, and the resulting expression is used to index the data frame. the net effect of the negative indices is to remove, for example, testvol and sex from juulgrl."
900,1,"['variables', 'data frames', 'data']", Appending frames,seg_269,"to put the data frames back together, you must first add in the missing variables"
901,1,"['data', 'data frames', 'method']", Appending frames,seg_269,and then it is just a matter of using the rbind method for data frames:
902,1,"['levels', 'variables', 'factor', 'data', 'data frames']", Appending frames,seg_269,notice that rbind uses the column names (so that it does not concatenate unrelated variables even though the order of columns differs in the two data frames) and that the order of variables in the first data frame “wins”: the result has the variables in the same order as juulboy. notice also that rbind is being smart about factor levels:
903,1,"['data', 'data frames', 'sets', 'data sets']", Merging data frames,seg_271,"just as you may have different groups of subjects collected in separate data sets, you may also have different sorts of data on the same patients collected separately. for example, you could have one data frame with registry data, one with clinical biochemistry data, and one with questionnaire data. it may work to use cbind to stick the data frames together side-by-side, but it could be dangerous: what if the data are not complete in all data frames or out of sequence? you typically have to work with a unique subject identification code to avoid mistakes of this sort."
904,1,"['variables', 'data', 'data frames', 'variable', 'set', 'function']", Merging data frames,seg_271,"the merge function deals with these issues. it works by matching on one or several variables from each data frame. by default, this is the set of variables that have the same name in both frames (typically, there is a variable called something like id, which holds the subject identification). assuming that this default works and that the two data frames are called respectively dfx and dfy, the merged frame is computed simply as"
905,1,"['variable name', 'variables', 'cases', 'variable']", Merging data frames,seg_271,"however, there may be variables of the same name in both frames. in such cases, you can add a by argument, which contains the variable name or names to match on as in"
906,1,"['variables', 'case', 'data', 'data frames']", Merging data frames,seg_271,"any other variables that appear in both frames will have .x or .y appended to their name in the result. it is recommended to use this format in any case as a safeguard and for readability and explicitness. if the matching variable(s) have different names in the two data frames, you can use by.x and by.y."
907,1,"['table', 'data', 'cases', 'sets', 'data frames', 'tables', 'population', 'data sets']", Merging data frames,seg_271,"matching is not necessarily one-to-one. one of the data sets might for instance hold tabular material corresponding to the study population. the common example is mortality tables. in such cases, there is generally a many-to-one relationship between the data frames. more than one subject in the study population will belong to the table entry for 40–49 year-olds, and the rows of the table will have to be duplicated accordingly during the merge."
908,1,"['table', 'data', 'intervals', 'set', 'data set', 'population']", Merging data frames,seg_271,"to illustrate these concepts, we use the data set nickel. this describes a cohort of nickel smelting workers in south wales. the data set ewrates contains a table of the population mortality by year and age group in fiveyear intervals."
909,1,"['data', 'sets', 'population', 'data sets']", Merging data frames,seg_271,"suppose we wish to merge these two data sets according to the values at entry into the study population. this age is contained in agein, and the date of entry is computed as dob + agein. you can compute group codes corresponding to ewrates as follows:"
910,1,"['data', 'variable', 'function', 'variable names']", Merging data frames,seg_271,"the trunc function rounds values towards zero. notice that the age groups start on values that are evenly divisible by 5, whereas the year groups end on such values; this is why the expression for ygr subtracts 1 and adds it back after truncation. (actually this does not matter because all enrollment dates were april 1 of 1934, 1939, 1944, or 1949.) notice also that we do not use the same variable names as in ewrates. we could have done so, but the names age and year would be unintuitive in the context of the nickel data."
911,1,"['data', 'data frames', 'variable', 'variable names']", Merging data frames,seg_271,"with the age and year groups defined, it is an easy matter to perform the merge. we just need to account for the fact that we have used different variable names in the two data frames."
912,1,['function'], Merging data frames,seg_271,"we have only described the main function of merge. there are also options to include rows that only exist in one of the two frames (all, all.x, all.y), and it may also be useful to know that the pseudo-variable row.names will allow matching on row names."
913,1,"['case', 'cases', 'set', 'combinations']", Merging data frames,seg_271,"we have discussed the cases of one-to-one and many-to-one matching. many-to-many is possible but rarely useful. what happens in that case is that the “cartesian product” is formed by generating all combinations of rows from the two frames within each matching set. the extreme case of many-to-many matching occurs if the by set is empty, which gives a result with as many rows as the product of the row counts. this sometimes surprises people who expect that the row number will act as an implicit id."
914,1,"['functions', 'case', 'data', 'cases', 'set', 'function', 'statistical']", Reshaping data frames,seg_273,"longitudinal data come in two different forms: a “wide” format, where there is a separate column for each time point but only one record per case; and a “long” format, where there are multiple records for each case, one for each time point. the long format is more general since it does not need to assume that the cases are recorded at the same set of times, but when applicable it may be easier to work with data in the wide format, and some statistical functions expect it that way. other functions expect to find data in the long format. either way, there is a need to convert from one format to another. this is what the reshape function does."
915,1,"['treatment', 'concentration', 'data']", Reshaping data frames,seg_273,"consider the following data from a randomized study of bone metabolism data during tamoxifen treatment after breast cancer. the concentration of alkaline phosphatase is recorded at baseline and 3, 6, 9, 12, 18, and 24 months after treatment start."
916,1,"['measurement', 'information', 'variable', 'function', 'variable names']", Reshaping data frames,seg_273,"in the simplest uses of reshape, the function will assume that the variable names encode the information necessary for reshaping to the long format. by default, it assumes that variable names are separated from time of measurement by a ""."" (dot), so we might oblige by modifying the name format."
917,1,"['function', 'case']", Reshaping data frames,seg_273,"the sub function does substitutions within character strings, in this case replacing the string ""c"" with ""c."". alternatively, the original name format (c0, . . . , c24) can be handled by adding sep="""" to the reshape call."
918,1,"['variables', 'variable', 'set']", Reshaping data frames,seg_273,"once we have the variable naming in place, the only things we need to specify are the direction of the reshape and the set of variables to be considered time-varying. as a convenience feature, the latter can be specified by index rather than by name."
919,0,[], Reshaping data frames,seg_273,"notice that the sort order of the result is that id varies within time. this is the most convenient format to generate technically, but if you prefer the opposite sort order, just use"
920,1,"['missing data', 'data', 'information']", Reshaping data frames,seg_273,"to demonstrate the reverse procedure, we use the same data, in the long format. actually, this is a bit too easy because reshape has inserted enough information in its output to let you convert to the wide format just by saying reshape(a.long). to simulate the situation where the original data are given in the long format, we remove the ""reshapelong"" attribute, which holds these data. furthermore, we remove the records for which we have missing data by using na.omit."
921,0,[], Reshaping data frames,seg_273,"to convert a.long2 to the wide format, use"
922,1,['observations'], Reshaping data frames,seg_273,"notice that na values are filled in for patient no. 6, for whom only the first four observations are available."
923,1,"['variables', 'variable', 'observation']", Reshaping data frames,seg_273,"the arguments idvar and timevar specify the names of the variables that contain the id and the time for each observation. it is not strictly necessary to specify them if they have their default names, but it is good practice to do so. the argument v.names specifies the time-varying variables; notice that if it were omitted, then the grp variable would also be treated as time-varying."
924,1,"['experiment', 'data', 'standardization']", Pergroup and percase procedures,seg_275,"a specific data management task involves operations within subsets of a data frame, particularly those where there are multiple records for each individual. examples include calculation of cumulative dosage in a pharmacokinetic experiment and various methods of normalization and standardization."
925,1,['data'], Pergroup and percase procedures,seg_275,"a nice general approach to such tasks is first to split the data into a list of groups, operate on each group, and then put the pieces back together."
926,1,['function'], Pergroup and percase procedures,seg_275,consider the task of normalizing the values of alkaline phosphatase in a.long to their baseline values. the split function can be used to generate a list of the individual time courses:
927,1,"['function', 'results']", Pergroup and percase procedures,seg_275,"next, we apply a function to each element of the list and collect the results using lapply."
928,1,"['varying', 'data']", Pergroup and percase procedures,seg_275,"finally, we put the pieces back together using unsplit, which is the reverse operation of split. notice that a.long has id varying within time, so this is not just a matter of concatenating the elements of l2. the data for the first patient are now"
929,1,"['function', 'data']", Pergroup and percase procedures,seg_275,"in fact, there is a function that formalizes this sort of split-modify-unsplit operation. it is called ave because the default use is to replace data with"
930,1,['transformations'], Pergroup and percase procedures,seg_275,"group averages, but it can also be used for more general transformations. the following is an alternative way of doing the same computation as above:"
931,1,['data'], Pergroup and percase procedures,seg_275,"in the preceding code, we worked on the single vector a.long$c. alternatively, we can split the entire data frame and use code like"
932,1,"['efficient', 'data', 'transformations', 'transform']", Pergroup and percase procedures,seg_275,"notice how the last argument to lapply is passed on to transform, so that you effectively call transform(x, c.adj = c / c[1]) for each data frame x in the list l. this procedure is somewhat less efficient than the first one because there is more copying of data, but it generalizes to more complex transformations."
933,0,[], Time splitting,seg_277,"this section is rather advanced, and the beginner may want to skip it on the first read. understanding the contents is not crucial for the later parts of the book. on the other hand, apart from solving the particular problem, this is also a rather nice first example of the use of ad hoc programming in r and also of the “lateral thinking” that is sometimes required."
934,1,"['risk', 'table', 'data', 'set', 'data set', 'population']", Time splitting,seg_277,"the merge operation of the nickel and ewrates data in section 10.3.2 does not really make sense statistically: we merged in the mortality table corresponding to the age at the time of entry into the study population. however, the data set is about cancer, a slow disease, and an exposure that perhaps leads to an increased risk 20 or more years later. if the subjects typically die around age 50, the population mortality for people of age 30 is hardly relevant."
935,1,"['statistical', 'population']", Time splitting,seg_277,a sensible statistical analysis needs to consider the population mortality during the entire follow-up period. one way to handle this issue is to split the individuals into multiple “sub-individuals”.
936,1,"['observations', 'data', 'set', 'data set']", Time splitting,seg_277,"in the data set, the first six observations are (after the merge in section 10.3.2)"
937,1,"['precision', 'method', 'observations', 'intervals']", Time splitting,seg_277,"consider the individual with id == 4; this person entered the study at the age of 48.2684 and died (from lung cancer) at the age of 63.2712 (apologies for the excess precision). the time-splitting method treats this subject as four separate subjects, one entering the study at age 48.2684 and leaving at age 50 (on his 50th birthday) and the others covering the intervals 50–55, 55–60, and 60–63.2712. the first three are censored observations, as the subject did not die."
938,1,"['interval', 'data', 'tables', 'population']", Time splitting,seg_277,"if we merge these data with the population tables, then we can compute the expected number of deaths in a given age interval and compare that with the actual number of deaths."
939,1,"['observation', 'interval', 'intervals']", Time splitting,seg_277,"taking advantage of the vectorized nature of computations in r, the nice way of doing this is to loop over age intervals, “trimming” every observation period to each interval."
940,1,"['interval', 'observation', 'cases', 'adjusted', 'set']", Time splitting,seg_277,"to trim the observation periods to ages between (say) 60 and 65, the entry and exit times should be adjusted to the interval if they fall outside of it, cases that are unobserved during the interval should be removed, and if the subject did not die inside the interval, icd should be set to 0."
941,1,['adjusted'], Time splitting,seg_277,the easiest procedure is to “shoot first and ask later”. the adjusted entry and exit times are
942,1,"['interval', 'case', 'observation', 'cases', 'population']", Time splitting,seg_277,"or rather they would be if there were always a suitable overlap between the observation period and the target age interval. however, there are people leaving the study population before age 60 (by death or otherwise) and people entering the study after age 65. in either case, what goes wrong is that entry >= exit, and we can check for such cases by calculating"
943,1,"['cases', 'indicator']", Time splitting,seg_277,the censoring indicator for valid cases is
944,1,['data'], Time splitting,seg_277,"(we might have used cens <- (exit == 65), but it is a good rule to avoid testing floating point data for equality.)"
945,1,"['set', 'data set', 'data']", Time splitting,seg_277,the trimmed data set can then be obtained as
946,0,[], Time splitting,seg_277,and the first lines of the result are
947,1,"['observation', 'interval', 'intervals']", Time splitting,seg_277,"a couple of fine points: if someone dies exactly at age 65, they are counted as dying inside the age interval. conversely, we do not include people dying exactly at age 60; they belong in the interval 55–60 (for purposes like those of chapter 15, one should avoid observation intervals of length zero). it was also necessary to recompute ygr since this was based on the original agein."
948,1,"['risk', 'interval', 'data', 'errors', 'data frames', 'set', 'data set', 'function']", Time splitting,seg_277,"to get the fully expanded data set, you could repeat the above for each age interval (20–25, . . . , 95–100) and append the resulting 16 data frames with rbind. however, this gets rather long-winded, and there is a substantial risk of copy-paste errors. instead, you can do a little programming. first, wrap up the procedure for one group as a function:"
949,0,[], Time splitting,seg_277,"(in practice, you should not type all this at the command line but use a script window or an editor; see section 2.1.3.)"
950,1,"['function', 'dependence']", Time splitting,seg_277,"this is typical ad hoc programming. the function is far from general since it relies on knowing various names, and it also hardcodes the interval length as 5. however, more generality is not required for a one-off calculation. the important thing for the purpose at hand is to make the dependence on start explicit so that we can loop over it."
951,0,[], Time splitting,seg_277,"with this definition, trim(60) is equivalent to the nickel60 we computed earlier:"
952,1,"['results', 'intervals']", Time splitting,seg_277,"to get results for all intervals, do the following:"
953,1,"['function', 'case']", Time splitting,seg_277,"the do.call construct works by creating a call to rbind with a given argument list, which in this case is the return value from lapply, which in turn has applied the trim function to each of the values 20, 25, . . . 95. that is, the whole thing is equivalent to"
954,0,[], Time splitting,seg_277,"displaying the result for a single subject yields, for example,"
955,1,"['data frames', 'data']", Time splitting,seg_277,(the strange row names occur because multiple data frames with the same row names are being rbind-ed together and data frames must have unique row names.)
956,1,"['case', 'events', 'rates']", Time splitting,seg_277,"a weakness of the ygr computation is that since ygr refers to the calendar time group at agein, it may be off by up to 5 years. however, lung cancer death rates by age do not change that quickly, so we leave it at this. a more careful procedure, and in fact the common practice in epidemiology, is to split on both age and calendar time. the epi package contains generalized time-splitters splitlexis and cutlexis, which are useful for this purpose and also for handling the related case of splitting time based on individual events (e.g., childbirth)."
957,1,['table'], Time splitting,seg_277,"as a final step, we can merge in the mortality table as we did in section 10.3.2."
958,1,"['data', 'set', 'data set', 'rates']", Time splitting,seg_277,"for later use, the expanded data set is made available “precooked” in the iswr package under the name nickel.expand. we return to the data set in connection with the analysis of rates in chapter 15."
959,1,"['factor', 'data', 'intervals', 'variable', 'level']", Exercises,seg_279,"10.1 create a factor in which the blood.glucose variable in the thuesen data is divided into the intervals (4, 7], (7, 9], (9, 12], and (12, 20]. change the level names to “low”, “intermediate”, “high”, and “very high”."
960,1,"['factors', 'factor', 'data', 'set', 'data set']", Exercises,seg_279,"10.2 in the bcmort data set, the four-level factor cohort can be considered the product of two two-level factors, say period and area. how can you generate them?"
961,1,"['measurement', 'data']", Exercises,seg_279,10.3 convert the ashina data to the long format. consider how to encode whether the vas measurement is from the first or the second measurement session.
962,1,"['data', 'intervals']", Exercises,seg_279,"10.4 split the stroke data according to obsmonths into time intervals 0–0.5, 0.5–2, 2–12, and 12+ months after stroke."
963,1,"['model', 'variables', 'regression analysis', 'case', 'regression', 'set', 'analysis of variance', 'variance', 'response']", Multiple regression,seg_281,"this chapter discusses the case of regression analysis with multiple predictors. there is not really much new here since model specification and output do not differ a lot from what has been described for regression analysis and analysis of variance. the news is mainly the model search aspect, namely among a set of potential descriptive variables to look for a subset that describes the response sufficiently well."
964,1,"['model', 'multiple regression analysis', 'regression analysis', 'regression', 'multiple regression']", Multiple regression,seg_281,the basic model for multiple regression analysis is
965,1,"['estimated', 'parameters', 'method', 'variables', 'estimates', 'least squares', 'explanatory', 'method of least squares']", Multiple regression,seg_281,"where x1, . . . xk are explanatory variables (also called predictors) and the parameters β1, . . . , βk can be estimated using the method of least squares (see section 6.1). a closed-form expression for the estimates can be derived using matrix calculus, but we do not go into the details of that here."
966,1,"['function', 'data']", Plotting multivariate data,seg_283,"as an example in this chapter, we use a study concerning lung function in patients with cystic fibrosis in altman (1991, p. 338). the data are in the cystfibr data frame in the iswr package."
967,1,"['variables', 'data', 'set', 'data set', 'scatterplots', 'function']", Plotting multivariate data,seg_283,"you can obtain pairwise scatterplots between all the variables in the data set. this is done using the function pairs. to get figure 11.1, you simply write"
968,1,"['graphics', 'control', 'parameter']", Plotting multivariate data,seg_283,the arguments gap and cex.labels control the visual appearance by removing the space between subplots and decreasing the font size. the mex graphics parameter reduces the interline distance in the margins.
969,1,"['plot', 'data', 'function']", Plotting multivariate data,seg_283,a similar plot is obtained by simply saying plot(cystfibr) since the plot function is generic and behaves differently depending on the class of its arguments (see section 2.3.2). here the argument is a data frame and a pairs plot is a fairly reasonable thing to get when asking for a plot of an
970,1,"['variable', 'histogram', 'data']", Plotting multivariate data,seg_283,entire data frame (although you might equally reasonably have expected a histogram or a barchart of each variable instead).
971,1,"['plot', 'plots']", Plotting multivariate data,seg_283,"the individual plots do get rather small, probably not suitable for direct publication, but such plots are quite an effective way of obtaining an overview of multidimensional issues. for example, the close relations among age, height, and weight appear clearly on the plot."
972,1,['variables'], Plotting multivariate data,seg_283,"in order to be able to refer directly to the variables in cystfibr, we add it to the search path (a harmless warning about masking of tlc ensues at this point):"
973,1,"['variables', 'data', 'variable', 'set', 'data set', 'variable names']", Plotting multivariate data,seg_283,"because this data set contains common variable names such as age, height, and weight, it is a good idea to ensure that you do not have identically named variables in the workspace at this point. in particular, such names were used in the introductory session."
974,1,"['model', 'variables', 'multiple regression analysis', 'regression analysis', 'regression', 'multiple regression', 'explanatory']", Model specification and output,seg_285,specification of a multiple regression analysis is done by setting up a model formula with + between the explanatory variables:
975,1,"['variables', 'model']", Model specification and output,seg_285,"which is meant to be read as “pemax is described using a model that is additive in age, sex, and so forth.” (pemax is the maximal expiratory pressure. see appendix b for a description of the other variables in cystfibr.)"
976,0,[], Model specification and output,seg_285,"as usual, there is not much output from lm itself, but with the aid of summary you can obtain some more interesting output:"
977,1,"['model', 'f test', 'statistically significant', 'variable', 'joint', 'tests', 'test']", Model specification and output,seg_285,"the layout should be well known by now. notice that there is not one single significant t value, but the joint f test is nevertheless significant, so there must be an effect somewhere. the reason is that the t tests only say something about what happens if you remove one variable and leave in all the others. you cannot see whether a variable would be statistically significant in a reduced model; all you can see is that no variable must be included."
978,1,"['degrees of freedom', 'model', 'variables', 'residual', 'sum of squares', 'adjusted', 'adjusted r2', 'residual sum of squares', 'variance', 'residual variance']", Model specification and output,seg_285,"note further that there is quite a large difference between the unadjusted and the adjusted r2, which is due to the large number of variables relative to the number of degrees of freedom for the variance. recall that the former is the change in residual sum of squares relative to an empty model, whereas the latter is the similar change in residual variance:"
979,1,['standard'], Model specification and output,seg_285,the 25.5 comes from “residual standard error” in the summary output.
980,1,"['table', 'multiple regression analysis', 'anova table', 'regression analysis', 'regression', 'multiple regression', 'anova']", Model specification and output,seg_285,the anova table for a multiple regression analysis is obtained using anova and gives a rather different picture:
981,1,"['model', 'process', 'stepwise', 'tests', 'limit']", Model specification and output,seg_285,"note that, except for the very last line (“tlc”), there is practically no correspondence between these f tests and the t tests from summary. in particular, the effect of age is now significant. that is because these tests are successive; they correspond to (reading upward from the bottom) a stepwise removal of terms from the model until finally only age is left. during the process, bmp came close to the magical 5% limit, but in view of the number of tests, this is hardly noteworthy."
982,1,"['independent', 'table', 'anova table', 'approximation', 'anova', 'probability', 'tests']", Model specification and output,seg_285,"the probability that one out of eight independent tests gives a p-value of 0.053 or below is actually just over 35%! the tests in the anova table are not completely independent, but the approximation should be good."
983,1,"['model', 'f test', 'variables', 'anova table', 'table', 'joint', 'test', 'anova']", Model specification and output,seg_285,"the anova table indicates that there is no significant improvement of the model once age is included. it is possible to perform a joint test for whether all the other variables can be removed by adding up the sums of squares contributions and using the sum for an f test; that is,"
984,1,['table'], Model specification and output,seg_285,this corresponds to collapsing the eight lines of the table so that it would look like this:
985,0,[], Model specification and output,seg_285,"(note that this is “cheat output”, in which we have manually inserted the numbers computed above.)"
986,0,[], Model specification and output,seg_285,a procedure leading directly to the result is
987,1,"['test', 'f test']", Model specification and output,seg_285,which gives the appropriate f test with no manual computation.
988,1,"['model', 'missing values', 'variables', 'observations', 'data', 'response']", Model specification and output,seg_285,"notice, however, that you need to be careful to ensure that the two models are actually nested. r does not check this, although it does verify that the number of response observations is the same to safeguard against the more obvious mistakes. (when there are missing values in the descriptive variables, it’s easy for the smaller model to contain more data points.)"
989,1,"['model', 'table', 'variables', 'anova table', 'variable', 'anova']", Model specification and output,seg_285,"from the anova table, we can thus see that it is allowable to remove all variables except age. however, that this particular variable is left in the model is primarily due to the fact that it was mentioned first in the model specification, as we see below."
990,1,"['model', 'information', 'akaike information criterion', 'function']", Model search,seg_287,"r has the step() function for performing model searches by the akaike information criterion. since that is well beyond the scope of this book, we use simple manual variants of backwards elimination."
991,1,"['model', 'data']", Model search,seg_287,"in the following, we go through a practical model reduction for the example data. notice that the output has been slightly edited to take up less space."
992,1,"['model', 'case', 'function', 'process']", Model search,seg_287,"one advantage of doing model reductions by hand is that you may impose some logical structure on the process. in the present case, it may, for instance, be natural to try to remove other lung function indicators first."
993,1,"['variables', 'function', 'significance', 'limit']", Model search,seg_287,"as is seen, there was no obstacle to removing the four lung function variables. next we try to reduce among the variables that describe the patient’s state of physical development or size. initially, we avoid removing weight and bmp since they appear to be close to the 5% significance limit."
994,0,[], Model search,seg_287,"notice that, once age and height were removed, bmp was no longer significant. in the original reference (altman, 1991), weight, fev1, and bmp all ended up with p-values below 5%. however, far from all elimination procedures lead to that result."
995,1,"['variables', 'correlated']", Model search,seg_287,"it is also a good idea to pay close attention to the age, weight, and height variables, which are heavily correlated since we are dealing with children and adolescents."
996,1,"['model', 'method', 'variables', 'data', 'variable', 'set', 'data set']", Model search,seg_287,"as it turns out, there is really no reason to prefer one of the three variables over the two others. the fact that an elimination method ends up with a model containing only weight is essentially a coincidence. you can easily be misled by model search procedures that end up with one highly significant variable — it is far from certain that the same variable would be chosen if you were to repeat the analysis on a new, similar data set."
997,1,"['results', 'data']", Model search,seg_287,"what you may reasonably conclude is that there is probably a connection with the patient’s physical development or size, which may be described in terms of age, height, or weight. which description to use is arbitrary. if you want to choose one over the others, a decision cannot be based on the data, although possibly on theoretical considerations and/or results from previous investigations."
998,1,"['prediction', 'regression coefficients', 'data', 'regression', 'coefficients']", Exercises,seg_289,11.1 the secher data are best analyzed after log-transforming birth weight as well as the abdominal and biparietal diameters. fit a prediction equation for birth weight. how much is gained by using both diameters in a prediction equation? the sum of the two regression coefficients is almost exactly 3 — can this be given a nice interpretation?
999,1,"['model', 'variables', 'data', 'variable', 'set', 'data set']", Exercises,seg_289,11.2 the tlc data set contains a variable also called tlc. this is not in general a good idea; explain why. describe tlc using the other variables in the data set and discuss the validity of the model.
1000,1,"['variable', 'results']", Exercises,seg_289,"11.3 the analyses of cystfibr involve sex, which is a binary variable. how would you interpret the results for this variable?"
1001,1,"['regression analysis', 'data', 'regression', 'set', 'data set']", Exercises,seg_289,"11.4 consider the juul2 data set and select the group of those over 25 years old. perform a regression analysis of √igf1 on age, and extend"
1002,1,"['model', 'variance', 'analysis of variance', 'table']", Exercises,seg_289,"the model by including height and weight. generate the analysis of variance table for the extended model. what is the surprise, and why does it happen?"
1003,1,"['model', 'regression model', 'variables', 'factor', 'data', 'regression', 'multiple regression', 'set', 'data set', 'explanatory']", Exercises,seg_289,11.5 analyze and interpret the effect of explanatory variables on the milk intake in the kfm data set using a multiple regression model. notice that sex is a factor here; what does that imply for the analyses?
1004,1,"['linear', 'data', 'cases', 'sets', 'standard', 'data sets']", Linear models,seg_291,many data sets are inherently too complex to be handled adequately by standard procedures and thus require the formulation of ad hoc models. the class of linear models provides a flexible framework into which many — although not all — of these cases can be fitted.
1005,1,"['linear', 'model', 'data', 'regression', 'cases', 'linear regression', 'function']", Linear models,seg_291,"you may have noticed that the lm function is applied to data classified into groups (chapter 7) as well as to (multiple) linear regression (chapters 6 and 11) problems, even though the theory for these procedures appears to be quite different. however, they are, in fact, special cases of the same general model."
1006,1,"['continuous distribution', 'case', 'multiple regression', 'regression coefficient', 'coefficient', 'model', 'regression model', 'regression', 'distribution', 'continuous', 'categories', 'variables', 'regression analysis', 'variable', 'normal', 'explanatory', 'normal distribution', 'slope']", Linear models,seg_291,"the basic point is that a multiple regression model can describe a wide variety of situations if you choose the explanatory variables suitably. there is no requirement that the explanatory variables should follow a normal distribution, or any continuous distribution for that matter. one simple example (which we use without comment in chapter 11) is that a grouping into two categories can be coded as a 0/1 variable and used in a regression analysis. the regression coefficient in that case corresponds to a difference between two groups rather than the slope of an actual line. to encode a grouping with more than two categories, you can use multiple 0/1 variables."
1007,1,"['model', 'variables', 'continuous', 'dummy variables']", Linear models,seg_291,"generating these dummy variables becomes tedious, but it can be automated by the use of model formulas. among other things, such formulas provide a convenient abstraction by treating classification variables (factors) and continuous variables symmetrically. you will need to learn"
1008,1,['model'], Linear models,seg_291,exactly what model formulas do in order to become able to express your own modelling ideas.
1009,0,[], Linear models,seg_291,"this chapter contains a collection of models and their handling by lm, mainly in the form of relatively minor extensions and modifications of methods described earlier. it is meant only to give you a feel for the scope of possibilities and does not pretend to be complete."
1010,1,"['linear', 'model', 'multiple regression analysis', 'regression analysis', 'observation', 'regression', 'multiple regression', 'variable']", Polynomial regression,seg_293,"one basic observation showing that multiple regression analysis can do more than meets the eye is that you can include second-order and higher powers of a variable in the model along with the original linear term. that is, you can have a model like"
1011,1,"['model', 'linear', 'linear model', 'parameters', 'variables', 'observations', 'nonlinear', 'regression']", Polynomial regression,seg_293,"this obviously describes a nonlinear relation between y and x, but that does not matter; the model is still a linear model. what does matter is that the relation between the parameters and the expected observations is linear. it also does not matter that there is a deterministic relation between the regression variables x, x2, x3, . . . , as long as there is no linear relation between them. however, fitting high-degree polynomials can be difficult because near-collinearity between terms makes the fit numerically unstable."
1012,1,"['plot', 'linear', 'data', 'set', 'data set', 'test']", Polynomial regression,seg_293,we return to the cystic fibrosis data set for an example. the plot of pemax and height in figure 11.1 may suggest that the relation is not quite linear. one way to test this is to try to add a term that is the square of the height.
1013,1,"['function', 'model']", Polynomial regression,seg_293,"notice that the computed height2 in the model formula needs to be “pro- tected” by i(...). this technique is often used to prevent any special interpretation of operators in a model formula. such an interpretation will not take place inside a function call, and i is the identity function that returns its argument unaltered."
1014,1,"['deviation', 'data', 'process']", Polynomial regression,seg_293,"we find a significant deviation from linearity. however, considering the process that led to doing this particular analysis, the p-values have to be taken with more than a grain of salt. this is getting dangerously close to “data dredging”, fishing expeditions in data. consider it more an illustration of a technique than an exemplary data analysis."
1015,1,"['plot', 'curve', 'prediction', 'confidence bands', 'data', 'set', 'confidence', 'prediction and confidence bands']", Polynomial regression,seg_293,"to draw a plot of the fitted curve with prediction and confidence bands, we can use predict. to avoid problems caused by data not being sorted by height, we use newdata, which allows the prediction of values for a chosen set of predictors. here we choose a set of heights between 110 and 180 cm in steps of 2 cm:"
1016,1,"['predicted', 'data']", Polynomial regression,seg_293,"based on these predicted data, figure 12.1 is obtained as follows:"
1017,1,"['deviation', 'model', 'curve', 'prediction intervals', 'prediction', 'data', 'intervals', 'distribution', 'standard', 'standard deviation']", Polynomial regression,seg_293,"it is seen that the fitted curve is slightly decreasing for small heights. this is probably an artifact caused by the choice of a second-order polynomial to fit data. more likely, the reality is that pemax is relatively constant up to about 150 cm, after which it increases quickly with height. note also that there seems to be a discrepancy between the prediction limits and the actual distribution of data for the smaller heights. the standard deviation might be larger for larger heights, but it is not impossible to obtain a similar distribution of points by coincidence, and there is also an issue with potential overfitting to the observed data. it is really not advisable to construct prediction intervals based on data as limited as these unless you are sure that the model is correct."
1018,1,"['model', 'regression line', 'intercept', 'regression']", Regression through the origin,seg_295,"it sometimes makes sense to assume that a regression line passes through (0, 0) — that the intercept of the regression line is zero. this can be specified in the model formula by adding the term -1 (“minus intercept”) to the right-hand side: y ~ x - 1."
1019,1,"['linear', 'model', 'regression model', 'intercept', 'regression', 'variable', 'linear regression', 'linear regression model']", Regression through the origin,seg_295,"the logic of the notation can be seen by writing the linear regression model as y = α× 1 + β× x + e. the intercept corresponds to having an extra descriptive variable, which is the constant 1. removing this variable yields regression through the origin."
1020,1,"['linear', 'simulated']", Regression through the origin,seg_295,this is a simulated example of a linear relationship through the origin (y = 2x + e):
1021,1,"['slope', 'intercept', 'estimate']", Regression through the origin,seg_295,"in the first analysis, the intercept is not significant, which is, of course, not surprising. in the second analysis we force the intercept to be zero, resulting in a slope estimate with a substantially improved accuracy."
1022,1,"['model', 'intercept', 'cases', 'tables', 'mean', 'variation', 'anova']", Regression through the origin,seg_295,"comparison of the r2-values in the two analyses shows something that occasionally causes confusion: r2 is much larger in the model with no intercept! this does not, however, mean that the relation is “more linear” when the intercept is not included or that more of the variation is explained. what is happening is that the definition of r2 itself is changing. it is most easily seen from the anova tables in the two cases:"
1023,1,"['degrees of freedom', 'model', 'intercept', 'sum of squares']", Regression through the origin,seg_295,"notice that the total sum of squares and the total number of degrees of freedom is not the same in the two analyses. in the model with an intercept, there are 19 df in all and the total sum of squares is ∑(yi− ȳ)2, while the model without an intercept has a total of 20 df and the total sum of squares is defined as ∑ y2"
1024,1,"['residual', 'variance', 'residual variance']", Regression through the origin,seg_295,"i . unless ȳ is close to zero, the latter “total ss” will be much larger than the former, so if the residual variance is similar, r2 will be much closer to 1."
1025,1,"['model', 'regression model', 'table', 'residual', 'anova table', 'regression analysis', 'regression', 'sum of squares', 'intercepts', 'residual sum of squares', 'anova']", Regression through the origin,seg_295,"the reason for defining the total sum of squares like this for models without intercepts is that it has to correspond to the residual sum of squares in a minimal model. the minimal model has to be a submodel of the regression model; otherwise the anova table simply does not make sense. in an ordinary regression analysis, the minimal model is y = α + e, but when the regression model does not include α, the only sensible minimal model is y = 0 + e."
1026,1,"['design matrix', 'model', 'design', 'function']", Design matrices and dummy variables,seg_297,the function model.matrix gives the design matrix for a given model. it can look like this:
1027,1,"['set', 'data set', 'data']", Design matrices and dummy variables,seg_297,(the cystfibr data set was attached previously.)
1028,1,"['regression coefficients', 'intercept', 'regression', 'fitted values', 'coefficient', 'coefficients']", Design matrices and dummy variables,seg_297,"you should not worry about the ""assign"" attribute at this stage, but the three columns are important. if you add them together, weighted by the corresponding regression coefficients, you get exactly the fitted values. notice that the intercept enters as the coefficient to a column of ones."
1029,1,"['factor', 'model']", Design matrices and dummy variables,seg_297,"if the same is attempted for a model containing a factor, the following happens. (we return to the anesthetic ventilation example from p. 129.)"
1030,1,"['variables', 'observations', 'results', 'regression coefficients', 'intercept', 'regression', 'regression coefficient', 'mean', 'coefficient', 'dummy variables', 'coefficients']", Design matrices and dummy variables,seg_297,"the two columns of zeros and ones are sometimes called dummy variables. they are interpreted exactly as above: multiplying them by the respective regression coefficients and adding the results yields the fitted value. notice that, for example, the second column is 1 for observations in group 2 and 0 otherwise; that is, the corresponding regression coefficient describes something that is added to the intercept for observations in that particular group. both columns have zeros for observations from the first group, the mean value of which is described by the intercept (β0) alone. the regression coefficient β1 thus describes the difference in means between groups 1 and 2, and β2 between groups 1 and 3."
1031,1,"['model', 'regression model', 'regression coefficients', 'regression', 'multiple regression', 'coefficients']", Design matrices and dummy variables,seg_297,"you may be confused by the use of the term “regression coefficients” even though no regression lines are present in models like that above. the point is that you formally rewrite a model for groups as a multiple regression model, so that you can use the same software. as is seen, there is a unique correspondence between the formal regression coefficients and the group means."
1032,1,"['variables', 'treatment', 'contrasts', 'treatment contrasts', 'dummy variables', 'coefficients']", Design matrices and dummy variables,seg_297,you can define dummy variables in several different ways to describe a grouping. this particular scheme is called treatment contrasts because if the first group is “no treatment” then the coefficients immediately give the treatment effects for each of the other groups. we do not discuss other
1033,1,"['design matrix', 'contrast', 'design', 'set']", Design matrices and dummy variables,seg_297,"choices here; see venables and ripley (2002) for a much deeper discussion. note only that contrast type can be set on a per-term basis and that this is what is reflected in the ""contrasts"" attribute of the design matrix."
1034,1,"['degrees of freedom', 'sum of squares', 'analysis of variance', 'variance', 'anova']", Design matrices and dummy variables,seg_297,"for completeness, the ""assign"" attribute indicates which columns belong together. when, for instance, you request an analysis of variance using anova, the sum of squares for ventilation will have 2 degrees of freedom, corresponding to the removal of both columns simultaneously."
1035,1,"['levels', 'factor', 'set', 'dummy variables', 'coefficients', 'mean', 'indicator', 'model', 'regression', 'indicator variables', 'variables', 'regression coefficients', 'intercept', 'fitted values']", Design matrices and dummy variables,seg_297,"removing the intercept from a model containing a factor term will not correspond to a model in which a particular group has mean zero since such models are usually nonsensical. instead, r generates a simpler set of dummy variables, which are indicator variables of the levels of the factor. this corresponds to the same model as when the intercept is included (the fitted values are identical), but the regression coefficients have a different interpretation."
1036,1,"['linear', 'experiment', 'results', 'data', 'regression', 'cases', 'set', 'linear regression', 'analysis of variance', 'continuous', 'variance']", Linearity over groups,seg_299,"sometimes data are grouped according to a division of a continuous scale (e.g., by age group), or an experiment was designed to take several measurements at each of a fixed set of x-values. in both cases it is relevant to compare the results of a linear regression with those of an analysis of variance."
1037,1,"['linear', 'linear regression', 'case', 'regression']", Linearity over groups,seg_299,"in the case of grouped x-values, you might take a central value as representative for everyone in a given group, for instance formally letting everyone in a “20–29-year” category be 25 years old. if individual xvalues are available, they may of course be used in a linear regression, but it makes the analysis a little more complicated, so we discuss only the situation where that is not the case."
1038,1,"['linear', 'model', 'regression model', 'parameters', 'data', 'regression', 'linear regression', 'analysis of variance', 'variance', 'linear regression model']", Linearity over groups,seg_299,we thus have two alternative models for the same data. both belong to the class of linear models that lm is capable of handling. the linear regression model is a submodel of the model for one-way analysis of variance because the former can be obtained by placing restrictions on the parameters of the latter (namely that the true group means lie on a straight line).
1039,1,"['model', 'f test', 'residual', 'residual variation', 'variation', 'test']", Linearity over groups,seg_299,"it is possible to test whether or not a model reduction is allowable by comparing the reduction in the amount of variation explained to the residual variation in the larger model, resulting in an f test."
1040,1,"['mean', 'data']", Linearity over groups,seg_299,"in the following example on trypsin concentrations in age groups (altman, 1991, p. 212), data are given as the mean and sd within each of six groups. this is a kind of data that r is not quite prepared to handle, and it"
1041,1,['data'], Linearity over groups,seg_299,has therefore been necessary to create “fake” data giving the same means and sds. these can be obtained via
1042,1,"['independent', 'results', 'analysis of variance', 'variance']", Linearity over groups,seg_299,the actual results of the analysis of variance depend only on the means and sds and are therefore independent of the faking. readers interested in how to perform the actual faking should take a look at the file fake.trypsin.r in the rawdata directory of the iswr package.
1043,1,"['variables', 'data']", Linearity over groups,seg_299,"the fake.trypsin data frame contains three variables, as seen by"
1044,1,"['factor', 'levels']", Linearity over groups,seg_299,"notice that there are both grp, which is a numeric vector, and grpf, which is a factor with six levels."
1045,1,"['table', 'anova table', 'data', 'analysis of variance', 'variance', 'anova']", Linearity over groups,seg_299,performing a one-way analysis of variance on the fake data gives the following anova table:
1046,1,"['linear', 'model', 'table', 'anova table', 'regression', 'intervals', 'regression coefficient', 'linear regression', 'coefficient', 'error', 'anova']", Linearity over groups,seg_299,"if you had used grp instead of grpf in the model formula, you would have obtained a linear regression on the group number instead. in some circumstances, that would have been a serious error, but here it actually makes sense. the midpoints of the age intervals are equidistant, so the model is equivalent to assuming a linear development with age (the interpretation of the regression coefficient requires some care, though). the anova table looks as follows:"
1047,1,"['model', 'linear', 'linear model', 'residual', 'data', 'mean squares', 'mean', 'test']", Linearity over groups,seg_299,"notice that the residual mean squares did not change very much, indicating that the two models describe the data nearly equally well. if you want to have a formal test of the simple linear model against the model where there is a separate mean for each group, it can be done easily as follows:"
1048,1,"['model', 'data']", Linearity over groups,seg_299,so we see that the model reduction has a nonsignificant p-value and hence that model2 does not fit data significantly better than model1.
1049,1,"['linear', 'model', 'linear model', 'case']", Linearity over groups,seg_299,"this technique works only when one model is a submodel of the other, which is the case here since the linear model is defined by a restriction on the group means."
1050,0,[], Linearity over groups,seg_299,another way to achieve the same result is to add the two models together formally as follows:
1051,1,"['model', 'parameters', 'table', 'anova table', 'sum of squares', 'anova']", Linearity over groups,seg_299,"this model is exactly the same as when only grpf was included. however, the anova table now contains a subdivision of the model sum of squares in which the grpf line describes the change incurred by expanding the model from one to five parameters. the anova table in altman (1991, p. 213) is different, erroneously."
1052,1,['plot'], Linearity over groups,seg_299,the plot in figure 12.2 is made like this:
1053,1,['graphical'], Linearity over groups,seg_299,"the graphical techniques used here are essentially identical to those used for figure 7.1, so we do not go into further details."
1054,1,"['skewed', 'data', 'distribution', 'concentration']", Linearity over groups,seg_299,notice that the fakeness of the data is exposed by a point showing a negative trypsin concentration! the original data are unavailable but would likely show a distribution skewed slightly upwards.
1055,1,"['table', 'anova table', 'observations', 'regression analysis', 'data', 'regression', 'number of observations', 'anova']", Linearity over groups,seg_299,"actually, it is possible to analyze the data in r without generating fake data. a weighted regression analysis of the group means, with weights equal to the number of observations in each group, will yield the first two lines of the anova table, and the last one can be computed from the sds. the details are as follows:"
1056,1,"['degrees of freedom', 'residuals', 'sum of squares', 'estimate', 'information', 'mean', 'standard', 'tests', 'error', 'residual sum of squares', 'standard deviations', 'residual', 'deviations', 'mean squares', 'variation']", Linearity over groups,seg_299,"notice that the “residuals” line is zero and that the f tests are not calculated. omitting the factor(gr) term will cause that line to go into residuals and be treated as an estimate of the error variation, but that is not what you want since it does not include the information about the variation within groups. instead, you need to fill in the missing information computed from the group standard deviations and sizes. the following gives the residual sum of squares and the corresponding degrees of freedom and mean squares:"
1057,1,"['table', 'estimate', 'anova table', 'variance', 'anova']", Linearity over groups,seg_299,"there is no simple way of updating the anova table with an external variance estimate, but it is easy enough to do the computations directly:"
1058,1,"['linear', 'model', 'regression model', 'interaction', 'regression', 'multiple regression', 'interaction terms', 'mean', 'level', 'response']", Interactions,seg_301,"a basic assumption in a multiple regression model is that terms act additively on the response. however, this does not mean that linear models cannot describe nonadditivity. you can add special interaction terms that specify that the effect of one term is modified according to the level of another. in the model formulas in r, such terms are generated using the colon operator; for example, a:b. usually, you will also include the terms a and b, and r allows the notation a b for a+b+a:b. higher-order"
1059,1,"['variables', 'interactions']", Interactions,seg_301,* interactions among three or more variables are also possible.
1060,1,"['interaction term', 'coefficients', 'factors', 'variables', 'regression coefficients', 'interaction', 'regression', 'interaction terms', 'associated']", Interactions,seg_301,"the exact definition of the interaction terms and the interpretation of their associated regression coefficients can be elusive. some peculiar things happen if an interaction term is present but one or more of the main effects are missing. the full details are probably best revealed through experimentation. however, depending on the nature of the terms a and b as factors or numeric variables, the overall effect of including interaction terms can be described as follows:"
1061,1,"['model', 'levels', 'factors', 'interaction', 'case', 'combinations']", Interactions,seg_301,• interaction between two factors. this is conceptually the simplest case. the model with interaction corresponds to having different levels for all possible combinations of levels of the two factors.
1062,1,"['model', 'linear', 'factor', 'interaction', 'case', 'variable', 'slopes', 'continuous', 'numeric variable']", Interactions,seg_301,"• interaction between a factor and a numeric variable. in this case, the model with interaction contains linear effects of the continuous variable but with different slopes within each group defined by the factor."
1063,1,"['model', 'linear', 'variables', 'interaction', 'regression', 'variable', 'vary', 'continuous', 'slope']", Interactions,seg_301,"• interaction between two continuous variables. this gives a slightly peculiar model containing a new regression variable that is the product of the two. the interpretation is that you have a linear effect of varying one variable while keeping the other constant, but with a slope that changes as you vary the other variable."
1064,1,"['combination', 'experiment', 'data', 'set', 'data set', 'replications', 'varying']", Twoway ANOVA with replication,seg_303,"the coking data set comes from johnson (1994, section 13.1). the time required to make coke from coal is analyzed in a 2× 3 experiment varying the oven temperature and the oven width. there were three replications at each combination."
1065,1,"['interaction term', 'interaction']", Twoway ANOVA with replication,seg_303,"we see that the interaction term is significant. if we take a look at the cell means, we can get an idea of why this happens:"
1066,1,"['model', 'factors', 'interaction', 'case', 'tests', 'additive model']", Twoway ANOVA with replication,seg_303,"the difference between high and low temperatures increases with oven width, making an additive model inadequate. when this is the case, the individual tests for the two factors make no sense. if the interaction had"
1067,1,"['factors', 'tests']", Twoway ANOVA with replication,seg_303,"not been significant, then we would have been able to perform separate f tests for the two factors."
1068,1,"['experiment', 'data', 'set', 'data set', 'average', 'concentration']", Analysis of covariance,seg_305,"as the example in this section, we use a data set concerning growth conditions of tetrahymena cells, collected by per hellung-larsen. data are from two groups of cell cultures where glucose was either added or not added to the growth medium. for each culture, the average cell diameter (µ) and cell concentration (count per ml) were recorded. the cell concentration was set at the beginning of the experiment, and there is no systematic difference in cell concentration between the two glucose groups. however, it is expected that the cell diameter is affected by the presence of glucose in the medium."
1069,1,['data'], Analysis of covariance,seg_305,"data are in the data frame hellung, which can be loaded and viewed like this:"
1070,1,"['missing values', 'mean']", Analysis of covariance,seg_305,"the coding of glucose is such that 1 and 2 mean yes and no, respectively. there are no missing values."
1071,1,['data'], Analysis of covariance,seg_305,summarizing the data frame yields
1072,1,"['distribution', 'mean', 'median']", Analysis of covariance,seg_305,"notice that the distribution of the concentrations is strongly right-skewed with a mean more than twice as big as the median. note also that glucose is regarded as a numeric vector by summary, even though it has only two different values."
1073,1,"['factor', 'variable', 'data']", Analysis of covariance,seg_305,"it will be more convenient to have glucose as a factor, so it is recoded as shown below. recall that to change a variable inside a data frame, you use $-notation (p. 21) to specify the component you want to change:"
1074,1,['variables'], Analysis of covariance,seg_305,"it is convenient to be able to refer to the variables of hellung without the hellung$ prefix, so we put hellung in the search path."
1075,1,"['plot', 'data']", Graphical description,seg_307,"first, we plot the raw data (figure 12.3):"
1076,1,"['factor', 'plotting']", Graphical description,seg_307,"by calculating as.numeric(glucose), we convert the factor glucose to the underlying codes, 1 and 2. the specification of pch thus implies that group 1 (“yes”) is drawn using plotting character 1 (circles) and group 2 with plotting character 2 (triangles)."
1077,1,"['factor', 'plotting', 'observation']", Graphical description,seg_307,"to get different plotting symbols, you must first create a vector containing the symbol numbers and give that as the pch argument. the following form yields open and filled circles: c(1,16)[glucose]. it looks a bit cryptic at first, but it is really just a consequence of r’s way of indexing. for indexing purposes, a factor like glucose behaves as a vector of 1s and 2s, so you get the first element of c(1,16), namely 1, whenever an observation is from group 1; when the observation is from group 2, you similarly get 16."
1078,1,['explanatory'], Graphical description,seg_307,the explanatory text is inserted with legend like this:
1079,1,['function'], Graphical description,seg_307,notice that both the function and one of its arguments are named legend.
1080,1,"['plot', 'locator', 'function']", Graphical description,seg_307,the function locator returns the coordinates of a point on a plot. it works so that the function awaits a click with a mouse button and then returns the cursor position. you may want to call locator() directly from
1081,0,['n'], Graphical description,seg_307,"the command line to see the effect. notice that if you do not specify a value for n, then you need to right-click when you are done selecting points."
1082,1,"['plot', 'nonlinear']", Graphical description,seg_307,"the plot shows a clear inverse and nonlinear relation between concentration and cell diameter. further, it is seen that the cultures without glucose are systematically below cultures with added glucose."
1083,1,['plot'], Graphical description,seg_307,you get a much nicer plot (figure 12.4) by using a logarithmic x-axis:
1084,1,['linear'], Graphical description,seg_307,now the relation suddenly looks linear!
1085,1,"['plot', 'regression']", Graphical description,seg_307,you could also try a log-log plot (shown in figure 12.5 with regression lines as described below):
1086,1,"['plot', 'data', 'concentration']", Graphical description,seg_307,"as is seen, this really does not change much, but it was nevertheless decided to analyze data with both diameter and concentration logtransformed because a power-law relation was expected (y = αxβ, which gives a straight line on a log-log plot)."
1087,1,"['plot', 'results', 'regression analysis', 'data', 'regression', 'data frames']", Graphical description,seg_307,"when adding regression lines to a log plot or log-log plot, you should notice that abline interprets them as lines in the coordinate system obtained after taking (base-10) logarithms. thus, you can add a line for each group with abline applied to the results of a regression analysis of log10(diameter) on log10(conc). first, however, it is convenient to define data frames corresponding to the two glucose groups:"
1088,1,"['factor', 'levels']", Graphical description,seg_307,"notice that you have to use the names, not the numbers, of the factor levels."
1089,1,"['variables', 'data', 'regression', 'data frames', 'plotting']", Graphical description,seg_307,"since we only need the two data frames for adding lines to the figure, it would be cumbersome to add them in turn to the search path with attach, do the plotting, and then use detach to remove them. it is easier to use the data argument to lm; this allows you to explicitly specify the data frame in which to look for variables. the two regression lines are drawn with"
1090,1,"['plot', 'data', 'statistically significant', 'slope']", Graphical description,seg_307,"after which the plot looks like figure 12.5. it is seen that the lines fit the data quite well and that they are almost, but not perfectly, parallel. the question is whether the difference in slope is statistically significant. this is the topic of the next section."
1091,1,['regression'], Comparison of regression lines,seg_309,"corresponding to the two lines from before, we have the following regression analyses:"
1092,1,"['model', 'regression analysis', 'regression', 'additive model']", Comparison of regression lines,seg_309,"notice that you can use arithmetic expressions in the model formula [here log10(...)]. there are limitations, though, because, for example, z~x+y means a model where z is described by an additive model in x and y, which is not the same as a regression analysis on the sum of the two. the latter may be specified using z~i(x+y) (i for “identity”)."
1093,1,"['estimates', 'standard error', 'slopes', 'standard', 'significance', 'error', 'slope']", Comparison of regression lines,seg_309,"a quick assessment of the significance of the difference between the slopes of the two lines can be obtained as follows: the difference between the slope estimates is 0.0065, and the standard error of that is"
1094,1,['slopes'], Comparison of regression lines,seg_309,"√0.00412 + 0.00272 = 0.0049. since t = 0.0065/0.0049 = 1.3, it would seem that we are allowed to assume that the slopes are the same."
1095,1,"['model', 'data', 'information', 'slopes', 'set', 'data set', 'test', 'hypothesis']", Comparison of regression lines,seg_309,"it is, however, preferable to fit a model to the entire data set and test the hypothesis of equal slopes in that model. one reason that this approach is preferable is that it can be generalized to more complicated models. another reason is that even though there is nothing seriously wrong with the simple test for equal slopes, that procedure gives you little information on how to proceed. if the slopes are the same, you would naturally want"
1096,1,"['slope', 'estimate']", Comparison of regression lines,seg_309,to find an estimate of the common slope and the distance between the parallel lines.
1097,1,"['model', 'slopes', 'set', 'intercepts', 'concentration']", Comparison of regression lines,seg_309,"first, we set up a model that allows the relation between concentration and cell diameter to have different slopes and intercepts in the two glucose groups:"
1098,1,"['regression coefficients', 'observation', 'regression', 'expected value', 'concentration', 'coefficients']", Comparison of regression lines,seg_309,these regression coefficients should be read as follows. the expected value of the log cell diameter for an observation with cell concentration c is obtained as the sum of the following four quantities:
1099,1,['intercept'], Comparison of regression lines,seg_309,"1. the intercept, 1.6313"
1100,0,[], Comparison of regression lines,seg_309,"3. 0.0034, but only for a culture without glucose"
1101,0,[], Comparison of regression lines,seg_309,"4. −0.0065× log10 c, but only for cultures without glucose"
1102,1,['linear'], Comparison of regression lines,seg_309,"accordingly, for cell cultures with glucose, we have the linear relation"
1103,0,[], Comparison of regression lines,seg_309,and for cultures without glucose we have
1104,1,"['model', 'estimates', 'slope', 'intercept', 'regression', 'joint', 'coefficients']", Comparison of regression lines,seg_309,"put differently, the first two coefficients in the joint model can be interpreted as the estimates for intercept and slope in group 1, whereas the latter two are the differences between group 1 and group 2 in intercept and slope, respectively. comparison with the separate regression analyses"
1105,1,"['standard errors', 'estimate', 't test', 'errors', 'pooled variance', 'slopes', 'joint', 'intercepts', 'variance', 'standard', 'coefficient', 'test', 'slope']", Comparison of regression lines,seg_309,shows that slopes and intercepts are the same as in the joint analysis. the standard errors differ a little from the separate analyses because a pooled variance estimate is now used. notice that the rough test of difference in slope outlined above is essentially the t test for the last coefficient.
1106,1,"['level', 'factor', 'levels']", Comparison of regression lines,seg_309,"notice also that the glucose and log10(conc).glucose terms indicate items to be added for cultures without glucose. this is because the factor levels are ordered yes = 1 and no = 2, and the base level is the first group."
1107,1,"['model', 'additive model']", Comparison of regression lines,seg_309,"fitting an additive model, we get"
1108,1,"['coefficients', 'estimated']", Comparison of regression lines,seg_309,here the interpretation of the coefficients is that the estimated relation for cultures with glucose is
1109,0,[], Comparison of regression lines,seg_309,and for cultures without glucose it is
1110,1,['logarithmic scale'], Comparison of regression lines,seg_309,"that is, the lines for the two cultures are parallel, but the log diameters for cultures without glucose are 0.0282 below those with glucose. on the original (nonlogarithmic) scale, this means that the former are 6.3% lower (a constant absolute difference on a logarithmic scale corresponds to constant relative differences on the original scale and 10−0.0282 = 0.937)."
1111,1,"['linear', 'model', 'regression', 'joint', 'test', 'variance', 'regression line']", Comparison of regression lines,seg_309,"the joint analysis presumes that the variance around the regression line is the same in the two groups. this assumption should really have been tested before embarking on the analysis above. a formal test can be performed with var.test, which conveniently allows a pair of linear models as arguments instead of a model formula or two group vectors:"
1112,1,"['robustness', 'linear', 'test']", Comparison of regression lines,seg_309,"when there are more than two groups, bartlett’s test can be used. it, too, allows linear models to be compared. the reservations about robustness against nonnormality apply here, too."
1113,1,"['intercept', 'data', 'slopes', 'hypothesis', 'concentration', 'slope']", Comparison of regression lines,seg_309,"it is seen that it is possible to assume that the lines have the same slope and that they have the same intercept, but — as we will see below — not both at once. the hypothesis of a common intercept is silly anyway unless the slopes are also identical: the intercept is by definition the y-value at x = 0, which because of the log scale corresponds to a cell concentration of 1. that is far outside the region the data cover, and it is a completely arbitrary point that will change if the concentrations are measured in different units."
1114,1,"['anova table', 'model', 'table', 'anova']", Comparison of regression lines,seg_309,the anova table for the model is
1115,1,"['model', 'case']", Comparison of regression lines,seg_309,"the model formula a b, where in the present case a is log10(conc)"
1116,1,"['model', 'f test', 'table', 'anova table', 'anova', 'regression', 'test', 'hypothesis']", Comparison of regression lines,seg_309,"* and b is glucose, is a short form for a + b + a:b, which is read “effect of a plus effect of b plus interaction”. the f test in the penultimate line of the anova table is a test for the hypothesis that the last term (a:b) can be omitted, reducing the model to be additive in log10(conc) and glucose, which corresponds to the parallel regression lines. the f test one line earlier indicates whether you can subsequently remove glucose and the one in the first line to whether you can additionally remove log10(conc), leaving an empty model."
1117,1,"['sum of squares', 'anova table', 'table', 'anova']", Comparison of regression lines,seg_309,"alternatively, you can read the table from top to bottom as adding terms describing more and more of the total sum of squares. to those familiar with the sas system, this kind of anova table is known as type i sums of squares."
1118,1,"['f test', 't test', 'regression coefficients', 'regression', 'statistic', 'coefficient', 'test', 'coefficients']", Comparison of regression lines,seg_309,"the p-value for log10(conc):glucose can be recognized as that of the t test for the coefficient labelled log10(conc).glucose in the previous output. the f statistic is exactly the square of t as well. however, this is true only because there are just two groups. had there been three or more, there would have been several regression coefficients and the f test would"
1119,1,"['variance', 'analysis of variance']", Comparison of regression lines,seg_309,"have tested them all against zero simultaneously, just like when all groups are tested equal in a one-way analysis of variance."
1120,1,"['model', 'table', 'anova table', 'regression', 'test', 'anova']", Comparison of regression lines,seg_309,"note that the test for removing log10(conc) does not make sense because you would have to remove glucose first, which is “forbidden” when glucose has a highly significant effect. it makes perfectly good sense to test log10(conc) without removing glucose — which corresponds to testing that the two parallel regression lines can be assumed horizontal — but that test is not found in the anova table. you can get the right test by changing the order of terms in the model formula; compare, for instance, these two regression analyses:"
1121,1,"['model', 'f test', 'sum of squares', 'tables', 'test']", Comparison of regression lines,seg_309,"they both describe exactly the same model, as is indicated by the residual sum of squares being identical. the partitioning of the sum of squares is not the same, though — and the difference may be much more dramatic than it is here. the difference is whether log10(conc) is added to a model already containing glucose or vice versa. since the second f test in both tables is highly significant, no model reduction is possible and the f test in the line above it is irrelevant."
1122,1,"['model', 'f test', 'regression coefficients', 'regression', 'tables', 'tests', 'test', 'coefficients']", Comparison of regression lines,seg_309,"if you go back and look at the regression coefficients in the model with parallel regression lines, you will see that the squares of the t tests are 579.49 and 113.8, precisely the last f test in the two tables above."
1123,1,"['covariance', 'concentration']", Comparison of regression lines,seg_309,it is informative to compare the covariance analysis above with the simpler analysis in which the effect of cell concentration is ignored:
1124,1,"['interval', 'case', 'significance', 'data', 'sets', 'statistical significance', 'efficiency', 'confidence', 'data sets', 'statistical', 'confidence interval', 'analysis of covariance', 'covariance']", Comparison of regression lines,seg_309,"notice that the p-value is much less extreme. it is still significant in this case, but in smaller data sets the statistical significance could easily disappear completely. the difference in means between the two groups is 0.026, which is comparable to the 0.028 that was the glucose effect in the analysis of covariance. however, the confidence interval goes from 0.006 to 0.045, whereas the analysis of covariance had 0.023 to 0.034 [0.0282± t.975(48)× 0.0026], which is almost four times as narrow, obviously a substantial gain in efficiency."
1125,1,"['plot', 'model', 'method', 'observations', 'set']", Diagnostics,seg_311,regression diagnostics are used to evaluate the model assumptions and investigate whether or not there are observations with a large influence on the analysis. a basic set of these is available via the plot method for lm objects. four of them are displayed in a 2× 2 layout (figure 12.6) as follows:
1126,1,"['set', 'normal', 'plotting']", Diagnostics,seg_311,the par commands set up for a 2×2 layout with compressed margin texts and go back to normal after plotting.
1127,1,"['plot', 'absolute value', 'design', 'residuals', 'standardized', 'distribution', 'normal', 'fitted values', 'skewness', 'normal distribution']", Diagnostics,seg_311,the top left panel shows residuals versus fitted values. the top right panel is a q–q normal distribution plot of standardized residuals. notice that there are residuals and standardized residuals; the latter have been corrected for differences in the sd of residuals depending on their position in the design. (residuals corresponding to extreme x-values generally have a lower sd due to overfitting.) the third plot is of the square root of the absolute value of the standardized residuals; this reduces the skewness of the distribution and makes it much easier to detect if there might be a
1128,1,"['plot', 'plots', 'regression coefficients', 'dispersion', 'observation', 'regression', 'set', 'level', 'coefficients']", Diagnostics,seg_311,"trend in the dispersion. the fourth plot is of “cook’s distance”, which is a measure of the influence of each observation on the regression coefficients. we will return to cook’s distance shortly. actually, this is not the default set of plots; the default replaces the fourth plot by a plot that contains the two components that enter into the calculation of cook’s distance, but this is harder to explain at this level."
1129,1,"['plot', 'residual', 'plots', 'data', 'observation']", Diagnostics,seg_311,"the plots for the thuesen data show observation no. 13 as extreme in several respects. it has the largest residual as well as a prominent spike in the cook’s distance plot. observation no. 20 also has a large residual, but not quite as conspicuous a cook’s distance."
1130,1,"['residuals', 'standardized', 'function']", Diagnostics,seg_311,it is also possible to obtain individual diagnostics; a selection is shown in figure 12.7. the function rstandard gives the standardized residuals
1131,1,"['deviation', 'model', 'residuals', 'case', 't distribution', 'distribution', 'standard', 'standard deviation']", Diagnostics,seg_311,"discussed above. there is also rstudent, which gives leave-out-one residuals, in which the fitted value is calculated omitting the current point; if the model is correct, then these will follow a (student’s) t distribution. (unfortunately, some texts use “studentized residuals” for residuals divided by their standard deviation; i.e., what rstandard calculates in r.) it takes a keen eye to see the difference between the two types of residuals, but the extreme residuals tend to be a little further out in the case of rstudent."
1132,1,"['observations', 'residuals', 'observation', 'function']", Diagnostics,seg_311,"the function dffits expresses how much an observation affects the associated fitted value. as with the residuals, observations 13 and maybe 20 seem to stick out. notice that there is a gap in the line. this is due to the missing observation 16 and the use of na.exclude. this looks a little awkward but has the advantage of making the x-axis match the observation number."
1133,1,"['plot', 'estimated', 'parameters', 'standard error', 'observation', 'standard', 'function', 'error']", Diagnostics,seg_311,"the function dfbetas gives the change in the estimated parameters if an observation is excluded relative to its standard error. it is a matrix, so matplot is useful to plot them all in one plot. notice that observation 13 affects both α (the solid line) and β by nearly one standard error."
1134,1,"['model', 'multiple regression analysis', 'regression analysis', 'regression', 'multiple regression']", Diagnostics,seg_311,"the name dfbetas refers to its use in multiple regression analysis, where you write the model as y = β0 + β1x1 + β2x2 + · · · . this gets a little con-"
1135,1,"['regression analysis', 'intercept', 'regression']", Diagnostics,seg_311,"fusing in a simple regression analysis, where the intercept is otherwise called α."
1136,1,"['estimated', 'covariance matrix', 'covariance', 'joint', 'coefficients']", Diagnostics,seg_311,cook’s distance d calculated by cooks.distance is essentially a joint measure of the components of dfbetas. the exact procedure is to take the unnormalized change in coefficients and use the norm defined by the estimated covariance matrix for β̂ and then divide by the number of co-
1137,1,"['plot', 'functions', 'efficient']", Diagnostics,seg_311,"efficients. √d is on the same scale as dfbetas and was added to that plot as a double-width line. (if you look inside the r functions for some of these quantities, you will find them apparently quite different from the descriptions above, but they are in fact the same, only computationally more efficient.)"
1138,1,['observation'], Diagnostics,seg_311,"thus, the picture is that observation 13 seems to be influential. let us look at the analysis without this observation."
1139,1,['observations'], Diagnostics,seg_311,"we use the subset argument to lm, which, like other indexing operations, can be used with negative numbers to remove observations."
1140,1,"['model', 'observations', 'data', 'observation', 'sets', 'data sets']", Diagnostics,seg_311,"the relation practically vanished in thin air! the whole analysis actually hinges on a single observation. if the data and model are valid, then of course the original p-value is correct, and perhaps you could also say that there will always be influential observations in small data sets, but some caution in the interpretation does seem advisable."
1141,1,"['outliers', 'variables', 'observations', 'plots', 'regression analysis', 'regression']", Diagnostics,seg_311,the methods for finding influential observations and outliers are even more important in regression analysis with multiple descriptive variables. one of the big problems is how to present the quantities graphically in a sensible way. this might be done using three-dimensional plots (the add-
1142,0,[], Diagnostics,seg_311,"on package scatterplot3d makes this possible), but you can get quite far using colour coding."
1143,1,['model'], Diagnostics,seg_311,"here, we see how to display the value of cook’s distance (which is always positive) graphically for a model where pemax is described using height and weight, as in figure 12.8:"
1144,1,"['function', 'scatterplot']", Diagnostics,seg_311,"the first line computes cook’s distance and the second scales it to a value between 0 and 1. thereafter, a colour coding of the values in cookd is made with the function gray. the latter interprets its argument as the degree of whiteness, so if you want a large distance represented as black, you need to subtract the value from 1. furthermore, it is convenient to take the square root of cookd because it is a quadratic distance measure (which in practice shows up in the form of too many white or nearly white points). then a scatterplot of height versus weight is drawn with the cho-"
1145,1,['plotting'], Diagnostics,seg_311,sen colours. a filled plotting symbol in enlarged symbol size is used to get the grayscale to stand out more clearly.
1146,1,"['residuals', 'case', 'data', 'set', 'data set', 'function']", Diagnostics,seg_311,"you can use similar techniques to describe other influence measures. in the case of signed measures, you might use different symbols for positive and negative values. here is an example on studentized residuals in a data set describing birth weight as a function of abdominal and biparietal diameters determined by ultrasonography of the fetus immediately before birth, also used in exercise 11.1 (figure 12.9):"
1147,1,"['model', 'treatment', 'results', 'data', 'set', 'tests', 'additive model']", Exercises,seg_313,"12.1 set up an additive model for the ashina data (see exercise 5.6) containing additive effects of subjects, period, and treatment. compare the results with those obtained from t tests."
1148,1,"['linear', 'model', 'interval', 'data', 'analysis of variance', 'mean', 'variance', 'confidence', 'test', 'confidence interval', 'slope']", Exercises,seg_313,"12.2 perform a two-way analysis of variance on the tb.dilute data. modify the model to have a dose effect that is linear in log dose. compute a confidence interval for the slope. an alternative approach could be to calculate a slope for each animal and perform a test based on them. compute a confidence interval for the mean slope, and compare it with the preceding result."
1149,0,[], Exercises,seg_313,12.3 consider the following definitions:
1150,1,['model'], Exercises,seg_313,"generate the model matrices for models z ~ a b, z ~ a:b, etc. dis-"
1151,1,['model'], Exercises,seg_313,"* cuss the implications. carry out the model fits, and notice which models contain singularities."
1152,1,"['linear', 'experiment', 'factor', 'level']", Exercises,seg_313,"12.4 (advanced) in the secretin experiment, you may expect to find inter-individual differences not only in the level of glucose but also in the change induced by the injection of secretin. the factor time.comb combines time values at 30, 60, and 90 minutes. the factor time20plus combines all values from 20 minutes onward. discuss the differences and relations among the following linear models:"
1153,1,"['data', 'set', 'data set', 'function']", Exercises,seg_313,12.5 analyze the blood pressure in the bp.obese data set as a function of obesity and gender.
1154,1,"['model', 'analysis of covariance', 'data', 'covariance', 'set', 'data set', 'function']", Exercises,seg_313,"12.6 analyze the vitcap2 data set using analysis of covariance. revisit exercise 5.2 and compare the conclusions. try using the drop1 function with test=""f"" instead of summary in this model."
1155,1,"['data', 'regression', 'set', 'data set']", Exercises,seg_313,12.7 in the juul data set make regression analyses for prepubescent children (tanner stage 1) of √igf1 versus age separately for boys and girls. compare the two regression lines.
1156,1,"['plot', 'model', 'data', 'observation']", Exercises,seg_313,12.8 try step on the kfm data and discuss the result. one observation appears to be influential on the diagnostic plot for this model — explain why. what happens if you reduce the model further?
1157,1,"['plot', 'model', 'dependent variable', 'plots', 'diagnostic plots', 'dependent', 'data', 'transformations', 'variable', 'interactions', 'fitted values']", Exercises,seg_313,"12.9 for the juul data, fit a model for igf1 with interactions between age, sex, and tanner stage for those under 25 years old. explain the interpretation of this model. hint: a plot of the fitted values against age should be helpful. use diagnostic plots to evaluate possible transformations of the dependent variable: untransformed, log, or square root."
1158,1,"['model', 'linear', 'risk', 'variables', 'variable', 'continuous', 'predictor', 'outcomes']", Logistic regression,seg_315,"sometimes you wish to model binary outcomes, variables that can have only two possible values: diseased or nondiseased, and so forth. for instance, you want to describe the risk of getting a disease depending on various kinds of exposures. chapter 8 discusses some simple techniques based on tabulation, but you might also want to model dose-response relationships (where the predictor is a continuous variable) or model the effect of multiple variables simultaneously. it would be very attractive to be able to use the same modelling techniques as for linear models."
1159,1,"['model', 'logistic', 'transformed', 'probabilities', 'range', 'logistic regression', 'regression analysis', 'regression']", Logistic regression,seg_315,"however, it is not really attractive to use additive models for probabilities since they have a limited range and regression models could predict off-scale values below zero or above 1. it makes better sense to model the probabilities on a transformed scale; this is what is done in logistic regression analysis."
1160,1,"['linear', 'model', 'linear model', 'transformed', 'probabilities', 'set']", Logistic regression,seg_315,a linear model for transformed probabilities can be set up as
1161,1,"['quantile', 'odds ratio', 'distribution', 'log odds', 'normal', 'logit', 'function', 'logit function', 'normal distribution']", Logistic regression,seg_315,"in which logit p = log[p/(1− p)] is the log odds. a constant additive effect on the logit scale corresponds to a constant odds ratio. the choice of the logit function is not the only one possible, but it has some mathematically convenient properties. other choices do exist; the probit function (the quantile function of the normal distribution) or log(− log p), which has a connection to survival analysis models."
1162,1,"['probability', 'probability of an event', 'linear', 'logistic', 'parameter', 'event', 'error', 'model', 'distribution', 'outcome', 'variance', 'variability', 'normal', 'logistic model', 'normal distribution']", Logistic regression,seg_315,"one thing to notice about the logistic model is that there is no error term as in linear models. we are modelling the probability of an event directly, and that in itself will determine the variability of the binary outcome. there is no variance parameter as in the normal distribution."
1163,1,"['method of maximum likelihood', 'set', 'likelihood function', 'probability', 'function', 'varying', 'data', 'model', 'parameters', 'data set', 'estimated', 'method', 'likelihood', 'maximum likelihood']", Logistic regression,seg_315,"the parameters of the model can be estimated by the method of maximum likelihood. this is a quite general technique, similar to the least-squares method in that it finds a set of parameters that optimizes a goodness-of- fit criterion (in fact, the least-squares method itself is a slightly modified maximum-likelihood procedure). the likelihood function l(β) is simply the probability of the entire observed data set for varying parameters."
1164,1,"['degrees of freedom', 'model', 'parameters', 'deviance', 'data']", Logistic regression,seg_315,the deviance is the difference between the maximized value of −2 log l and the similar quantity under a “maximal model” that fits data perfectly. changes in deviance caused by a model reduction will be approximately χ2-distributed with degrees of freedom equal to the change in the number of parameters.
1165,1,"['linear', 'logistic', 'logistic regression', 'deviance', 'regression analysis', 'data', 'regression', 'tables']", Logistic regression,seg_315,"in this chapter, we see how to perform logistic regression analysis in r. there naturally is quite a large overlap with the material on linear models since the description of models is quite similar, but there are also some special issues concerning deviance tables and the specification of models for pretabulated data."
1166,1,"['function', 'binomial distribution', 'linear', 'logistic', 'mean', 'generalized linear models', 'logistic regression', 'regression', 'distribution', 'binomial', 'response', 'variables', 'regression analysis']", Generalized linear models,seg_317,"logistic regression analysis belongs to the class of generalized linear models. these models are characterized by their response distribution (here the binomial distribution) and a link function, which transfers the mean value to a scale in which the relation to background variables is described as linear ans additive. in a logistic regression analysis, the link function is"
1167,1,"['poisson', 'functions', 'model', 'linear', 'algorithm', 'data', 'observation', 'mean', 'generalized linear models', 'function']", Generalized linear models,seg_317,"there are several other examples of generalized linear models; for instance, analysis of count data is often handled by the multiplicative poisson model, where the link function is log λ, with λ the mean of the poisson-distributed observation. all of these models can be handled using the same algorithm, which also allows the user some freedom to define his or her own models by defining suitable link functions."
1168,1,"['glm', 'functions', 'model', 'linear', 'linear model', 'generalized linear model', 'normal', 'extractor functions', 'generalized linear models', 'function']", Generalized linear models,seg_317,"in r generalized linear models are handled by the glm function. this function is very similar to lm, which we have used many times for linear normal models. the two functions use essentially the same model formulas and extractor functions (summary, etc.), but glm also needs to have specified which generalized linear model is desired. this is done via"
1169,1,"['model', 'logistic', 'logistic regression', 'regression analysis', 'regression', 'binomial', 'logit']", Generalized linear models,seg_317,"the family argument. to specify a binomial model with logit link (i.e., logistic regression analysis), you write family=binomial(""logit"")."
1170,1,['data'], Logistic regression on tabular data,seg_319,"in this section, we analyze the example concerning hypertension from altman (1991, p. 353). first, we need to enter data, which is done as follows:"
1171,1,"['level', 'levels', 'variables', 'factor', 'data', 'function']", Logistic regression on tabular data,seg_319,"the gl function to “generate levels” was briefly introduced in section 7.3. the first three arguments to gl are, respectively, the number of levels, the repeat count of each level, and the total length of the vector. a fourth argument can be used to specify the level names of the resulting factor. the result is apparent from the printout of the generated variables. they were put together in a data frame to get a nicer layout. another way of generating a regular pattern like this is to use expand.grid:"
1172,1,"['logistic', 'logistic regression', 'data', 'regression', 'response']", Logistic regression on tabular data,seg_319,"r is able to fit logistic regression analyses for tabular data in two different ways. you have to specify the response as a matrix, where one column is"
1173,1,"['failures', 'variables', 'function']", Logistic regression on tabular data,seg_319,"the cbind function (“c” for “column”) is used to bind variables together, columnwise, to form a matrix. note that it would be a horrible mistake to use the total count for column 2 instead of the number of failures."
1174,1,"['model', 'logistic', 'regression model', 'logistic regression', 'logistic regression model', 'regression']", Logistic regression on tabular data,seg_319,"then, you can specify the logistic regression model as"
1175,1,"['glm', 'binomial']", Logistic regression on tabular data,seg_319,"actually, ""logit"" is the default for binomial and the family argument is the second argument to glm, so it suffices to write"
1176,1,"['model', 'logistic', 'regression model', 'logistic regression', 'logistic regression model', 'regression']", Logistic regression on tabular data,seg_319,the other way to specify a logistic regression model is to give the proportion of diseased in each cell:
1177,1,['observations'], Logistic regression on tabular data,seg_319,it is necessary to give weights because r cannot see how many observations a proportion is based on.
1178,1,['case'], Logistic regression on tabular data,seg_319,"as output, you get in either case (except for minor details)"
1179,1,"['glm', 'functions', 'model', 'linear', 'linear model', 'table', 'generalized linear model', 'regression coefficients', 'regression', 'information', 'variable', 'coefficients']", Logistic regression on tabular data,seg_319,"which is in a minimal style similar to that used for printing lm objects. also in the result of glm is some nonvisible information, which may be extracted with particular functions. you can, for instance, save the result of a fit of a generalized linear model in a variable and obtain a table of regression coefficients and so forth using summary:"
1180,1,"['linear', 'generalized linear models']", Logistic regression on tabular data,seg_319,"in the following, we go through the components of summary output for generalized linear models:"
1181,1,"['function', 'model']", Logistic regression on tabular data,seg_319,"as usual, we start off with a repeat of the model specification. obviously, more interesting is when the output is not viewed in connection with the function call that generated it."
1182,1,"['linear', 'model', 'table', 'deviance', 'observation', 'sum of squares', 'tables', 'normal']", Logistic regression on tabular data,seg_319,"this is the contribution of each cell of the table to the deviance of the model (the deviance corresponds to the sum of squares in linear normal models), with a sign according to whether the observation is larger or smaller than expected. they can be used to pinpoint cells that are particularly poorly fitted, but you have to be wary of the interpretation in sparse tables."
1183,1,"['standard errors', 'estimates', 'table', 'errors', 'regression', 'regression coefficient', 'standard', 'coefficient', 'tests', 'coefficients']", Logistic regression on tabular data,seg_319,"this is the table of primary interest. here, we get estimates of the regression coefficients, standard errors of same, and tests for whether each regression coefficient can be assumed to be zero. the layout is nearly identical to the corresponding part of the lm output."
1184,1,"['distribution', 'normal', 'mean', 'parameter', 'variance', 'normal distribution', 'dispersion']", Logistic regression on tabular data,seg_319,the note about the dispersion parameter is related to the fact that the binomial variance depends entirely on the mean. there is no scale parameter like the variance in the normal distribution.
1185,1,"['goodness of fit', 'observations', 'sum of squares', 'estimate', 'deviance', 'information', 'standard', 'standard deviation', 'model', 'parameters', 'regression', 'akaike information criterion', 'binomial', 'residual sum of squares', 'test', 'regression line', 'deviation', 'residual']", Logistic regression on tabular data,seg_319,"“residual deviance” corresponds to the residual sum of squares in ordinary regression analyses which is used to estimate the standard deviation about the regression line. in binomial models, however, the standard deviation of the observations is known, and you can therefore use the deviance in a test for model specification. the aic (akaike information criterion) is a measure of goodness of fit that takes the number of fitted parameters into account."
1186,1,"['approximation', 'deviance', 'case']", Logistic regression on tabular data,seg_319,"r is reluctant to associate a p-value with the deviance. this is just as well because no exact p-value can be found, only an approximation that is valid for large expected counts. in the present case, there are actually a couple of places where the expected cell count is rather small."
1187,1,"['degrees of freedom', 'model', 'asymptotic', 'residual', 'approximation', 'deviance', 'data', 'distribution', 'significance', 'limit']", Logistic regression on tabular data,seg_319,"the asymptotic distribution of the residual deviance is a χ2 distribution with the stated degrees of freedom, so even though the approximation may be poor, nothing in the data indicates that the model is wrong (the 5% significance limit is at 9.49 and the value found here is 1.62)."
1188,1,"['model', 'residual', 'deviance', 'case', 'joint', 'probability', 'test']", Logistic regression on tabular data,seg_319,"the null deviance is the deviance of a model that contains only the intercept (that is, describes a fixed probability, here for hypertension, in all cells). what you would normally be interested in is the difference from the residual deviance, here 14.13− 1.62 = 12.51, which can be used for a joint test for whether any effects are present in the model. in the present case, a p-value of approximately 0.6% is obtained."
1189,1,"['glm', 'model', 'data', 'information', 'statistical', 'limit']", Logistic regression on tabular data,seg_319,"this refers to the actual fitting procedure and is a purely technical item. there is no statistical information in it, but you should keep an eye on whether the number of iterations becomes too large because that might be a sign that the model is too complex to fit based on the available data. normally, glm halts the fitting procedure if the number of iterations exceeds 25, but it is possible to configure the limit."
1190,1,"['estimates', 'set']", Logistic regression on tabular data,seg_319,"the fitting procedure is iterative in that there is no explicit formula that can be used to compute the estimates, only a set of equations that they should satisfy. however, there is an approximate solution of the equations if you supply an initial guess at the solution. this solution is then used as a starting point for an improved solution, and the procedure is repeated until the guesses are sufficiently stable."
1191,1,"['linear', 'estimates', 'table', 'correlations', 'parameter']", Logistic regression on tabular data,seg_319,a table of correlations between parameter estimates can be obtained via the optional argument corr=t to summary (this also works for linear models). it looks like this:
1192,1,"['model', 'coefficients', 'variables', 'estimates', 'regression coefficients', 'observations', 'intercept', 'correlation', 'correlations', 'variable', 'regression']", Logistic regression on tabular data,seg_319,"it is seen that the correlation between the estimates is fairly small, so that it may be expected that removing a variable from the model does not change the coefficients and p-values for other variables much. (the correlations between the regression coefficients and intercept are not very informative; they mostly relate to whether the variable in question has many or few observations in the “yes” category.)"
1193,1,"['model', 'table', 'regression coefficients', 'regression', 'z test', 'test', 'coefficients']", Logistic regression on tabular data,seg_319,the z test in the table of regression coefficients immediately shows that the model can be simplified by removing smoking. the result then looks as follows (abbreviated):
1194,1,"['regression', 'multiple regression', 'tables', 'function', 'anova']", The analysis of deviance table,seg_321,deviance tables correspond to anova tables for multiple regression analyses and are generated like these with the anova function:
1195,1,"['degrees of freedom', 'model', 'variables', 'deviance', 'tests']", The analysis of deviance table,seg_321,"notice that the deviance column gives differences between models as variables are added to the model in turn. the deviances are approximately χ2-distributed with the stated degrees of freedom. it is necessary to add the test=""chisq"" argument to get the approximate χ2 tests."
1196,1,"['model', 'table', 'variable', 'test']", The analysis of deviance table,seg_321,"since the snoring variable on the last line is significant, it may not be removed from the model and we cannot use the table to justify model reductions. if, however, the terms are rearranged so that smoking comes last, we get a deviance-based test for removal of that variable:"
1197,0,[], The analysis of deviance table,seg_321,"from this you can read that smoking is removable, whereas obesity is not, after removal of smoking."
1198,1,"['model', 'variables', 'set', 'explanatory', 'test']", The analysis of deviance table,seg_321,"for good measure, you should also set up the analysis with the two remaining explanatory variables interchanged, so that you get a test of whether snoring may be removed from a model that also contains obesity:"
1199,1,['method'], The analysis of deviance table,seg_321,an alternative method is to use drop1 to try removing one term at a time:
1200,1,"['deviance', 'likelihood', 'likelihood ratio', 'test']", The analysis of deviance table,seg_321,"here lrt is the likelihood ratio test, another name for the deviance change."
1201,1,"['coefficients', 'approximation', 'results', 'deviance', 'information', 'tests', 'factors', 'regression', 'categories', 'test', 'table', 'regression coefficients', 'tables', 'degree of freedom']", The analysis of deviance table,seg_321,"the information in the deviance tables is fundamentally the same as that given by the z tests in the table of regression coefficients. the results may differ due to the use of different approximations, though. from theoretical considerations, the deviance test is preferable, but in practice the difference is often small because of the large-sample approximation χ2 ≈ z2 for tests with a single degree of freedom. however, to test factors with more than two categories, you have to use the deviance table because the z tests only relate to some of the possible group comparisons. also, the small-sample situation requires special attention; see the next section."
1202,1,"['logistic', 'logistic regression', 'frequencies', 'regression analysis', 'relative frequencies', 'regression', 'tests']", Connection to test for trend,seg_323,"in chapter 8, we considered tests for comparing relative frequencies using prop.test and prop.trend.test, in particular the example of caesarean section versus shoe size. this example can also be analyzed as a logistic regression analysis on a “shoe score”, which — for want of a better idea — may be chosen as the group number. this gives essentially the same analysis in the sense that the same models are involved."
1203,1,"['glm', 'variable', 'response variable', 'response']", Connection to test for trend,seg_323,"notice that caesar.shoe had to be transposed with t(...), so that the matrix was “stood on its end” in order to be used as the response variable by glm."
1204,1,"['deviance', 'results', 'table']", Connection to test for trend,seg_323,you can also write the results in a deviance table
1205,1,"['deviation', 'degrees of freedom']", Connection to test for trend,seg_323,"from the last line of which you see that there is no significant deviation from linearity (1.78 on 4 degrees of freedom), whereas shoe.score has a significant contribution."
1206,1,"['tests', 'standard']", Connection to test for trend,seg_323,"for comparison, the previous analyses using standard tests are repeated:"
1207,1,"['tests', 'model', 'residual', 'deviance', 'significance', 'test']", Connection to test for trend,seg_323,"the 9.29 from prop.test corresponds to the 9.34 in residual deviance from a null model, whereas the 8.02 in the trend test corresponds to the 7.56 in the test of significance of shoe.score. thus, the tests do not give exactly the same result but generally almost the same. theoretical considerations indicate that the specialized trend test is probably slightly better than the regression-based test. however, testing the linearity by subtracting the two χ2 tests is definitely not as good as the real test for linearity."
1208,1,"['parameters', 'estimate', 'standard error', 'approximation', 'likelihood', 'data', 'sets', 'likelihood ratio', 'data sets', 'parameter', 'standard', 'tests', 'test', 'error']", Likelihood profiling,seg_325,"the z tests in the summary output are based on the wald approximation, which calculates what the approximate standard error of the parameter estimate would be if the true values of the parameters were equal to the estimates. in large data sets, this is fine because the result is nearly the same for all parameter values that fit the data reasonably well. in smaller data sets, however, the difference between the wald tests and the likelihood ratio test can be considerable."
1209,1,"['tests', 'confidence intervals', 'likelihood', 'intervals', 'statistical', 'set', 'statistical test', 'likelihood ratio', 'parameter', 'confidence', 'test']", Likelihood profiling,seg_325,"this also affects the calculation of confidence intervals since these are based on inverting the tests, giving a set of parameter values that are not rejected by a statistical test. as an alternative to the wald-based ±1.96 × s.e. technique, the mass package allows you to compute intervals that are based on inverting the likelihood ratio test. in practice, this works like this"
1210,1,"['case', 'intercept', 'standard']", Likelihood profiling,seg_325,"the standard type of result can be obtained using confint.default. the difference in this case is not very large, although visible in the lines relating to snoring and the intercept:"
1211,1,"['plot', 'model', 'parameters', 'likelihood', 'trial', 'set', 'profiling', 'parameter', 'profile']", Likelihood profiling,seg_325,"the way this works is via likelihood profiling. for a set of trial values of the parameter, the likelihood is maximized over the other parameters in the model. the result can be displayed in a profile plot as follows:"
1212,1,['function'], Likelihood profiling,seg_325,"notice that we need to load the mass package at this point. (the function was used by confint earlier on, but without putting it on the search path.)"
1213,1,"['plots', 'likelihood', 'likelihood ratio', 'test']", Likelihood profiling,seg_325,"the plots require a little explanation. the quantity on the y-axis, labelled tau, is the signed square root of the likelihood ratio test."
1214,1,"['functions', 'linear', 'plots', 'profile', 'nonlinear', 'likelihood', 'likelihood function', 'function']", Likelihood profiling,seg_325,"here ` denotes the profile log-likelihood. the main idea is that when the profile likelihood function is approximately quadratic, τ(β) is approximately linear. conversely, likelihood functions not well approximated by a quadratic show up as nonlinear profile plots."
1215,1,"['confidence intervals', 'method', 'approximation', 'likelihood', 'intervals', 'distribution', 'likelihood ratio', 'profiling', 'likelihood function', 'function', 'confidence', 'test']", Likelihood profiling,seg_325,"one important thing to notice, though, is that although the profiling method will capture nonquadratic behaviour of the likelihood function, confidence intervals based on the likelihood ratio test will always be limited in accuracy by the approximation of the distribution of the test."
1216,1,"['confidence intervals', 'quantitative', 'case', 'transformation', 'coefficients', 'logistic', 'odds ratio', 'intervals', 'standard', 'confidence', 'standard errors', 'covariate', 'logistic regression', 'regression', 'regression coefficients', 'errors']", Presentation as oddsratio estimates,seg_327,"in parts of the epidemiological literature, it has become traditional to present logistic regression analyses in terms of odds ratios. in the case of a quantitative covariate, this means odds ratio per unit change in the covariate. that is, the antilogarithm (exp) of the regression coefficients is given instead of the coefficients themselves. since standard errors make little sense after the transformation, it is also customary to give confidence intervals instead. this can be obtained quite easily as follows:"
1217,1,"['intercept', 'odds ratio']", Presentation as oddsratio estimates,seg_327,the (intercept) is really the odds of hypertension (for the not snoring non-obese) and not an odds ratio.
1218,1,"['factors', 'variables', 'data']", Logistic regression using raw data,seg_329,"in this section, we again use anders juul’s data (see p. 85). for easy reference, here is how to read data and convert the variables that describe groupings into factors (this time slightly simplified):"
1219,1,"['data', 'variable', 'response variable', 'response']", Logistic regression using raw data,seg_329,"in the following, we look at menarche as the response variable. this variable indicates for each girl whether or not she has had her first period. it is coded 1 for “no” and 2 for “yes”. it is convenient to look at a subset of data consisting of 8–20-year-old girls. this can be extracted as follows:"
1220,0,[], Logistic regression using raw data,seg_329,"for obvious reasons, no boys have a nonmissing menarche, so it is not necessary to select on gender explicitly."
1221,1,['function'], Logistic regression using raw data,seg_329,then you can analyze menarche as a function of age like this:
1222,1,"['level', 'levels', 'factor', 'variable', 'response', 'event', 'response variable']", Logistic regression using raw data,seg_329,"the response variable menarche is a factor with two levels, where the last level is considered the event. it also works to use a variable that has the values 0 and 1 (but not, for instance, 1 and 2!)."
1223,1,"['model', 'estimate', 'logit', 'median']", Logistic regression using raw data,seg_329,notice that from this model you can estimate the median menarcheal age as the age where logit p = 0. a little thought (solve −20.0132 + 1.5173× age = 0) reveals that it is 20.0132/1.5173 = 13.19 years.
1224,1,"['residual', 'deviance', 'residuals', 'observations', 'case', 'observation', 'cases', 'probability']", Logistic regression using raw data,seg_329,you should not pay too much attention to the deviance residuals in this case since they automatically become large in every case where the fitted probability “goes against” the observations (which is bound to happen in some cases). the residual deviance is also difficult to interpret when there is only one observation per cell.
1225,1,"['function', 'model']", Logistic regression using raw data,seg_329,a hint of a more complicated analysis is obtained by including the tanner stage of puberty in the model. you should be warned that the exact interpretation of such an analysis is quite tricky and qualitatively different from the analysis of menarche as a function of age. it can be used for prediction purposes (although asking the girl whether she has had her first
1226,0,[], Logistic regression using raw data,seg_329,"period would likely be much easier than determining her tanner stage!), but the interpretation of the terms is not clear-cut."
1227,1,"['data', 'variable', 'joint', 'test']", Logistic regression using raw data,seg_329,"notice that there is no joint test for the effect of tanner. there are a couple of significant z-values, so you would expect that the tanner variable has some effect (which, of course, you would probably expect even in the absence of data!). the formal test, however, must be obtained from the deviances:"
1228,0,[], Logistic regression using raw data,seg_329,"clearly, both terms are highly significant."
1229,1,"['linear', 'data', 'generalized linear models', 'function']", Prediction,seg_331,"the predict function works for generalized linear models, too. let us first consider the hypertension example, where data were given in tabular form:"
1230,1,"['model', 'expected values']", Prediction,seg_331,"recall that smoking was eliminated from the model, which is why the expected values come in identical pairs."
1231,1,['logit'], Prediction,seg_331,"these numbers are on the logit scale, which reveals the additive structure. notice that 2.392− 1.697 = 1.527− 0.831 = 0.695 (except for roundoff er-"
1232,1,"['coefficient', 'regression coefficient', 'regression']", Prediction,seg_331,"ror), which is exactly the regression coefficient to obesity. likewise, the regression coefficient to snoring is obtained by looking at the differences 2.392− 1.527 = 1.697− 0.831 = 0.866."
1233,1,"['predicted', 'response', 'probabilities']", Prediction,seg_331,"to get predicted values on the response scale (i.e., probabilities), use the type=""response"" argument to predict:"
1234,1,['data'], Prediction,seg_331,"these may also be obtained using fitted, although you then cannot use the techniques for predicting on new data, etc."
1235,1,"['plot', 'probabilities']", Prediction,seg_331,"in the analysis of menarche, the primary interest is probably in seeing a plot of the expected probabilities versus age (figure 13.2). a crude plot could be obtained using something like"
1236,1,['plotting'], Prediction,seg_331,"(it will look better if a different plotting symbol in a smaller size, using the pch and cex arguments, is used) but here is a more ambitious plan:"
1237,1,['curve'], Prediction,seg_331,"this is figure 13.2. recall that seq generates equispaced vectors, here ages from 8 to 20 in steps of 0.1, so that connecting the points with lines will give a nearly smooth curve."
1238,1,['data'], Model checking,seg_333,for tabular data it is obvious to try to compare observed and fitted proportions. in the hypertension example you get
1239,1,['frequencies'], Model checking,seg_333,the problem with this is that you get no feeling for how well the relative frequencies are determined. it can be better to look at observed and expected counts instead. the former can be computed as
1240,0,[], Model checking,seg_333,"and to get a nice print for the comparison, you can use"
1241,1,"['model', 'expectation']", Model checking,seg_333,notice that the discrepancy in cell 4 between 15% expected and 0% observed really is that there are 0 hypertensives out of 2 in a cell where the model yields an expectation of 0.3 hypertensives!
1242,1,"['plot', 'model', 'variables', 'residual', 'observations', 'residual plot', 'continuous']", Model checking,seg_333,"for complex models with continuous background variables, it becomes more difficult to perform an adequate model check. it is especially a hindrance that nothing really corresponds to a residual plot when the observations have only two different values."
1243,1,"['linear', 'probabilities', 'interval', 'case', 'intervals', 'probability', 'logit', 'function']", Model checking,seg_333,"consider the example of the probability of menarche as a function of age. the problem here is whether the relation can really be assumed linear on the logit scale. for this case, you might try subdividing the x-axis in a number of intervals and see how the counts in each interval fit with the expected probabilities. this is presented graphically in figure 13.3. notice that the code adds points to figure 13.2, which you are assumed not to have deleted at this point."
1244,1,"['plot', 'probabilities', 'table', 'factor', 'intervals', 'row total']", Model checking,seg_333,"the technique used above probably requires some explanation. first, cut is used to define the factor age.group, which describes a grouping into age intervals. then a crosstable tb is formed from menarche and age.group. using prop.table, the numbers are expressed relative to the row total, and column 2 of the resulting table is extracted. this contains the relative proportion in each age group of girls for whom menarche has occurred. finally, a plot of expected probabilities is made, overlaid by the observed proportions."
1245,1,['plot'], Model checking,seg_333,"the plot looks reasonable on the whole, although the observed proportion among 12–13-year-olds appears a bit high and the proportion among 13– 14-year-olds is a bit too low."
1246,1,"['deviation', 'model', 'factor', 'intervals', 'variation', 'statistical']", Model checking,seg_333,but how do you evaluate whether the deviation is larger than what can be expected from the statistical variation? one thing to try is to extend the model with a factor that describes a division into intervals. it is not practical to use the full division of age.group because there are cells where either none or all of the girls have had their menarche.
1247,1,"['linear', 'factor', 'model']", Model checking,seg_333,"we therefore try a division into four groups, with cutpoints at 12, 13, and 14 years, and add this factor to the model containing a linear age effect."
1248,1,['deviance'], Model checking,seg_333,"that is, the addition of the grouping actually does give a significantly better deviance. the effect is not highly significant, but since the deviation concerns the ages where “much happens”, you should probably be cautious about postulating a logit-linear age effect."
1249,1,"['polynomial regression model', 'curve', 'model', 'graphical', 'regression model', 'regression', 'polynomial regression']", Model checking,seg_333,"another possibility is to try a polynomial regression model. here you need at least a third-degree polynomial to describe the apparent stagnation of the curve around 13 years of age. we do not look at this in great detail, but just show part of the output and in figure 13.4 a graphical presentation of the model."
1250,1,"['linear', 'model', 'probabilities', 'linear model', 'logit', 'anova']", Model checking,seg_333,the warnings about fitted probabilities of 0 or 1 occur because the cubic term makes the logit tend much faster to ±∞ than the linear model did. there are two occurrences for the anova call because two of the models include the cubic term.
1251,1,"['plot', 'table', 'deviance']", Model checking,seg_333,"the thing to note in the deviance table is that the cubic term gives a substantial improvement of the deviance, but once that is included, the age grouping gives no additional improvement. the plot should speak for itself."
1252,1,"['risk', 'variables', 'data', 'set', 'data set', 'explanatory', 'level']", Exercises,seg_335,"13.1 in the malaria data set, analyze the risk of malaria with age and log-transformed antibody level as explanatory variables."
1253,1,"['model', 'logistic', 'regression model', 'logistic regression', 'logistic regression model', 'data', 'transformations', 'regression', 'variable', 'set', 'data set', 'response']", Exercises,seg_335,"13.2 fit a logistic regression model to the graft.vs.host data set, predicting the gvhd response. use different transformations of the index variable. reduce the model using backwards elimination."
1254,1,"['confidence intervals', 'regression coefficients', 'data', 'regression', 'intervals', 'function', 'confidence', 'coefficients']", Exercises,seg_335,"13.3 in the analyses of the malaria and graft.vs.host data, try using the confint function to find improved confidence intervals for the regression coefficients."
1255,1,"['data', 'table']", Exercises,seg_335,"13.4 following up on exercise 8.2 about “rocky mountain spotted fever”, splitting the data by age groups gives the table below. does this"
1256,0,[], Exercises,seg_335,confirm the earlier analysis?
1257,0,[], Exercises,seg_335,western type eastern type age group total fatal total fatal under 15 108 13 310 40 15–39 264 40 189 21 40 or above 375 157 162 61 747 210 661 122
1258,1,"['logistic', 'logistic regression', 'data', 'regression', 'variable', 'set', 'data set', 'function']", Exercises,seg_335,13.5 a probit regression is just like a logistic regression but uses a different link function. try the analysis of the menarche variable in the juul data set with this link. does the fit improve?
1259,1,"['linear', 'data', 'standard']", Survival analysis,seg_337,"the analysis of lifetimes is an important topic within biology and medicine in particular but also in reliability analysis with engineering applications. such data are often highly nonnormally distributed, so that the use of standard linear models is problematic."
1260,1,"['consequences', 'treatment', 'case', 'data', 'statistical', 'trial', 'error']", Survival analysis,seg_337,"lifetime data are often censored: you do not know the exact lifetime, only that it is longer than a given value. for instance, in a cancer trial, some people are lost to follow-up or simply live beyond the study period. it is an error to ignore the censoring in the statistical analysis, sometimes with extreme consequences. consider, for instance, the case where a new treatment is introduced towards the end of the study period, so that nearly all the observed lifetimes will be cut short."
1261,1,"['random variable', 'variable', 'random', 'event']", Essential concepts,seg_339,"let x be the true lifetime and t a censoring time. what you observe is the minimum of x and t together with an indication of whether it is one or the other. t can be a random variable or a fixed time depending on context, but if it is random, then it should generally be noninformative for the methods we describe here to be applicable. sometimes “dead from other causes” is considered a censoring event for the mortality of a given"
1262,1,['cases'], Essential concepts,seg_339,"disease, and in those cases it is particularly important to ensure that these other causes are unassociated with the disease state."
1263,1,"['cumulative distribution function', 'distribution function', 'distribution', 'probability', 'function']", Essential concepts,seg_339,"the survival function s(t) measures the probability of being alive at a given time. it is really just 1 minus the cumulative distribution function for x,"
1264,1,"['risk', 'interval', 'median', 'distribution', 'hazard function', 'mean', 'function']", Essential concepts,seg_339,"the hazard function or force of mortality h(t) measures the (infinitesimal) risk of dying within a short interval of time t, given that the subject is alive at time t. if the lifetime distribution has density f , then h(t) = f (t)/s(t). this is often considered a more fundamental quantity than (say) the mean or median of the survival distribution and is used as a basis for modelling."
1265,0,[], Survival objects,seg_341,"we use the package survival, written by terry therneau and ported to r by thomas lumley. the package implements a large number of advanced techniques. for the present purposes, we use only a small subset of it."
1266,0,[], Survival objects,seg_341,"to load survival, use"
1267,1,"['set', 'data set', 'data']", Survival objects,seg_341,(this may produce a harmless warning about masking the lung data set from the iswr package.)
1268,1,"['data', 'information', 'observation', 'variable', 'event', 'function', 'indicator']", Survival objects,seg_341,"the routines in survival work with objects of class ""surv"", which is a data structure that combines times and censoring information. such objects are constructed using the surv function, which takes two arguments: an observation time and an event indicator. the latter can be coded as a logical variable, a 0/1 variable, or a 1/2 variable. the latter coding is not recommended since surv will assume 0/1 coding if all values are 1."
1269,1,"['interval', 'data', 'event']", Survival objects,seg_341,"actually, surv can also be used with three arguments for dealing with data that have a start time as well as an end time (“staggered entry”) and also interval censored data (where you know that an event happened between two dates, as happens, for instance, in repeated testing for a disease) can be handled."
1270,1,"['set', 'data set', 'data']", Survival objects,seg_341,we use the data set melanom collected by k. t. drzewiecki and reproduced in andersen et al. (1991). the data become accessible as follows:
1271,1,"['observation', 'variable', 'indicator']", Survival objects,seg_341,"the variable status is an indicator of the patient’s status by the end of the study: 1 means “dead from malignant melanoma”, 2 means “alive on january 1, 1978”, and 3 means “dead from other causes”. the variable days is the observation time in days, ulc indicates (1 for present and 2 for absent) whether the tumor was ulcerated, thick is the thickness in 1/100 mm, and sex contains the gender of the patient (1 for women and 2 for men)."
1272,1,['variable'], Survival objects,seg_341,we want to create a surv object in which we consider the values 2 and 3 of the status variable as censorings. this is done as follows:
1273,1,"['observations', 'method']", Survival objects,seg_341,"associated with the surv objects is a print method that displays the objects in the format above, with a ‘+’ marking censored observations. for example, 10+ means that the patient did not die from melanoma within 10 days and was then unavailable for further study (in fact, he died from other causes), whereas 185 means that the patient died from the disease a little over half a year after his operation."
1274,0,[], Survival objects,seg_341,notice that the second argument to surv is a logical vector; status==1 is true for those who died of malignant melanoma and false otherwise.
1275,1,"['estimated', 'factor', 'observations', 'conditional', 'intervals', 'step function', 'population', 'function', 'estimator']", KaplanMeier estimates,seg_343,the kaplan–meier estimator allows the computation of an estimated survival function in the presence of right-censoring. it is also called the product-limit estimator because one way of describing the procedure is that it multiplies together conditional survival curves for intervals in which there are either no censored observations or no deaths. this becomes a step function where the estimated survival is reduced by a factor (1− 1/rt) if there is a death at time t and a population of rt is still alive and uncensored at that time.
1276,1,"['function', 'estimator']", KaplanMeier estimates,seg_343,"computing the kaplan–meier estimator for the survival function is done with a function called survfit. in its simplest form, it takes just a single"
1277,0,[], KaplanMeier estimates,seg_343,"argument, namely a surv object. it returns a survfit object. as described above, we consider “dead from other causes” a kind of censoring and do as follows:"
1278,1,"['curve', 'estimate', 'case', 'median']", KaplanMeier estimates,seg_343,"as is seen, using survfit by itself is not very informative (just as the printed output of a “bare” lm is not). you get a couple of summary statistics and an estimate of the median survival, and in this case the latter is not even interesting because the estimate is infinite. the survival curve does not cross the 50% mark before all patients are censored."
1279,1,"['estimate', 'variable', 'function']", KaplanMeier estimates,seg_343,"to see the actual kaplan–meier estimate, use summary on the survfit object. we first save the survfit object into a variable, here named surv.all because it contains the raw survival function for all patients without regard to patient characteristics."
1280,1,"['function', 'event']", KaplanMeier estimates,seg_343,this contains the values of the survival function at the event times. the censoring times are not displayed but are contained in the survfit object and can be obtained by passing censored=t to summary (see the help page for summary.survfit for such details).
1281,1,"['curve', 'interval', 'estimate', 'standard error', 'step function', 'standard', 'function', 'confidence', 'error', 'confidence interval']", KaplanMeier estimates,seg_343,"the kaplan–meier estimate is the step function whose jump points are given in time and whose values right after a jump are given in survival. additionally, both an estimate of the standard error of the curve and a (pointwise) confidence interval for the true curve are given."
1282,1,['estimate'], KaplanMeier estimates,seg_343,"normally, you would be more interested in showing the kaplan–meier estimate graphically than numerically. to do this (figure 14.1), you simply write"
1283,1,"['curve', 'transformed', 'confidence intervals', 'interval', 'estimate', 'symmetric', 'intervals', 'confidence']", KaplanMeier estimates,seg_343,"the markings on the curve indicate censoring times, and the bands give approximate confidence intervals. if you look closely, you will see that the bands are not symmetrical around the estimate. they are constructed as a symmetric interval on the log scale and transformed back to the original scale."
1284,1,"['functions', 'plot']", KaplanMeier estimates,seg_343,"it is often useful to plot two or more survival functions on the same plot so that they can be directly compared (figure 14.2). to obtain survival functions split by gender, do the following:"
1285,1,"['glm', 'model', 'confidence intervals', 'intervals', 'confidence']", KaplanMeier estimates,seg_343,"that is, you use a model formula as in lm and glm, specifying that the survival object generated from day and status should be described by sex. notice that there are no confidence intervals on the curves. these are"
1286,1,"['plot', 'case']", KaplanMeier estimates,seg_343,"turned off when there are two or more curves because the display easily becomes confusing. they can be turned on again by passing conf.int=t to plot, in which case it can be recommended to use separate colours for the curves, as in"
1287,1,"['plot', 'level', 'confidence bands', 'case', 'confidence limits', 'plotting', 'function', 'confidence', 'confidence level']", KaplanMeier estimates,seg_343,"similarly, you can avoid plotting the confidence bands in the singlesample case by setting conf.int=f. if you want the bands but at a 99% confidence level, you should pass conf.int=0.99 to survfit. notice that the level of confidence is an argument to the fitting function (which needs it to compute the confidence limits), whereas the decision to plot the bands is controlled by a similarly named argument to plot."
1288,1,"['test', 'population']", The logrank test,seg_345,the log-rank test is used to test whether two or more survival curves are identical. it is based on looking at the population at each death time and computing the expected number of deaths in proportion to the number of
1289,1,"['risk', 'random variation', 'expected values', 'variation', 'χ2 test', 'population', 'random', 'test', 'trial']", The logrank test,seg_345,"individuals at risk in each group. this is then summed over all death times and compared with the observed number of deaths by a procedure similar (but not identical) to the χ2 test. notice that the interpretation of “ex- pected” and “observed” is slightly peculiar: if the difference in mortality is sufficiently large, then you can easily “expect” the same individuals to die several times over the course of the trial. if the population is observed to extinction with no censoring, then the observed number of deaths will equal the group size by definition and the expected values will contain all the random variation."
1290,1,"['model', 'factors', 'test statistic', 'distribution', 'statistic', 'set', 'function', 'test']", The logrank test,seg_345,"the log-rank test is formally nonparametric since the distribution of the test statistic depends only on the assumption that the groups have the same survival function. however, it can also be viewed as a model-based test under the assumption of proportional hazards (see section 14.1). you can set up a semiparametric model in which the hazard itself is unspecified but it is assumed that the hazards are proportional between groups. testing that the proportionality factors are all unity then leads to a log-rank test. the log-rank test will work best against this class of alternatives."
1291,1,"['function', 'tests', 'test', 'null hypothesis', 'hypothesis']", The logrank test,seg_345,"computation of the log-rank test is done by the function survdiff. this actually implements a whole family of tests specified by a parameter ρ, allowing various nonproportional hazards alternatives to the null hypothesis, but the default value of ρ = 0 gives the log-rank test."
1292,1,"['model', 'linear', 'factors', 'variables', 'combinations', 'grouped data', 'data', 'generalized linear models', 'predictor variables', 'predictor', 'test', 'numerical']", The logrank test,seg_345,"the specification is using a model formula as for linear and generalized linear models. however, the test can deal only with grouped data, so if you specify multiple variables on the right-hand side it will work on the grouping of data generated by all combinations of predictor variables. it also makes no distinction between factors and numerical codes. the same is true of survfit."
1293,1,"['data', 'expected value', 'set', 'data set', 'test']", The logrank test,seg_345,"it is also possible to specify stratified analyses, in which the observed and expected value calculations are carried out separately within a stratification of the data set. for instance, you can compute the log-rank test for a gender effect stratified by ulceration as follows:"
1294,1,"['adjusted', 'treatment']", The logrank test,seg_345,"notice that this makes the effect of sex appear less significant. a possible explanation might be that males seek treatment when the disease is in a more advanced state than women do, so that the gender difference is reduced when adjusted for a measure of disease progression."
1295,1,"['glm', 'model', 'likelihood', 'conditional', 'data', 'regression', 'test']", The Cox proportional hazards model,seg_347,"the proportional hazards model allows the analysis of survival data by regression models similar to those of lm and glm. the scale on which linearity is assumed is the log-hazard scale. models can be fitted via the maximization of cox’s likelihood, which is not a true likelihood but it can be shown that it may be used as one. it is calculated in a manner similar to that of the log-rank test, as the product of conditional likelihoods of the observed death at each death time."
1296,1,['model'], The Cox proportional hazards model,seg_347,"as a first example, consider a model with the single regressor sex:"
1297,1,"['tests', 'model', 'estimated', 'confidence intervals', 'intervals', 'samples', 'confidence']", The Cox proportional hazards model,seg_347,"the coef is the estimated logarithm of the hazard ratio between the two groups, which for convenience is also given as the actual hazard ratio exp(coef). the line following that also gives the inverted ratio (swapping the groups) and confidence intervals for the hazard ratio. finally, three overall tests for significant effects in the model are given. these are all equivalent in large samples but may differ somewhat in small-sample"
1298,1,"['model', 'standard error', 'z test', 'standard', 'coefficient', 'test', 'error']", The Cox proportional hazards model,seg_347,"cases. notice that the wald test is identical to the z test based on the estimated coefficient divided by its standard error, whereas the score test is equivalent to the log-rank test (as long as the model involves only a simple grouping)."
1299,1,"['continuous', 'variable', 'covariate']", The Cox proportional hazards model,seg_347,"a more elaborate example, involving a continuous covariate and a stratification variable, is"
1300,1,"['variable', 'significance']", The Cox proportional hazards model,seg_347,it is seen that the significance of the sex variable has been further reduced.
1301,1,"['plot', 'curve', 'model', 'method', 'hazard function', 'function']", The Cox proportional hazards model,seg_347,"the cox model assumes an underlying baseline hazard function with a corresponding survival curve. in a stratified analysis, there will be one such curve for each stratum. they can be extracted by using survfit on the output of coxph and of course be plotted using the plot method for survfit objects (figure 14.3):"
1302,1,"['factor', 'treatment', 'contrasts', 'case', 'data', 'covariates', 'variable', 'treatment contrasts', 'mean']", The Cox proportional hazards model,seg_347,"be aware that the default for survfit is to generate curves for a pseudoindividual for which the covariates are at their mean values. in the present case, that would correspond to a tumor thickness of 1.86 mm and a gender of 1.39 (!). notice that we have been sloppy in not defining sex as a factor variable, but that would not actually give a different result (coxph subtracts the means of the regressors before fitting, so a 1/2 coding is the same as 0/1, which is what a factor with treatment contrasts gives you). however, you can use the newdata argument of survfit to specify a data frame for which you want to calculate survival curves."
1303,1,"['estimate', 'variables', 'data', 'set', 'data set', 'explanatory', 'function', 'test', 'hypothesis']", Exercises,seg_349,"14.1 in the graft.vs.host data set, estimate the survival function for patients with or without gvhd. test the hypothesis that the survival is the same in both groups. extend the analysis by including the other explanatory variables."
1304,1,"['plot', 'model', 'estimated', 'strata']", Exercises,seg_349,"14.2 with the cox model in the last section of the text, generate a plot with estimated survival curves for men with nonulcerated tumors of thicknesses 0.1, 0.2, and 0.5 mm (three curves in one plot). hint: survfit objects can be indexed with [] to extract individual strata."
1305,1,['data'], Exercises,seg_349,14.3 fit cox models to the stroke data with age and sex as predictors and with sex alone. explain the difference.
1306,1,"['model', 'case', 'data', 'set']", Exercises,seg_349,"14.4 with the split data from exercise 10.4, you can fit a cox model with delayed entry to the stroke data; help(surv) shows how to set up the surv object in that case. refit the model(s) from the previous exercise."
1307,1,"['poisson', 'method', 'data', 'regression', 'events', 'measurements', 'event', 'statistical', 'rates']", Rates and Poisson regression,seg_351,"epidemiological studies often involve the calculation of rates, typically rates of death or incidence rates of a chronic or acute disease. this is based upon counts of events occurring within a certain amount of time. the poisson regression method is often employed for the statistical analysis of such data. however, data that are not actually counts of events but rather measurements of time until an event (or nonevent) can be analyzed by a technique which is formally equivalent."
1308,1,"['approximation', 'data', 'tables', 'population', 'rates']", Basic ideas,seg_353,"the data that we wish to analyze can be in one of two forms. they can be in aggregate form as an observed count x based on a number of personyears t. often the latter is an approximation based on tables of population size. there may of course be more than one group, and we may wish to formulate various models describing the rates in different groups."
1309,1,"['aggregate data', 'data', 'observation', 'event', 'indicator']", Basic ideas,seg_353,"we may also have individual-level data, in which for each subject we have a time under observation ti and a 0/1 indicator xi of whether the subject has had an event. the aggregate data can be thought of as being x = ∑ xi and t = ∑ ti, where the sums are over all individuals in the group."
1310,1,"['poisson', 'probabilities', 'successes', 'populations', 'case', 'distribution', 'parameter', 'event', 'poisson distribution', 'distributions']", The Poisson distribution,seg_355,the poisson distribution can be described as the limiting case of the binomial distributions when the size parameter n increases while the expected number of successes λ = np is fixed. this is useful to describe rare event in large populations. the resulting distribution has point probabilities
1311,1,"['poisson', 'functions', 'probabilities', 'distribution', 'poisson distribution']", The Poisson distribution,seg_355,"the distribution is theoretically unbounded, although the probabilities for large x will be very small. in r, the poisson distribution is available via the functions dpois, ppois, etc."
1312,1,"['poisson', 'rate', 'populations', 'data', 'distribution', 'parameter of interest', 'events', 'parameter', 'poisson distribution']", The Poisson distribution,seg_355,"in the context of epidemiological data, the parameter of interest is usually the expected counts per unit of observed time; i.e., the rate at which events occur. this enables the comparison of populations that may be of different size or observed for different lengths of time. accordingly, we may parameterize the poisson distribution using"
1313,1,['rate'], The Poisson distribution,seg_355,notice that parts of the literature use λ to denote the rate. the notation used here is chosen so as to stay compatible with the argument name in dpois.
1314,1,"['poisson', 'method of maximum likelihood', 'method', 'likelihood', 'data', 'maximum likelihood']", The Poisson distribution,seg_355,"models for poisson data can be fitted by the method of maximum likelihood. if we parameterize in terms of ρ, the log-likelihood becomes"
1315,0,[], The Poisson distribution,seg_355,which is maximized when ρ = x/t. the log-likelihood can be generalized to models involving several counts by summing terms of the same form.
1316,1,['event'], Survival analysis with constant hazard,seg_357,"in this section, for convenience, we use terminology appropriate for mortality studies, although the event may be many things other than the death of the subject."
1317,1,"['data', 'rates']", Survival analysis with constant hazard,seg_357,"individual-level data are essentially survival data as described in chapter 14, except for changes in notation. one difference, though, is that in the analysis of rates it is often reasonable to assume that the hazard does not change over time, or at least not abruptly so. rates tend to be obtained over rather short individual time periods, and the origin of the timescale"
1318,1,['event'], Survival analysis with constant hazard,seg_357,is not usually keyed to a life-changing event such as disease onset or major surgery.
1319,1,['distribution'], Survival analysis with constant hazard,seg_357,"if the hazard is constant, then the distribution of the lifetime is the"
1320,1,"['exponential distribution', 'distribution', 'exponential', 'function']", Survival analysis with constant hazard,seg_357,−ρt −ρt exponential distribution with density ρe and survival function e .
1321,1,"['factor', 'case', 'data', 'probability', 'event', 'indicator']", Survival analysis with constant hazard,seg_357,"likelihoods for censored data can be constructed using terms that are either the probability density at the time of death or the survival probability in the case of censoring. in the constant-hazard case, the two kinds of terms differ only in the presence of the factor ρ, which we may conveniently encode using the event indicator xi so that the log-likelihood terms are"
1322,1,"['poisson', 'likelihood', 'data', 'regression']", Survival analysis with constant hazard,seg_357,"except for the constant, which does not depend on ρ, these terms are formally identical to a poisson likelihood, where the count is 1 (death) or zero (censoring). this is the crucial “trick” that allows survival data with constant hazard to be analyzed by poisson regression methods."
1323,0,[], Survival analysis with constant hazard,seg_357,the trick can be extended to hazards that are only piecewise constant.
1324,0,[], Survival analysis with constant hazard,seg_357,suppose the lifetime of an individual is subdivided as ti = ti
1325,0,[], Survival analysis with constant hazard,seg_357,"ti (k), where the hazard is assumed constant during each section of time. the corresponding log-likelihood term is"
1326,0,[], Survival analysis with constant hazard,seg_357,in which the first k − 1 of the xi
1327,1,"['observations', 'likelihood']", Survival analysis with constant hazard,seg_357,can be either 0 or one. the point of writing it in this elaborate form is that it then becomes obvious that the likelihood contribution might as well have come from k different individuals where the first k− 1 had censored observations.
1328,1,"['observations', 'observation']", Survival analysis with constant hazard,seg_357,this is the rationale behind time-splitting techniques where the observation time of one subject is divided into observations for multiple pseudo-individuals.
1329,1,"['poisson', 'model', 'likelihood', 'data', 'distribution', 'events', 'event', 'poisson distribution']", Survival analysis with constant hazard,seg_357,"it should be noted that although the models with (piecewise) constant hazard can be fitted and analyzed by likelihood techniques, pretending that the data have come from a poisson distribution, this does not extend to all aspects of the model. for instance following a cohort to extinction will lead to a fixed total number of events by definition, whereas the corresponding poisson model implies that the total event count has a poisson"
1330,1,"['poisson', 'model', 'random variation', 'data', 'variation', 'random', 'event', 'rates']", Survival analysis with constant hazard,seg_357,"distribution. both types of models deal in rates, counts per time, but the difference is to what extent the random variation lies in the counts or in the amount of time. when data are frequently censored (i.e., the event is rare), the survival model becomes well approximated by the poisson model."
1331,1,"['poisson', 'linear', 'linear predictor', 'distribution', 'function', 'generalized linear models', 'poisson distribution', 'predictor', 'rates']", Fitting Poisson models,seg_359,"the class of generalized linear models (see section 13.1) also includes the poisson distribution, which by default uses a log link function. this is the mathematically convenient option and also a quite natural choice since it allows the linear predictor to span the entire real line. we can use this to formulate models for the log rates of the form"
1332,1,"['glm', 'model', 'rates']", Fitting Poisson models,seg_359,"or, since glm needs a model for the expected counts rather than rates,"
1333,1,"['poisson', 'model', 'linear', 'linear predictor', 'case', 'regression', 'variable', 'regression coefficient', 'coefficient', 'predictor']", Fitting Poisson models,seg_359,"a feature of many poisson models is that the model contains an offset in the linear predictor, log t in this case. notice that this is not the same as including the term as a regression variable since the regression coefficient is fixed at 1."
1334,1,['rates'], Fitting Poisson models,seg_359,the following example was used by erling b. andersen in 1977. it involves the rates of lung cancer by age in four danish cities and may be found as eba1977 in the iswr package.
1335,1,"['glm', 'model', 'logistic', 'rate', 'logistic regression', 'data', 'regression', 'cases', 'populations', 'function']", Fitting Poisson models,seg_359,"to fit a model that has multiplicative effects of age and city on the rate of lung cancer cases, we use the glm function in much the same way as in logistic regression. of course, we need to change the family argument to accommodate poisson-distributed data. we also need to incorporate an offset to account for the different sizes and age structures of the populations in the four cities."
1336,1,"['model', 'case']", Fitting Poisson models,seg_359,"the offset was included in the model formula in this case. alternatively, it could have been given as a separate argument as in"
1337,1,"['predictor', 'coefficients', 'linear', 'logistic', 'rate', 'multiple regression', 'standard', 'tests', 'standard errors', 'factors', 'logistic regression', 'contrasts', 'regression', 'treatment contrasts', 'table', 'variables', 'treatment', 'regression coefficients', 'linear predictor', 'errors']", Fitting Poisson models,seg_359,"the table labelled “coefficients:” contains regression coefficients for the linear predictor along with standard errors and z tests. these can be interpreted in the same way as in ordinary multiple regression or logistic regression. since both variables are factors and we are using treatment contrasts (see section 12.3), the coefficients indicate differences in the log rate (i.e., the log of the rate ratio) compared with the city of fredericia and with the 50–54-year-olds, respectively."
1338,1,"['rate', 'intercept', 'data', 'population']", Fitting Poisson models,seg_359,"the intercept term refers to the log rate for the group of 50–54-year-olds in fredericia. notice that because we used the population size rather than the number of person-years in the offset and the data cover the years 1968– 1971, this rate will effectively be per 4 person-years."
1339,1,"['degrees of freedom', 'residual', 'deviance', 'distribution', 'statistic']", Fitting Poisson models,seg_359,"a goodness-of-fit statistic is provided by comparing the residual deviance to a χ2 distribution on the stated degrees of freedom. this statistic is generally considered valid if the expected count in all cells is larger than 5. accordingly,"
1340,1,"['degrees of freedom', 'model', 'residual', 'deviance', 'data']", Fitting Poisson models,seg_359,"and we see that the model fits the data acceptably. of course, we could also just have read off the residual deviance and degrees of freedom from the summary output:"
1341,1,"['table', 'deviance', 'coefficient', 'tests']", Fitting Poisson models,seg_359,"from the coefficient table, it is obvious that there is an age effect, but it is less clear whether there is a city effect. we can perform χ2 tests for each term by using drop1 and looking at the changes in the deviance."
1342,1,['rate'], Fitting Poisson models,seg_359,"we see that the age term is significant, hardly surprisingly, but the city term apparently is not. however, if you can argue a priori that fredericia could be expected to have a higher cancer rate than the three other cities, then it could be warranted to combine the three other cities into one and perform an analysis as below."
1343,0,[], Fitting Poisson models,seg_359,"according to this, you may combine the three cities other than fredericia, and, once this is done, fredericia does indeed appear to be significantly"
1344,1,['coefficients'], Fitting Poisson models,seg_359,"different from the others. alternatively, you can look at the coefficients in fit2 directly"
1345,1,"['asymptotic', 'test']", Fitting Poisson models,seg_359,"and see the p-value of 0.0278. this agrees with the 0.03185 from drop1; you cannot expect the two p-values to be perfectly equal since they rely on different asymptotic approximations. if you really push it, you can argue that a one-sided test with half the p-value is appropriate since you would only expect fredericia to be more harmful than the others, not less. however, the argumentation becomes tenuous, and in his paper andersen outlines the possibility of testing fredericia against the other cities but stops short of providing any p-value, stating that in his opinion “there is no reason to believe a priori that fredericia is the more dangerous city”."
1346,1,"['confidence intervals', 'estimates', 'transformation', 'coefficients', 'rate', 'logistic', 'results', 'covariates', 'intervals', 'standard', 'coefficient', 'confidence', 'standard errors', 'covariate', 'logistic regression', 'nonlinear', 'regression', 'poisson', 'regression analysis', 'errors', 'intercept']", Fitting Poisson models,seg_359,"it is sometimes preferred to state the results of poisson regression analysis in terms of rate ratios by taking exp() of the estimates (this parallels the presentation of logistic regression analysis in terms of odds ratios in section 13.4). the intercept term is not really a ratio but a rate, and for nonfactor covariates it should be understood that the coefficient is the relative change per unit change in the covariate. because of the nonlinear transformation, standard errors are not useful; instead one can calculate confidence intervals for the coefficients as follows:"
1347,1,"['confidence intervals', 'likelihood function', 'function', 'approximation', 'intervals', 'profiling', 'standard', 'confidence', 'standard errors', 'asymptotic', 'distribution', 'likelihood', 'errors', 'normal', 'normal distribution']", Fitting Poisson models,seg_359,"actually, we can do better by using the confint function. this calculates confidence intervals by profiling the likelihood function instead of using the approximation with the normal distribution inherent in the use of asymptotic standard errors. this is done like this:"
1348,1,"['normal approximation', 'asymptotic', 'approximation', 'case', 'cases', 'normal', 'coefficients']", Fitting Poisson models,seg_359,"in the present case, we are well within the regime where the asymptotic normal approximation works well, so there is little difference between the two displays. however, in some cases where some expected cell counts are low and one or several coefficients are poorly determined, the difference can be substantial."
1349,1,"['table', 'data', 'standard']", Computing rates,seg_361,"we return to the welsh nickel worker data discussed in chapter 10. in that section, we discussed how to split the individual lifetime data into smaller pieces that could reasonably be merged with the standard mortality table in the ewrates data."
1350,1,"['data', 'intervals', 'set', 'data set']", Computing rates,seg_361,the result of this initial data restructuring is in the nickel.expand data set. it contains data from a lot of short time intervals like this:
1351,1,['data'], Computing rates,seg_361,"the same individuals reappear later in the data at older ages. for example, all data for the individual with id number 325 are"
1352,0,[], Computing rates,seg_361,"accordingly, this subject enters the study at age 23.7 and we follow him through five age groups until his death at age 43."
1353,1,"['variable', 'interval']", Computing rates,seg_361,"the variable ygr reflects the year of entry into the interval, so even though the subject dies in 1953, the last record is coded as belonging to the years 1946–1950."
1354,1,"['data', 'variable', 'set', 'data set']", Computing rates,seg_361,"subject no. 325 has the icd code 434 in his last record. this refers to the international classification of diseases (version 7) and indicates “other and unspecified diseases of the heart” as the cause of death. for the purposes of this chapter, we are primarily interested in lung cancer, which has codes 162 and 163, so we define a variable to indicate whether this is the cause of death. (expect a warning about masking the lung data set upon attaching.)"
1355,1,"['poisson', 'cases', 'case', 'data']", Computing rates,seg_361,"the %in% operator returns a logical vector that is true when the corresponding element of the operand on the left is contained in the vector that is the operand on the right and false in all other cases. use of this operator is slightly dangerous in the case of an na element in icd, but in these particular data, there are none. we convert the result to zero or one since we are going to pretend that it is a poisson count later on (this is not strictly necessary). notice that by using lung.cancer as the endpoint, we treat death from all other causes, including “unknown”, as censoring."
1356,1,['risk'], Computing rates,seg_361,"each record provides ageout - agein person-years of risk time, so to tabulate the risk times, we can just do as follows:"
1357,1,['missing values'], Computing rates,seg_361,"notice that there are many na entries in cells that no subject ever entered. the subjects in the study were born between 1864 and 1910, so there is a large block missing in the lower left and a smaller block in the upper right. the na.print option to print allows you to represent these missing values by a string that is less visually imposing than the default ""na""."
1358,1,['cases'], Computing rates,seg_361,the corresponding counts of lung cancer cases are obtained as
1359,1,"['risk', 'rates']", Computing rates,seg_361,"and the cancer rates can be obtained as the ratio of the counts to the risk time. these are small, so we multiply by 1000 to get rates per 1000 personyears."
1360,1,"['case', 'rates']", Computing rates,seg_361,"comparison of these rates with those in ewrates suggests that they are very high. however, this kind of display has the disadvantage that it hides the actual counts on which the rates are based. for instance, the lower part of the column for 80–84-year-olds jumps by roughly 20 units for each additional case since there are only about 50 person-years per cell."
1361,1,"['rate', 'risk', 'table', 'data', 'standard', 'rates']", Computing rates,seg_361,"it may be better to compute the expected counts in each cell based on the standard mortality table and then compare that to the actual counts. since we have already merged in the ewrates data, this is just a matter of multiplying each piece of risk time by the rate. we need to divide by 1e6 (i.e., 106 = 1000000) since the standard rates are given per million person-years."
1362,1,"['rate', 'standardized', 'cases']", Computing rates,seg_361,"the observed counts are clearly much larger than expected. we can summarize them by calculating the overall smr (standardized mortality rate), which is simply the ratio of the total number of cases to the total expected number of cases."
1363,1,"['data', 'set', 'data set', 'population']", Computing rates,seg_361,"that is, this data set has almost six times as many cancer deaths as you would expect from the mortality of the general population."
1364,1,"['rate', 'model', 'standard', 'regression model', 'intercept', 'regression', 'population', 'rates']", Models with piecewise constant intensities,seg_363,"we can formulate the smr analysis as a “poisson” regression model in the sense of section 15.1.2. the assumption behind the smr is that there is a constant rate ratio to the standard mortality, so we can fit a model with only an intercept while having an offset, which is the log of the expected count. this is not really different from modelling rates — the population mortality ρi is just absorbed into the offset, log ρi + log ti = log ρiti."
1365,1,"['glm', 'dependent variable', 'data', 'covariates', 'cases', 'variable', 'dependent']", Models with piecewise constant intensities,seg_363,"notice that this is based on individual data; the dependent variable lung.cancer is zero or one. we could have aggregated the data according to the cross-classification of agr and ygr and analyzed the number of cases in each cell. this would have allowed glm to run much faster, but on the other hand it would then not be possible to add individual covariates such as age at first exposure."
1366,1,"['model', 'standard error', 'case', 'data', 'standard', 'error']", Models with piecewise constant intensities,seg_363,"in this case, we cannot use the deviances for model checking both because the expected counts per cell are very small and because we do not actually have poisson-distributed data. however, the standard error and the p-value should be reliable if the assumptions hold."
1367,0,[], Models with piecewise constant intensities,seg_363,the connection between this analysis and the smr can be seen immediately from
1368,0,[], Models with piecewise constant intensities,seg_363,this value is exactly the smr value from the previous section.
1369,1,"['poisson', 'model', 'data', 'regression']", Models with piecewise constant intensities,seg_363,"we can analyze the data more thoroughly using regression methods. as a first approach, we investigate whether the smr is constant over year and age groups using a multiplicative poisson model."
1370,1,"['marginal', 'cases', 'tables']", Models with piecewise constant intensities,seg_363,"we need to simplify the groupings because some of the groups contain very few cases. by calculating the marginal tables of counts, we get some idea of what to do."
1371,1,"['level', 'cases']", Models with piecewise constant intensities,seg_363,"to get at least 10 cases per level, we combine all values of agr up to 45 (i.e., ages less than 50) and also those from 70 and up. similarly, we combine all values of ygr for the periods from 1961 onwards."
1372,1,"['variables', 'case', 'function', 'transform']", Models with piecewise constant intensities,seg_363,"notice that this is a case where the within function (see section 2.1.8) works better than transform because it allows more flexibility, including the creation of temporary variables such as lv."
1373,1,"['model', 'factors', 'significance', 'test']", Models with piecewise constant intensities,seg_363,"we can analyze the effect of a and y on the mortality ratio by building a log-additive model in the usual way. notice that we still use the original grouping in the calculation of the offset; it is only the smr that is assumed to be the same for everyone below 50, etc. we use drop1 to test the significance of the two factors."
1374,1,"['level', 'model', 'intercept']", Models with piecewise constant intensities,seg_363,"so it seems that we do not need the age grouping in the model, but the year grouping is needed. accordingly, we fit a model with y alone, and by dropping the intercept, we get a parameterization with a separate intercept for each level of y."
1375,1,"['regression coefficients', 'regression', 'coefficients']", Models with piecewise constant intensities,seg_363,"the regression coefficients may again be recognized as log-smr values, as the following demonstrates:"
1376,1,"['variables', 'statistical tests', 'regression', 'multiple regression', 'statistical', 'tests']", Models with piecewise constant intensities,seg_363,the advantage of using the regression approach is that it provides a framework in which you can formulate statistical tests and investigate the effect of multiple regression variables simultaneously.
1377,1,"['covariate', 'risk', 'interval', 'table', 'variables', 'data', 'standard', 'level']", Models with piecewise constant intensities,seg_363,"breslow and day analyzed the nickel data in their seminal book (breslow and day, 1987) on the analysis of cohort studies. in their analysis, they split the individual risk times according to three criteria, two of them being age and period, to match the standard mortality table, but they also treat time from employment as a time-dependent covariate with a piecewise constant effect, which requires that the person-year be split further according to the interval boundaries. they then represent time effects using three variables: time since, age at, and year of first employment, tfe, afe, and yfe, respectively. in addition, they include a measure of exposure level."
1378,1,"['interval', 'results', 'variable', 'standard', 'rates']", Models with piecewise constant intensities,seg_363,"the following analysis roughly reproduces the breslow and day analysis. it is not completely similar because we settle for splitting time according to agr only and use the age at entry into each interval to define the tfe variable as well as for choosing the relevant standard mortality rates. however, to enable some comparison of results, we define cut groups in a manner that is similar to that of breslow and day."
1379,1,['levels'], Models with piecewise constant intensities,seg_363,"some relabelling of group levels might be called for — e.g., the levels for exp are really 0, 0.5–4, 4.5–8, 8.5–12, 12.5+ — but let us not make more of it than necessary."
1380,1,"['model', 'test', 'significance']", Models with piecewise constant intensities,seg_363,we fit a multiplicative model and test the significance of the individual terms as follows:
1381,1,['model'], Models with piecewise constant intensities,seg_363,"this suggests that the two major terms are tfe and exp, whereas afe and yfe could be taken out of the model. notice, though, that it cannot be"
1382,1,['case'], Models with piecewise constant intensities,seg_363,"concluded from the above that both can be removed. in principle, one of them could become significant when the other is removed. this does not happen in this case, though."
1383,1,"['table', 'coefficients']", Models with piecewise constant intensities,seg_363,the table of coefficients looks like this:
1384,0,[], Models with piecewise constant intensities,seg_363,a dose-response pattern and a declining effect of time since first employment seem to be present.
1385,1,"['confidence intervals', 'results', 'data', 'intervals', 'confidence']", Models with piecewise constant intensities,seg_363,the results may be more readily interpreted if they are given in terms of ratios and confidence intervals. these can be obtained in exactly the same way as in the analysis of the eba1977 data.
1386,1,"['poisson', 'model', 'factors', 'regression model', 'interaction', 'data', 'regression', 'set', 'data set']", Exercises,seg_365,"15.1 in the bcmort data set, we defined the period and area factors in exercise 10.2. fit a poisson regression model to the data with age, period, and area as descriptors, as well as the three two-factor interaction terms. the interaction between period and area can be interpreted as the effect of screening."
1387,1,"['poisson', 'model', 'interval', 'data']", Exercises,seg_365,"15.2 with the split stroke data from exercise 10.4, fit a poisson regression model corresponding to a constant hazard in each interval and with multiplicative effects of age and sex."
1388,1,"['predictor', 'response', 'case', 'parameter']", Nonlinear curve fitting,seg_367,"curve fitting problems occur in many scientific areas. the typical case is that you wish to fit the relation between some response y and a one-dimensional predictor x, by adjusting a (possibly multidimensional) parameter β. that is,"
1389,1,['error'], Nonlinear curve fitting,seg_367,y = f (x; β) + error
1390,1,"['deviation', 'independent', 'standard', 'case', 'variation', 'standard deviation', 'error']", Nonlinear curve fitting,seg_367,"in which the “error” term is usually assumed to contain independent normally distributed terms with a constant standard deviation σ. the class of models can be easily extended to multivariate x and somewhat less easily to models with nonconstant error variation, but we settle for the simple case."
1391,1,"['linear', 'case']", Nonlinear curve fitting,seg_367,chapter 6 described the special case of a linear relation
1392,1,['error'], Nonlinear curve fitting,seg_367,y = β0 + β1x + error
1393,1,"['linear', 'multiple regression analysis', 'regression analysis', 'regression', 'multiple regression']", Nonlinear curve fitting,seg_367,"and we discussed the fitting of polynomials by including quadratic and higher-order terms in section 12.1. there are other techniques, notably trigonometric regression and spline regression, that can also be formulated in linear form and handled by software for multiple regression analysis like lm."
1394,1,"['linear', 'case', 'function']", Nonlinear curve fitting,seg_367,"however, sometimes linear methods are inadequate. the common case is that you have a priori knowledge of the form of the function. this may come from theoretical analysis of an underlying physical and chemical"
1395,1,['parameters'], Nonlinear curve fitting,seg_367,"system, and the parameters of the relation have a specific meaning in that theory."
1396,1,"['linear', 'parameters', 'method', 'estimate', 'data', 'least squares', 'method of least squares']", Nonlinear curve fitting,seg_367,"the method of least squares makes good sense even when the relation between data and parameters is not linear. that is, we can estimate β by minimizing"
1397,1,"['treatment', 'regression analysis', 'regression', 'location', 'standard']", Nonlinear curve fitting,seg_367,"there is no explicit formula for the location of the minimum, but the minimization can be performed numerically by algorithms that we describe only superficially here. this general technique is also known as nonlinear regression analysis. for an in-depth treatment of the topic, a standard reference is bates and watts (1988)."
1398,1,"['linear', 'model', 'linear model', 'estimates', 'errors', 'parameter', 'standard', 'standard errors']", Nonlinear curve fitting,seg_367,"if the model is “well-behaved” (to use a deliberately vague term), then the model can be approximated by a linear model in the vicinity of the optimum, and it then makes sense to calculate approximate standard errors for the parameter estimates."
1399,1,['linearization'], Nonlinear curve fitting,seg_367,"most of the available optimization algorithms build on the same idea of linearization; i.e.,"
1400,1,"['design matrix', 'linear', 'model', 'linear model', 'design', 'algorithm', 'least squares', 'convergence', 'numerical']", Nonlinear curve fitting,seg_367,"in which d f denotes the gradient matrix of derivatives of f with respect to β. this effectively becomes a design matrix of a linear model, and you can proceed from a starting guess at β to find an approximate least squares fit of δ. then you replace β by β + δ and repeat until convergence. variations on this basic algorithm include numerical computation of d f and techniques to avoid instability if the starting guess is too far from the optimum."
1401,1,"['function', 'glm']", Nonlinear curve fitting,seg_367,"to perform the optimization in r, you can use the nls function, which is broadly similar to lm and glm."
1402,1,"['model', 'data', 'set', 'data set', 'exponential', 'simulated']", Basic usage,seg_369,"in this section, we use a simulated data set just so that we know what we are doing. the model is a simple exponential decay."
1403,1,"['simulated', 'data']", Basic usage,seg_369,the simulated data can be seen in figure 16.1.
1404,1,"['glm', 'model', 'linear', 'parameters', 'factors', 'variables', 'data', 'expected value', 'interactions']", Basic usage,seg_369,"we now fit the model to data using nls. unlike lm and glm, the model formula for nls does not use the special codings for linear terms, grouping factors, interactions, etc. instead, the right-hand side is an explicit expression to calculate the expected value of the left-hand side. this can depend on external variables as well as the parameters, so we need to specify which is which. the simplest way to do this is to specify a named vector (or a named list) of starting values."
1405,1,"['parameter', 'variable', 'algorithm']", Basic usage,seg_369,"notice that nls treats t as a variable and not a parameter because it is not mentioned in the start argument. whenever the fitting algorithm needs to evaluate a exp(-alpha t), t is taken from the variable in the"
1406,1,['algorithm'], Basic usage,seg_369,"* * global environment, whereas a and alpha are varied by the algorithm."
1407,1,"['glm', 't test', 'nonlinear', 'parameter', 'tests', 'test', 'hypothesis']", Basic usage,seg_369,"the general form of the output is quite similar to that of glm, so we shall not dwell too long upon it. one thing that might be noted is that the t test and p-value stated for each parameter are tests for a hypothesis that the parameter is zero, which is often quite meaningless for nonlinear models."
1408,1,"['parameters', 'nonlinear', 'convergence']", Finding starting values,seg_371,"in the previous section, we had quite fast convergence, even though the initial guess of parameters was (deliberately) rather badly off. unfortunately, things are not always that simple; convergence of nonlinear models can depend critically on having good starting values. even when the algorithm is fairly robust, we at least need to get the order of magnitude right."
1409,1,"['estimation', 'slopes', 'transformation']", Finding starting values,seg_371,"methods for obtaining starting values will most often rely on an analysis of the functional form; common techniques involve transformation to linearity and the estimation of “landmarks” such asymptotes, maximum points, and initial slopes."
1410,1,"['homogeneous', 'data', 'set', 'data set']", Finding starting values,seg_371,"to illustrate this, we again consider the juul data. this time we focus on the relation between age and height. to obtain a reasonably homogeneous data set, we look at males only and subset the data to the ages between 5 and 20."
1411,1,"['plot', 'linear', 'data']", Finding starting values,seg_371,"a plot of the data is shown in figure 16.2. the plot looks linear over a large portion of its domain, but there is some levelling off at the right end and of course it is basic human biology that we stop growing at some point in the later teens."
1412,1,['curve'], Finding starting values,seg_371,the gompertz curve is often used to describe growth. it can be expressed in the following form:
1413,1,"['curve', 'parameters', 'location', 'level']", Finding starting values,seg_371,"the curve has a sigmoid shape, approaching a constant level α as x increases and (in principle) zero for large negative x. the β and γ parameters determine the location and steepness of the transition."
1414,1,"['linear', 'nonlinear']", Finding starting values,seg_371,"to obtain starting values for a nonlinear fit, one approach is to notice that the relation between y and x is something like log-log linear. specifically, we can rewrite the relation as"
1415,0,[], Finding starting values,seg_371,"which we may rearrange and take logarithms on both sides again, yielding"
1416,1,"['plot', 'linear', 'transformed', 'parameters', 'asymptotic', 'data']", Finding starting values,seg_371,"that means that if we can come up with a guess for α, then we can guess the two other parameters by a linear fit to transformed data. since α is the asymptotic maximum, a guess of α = 200 could be reasonable. with this guess, we can make a plot that should show an approximately linear relationship (log 200 ≈ 5.3):"
1417,1,"['plot', 'linear', 'residual', 'regression analysis', 'regression', 'distribution', 'linearized', 'linear regression', 'variance']", Finding starting values,seg_371,"notice that we got a warning that an nan (not a number) value was produced. this is because one individual was taller than 200 cm, and we therefore tried to take the logarithm of a negative value. the linearized plot shows a clearly nonconstant variance and probably also some asymmetry of the residual distribution, so the assumptions for linear regression analysis are clearly violated. however, it is good enough for our purpose, and a linear fit gives"
1418,1,['parameters'], Finding starting values,seg_371,"accordingly, an initial guess of the parameters is"
1419,1,['curve'], Finding starting values,seg_371,supplying these guesses to nls and fitting the gompertz curve yields
1420,1,"['estimates', 'estimation', 'transformations', 'parameter', 'variation', 'function']", Finding starting values,seg_371,"the final estimates are quite a bit different from the starting values. this reflects the crudeness of the estimation methods used for the starting values. in particular, we used transformations that were based on the mathematical form of the function but did not take the structure of the error variation into account. also, the important parameter α was obtained by eye."
1421,1,"['curve', 'model', 'data', 'estimate']", Finding starting values,seg_371,"looking at the fitted model, however, it is not reassuring that the final estimate for α suggests that boys would continue growing until they are 243 cm tall (for readers in nonmetric countries, that is almost eight feet!). possibly, the gompertz curve is just not a good fit for these data."
1422,1,"['curve', 'data']", Finding starting values,seg_371,we can overlay the original data with the fitted curve as follows (figure 16.4)
1423,1,"['plot', 'model', 'fitted values', 'transforming', 'dispersion']", Finding starting values,seg_371,"the plot suggests that there is a tendency for the dispersion to increase with increasing fitted values, so we attempt a log-scale fit. this can be done expediently by transforming both sides of the model formula."
1424,1,"['plot', 'curve', 'estimates', 'distribution', 'parameter']", Finding starting values,seg_371,"on the log-scale plot (figure 16.5), the distribution around the curve appears to be more stable. the parameter estimates did not change much, although the maximum height is now increased by a further 13 cm (5 inches) and the γ parameter is reduced to compensate."
1425,1,"['curve', 'range', 'plots', 'data']", Finding starting values,seg_371,"closer inspection of the plots (whether on log scale or not), however, reveals that the gompertz curve tends to overshoot the data points at the right end, where a much flatter curve would fit the data in the range from 15 years upwards. although visually there is a nice overall fit, this is not hard to obtain for a three-parameter family of curves, and the gompertz curves seem unable to fit the characteristic patterns of human growth."
1426,1,"['functions', 'model', 'method', 'data', 'sets', 'function', 'data sets']", Selfstarting models,seg_373,"finding starting values is an art rather than a craft, but once a stable method has been found, it may be reasonable to assume that will apply to most data sets from a given model. nls allows the nice feature that the procedure for calculating starting values can be embodied in the expressions that are used on the right-hand side of the model formula. such functions are by convention named starting with “ss”, and r 2.6.2 comes with 10 of these built-in. in particular, there is in fact an ssgompertz function, so we could have saved ourselves much of the trouble of the previous section by just writing"
1427,1,"['parameters', 'parameter']", Selfstarting models,seg_373,"notice, though, that the parameterization is different: the parameter b3 is actually eγ, whereas the two other parameters are recognized as α and β."
1428,1,['model'], Selfstarting models,seg_373,"one minor drawback of self-starting models is that you cannot just transform them if you want to see if the model fits better on, for example, a log scale. in other words, this fails:"
1429,1,"['parameters', 'error']", Selfstarting models,seg_373,"the error message means, in essence, that the self-start machinery is turned off, so nls tries a wild guess, setting all parameters to 1, and then fails to converge from that starting point."
1430,1,['expected value'], Selfstarting models,seg_373,"using expression log(ssgompertz(age, asym, b2, b3)) to compute the expected value of log(height) is not a problem (in itself). we can take the starting values from the untransformed fit but this is still not enough to make things work."
1431,1,"['model', 'transformed', 'parameters', 'failure', 'fitted values', 'convergence', 'process']", Selfstarting models,seg_373,"there is a hitch: ssgompertz returns a gradient attribute along with the fitted values. this is the derivative of the fitted value with respect to each of the model parameters. this speeds up the convergence process for the original model but is plainly wrong for the transformed model, where it causes convergence failure. we could patch this up by calculating the correct gradient, but it is expedient simply to discard the attribute by taking as.vector."
1432,1,"['function', 'model']", Selfstarting models,seg_373,"it is possible to write your own self-starting models. it is not hard once you have some experience with r programming, but we shall not go into details here. the essence is that you need two basic items: the model expression and a function that calculates the starting values. you must ensure that these adhere to some formal requirements, and then a constructor function selfstart can be called to create the actual self-starting function."
1433,1,"['confidence intervals', 'sum of squared', 't distribution', 'function', 'logistic', 'intervals', 'profiling', 'confidence', 'glm', 'nonlinear regression', 'nonlinear', 'regression', 'distribution', 'plotting', 'method', 'likelihood', 'deviations', 'normal', 'profile']", Profiling,seg_375,"we discussed profiling before in connection with glm and logistic regression in section 13.3. for nonlinear regression, there are some slight differences: the function that is being profiled is not the likelihood function but the sum of squared deviations, and the approximate confidence intervals are based on the t distribution rather than the normal distribution. also, the plotting method does not by default use the signed version of the profile, just the square root of the difference in the sum of squared deviations."
1434,1,"['model', 'distribution', 'parameter']", Profiling,seg_375,profiling is designed to eliminate parameter curvature. the same model can be formulated using different parameterizations (such as when gompertz curves could be defined using γ or b3 = eγ). the choice of parameterization can have a substantial influence on whether the distribution of the
1435,1,"['standard errors', 'model', 'transformed', 'confidence intervals', 'interval', 'symmetric', 'errors', 'intervals', 'normal', 'parameter', 'standard', 'confidence', 'transform', 'confidence interval']", Profiling,seg_375,"estimate is approximately normal or not, and this in turn means that the use of symmetric confidence intervals based on the standard errors from the model summary can be misleading. profile-based confidence intervals do not depend on parameterization — if you transform a parameter, the ends of the confidence interval are just transformed in the same way."
1436,1,"['linear', 'model', 'independent', 'linear model', 'confidence intervals', 't distribution', 'intervals', 'distribution', 'parameter', 'confidence']", Profiling,seg_375,"there is, however, also intrinsic curvature of the models. this describes how far the model is from an approximating linear model. this kind of curvature is independent of parameterization and is harder to adjust for than parameter curvature. the effect of intrinsic curvature is that the t distribution used for the calculation of profile-based confidence intervals is not exactly the right distribution to use. experience suggests that this effect is usually much smaller than the distortions caused by parameter curvature."
1437,1,"['transformation', 'plots']", Profiling,seg_375,"for the gompertz fit (after log transformation), we get the plots shown in figure 16.6."
1438,1,"['normal approximation', 'confidence intervals', 'plots', 'linear', 'approximation', 'symmetric', 'intervals', 'standard', 'confidence', 'standard errors', 'asymmetric', 'errors', 'normal', 'profile']", Profiling,seg_375,"the plots show that there is a marked curvature for the α and β parameters, reflected in the curved and asymmetric profiles, whereas the γ profile is more linear and symmetric. this is also seen when comparing the profile-based confidence intervals with those of confint.default, which uses the normal approximation and the approximate standard errors."
1439,1,"['parameters', 'observations', 'nonlinear', 'adjusted', 'function', 'convergence', 'number of observations']", Finer control of the fitting algorithm,seg_377,"the juul example that has been used in this chapter has been quite benign because there are a large number of observations and an objective function that is relatively smooth as a function of the parameters. however, convergence problems easily come up in less nice examples. nonlinear optimization is simply a tricky topic, to which we have no chance of doing justice in this short chapter. the algorithms have several parameters that can be adjusted in order to help convergence, but since we are not describing the algorithms, it is hardly possible to give more than a feeling for what can be done."
1440,1,"['curve', 'parameters', 'estimate', 'algorithm', 'function']", Finer control of the fitting algorithm,seg_377,"the possibility of supplying a gradient of the fitted curve with respect to parameters was mentioned earlier. if the curve is given by a simple mathematical expression, then the deriv function can even be used to generate the gradient automatically. if a gradient is not available, then the algorithm will estimate it numerically; in practice, this often turns out to be equally fast."
1441,1,"['parameters', 'algorithm', 'set', 'function', 'control']", Finer control of the fitting algorithm,seg_377,"the nls function features a trace argument that, if set to true, allows you to follow the parameters and the ssd iteration by iteration. this is sometimes useful to get a handle on what is happening, for instance whether the algorithm is making unreasonably large jumps. to actually modify the behaviour, there is a single control argument, which can be set to the return value of nls.control, which in turn has arguments to set iteration limits and tolerances (and more)."
1442,1,"['method', 'algorithm']", Finer control of the fitting algorithm,seg_377,"you can switch out the entire fitting method by using the algorithm argument. apart from the default algorithm, this allows the settings ""plinear"" and ""port"". the former allows models of the form"
1443,1,"['linear', 'model', 'parameters', 'algorithm', 'regression', 'set']", Finer control of the fitting algorithm,seg_377,"that are partially linear since the αi can be determined by multiple linear regression if the βi are considered fixed. to specify models with more than one term, you let the expression on the right-hand side of the model formula return a matrix instead of a vector. the latter algorithm uses a routine from the port library from lucent technologies; this in particular allows you to set contraints on parameters by using the upper and lower arguments to nls."
1444,1,"['cases', 'function']", Finer control of the fitting algorithm,seg_377,"it should be noted that all the available algorithms operate under the implicit assumption that the ssd(β) is fairly smooth and well behaved, with a well-defined global minimum and no other local minima nearby. there are cases where this assumption is not warranted. in such cases, you might attack the minimization problem directly using the optim function."
1445,1,"['model', 'data']", Exercises,seg_379,16.1 try fitting the gompertz model for girls in the juul data. how would you go about testing whether the same model fits both genders?
1446,1,"['poisson', 'model', 'confidence intervals', 'algorithm', 'data', 'distribution', 'intervals', 'set', 'confidence', 'profiling', 'transform', 'poisson distribution', 'experiments', 'variance']", Exercises,seg_379,"16.2 the philion data contain four small-sample ec50 experiments that are somewhat tricky to handle. we suggest the model y = ymax/(1 + (x/β)α). it may be useful to transform y by the square root since the data are counts, and this stabilizes the variance of the poisson distribution. consider how to obtain starting values for the model, and fit it with nls. the ""port"" algorithm seems more stable for these data. for profiling and confidence intervals, it seems to help if you set the alphamax argument to 0.2."
1447,1,"['model', 'data']", Exercises,seg_379,"16.3 (theoretical) continuing with the philion data, consider what happens if you modify the model to be y = ymax/(1 + x/β)α."
1448,0,[],A Obtaining and installing R and the ISwR package,seg_381,the way to obtain r is to download it from one of the cran (comprehensive r archive network) sites. the main site is
1449,0,[],A Obtaining and installing R and the ISwR package,seg_381,it has a number of mirror sites worldwide that may be closer to you and give faster download times.
1450,1,"['information', 'vary']",A Obtaining and installing R and the ISwR package,seg_381,"installation details tend to vary over time, so you should read the accompanying documents and any other information offered on cran."
1451,1,['process'],A Obtaining and installing R and the ISwR package,seg_381,"as of this writing, the version for recent variants of microsoft windows comes as a single r-2.6.2-win32.exe file, on which you simply double-click with the mouse and then follow the on-screen instructions. when the process is completed, you will have an entry under programs on the start menu for invoking r, as well as a desktop icon."
1452,1,"['standard', 'distributions']",A Obtaining and installing R and the ISwR package,seg_381,"for linux distributions that use the rpm package format (primarily redhat, fedora, and suse), .rpm files of r and the recommended add-on packages can be installed using the rpm command and the respective system software management tools. fedora now has r in its standard repositories, and it is also in the repository of opensuse.org. debian"
1453,1,['cases'],A Obtaining and installing R and the ISwR package,seg_381,"packages can be accessed through apt, the debian package maintenance tool, as can packages for ubuntu (in both cases, make sure that you get the r-recommended package). further details are in the faq."
1454,0,[],A Obtaining and installing R and the ISwR package,seg_381,"for the macintosh platforms, only os x 10.2 and above are supported. installation is by downloading the disk image r-2.6.2.dmg and doubleclicking the “r.mpkg” icon found inside it."
1455,1,['process'],A Obtaining and installing R and the ISwR package,seg_381,"installation of rfrom source code is possible on all supported platforms, although not quite trivial on windows, mainly because the build environment is not part of the system. on unix-like systems (macintosh os x included), the process can be as simple as unpacking the sources and writing"
1456,0,[],A Obtaining and installing R and the ISwR package,seg_381,./configure make make install
1457,0,[],A Obtaining and installing R and the ISwR package,seg_381,"the above works on widely used platforms, provided that the relevant compilers and support libraries are installed. if your system is more esoteric or you want to use special compilers or libraries, then you may need to dig deeper."
1458,1,['information'],A Obtaining and installing R and the ISwR package,seg_381,"for windows, the directory src/gnuwin32 has an install file with detailed information about the procedure to follow."
1459,1,"['data', 'sets', 'data sets']",A Obtaining and installing R and the ISwR package,seg_381,"to work through the examples and exercises in this book, you should install the iswr package, which contains the data sets."
1460,0,[],A Obtaining and installing R and the ISwR package,seg_381,"assuming that you are connected to the internet, you can start r and from the windows and macintosh versions use their convenient menu interfaces."
1461,0,[],A Obtaining and installing R and the ISwR package,seg_381,"on other platforms, you can type"
1462,0,[],A Obtaining and installing R and the ISwR package,seg_381,"install.packages(""iswr"")"
1463,1,['location'],A Obtaining and installing R and the ISwR package,seg_381,this will give off a harmless warning and install the package in the default location.
1464,1,['location'],A Obtaining and installing R and the ISwR package,seg_381,"on unix and linux systems you will need superuser permissions to install in the system location. similarly, you may require administrator access on some windows versions."
1465,1,"['variable', 'set']",A Obtaining and installing R and the ISwR package,seg_381,otherwise you can set up a private library directory and install into that. set the r_libs environment variable to use your private library subsequently. further details can be found on the help page for library.
1466,0,[],A Obtaining and installing R and the ISwR package,seg_381,"if your r machine is not connected to the internet, you can also download the package as a file via a different computer. for windows and macintosh, you should get the binary package (.zip or .tgz extension) and then installation from a local file is possible via a menu entry. for unix and linux, you can issue the following at the shell prompt (the -l option allows you to give a private library if needed):"
1467,0,[],A Obtaining and installing R and the ISwR package,seg_381,r cmd install iswr
1468,0,[],A Obtaining and installing R and the ISwR package,seg_381,information and further internet resources for r can be obtained from cran and the r homepage at
1469,0,[],A Obtaining and installing R and the ISwR package,seg_381,"notice in particular the mailing lists, the user-contributed documents, and the faqs."
1470,0,[],B Data sets in the ISwR package,seg_383,serum igm in 298 children aged 6 months to 6 years.
1471,1,"['statistics', 'table']",B Data sets in the ISwR package,seg_383,"d.g. altman (1991), practical statistics for medical research, table 3.2, chapman & hall."
1472,1,['data'],B Data sets in the ISwR package,seg_383,alkfos alkaline phosphatase data
1473,1,"['treatment', 'measurements', 'trial']",B Data sets in the ISwR package,seg_383,repeated measurements of alkaline phosphatase in a randomized trial of tamoxifen treatment of breast cancer patients.
1474,1,"['variables', 'observations', 'data']",B Data sets in the ISwR package,seg_383,a data frame with 43 observations on the following 8 variables.
1475,1,['concentration'],B Data sets in the ISwR package,seg_383,"grp a numeric vector, group code (1=placebo, 2=tamoxifen). c0 a numeric vector, concentration at baseline. c3 a numeric vector, concentration after 3 months. c6 a numeric vector, concentration after 6 months. c9 a numeric vector, concentration after 9 months. c12 a numeric vector, concentration after 12 months. c18 a numeric vector, concentration after 18 months. c24 a numeric vector, concentration after 24 months."
1476,1,['data'],B Data sets in the ISwR package,seg_383,original data.
1477,0,[],B Data sets in the ISwR package,seg_383,"b. kristensen et al. (1994), tamoxifen and bone metabolism in postmenopausal low-risk breast cancer patients: a randomized study. journal of clinical oncology, 12(2):992–997."
1478,1,['trial'],B Data sets in the ISwR package,seg_383,ashina ashina’s crossover trial
1479,1,"['levels', 'treatment', 'data', 'trial']",B Data sets in the ISwR package,seg_383,the ashina data frame has 16 rows and 3 columns. it contains data from a crossover trial for the effect of an no synthase inhibitor on headaches. visual analog scale recordings of pain levels were made at baseline and at five time points after infusion of the drug or placebo. a score was calculated as the sum of the differences from baseline. data were recorded during two sessions for each patient. six patients were given treatment on the first occasion and the placebo on the second. ten patients had placebo first and then treatment. the order of treatment and the placebo was randomized.
1480,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1481,1,['treatment'],B Data sets in the ISwR package,seg_383,"vas.active a numeric vector, summary score when given active substance. vas.plac a numeric vector, summary score when given placebo treatment. grp a numeric vector code, 1: placebo first, 2: active first."
1482,1,['data'],B Data sets in the ISwR package,seg_383,original data.
1483,1,['trial'],B Data sets in the ISwR package,seg_383,"m.ashina et al. (1999), effect of inhibition of nitric oxide synthase on chronic tension-type headache: a randomised crossover trial. lancet 353, 287–289"
1484,0,[],B Data sets in the ISwR package,seg_383,bcmort breast cancer mortality
1485,0,[],B Data sets in the ISwR package,seg_383,danish study on the effect of screening for breast cancer.
1486,1,"['variables', 'observations', 'data']",B Data sets in the ISwR package,seg_383,a data frame with 24 observations on the following 4 variables.
1487,1,"['factor', 'levels']",B Data sets in the ISwR package,seg_383,"age a factor with levels 50-54, 55-59, 60-64, 65-69, 70-74, and 75-79. cohort a factor with levels study gr., nat.ctr., hist.ctr., and hist.nat.ctr.. bc.deaths a numeric vector, number of breast cancer deaths. p.yr a numeric vector, person-years under study."
1488,1,"['range', 'population', 'control']",B Data sets in the ISwR package,seg_383,"four cohorts were collected. the “study group” consists of the population of women in the appropriate age range in copenhagen and frederiksberg after the introduction of routine mammography screening. the “national control group” consisted of the population in the parts of denmark in which routine mammography screening was not available. these two groups were both collected in the years 1991–2001. the “historical control group” and the “historical national control group” are similar cohorts from 10 years earlier (1981–1991), before the introduction of screening in copenhagen and frederiksberg. the study group comprises the entire population, not just those accepting the invitation to be screened."
1489,0,[],B Data sets in the ISwR package,seg_383,"a.h. olsen et al. (2005), breast cancer mortality in copenhagen after introduction of mammography screening. british medical journal, 330: 220–222."
1490,0,[],B Data sets in the ISwR package,seg_383,bp.obese obesity and blood pressure
1491,1,"['sample', 'random', 'data', 'random sample']",B Data sets in the ISwR package,seg_383,the bp.obese data frame has 102 rows and 3 columns. it contains data from a random sample of mexican-american adults in a small california town.
1492,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1493,1,['tables'],B Data sets in the ISwR package,seg_383,"sex a numeric vector code, 0: male, 1: female. obese a numeric vector, ratio of actual weight to ideal weight from new york metropolitan life tables. bp a numeric vector,systolic blood pressure (mm hg)."
1494,1,['statistics'],B Data sets in the ISwR package,seg_383,"b.w. brown and m. hollander (1977), statistics: a biomedical introduction, wiley."
1495,0,[],B Data sets in the ISwR package,seg_383,caesarean caesarean section and maternal shoe size
1496,1,['table'],B Data sets in the ISwR package,seg_383,the table caesar.shoe contains the relation between caesarean section and maternal shoe size (uk sizes!).
1497,0,[],B Data sets in the ISwR package,seg_383,a matrix with two rows and six columns.
1498,1,"['statistics', 'table']",B Data sets in the ISwR package,seg_383,"d.g. altman (1991), practical statistics for medical research, table 10.1, chapman & hall."
1499,1,['data'],B Data sets in the ISwR package,seg_383,coking coking data
1500,1,"['data', 'experiment']",B Data sets in the ISwR package,seg_383,the coking data frame has 18 rows and 3 columns. it contains the time to coking in an experiment where the oven width and temperature were varied.
1501,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1502,1,"['factor', 'levels']",B Data sets in the ISwR package,seg_383,"width a factor with levels 4, 8, and 12, giving the oven width in inches. temp a factor with levels 1600 and 1900, giving the temperature in fahrenheit. time a numeric vector, time to coking."
1503,1,"['statistics', 'probability']",B Data sets in the ISwR package,seg_383,"r.a. johnson (1994), miller and freund’s probability and statistics for engineers, 5th ed., prentice-hall."
1504,1,"['function', 'data']",B Data sets in the ISwR package,seg_383,cystfibr cystic fibrosis lung function data
1505,1,"['function', 'data']",B Data sets in the ISwR package,seg_383,the cystfibr data frame has 25 rows and 10 columns. it contains lung function data for cystic fibrosis patients (7–23 years old).
1506,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1507,1,"['normal', 'residual']",B Data sets in the ISwR package,seg_383,"age a numeric vector, age in years. sex a numeric vector code, 0: male, 1:female. height a numeric vector, height (cm). weight a numeric vector, weight (kg). bmp a numeric vector, body mass (% of normal). fev1 a numeric vector, forced expiratory volume. rv a numeric vector, residual volume. frc a numeric vector, functional residual capacity. tlc a numeric vector, total lung capacity. pemax a numeric vector, maximum expiratory pressure."
1508,1,"['statistics', 'table']",B Data sets in the ISwR package,seg_383,"d.g. altman (1991), practical statistics for medical research, table 12.11, chapman & hall."
1509,0,[],B Data sets in the ISwR package,seg_383,"o’neill et al. (1983), the effects of chronic hyperinflation, nutritional status, and posture on respiratory muscle strength in cystic fibrosis, am. rev. respir. dis., 128:1051–1054."
1510,0,[],B Data sets in the ISwR package,seg_383,eba1977 lung cancer incidence in four danish cities 1968–
1511,1,"['data', 'cases', 'set', 'data set', 'population']",B Data sets in the ISwR package,seg_383,this data set contains counts of incident lung cancer cases and population size in four neighbouring danish cities by age group.
1512,1,"['variables', 'observations', 'data']",B Data sets in the ISwR package,seg_383,a data frame with 24 observations on the following 4 variables:
1513,1,"['levels', 'factor', 'cases']",B Data sets in the ISwR package,seg_383,"city a factor with levels fredericia, horsens, kolding, and vejle. age a factor with levels 40-54, 55-59, 60-64, 65-69, 70-74, and 75+. pop a numeric vector, number of inhabitants. cases a numeric vector, number of lung cancer cases."
1514,1,['data'],B Data sets in the ISwR package,seg_383,"these data were “at the center of public interest in denmark in 1974”, according to erling andersen’s paper. the city of fredericia has a substantial petrochemical industry in the harbour area."
1515,1,"['poisson', 'statistics', 'rates']",B Data sets in the ISwR package,seg_383,"e.b. andersen (1977), multiplicative poisson models with unequal cell rates, scandinavian journal of statistics, 4:153–158."
1516,0,[],B Data sets in the ISwR package,seg_383,"j. clemmensen et al. (1974), ugeskrift for læger, pp. 2260–2268."
1517,1,['data'],B Data sets in the ISwR package,seg_383,the energy data frame has 22 rows and 2 columns. it contains data on the energy expenditure in groups of lean and obese women.
1518,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1519,1,"['factor', 'levels']",B Data sets in the ISwR package,seg_383,"expend a numeric vector, 24 hour energy expenditure (mj). stature a factor with levels lean and obese."
1520,1,"['statistics', 'table']",B Data sets in the ISwR package,seg_383,"d.g. altman (1991), practical statistics for medical research, table 9.4, chapman & hall."
1521,1,['rates'],B Data sets in the ISwR package,seg_383,"ewrates rates of lung and nasal cancer mortality, and total"
1522,1,['rates'],B Data sets in the ISwR package,seg_383,"england and wales mortality rates from lung cancer, nasal cancer, and all causes, 1936–1980. the 1936 rates are repeated as 1931 rates in order to accommodate follow-up for the nickel study."
1523,1,"['variables', 'observations', 'data']",B Data sets in the ISwR package,seg_383,a data frame with 150 observations on the following 5 variables:
1524,1,['rate'],B Data sets in the ISwR package,seg_383,"year calendar period, 1931: 1931–35, 1936: 1936–40, . . . . age age class, 10: 10–14, 15:15–19, . . . . lung lung cancer mortality rate per 1 million person-years nasal nasal cancer mortality rate per 1 million person-years other all cause mortality rate per 1 million person-years"
1525,0,[],B Data sets in the ISwR package,seg_383,taken from the “epi” package by bendix carstensen et al.
1526,1,"['design', 'statistical']",B Data sets in the ISwR package,seg_383,"n.e. breslow, and n. day (1987). statistical methods in cancer research. volume ii: the design and analysis of cohort studies, appendix ix. iarc scientific publications, lyon."
1527,0,[],B Data sets in the ISwR package,seg_383,fake.trypsin trypsin by age groups
1528,1,"['levels', 'data']",B Data sets in the ISwR package,seg_383,the trypsin data frame has 271 rows and 3 columns. serum levels of immunoreactive trypsin in healthy volunteers (faked!).
1529,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1530,1,"['factor', 'levels']",B Data sets in the ISwR package,seg_383,"trypsin a numeric vector, serum-trypsin in ng/ml. grp a numeric vector, age coding. see below. grpf a factor with levels 1: age 10–19, 2: age 20–29, 3: age 30–39, 4: age 40–49, 5: age 50–59, and 6: age 60–69."
1531,1,['simulated'],B Data sets in the ISwR package,seg_383,data have been simulated to match given group means and sd.
1532,1,"['statistics', 'table']",B Data sets in the ISwR package,seg_383,"d.g. altman (1991), practical statistics for medical research, table 9.12, chapman & hall."
1533,0,[],B Data sets in the ISwR package,seg_383,graft.vs.host graft versus host disease
1534,1,"['variables', 'data', 'associated']",B Data sets in the ISwR package,seg_383,the gvhd data frame has 37 rows and 7 columns. it contains data from patients receiving a nondepleted allogenic bone marrow transplant with the purpose of finding variables associated with the development of acute graft-versus-host disease.
1535,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1536,0,[],B Data sets in the ISwR package,seg_383,"pnr a numeric vector patient number. rcpage a numeric vector, age of recipient (years). donage a numeric vector, age of donor (years). type a numeric vector, type of leukaemia coded 1: aml, 2: all, 3: cml for acute myeloid, acute lymphatic, and chronic myeloid leukaemia. preg a numeric vector code indicating whether donor has been pregnant. 0: no, 1: yes. index a numeric vector giving an index of mixed epidermal celllymphocyte reactions. gvhd a numeric vector code, graft-versus-host disease, 0: no, 1: yes. time a numeric vector, follow-up time dead a numeric vector code, 0: no (censored), 1: yes"
1537,1,['statistics'],B Data sets in the ISwR package,seg_383,"d.g. altman (1991), practical statistics for medical research, exercise 12.3, chapman & hall."
1538,1,['rates'],B Data sets in the ISwR package,seg_383,heart.rate heart rates after enalaprilat
1539,1,"['failure', 'data']",B Data sets in the ISwR package,seg_383,"the heart.rate data frame has 36 rows and 3 columns. it contains data for nine patients with congestive heart failure before and shortly after administration of enalaprilat, in a balanced two-way layout."
1540,0,[],B Data sets in the ISwR package,seg_383,heart.rate
1541,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1542,1,"['rate', 'levels', 'factor']",B Data sets in the ISwR package,seg_383,"hr a numeric vector, heart rate in beats per minute. subj a factor with levels 1 to 9. time a factor with levels 0 (before), 30, 60, and 120 (minutes after administration)."
1543,1,"['statistics', 'table']",B Data sets in the ISwR package,seg_383,"d.g. altman (1991), practical statistics for medical research, table 12.2, chapman & hall."
1544,0,[],B Data sets in the ISwR package,seg_383,hellung growth of tetrahymena cells
1545,1,"['concentration', 'data']",B Data sets in the ISwR package,seg_383,the hellung data frame has 51 rows and 3 columns. diameter and concentration of tetrahymena cells with and without glucose added to growth medium.
1546,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1547,1,['concentration'],B Data sets in the ISwR package,seg_383,"glucose a numeric vector code, 1: yes, 2: no. conc a numeric vector, cell concentration (counts/ml). diameter a numeric vector, cell diameter (µm)."
1548,1,['table'],B Data sets in the ISwR package,seg_383,"d. kronborg and l.t. skovgaard (1990), regressionsanalyse, table 1.1, fadls forlag (in danish)."
1549,1,"['data', 'paired']",B Data sets in the ISwR package,seg_383,the intake data frame has 11 rows and 2 columns. it contains paired values of energy intake for 11 women.
1550,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1551,0,[],B Data sets in the ISwR package,seg_383,"pre a numeric vector, premenstrual intake (kj). post a numeric vector, postmenstrual intake (kj)."
1552,1,"['statistics', 'table']",B Data sets in the ISwR package,seg_383,"d.g. altman (1991), practical statistics for medical research, table 9.3, chapman & hall."
1553,1,['data'],B Data sets in the ISwR package,seg_383,juul juul’s igf data
1554,1,"['sample', 'factor', 'data', 'observation', 'distribution']",B Data sets in the ISwR package,seg_383,"the juul data frame has 1339 rows and 6 columns. it contains a reference sample of the distribution of insulin-like growth factor (igf-i), one observation per subject in various ages, with the bulk of the data collected in connection with school physical examinations."
1555,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1556,1,['factor'],B Data sets in the ISwR package,seg_383,"age a numeric vector (years). menarche a numeric vector. has menarche occurred (code 1: no, 2: yes)? sex a numeric vector (1: boy, 2: girl). igf1 a numeric vector, insulin-like growth factor (µg/l). tanner a numeric vector, codes 1–5: stages of puberty ad modum tanner. testvol a numeric vector, testicular volume (ml)."
1557,1,['data'],B Data sets in the ISwR package,seg_383,original data.
1558,1,['data'],B Data sets in the ISwR package,seg_383,"juul2 juul’s igf data, extended version"
1559,1,['data'],B Data sets in the ISwR package,seg_383,the juul2 data frame has 1339 rows and 8 columns; extended version of juul.
1560,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1561,1,['factor'],B Data sets in the ISwR package,seg_383,"age a numeric vector (years). height a numeric vector (cm). menarche a numeric vector. has menarche occurred (code 1: no, 2: yes)? sex a numeric vector (1: boy, 2: girl). igf1 a numeric vector, insulin-like growth factor (µg/l). tanner a numeric vector, codes 1–5: stages of puberty ad modum tanner. testvol a numeric vector, testicular volume (ml). weight a numeric vector, weight (kg)."
1562,1,['data'],B Data sets in the ISwR package,seg_383,original data.
1563,1,['data'],B Data sets in the ISwR package,seg_383,kfm breast-feeding data
1564,1,['data'],B Data sets in the ISwR package,seg_383,the kfm data frame has 50 rows and 7 columns. it was collected by kim fleischer michaelsen and contains data for 50 infants of age approximately 2 months. they were weighed immediately before and
1565,1,['data'],B Data sets in the ISwR package,seg_383,after each breast feeding. and the measured intake of breast milk was registered along with various other data.
1566,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1567,1,"['factor', 'levels']",B Data sets in the ISwR package,seg_383,"no a numeric vector, identification number. dl.milk a numeric vector, breast-milk intake (dl/24h). sex a factor with levels boy and girl. weight a numeric vector, weight of child (kg). ml.suppl a numeric vector, supplementary milk substitute (ml/24h). mat.weight a numeric vector, weight of mother (kg). mat.height a numeric vector, height of mother (cm)."
1568,1,"['data collection', 'data']",B Data sets in the ISwR package,seg_383,the amount of supplementary milk substitute refers to a period before the data collection.
1569,1,['data'],B Data sets in the ISwR package,seg_383,original data.
1570,0,[],B Data sets in the ISwR package,seg_383,lung methods for determining lung volume
1571,1,['data'],B Data sets in the ISwR package,seg_383,the lung data frame has 18 rows and 3 columns. it contains data on three different methods of determining human lung volume.
1572,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1573,1,"['factor', 'levels', 'method']",B Data sets in the ISwR package,seg_383,"volume a numeric vector, measured lung volume. method a factor with levels a, b, and c. subject a factor with levels 1–6."
1574,1,['statistics'],B Data sets in the ISwR package,seg_383,"anon. (1977), exercises in applied statistics, exercise 4.15, dept. of theoretical statistics, aarhus university."
1575,1,['data'],B Data sets in the ISwR package,seg_383,malaria malaria antibody data
1576,1,['data'],B Data sets in the ISwR package,seg_383,the malaria data frame has 100 rows and 4 columns.
1577,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1578,1,['level'],B Data sets in the ISwR package,seg_383,"subject subject code. age age in years. ab antibody level. mal a numeric vector code, malaria: 0: no, 1: yes."
1579,1,"['sample', 'random', 'observations', 'random sample']",B Data sets in the ISwR package,seg_383,"a random sample of 100 children aged 3–15 years from a village in ghana. the children were followed for a period of 8 months. at the beginning of the study, values of a particular antibody were assessed. based on observations during the study period, the children were categorized into two groups: individuals with and without symptoms of malaria."
1580,1,['data'],B Data sets in the ISwR package,seg_383,unpublished data.
1581,0,[],B Data sets in the ISwR package,seg_383,melanom survival after malignant melanoma
1582,1,['data'],B Data sets in the ISwR package,seg_383,"the melanom data frame has 205 rows and 7 columns. it contains data relating to the survival of patients after an operation for malignant melanoma, collected at odense university hospital by k.t. drzewiecki."
1583,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1584,1,['observation'],B Data sets in the ISwR package,seg_383,"no a numeric vector, patient code. status a numeric vector code, survival status; 1: dead from melanoma, 2: alive, 3: dead from other cause. days a numeric vector, observation time. ulc a numeric vector code, ulceration; 1: present, 2: absent. thick a numeric vector, tumor thickness (1/100 mm). sex a numeric vector code; 1: female, 2: male."
1585,1,"['statistical', 'processes']",B Data sets in the ISwR package,seg_383,"p.k. andersen, ø. borgan, r.d. gill, and n. keiding (1991), statistical models based on counting processes, appendix 1, springer-verlag."
1586,0,[],B Data sets in the ISwR package,seg_383,nickel nickel smelters in south wales
1587,1,"['data', 'information']",B Data sets in the ISwR package,seg_383,"the data concern a cohort of nickel smelting workers in south wales, with information on exposure, follow-up period, and cause of death."
1588,1,"['variables', 'observations', 'data']",B Data sets in the ISwR package,seg_383,a data frame containing 679 observations of the following 7 variables:
1589,0,[],B Data sets in the ISwR package,seg_383,"id subject identifier (numeric). icd icd cause of death if dead, 0 otherwise (numeric). exposure exposure index for workplace (numeric) dob date of birth (numeric). age1st age at first exposure (numeric). agein age at start of follow-up (numeric). ageout age at end of follow-up (numeric)."
1590,1,"['data', 'set', 'data set', 'rates']",B Data sets in the ISwR package,seg_383,"taken from the “epi” package by bendix carstensen et al. for comparison purposes, england and wales mortality rates (per 1,000,000 per annum) from lung cancer (icds 162 and 163), nasal cancer (icd 160), and all causes, by age group and calendar period, are supplied in the data set ewrates."
1591,1,"['design', 'statistical']",B Data sets in the ISwR package,seg_383,"n.e. breslow and n. day (1987). statistical methods in cancer research. volume ii: the design and analysis of cohort studies, iarc scientific publications, lyon."
1592,0,[],B Data sets in the ISwR package,seg_383,"nickel.expand nickel smelters in south wales, expanded"
1593,1,"['rates', 'data', 'information']",B Data sets in the ISwR package,seg_383,"the data concern a cohort of nickel smelting workers in south wales, with information on exposure, follow-up period, and cause of death, as in the nickel data. this version has follow-up times split according to age groups and is merged with the mortality rates in ewrates."
1594,1,"['variables', 'observations', 'data']",B Data sets in the ISwR package,seg_383,a data frame with 3724 observations on the following 12 variables:
1595,1,['rate'],B Data sets in the ISwR package,seg_383,"agr age class: 10: 10–14, 15: 15–19, . . . . ygr calendar period, 1931: 1931–35, 1936: 1936–40, . . . . id subject identifier (numeric). icd icd cause of death if dead, 0 otherwise (numeric). exposure exposure index for workplace (numeric). dob date of birth (numeric). age1st age at first exposure (numeric). agein age at start of follow-up (numeric). ageout age at end of follow-up (numeric). lung lung cancer mortality rate per 1 million person-years. nasal nasal cancer mortality rate per 1 million person-years. other all cause mortality rate per 1 million person-years."
1596,1,"['data', 'sets', 'data sets']",B Data sets in the ISwR package,seg_383,computed from nickel and ewrates data sets.
1597,1,"['response', 'data']",B Data sets in the ISwR package,seg_383,philion dose response data
1598,1,['experiments'],B Data sets in the ISwR package,seg_383,four small experiments with the purpose of estimating the ec50 of a biological dose-response relation.
1599,1,"['variables', 'observations', 'data']",B Data sets in the ISwR package,seg_383,a data frame with 30 observations on the following 3 variables:
1600,1,['response'],B Data sets in the ISwR package,seg_383,"experiment a numeric vector; codes 1 through 4 denote the experiment number. dose a numeric vector, the dose. response a numeric vector, the response (counts)."
1601,1,"['poisson', 'data', 'regression']",B Data sets in the ISwR package,seg_383,"these data were discussed on the r mailing lists, initially suggesting a log-linear poisson regression, but actually a relation like y = ymax/(1 + (x/β)α) is more suitable."
1602,1,['data'],B Data sets in the ISwR package,seg_383,"original data from vincent philion, irda, québec."
1603,0,[],B Data sets in the ISwR package,seg_383,the numeric vector react contains differences between two nurses’ determinations of 334 tuberculin reaction sizes.
1604,0,[],B Data sets in the ISwR package,seg_383,"a single vector, differences between reaction sizes in mm."
1605,1,['statistics'],B Data sets in the ISwR package,seg_383,"anon. (1977), exercises in applied statistics, exercise 2.9, dept. of theoretical statistics, aarhus university."
1606,1,['data'],B Data sets in the ISwR package,seg_383,red.cell.folate red cell folate data
1607,1,"['data', 'levels']",B Data sets in the ISwR package,seg_383,the folate data frame has 22 rows and 2 columns. it contains data on red cell folate levels in patients receiving three different methods of ventilation during anesthesia.
1608,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1609,1,"['levels', 'factor', 'concentration']",B Data sets in the ISwR package,seg_383,"folate a numeric vector, folate concentration (µg/l). ventilation a factor with levels n2o+o2,24h: 50% nitrous oxide and 50% oxygen, continuously for 24 hours; n2o+o2,op: 50% nitrous oxide and 50% oxygen, only during operation; o2,24h: no nitrous oxide but 35%–50% oxygen for 24 hours."
1610,1,"['statistics', 'table']",B Data sets in the ISwR package,seg_383,"d.g. altman (1991), practical statistics for medical research, table 9.10, chapman & hall."
1611,1,['rate'],B Data sets in the ISwR package,seg_383,rmr resting metabolic rate
1612,1,"['rate', 'data']",B Data sets in the ISwR package,seg_383,the rmr data frame has 44 rows and 2 columns. it contains the resting metabolic rate and body weight data for 44 women.
1613,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1614,1,['rate'],B Data sets in the ISwR package,seg_383,"body.weight a numeric vector, body weight (kg). metabolic.rate a numeric vector, metabolic rate (kcal/24hr)."
1615,1,['statistics'],B Data sets in the ISwR package,seg_383,"d.g. altman (1991), practical statistics for medical research, exercise 11.2, chapman & hall."
1616,0,[],B Data sets in the ISwR package,seg_383,secher birth weight and ultrasonography
1617,1,"['measurements', 'data']",B Data sets in the ISwR package,seg_383,the secher data frame has 107 rows and 4 columns. it contains ultrasonographic measurements of fetuses immediately before birth and their subsequent birth weight.
1618,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1619,1,['observation'],B Data sets in the ISwR package,seg_383,"bwt a numeric vector, birth weight (g). bpd a numeric vector, biparietal diameter (mm). ad a numeric vector, abdominal diameter (mm). no a numeric vector, observation number."
1620,1,['table'],B Data sets in the ISwR package,seg_383,"d. kronborg and l.t. skovgaard (1990), regressionsanalyse, table 3.1, fadls forlag (in danish). secher et al. (1987), european journal of obstetrics, gynecology, and reproductive biology, 24: 1–11."
1621,0,[],B Data sets in the ISwR package,seg_383,secretin secretin-induced blood glucose changes
1622,1,"['response', 'data', 'experiment']",B Data sets in the ISwR package,seg_383,the secretin data frame has 50 rows and 6 columns. it contains data from a glucose response experiment.
1623,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1624,1,"['sample', 'levels', 'factor', 'level']",B Data sets in the ISwR package,seg_383,"gluc a numeric vector, blood glucose level. person a factor with levels a–e. time a factor with levels 20, 30, 60, 90 (minutes since injection), and pre (before injection). repl a factor with levels a: 1st sample; b: 2nd sample. time20plus a factor with levels 20+: 20 minutes or longer since injection; pre: before injection. time.comb a factor with levels 20: 20 minutes since injection; 30+: 30 minutes or longer since injection; pre: before injection."
1625,1,['measurements'],B Data sets in the ISwR package,seg_383,secretin is a hormone of the duodenal mucous membrane. an extract was administered to five patients with arterial hypertension. primary registrations (double determination) of blood glucose were on graph paper and later quantified with the smallest of the two measurements recorded first.
1626,1,['statistics'],B Data sets in the ISwR package,seg_383,"anon. (1977), exercises in applied statistics, exercise 5.8, dept. of theoretical statistics, aarhus university."
1627,1,['data'],B Data sets in the ISwR package,seg_383,stroke estonian stroke data
1628,1,['cases'],B Data sets in the ISwR package,seg_383,"all cases of stroke in tartu, estonia, during the period 1991–1993, with follow-up until january 1, 1996."
1629,1,"['variables', 'observations', 'data']",B Data sets in the ISwR package,seg_383,a data frame with 829 observations on the following 10 variables.
1630,1,"['levels', 'factor', 'observation', 'set']",B Data sets in the ISwR package,seg_383,"sex a factor with levels female and male. died a date, date of death. dstr a date, date of stroke. age a numeric vector, age at stroke. dgn a factor, diagnosis, with levels ich (intracranial haemorrhage), id (unidentified). inf (infarction, ischaemic), sah (subarchnoid haemorrhage). coma a factor with levels no and yes, indicating whether patient was in coma after the stroke. diab a factor with levels no and yes, history of diabetes. minf a factor with levels no and yes, history of myocardial infarction. han a factor with levels no and yes, history of hypertension. obsmonths a numeric vector, observation times in months (set to 0.1 for patients dying on the same day as the stroke). dead a logical vector, whether patient died during the study."
1631,1,['data'],B Data sets in the ISwR package,seg_383,original data.
1632,0,[],B Data sets in the ISwR package,seg_383,"j. korv, m. roose, and a.e. kaasik (1997). stroke registry of tartu, estonia, from 1991 through 1993. cerebrovascular disorders 7:154– 162."
1633,0,[],B Data sets in the ISwR package,seg_383,tb.dilute tuberculin dilution assay
1634,1,"['test', 'data']",B Data sets in the ISwR package,seg_383,the tb.dilute data frame has 18 rows and 3 columns. it contains data from a drug test involving dilutions of tuberculin.
1635,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1636,1,"['factor', 'average', 'levels']",B Data sets in the ISwR package,seg_383,"reaction a numeric vector, reaction sizes (average of diameters) for tuberculin skin pricks. animal a factor with levels 1–6. logdose a factor with levels 0.5, 0, and -0.5."
1637,0,[],B Data sets in the ISwR package,seg_383,"the actual dilutions were 1:100, 1:100√10, 1:1000. setting the middle one to 1 and using base-10 logarithms gives the logdose values."
1638,1,['statistics'],B Data sets in the ISwR package,seg_383,"anon. (1977), exercises in applied statistics, part of exercise 4.15, dept. of theoretical statistics, aarhus university."
1639,0,[],B Data sets in the ISwR package,seg_383,thuesen ventricular shortening velocity
1640,1,['data'],B Data sets in the ISwR package,seg_383,the thuesen data frame has 24 rows and 2 columns. it contains ventricular shortening velocity and blood glucose for type 1 diabetic patients.
1641,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1642,1,['mean'],B Data sets in the ISwR package,seg_383,"blood.glucose a numeric vector, fasting blood glucose (mmol/l). short.velocity a numeric vector, mean circumferential shortening velocity (%/s)."
1643,1,"['statistics', 'table']",B Data sets in the ISwR package,seg_383,"d.g. altman (1991), practical statistics for medical research, table 11.6, chapman & hall."
1644,0,[],B Data sets in the ISwR package,seg_383,tlc total lung capacity
1645,1,['data'],B Data sets in the ISwR package,seg_383,the tlc data frame has 32 rows and 4 columns. it contains data on pretransplant total lung capacity (tlc) for recipients of heart-lung transplants by whole-body plethysmography.
1646,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1647,0,[],B Data sets in the ISwR package,seg_383,"age a numeric vector, age of recipient (years). sex a numeric vector code, female: 1, male: 2. height a numeric vector, height of recipient (cm). tlc a numeric vector, total lung capacity (l)."
1648,1,['statistics'],B Data sets in the ISwR package,seg_383,"d.g. altman (1991), practical statistics for medical research, exercise 12.5, 10.1, chapman & hall."
1649,1,"['set', 'data set', 'data']",B Data sets in the ISwR package,seg_383,the vitcap data frame has 24 rows and 3 columns. it contains data on vital capacity for workers in the cadmium industry. it is a subset of the vitcap2 data set.
1650,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1651,0,[],B Data sets in the ISwR package,seg_383,"group a numeric vector; group codes are 1: exposed > 10 years, 3: not exposed. age a numeric vector, age in years. vital.capacity a numeric vector, vital capacity (a measure of lung volume) in liters."
1652,1,['statistical'],B Data sets in the ISwR package,seg_383,"p. armitage and g. berry (1987), statistical methods in medical research, 2nd ed., blackwell, p.286."
1653,1,"['set', 'data set', 'data']",B Data sets in the ISwR package,seg_383,"vitcap2 vital capacity, full data set"
1654,1,['data'],B Data sets in the ISwR package,seg_383,the vitcap2 data frame has 84 rows and 3 columns. age and vital capacity for workers in the cadmium industry.
1655,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1656,0,[],B Data sets in the ISwR package,seg_383,"group a numeric vector; group codes are 1: exposed > 10 years, 2: exposed < 10 years, 3: not exposed. age a numeric vector, age in years. vital.capacity a numeric vector, vital capacity (a measure of lung volume) (l)."
1657,1,['statistical'],B Data sets in the ISwR package,seg_383,"p. armitage and g. berry (1987), statistical methods in medical research, 2nd ed., blackwell, p.286."
1658,0,[],B Data sets in the ISwR package,seg_383,wright comparison of wright peak-flow meters
1659,1,"['rate', 'data']",B Data sets in the ISwR package,seg_383,the wright data frame has 17 rows and 2 columns. it contains data on peak expiratory flow rate with two different flow meters on each of 17 subjects.
1660,1,['data'],B Data sets in the ISwR package,seg_383,this data frame contains the following columns:
1661,1,['data'],B Data sets in the ISwR package,seg_383,"std.wright a numeric vector, data from large flow meter (l/min). mini.wright a numeric vector, data from mini flow meter (l/min)."
1662,1,"['statistical', 'measurement']",B Data sets in the ISwR package,seg_383,"j.m. bland and d.g. altman (1986), statistical methods for assessing agreement between two methods of clinical measurement, lancet, 1:307–310."
1663,0,[],B Data sets in the ISwR package,seg_383,zelazo age at walking
1664,0,[],B Data sets in the ISwR package,seg_383,the zelazo object is a list with four components.
1665,1,['data'],B Data sets in the ISwR package,seg_383,this is a list containing data on age at walking (in months) for four groups of infants:
1666,1,['test'],B Data sets in the ISwR package,seg_383,"active test group receiving active training; these children had their walking and placing reflexes trained during four three-minute sessions that took place every day from their second to their eighth week of life. passive passive training group; these children received the same types of social and gross motor stimulation, but did not have their specific walking and placing reflexes trained."
1667,0,[],B Data sets in the ISwR package,seg_383,"none no training; these children had no special training, but were tested along with the children who underwent active or passive training. ctr.8w eighth-week controls; these children had no training and were only tested at the age of 8 weeks."
1668,1,"['set', 'data set', 'data']",B Data sets in the ISwR package,seg_383,"when asked to enter these data from a text source, many students will use one vector per group and will need to reformat data into a data frame for some uses. the rather unusual format of this data set mimics that situation."
1669,0,[],C Compendium,seg_385,ls() or objects() list objects in workspace
1670,0,[],C Compendium,seg_385,rm(object) delete object search() search path
1671,0,[],C Compendium,seg_385,"combinations of letters, digits, and period. must not start with a digit. avoid starting with period."
1672,1,"['variable', 'variable ', 'functions']",C Compendium,seg_385,<- assign value to variable -> assignment “to the right” <<- global assignment (in functions)
1673,0,[],C Compendium,seg_385,"+ addition - subtraction, sign multiplication * / division ^ raise to power %/% integer division %% remainder from integer division"
1674,0,[],C Compendium,seg_385,== equal to != not equal to < less than > greater than <= less than or equal to >= greater than or equal to is.na(x) missing?
1675,0,[],C Compendium,seg_385,& logical and | logical or ! logical not & and | are elementwise. see “programming” (p. 336) for && and ||.
1676,1,['data'],C Compendium,seg_385,vectors and data types
1677,1,"['level', 'factor', 'levels']",C Compendium,seg_385,"rep(1,10) 1 1 1 1 1 1 1 1 1 1 gl(3,2,12) factor with 3 levels, repeat each level in blocks of 2, up to length 12 (i.e., 1 1 2 2 3 3 1 1 2 2 3 3)"
1678,0,[],C Compendium,seg_385,as.numeric(x) convert to numeric as.character(x) convert to text string
1679,1,"['factor', 'factors']",C Compendium,seg_385,"as.logical(x) convert to logical factor(x) create factor from vector x for factors, see also “tabulation, grouping, and recoding” (p. 331)."
1680,1,['data'],C Compendium,seg_385,data.frame(height = data frame with two named vectors
1681,1,['data'],C Compendium,seg_385,"c(165,185), weight = c(90,65)) data.frame(height, collect vectors into data frame"
1682,1,"['data frames', 'data']",C Compendium,seg_385,weight) dfr$var select vector var in data frame dfr attach(dfr) put data frame in search path detach() — and remove it from path attached data frames always come after .globalenv in the search path. attached data frames are copies; subsequent changes to dfr have no effect.
1683,1,['functions'],C Compendium,seg_385,numerical functions
1684,1,"['exponential', 'function']",C Compendium,seg_385,"log(x) logarithm of x, natural (base-e) logarithm log10(x) base-10 logarithm exp(x) exponential function ex sin(x) sine cos(x) cosine tan(x) tangent asin(x) arcsin (inverse sine) acos(x) atan(x) min(x) smallest value in vector min(x1,x2,...) minimum over several vectors (one number) max(x) largest value in vector range(x) like c(min(x),max(x)) pmin(x1,x2,...) parallel (elementwise) minimum over multiple equally long vectors pmax(x1,x2,...) parallel maximum length(x) number of elements in vector sum(complete.cases(x)) number of nonmissing elements in"
1685,1,"['deviation', 'median', 'quantiles', 'correlation', 'variance', 'standard', 'standard deviation', 'average']",C Compendium,seg_385,"mean(x) average sd(x) standard deviation var(x) variance median(x) median quantile(x,p) quantiles cor(x,y) correlation"
1686,1,"['factor', 'variable', 'numeric variable']",C Compendium,seg_385,"x[1] first element x[1:5] subvector containing first five elements x[c(2,3,5,7,11)] element nos. 2, 3, 5, 7, and 11 x[y<=30] selection by logical expression x[sex==""male""] selection by factor variable i <- c(2,3,5,7,11); x[i] selection by numeric variable"
1687,1,['variable'],C Compendium,seg_385,l <- (y<=30); x[l] selection by logical variable
1688,1,"['data frames', 'data']",C Compendium,seg_385,matrices and data frames
1689,1,['data'],C Compendium,seg_385,"m[4,] fourth row m[,3] third column dfr[dfr$var<=30,] partial data frame subset(dfr,var<=30) same, often simpler"
1690,1,['data'],C Compendium,seg_385,input of data
1691,1,"['data', 'set', 'data set']",C Compendium,seg_385,"data(name) built-in data set read.table(""filename"") read from external file"
1692,0,[],C Compendium,seg_385,common arguments to read.table
1693,1,"['missing value', 'data', 'variable', 'variable names']",C Compendium,seg_385,"header=true first line has variable names sep="","" data are separated by commas dec="","" decimal point is comma na.strings=""."" missing value is dot"
1694,0,[],C Compendium,seg_385,variants of read.table
1695,0,[],C Compendium,seg_385,"read.csv(""filename"") comma-separated read.delim(""filename"") tab-delimited"
1696,0,[],C Compendium,seg_385,"read.csv2(""filename"") semicolon-separated, comma decimal point read.delim2(""filename"") tab-delimited, comma decimal"
1697,1,['set'],C Compendium,seg_385,point these all set header=true.
1698,0,[],C Compendium,seg_385,"is.na(x) logical vector. true where x has na. complete.cases(x1,x2,...) missing neither in x1, nor x2,"
1699,1,['functions'],C Compendium,seg_385,arguments to other functions
1700,1,"['functions', 'mean', 'statistical']",C Compendium,seg_385,"na.rm= in statistical functions, remove missing if true, return na if false. na.last= in sort; true, false and na mean, respectively, “last”, “first”, and “throw away”. na.action= in lm, etc., values na.fail, na.omit, na.exclude; also in options(""na.action""). na.print= in summary and print.default; how to represent na in output. na.strings= in read.table(); code(s) for na in input."
1701,0,[],C Compendium,seg_385,"tabulation, grouping, and recoding"
1702,1,"['table', 'variables', 'tables']",C Compendium,seg_385,"table(f1,...) (cross)-tabulation xtabs(~ f1 + ...) ditto, formula interface ftable(f1 ~ f2 + ...) “flat” tables tapply(x,f,mean) table of means aggregate(df,list(f),mean) means for several variables"
1703,1,"['continuous', 'variable', 'factor', 'data']",C Compendium,seg_385,"by(df, list(f), summary) summarize data frame by group factor(x) convert vector to factor cut(x,breaks) groups from cutpoints for continuous variable"
1704,1,['factor'],C Compendium,seg_385,arguments to factor
1705,1,"['missing values', 'levels', 'factor', 'data', 'set', 'associated', 'level']",C Compendium,seg_385,levels values of x to code. use if some values are not present in data or if the order would be wrong. labels values associated with factor levels. exclude values to exclude. default na. set to null to have missing values included as a level.
1706,0,[],C Compendium,seg_385,"breaks cutpoints. note that values of x outside breaks give na. labels names for groups. default is (0,30], etc. right right endpoint included? (false: left)"
1707,1,['factors'],C Compendium,seg_385,recoding factors
1708,1,"['level', 'levels']",C Compendium,seg_385,"levels(f) <- names new level names levels(f) <- list( combining levels new1=c(""old1"",""old2"") new2=""old3"")"
1709,1,['distributions'],C Compendium,seg_385,statistical distributions
1710,1,['distribution'],C Compendium,seg_385,normal distribution
1711,1,"['cumulative distribution function', 'distribution function', 'normally distributed', 'distribution', 'function']",C Compendium,seg_385,"dnorm(x) density pnorm(x) cumulative distribution function, p(x ≤ x) qnorm(p) p-quantile, x : p(x ≤ x) = p rnorm(n) n (pseudo-)random normally distributed numbers"
1712,1,"['poisson', 'f distribution', 'lognormal', 'gamma', 'distribution', 'normal', 'exponential', 'binomial']",C Compendium,seg_385,"pnorm(x,mean,sd) normal plnorm(x,mean,sd) lognormal pt(x,df) student’s t pf(x,n1,n2) f distribution pchisq(x,df) χ2 pbinom(x,n,p) binomial ppois(x,lambda) poisson punif(x,min,max) uniform pexp(x,rate) exponential pgamma(x,shape,scale) gamma"
1713,1,['standard'],C Compendium,seg_385,statistical standard methods
1714,1,['response'],C Compendium,seg_385,continuous response
1715,1,"['analysis of covariance', 'f test', 'multiple regression analysis', 'regression analysis', 'pairwise comparisons', 'correlation', 'regression', 'covariance', 'multiple regression', 'analysis of variance', 'variance', 'tests', 'test', 'variances']",C Compendium,seg_385,t.test oneand two-sample t tests pairwise.t.test pairwise comparisons cor.test correlation var.test comparison of two variances (f test) lm(y ~ x) regression analysis lm(y ~ f) one-way analysis of variance lm(y ~ f1 + f2) two-way analysis of variance lm(y ~ f + x) analysis of covariance lm(y ~ x1 + x2 + x3) multiple regression analysis bartlett.test bartlett’s test (k variances) nonparametric:
1716,1,"['analysis of variance', 'variance', 'wilcoxon tests', 'tests', 'test']",C Compendium,seg_385,wilcox.test oneand two-sample wilcoxon tests kruskal.test kruskal–wallis test friedman.test friedman’s two-way analysis of variance cor.test variants:
1717,0,[],C Compendium,seg_385,"method=""kendall"" kendall’s τ method=""spearman"" spearman’s ρ"
1718,1,['response'],C Compendium,seg_385,discrete response
1719,1,"['logistic', 'logistic regression', 'regression', 'sign test', 'χ2 test', 'tables', 'binomial', 'test']",C Compendium,seg_385,"binom.test binomial test (incl. sign test) prop.test comparison of proportions prop.trend.test test for trend in relative proportions fisher.test exact test in small tables chisq.test χ2 test glm(y ~ x1+x2+x3, binomial) logistic regression"
1720,1,"['interaction ', 'factor', 'interaction', 'intercept', 'variable']",C Compendium,seg_385,~ described by + additive effects : interaction main effects + interaction * (a b = a + b + a:b) * -1 remove intercept classifications are represented by descriptive variable being a factor.
1721,1,"['linear', 'nonlinear', 'generalized linear models']",C Compendium,seg_385,"linear, nonlinear, and generalized linear models"
1722,1,"['model', 'table', 'residuals', 'data', 'analysis of variance', 'fitted values', 'variance', 'predictions', 'coefficients']",C Compendium,seg_385,"lm.out <- lm(y ~ x) fit model and save result summary(lm.out) coefficients, etc. anova(lm.out) analysis of variance table fitted(lm.out) fitted values resid(lm.out) residuals predict(lm.out, newdata) predictions for new data frame"
1723,1,"['poisson', 'logistic', 'nonlinear regression', 'logistic regression', 'nonlinear', 'regression', 'binomial']",C Compendium,seg_385,"glm(y ~ x, binomial) logistic regression glm(y ~ x, poisson) poisson regression nls(y ~ a exp(-b x), nonlinear regression * * start=c(a=5, b=.2))"
1724,1,['residuals'],C Compendium,seg_385,rstudent(lm.out) studentized residuals dfbetas(lm.out) change in β if obs. removed dffits(lm.out) change in fit if obs. removed
1725,1,"['curve', 'model', 'estimate', 'test']",C Compendium,seg_385,"s <- surv(time, ev) create survival object survfit(s) kaplan–meier estimate plot(survfit(s)) survival curve survdiff(s ~ g) (log-rank) test for equal survival curves coxph(s ~ x1 + x2) cox’s proportional hazards model"
1726,1,['plots'],C Compendium,seg_385,standard plots
1727,1,"['plot', 'histogram', 'barplot', 'interaction', 'scatterplot', 'boxplot', 'dotplot']",C Compendium,seg_385,plot() scatterplot (and more) hist() histogram boxplot() box-and-whiskers plot stripplot() stripplot barplot() bar diagram dotplot() dot diagram piechart() cakes. . . interaction.plot() interaction plot
1728,1,"['plot', 'error bars', 'plots', 'intercept', 'error', 'slope']",C Compendium,seg_385,lines() lines abline() line given by intercept and slope (and more) points() points segments() line segments arrows() arrows (n.b.: angle=90 for error bars) axis() axis box() frame around plot title() title (above plot) text() text in plot mtext() text in margin legend() list of symbols these are all added to existing plots.
1729,1,['parameters'],C Compendium,seg_385,graphical parameters
1730,1,"['plot', 'plots', 'plotting']",C Compendium,seg_385,"pch symbol (plotting character) mfrow, mfcol several plots on one (multif rame) xlim, ylim plot limits lty,lwd line type/width col colour cex, mex character size and line spacing in margins see the help page for par for more details."
1731,1,['conditional'],C Compendium,seg_385,"if(p<0.05) conditional execution print(""hooray!"")"
1732,1,['control'],C Compendium,seg_385,"in flow control, one uses a && b and a || b, where b is only computed if necessary; that is, if a then b else false and if a then true else b."
1733,0,[],D Answers to exercises,seg_387,1.1 one possibility is
1734,1,['case'],D Answers to exercises,seg_387,"notice that false & na is false, so the case of different na patterns is handled correctly."
1735,1,['factor'],D Answers to exercises,seg_387,1.2 factor x gets treated as if it contained the integer codes.
1736,1,['plot'],D Answers to exercises,seg_387,"(this is useful, e.g., when selecting plot symbols.)"
1737,1,['levels'],D Answers to exercises,seg_387,1.4 the levels with the same name are collapsed into one.
1738,1,['cases'],D Answers to exercises,seg_387,"or, more generally, to insert v just after index k (the boundary cases require some care),"
1739,1,['case'],D Answers to exercises,seg_387,"(notice that if you do not edit the file in the first case, then the second column gets read as a character vector.)"
1740,1,"['cumulative distribution function', 'distribution function', 'distribution', 'probability', 'function']",D Answers to exercises,seg_387,"it might be better to use lower.tail=false instead of subtracting from 1 in (a), (b), and (e). notice that question (c) is about a point probability, whereas the others involve the cumulative distribution function."
1741,1,"['standard normal', 'normal', 'standard']",D Answers to exercises,seg_387,3.2 evaluate each of the following. notice that the standard normal can be used for all questions.
1742,1,['cases'],D Answers to exercises,seg_387,"again, lower.tail can be used in some cases."
1743,0,[],D Answers to exercises,seg_387,3.4 either of the following should work:
1744,1,"['sample', 'success', 'probabilities']",D Answers to exercises,seg_387,"the first one gives a 0/1 result, the two others h/t like the sample example in the text. one advantage of using rbinom is that its prob argument can be a vector, so you can have different probabilities of success for each element of the result."
1745,1,"['plot', 'set']",D Answers to exercises,seg_387,"4.2 use a filled symbol, and set the fill colour equal to the plot background:"
1746,1,"['range', 'qqnorm', 'information']",D Answers to exercises,seg_387,4.3 you can use qqnorm with plot.it=f and get a return value from which you can extract the range information (you could of course also get this “by eye”).
1747,1,"['plot', 'qqnorm']",D Answers to exercises,seg_387,"here, qqnorm is used for the basic plot to get the labels right. then points is used with q2 for the overlay."
1748,1,['plot'],D Answers to exercises,seg_387,"setting type=""l"" gives a messy plot because the values are not plotted in order. the remedy is to use sort(x1) and sort(x2)."
1749,1,['data'],D Answers to exercises,seg_387,"4.4 the breaks occur at integer values, as do the data. data on the boundary are counted in the column to the left of it, effectively shifting the"
1750,1,"['function', 'set']",D Answers to exercises,seg_387,histogram half a unit left. the truehist function allows you to specify a better set of breaks.
1751,1,"['linear', 'data']",D Answers to exercises,seg_387,4.5 the thing to notice is the linear interpolation between data points:
1752,1,"['distribution', 'normal', 'outliers']",D Answers to exercises,seg_387,"5.1 the distribution appears reasonably normal, with some discretization effect and two weak outliers, one at each end. there is a significant difference from zero (t = −7.75, p = 1.1× 10−13)."
1753,1,['bias'],D Answers to exercises,seg_387,"5.2 t.test(vital.capacity~group,conf=0.99,data=vitcap). the fact that age also differs by group may cause bias."
1754,0,[],D Answers to exercises,seg_387,5.3 this is quite parallel to t.test usage
1755,1,"['plot', 'histogram']",D Answers to exercises,seg_387,"5.4 the following builds a post-vs.-pre plot, a difference-vs.-average) (bland-altman) plot, and a histogram and a q-q plot of the differences."
1756,1,"['outliers', 'observations', 'data']",D Answers to exercises,seg_387,5.5 the outliers are the first and last observations in the (sorted) data vector and can be removed as follows
1757,1,"['outliers', 'plot', 'discretization', 'qqnorm', 'test']",D Answers to exercises,seg_387,the test comes out highly significant even with outliers removed because it picks up the discretization effect in the otherwise nearly straight-line qqnorm plot.
1758,1,"['t test', 'treatment', 'test', 'paired t test', 'paired']",D Answers to exercises,seg_387,"5.6 a paired t test is appropriate if there is no period effect. however, even with a period effect (assumed additive), you would expect the difference between the two periods to be the same in both groups if there were no effect of treatment. this can be used to test for a treatment effect."
1759,1,"['interval', 'treatment', 'case', 'confidence', 'confidence interval']",D Answers to exercises,seg_387,notice that the subtraction is reversed in one group. observe that the confidence interval in the second case is for twice the treatment effect.
1760,1,"['plot', 'replicate']",D Answers to exercises,seg_387,"5.7 this is the sort of thing replicate is for. the plot at the end shows a p-p plot with logarithmic axes, showing that extreme p-values tend to be exaggerated."
1761,0,[],D Answers to exercises,seg_387,6.1 the following gives both elementary and more general answers. notice the use of confint.
1762,0,[],D Answers to exercises,seg_387,"6.2 summary(lm(sqrt(igf1)~age,data=juul,subset=age>25))"
1763,1,"['plot', 'linear', 'model', 'linear model', 'data']",D Answers to exercises,seg_387,6.3 we can fit a linear model and plot the data as follows:
1764,1,"['plot', 'model']",D Answers to exercises,seg_387,"the plot appears to show a cyclic pattern. it is unclear whether it reflects a significant departure from the model, though. malaria is a disease with epidemic behaviour, so cycles are plausible."
1765,1,"['random number', 'function', 'random']",D Answers to exercises,seg_387,"6.4 (this could be elaborated by wrapping the random number generation in a function, etc.)"
1766,1,['correlation'],D Answers to exercises,seg_387,you will most likely find that the kendall correlation is somewhat smaller than the two others.
1767,1,"['standard error', 'design', 'error', 'standard']",D Answers to exercises,seg_387,"7.2 a and c differ with b intermediate, not significantly different from either. (the b–c comparison is not available from the summary, but due to the balanced design, the standard error of that difference is 0.16656 like the two others.)"
1768,1,"['transform', 'data']",D Answers to exercises,seg_387,7.4 (only the square-root transform is shown; you can do the same for log-transformed and untransformed data.)
1769,1,"['skewed', 'transformations', 'test']",D Answers to exercises,seg_387,"the square root looks nice, logarithms become skewed in the opposite direction. the transformations do not make much of a difference for the test. it is, however, a problem that strong age effects are being ignored, particularly within tanner stage 1."
1770,1,"['level', 'significance']",D Answers to exercises,seg_387,"8.1 with 10 patients, p = 0.1074. fourteen or more are needed for significance at level 0.05."
1771,0,[],D Answers to exercises,seg_387,"8.2 yes, it is highly significant."
1772,1,"['interval', 'confidence', 'confidence interval']",D Answers to exercises,seg_387,"8.3 the confidence interval (from prop.test) is (−0.085, 0.507)"
1773,0,[],D Answers to exercises,seg_387,"8.4 the following is a simplified analysis, which uses fisher.test because of the small cell counts:"
1774,1,"['categories', 'marginal']",D Answers to exercises,seg_387,"you may wish to check that there is little or no effect of egg size on breakage, so that the marginal analysis is defensible. you could also try collapsing the “broken” and “cracked” categories."
1775,1,"['curve', 'interval', 'confidence', 'probability', 'level', 'tail']",D Answers to exercises,seg_387,"8.5 the curve shows substantial discontinuities where probability mass is shifted from one tail to the other and also a number of local minima. a confidence region could be defined as those p against which there is no significant evidence at level α, but for some α that is not an interval."
1776,1,"['sample size', 'sample', 'estimated', 'case', 'sampling']",D Answers to exercises,seg_387,9.1 the estimated sample size is 6.29 or 8.06 per group depending on whether you use oneor two-sided testing. the approximate formula gives 6.98 for the two-sided case. the reduction in power due to the unbalanced sampling can be accounted for by reducing delta by the ratio of the two sedm.
1777,0,[],D Answers to exercises,seg_387,9.2 this is straightforward:
1778,1,"['t distribution', 'distribution', 'asymmetric', 'tail', 'noncentral']",D Answers to exercises,seg_387,"9.3 notice that the noncentral t distribution is asymmetric, with a rather heavy right tail."
1779,1,"['level', 'effect size', 'significance', 'significance level']",D Answers to exercises,seg_387,"9.4 this causes the “power” at zero effect size (i.e., under the null hypothesis) to be half the significance level, in contradiction to theory. for any relevant true effect size, the difference is immaterial."
1780,1,"['variance', 'case']",D Answers to exercises,seg_387,9.5 the power in that case is approximately 0.50; exactly so if the variance is assumed known.
1781,1,"['variables', 'factors']",D Answers to exercises,seg_387,"10.3 one way is the following (for later use, we also make sure that variables are converted to factors):"
1782,0,[],D Answers to exercises,seg_387,"notice the use of array indexing. alternatively, an ifelse construct can be used; e.g., the following (notice that (3 - grp) is 2 when grp is 1 and vice versa):"
1783,1,['factor'],D Answers to exercises,seg_387,"arithmetic involving grp does not work after it was converted to a factor, hence the conversion with as.numeric."
1784,1,"['transform', 'case', 'observation']",D Answers to exercises,seg_387,10.4 this can be done a little more easily than in the nickel example by using subset and transform. it also helps that all observation periods start at time zero in this case.
1785,1,"['function', 'interval', 'intervals']",D Answers to exercises,seg_387,"notice the use of mapply here. this is like sapply and lapply but allows the function to have multiple arguments. alternatively, one could arrange for stroke.trim to have a single interval argument and use lapply on a list of such intervals."
1786,1,['cases'],D Answers to exercises,seg_387,the tabulation at the end is a “sanity check” to show that we have the same number of deaths but many more censored cases after time-splitting.
1787,1,"['model', 'residual', 'error', 'coefficients']",D Answers to exercises,seg_387,"11.1 the model with both diameters has a residual error of 0.107, compared with 0.128 using abdominal diameter alone and 0.281 with no predictors at all. if a fetus is scaled isotropically, a cubic relation with weight is expected, and you could speculate that this is reflected in the sum of coefficients when using log scales."
1788,1,"['variable', 'data']",D Answers to exercises,seg_387,"11.2 if you use attach(tlc), the tlc variable will mask the data frame of the same name, which makes it awkward to access the data frame if you need to. if the data frame is in the global environment rather than in"
1789,1,"['variable', 'data']",D Answers to exercises,seg_387,"a package, you get the opposite problem, masking of the variable by the data frame. the simplest workaround is to avoid attach."
1790,1,"['model', 'data', 'variations']",D Answers to exercises,seg_387,"some new variations of model formulas were introduced above. a dot on the right-hand side in this context means “everything not used on the left-hand side” within the scope of the data frame. a minus term is removed from the model. in other words, ... ~ . - age is the same as ... ~ sex + height."
1791,1,"['coefficient', 'regression coefficient', 'regression']",D Answers to exercises,seg_387,11.3 the regression coefficient describes a value to be added for females.
1792,1,"['observations', 'results', 'cases', 'number of observations']",D Answers to exercises,seg_387,"11.4 age is highly significant in the first analysis but only borderline significant (p = 0.06) in the second analysis after removing height and weight. you would expect similar results, but the number of observations differs in the two cases, due to missing observations."
1793,1,['indicator'],D Answers to exercises,seg_387,"11.5 sex is treated as a binary indicator for girls. notice that there are effects both of the mother’s and the child’s size. the reason why height rather than weight of the mother enters into the equation is somewhat obscure, but one could speculate that weight is an unreliable indicator shortly after pregnancy."
1794,1,"['model', 'variations']",D Answers to exercises,seg_387,the variations on model formulas used here were described in the solution to exercise 11.2.
1795,1,"['treatment', 'tests', 'anova']",D Answers to exercises,seg_387,notice that the imbalance in group sizes makes the tests for period and treatment effects order-dependent. the t tests are equivalent to the f tests from drop1 but not those from anova.
1796,1,"['data', 'slope']",D Answers to exercises,seg_387,"notice that the formula for the fitted slope is β̂ = ∑ xy/ ∑ x2 since x̄ = 0, which reduces to taking differences. (the calculation does rely on data being in the right order.)"
1797,1,"['degrees of freedom', 'interval', 't test', 'variation', 'slopes', 'vary', 'confidence', 'test', 'confidence interval']",D Answers to exercises,seg_387,"the confidence interval is wider in the t test, reflecting that slopes may vary between rats and that there are fewer degrees of freedom for estimating the variation."
1798,1,"['interval', 'data', 'slopes', 'statistic', 'vary', 'confidence', 'test', 'confidence interval', 'anova']",D Answers to exercises,seg_387,"the final anova contains a test for parallel slopes, and the f statistic is less than one, so in these data the slopes vary less than expected and the df must be the important issue for the confidence interval."
1799,0,[],D Answers to exercises,seg_387,"12.3 this can be varied indefinitely, but consider these examples:"
1800,1,"['interaction term', 'model', 'categorical', 'variables', 'design', 'interaction', 'intercept', 'set', 'indicator variables', 'indicator', 'categorical variables']",D Answers to exercises,seg_387,"the first model is singular because indicator variables are created for all four groups, but the intercept is not removed. r will only reduce the set of design variables for an interaction term between categorical variables"
1801,1,"['model', 'categorical', 'cases', 'variable', 'parameter', 'continuous']",D Answers to exercises,seg_387,"when one of the main effects is present. there are no singularities in either of the two cases involving a categorical and a continuous variable, but the first one has one parameter less (common-intercept model)."
1802,1,['level'],D Answers to exercises,seg_387,the last example has a “coincidental” singularity (x and y are proportional within each level of b) that r has no chance of detecting.
1803,1,"['plotting', 'fitted values']",D Answers to exercises,seg_387,"12.4 the models can be illustrated by plotting the fitted values against time with separate symbols for each person; e.g.,"
1804,1,['vary'],D Answers to exercises,seg_387,"with model1 there is no imposed structure, model2 is completely additive so that the individual traces are parallel to each other, model3 allows the jump from the “pre” value to the value at 20 minutes to vary between individuals, and finally model4 is like model3 except that there is no change after 30 minutes (traces become horizontal). so model3 is nested in model1 and both model2 and model4 are nested in model3, but there is no nesting relation between model2 and model4."
1805,1,['interaction'],D Answers to exercises,seg_387,"notice that there is a significant interaction; i.e., the lines are not parallel."
1806,1,"['model', 'regression', 'regression coefficient', 'coefficient', 'cook’s distances']",D Answers to exercises,seg_387,"observation 32 contains an extremely large value of ml.suppl and therefore has a large influence on its regression coefficient. without ml.suppl in the model, the cook’s distances are much smaller."
1807,1,['interaction'],D Answers to exercises,seg_387,(you may also want to check for interaction.)
1808,1,"['coefficient', 'model', 'deviance']",D Answers to exercises,seg_387,"the coefficient to log(index) is more significant, but the model with index has a slightly better deviance. there is little hard evidence for either. the log-transform has the advantage that it reduces the influence of two very large values of index."
1809,1,"['variables', 'model']",D Answers to exercises,seg_387,notice that except for log(index) it is essentially an arbitrary decision which variables to put in the final model. altman (1991) treats the type classification as separate binary variables and gets a final model where all and aml are combined into one group and includes preg but not donage.
1810,1,"['interval', 'estimate', 'case', 'intervals', 'tests']",D Answers to exercises,seg_387,"notice that the confint-generated intervals lie asymmetrically around the estimate. in this case, both ends of the interval are shifted away from zero, in accordance with the fact that the deviance-based tests from drop1 have lower p-values than the approximate t tests in summary."
1811,1,['model'],D Answers to exercises,seg_387,13.4 the model can be fitted as follows
1812,0,[],D Answers to exercises,seg_387,"the effect of type vanished once age was included, suggesting that it really is the same disease, which has affected mostly younger (and fitter) subjects in the eastern region."
1813,1,['predictor'],D Answers to exercises,seg_387,subsequent elimination suggests that preg might be a better predictor than gvhd.
1814,0,[],D Answers to exercises,seg_387,"the men were considerably younger than the women when they had their stroke, which may explain their apparently better survival."
1815,0,[],D Answers to exercises,seg_387,"14.4 using stroke2 from exercise 10.4,"
1816,0,['n'],D Answers to exercises,seg_387,notice that the result is essentially the same as in the unsplit analysis; only n and rsquare are changed.
1817,0,[],D Answers to exercises,seg_387,"15.1 using bcmort2 from exercise 10.2,"
1818,1,"['factor', 'interval']",D Answers to exercises,seg_387,"15.2 continuing with stroke2 from exercise 10.4, the only slight complication is to convert entry to a factor to specify the relevant time interval."
1819,1,['results'],D Answers to exercises,seg_387,notice how similar the results are to the cox analysis in exercise 14.3.
1820,1,"['model', 'data']",D Answers to exercises,seg_387,"16.1 to fit to the data for girls, just copy the procedure for boys. even though the growth curves differ, there is no real reason to redo the starting value calculation, so we can fit the model to boys, girls, and both as follows"
1821,1,"['model', 'test', 'f test']",D Answers to exercises,seg_387,"to test whether we can use the same model for boys and girls, there are two approaches. one is to make an f test based on the three fits above:"
1822,1,['degrees of freedom'],D Answers to exercises,seg_387,"this gives f = 90.58 on 3 and 1124 degrees of freedom, which is of course highly significant."
1823,1,"['model', 'parameters', 'data', 'set', 'joint', 'test']",D Answers to exercises,seg_387,"alternatively, we can set up the joint model with separate parameters for boys and girls and test whether it fits the data better than the model with the same parameters, like this:"
1824,1,['parameters'],D Answers to exercises,seg_387,"notice that da, db, and dg represent differences between the parameters for the two genders. the term sex==1 is 0 for girls and 1 for boys."
1825,1,"['observation', 'experiment']",D Answers to exercises,seg_387,"16.2 we consider experiment 1 only. starting values can be eyeballed by using the observation at zero dose for ymax and the x (dose) value at approximately ymax/2 as β. the value of α can be guessed as the fixed constant 1. below, we use α via its logarithm, called la."
1826,1,"['factor', 'model', 'tail']",D Answers to exercises,seg_387,"16.3 the alternative model has similar tail behaviour but behaves differently when x is close to zero. (in particular, the original model has a term proportional to −xα−1 in its derivative. at zero, this is −∞ when α < 1 and 0 when α > 1, so the model describes curves that are either very steep or very flat near zero.) notice that, in the modified model, β is no longer the ec50; the latter is now the solution for x of (1 + x/β)α = 2. the two are connected by a factor of 21/α − 1."
1827,1,"['curve', 'model']",D Answers to exercises,seg_387,"here, fit1 and fit2 are equivalent models, except that the latter is parameterized in terms of ec50. we can compare the fitted curve with the model from the previous exercise as follows:"
1828,1,"['transformation', 'fitted values']",D Answers to exercises,seg_387,(notice that the fitted values should be squared because of the square-root transformation in the models.)
1829,1,"['categorical data', 'data', 'categorical']",Bibliography,seg_389,"agresti, a. (1990), categorical data analysis, john wiley & sons, new york."
1830,1,['statistics'],Bibliography,seg_389,"altman, d. g. (1991), practical statistics for medical research, chapman & hall, london."
1831,1,"['statistical', 'processes']",Bibliography,seg_389,"andersen, p. k., borgan, ø., gill, r. d., and keiding, n. (1991), statistical models based on counting processes, springer-verlag, new york."
1832,1,['statistical'],Bibliography,seg_389,"armitage, p. and berry, g. (1994), statistical methods in medical research, 3rd ed., blackwell, oxford."
1833,1,"['nonlinear regression', 'regression analysis', 'nonlinear', 'regression']",Bibliography,seg_389,"bates, d. m. and watts, d. g. (1988), nonlinear regression analysis and its applications, john wiley & sons, new york."
1834,1,"['design', 'statistical']",Bibliography,seg_389,"breslow, n. e. and day, n. (1987), statistical methods in cancer research. volume ii: the design and analysis of cohort studies, iarc scientific publications, lyon."
1835,1,['statistics'],Bibliography,seg_389,"campbell, m. j. and machin, d. (1993), medical statistics. a commonsense approach, 2nd ed., john wiley & sons, chichester."
1836,1,['statistical'],Bibliography,seg_389,"chambers, j. m. and hastie, t. j. (1992), statistical models in s, chapman & hall, london."
1837,1,['statistical'],Bibliography,seg_389,"clayton, d. and hills, m. (1993), statistical models in epidemiology, oxford university press, oxford."
1838,1,['data'],Bibliography,seg_389,"cleveland, w. s. (1994), the elements of graphing data, hobart press, new jersey."
1839,1,['experimental'],Bibliography,seg_389,"cochran, w. g. and cox, g. m. (1957), experimental designs, 2nd ed., john wiley & sons, new york."
1840,1,['data'],Bibliography,seg_389,"cox, d. r. (1970), analysis of binary data, chapman & hall, london."
1841,1,['data'],Bibliography,seg_389,"cox, d. r. and oakes, d. (1984), analysis of survival data, chapman & hall, london."
1842,1,['statistical'],Bibliography,seg_389,"everitt, b. s. (1994), a handbook of statistical analyses using s-plus, chapman & hall, london."
1843,1,['tests'],Bibliography,seg_389,"hájek, j., šidák, z., and sen, p. k. (1999), theory of rank tests, 2nd ed., academic press, san diego."
1844,1,['statistical'],Bibliography,seg_389,"hald, a. (1952), statistical theory with engineering applications, john wiley & sons, new york."
1845,1,"['logistic regression', 'logistic', 'regression']",Bibliography,seg_389,"hosmer, d. w. and lemeshow, s. (2000), applied logistic regression, 2nd ed., john wiley & sons, new york."
1846,1,"['statistics', 'probability']",Bibliography,seg_389,"johnson, r. a. (1994), miller & freund’s probability & statistics for engineers, 5th ed., prentice-hall, englewood cliffs, nj."
1847,1,"['statistical', 'failure', 'data']",Bibliography,seg_389,"kalbfleisch, j. d. and prentice, r. l. (1980), the statistical analysis of failure time data, john wiley & sons, new york."
1848,1,['statistical'],Bibliography,seg_389,"lehmann, e. l. (1975), nonparametrics, statistical methods based on ranks, mcgraw-hill, new york."
1849,1,['statistics'],Bibliography,seg_389,"matthews, d. e. and farewell, v. t. (1988), using and understanding medical statistics, 2nd ed., karger, basel."
1850,1,"['linear', 'generalized linear models']",Bibliography,seg_389,"mccullagh, p. and nelder, j. a. (1989), generalized linear models, 2nd ed., chapman & hall, london."
1851,1,['graphics'],Bibliography,seg_389,"murrell, p. (2005), r graphics, chapman & hall/crc, boca raton, florida."
1852,1,"['nonparametric statistics', 'statistics']",Bibliography,seg_389,"siegel, s. (1956), nonparametric statistics for the behavioral sciences, mcgraw-hill international, auckland."
1853,1,['statistics'],Bibliography,seg_389,"venables, w. n. and ripley, b. d. (2002), modern applied statistics with s, 4th ed., springer-verlag, new york."
1854,1,"['linear', 'linear regression', 'regression']",Bibliography,seg_389,"weisberg, s. (1985), applied linear regression, 2nd ed., john wiley & sons, new york."
1855,0,[],Bibliography,seg_389,"zar, j. h. (1999), biostatistical analysis, prentice hall, englewood cliffs, nj."
