,Relevance,Tags,Heading,Seg,Sentence
0,1,"['statistical', 'success']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"beginning in the 1980s and continuing into the 21st century, an inordinate amount of attention has been focused on improvement of quality in american industry. much has been said and written about the japanese “industrial miracle,” which began in the middle of the 20th century. the japanese were able to succeed where we and other countries had failed–namely, to create an atmosphere that allows the production of high-quality products. much of the success of the japanese has been attributed to the use of statistical methods and statistical thinking among management personnel."
1,1,"['statistics', 'data', 'information', 'statistical', 'inferential statistics']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"the use of statistical methods in manufacturing, development of food products, computer software, energy sources, pharmaceuticals, and many other areas involves the gathering of information or scientific data. of course, the gathering of data is nothing new. it has been done for well over a thousand years. data have been collected, summarized, reported, and stored for perusal. however, there is a profound distinction between collection of scientific information and inferential statistics. it is the latter that has received rightful attention in recent decades."
2,1,"['process', 'uncertainty', 'statistics', 'data', 'statistical', 'variation', 'continuous', 'inferential statistics']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"the offspring of inferential statistics has been a large “toolbox” of statistical methods employed by statistical practitioners. these statistical methods are designed to contribute to the process of making scientific judgments in the face of uncertainty and variation. the product density of a particular material from a manufacturing process will not always be the same. indeed, if the process involved is a batch process rather than continuous, there will be not only variation in material density among the batches that come off the line (batch-to-batch variation), but also within-batch variation. statistical methods are used to analyze data from a process such as this one in order to gain more sense of where in the process changes may be made to improve the quality of the process. in this process, qual-"
3,1,"['experienced', 'standard', 'set', 'variation', 'process']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"ity may well be defined in relation to closeness to a target density value in harmony with what portion of the time this closeness criterion is met. an engineer may be concerned with a specific instrument that is used to measure sulfur monoxide in the air during pollution studies. if the engineer has doubts about the effectiveness of the instrument, there are two sources of variation that must be dealt with. the first is the variation in sulfur monoxide values that are found at the same locale on the same day. the second is the variation between values observed and the true amount of sulfur monoxide that is in the air at the time. if either of these two sources of variation is exceedingly large (according to some standard set by the engineer), the instrument may need to be replaced. in a biomedical study of a new drug that reduces hypertension, 85% of patients experienced relief, while it is generally recognized that the current drug, or “old” drug, brings relief to 80% of patients that have chronic hypertension. however, the new drug is more expensive to make and may result in certain side effects. should the new drug be adopted? this is a problem that is encountered (often with much more complexity) frequently by pharmaceutical firms in conjunction with the fda (federal drug administration). again, the consideration of variation needs to be taken into account. the “85%” value is based on a certain number of patients chosen for the study. perhaps if the study were repeated with new patients the observed number of “successes” would be 75%! it is the natural variation from study to study that must be taken into account in the decision process. clearly this variation is important, since variation from patient to patient is endemic to the problem."
4,1,"['observations', 'statistics', 'case', 'probability', 'process', 'data', 'information', 'samples', 'statistical', 'inferential statistics', 'statistical inference', 'statistician', 'response', 'variability', 'statisticians', 'sampling', 'measuring']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"in the problems discussed above the statistical methods used involve dealing with variability, and in each case the variability to be studied is that encountered in scientific data. if the observed product density in the process were always the same and were always on target, there would be no need for statistical methods. if the device for measuring sulfur monoxide always gives the same value and the value is accurate (i.e., it is correct), no statistical analysis is needed. if there were no patient-to-patient variability inherent in the response to the drug (i.e., it either always brings relief or not), life would be simple for scientists in the pharmaceutical firms and fda and no statistician would be needed in the decision process. statistics researchers have produced an enormous number of analytical methods that allow for analysis of data from systems like those described above. this reflects the true nature of the science that we call inferential statistics, namely, using techniques that allow us to go beyond merely reporting data to drawing conclusions (or inferences) about the scientific system. statisticians make use of fundamental laws of probability and statistical inference to draw conclusions about scientific systems. information is gathered in the form of samples, or collections of observations. the process of sampling is introduced in chapter 2, and the discussion continues throughout the entire book."
5,1,"['populations', 'information', 'sampling', 'population', 'process']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"samples are collected from populations, which are collections of all individuals or individual items of a particular type. at times a population signifies a scientific system. for example, a manufacturer of computer boards may wish to eliminate defects. a sampling process may involve collecting information on 50 computer boards sampled randomly from the process. here, the population is all"
6,1,"['sample', 'experiment', 'population', 'process']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"computer boards manufactured by the firm over a specific period of time. if an improvement is made in the computer board process and a second sample of boards is collected, any conclusions drawn regarding the effectiveness of the change in process should extend to the entire population of computer boards produced under the “improved process.” in a drug experiment, a sample of patients is taken and each is given a specific drug to reduce blood pressure. the interest is focused on drawing conclusions about the population of those who suffer from hypertension."
7,1,"['levels', 'factor', 'design', 'case', 'process', 'experiment', 'observational study', 'data', 'population', 'statistical', 'experimental', 'statistical inference', 'factors']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"often, it is very important to collect scientific data in a systematic way, with planning being high on the agenda. at times the planning is, by necessity, quite limited. we often focus only on certain properties or characteristics of the items or objects in the population. each characteristic has particular engineering or, say, biological importance to the “customer,” the scientist or engineer who seeks to learn about the population. for example, in one of the illustrations above the quality of the process had to do with the product density of the output of a process. an engineer may need to study the effect of process conditions, temperature, humidity, amount of a particular ingredient, and so on. he or she can systematically move these factors to whatever levels are suggested according to whatever prescription or experimental design is desired. however, a forest scientist who is interested in a study of factors that influence wood density in a certain kind of tree cannot necessarily design an experiment. this case may require an observational study in which data are collected in the field but factor levels can not be preselected. both of these types of studies lend themselves to methods of statistical inference. in the former, the quality of the inferences will depend on proper planning of the experiment. in the latter, the scientist is at the mercy of what can be gathered. for example, it is sad if an agronomist is interested in studying the effect of rainfall on plant yield and the data are gathered during a drought."
8,1,"['process', 'statistical inference', 'data', 'level', 'statistical']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,the importance of statistical thinking by managers and the use of statistical inference by scientific personnel is widely acknowledged. research scientists gain much from scientific data. data provide understanding of scientific phenomena. product and process engineers learn a great deal in their off-line efforts to improve the process. they also gain valuable insight by gathering production data (online monitoring) on a regular basis. this allows them to determine necessary modifications in order to keep the process at a desired level of quality.
9,1,"['observations', 'plots', 'statistics', 'location', 'set', 'medians', 'sample', 'data', 'standard', 'histograms', 'inferential statistics', 'statistical', 'graphics', 'statistical inference', 'dot plots', 'descriptive statistics', 'distribution', 'box plots', 'standard deviations', 'variability', 'scatter plots', 'deviations']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"there are times when a scientific practitioner wishes only to gain some sort of summary of a set of data represented in the sample. in other words, inferential statistics is not required. rather, a set of single-number statistics or descriptive statistics is helpful. these numbers give a sense of center of the location of the data, variability in the data, and the general nature of the distribution of observations in the sample. though no specific statistical methods leading to statistical inference are incorporated, much can be learned. at times, descriptive statistics are accompanied by graphics. modern statistical software packages allow for computation of means, medians, standard deviations, and other singlenumber statistics as well as production of graphs that show a “footprint” of the nature of the sample. definitions and illustrations of the single-number statistics and graphs, including histograms, stem-and-leaf plots, scatter plots, dot plots, and box plots, will be given in sections that follow."
10,1,"['statistical inference', 'descriptive statistics', 'results', 'statistics', 'data', 'probability theory', 'probability', 'statistical']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"in this book, chapters 2 to 6 deal with fundamental notions of probability. a thorough grounding in these concepts allows the reader to have a better understanding of statistical inference. without some formalism of probability theory, the student cannot appreciate the true interpretation from data analysis through modern statistical methods. it is quite natural to study probability prior to studying statistical inference. elements of probability allow us to quantify the strength or “confidence” in our conclusions. in this sense, concepts in probability form a major component that supplements statistical methods and helps us gauge the strength of the statistical inference. the discipline of probability, then, provides the transition between descriptive statistics and inferential methods. elements of probability allow the conclusion to be put into the language that the science or engineering practitioners require. an example follows that will enable the reader to understand the notion of a p -value, which often provides the “bottom line” in the interpretation of results from the use of statistical methods."
11,1,"['sample', 'rate', 'random', 'condition', 'case', 'data', 'information', 'population', 'probability', 'random sample', 'process']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"example 1.1: suppose that an engineer encounters data from a manufacturing process in which 100 items are sampled and 10 are found to be defective. it is expected and anticipated that occasionally there will be defective items. obviously these 100 items represent the sample. however, it has been determined that in the long run, the company can only tolerate 5% defective in the process. now, the elements of probability allow the engineer to determine how conclusive the sample information is regarding the nature of the process. in this case, the population conceptually represents all possible items from the process. suppose we learn that if the process is acceptable, that is, if it does produce items no more than 5% of which are defective, there is a probability of 0.0282 of obtaining 10 or more defective items in a random sample of 100 items from the process. this small probability suggests that the process does, indeed, have a long-run rate of defective items that exceeds 5%. in other words, under the condition of an acceptable process, the sample information obtained would rarely occur. however, it did occur! clearly, though, it would occur with a much higher probability if the process defective rate exceeded 5% by a significant amount."
12,1,"['sample', 'information', 'statistical', 'probability', 'process']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"from this example it becomes clear that the elements of probability aid in the translation of sample information into something conclusive or inconclusive about the scientific system. in fact, what was learned likely is alarming information to the engineer or manager. statistical methods, which we will actually detail in chapter 10, produced a p -value of 0.0282. the result suggests that the process very likely is not acceptable. the concept of a p-value is dealt with at length in succeeding chapters. the example that follows provides a second illustration."
13,1,"['deductive reasoning', 'statistical inference', 'data', 'samples', 'probability', 'associated', 'statistical']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"example 1.2: often the nature of the scientific study will dictate the role that probability and deductive reasoning play in statistical inference. exercise 9.40 on page 294 provides data associated with a study conducted at the virginia polytechnic institute and state university on the development of a relationship between the roots of trees and the action of a fungus. minerals are transferred from the fungus to the trees and sugars from the trees to the fungus. two samples of 10 northern red oak seedlings were planted in a greenhouse, one containing seedlings treated with nitrogen and"
14,1,"['data', 'table']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,the other containing seedlings with no nitrogen. all other environmental conditions were held constant. all seedlings contained the fungus pisolithus tinctorus. more details are supplied in chapter 9. the stem weights in grams were recorded after the end of 140 days. the data are given in table 1.1.
15,1,"['set', 'data set', 'data']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,table 1.1: data set for example 1.2
16,1,"['plot', 'data', 'dot plot']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,figure 1.1: a dot plot of stem weight data.
17,1,"['plot', 'experiment', 'data', 'samples', 'populations', 'dot plot']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"in this example there are two samples from two separate populations. the purpose of the experiment is to determine if the use of nitrogen has an influence on the growth of the roots. the study is a comparative study (i.e., we seek to compare the two populations with regard to a certain important characteristic). it is instructive to plot the data as shown in the dot plot of figure 1.1. the ◦ values represent the “nitrogen” data and the × values represent the “no-nitrogen” data."
18,1,"['statistical inference', 'observations', 'data', 'samples', 'set', 'statistical', 'data set', 'probability', 'population', 'average']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"notice that the general appearance of the data might suggest to the reader that, on average, the use of nitrogen increases the stem weight. four nitrogen observations are considerably larger than any of the no-nitrogen observations. most of the no-nitrogen observations appear to be below the center of the data. the appearance of the data set would seem to indicate that nitrogen is effective. but how can this be quantified? how can all of the apparent visual evidence be summarized in some sense? as in the preceding example, the fundamentals of probability can be used. the conclusions may be summarized in a probability statement or p-value. we will not show here the statistical inference that produces the summary probability. as in example 1.1, these methods will be discussed in chapter 10. the issue revolves around the “probability that data like these could be observed” given that nitrogen has no effect, in other words, given that both samples were generated from the same population. suppose that this probability is small, say 0.03. that would certainly be strong evidence that the use of nitrogen does indeed influence (apparently increases) average stem weight of the red oak seedlings."
19,1,"['sample', 'statistical inference', 'results', 'statistics', 'data', 'information', 'statistical', 'probability', 'population', 'inferential statistics', 'average']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"it is important for the reader to understand the clear distinction between the discipline of probability, a science in its own right, and the discipline of inferential statistics. as we have already indicated, the use or application of concepts in probability allows real-life interpretation of the results of statistical inference. as a result, it can be said that statistical inference makes use of concepts in probability. one can glean from the two examples above that the sample information is made available to the analyst and, with the aid of statistical methods and elements of probability, conclusions are drawn about some feature of the population (the process does not appear to be acceptable in example 1.1, and nitrogen does appear to influence average stem weights in example 1.2). thus for a statistical problem, the sample along with inferential statistics allows us to draw conclusions about the population, with inferential statistics making clear use of elements of probability. this reasoning is inductive in nature. now as we move into chapter 2 and beyond, the reader will note that, unlike what we do in our two examples here, we will not focus on solving statistical problems. many examples will be given in which no sample is involved. there will be a population clearly described with all features of the population known. then questions of importance will focus on the nature of data that might hypothetically be drawn from the population. thus, one can say that elements in probability allow us to draw conclusions about characteristics of hypothetical data taken from the population, based on known features of the population. this type of reasoning is deductive in nature. figure 1.2 shows the fundamental relationship between probability and inferential statistics."
20,1,['sample'], Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,population sample
21,1,"['inferential statistics', 'statistics', 'probability']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,figure 1.2: fundamental relationship between probability and inferential statistics.
22,1,"['sample', 'process', 'uncertainty', 'statistics', 'probability', 'population', 'level', 'average']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"now, in the grand scheme of things, which is more important, the field of probability or the field of statistics? they are both very important and clearly are complementary. the only certainty concerning the pedagogy of the two disciplines lies in the fact that if statistics is to be taught at more than merely a “cookbook” level, then the discipline of probability must be taught first. this rule stems from the fact that nothing can be learned about a population from a sample until the analyst learns the rudiments of uncertainty in that sample. for example, consider example 1.1. the question centers around whether or not the population, defined by the process, is no more than 5% defective. in other words, the conjecture is that on the average 5 out of 100 items are defective. now, the sample contains 100 items and 10 are defective. does this support the conjecture or refute it? on the"
23,1,"['sample', 'probability', 'process']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"surface it would appear to be a refutation of the conjecture because 10 out of 100 seem to be “a bit much.” but without elements of probability, how do we know? only through the study of material in future chapters will we learn the conditions under which the process is acceptable (5% defective). the probability of obtaining 10 or more defective items in a sample of 100 is 0.0282."
24,1,"['statistical inference', 'data', 'sampling', 'distribution', 'probability', 'sampling distributions', 'statistical', 'distributions']", Overview Statistical Inference Samples Populations and the Role of Probability,seg_3,"we have given two examples where the elements of probability provide a summary that the scientist or engineer can use as evidence on which to build a decision. the bridge between the data and the conclusion is, of course, based on foundations of statistical inference, distribution theory, and sampling distributions discussed in future chapters."
25,1,"['variability', 'sampling', 'population', 'populations', 'process']", Sampling Procedures Collection of Data,seg_5,"in section 1.1 we discussed very briefly the notion of sampling and the sampling process. while sampling appears to be a simple concept, the complexity of the questions that must be answered about the population or populations necessitates that the sampling process be very complex at times. while the notion of sampling is discussed in a technical way in chapter 8, we shall endeavor here to give some common-sense notions of sampling. this is a natural transition to a discussion of the concept of variability."
26,1,"['states', 'case', 'random', 'sample', 'biased', 'populations', 'population', 'confidence', 'random numbers', 'random sampling', 'sample size', 'simple random sampling', 'table', 'sampling']", Sampling Procedures Collection of Data,seg_5,"the importance of proper sampling revolves around the degree of confidence with which the analyst is able to answer the questions being asked. let us assume that only a single population exists in the problem. recall that in example 1.2 two populations were involved. simple random sampling implies that any particular sample of a specified sample size has the same chance of being selected as any other sample of the same size. the term sample size simply means the number of elements in the sample. obviously, a table of random numbers can be utilized in sample selection in many instances. the virtue of simple random sampling is that it aids in the elimination of the problem of having the sample reflect a different (possibly more confined) population than the one about which inferences need to be made. for example, a sample is to be chosen to answer certain questions regarding political preferences in a certain state in the united states. the sample involves the choice of, say, 1000 families, and a survey is to be conducted. now, suppose it turns out that random sampling is not used. rather, all or nearly all of the 1000 families chosen live in an urban setting. it is believed that political preferences in rural areas differ from those in urban areas. in other words, the sample drawn actually confined the population and thus the inferences need to be confined to the “limited population,” and in this case confining may be undesirable. if, indeed, the inferences need to be made about the state as a whole, the sample of size 1000 described here is often referred to as a biased sample."
27,1,"['simple random sampling', 'sampling units', 'homogeneous', 'sampling', 'strata', 'random', 'random sampling']", Sampling Procedures Collection of Data,seg_5,"as we hinted earlier, simple random sampling is not always appropriate. which alternative approach is used depends on the complexity of the problem. often, for example, the sampling units are not homogeneous and naturally divide themselves into nonoverlapping groups that are homogeneous. these groups are called strata,"
28,1,"['sample', 'random samples', 'sampling', 'samples', 'strata', 'random', 'random sampling']", Sampling Procedures Collection of Data,seg_5,"and a procedure called stratified random sampling involves random selection of a sample within each stratum. the purpose is to be sure that each of the strata is neither overnor underrepresented. for example, suppose a sample survey is conducted in order to gather preliminary opinions regarding a bond referendum that is being considered in a certain city. the city is subdivided into several ethnic groups which represent natural strata. in order not to disregard or overrepresent any group, separate random samples of families could be chosen from each group."
29,1,"['factor', 'design', 'statistics', 'case', 'set', 'random', 'experiment', 'data', 'populations', 'statistical', 'random sampling', 'experimental', 'condition', 'experimental unit', 'experimental units', 'variability', 'treatment', 'sampling', 'combinations']", Sampling Procedures Collection of Data,seg_5,"the concept of randomness or random assignment plays a huge role in the area of experimental design, which was introduced very briefly in section 1.1 and is an important staple in almost any area of engineering or experimental science. this will be discussed at length in chapters 13 through 15. however, it is instructive to give a brief presentation here in the context of random sampling. a set of so-called treatments or treatment combinations becomes the populations to be studied or compared in some sense. an example is the nitrogen versus no-nitrogen treatments in example 1.2. another simple example would be “placebo” versus “active drug,” or in a corrosion fatigue study we might have treatment combinations that involve specimens that are coated or uncoated as well as conditions of low or high humidity to which the specimens are exposed. in fact, there are four treatment or factor combinations (i.e., 4 populations), and many scientific questions may be asked and answered through statistical and inferential methods. consider first the situation in example 1.2. there are 20 diseased seedlings involved in the experiment. it is easy to see from the data themselves that the seedlings are different from each other. within the nitrogen group (or the no-nitrogen group) there is considerable variability in the stem weights. this variability is due to what is generally called the experimental unit. this is a very important concept in inferential statistics, in fact one whose description will not end in this chapter. the nature of the variability is very important. if it is too large, stemming from a condition of excessive nonhomogeneity in experimental units, the variability will “wash out” any detectable difference between the two populations. recall that in this case that did not occur."
30,1,"['plot', 'experimental', 'standard', 'condition', 'variability', 'design', 'treatment', 'results', 'data', 'experimental units', 'process', 'dot plot']", Sampling Procedures Collection of Data,seg_5,"the dot plot in figure 1.1 and p-value indicated a clear distinction between these two conditions. what role do those experimental units play in the datataking process itself? the common-sense and, indeed, quite standard approach is to assign the 20 seedlings or experimental units randomly to the two treatments or conditions. in the drug study, we may decide to use a total of 200 available patients, patients that clearly will be different in some sense. they are the experimental units. however, they all may have the same chronic condition for which the drug is a potential treatment. then in a so-called completely randomized design, 100 patients are assigned randomly to the placebo and 100 to the active drug. again, it is these experimental units within a group or treatment that produce the variability in data results (i.e., variability in the measured result), say blood pressure, or whatever drug efficacy value is important. in the corrosion fatigue study, the experimental units are the specimens that are the subjects of the corrosion."
31,1,"['treatment group', 'case', 'sample', 'results', 'samples', 'statistical', 'experimental', 'statistical inference', 'experimental units', 'variability', 'treatment', 'biases', 'combinations']", Sampling Procedures Collection of Data,seg_5,"what is the possible negative impact of not randomly assigning experimental units to the treatments or treatment combinations? this is seen most clearly in the case of the drug study. among the characteristics of the patients that produce variability in the results are age, gender, and weight. suppose merely by chance the placebo group contains a sample of people that are predominately heavier than those in the treatment group. perhaps heavier individuals have a tendency to have a higher blood pressure. this clearly biases the result, and indeed, any result obtained through the application of statistical inference may have little to do with the drug and more to do with differences in weights among the two samples of patients."
32,1,"['design', 'statistics', 'location', 'set', 'sample', 'graphical', 'data', 'information', 'samples', 'statistical', 'experimental', 'descriptive statistics', 'data set', 'experimental units', 'variability']", Sampling Procedures Collection of Data,seg_5,"we should emphasize the attachment of importance to the term variability. excessive variability among experimental units “camouflages” scientific findings. in future sections, we attempt to characterize and quantify measures of variability. in sections that follow, we introduce and discuss specific quantities that can be computed in samples; the quantities give a sense of the nature of the sample with respect to center of location of the data and variability in the data. a discussion of several of these single-number measures serves to provide a preview of what statistical information will be important components of the statistical methods that are used in future chapters. these measures that help characterize the nature of the data set fall into the category of descriptive statistics. this material is a prelude to a brief presentation of pictorial and graphical methods that go even further in characterization of the data set. the reader should understand that the statistical methods illustrated here will be used throughout the text. in order to offer the reader a clearer picture of what is involved in experimental design studies, we offer example 1.3."
33,1,"['levels', 'measurement', 'failure']", Sampling Procedures Collection of Data,seg_5,"example 1.3: a corrosion study was made in order to determine whether coating an aluminum metal with a corrosion retardation substance reduced the amount of corrosion. the coating is a protectant that is advertised to minimize fatigue damage in this type of material. also of interest is the influence of humidity on the amount of corrosion. a corrosion measurement can be expressed in thousands of cycles to failure. two levels of coating, no coating and chemical corrosion coating, were used. in addition, the two relative humidity levels are 20% relative humidity and 80% relative humidity."
34,1,"['experimental', 'combinations', 'experiment', 'table', 'treatment', 'data', 'experimental units']", Sampling Procedures Collection of Data,seg_5,"the experiment involves four treatment combinations that are listed in the table that follows. there are eight experimental units used, and they are aluminum specimens prepared; two are assigned randomly to each of the four treatment combinations. the data are presented in table 1.2."
35,1,"['plot', 'failure', 'data']", Sampling Procedures Collection of Data,seg_5,"the corrosion data are averages of two specimens. a plot of the averages is pictured in figure 1.3. a relatively large value of cycles to failure represents a small amount of corrosion. as one might expect, an increase in humidity appears to make the corrosion worse. the use of the chemical corrosion coating procedure appears to reduce corrosion."
36,1,"['experimental', 'design', 'treatment', 'combinations']", Sampling Procedures Collection of Data,seg_5,"in this experimental design illustration, the engineer has systematically selected the four treatment combinations. in order to connect this situation to concepts with which the reader has been exposed to this point, it should be assumed that the"
37,1,['data'], Sampling Procedures Collection of Data,seg_5,table 1.2: data for example 1.3
38,1,['failure'], Sampling Procedures Collection of Data,seg_5,average corrosion in coating humidity thousands of cycles to failure
39,0,[], Sampling Procedures Collection of Data,seg_5,20% 1750 chemical corrosion 80% 1550
40,1,['results'], Sampling Procedures Collection of Data,seg_5,figure 1.3: corrosion results for example 1.3.
41,1,"['combinations', 'combination', 'variability', 'treatment', 'results', 'information', 'population', 'average']", Sampling Procedures Collection of Data,seg_5,"conditions representing the four treatment combinations are four separate populations and that the two corrosion values observed for each population are important pieces of information. the importance of the average in capturing and summarizing certain features in the population will be highlighted in section 1.3. while we might draw conclusions about the role of humidity and the impact of coating the specimens from the figure, we cannot truly evaluate the results from an analytical point of view without taking into account the variability around the average. again, as we indicated earlier, if the two corrosion values for each treatment combination are close together, the picture in figure 1.3 may be an accurate depiction. but if each corrosion value in the figure is an average of two values that are widely dispersed, then this variability may, indeed, truly “wash away” any information that appears to come through when one observes averages only. the foregoing example illustrates these concepts:"
42,1,"['treatment', 'combinations', 'random']", Sampling Procedures Collection of Data,seg_5,"(1) random assignment of treatment combinations (coating, humidity) to experi-"
43,0,[], Sampling Procedures Collection of Data,seg_5,mental units (specimens)
44,1,"['sample', 'average']", Sampling Procedures Collection of Data,seg_5,(2) the use of sample averages (average corrosion values) in summarizing sample
45,1,['variability'], Sampling Procedures Collection of Data,seg_5,(3) the need for consideration of measures of variability in the analysis of any
46,1,"['sample', 'samples', 'sets']", Sampling Procedures Collection of Data,seg_5,sample or sets of samples
47,1,"['variability', 'descriptive statistics', 'statistics', 'data', 'location', 'set']", Sampling Procedures Collection of Data,seg_5,"this example suggests the need for what follows in sections 1.3 and 1.4, namely, descriptive statistics that indicate measures of center of location in a set of data, and those that measure variability."
48,1,"['sample', 'quantitative', 'sample mean', 'data', 'location', 'mean', 'average', 'numerical']", Measures of Location The Sample Mean and Median,seg_7,"measures of location are designed to provide the analyst with some quantitative values of where the center, or some other location, of data is located. in example 1.2, it appears as if the center of the nitrogen sample clearly exceeds that of the no-nitrogen sample. one obvious and very useful measure is the sample mean. the mean is simply a numerical average."
49,1,"['sample', 'sample mean', 'observations', 'mean']", Measures of Location The Sample Mean and Median,seg_7,"definition 1.1: suppose that the observations in a sample are x1, x2, . . . , xn. the sample mean, denoted by x̄, is"
50,1,"['sample median', 'sample', 'extreme values', 'outliers', 'median']", Measures of Location The Sample Mean and Median,seg_7,there are other measures of central tendency that are discussed in detail in future chapters. one important measure is the sample median. the purpose of the sample median is to reflect the central tendency of the sample in such a way that it is uninfluenced by extreme values or outliers.
51,1,"['sample median', 'sample', 'median', 'observations']", Measures of Location The Sample Mean and Median,seg_7,"definition 1.2: given that the observations in a sample are x1, x2, . . . , xn, arranged in increasing order of magnitude, the sample median is"
52,1,"['sample', 'sample mean', 'median', 'data', 'set', 'data set', 'mean']", Measures of Location The Sample Mean and Median,seg_7,"as an example, suppose the data set is the following: 1.7, 2.2, 3.9, 3.11, and 14.7. the sample mean and median are, respectively,"
53,1,"['median', 'case', 'data', 'samples', 'set', 'data set', 'mean']", Measures of Location The Sample Mean and Median,seg_7,"clearly, the mean is influenced considerably by the presence of the extreme observation, 14.7, whereas the median places emphasis on the true “center” of the data set. in the case of the two-sample data set of example 1.2, the two measures of central tendency for the individual samples are"
54,1,"['sample', 'sample mean', 'median', 'mean']", Measures of Location The Sample Mean and Median,seg_7,clearly there is a difference in concept between the mean and median. it may be of interest to the reader with an engineering background that the sample mean
55,1,"['sample', 'locations', 'data']", Measures of Location The Sample Mean and Median,seg_7,"is the centroid of the data in a sample. in a sense, it is the point at which a fulcrum can be placed to balance a system of “weights” which are the locations of the individual data. this is shown in figure 1.4 with regard to the with-nitrogen sample."
56,1,"['sample', 'mean', 'sample mean']", Measures of Location The Sample Mean and Median,seg_7,figure 1.4: sample mean as a centroid of the with-nitrogen stem weight.
57,1,"['parameters', 'statistical inference', 'estimate', 'estimation', 'population mean', 'mean', 'population', 'statistical']", Measures of Location The Sample Mean and Median,seg_7,"in future chapters, the basis for the computation of x̄ is that of an estimate of the population mean. as we indicated earlier, the purpose of statistical inference is to draw conclusions about population characteristics or parameters and estimation is a very important feature of statistical inference."
58,1,"['sample', 'sample mean', 'median', 'case', 'data', 'mean']", Measures of Location The Sample Mean and Median,seg_7,"the median and mean can be quite different from each other. note, however, that in the case of the stem weight data the sample mean value for no-nitrogen is quite similar to the median value."
59,1,"['trimmed means', 'case', 'location', 'estimators', 'set', 'sample', 'sample mean', 'data', 'mean', 'trimmed mean', 'sample size', 'median', 'percent', 'average']", Measures of Location The Sample Mean and Median,seg_7,"there are several other methods of quantifying the center of location of the data in the sample. we will not deal with them at this point. for the most part, alternatives to the sample mean are designed to produce values that represent compromises between the mean and the median. rarely do we make use of these other measures. however, it is instructive to discuss one class of estimators, namely the class of trimmed means. a trimmed mean is computed by “trimming away” a certain percent of both the largest and the smallest set of values. for example, the 10% trimmed mean is found by eliminating the largest 10% and smallest 10% and computing the average of the remaining values. for example, in the case of the stem weight data, we would eliminate the largest and smallest since the sample size is 10 for each sample. so for the without-nitrogen group the 10% trimmed mean is given by"
60,1,"['mean', 'trimmed mean']", Measures of Location The Sample Mean and Median,seg_7,and for the 10% trimmed mean for the with-nitrogen group we have
61,1,"['sample median', 'sample', 'outliers', 'trimmed means', 'sample mean', 'median', 'observations', 'case', 'data', 'information', 'samples', 'mean', 'trimmed mean']", Measures of Location The Sample Mean and Median,seg_7,"note that in this case, as expected, the trimmed means are close to both the mean and the median for the individual samples. the trimmed mean is, of course, more insensitive to outliers than the sample mean but not as insensitive as the median. on the other hand, the trimmed mean approach makes use of more information than the sample median. note that the sample median is, indeed, a special case of the trimmed mean in which all of the sample data are eliminated apart from the middle one or two observations."
62,1,"['observations', 'location', 'sample variability', 'set', 'function', 'process', 'sample', 'data', 'success', 'statistical', 'data set', 'control', 'method', 'variability']", Measures of Variability,seg_11,"sample variability plays an important role in data analysis. process and product variability is a fact of life in engineering and scientific systems: the control or reduction of process variability is often a source of major difficulty. more and more process engineers and managers are learning that product quality and, as a result, profits derived from manufactured products are very much a function of process variability. as a result, much of chapters 9 through 15 deals with data analysis and modeling procedures in which sample variability plays a major role. even in small data analysis problems, the success of a particular statistical method may depend on the magnitude of the variability among the observations in the sample. measures of location in a sample do not provide a proper summary of the nature of a data set. for instance, in example 1.2 we cannot conclude that the use of nitrogen enhances growth without taking sample variability into account."
63,1,"['sample', 'variability', 'observations', 'data', 'set', 'data set']", Measures of Variability,seg_11,"while the details of the analysis of this type of data set are deferred to chapter 9, it should be clear from figure 1.1 that variability among the no-nitrogen observations and variability among the nitrogen observations are certainly of some consequence. in fact, it appears that the variability within the nitrogen sample is larger than that of the no-nitrogen sample. perhaps there is something about the inclusion of nitrogen that not only increases the stem height (x̄ of 0.565 gram compared to an x̄ of 0.399 gram for the no-nitrogen sample) but also increases the variability in stem height (i.e., renders the stem height more inconsistent)."
64,1,"['contrast', 'experiment', 'variability', 'data', 'case', 'samples', 'sets', 'set', 'data set', 'populations', 'data sets']", Measures of Variability,seg_11,"as another example, contrast the two data sets below. each contains two samples and the difference in the means is roughly the same for the two samples, but data set b seems to provide a much sharper contrast between the two populations from which the samples were taken. if the purpose of such an experiment is to detect differences between the two populations, the task is accomplished in the case of data set b. however, in data set a the large variability within the two samples creates difficulty. in fact, it is not clear that there is a distinction between the two populations."
65,1,['set'], Measures of Variability,seg_11,data set a: x x x x x x 0 x x 0 0 x x x 0 0 0 0 0 0 0 0
66,1,['set'], Measures of Variability,seg_11,data set b: x x x x x x x x x x x 0 0 0 0 0 0 0 0 0 0 0
67,1,"['deviation', 'sample', 'variability', 'range', 'sample standard deviation', 'location', 'standard', 'standard deviation', 'control', 'statistical', 'quality control']", Measures of Variability,seg_11,"just as there are many measures of central tendency or location, there are many measures of spread or variability. perhaps the simplest one is the sample range xmax −xmin. the range can be very useful and is discussed at length in chapter 17 on statistical quality control. the sample measure of spread that is used most often is the sample standard deviation. we again let x1, x2, . . . , xn denote sample values."
68,1,"['sample', 'variance', 'sample variance']", Measures of Variability,seg_11,"definition 1.3: the sample variance, denoted by s2, is given by"
69,1,"['deviation', 'sample', 'sample standard deviation', 'standard', 'standard deviation']", Measures of Variability,seg_11,"the sample standard deviation, denoted by s, is the positive square root of s2, that is,"
70,1,"['degrees of freedom', 'set', 'associated', 'sample', 'estimate', 'data', 'information', 'standard', 'sample variance', 'standard deviation', 'data set', 'sample average', 'variance', 'deviation', 'independent', 'variability', 'sample standard deviation', 'average']", Measures of Variability,seg_11,"it should be clear to the reader that the sample standard deviation is, in fact, a measure of variability. large variability in a data set produces relatively large values of (x − x̄)2 and thus a large sample variance. the quantity n − 1 is often called the degrees of freedom associated with the variance estimate. in this simple example, the degrees of freedom depict the number of independent pieces of information available for computing variability. for example, suppose that we wish to compute the sample variance and standard deviation of the data set (5, 17, 6, 4). the sample average is x̄ = 8. the computation of the variance involves"
71,0,['n'], Measures of Variability,seg_11,"n the quantities inside parentheses sum to zero. in general, ∑ (xi − x̄) = 0 (see"
72,1,"['sample', 'squared deviations from the mean', 'degrees of freedom', 'independent', 'deviations', 'mean', 'sample variance', 'variance']", Measures of Variability,seg_11,"i=1 exercise 1.16 on page 31). then the computation of a sample variance does not involve n independent squared deviations from the mean x̄. in fact, since the last value of x − x̄ is determined by the initial n − 1 of them, we say that these are n − 1 “pieces of information” that produce s2. thus, there are n − 1 degrees of freedom rather than n degrees of freedom for computing a sample variance."
73,1,"['sample', 'results', 'data', 'measuring']", Measures of Variability,seg_11,"example 1.4: in an example discussed extensively in chapter 10, an engineer is interested in testing the “bias” in a ph meter. data are collected on the meter by measuring the ph of a neutral substance (ph = 7.0). a sample of size 10 is taken, with results given by"
74,1,"['sample', 'mean', 'sample mean']", Measures of Variability,seg_11,the sample mean x̄ is given by
75,1,"['sample', 'variance', 'sample variance']", Measures of Variability,seg_11,the sample variance s2 is given by
76,1,"['deviation', 'sample', 'sample standard deviation', 'standard', 'standard deviation']", Measures of Variability,seg_11,"as a result, the sample standard deviation is given by"
77,1,"['deviation', 'sample', 'degrees of freedom', 'sample standard deviation', 'standard', 'standard deviation']", Measures of Variability,seg_11,so the sample standard deviation is 0.0440 with n− 1 = 9 degrees of freedom.
78,1,"['degrees of freedom', 'sample standard deviations', 'case', 'sample', 'linear', 'data', 'mean', 'standard', 'sample variance', 'standard deviation', 'condition', 'variance', 'deviation', 'standard deviations', 'variability', 'sample standard deviation', 'deviations', 'average', 'variances']", Measures of Variability,seg_11,"it should be apparent from definition 1.3 that the variance is a measure of the average squared deviation from the mean x̄. we use the term average squared deviation even though the definition makes use of a division by degrees of freedom n − 1 rather than n. of course, if n is large, the difference in the denominator is inconsequential. as a result, the sample variance possesses units that are the square of the units in the observed data whereas the sample standard deviation is found in linear units. as an example, consider the data of example 1.2. the stem weights are measured in grams. as a result, the sample standard deviations are in grams and the variances are measured in grams2. in fact, the individual standard deviations are 0.0728 gram for the no-nitrogen case and 0.1867 gram for the nitrogen group. note that the standard deviation does indicate considerably larger variability in the nitrogen sample. this condition was displayed in figure 1.1."
79,1,"['range', 'population variance', 'sample', 'linear', 'sample mean', 'mean', 'populations', 'standard', 'sample variance', 'standard deviation', 'statistical', 'quality control', 'population', 'parameters', 'statistical inference', 'variance', 'control', 'deviation', 'variability', 'sample standard deviation', 'population mean', 'sample standard deviation measures variability', 'measuring']", Measures of Variability,seg_11,"as we indicated earlier, the sample range has applications in the area of statistical quality control. it may appear to the reader that the use of both the sample variance and the sample standard deviation is redundant. both measures reflect the same concept in measuring variability, but the sample standard deviation measures variability in linear units whereas the sample variance is measured in squared units. both play huge roles in the use of statistical methods. much of what is accomplished in the context of statistical inference involves drawing conclusions about characteristics of populations. among these characteristics are constants which are called population parameters. two important parameters are the population mean and the population variance. the sample variance plays an explicit role in the statistical methods used to draw inferences about the population variance. the sample standard deviation has an important role along with the sample mean in inferences that are made about the population mean. in general, the variance is considered more in inferential theory, while the standard deviation is used more in applications."
80,1,"['combination', 'experiment', 'observational studies', 'data', 'discrete', 'continuous', 'percent']", Discrete and Continuous Data,seg_15,"statistical inference through the analysis of observational studies or designed experiments is used in many scientific areas. the data gathered may be discrete or continuous, depending on the area of application. for example, a chemical engineer may be interested in conducting an experiment that will lead to conditions where yield is maximized. here, of course, the yield may be in percent or grams/pound, measured on a continuum. on the other hand, a toxicologist conducting a combination drug experiment may encounter data that are binary in nature (i.e., the patient either responds or does not)."
81,1,"['statistical inference', 'continuous', 'data', 'discrete', 'efficiency', 'statistical']", Discrete and Continuous Data,seg_15,"great distinctions are made between discrete and continuous data in the probability theory that allow us to draw statistical inferences. often applications of statistical inference are found when the data are count data. for example, an engineer may be interested in studying the number of radioactive particles passing through a counter in, say, 1 millisecond. personnel responsible for the efficiency of a port facility may be interested in the properties of the number of oil tankers arriving each day at a certain port city. in chapter 5, several distinct scenarios, leading to different ways of handling data, are discussed for situations with count data."
82,1,"['sample', 'experienced', 'data', 'associated', 'categories', 'statistical']", Discrete and Continuous Data,seg_15,"special attention even at this early stage of the textbook should be paid to some details associated with binary data. applications requiring statistical analysis of binary data are voluminous. often the measure that is used in the analysis is the sample proportion. obviously the binary situation involves two categories. if there are n units involved in the data and x is defined as the number that fall into category 1, then n − x fall into category 2. thus, x/n is the sample proportion in category 1, and 1− x/n is the sample proportion in category 2. in the biomedical application, 50 patients may represent the sample units, and if 20 out of 50 experienced an improvement in a stomach ailment (common to all 50)"
83,1,['sample'], Discrete and Continuous Data,seg_15,"20 after all were given the drug, then = 0.4 is the sample proportion for which"
84,1,"['sample', 'measurement', 'sample mean', 'successful', 'data', 'success', 'mean', 'numerical']", Discrete and Continuous Data,seg_15,"the drug was a success and 1 − 0.4 = 0.6 is the sample proportion for which the drug was not successful. actually the basic numerical measurement for binary data is generally denoted by either 0 or 1. for example, in our medical example, a successful result is denoted by a 1 and a nonsuccess a 0. as a result, the sample proportion is actually a sample mean of the ones and zeros. for the successful category,"
85,1,"['measurements', 'associated', 'process', 'sample', 'sample means', 'results', 'data', 'mean', 'populations', 'population', 'statistical', 'quality control', 'continuous', 'control', 'population mean']", Discrete and Continuous Data,seg_15,"the kinds of problems facing scientists and engineers dealing in binary data are not a great deal unlike those seen where continuous measurements are of interest. however, different techniques are used since the statistical properties of sample proportions are quite different from those of the sample means that result from averages taken from continuous populations. consider the example data in exercise 1.6 on page 13. the statistical problem underlying this illustration focuses on whether an intervention, say, an increase in curing temperature, will alter the population mean tensile strength associated with the silicone rubber process. on the other hand, in a quality control area, suppose an automobile tire manufacturer reports that a shipment of 5000 tires selected randomly from the process results"
86,1,['sample'], Discrete and Continuous Data,seg_15,100 in 100 of them showing blemishes. here the sample proportion is = 0.02.
87,1,"['sample', 'process']", Discrete and Continuous Data,seg_15,"5000 following a change in the process designed to reduce blemishes, a second sample of 5000 is taken and 90 tires are blemished. the sample proportion has been reduced"
88,1,['sample'], Discrete and Continuous Data,seg_15,"90 to = 0.018. the question arises, “is the decrease in the sample proportion"
89,1,"['estimates', 'case', 'discrete', 'sample', 'sample mean', 'estimate', 'cases', 'samples', 'mean', 'parameter', 'population', 'statistical', 'parameters', 'statistical inference', 'continuous', 'population mean']", Discrete and Continuous Data,seg_15,"5000 from 0.02 to 0.018 substantial enough to suggest a real improvement in the population proportion?” both of these illustrations require the use of the statistical properties of sample averages—one from samples from a continuous population, and the other from samples from a discrete (binary) population. in both cases, the sample mean is an estimate of a population parameter, a population mean in the first illustration (i.e., mean tensile strength), and a population proportion in the second case (i.e., proportion of blemished tires in the population). so here we have sample estimates used to draw scientific conclusions regarding population parameters. as we indicated in section 1.3, this is the general theme in many practical problems using statistical inference."
90,1,"['sample', 'model', 'level', 'parameters', 'estimation', 'probabilistic', 'information', 'populations', 'statistical model', 'statistical']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"often the end result of a statistical analysis is the estimation of parameters of a postulated model. this is natural for scientists and engineers since they often deal in modeling. a statistical model is not deterministic but, rather, must entail some probabilistic aspects. a model form is often the foundation of assumptions that are made by the analyst. for example, in example 1.2 the scientist may wish to draw some level of distinction between the nitrogen and no-nitrogen populations through the sample information. the analysis may require a certain model for"
91,1,"['data', 'distribution', 'samples', 'normal or gaussian distributions', 'normal', 'gaussian distributions', 'normal distribution', 'distributions']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"the data, for example, that the two samples come from normal or gaussian distributions. see chapter 6 for a discussion of the normal distribution."
92,1,"['graphical', 'experimental', 'dot plots', 'plots', 'data', 'sets', 'data sets', 'population', 'statistical']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"obviously, the user of statistical methods cannot generate sufficient information or experimental data to characterize the population totally. but sets of data are often used to learn about certain properties of the population. scientists and engineers are accustomed to dealing with data sets. the importance of characterizing or summarizing the nature of collections of data should be obvious. often a summary of a collection of data via a graphical display can provide insight regarding the system from which the data were taken. for instance, in sections 1.1 and 1.3, we have shown dot plots."
93,1,"['statistical inference', 'data', 'sampling', 'complement', 'populations', 'statistical']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"in this section, the role of sampling and the display of data for enhancement of statistical inference is explored in detail. we merely introduce some simple but often effective displays that complement the study of statistical populations."
94,1,"['model', 'experiment', 'table', 'data', 'percentages']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"at times the model postulated may take on a somewhat complicated form. consider, for example, a textile manufacturer who designs an experiment where cloth specimen that contain various percentages of cotton are produced. consider the data in table 1.3."
95,0,[], Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,table 1.3: tensile strength
96,1,['percentage'], Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,cotton percentage tensile strength
97,1,"['range', 'case', 'sample', 'experiment', 'sample means', 'data', 'hypothesis testing', 'samples', 'mean', 'populations', 'population', 'percentages', 'model', 'graphics', 'distribution', 'concentration', 'hypothesis', 'plot', 'variability', 'scatter plot', 'population mean', 'normal', 'normal distribution']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"five cloth specimens are manufactured for each of the four cotton percentages. in this case, both the model for the experiment and the type of analysis used should take into account the goal of the experiment and important input from the textile scientist. some simple graphics can shed important light on the clear distinction between the samples. see figure 1.5; the sample means and variability are depicted nicely in the scatter plot. one possible goal of this experiment is simply to determine which cotton percentages are truly distinct from the others. in other words, as in the case of the nitrogen/no-nitrogen data, for which cotton percentages are there clear distinctions between the populations or, more specifically, between the population means? in this case, perhaps a reasonable model is that each sample comes from a normal distribution. here the goal is very much like that of the nitrogen/no-nitrogen data except that more samples are involved. the formalism of the analysis involves notions of hypothesis testing discussed in chapter 10. incidentally, this formality is perhaps not necessary in light of the diagnostic plot. but does this describe the real goal of the experiment and hence the proper approach to data analysis? it is likely that the scientist anticipates the existence of a maximum population mean tensile strength in the range of cotton concentration in the experiment. here the analysis of the data should revolve"
98,1,"['model', 'population mean', 'mean', 'population', 'concentration']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"around a different type of model, one that postulates a type of structure relating the population mean tensile strength to the cotton concentration. in other words, a model may be written"
99,1,"['model', 'level', 'estimated', 'statistical inference', 'regression model', 'estimation', 'population mean', 'data', 'regression', 'measurements', 'mean', 'population', 'statistical', 'estimation theory']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"where μt,c is the population mean tensile strength, which varies with the amount of cotton in the product c. the implication of this model is that for a fixed cotton level, there is a population of tensile strength measurements and the population mean is μt,c. this type of model, called a regression model, is discussed in chapters 11 and 12. the functional form is chosen by the scientist. at times the data analysis may suggest that the model be changed. then the data analyst “entertains” a model that may be altered after some analysis is done. the use of an empirical model is accompanied by estimation theory, where β0, β1, and β2 are estimated by the data. further, statistical inference can then be used to determine model adequacy."
100,1,"['plot', 'scatter plot', 'percentages']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,figure 1.5: scatter plot of tensile strength and cotton percentages.
101,1,"['model', 'graphical', 'graphics', 'statistical inference', 'experiment', 'results', 'plots', 'data', 'information', 'statistical']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"two points become evident from the two data illustrations here: (1) the type of model used to describe the data often depends on the goal of the experiment; and (2) the structure of the model should take advantage of nonstatistical scientific input. a selection of a model represents a fundamental assumption upon which the resulting statistical inference is based. it will become apparent throughout the book how important graphics can be. often, plots can illustrate information that allows the results of the formal statistical inference to be better communicated to the scientist or engineer. at times, plots or exploratory data analysis can teach the analyst something not retrieved from the formal analysis. almost any formal analysis requires assumptions that evolve from the model of the data. graphics can nicely highlight violation of assumptions that would otherwise go unnoticed. throughout the book, graphics are used extensively to supplement formal data analysis. the following sections reveal some graphical tools that are useful in exploratory or descriptive data analysis."
102,1,"['plot', 'data', 'distribution']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"statistical data, generated in large masses, can be very useful for studying the behavior of the distribution if presented in a combined tabular and graphic display called a stem-and-leaf plot."
103,1,"['plot', 'table', 'data', 'observation', 'frequency']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"to illustrate the construction of a stem-and-leaf plot, consider the data of table 1.4, which specifies the “life” of 40 similar car batteries recorded to the nearest tenth of a year. the batteries are guaranteed to last 3 years. first, split each observation into two parts consisting of a stem and a leaf such that the stem represents the digit preceding the decimal and the leaf corresponds to the decimal part of the number. in other words, for the number 3.7, the digit 3 is designated the stem and the digit 7 is the leaf. the four stems 1, 2, 3, and 4 for our data are listed vertically on the left side in table 1.5; the leaves are recorded on the right side opposite the appropriate stem value. thus, the leaf 6 of the number 1.6 is recorded opposite the stem 1; the leaf 5 of the number 2.5 is recorded opposite the stem 2; and so forth. the number of leaves recorded opposite each stem is summarized under the frequency column."
104,0,[], Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,table 1.4: car battery life
105,1,['plot'], Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,table 1.5: stem-and-leaf plot of battery life
106,1,['frequency'], Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,stem leaf frequency 1 69 2 2 25669 5 3 0011112223334445567778899 25 4 11234577 8
107,1,"['plot', 'table', 'distribution']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"the stem-and-leaf plot of table 1.5 contains only four stems and consequently does not provide an adequate picture of the distribution. to remedy this problem, we need to increase the number of stems in our plot. one simple way to accomplish this is to write each stem value twice and then record the leaves 0, 1, 2, 3, and 4 opposite the appropriate stem value where it appears for the first time, and the leaves 5, 6, 7, 8, and 9 opposite this same stem value where it appears for the second time. this modified double-stem-and-leaf plot is illustrated in table 1.6, where the stems corresponding to leaves 0 through 4 have been coded by the symbol and the stems corresponding to leaves 5 through 9 by the symbol ·."
108,1,"['sample', 'data']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"in any given problem, we must decide on the appropriate stem values. this decision is made somewhat arbitrarily, although we are guided by the size of our sample. usually, we choose between 5 and 20 stems. the smaller the number of data available, the smaller is our choice for the number of stems. for example, if"
109,1,"['plot', 'case', 'data', 'observation', 'tables']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"the data consist of numbers from 1 to 21 representing the number of people in a cafeteria line on 40 randomly selected workdays and we choose a double-stem-and- leaf plot, the stems will be 0 , 0·, 1 , 1·, and 2 so that the smallest observation 1 has stem 0 and leaf 1, the number 18 has stem 1· and leaf 8, and the largest observation 21 has stem 2 and leaf 1. on the other hand, if the data consist of numbers from $18,800 to $19,600 representing the best possible deals on 100 new automobiles from a certain dealership and we choose a single-stem-and-leaf plot, the stems will be 188, 189, 190, . . . , 196 and the leaves will now each contain two digits. a car that sold for $19,385 would have a stem value of 193 and the two-digit leaf 85. multiple-digit leaves belonging to the same stem are usually separated by commas in the stem-and-leaf plot. decimal points in the data are generally ignored when all the digits to the right of the decimal represent the leaf. such was the case in tables 1.5 and 1.6. however, if the data consist of numbers ranging from 21.8 to 74.9, we might choose the digits 2, 3, 4, 5, 6, and 7 as our stems so that a number such as 48.3 would have a stem value of 4 and a leaf of 8.3."
110,1,['plot'], Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,table 1.6: double-stem-and-leaf plot of battery life
111,1,['frequency'], Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,stem leaf frequency 1· 69 2 2 2 1 2· 5669 4 3 001111222333444 15 3· 5567778899 10 4 11234 5 4· 577 3
112,1,"['plot', 'class interval', 'class intervals', 'interval', 'table', 'frequencies', 'observations', 'data', 'intervals', 'distribution', 'frequency', 'frequency distribution']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"the stem-and-leaf plot represents an effective way to summarize data. another way is through the use of the frequency distribution, where the data, grouped into different classes or intervals, can be constructed by counting the leaves belonging to each stem and noting that each stem defines a class interval. in table 1.5, the stem 1 with 2 leaves defines the interval 1.0–1.9 containing 2 observations; the stem 2 with 5 leaves defines the interval 2.0–2.9 containing 5 observations; the stem 3 with 25 leaves defines the interval 3.0–3.9 with 25 observations; and the stem 4 with 8 leaves defines the interval 4.0–4.9 containing 8 observations. for the double-stem-and-leaf plot of table 1.6, the stems define the seven class intervals 1.5–1.9, 2.0–2.4, 2.5–2.9, 3.0–3.4, 3.5–3.9, 4.0–4.4, and 4.5–4.9 with frequencies 2, 1, 4, 15, 10, 5, and 3, respectively."
113,1,"['class interval', 'interval', 'table', 'frequencies', 'relative frequency', 'observations', 'data', 'relative frequencies', 'distribution', 'set', 'frequency', 'number of observations', 'frequency distribution']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"dividing each class frequency by the total number of observations, we obtain the proportion of the set of observations in each of the classes. a table listing relative frequencies is called a relative frequency distribution. the relative frequency distribution for the data of table 1.4, showing the midpoint of each class interval, is given in table 1.7."
114,1,"['interval', 'relative frequency', 'information', 'distribution', 'frequency', 'frequency distribution']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,the information provided by a relative frequency distribution in tabular form is easier to grasp if presented graphically. using the midpoint of each interval and the
115,1,"['relative frequency', 'distribution', 'frequency', 'frequency distribution']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,table 1.7: relative frequency distribution of battery life
116,1,"['frequency', 'interval']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"class class frequency, relative interval midpoint f frequency 1.5–1.9 1.7 2 0.050 2.0–2.4 2.2 1 0.025 2.5–2.9 2.7 4 0.100 3.0–3.4 3.2 15 0.375 3.5–3.9 3.7 10 0.250 4.0–4.4 4.2 5 0.125 4.5–4.9 4.7 3 0.075"
117,1,"['histogram', 'frequency', 'relative frequency']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,figure 1.6: relative frequency histogram.
118,1,"['histogram', 'frequency', 'relative frequency']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"corresponding relative frequency, we construct a relative frequency histogram (figure 1.6)."
119,1,"['sample', 'curve', 'graphical', 'limit', 'distribution', 'probability distribution', 'frequency', 'probability', 'population', 'continuous', 'distributions']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"many continuous frequency distributions can be represented graphically by the characteristic bell-shaped curve of figure 1.7. graphical tools such as what we see in figures 1.6 and 1.7 aid in the characterization of the nature of the population. in chapters 5 and 6 we discuss a property of the population called its distribution. while a more rigorous definition of a distribution or probability distribution will be given later in the text, at this point one can view it as what would be seen in figure 1.7 in the limit as the size of the sample becomes larger."
120,1,"['long right tail', 'symmetric', 'skewed', 'distribution', 'symmetry', 'tail']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"a distribution is said to be symmetric if it can be folded along a vertical axis so that the two sides coincide. a distribution that lacks symmetry with respect to a vertical axis is said to be skewed. the distribution illustrated in figure 1.8(a) is said to be skewed to the right since it has a long right tail and a much shorter left tail. in figure 1.8(b) we see that the distribution is symmetric, while in figure 1.8(c) it is skewed to the left."
121,1,"['plot', 'histogram', 'data', 'distribution']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"if we rotate a stem-and-leaf plot counterclockwise through an angle of 90◦, we observe that the resulting columns of leaves form a picture that is similar to a histogram. consequently, if our primary purpose in looking at the data is to determine the general shape or form of the distribution, it will seldom be necessary"
122,1,"['distribution', 'frequency', 'frequency distribution']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,figure 1.7: estimating frequency distribution.
123,1,"['skewness', 'data']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,figure 1.8: skewness of data.
124,1,"['histogram', 'frequency', 'relative frequency']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,to construct a relative frequency histogram.
125,1,"['plot', 'sample', 'percentile', 'variability', 'median', 'range', 'observations', 'data', 'location', 'quartile', 'samples', 'upper quartile', 'interquartile range']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"another display that is helpful for reflecting properties of a sample is the boxand-whisker plot. this plot encloses the interquartile range of the data in a box that has the median displayed within. the interquartile range has as its extremes the 75th percentile (upper quartile) and the 25th percentile (lower quartile). in addition to the box, “whiskers” extend, showing extreme observations in the sample. for reasonably large samples, the display shows center of location, variability, and the degree of asymmetry."
126,1,"['outliers', 'observations', 'observation', 'outlier', 'probability', 'data', 'statistical tests', 'tests', 'statistical', 'regression', 'plot', 'regression analysis', 'box plot', 'variation']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"in addition, a variation called a box plot can provide the viewer with information regarding which observations may be outliers. outliers are observations that are considered to be unusually far from the bulk of the data. there are many statistical tests that are designed to detect outliers. technically, one may view an outlier as being an observation that represents a “rare event” (there is a small probability of obtaining a value that far from the bulk of the data). the concept of outliers resurfaces in chapter 12 in the context of regression analysis."
127,1,"['outliers', 'plot', 'range', 'observations', 'box plot', 'information', 'outlier', 'observation', 'plot or box', 'test', 'interquartile range']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"the visual information in the box-and-whisker plot or box plot is not intended to be a formal test for outliers. rather, it is viewed as a diagnostic tool. while the determination of which observations are outliers varies with the type of software that is used, one common procedure is to use a multiple of the interquartile range. for example, if the distance from the box exceeds 1.5 times the interquartile range (in either direction), the observation may be labeled an outlier."
128,1,"['sample', 'random', 'table', 'data', 'random sample']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,example 1.5: nicotine content was measured in a random sample of 40 cigarettes. the data are displayed in table 1.8.
129,1,['data'], Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,table 1.8: nicotine data for example 1.5
130,1,['plot'], Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,figure 1.9: box-and-whisker plot for example 1.5.
131,1,"['outliers', 'plot', 'mild outliers', 'range', 'data', 'observation', 'outlier', 'tail', 'mild outlier', 'interquartile range']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"figure 1.9 shows the box-and-whisker plot of the data, depicting the observations 0.72 and 0.85 as mild outliers in the lower tail, whereas the observation 2.55 is a mild outlier in the upper tail. in this example, the interquartile range is 0.365, and 1.5 times the interquartile range is 0.5475. figure 1.10, on the other hand, provides a stem-and-leaf plot."
132,1,"['outliers', 'plot', 'median', 'table', 'data', 'observation', 'quartile', 'samples', 'asymmetric', 'set', 'data set', 'upper quartile', 'measuring']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"example 1.6: consider the data in table 1.9, consisting of 30 samples measuring the thickness of paint can “ears” (see the work by hogg and ledolter, 1992, in the bibliography). figure 1.11 depicts a box-and-whisker plot for this asymmetric set of data. notice that the left block is considerably larger than the block on the right. the median is 35. the lower quartile is 31, while the upper quartile is 36. notice also that the extreme observation on the right is farther away from the box than the extreme observation on the left. there are no outliers in this data set."
133,1,"['plot', 'data']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,figure 1.10: stem-and-leaf plot for the nicotine data.
134,1,['data'], Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,table 1.9: data for example 1.6
135,1,"['sample', 'measurements']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,sample measurements sample measurements 1 29 36 39 34 34 16 35 30 35 29 37 2 29 29 28 32 31 17 40 31 38 35 31 3 34 34 39 38 37 18 35 36 30 33 32 4 35 37 33 38 41 19 35 34 35 30 36 5 30 29 31 38 29 20 35 35 31 38 36 6 34 31 37 39 36 21 32 36 36 32 36 7 30 35 33 40 36 22 36 37 32 34 34 8 28 28 31 34 30 23 29 34 33 37 35 9 32 36 38 38 35 24 36 36 35 37 37 10 35 30 37 35 31 25 36 30 35 33 31 11 35 30 35 38 35 26 35 30 29 38 35 12 38 34 35 35 31 27 35 36 30 34 36 13 34 35 33 30 34 28 35 30 36 29 35 14 40 35 34 33 35 29 38 36 35 31 31 15 34 35 38 35 30 30 30 34 40 28 30
136,1,"['graphical', 'variables', 'outlying', 'plots', 'observations', 'data', 'samples']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,there are additional ways that box-and-whisker plots and other graphical displays can aid the analyst. multiple samples can be compared graphically. plots of data can suggest relationships between variables. graphs can aid in the detection of anomalies or outlying observations in samples.
137,1,"['graphical', 'plots']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,there are other types of graphical tools and plots that are used. these are discussed in chapter 8 after we introduce additional theoretical details.
138,1,['plot'], Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,figure 1.11: box-and-whisker plot for thickness of paint can “ears.”
139,1,"['quartiles', 'sample', 'percentile', 'variability', 'median', 'data', 'location', 'distribution', 'quartile', 'third quartile', 'percentiles', 'tail', 'tails', 'first quartile']", Statistical Modeling Scientific Inspection and Graphical Diagnostics,seg_17,"there are features of the distribution or sample other than measures of center of location and variability that further define its nature. for example, while the median divides the data (or distribution) into two parts, there are other measures that divide parts or pieces of the distribution that can be very useful. separation is made into four parts by quartiles, with the third quartile separating the upper quarter of the data from the rest, the second quartile being the median, and the first quartile separating the lower quarter of the data from the rest. the distribution can be even more finely divided by computing percentiles of the distribution. these quantities give the analyst a sense of the so-called tails of the distribution (i.e., values that are relatively extreme, either small or large). for example, the 95th percentile separates the highest 5% from the bottom 95%. similar definitions prevail for extremes on the lower side or lower tail of the distribution. the 1st percentile separates the bottom 1% from the rest of the distribution. the concept of percentiles will play a major role in much that will be covered in future chapters."
140,1,"['experimental', 'combinations', 'experiment', 'results', 'information', 'sampling', 'population', 'statistical']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"in the foregoing sections we have emphasized the notion of sampling from a population and the use of statistical methods to learn or perhaps affirm important information about the population. the information sought and learned through the use of these statistical methods can often be influential in decision making and problem solving in many important scientific and engineering areas. as an illustration, example 1.3 describes a simple experiment in which the results may provide an aid in determining the kinds of conditions under which it is not advisable to use a particular aluminum alloy that may have a dangerous vulnerability to corrosion. the results may be of use not only to those who produce the alloy, but also to the customer who may consider using it. this illustration, as well as many more that appear in chapters 13 through 15, highlights the concept of designing or controlling experimental conditions (combinations of coating conditions and humidity) of"
141,1,"['model', 'level', 'measurement', 'variability', 'estimate', 'descriptive statistics', 'results', 'statistics', 'case', 'function', 'statistical model', 'statistical']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"interest to learn about some characteristic or measurement (level of corrosion) that results from these conditions. statistical methods that make use of measures of central tendency in the corrosion measure, as well as measures of variability, are employed. as the reader will observe later in the text, these methods often lead to a statistical model like that discussed in section 1.6. in this case, the model may be used to estimate (or predict) the corrosion measure as a function of humidity and the type of coating employed. again, in developing this kind of model, descriptive statistics that highlight central tendency and variability become very useful."
142,1,"['experiment', 'information', 'statistical']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,the information supplied in example 1.3 illustrates nicely the types of engineering questions asked and answered by the use of statistical methods that are employed through a designed experiment and presented in this text. they are
143,0,[], General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,(i) what is the nature of the impact of relative humidity on the corrosion of the
144,1,"['range', 'experiment']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,aluminum alloy within the range of relative humidity in this experiment?
145,1,['levels'], General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,(ii) does the chemical corrosion coating reduce corrosion levels and can the effect
146,0,[], General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,be quantified in some fashion?
147,1,['interaction'], General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,(iii) is there interaction between coating type and relative humidity that impacts
148,0,[], General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"their influence on corrosion of the alloy? if so, what is its interpretation?"
149,1,"['plot', 'sample', 'condition', 'experiment', 'sample means', 'factors', 'interaction', 'slope']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"the importance of questions (i) and (ii) should be clear to the reader, as they deal with issues important to both producers and users of the alloy. but what about question (iii)? the concept of interaction will be discussed at length in chapters 14 and 15. consider the plot in figure 1.3. this is an illustration of the detection of interaction between two factors in a simple designed experiment. note that the lines connecting the sample means are not parallel. parallelism would have indicated that the effect (seen as a result of the slope of the lines) of relative humidity is the same, namely a negative effect, for both an uncoated condition and the chemical corrosion coating. recall that the negative slope implies that corrosion becomes more pronounced as humidity rises. lack of parallelism implies an interaction between coating type and relative humidity. the nearly “flat” line for the corrosion coating as opposed to a steeper slope for the uncoated condition suggests that not only is the chemical corrosion coating beneficial (note the displacement between the lines), but the presence of the coating renders the effect of humidity negligible. clearly all these questions are very important to the effect of the two individual factors and to the interpretation of the interaction, if it is present."
150,1,"['levels', 'experiment', 'factors', 'data', 'confident', 'control']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"statistical models are extremely useful in answering questions such as those listed in (i), (ii), and (iii), where the data come from a designed experiment. but one does not always have the luxury or resources to employ a designed experiment. for example, there are many instances in which the conditions of interest to the scientist or engineer cannot be implemented simply because the important factors cannot be controlled. in example 1.3, the relative humidity and coating type (or lack of coating) are quite easy to control. this of course is the defining feature of a designed experiment. in many fields, factors that need to be studied cannot be controlled for any one of various reasons. tight control as in example 1.3 allows the analyst to be confident that any differences found (for example, in corrosion levels)"
151,1,"['levels', 'factors', 'experiment', 'factor', 'case', 'mean', 'control']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"are due to the factors under control. as a second illustration, consider exercise 1.6 on page 13. suppose in this case 24 specimens of silicone rubber are selected and 12 assigned to each of the curing temperature levels. the temperatures are controlled carefully, and thus this is an example of a designed experiment with a single factor being curing temperature. differences found in the mean tensile strength would be assumed to be attributed to the different curing temperatures."
152,1,"['experienced', 'experimental', 'levels', 'factors', 'observational study', 'factor', 'data', 'information', 'set', 'data set', 'random', 'level', 'experimental units', 'control']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"suppose there are no factors controlled and no random assignment of fixed treatments to experimental units and yet there is a need to glean information from a data set. as an illustration, consider a study in which interest centers around the relationship between blood cholesterol levels and the amount of sodium measured in the blood. a group of individuals were monitored over time for both blood cholesterol and sodium. certainly some useful information can be gathered from such a data set. however, it should be clear that there certainly can be no strict control of blood sodium levels. ideally, the subjects should be divided randomly into two groups, with one group assigned a specific high level of blood sodium and the other a specific low level of blood sodium. obviously this cannot be done. clearly changes in cholesterol can be experienced because of changes in one of a number of other factors that were not controlled. this kind of study, without factor control, is called an observational study. much of the time it involves a situation in which subjects are observed across time."
153,1,"['levels', 'observational studies', 'data']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"biological and biomedical studies are often by necessity observational studies. however, observational studies are not confined to those areas. for example, consider a study that is designed to determine the influence of ambient temperature on the electric power consumed by a chemical plant. clearly, levels of ambient temperature cannot be controlled, and thus the data structure can only be a monitoring of the data from the plant over time."
154,1,"['levels', 'factors', 'experiment', 'observational studies', 'randomization', 'process', 'response']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"it should be apparent that the striking difference between a well-designed experiment and observational studies is the difficulty in determination of true cause and effect with the latter. also, differences found in the fundamental response (e.g., corrosion levels, blood cholesterol, plant electric power consumption) may be due to other underlying factors that were not controlled. ideally, in a designed experiment the nuisance factors would be equalized via the randomization process. certainly changes in blood cholesterol could be due to fat intake, exercise activity, and so on. electric power consumption could be affected by the amount of product produced or even the purity of the product produced."
155,1,"['levels', 'factors', 'experiment', 'observational study', 'observational studies', 'data', 'set', 'data set', 'variation', 'experiments']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"another often ignored disadvantage of an observational study when compared to carefully designed experiments is that, unlike the latter, observational studies are at the mercy of nature, environmental or other uncontrolled circumstances that impact the ranges of factors of interest. for example, in the biomedical study regarding the influence of blood sodium levels on blood cholesterol, it is possible that there is indeed a strong influence but the particular data set used did not involve enough observed variation in sodium levels because of the nature of the subjects chosen. of course, in a designed experiment, the analyst chooses and controls ranges of factors."
156,1,"['statistics', 'data']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,30 chapter 1 introduction to statistics and data analysis
157,1,"['experiment', 'retrospective study', 'data', 'statistical', 'historical data']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"a third type of statistical study which can be very useful but has clear disadvantages when compared to a designed experiment is a retrospective study. this type of study uses strictly historical data, data taken over a specific period of time. one obvious advantage of retrospective data is that there is reduced cost in collecting the data. however, as one might expect, there are clear disadvantages."
158,1,"['historical data', 'data']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,(i) validity and reliability of historical data are often in doubt.
159,1,['data'], General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"(ii) if time is an important aspect of the structure of the data, there may be data"
160,1,"['errors', 'data']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,(iii) there may be errors in collection of the data that are not known.
161,1,"['case', 'data', 'control', 'observational data']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"(iv) again, as in the case of observational data, there is no control on the ranges"
162,1,"['factors', 'variables', 'data', 'historical data']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"of the measured variables (the factors in a study). indeed, the ranges found in historical data may not be relevant for current studies."
163,1,"['model', 'experimental', 'experiment', 'population mean', 'percentages', 'data', 'regression', 'case', 'regression analysis', 'mean', 'population', 'experimental units', 'experiments']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"in section 1.6, some attention was given to modeling of relationships among variables. we introduced the notion of regression analysis, which is covered in chapters 11 and 12 and is illustrated as a form of data analysis for designed experiments discussed in chapters 14 and 15. in section 1.6, a model relating population mean tensile strength of cloth to percentages of cotton was used for illustration, where 20 specimens of cloth represented the experimental units. in that case, the data came from a simple designed experiment where the individual cotton percentages were selected by the scientist."
164,1,"['model', 'model building', 'variables', 'data', 'set', 'statistical', 'data set', 'statistical model', 'experiments', 'average', 'historical data', 'observational data']", General Types of Statistical Studies Designed Experiment Observational Study and Retrospective Study,seg_19,"often both observational data and retrospective data are used for the purpose of observing relationships among variables through model-building procedures discussed in chapters 11 and 12. while the advantages of designed experiments certainly apply when the goal is statistical model building, there are many areas in which designing of experiments is not possible. thus, observational or historical data must be used. we refer here to a historical data set that is found in exercise 12.5 on page 450. the goal is to build a model that will result in an equation or relationship that relates monthly electric power consumed to average ambient temperature x1, the number of days in the month x2, the average product purity x3, and the tons of product produced x4. the data are the past year’s historical data."
165,1,"['categorical data', 'categorical', 'statistician', 'statistics', 'intersection', 'data', 'concentration', 'measurements', 'outcomes', 'numerical']", Sample Space,seg_25,"in the study of statistics, we are concerned basically with the presentation and interpretation of chance outcomes that occur in a planned study or scientific investigation. for example, we may record the number of accidents that occur monthly at the intersection of driftwood lane and royal oak drive, hoping to justify the installation of a traffic light; we might classify items coming off an assembly line as “defective” or “nondefective”; or we may be interested in the volume of gas released in a chemical reaction when the concentration of an acid is varied. hence, the statistician is often dealing with either numerical data, representing counts or measurements, or categorical data, which can be classified according to some criterion."
166,1,"['categorical data', 'categorical', 'observations', 'intersection', 'data', 'information', 'observation', 'set', 'numerical']", Sample Space,seg_25,"we shall refer to any recording of information, whether it be numerical or categorical, as an observation. thus, the numbers 2, 0, 1, and 2, representing the number of accidents that occurred for each month from january through april during the past year at the intersection of driftwood lane and royal oak drive, constitute a set of observations. similarly, the categorical data n, d, n, n, and d, representing the items found to be defective or nondefective when five items are inspected, are recorded as observations."
167,1,"['predicted', 'observations', 'set', 'measurements', 'process', 'experiment', 'data', 'cases', 'statistical', 'experimental', 'outcomes', 'tails']", Sample Space,seg_25,"statisticians use the word experiment to describe any process that generates a set of data. a simple example of a statistical experiment is the tossing of a coin. in this experiment, there are only two possible outcomes, heads or tails. another experiment might be the launching of a missile and observing of its velocity at specified times. the opinions of voters concerning a new sales tax can also be considered as observations of an experiment. we are particularly interested in the observations obtained by repeating the experiment several times. in most cases, the outcomes will depend on chance and, therefore, cannot be predicted with certainty. if a chemist runs an analysis several times under the same conditions, he or she will obtain different measurements, indicating an element of chance in the experimental procedure. even when a coin is tossed repeatedly, we cannot be certain that a given toss will result in a head. however, we know the entire set of possibilities for each toss."
168,1,"['experiment', 'observational studies', 'data', 'cases', 'set', 'experiments', 'statistical', 'retrospective studies']", Sample Space,seg_25,"given the discussion in section 1.7, we should deal with the breadth of the term experiment. three types of statistical studies were reviewed, and several examples were given of each. in each of the three cases, designed experiments, observational studies, and retrospective studies, the end result was a set of data that of course is"
169,1,"['process', 'experiment', 'observational study', 'retrospective study', 'uncertainty', 'data', 'outcome', 'average', 'historical data']", Sample Space,seg_25,"subject to uncertainty. though only one of these has the word experiment in its description, the process of generating the data or the process of observing the data is part of an experiment. the corrosion study discussed in section 1.2 certainly involves an experiment, with measures of corrosion representing the data. the example given in section 1.7 in which blood cholesterol and sodium were observed on a group of individuals represented an observational study (as opposed to a designed experiment), and yet the process generated data and the outcome is subject to uncertainty. thus, it is an experiment. a third example in section 1.7 represented a retrospective study in which historical data on monthly electric power consumption and average monthly ambient temperature were observed. even though the data may have been in the files for decades, the process is still referred to as an experiment."
170,1,"['sample', 'experiment', 'statistical', 'set', 'sample space', 'outcomes']", Sample Space,seg_25,definition 2.1: the set of all possible outcomes of a statistical experiment is called the sample space and is represented by the symbol s.
171,1,"['sample', 'outcome', 'sample space', 'outcomes']", Sample Space,seg_25,"each outcome in a sample space is called an element or a member of the sample space, or simply a sample point. if the sample space has a finite number of elements, we may list the members separated by commas and enclosed in braces. thus, the sample space s, of possible outcomes when a coin is flipped, may be written"
172,1,['tails'], Sample Space,seg_25,"where h and t correspond to heads and tails, respectively."
173,1,"['sample', 'experiment', 'sample space']", Sample Space,seg_25,"example 2.1: consider the experiment of tossing a die. if we are interested in the number that shows on the top face, the sample space is"
174,1,"['sample', 'sample space']", Sample Space,seg_25,"if we are interested only in whether the number is even or odd, the sample space is simply"
175,1,"['sample', 'experiment', 'tree diagram', 'case', 'information', 'outcome', 'experiments', 'sample space', 'outcomes']", Sample Space,seg_25,"example 2.1 illustrates the fact that more than one sample space can be used to describe the outcomes of an experiment. in this case, s1 provides more information than s2. if we know which element in s1 occurs, we can tell which outcome in s2 occurs; however, a knowledge of what happens in s2 is of little help in determining which element in s1 occurs. in general, it is desirable to use the sample space that gives the most information concerning the outcomes of the experiment. in some experiments, it is helpful to list the elements of the sample space systematically by means of a tree diagram."
176,1,"['sample', 'experiment', 'tree diagram', 'information', 'tail', 'sample space']", Sample Space,seg_25,"example 2.2: an experiment consists of flipping a coin and then flipping it a second time if a head occurs. if a tail occurs on the first flip, then a die is tossed once. to list the elements of the sample space providing the most information, we construct the tree diagram of figure 2.1. the various paths along the branches of the tree give the distinct sample points. starting with the top left branch and moving to the right along the first path, we get the sample point hh, indicating the possibility that heads occurs on two successive flips of the coin. likewise, the sample point t3 indicates the possibility that the coin will show a tail followed by a 3 on the toss of the die. by proceeding along all paths, we see that the sample space is"
177,1,['tree diagram'], Sample Space,seg_25,figure 2.1: tree diagram for example 2.2.
178,1,['process'], Sample Space,seg_25,"many of the concepts in this chapter are best illustrated with examples involving the use of dice and cards. these are particularly important applications to use early in the learning process, to facilitate the flow of these new concepts into scientific and engineering examples such as the following."
179,1,"['sample', 'process', 'tree diagram', 'information', 'random', 'sample space']", Sample Space,seg_25,"example 2.3: suppose that three items are selected at random from a manufacturing process. each item is inspected and classified defective, d, or nondefective, n. to list the elements of the sample space providing the most information, we construct the tree diagram of figure 2.2. now, the various paths along the branches of the tree give the distinct sample points. starting with the first path, we get the sample point ddd, indicating the possibility that all three items inspected are defective. as we proceed along the other paths, we see that the sample space is"
180,1,"['sample', 'method', 'experiment', 'rule method', 'set', 'population', 'sample space', 'outcomes']", Sample Space,seg_25,"sample spaces with a large or infinite number of sample points are best described by a statement or rule method. for example, if the possible outcomes of an experiment are the set of cities in the world with a population over 1 million, our sample space is written"
181,1,['population'], Sample Space,seg_25,"s = {x | x is a city with a population over 1 million},"
182,1,"['set', 'population']", Sample Space,seg_25,"which reads “s is the set of all x such that x is a city with a population over 1 million.” the vertical bar is read “such that.” similarly, if s is the set of all points (x, y) on the boundary or the interior of a circle of radius 2 with center at the origin, we write the rule"
183,1,['sample'], Sample Space,seg_25,first second third sample item item item point
184,1,['tree diagram'], Sample Space,seg_25,figure 2.2: tree diagram for example 2.3.
185,1,"['sample', 'method', 'rule method', 'experiments', 'sample space']", Sample Space,seg_25,"whether we describe the sample space by the rule method or by listing the elements will depend on the specific problem at hand. the rule method has practical advantages, particularly for many experiments where listing becomes a tedious chore."
186,1,"['sample', 'process', 'experiment', 'case', 'sampling', 'statistical', 'sample space']", Sample Space,seg_25,"consider the situation of example 2.3 in which items from a manufacturing process are either d, defective, or n , nondefective. there are many important statistical procedures called sampling plans that determine whether or not a “lot” of items is considered satisfactory. one such plan involves sampling until k defectives are observed. suppose the experiment is to sample items randomly until one defective item is observed. the sample space for this case is"
187,1,"['sample', 'experiment', 'events', 'event', 'outcome', 'sample space']", Events,seg_27,"for any given experiment, we may be interested in the occurrence of certain events rather than in the occurrence of a specific element in the sample space. for instance, we may be interested in the event a that the outcome when a die is tossed is divisible by 3. this will occur if the outcome is an element of the subset a = {3, 6} of the sample space s1 in example 2.1. as a further illustration, we may be interested in the event b that the number of defectives is greater than 1 in example 2.3. this will occur if the outcome is an element of the subset"
188,1,"['sample', 'sample space']", Events,seg_27,of the sample space s.
189,1,"['sample', 'event', 'sample space']", Events,seg_27,"to each event we assign a collection of sample points, which constitute a subset of the sample space. that subset represents all of the elements for which the event is true."
190,1,"['sample', 'event', 'sample space']", Events,seg_27,definition 2.2: an event is a subset of a sample space.
191,1,"['sample', 'event', 'sample space']", Events,seg_27,"example 2.4: given the sample space s = {t | t ≥ 0}, where t is the life in years of a certain electronic component, then the event a that the component fails before the end of the fifth year is the subset a = {t | 0 ≤ t < 5}."
192,1,"['sample', 'experiment', 'set', 'null set', 'event', 'sample space']", Events,seg_27,"it is conceivable that an event may be a subset that includes the entire sample space s or a subset of s called the null set and denoted by the symbol φ, which contains no elements at all. for instance, if we let a be the event of detecting a microscopic organism by the naked eye in a biological experiment, then a = φ. also, if"
193,1,['factor'], Events,seg_27,"b = {x | x is an even factor of 7},"
194,1,"['factors', 'set', 'null set']", Events,seg_27,"then b must be the null set, since the only possible factors of 7 are the odd numbers 1 and 7."
195,1,"['sample', 'experiment', 'complement', 'set', 'event', 'sample space']", Events,seg_27,"consider an experiment where the smoking habits of the employees of a manufacturing firm are recorded. a possible sample space might classify an individual as a nonsmoker, a light smoker, a moderate smoker, or a heavy smoker. let the subset of smokers be some event. then all the nonsmokers correspond to a different event, also a subset of s, which is called the complement of the set of smokers."
196,1,"['complement of an event', 'complement', 'event']", Events,seg_27,definition 2.3: the complement of an event a with respect to s is the subset of all elements of s that are not in a. we denote the complement of a by the symbol a′.
197,1,['event'], Events,seg_27,"example 2.5: let r be the event that a red card is selected from an ordinary deck of 52 playing cards, and let s be the entire deck. then r′ is the event that the card selected from the deck is not a red card but a black card."
198,1,"['sample', 'sample space']", Events,seg_27,example 2.6: consider the sample space
199,0,[], Events,seg_27,"s = {book, cell phone, mp3, paper, stationery, laptop}."
200,1,['complement'], Events,seg_27,"let a = {book, stationery, laptop, paper}. then the complement of a is a′ = {cell phone, mp3}."
201,1,"['sample', 'experiment', 'events', 'associated', 'event', 'sample space']", Events,seg_27,"we now consider certain operations with events that will result in the formation of new events. these new events will be subsets of the same sample space as the given events. suppose that a and b are two events associated with an experiment. in other words, a and b are subsets of the same sample space s. for example, in the tossing of a die we might let a be the event that an even number occurs and b the event that a number greater than 3 shows. then the subsets a = {2, 4, 6} and b = {4, 5, 6} are subsets of the same sample space"
202,1,"['outcome', 'intersection']", Events,seg_27,"note that both a and b will occur on a given toss if the outcome is an element of the subset {4, 6}, which is just the intersection of a and b."
203,1,"['events', 'intersection', 'event']", Events,seg_27,"definition 2.4: the intersection of two events a and b, denoted by the symbol a ∩ b, is the event containing all elements that are common to a and b."
204,1,"['random', 'event']", Events,seg_27,"example 2.7: let e be the event that a person selected at random in a classroom is majoring in engineering, and let f be the event that the person is female. then e ∩ f is the event of all female engineering students in the classroom."
205,1,"['mutually exclusive', 'events', 'experiments', 'statistical']", Events,seg_27,"for certain statistical experiments it is by no means unusual to define two events, a and b, that cannot both occur simultaneously. the events a and b are then said to be mutually exclusive. stated more formally, we have the following definition:"
206,1,"['events', 'mutually exclusive', 'disjoint']", Events,seg_27,"definition 2.5: two events a and b are mutually exclusive, or disjoint, if a ∩ b = φ, that is, if a and b have no elements in common."
207,1,"['mutually exclusive', 'intersection', 'events', 'set', 'event']", Events,seg_27,"example 2.9: a cable television company offers programs on eight different channels, three of which are affiliated with abc, two with nbc, and one with cbs. the other two are an educational channel and the espn sports channel. suppose that a person subscribing to this service turns on a television set without first selecting the channel. let a be the event that the program belongs to the nbc network and b the event that it belongs to the cbs network. since a television program cannot belong to more than one network, the events a and b have no programs in common. therefore, the intersection a ∩ b contains no programs, and consequently the events a and b are mutually exclusive."
208,1,"['experiment', 'events', 'associated']", Events,seg_27,"often one is interested in the occurrence of at least one of two events associated with an experiment. thus, in the die-tossing experiment, if"
209,1,"['union', 'event', 'outcome']", Events,seg_27,"we might be interested in either a or b occurring or both a and b occurring. such an event, called the union of a and b, will occur if the outcome is an element of the subset {2, 4, 5, 6}."
210,1,"['events', 'union', 'event']", Events,seg_27,"definition 2.6: the union of the two events a and b, denoted by the symbol a∪b, is the event containing all the elements that belong to a or b or both."
211,1,"['set', 'random', 'event']", Events,seg_27,example 2.11: let p be the event that an employee selected at random from an oil drilling company smokes cigarettes. let q be the event that the employee selected drinks alcoholic beverages. then the event p ∪ q is the set of all employees who either drink or smoke or do both.
212,1,"['sample', 'events', 'venn diagram', 'venn diagrams', 'venn', 'sample space']", Events,seg_27,"m ∪n = {z | 3 < z < 12}. the relationship between events and the corresponding sample space can be illustrated graphically by means of venn diagrams. in a venn diagram we let the sample space be a rectangle and represent events by circles drawn inside the rectangle. thus, in figure 2.3, we see that"
213,1,['events'], Events,seg_27,figure 2.3: events represented by various regions.
214,1,"['sample', 'events', 'sample space']", Events,seg_27,figure 2.4: events of the sample space s.
215,1,"['sample', 'mutually exclusive', 'events', 'random', 'sample space', 'event']", Events,seg_27,"in figure 2.4, we see that events a, b, and c are all subsets of the sample space s. it is also clear that event b is a subset of event a; event b ∩ c has no elements and hence b and c are mutually exclusive; event a ∩ c has at least one element; and event a ∪ b = a. figure 2.4 might, therefore, depict a situation where we select a card at random from an ordinary deck of 52 playing cards and observe whether the following events occur:"
216,1,['probability'], Events,seg_27,42 chapter 2 probability
217,0,[], Events,seg_27,"b: the card is the jack, queen, or king of diamonds,"
218,1,['event'], Events,seg_27,"clearly, the event a ∩ c consists of only the two red aces."
219,1,"['results', 'venn', 'venn diagrams']", Events,seg_27,"several results that follow from the foregoing definitions, which may easily be verified by means of venn diagrams, are as follows:"
220,1,"['sample', 'multiplication rule', 'experiment', 'statistician', 'cases', 'events', 'associated', 'probability', 'sample space']", Counting Sample Points,seg_31,"one of the problems that the statistician must consider and attempt to evaluate is the element of chance associated with the occurrence of certain events when an experiment is performed. these problems belong in the field of probability, a subject to be introduced in section 2.4. in many cases, we shall be able to solve a probability problem by counting the number of points in the sample space without actually listing each element. the fundamental principle of counting, often referred to as the multiplication rule, is stated in rule 2.1."
221,0,[], Counting Sample Points,seg_31,"rule 2.1: if an operation can be performed in n1 ways, and if for each of these ways a second operation can be performed in n2 ways, then the two operations can be performed together in n1n2 ways."
222,1,"['sample', 'sample space']", Counting Sample Points,seg_31,example 2.13: how many sample points are there in the sample space when a pair of dice is thrown once?
223,0,[], Counting Sample Points,seg_31,"solution : the first die can land face-up in any one of n1 = 6 ways. for each of these 6 ways, the second die can also land face-up in n2 = 6 ways. therefore, the pair of dice can land in n1n2 = (6)(6) = 36 possible ways."
224,0,[], Counting Sample Points,seg_31,"example 2.14: a developer of a new subdivision offers prospective home buyers a choice of tudor, rustic, colonial, and traditional exterior styling in ranch, two-story, and split-level floor plans. in how many different ways can a buyer order one of these homes?"
225,1,['tree diagram'], Counting Sample Points,seg_31,figure 2.6: tree diagram for example 2.14.
226,0,[], Counting Sample Points,seg_31,"solution : since n1 = 4 and n2 = 3, a buyer must choose from"
227,1,['tree diagrams'], Counting Sample Points,seg_31,"the answers to the two preceding examples can be verified by constructing tree diagrams and counting the various paths along the branches. for instance,"
228,1,['tree diagram'], Counting Sample Points,seg_31,"in example 2.14 there will be n1 = 4 branches corresponding to the different exterior styles, and then there will be n2 = 3 branches extending from each of these 4 branches to represent the different floor plans. this tree diagram yields the n1n2 = 12 choices of homes given by the paths along the branches, as illustrated in figure 2.6."
229,0,[], Counting Sample Points,seg_31,"example 2.15: if a 22-member club needs to elect a chair and a treasurer, how many different ways can these two to be elected?"
230,1,['multiplication rule'], Counting Sample Points,seg_31,"solution : for the chair position, there are 22 total possibilities. for each of those 22 possibilities, there are 21 possibilities to elect the treasurer. using the multiplication rule, we obtain n1 × n2 = 22× 21 = 462 different ways."
231,1,"['sets', 'multiplication rule']", Counting Sample Points,seg_31,"the multiplication rule, rule 2.1 may be extended to cover any number of operations. suppose, for instance, that a customer wishes to buy a new cell phone and can choose from n1 = 5 brands, n2 = 5 sets of capability, and n3 = 4 colors. these three classifications result in n1n2n3 = (5)(5)(4) = 100 different ways for a customer to order one of these phones. the generalized multiplication rule covering k operations is stated in the following."
232,0,[], Counting Sample Points,seg_31,"if an operation can be performed in n1 ways, and if for each of these a second"
233,0,[], Counting Sample Points,seg_31,"rule 2.2: operation can be performed in n2 ways, and for each of the first two a third operation can be performed in n3 ways, and so forth, then the sequence of k operations can be performed in n1n2 · · ·nk ways."
234,1,['memory'], Counting Sample Points,seg_31,"example 2.16: sam is going to assemble a computer by himself. he has the choice of chips from two brands, a hard drive from four, memory from three, and an accessory bundle from five local stores. how many different ways can sam order the parts?"
235,0,[], Counting Sample Points,seg_31,different ways to order the parts.
236,0,[], Counting Sample Points,seg_31,"example 2.17: how many even four-digit numbers can be formed from the digits 0, 1, 2, 5, 6, and 9 if each digit can be used only once?"
237,1,['case'], Counting Sample Points,seg_31,"solution : since the number must be even, we have only n1 = 3 choices for the units position. however, for a four-digit number the thousands position cannot be 0. hence, we consider the units position in two parts, 0 or not 0. if the units position is 0 (i.e., n1 = 1), we have n2 = 5 choices for the thousands position, n3 = 4 for the hundreds position, and n4 = 3 for the tens position. therefore, in this case we have a total of"
238,0,[], Counting Sample Points,seg_31,"even four-digit numbers. on the other hand, if the units position is not 0 (i.e., n1 = 2), we have n2 = 4 choices for the thousands position, n3 = 4 for the hundreds position, and n4 = 3 for the tens position. in this situation, there are a total of"
239,0,[], Counting Sample Points,seg_31,even four-digit numbers.
240,1,"['cases', 'mutually exclusive']", Counting Sample Points,seg_31,"since the above two cases are mutually exclusive, the total number of even four-digit numbers can be calculated as 60 + 96 = 156."
241,1,"['sample', 'permutations', 'table', 'sample space']", Counting Sample Points,seg_31,"frequently, we are interested in a sample space that contains as elements all possible orders or arrangements of a group of objects. for example, we may want to know how many different arrangements are possible for sitting 6 people around a table, or we may ask how many different orders are possible for drawing 2 lottery tickets from a total of 20. the different arrangements are called permutations."
242,1,"['permutation', 'set']", Counting Sample Points,seg_31,definition 2.7: a permutation is an arrangement of all or part of a set of objects.
243,1,['permutations'], Counting Sample Points,seg_31,"consider the three letters a, b, and c. the possible permutations are abc, acb, bac, bca, cab, and cba. thus, we see that there are 6 distinct arrangements. using rule 2.2, we could arrive at the answer 6 without actually listing the different orders by the following arguments: there are n1 = 3 choices for the first position. no matter which letter is chosen, there are always n2 = 2 choices for the second position. no matter which two letters are chosen for the first two positions, there is only n3 = 1 choice for the last position, giving a total of"
244,1,['permutations'], Counting Sample Points,seg_31,n1n2n3 = (3)(2)(1) = 6 permutations
245,0,['n'], Counting Sample Points,seg_31,"by rule 2.2. in general, n distinct objects can be arranged in"
246,0,[], Counting Sample Points,seg_31,there is a notation for such a number.
247,0,[], Counting Sample Points,seg_31,"definition 2.8: for any non-negative integer n, n!, called “n factorial,” is defined as"
248,1,['case'], Counting Sample Points,seg_31,with special case 0! = 1.
249,0,[], Counting Sample Points,seg_31,"using the argument above, we arrive at the following theorem."
250,1,['permutations'], Counting Sample Points,seg_31,the number of permutations of n objects is n!.
251,1,['permutations'], Counting Sample Points,seg_31,"the number of permutations of the four letters a, b, c, and d will be 4! = 24. now consider the number of permutations that are possible by taking two letters at a time from four. these would be ab, ac, ad, ba, bc, bd, ca, cb, cd, da, db, and dc. using rule 2.1 again, we have two positions to fill, with n1 = 4 choices for the first and then n2 = 3 choices for the second, for a total of"
252,0,['n'], Counting Sample Points,seg_31,"permutations. in general, n distinct objects taken r at a time can be arranged in"
253,0,[], Counting Sample Points,seg_31,ways. we represent this product by the symbol
254,0,[], Counting Sample Points,seg_31,"as a result, we have the theorem that follows."
255,1,['permutations'], Counting Sample Points,seg_31,the number of permutations of n distinct objects taken r at a time is
256,1,['statistics'], Counting Sample Points,seg_31,"example 2.18: in one year, three awards (research, teaching, and service) will be given to a class of 25 graduate students in a statistics department. if each student can receive at most one award, how many possible selections are there?"
257,1,"['sample', 'permutation']", Counting Sample Points,seg_31,"solution : since the awards are distinguishable, it is a permutation problem. the total number of sample points is"
258,0,[], Counting Sample Points,seg_31,example 2.19: a president and a treasurer are to be chosen from a student club consisting of 50 people. how many different choices of officers are possible if
259,0,[], Counting Sample Points,seg_31,(a) there are no restrictions;
260,0,[], Counting Sample Points,seg_31,(b) a will serve only if he is president;
261,0,[], Counting Sample Points,seg_31,"(a) the total number of choices of officers, without any restrictions, is"
262,0,[], Counting Sample Points,seg_31,"(b) since a will serve only if he is president, we have two situations here: (i) a is"
263,1,['outcomes'], Counting Sample Points,seg_31,"selected as the president, which yields 49 possible outcomes for the treasurer’s position, or (ii) officers are selected from the remaining 49 people without a, which has the number of choices 49p2 = (49)(48) = 2352. therefore, the total number of choices is 49 + 2352 = 2401."
264,0,[], Counting Sample Points,seg_31,(c) the number of selections when b and c serve together is 2. the number of
265,0,[], Counting Sample Points,seg_31,"selections when both b and c are not chosen is 48p2 = 2256. therefore, the total number of choices in this situation is 2 + 2256 = 2258."
266,0,['e'], Counting Sample Points,seg_31,(d) the number of selections when d serves as an officer but not e is (2)(48) =
267,0,['e'], Counting Sample Points,seg_31,"96, where 2 is the number of positions d can take and 48 is the number of selections of the other officer from the remaining people in the club except e. the number of selections when e serves as an officer but not d is also (2)(48) = 96. the number of selections when both d and e are not chosen is 48p2 = 2256. therefore, the total number of choices is (2)(96) + 2256 = 2448. this problem also has another short solution: since d and e can only serve together in 2 ways, the answer is 2450− 2 = 2448."
268,1,"['permutation', 'permutations']", Counting Sample Points,seg_31,"permutations that occur by arranging objects in a circle are called circular permutations. two circular permutations are not considered different unless corresponding objects in the two arrangements are preceded or followed by a different object as we proceed in a clockwise direction. for example, if 4 people are playing bridge, we do not have a new permutation if they all move one position in a clockwise direction. by considering one person in a fixed position and arranging the other three in 3! ways, we find that there are 6 distinct arrangements for the bridge game."
269,1,['permutations'], Counting Sample Points,seg_31,theorem 2.3: the number of permutations of n objects arranged in a circle is (n− 1)!.
270,1,['permutations'], Counting Sample Points,seg_31,"so far we have considered permutations of distinct objects. that is, all the objects were completely different or distinguishable. obviously, if the letters b and c are both equal to x, then the 6 permutations of the letters a, b, and c become axx, axx, xax, xax, xxa, and xxa, of which only 3 are distinct. therefore, with 3 letters, 2 being the same, we have 3!/2! = 3 distinct permutations. with 4 different letters a, b, c, and d, we have 24 distinct permutations. if we let a = b = x and c = d = y, we can list only the following distinct permutations: xxyy, xyxy, yxxy, yyxx, xyyx, and yxyx. thus, we have 4!/(2! 2!) = 6 distinct permutations."
271,1,['permutations'], Counting Sample Points,seg_31,"theorem 2.4: the number of distinct permutations of n things of which n1 are of one kind, n2 of a second kind, . . . , nk of a kth kind is"
272,1,['level'], Counting Sample Points,seg_31,"example 2.20: in a college football training session, the defensive coordinator needs to have 10 players standing in a row. among these 10 players, there are 1 freshman, 2 sophomores, 4 juniors, and 3 seniors. how many different ways can they be arranged in a row if only their class level will be distinguished?"
273,0,[], Counting Sample Points,seg_31,"solution : directly using theorem 2.4, we find that the total number of arrangements is"
274,1,"['partitions', 'intersection', 'set', 'union']", Counting Sample Points,seg_31,"often we are concerned with the number of ways of partitioning a set of n objects into r subsets called cells. a partition has been achieved if the intersection of every possible pair of the r subsets is the empty set φ and if the union of all subsets gives the original set. the order of the elements within a cell is of no importance. consider the set {a, e, i, o, u}. the possible partitions into two cells in which the first cell contains 4 elements and the second cell 1 element are"
275,1,['set'], Counting Sample Points,seg_31,"we see that there are 5 ways to partition a set of 4 elements into two subsets, or cells, containing 4 elements in the first cell and 1 element in the second."
276,1,['partitions'], Counting Sample Points,seg_31,the number of partitions for this illustration is denoted by the symbol
277,0,[], Counting Sample Points,seg_31,where the top number represents the total number of elements and the bottom numbers represent the number of elements going into each cell. we state this more generally in theorem 2.5.
278,1,['set'], Counting Sample Points,seg_31,"theorem 2.5: the number of ways of partitioning a set of n objects into r cells with n1 elements in the first cell, n2 elements in the second, and so forth, is"
279,0,[], Counting Sample Points,seg_31,example 2.21: in how many ways can 7 graduate students be assigned to 1 triple and 2 double hotel rooms during a conference?
280,1,['partitions'], Counting Sample Points,seg_31,solution : the total number of possible partitions would be
281,1,"['combination', 'combinations']", Counting Sample Points,seg_31,"in many problems, we are interested in the number of ways of selecting r objects from n without regard to order. these selections are called combinations. a combination is actually a partition with two cells, the one cell containing the r objects selected and the other cell containing the (n− r) objects that are left. the number of such combinations, denoted by"
282,0,[], Counting Sample Points,seg_31,since the number of elements in the second cell must be n− r.
283,1,['combinations'], Counting Sample Points,seg_31,the number of combinations of n distinct objects taken r at a time is
284,0,[], Counting Sample Points,seg_31,example 2.22: a young boy asks his mother to get 5 game-boytm cartridges from his collection of 10 arcade and 5 sports games. how many ways are there that his mother can get 3 arcade and 2 sports games?
285,0,[], Counting Sample Points,seg_31,solution : the number of ways of selecting 3 cartridges from 10 is
286,0,[], Counting Sample Points,seg_31,the number of ways of selecting 2 cartridges from 5 is
287,1,['multiplication rule'], Counting Sample Points,seg_31,"using the multiplication rule (rule 2.1) with n1 = 120 and n2 = 10, we have (120)(10) = 1200 ways."
288,1,['statistics'], Counting Sample Points,seg_31,example 2.23: how many different letter arrangements can be made from the letters in the word statistics?
289,0,[], Counting Sample Points,seg_31,"solution : using the same argument as in the discussion for theorem 2.6, in this example we can actually apply theorem 2.5 to obtain"
290,0,[], Counting Sample Points,seg_31,"here we have 10 total letters, with 2 letters (s, t ) appearing 3 times each, letter i appearing twice, and letters a and c appearing once each. on the other hand, this result can be directly obtained by using theorem 2.4."
291,1,"['statistical inference', 'probability theory', 'bernoulli', 'probability', 'statistical', 'predictions']", Probability of an Event,seg_35,"perhaps it was humankind’s unquenchable thirst for gambling that led to the early development of probability theory. in an effort to increase their winnings, gamblers called upon mathematicians to provide optimum strategies for various games of chance. some of the mathematicians providing these strategies were pascal, leibniz, fermat, and james bernoulli. as a result of this development of probability theory, statistical inference, with all its predictions and generalizations, has branched out far beyond games of chance to encompass many other fields associated with chance occurrences, such as politics, business, weather forecasting,"
292,1,"['predictions', 'probability theory', 'probability']", Probability of an Event,seg_35,"and scientific research. for these predictions and generalizations to be reasonably accurate, an understanding of basic probability theory is essential."
293,1,"['experiment', 'case', 'information', 'mean', 'outcome', 'confidence']", Probability of an Event,seg_35,"what do we mean when we make the statement “john will probably win the tennis match,” or “i have a fifty-fifty chance of getting an even number when a die is tossed,” or “the university is not likely to win the football game tonight,” or “most of our graduating class will likely be married within 3 years”? in each case, we are expressing an outcome of which we are not certain, but owing to past information or from an understanding of the structure of the experiment, we have some degree of confidence in the validity of the statement."
294,1,"['simple events', 'set', 'probability', 'sample', 'experiment', 'events', 'event', 'statistical', 'sample space', 'probabilities', 'likelihood', 'experiments']", Probability of an Event,seg_35,"throughout the remainder of this chapter, we consider only those experiments for which the sample space contains a finite number of elements. the likelihood of the occurrence of an event resulting from such a statistical experiment is evaluated by means of a set of real numbers, called weights or probabilities, ranging from 0 to 1. to every point in the sample space we assign a probability such that the sum of all probabilities is 1. if we have reason to believe that a certain sample point is quite likely to occur when the experiment is conducted, the probability assigned should be close to 1. on the other hand, a probability closer to 0 is assigned to a sample point that is not likely to occur. in many experiments, such as tossing a coin or a die, all the sample points have the same chance of occurring and are assigned equal probabilities. for points outside the sample space, that is, for simple events that cannot possibly occur, we assign a probability of 0."
295,1,"['sample', 'probabilities', 'probability of an event', 'probability', 'event']", Probability of an Event,seg_35,"to find the probability of an event a, we sum all the probabilities assigned to the sample points in a. this sum is called the probability of a and is denoted by p (a)."
296,1,"['sample', 'probability of an event', 'probability', 'event']", Probability of an Event,seg_35,"definition 2.9: the probability of an event a is the sum of the weights of all sample points in a. therefore,"
297,1,"['mutually exclusive', 'events', 'mutually exclusive events']", Probability of an Event,seg_35,"furthermore, if a1, a2, a3, . . . is a sequence of mutually exclusive events, then"
298,1,"['sample', 'experiment', 'probability', 'sample space']", Probability of an Event,seg_35,example 2.24: a coin is tossed twice. what is the probability that at least 1 head occurs? solution : the sample space for this experiment is
299,1,"['sample', 'probability', 'event', 'outcomes']", Probability of an Event,seg_35,"if the coin is balanced, each of these outcomes is equally likely to occur. therefore, we assign a probability of ω to each sample point. then 4ω = 1, or ω = 1/4. if a represents the event of at least 1 head occurring, then"
300,1,['event'], Probability of an Event,seg_35,"example 2.25: a die is loaded in such a way that an even number is twice as likely to occur as an odd number. if e is the event that a number less than 4 occurs on a single toss of the die, find p (e)."
301,1,"['sample', 'probabilities', 'probability', 'sample space']", Probability of an Event,seg_35,"solution : the sample space is s = {1, 2, 3, 4, 5, 6}. we assign a probability of w to each odd number and a probability of 2w to each even number. since the sum of the probabilities must be 1, we have 9w = 1 or w = 1/9. hence, probabilities of 1/9 and 2/9 are assigned to each odd and even number, respectively. therefore,"
302,1,['event'], Probability of an Event,seg_35,"example 2.26: in example 2.25, let a be the event that an even number turns up and let b be the event that a number divisible by 3 occurs. find p (a ∪b) and p (a ∩b)."
303,1,['events'], Probability of an Event,seg_35,"solution : for the events a = {2, 4, 6} and b = {3, 6}, we have"
304,1,['probability'], Probability of an Event,seg_35,"by assigning a probability of 1/9 to each odd number and 2/9 to each even number, we have"
305,1,"['sample', 'experiment', 'probability', 'event', 'sample space']", Probability of an Event,seg_35,"if the sample space for an experiment contains n elements, all of which are equally likely to occur, we assign a probability equal to 1/n to each of the n points. the probability of any event a containing n of these n sample points is then the ratio of the number of elements in a to the number of elements in s."
306,1,"['outcomes', 'experiment']", Probability of an Event,seg_35,"if an experiment can result in any one of n different equally likely outcomes, and"
307,1,"['probability of event', 'probability', 'event', 'outcomes']", Probability of an Event,seg_35,"rule 2.3: if exactly n of these outcomes correspond to event a, then the probability of event a is"
308,1,"['statistics', 'probability']", Probability of an Event,seg_35,"example 2.27: a statistics class for engineers consists of 25 industrial, 10 mechanical, 10 electrical, and 8 civil engineering students. if a person is randomly selected by the instructor to answer a question, find the probability that the student chosen is (a) an industrial engineering major and (b) a civil engineering or an electrical engineering major."
309,0,['e'], Probability of an Event,seg_35,"solution : denote by i, m , e, and c the students majoring in industrial, mechanical, electrical, and civil engineering, respectively. the total number of students in the class is 53, all of whom are equally likely to be selected."
310,0,[], Probability of an Event,seg_35,"(a) since 25 of the 53 students are majoring in industrial engineering, the prob-"
311,1,"['random', 'event']", Probability of an Event,seg_35,"ability of event i, selecting an industrial engineering major at random, is"
312,0,[], Probability of an Event,seg_35,"(b) since 18 of the 53 students are civil or electrical engineering majors, it follows"
313,1,['probability'], Probability of an Event,seg_35,"example 2.28: in a poker hand consisting of 5 cards, find the probability of holding 2 aces and 3 jacks."
314,0,[], Probability of an Event,seg_35,solution : the number of ways of being dealt 2 aces from 4 cards is
315,0,[], Probability of an Event,seg_35,and the number of ways of being dealt 3 jacks from 4 cards is
316,1,['multiplication rule'], Probability of an Event,seg_35,"by the multiplication rule (rule 2.1), there are n = (6)(4) = 24 hands with 2 aces and 3 jacks. the total number of 5-card poker hands, all of which are equally likely, is"
317,1,['probability'], Probability of an Event,seg_35,"therefore, the probability of getting 2 aces and 3 jacks in a 5-card poker hand is"
318,1,"['experimental', 'probabilities', 'estimate', 'experiment', 'relative frequency', 'frequency', 'probability', 'tails', 'outcomes']", Probability of an Event,seg_35,"if the outcomes of an experiment are not equally likely to occur, the probabilities must be assigned on the basis of prior knowledge or experimental evidence. for example, if a coin is not balanced, we could estimate the probabilities of heads and tails by tossing the coin a large number of times and recording the outcomes. according to the relative frequency definition of probability, the true probabilities would be the fractions of heads and tails that occur in the long run. another intuitive way of understanding probability is the indifference approach. for instance, if you have a die that you believe is balanced, then using this indifference approach, you determine that the probability that each of the six sides will show up after a throw is 1/6."
319,1,"['probabilities', 'information', 'probability', 'subjective', 'numerical']", Probability of an Event,seg_35,"to find a numerical value that represents adequately the probability of winning at tennis, we must depend on our past performance at the game as well as that of the opponent and, to some extent, our belief in our ability to win. similarly, to find the probability that a horse will win a race, we must arrive at a probability based on the previous records of all the horses entered in the race as well as the records of the jockeys riding the horses. intuition would undoubtedly also play a part in determining the size of the bet that we might be willing to wager. the use of intuition, personal beliefs, and other indirect information in arriving at probabilities is referred to as the subjective definition of probability."
320,1,"['probabilities', 'experiment', 'relative frequency', 'limiting relative frequency', 'frequency', 'probability', 'experiments', 'statistical']", Probability of an Event,seg_35,"in most of the applications of probability in this book, the relative frequency interpretation of probability is the operative one. its foundation is the statistical experiment rather than subjectivity, and it is best viewed as the limiting relative frequency. as a result, many applications of probability in science and engineering must be based on experiments that can be repeated. less objective notions of probability are encountered when we assign probabilities based on prior information and opinions, as in “there is a good chance that the giants will lose the super"
321,1,"['bayesian', 'bayesian statistics', 'statistics', 'information', 'prior probability', 'probability', 'subjective', 'subjective probability']", Probability of an Event,seg_35,"bowl.” when opinions and prior information differ from individual to individual, subjective probability becomes the relevant resource. in bayesian statistics (see chapter 18), a more subjective interpretation of probability will be used, based on an elicitation of prior probability information."
322,1,"['probabilities', 'additive rule', 'events', 'complement', 'union', 'probability', 'event']", Additive Rules,seg_37,"often it is easiest to calculate the probability of some event from known probabilities of other events. this may well be true if the event in question can be represented as the union of two other events or as the complement of some event. several important laws that frequently simplify the computation of probabilities follow. the first, called the additive rule, applies to unions of events."
323,1,['events'], Additive Rules,seg_37,"if a and b are two events, then"
324,1,"['probability', 'additive rule']", Additive Rules,seg_37,figure 2.7: additive rule of probability.
325,1,"['sample', 'probabilities', 'venn diagram', 'probability', 'venn']", Additive Rules,seg_37,"proof : consider the venn diagram in figure 2.7. the p (a ∪ b) is the sum of the probabilities of the sample points in a ∪ b. now p (a) + p (b) is the sum of all the probabilities in a plus the sum of all the probabilities in b. therefore, we have added the probabilities in (a ∩ b) twice. since these probabilities add up to p (a ∩ b), we must subtract this probability once to obtain the sum of the probabilities in a ∪b."
326,1,['mutually exclusive'], Additive Rules,seg_37,"if a and b are mutually exclusive, then"
327,1,['mutually exclusive'], Additive Rules,seg_37,"corollary 2.1 is an immediate result of theorem 2.7, since if a and b are mutually exclusive, a∩b = 0 and then p (a∩b) = p (φ) = 0. in general, we can write corollary 2.2."
328,1,['mutually exclusive'], Additive Rules,seg_37,"corollary 2.2: if a1, a2, . . . , an are mutually exclusive, then"
329,1,"['sample', 'mutually exclusive', 'events', 'sample space']", Additive Rules,seg_37,"a collection of events {a1, a2, . . . , an} of a sample space s is called a partition of s if a1, a2, . . . , an are mutually exclusive and a1 ∪ a2 ∪ · · · ∪ an = s. thus, we have"
330,1,"['sample', 'sample space']", Additive Rules,seg_37,"corollary 2.3: if a1, a2, . . . , an is a partition of sample space s, then"
331,0,[], Additive Rules,seg_37,"as one might expect, theorem 2.7 extends in an analogous fashion."
332,1,['events'], Additive Rules,seg_37,"for three events a, b, and c,"
333,1,['probability'], Additive Rules,seg_37,"example 2.29: john is going to graduate from an industrial engineering department in a university by the end of the semester. after being interviewed at two companies he likes, he assesses that his probability of getting an offer from company a is 0.8, and his probability of getting an offer from company b is 0.6. if he believes that the probability that he will get offers from both companies is 0.5, what is the probability that he will get at least one offer from these two companies?"
334,1,['additive rule'], Additive Rules,seg_37,"solution : using the additive rule, we have"
335,1,['probability'], Additive Rules,seg_37,example 2.30: what is the probability of getting a total of 7 or 11 when a pair of fair dice is tossed?
336,1,"['sample', 'mutually exclusive', 'events', 'event']", Additive Rules,seg_37,"solution : let a be the event that 7 occurs and b the event that 11 comes up. now, a total of 7 occurs for 6 of the 36 sample points, and a total of 11 occurs for only 2 of the sample points. since all sample points are equally likely, we have p (a) = 1/6 and p (b) = 1/18. the events a and b are mutually exclusive, since a total of 7 and 11 cannot both occur on the same toss. therefore,"
337,1,['event'], Additive Rules,seg_37,"this result could also have been obtained by counting the total number of points for the event a ∪b, namely 8, and writing"
338,1,"['sample', 'probabilities', 'states', 'events', 'probability', 'sample space']", Additive Rules,seg_37,"theorem 2.7 and its three corollaries should help the reader gain more insight into probability and its interpretation. corollaries 2.1 and 2.2 suggest the very intuitive result dealing with the probability of occurrence of at least one of a number of events, no two of which can occur simultaneously. the probability that at least one occurs is the sum of the probabilities of occurrence of the individual events. the third corollary simply states that the highest value of a probability (unity) is assigned to the entire sample space s."
339,1,"['probabilities', 'probability']", Additive Rules,seg_37,"example 2.31: if the probabilities are, respectively, 0.09, 0.15, 0.21, and 0.23 that a person purchasing a new automobile will choose the color green, white, red, or blue, what is the probability that a given buyer will purchase a new automobile that comes in one of those colors?"
340,1,"['events', 'mutually exclusive', 'probability']", Additive Rules,seg_37,"solution : let g, w , r, and b be the events that a buyer selects, respectively, a green, white, red, or blue automobile. since these four events are mutually exclusive, the probability is"
341,1,"['case', 'probability', 'event']", Additive Rules,seg_37,"often it is more difficult to calculate the probability that an event occurs than it is to calculate the probability that the event does not occur. should this be the case for some event a, we simply find p (a′) first and then, using theorem 2.7, find p (a) by subtraction."
342,1,['events'], Additive Rules,seg_37,"theorem 2.9: if a and a′ are complementary events, then"
343,1,"['sets', 'disjoint']", Additive Rules,seg_37,"proof : since a ∪a′ = s and the sets a and a′ are disjoint,"
344,1,"['probabilities', 'probability']", Additive Rules,seg_37,"example 2.32: if the probabilities that an automobile mechanic will service 3, 4, 5, 6, 7, or 8 or more cars on any given workday are, respectively, 0.12, 0.19, 0.28, 0.24, 0.10, and 0.07, what is the probability that he will service at least 5 cars on his next day at work?"
345,1,['event'], Additive Rules,seg_37,"solution : let e be the event that at least 5 cars are serviced. now, p (e) = 1 − p (e′), where e′ is the event that fewer than 5 cars are serviced. since"
346,0,[], Additive Rules,seg_37,it follows from theorem 2.9 that
347,0,[], Additive Rules,seg_37,"example 2.33: suppose the manufacturer’s specifications for the length of a certain type of computer cable are 2000 ± 10 millimeters. in this industry, it is known that small cable is just as likely to be defective (not meeting specifications) as large cable. that is,"
348,1,['probability'], Additive Rules,seg_37,the probability of randomly producing a cable with length exceeding 2010 millimeters is equal to the probability of producing a cable with length smaller than 1990 millimeters. the probability that the production procedure meets specifications is known to be 0.99.
349,1,['probability'], Additive Rules,seg_37,(a) what is the probability that a cable selected randomly is too large?
350,1,['probability'], Additive Rules,seg_37,(b) what is the probability that a randomly selected cable is larger than 1990
351,1,"['events', 'event']", Additive Rules,seg_37,"solution : let m be the event that a cable meets specifications. let s and l be the events that the cable is too small and too large, respectively. then"
352,0,[], Additive Rules,seg_37,"(b) denoting by x the length of a randomly selected cable, we have"
353,0,[], Additive Rules,seg_37,this also can be solved by using theorem 2.9:
354,1,"['range', 'conditional probability', 'conditional', 'probability theory', 'probability', 'population']", Conditional Probability Independence and the Product Rule,seg_41,"one very important concept in probability theory is conditional probability. in some applications, the practitioner is interested in the probability structure under certain restrictions. for instance, in epidemiology, rather than studying the chance that a person from the general population has diabetes, it might be of more interest to know this probability for a distinct group such as asian women in the age range of 35 to 50 or hispanic men in the age range of 40 to 60. this type of probability is called a conditional probability."
355,1,"['conditional probability', 'conditional', 'probability of an event', 'probability', 'event']", Conditional Probability Independence and the Product Rule,seg_41,"the probability of an event b occurring when it is known that some event a has occurred is called a conditional probability and is denoted by p (b|a). the symbol p (b|a) is usually read “the probability that b occurs given that a occurs” or simply “the probability of b, given a.”"
356,1,"['sample', 'probabilities', 'probability', 'event', 'sample space']", Conditional Probability Independence and the Product Rule,seg_41,"consider the event b of getting a perfect square when a die is tossed. the die is constructed so that the even numbers are twice as likely to occur as the odd numbers. based on the sample space s = {1, 2, 3, 4, 5, 6}, with probabilities of 1/9 and 2/9 assigned, respectively, to the odd and even numbers, the probability of b occurring is 1/3. now suppose that it is known that the toss of the die resulted in a number greater than 3. we are now dealing with a reduced sample space a = {4, 5, 6}, which is a subset of s. to find the probability that b occurs, relative to the space a, we must first assign new probabilities to the elements of a proportional to their original probabilities such that their sum is 1. assigning a probability of w to the odd number in a and a probability of 2w to the two even numbers, we have 5w = 1, or w = 1/5. relative to the space a, we find that b contains the single element 4. denoting this event by the symbol b|a, we write b|a = {4}, and hence"
357,1,"['sample', 'sample spaces', 'probabilities', 'events']", Conditional Probability Independence and the Product Rule,seg_41,this example illustrates that events may have different probabilities when considered relative to different sample spaces.
358,0,[], Conditional Probability Independence and the Product Rule,seg_41,we can also write
359,1,"['sample', 'probabilities', 'conditional', 'probability', 'conditional probability', 'sample space']", Conditional Probability Independence and the Product Rule,seg_41,"where p (a ∩ b) and p (a) are found from the original sample space s. in other words, a conditional probability relative to a subspace a of s may be calculated directly from the probabilities assigned to the elements of the original sample space s."
360,1,"['probability', 'conditional', 'conditional probability']", Conditional Probability Independence and the Product Rule,seg_41,"definition 2.10: the conditional probability of b, given a, denoted by p (b|a), is defined by"
361,1,"['sample', 'table', 'data', 'population', 'sample space']", Conditional Probability Independence and the Product Rule,seg_41,"as an additional illustration, suppose that our sample space s is the population of adults in a small town who have completed the requirements for a college degree. we shall categorize them according to gender and employment status. the data are given in table 2.1."
362,1,['categorization'], Conditional Probability Independence and the Product Rule,seg_41,table 2.1: categorization of the adults in a small town
363,0,[], Conditional Probability Independence and the Product Rule,seg_41,employed unemployed total male 460 40 500 female 140 260 400 total 600 300 900
364,1,"['events', 'random']", Conditional Probability Independence and the Product Rule,seg_41,one of these individuals is to be selected at random for a tour throughout the country to publicize the advantages of establishing new industries in the town. we shall be concerned with the following events:
365,0,['e'], Conditional Probability Independence and the Product Rule,seg_41,e: the one chosen is employed.
366,1,"['sample', 'sample space']", Conditional Probability Independence and the Product Rule,seg_41,"using the reduced sample space e, we find that"
367,1,['set'], Conditional Probability Independence and the Product Rule,seg_41,"let n(a) denote the number of elements in any set a. using this notation, since each adult has an equal chance of being selected, we can write"
368,1,"['sample', 'sample space']", Conditional Probability Independence and the Product Rule,seg_41,"where p (e ∩m) and p (e) are found from the original sample space s. to verify this result, note that"
369,1,['probability'], Conditional Probability Independence and the Product Rule,seg_41,example 2.34: the probability that a regularly scheduled flight departs on time is p (d) = 0.83; the probability that it arrives on time is p (a) = 0.82; and the probability that it departs and arrives on time is p (d ∩a) = 0.78. find the probability that a plane
370,0,[], Conditional Probability Independence and the Product Rule,seg_41,"(a) arrives on time, given that it departed on time, and (b) departed on time, given that it has arrived on time."
371,0,[], Conditional Probability Independence and the Product Rule,seg_41,"solution : using definition 2.10, we have the following."
372,1,['probability'], Conditional Probability Independence and the Product Rule,seg_41,"(a) the probability that a plane arrives on time, given that it departed on time,"
373,1,['probability'], Conditional Probability Independence and the Product Rule,seg_41,"(b) the probability that a plane departed on time, given that it has arrived on"
374,1,"['conditional probability', 'conditional', 'information', 'probability of an event', 'probability', 'event']", Conditional Probability Independence and the Product Rule,seg_41,"p (d ∩a) 0.78 p (d|a) = = = 0.95. p (a) 0.82 the notion of conditional probability provides the capability of reevaluating the idea of probability of an event in light of additional information, that is, when it is known that another event has occurred. the probability p (a|b) is an updating of p (a) based on the knowledge that event b has occurred. in example 2.34, it is important to know the probability that the flight arrives on time. one is given the information that the flight did not depart on time. armed with this additional information, one can calculate the more pertinent probability p (a|d′), that is, the probability that it arrives on time, given that it did not depart on time. in many situations, the conclusions drawn from observing the more important conditional probability change the picture entirely. in this example, the computation of p (a|d′) is"
375,1,"['probability', 'information']", Conditional Probability Independence and the Product Rule,seg_41,"as a result, the probability of an on-time arrival is diminished severely in the presence of the additional information."
376,1,"['process', 'measurement', 'conditional probability', 'case', 'conditional', 'information', 'probability', 'tests', 'test']", Conditional Probability Independence and the Product Rule,seg_41,"example 2.35: the concept of conditional probability has countless uses in both industrial and biomedical applications. consider an industrial process in the textile industry in which strips of a particular type of cloth are being produced. these strips can be defective in two ways, length and nature of texture. for the case of the latter, the process of identification is very complicated. it is known from historical information on the process that 10% of strips fail the length test, 5% fail the texture test, and only 0.8% fail both tests. if a strip is selected randomly from the process and a quick measurement identifies it as failing the length test, what is the probability that it is texture defective?"
377,1,['events'], Conditional Probability Independence and the Product Rule,seg_41,solution : consider the events
378,0,[], Conditional Probability Independence and the Product Rule,seg_41,"l: length defective, t : texture defective."
379,1,['probability'], Conditional Probability Independence and the Product Rule,seg_41,"given that the strip is length defective, the probability that this strip is texture defective is given by"
380,1,"['conditional', 'information', 'probability', 'conditional probability']", Conditional Probability Independence and the Product Rule,seg_41,"thus, knowing the conditional probability provides considerably more information than merely knowing p (t )."
381,1,"['experiment', 'with replacement', 'replacement', 'events']", Conditional Probability Independence and the Product Rule,seg_41,"in the die-tossing experiment discussed on page 62, we note that p (b|a) = 2/5 whereas p (b) = 1/3. that is, p (b|a) = p (b), indicating that b depends on a. now consider an experiment in which 2 cards are drawn in succession from an ordinary deck, with replacement. the events are defined as"
382,1,"['sample', 'sample space']", Conditional Probability Independence and the Product Rule,seg_41,"since the first card is replaced, our sample space for both the first and the second draw consists of 52 cards, containing 4 aces and 13 spades. hence,"
383,1,"['events', 'independent']", Conditional Probability Independence and the Product Rule,seg_41,"that is, p (b|a) = p (b). when this is true, the events a and b are said to be independent."
384,1,"['independent', 'conditional probability', 'conditional', 'independence', 'probability of an event', 'events', 'independent events', 'probability', 'event']", Conditional Probability Independence and the Product Rule,seg_41,"although conditional probability allows for an alteration of the probability of an event in the light of additional material, it also enables us to understand better the very important concept of independence or, in the present context, independent events. in the airport illustration in example 2.34, p (a|d) differs from p (a). this suggests that the occurrence of d influenced a, and this is certainly expected in this illustration. however, consider the situation where we have events a and b and"
385,1,"['independent', 'statistics', 'independence']", Conditional Probability Independence and the Product Rule,seg_41,"in other words, the occurrence of b had no impact on the odds of occurrence of a. here the occurrence of a is independent of the occurrence of b. the importance of the concept of independence cannot be overemphasized. it plays a vital role in material in virtually all chapters in this book and in all areas of applied statistics."
386,1,"['events', 'independent']", Conditional Probability Independence and the Product Rule,seg_41,definition 2.11: two events a and b are independent if and only if
387,1,"['conditional probabilities', 'probabilities', 'dependent', 'conditional']", Conditional Probability Independence and the Product Rule,seg_41,"assuming the existences of the conditional probabilities. otherwise, a and b are dependent."
388,1,"['experiments', 'condition']", Conditional Probability Independence and the Product Rule,seg_41,"the condition p (b|a) = p (b) implies that p (a|b) = p (a), and conversely. for the card-drawing experiments, where we showed that p (b|a) = p (b) = 1/4, we also can see that p (a|b) = p (a) = 1/13."
389,1,"['product rule', 'multiplicative rule']", Conditional Probability Independence and the Product Rule,seg_41,"multiplying the formula in definition 2.10 by p (a), we obtain the following important multiplicative rule (or product rule), which enables us to calculate"
390,1,"['events', 'probability']", Conditional Probability Independence and the Product Rule,seg_41,the probability that two events will both occur.
391,1,"['events', 'experiment']", Conditional Probability Independence and the Product Rule,seg_41,"if in an experiment the events a and b can both occur, then"
392,1,"['conditional probability', 'conditional', 'events', 'probability']", Conditional Probability Independence and the Product Rule,seg_41,"thus, the probability that both a and b occur is equal to the probability that a occurs multiplied by the conditional probability that b occurs, given that a occurs. since the events a∩b and b ∩a are equivalent, it follows from theorem 2.10 that we can also write"
393,1,['event'], Conditional Probability Independence and the Product Rule,seg_41,"in other words, it does not matter which event is referred to as a and which event is referred to as b."
394,1,"['probability', 'random']", Conditional Probability Independence and the Product Rule,seg_41,"example 2.36: suppose that we have a fuse box containing 20 fuses, of which 5 are defective. if 2 fuses are selected at random and removed from the box in succession without replacing the first, what is the probability that both fuses are defective?"
395,1,"['probability', 'event']", Conditional Probability Independence and the Product Rule,seg_41,"solution : we shall let a be the event that the first fuse is defective and b the event that the second fuse is defective; then we interpret a ∩ b as the event that a occurs and then b occurs after a has occurred. the probability of first removing a defective fuse is 1/4; then the probability of removing a second defective fuse from the remaining 4 is 4/19. hence,"
396,1,['probability'], Conditional Probability Independence and the Product Rule,seg_41,"example 2.37: one bag contains 4 white balls and 3 black balls, and a second bag contains 3 white balls and 5 black balls. one ball is drawn from the first bag and placed unseen in the second bag. what is the probability that a ball now drawn from the second bag is black?"
397,1,"['mutually exclusive', 'probabilities', 'events', 'union', 'mutually exclusive events']", Conditional Probability Independence and the Product Rule,seg_41,"solution : let b1, b2, and w1 represent, respectively, the drawing of a black ball from bag 1, a black ball from bag 2, and a white ball from bag 1. we are interested in the union of the mutually exclusive events b1 ∩ b2 and w1 ∩ b2. the various possibilities and their probabilities are illustrated in figure 2.8. now"
398,1,"['independent', 'events', 'probability', 'multiplicative rule']", Conditional Probability Independence and the Product Rule,seg_41,"if, in example 2.36, the first fuse is replaced and the fuses thoroughly rearranged before the second is removed, then the probability of a defective fuse on the second selection is still 1/4; that is, p (b|a) = p (b) and the events a and b are independent. when this is true, we can substitute p (b) for p (b|a) in theorem 2.10 to obtain the following special multiplicative rule."
399,1,['tree diagram'], Conditional Probability Independence and the Product Rule,seg_41,figure 2.8: tree diagram for example 2.37.
400,1,"['events', 'independent']", Conditional Probability Independence and the Product Rule,seg_41,two events a and b are independent if and only if
401,1,"['independent', 'probabilities', 'events', 'independent events', 'probability']", Conditional Probability Independence and the Product Rule,seg_41,"therefore, to obtain the probability that two independent events will both occur, we simply find the product of their individual probabilities."
402,1,"['probability', 'event']", Conditional Probability Independence and the Product Rule,seg_41,"example 2.38: a small town has one fire engine and one ambulance available for emergencies. the probability that the fire engine is available when needed is 0.98, and the probability that the ambulance is available when called is 0.92. in the event of an injury resulting from a burning building, find the probability that both the ambulance and the fire engine will be available, assuming they operate independently."
403,1,['events'], Conditional Probability Independence and the Product Rule,seg_41,solution : let a and b represent the respective events that the fire engine and the ambulance are available. then
404,1,['probability'], Conditional Probability Independence and the Product Rule,seg_41,"example 2.39: an electrical system consists of four components as illustrated in figure 2.9. the system works if components a and b work and either of the components c or d works. the reliability (probability of working) of each component is also shown in figure 2.9. find the probability that (a) the entire system works and (b) the component c does not work, given that the entire system works. assume that the four components work independently."
405,0,[], Conditional Probability Independence and the Product Rule,seg_41,"solution : in this configuration of the system, a, b, and the subsystem c and d constitute a serial circuit system, whereas the subsystem c and d itself is a parallel circuit system."
406,1,['probability'], Conditional Probability Independence and the Product Rule,seg_41,(a) clearly the probability that the entire system works can be calculated as
407,1,['independence'], Conditional Probability Independence and the Product Rule,seg_41,the equalities above hold because of the independence among the four components.
408,1,"['case', 'conditional', 'probability', 'conditional probability']", Conditional Probability Independence and the Product Rule,seg_41,"(b) to calculate the conditional probability in this case, notice that"
409,0,[], Conditional Probability Independence and the Product Rule,seg_41,p (the system works but c does not work) p = p (the system works)
410,0,[], Conditional Probability Independence and the Product Rule,seg_41,figure 2.9: an electrical system for example 2.39.
411,1,['multiplicative rule'], Conditional Probability Independence and the Product Rule,seg_41,the multiplicative rule can be extended to more than two-event situations.
412,1,"['events', 'experiment']", Conditional Probability Independence and the Product Rule,seg_41,"if, in an experiment, the events a1, a2, . . . , ak can occur, then"
413,1,"['events', 'independent']", Conditional Probability Independence and the Product Rule,seg_41,"if the events a1, a2, . . . , ak are independent, then"
414,1,"['without replacement', 'replacement', 'probability', 'event']", Conditional Probability Independence and the Product Rule,seg_41,"example 2.40: three cards are drawn in succession, without replacement, from an ordinary deck of playing cards. find the probability that the event a1 ∩ a2 ∩ a3 occurs, where a1 is the event that the first card is a red ace, a2 is the event that the second card is a 10 or a jack, and a3 is the event that the third card is greater than 3 but less than 7."
415,1,['events'], Conditional Probability Independence and the Product Rule,seg_41,solution : first we define the events
416,0,[], Conditional Probability Independence and the Product Rule,seg_41,a3: the third card is greater than 3 but less than 7.
417,1,"['independent', 'results', 'case', 'independence', 'events', 'null set', 'set']", Conditional Probability Independence and the Product Rule,seg_41,"the property of independence stated in theorem 2.11 can be extended to deal with more than two events. consider, for example, the case of three events a, b, and c. it is not sufficient to only have that p (a ∩b ∩c) = p (a)p (b)p (c) as a definition of independence among the three. suppose a = b and c = φ, the null set. although a∩b∩c = φ, which results in p (a∩b∩c) = 0 = p (a)p (b)p (c), events a and b are not independent. hence, we have the following definition."
418,1,"['events', 'mutually independent', 'independent']", Conditional Probability Independence and the Product Rule,seg_41,"definition 2.12: a collection of events a = {a1, . . . , an} are mutually independent if for any subset of a, ai1 , . . . , aik , for k ≤ n, we have"
419,1,"['bayesian', 'experimental', 'statistical inference', 'statistics', 'data', 'probability theory', 'probability', 'statistical']", Bayes Rule,seg_45,"bayesian statistics is a collection of tools that is used in a special form of statistical inference which applies in the analysis of experimental data in many practical situations in science and engineering. bayes’ rule is one of the most important rules in probability theory. it is the foundation of bayesian inference, which will be discussed in chapter 18."
420,1,"['mutually exclusive', 'random', 'probability of the event', 'information', 'events', 'union', 'probability', 'event', 'mutually exclusive events']", Bayes Rule,seg_45,"let us now return to the illustration of section 2.6, where an individual is being selected at random from the adults of a small town to tour the country and publicize the advantages of establishing new industries in the town. suppose that we are now given the additional information that 36 of those employed and 12 of those unemployed are members of the rotary club. we wish to find the probability of the event a that the individual selected is a member of the rotary club. referring to figure 2.12, we can write a as the union of the two mutually exclusive events e∩a and e′∩a. hence, a = (e∩a)∪ (e′∩a), and by corollary 2.1 of theorem 2.7, and then theorem 2.10, we can write"
421,1,"['events', 'venn diagram', 'venn']", Bayes Rule,seg_45,"figure 2.12: venn diagram for the events a, e, and e′."
422,1,"['set', 'data']", Bayes Rule,seg_45,"the data of section 2.6, together with the additional data given above for the set a, enable us to compute"
423,1,"['probability', 'tree diagram', 'probabilities']", Bayes Rule,seg_45,"if we display these probabilities by means of the tree diagram of figure 2.13, where the first branch yields the probability p (e)p (a|e) and the second branch yields"
424,1,"['tree diagram', 'data', 'information']", Bayes Rule,seg_45,"figure 2.13: tree diagram for the data on page 63, using additional information on page 72."
425,1,['probability'], Bayes Rule,seg_45,"the probability p (e′)p (a|e′), it follows that"
426,1,"['sample', 'case', 'rule of elimination', 'probability', 'total probability', 'sample space']", Bayes Rule,seg_45,"a generalization of the foregoing illustration to the case where the sample space is partitioned into k subsets is covered by the following theorem, sometimes called the theorem of total probability or the rule of elimination."
427,1,"['sample', 'events', 'event', 'sample space']", Bayes Rule,seg_45,"theorem 2.13: if the events b1, b2, . . . , bk constitute a partition of the sample space s such that p (bi) = 0 for i = 1, 2, . . . , k, then for any event a of s,"
428,1,"['sample', 'sample space']", Bayes Rule,seg_45,figure 2.14: partitioning the sample space s.
429,1,"['mutually exclusive', 'events', 'union', 'venn diagram', 'venn', 'event', 'mutually exclusive events']", Bayes Rule,seg_45,proof : consider the venn diagram of figure 2.14. the event a is seen to be the union of the mutually exclusive events
430,1,['probability'], Bayes Rule,seg_45,"example 2.41: in a certain assembly plant, three machines, b1, b2, and b3, make 30%, 45%, and 25%, respectively, of the products. it is known from past experience that 2%, 3%, and 2% of the products made by each machine, respectively, are defective. now, suppose that a finished product is randomly selected. what is the probability that it is defective?"
431,1,['events'], Bayes Rule,seg_45,solution : consider the following events:
432,0,[], Bayes Rule,seg_45,"a: the product is defective,"
433,0,[], Bayes Rule,seg_45,"b1: the product is made by machine b1,"
434,0,[], Bayes Rule,seg_45,"b2: the product is made by machine b2,"
435,0,[], Bayes Rule,seg_45,b3: the product is made by machine b3.
436,1,['rule of elimination'], Bayes Rule,seg_45,"applying the rule of elimination, we can write"
437,1,"['probabilities', 'tree diagram']", Bayes Rule,seg_45,"referring to the tree diagram of figure 2.15, we find that the three branches give the probabilities"
438,1,['tree diagram'], Bayes Rule,seg_45,figure 2.15: tree diagram for example 2.41.
439,1,"['conditional probability', 'conditional', 'rule of elimination', 'probability']", Bayes Rule,seg_45,"instead of asking for p (a) in example 2.41, by the rule of elimination, suppose that we now consider the problem of finding the conditional probability p (bi|a). in other words, suppose that a product was randomly selected and it is defective. what is the probability that this product was made by machine bi? questions of this type can be answered by using the following theorem, called bayes’ rule:"
440,1,"['sample', 'events', 'event', 'sample space']", Bayes Rule,seg_45,"theorem 2.14: (bayes’ rule) if the events b1, b2, . . . , bk constitute a partition of the sample space s such that p (bi) = 0 for i = 1, 2, . . . , k, then for any event a in s such that p (a) = 0,"
441,1,"['probability', 'conditional', 'conditional probability']", Bayes Rule,seg_45,"proof : by the definition of conditional probability,"
442,0,[], Bayes Rule,seg_45,"and then using theorem 2.13 in the denominator, we have"
443,0,[], Bayes Rule,seg_45,which completes the proof.
444,1,['probability'], Bayes Rule,seg_45,"example 2.42: with reference to example 2.41, if a product was chosen randomly and found to be defective, what is the probability that it was made by machine b3?"
445,0,[], Bayes Rule,seg_45,solution : using bayes’ rule to write
446,1,['probability'], Bayes Rule,seg_45,76 chapter 2 probability
447,1,['probabilities'], Bayes Rule,seg_45,"and then substituting the probabilities calculated in example 2.41, we have"
448,0,[], Bayes Rule,seg_45,"in view of the fact that a defective product was selected, this result suggests that it probably was not made by machine b3."
449,1,"['rate', 'design', 'varying']", Bayes Rule,seg_45,"example 2.43: a manufacturing firm employs three analytical plans for the design and development of a particular product. for cost reasons, all three are used at varying times. in fact, plans 1, 2, and 3 are used for 30%, 20%, and 50% of the products, respectively. the defect rate is different for the three procedures as follows:"
450,1,"['probability', 'random']", Bayes Rule,seg_45,"where p (d|pj) is the probability of a defective product, given plan j. if a random product was observed and found to be defective, which plan was most likely used and thus responsible?"
451,0,[], Bayes Rule,seg_45,solution : from the statement of the problem
452,0,[], Bayes Rule,seg_45,"we must find p (pj |d) for j = 1, 2, 3. bayes’ rule (theorem 2.14) shows"
453,1,"['random', 'conditional', 'probability', 'conditional probability']", Bayes Rule,seg_45,the conditional probability of a defect given plan 3 is the largest of the three; thus a defective for a random product is most likely the result of the use of plan 3.
454,1,"['bayesian', 'method', 'statistical']", Bayes Rule,seg_45,"using bayes’ rule, a statistical methodology called the bayesian approach has attracted a lot of attention in applications. an introduction to the bayesian method will be discussed in chapter 18."
455,1,"['evaluating', 'probability']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_51,"this chapter contains the fundamental definitions, rules, and theorems that provide a foundation that renders probability an important tool for evaluating"
456,1,"['conditional', 'independence', 'probability', 'conditional probability']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_51,"scientific and engineering systems. the evaluations are often in the form of probability computations, as is illustrated in examples and exercises. concepts such as independence, conditional probability, bayes’ rule, and others tend to mesh nicely to solve practical problems in which the bottom line is to produce a probability value. illustrations in exercises are abundant. see, for example, exercises 2.100 and 2.101. in these and many other exercises, an evaluation of a scientific system is being made judiciously from a probability calculation, using rules and definitions discussed in the chapter."
457,1,"['conditional', 'discrete', 'independence', 'frequency', 'probability', 'random', 'process', 'probability distributions', 'information', 'random variable', 'distributions', 'distribution', 'measurement', 'probabilities', 'variable', 'probability distribution', 'conditional probability']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_51,"now, how does the material in this chapter relate to that in other chapters? it is best to answer this question by looking ahead to chapter 3. chapter 3 also deals with the type of problems in which it is important to calculate probabilities. we illustrate how system performance depends on the value of one or more probabilities. once again, conditional probability and independence play a role. however, new concepts arise which allow more structure based on the notion of a random variable and its probability distribution. recall that the idea of frequency distributions was discussed briefly in chapter 1. the probability distribution displays, in equation form or graphically, the total information necessary to describe a probability structure. for example, in review exercise 2.122 the random variable of interest is the number of defective items, a discrete measurement. thus, the probability distribution would reveal the probability structure for the number of defective items out of the number selected from the process. as the reader moves into chapter 3 and beyond, it will become apparent that assumptions will be required in order to determine and thus make use of probability distributions for solving scientific problems."
458,1,"['sample', 'process', 'experiment', 'observations', 'results', 'sample space', 'statistical', 'population', 'populations', 'outcome', 'experiments', 'numerical']", Concept of a Random Variable,seg_55,"statistics is concerned with making inferences about populations and population characteristics. experiments are conducted with results that are subject to chance. the testing of a number of electronic components is an example of a statistical experiment, a term that is used to describe any process by which several chance observations are generated. it is often important to allocate a numerical description to the outcome. for example, the sample space giving a detailed description of each possible outcome when three electronic components are tested may be written"
459,1,"['sample', 'experiment', 'sample space', 'variable', 'random variable', 'random', 'outcome', 'numerical']", Concept of a Random Variable,seg_55,"where n denotes nondefective and d denotes defective. one is naturally concerned with the number of defectives that occur. thus, each point in the sample space will be assigned a numerical value of 0, 1, 2, or 3. these values are, of course, random quantities determined by the outcome of the experiment. they may be viewed as values assumed by the random variable x , the number of defective items when three electronic components are tested."
460,1,"['sample', 'variable', 'random variable', 'random', 'function', 'sample space']", Concept of a Random Variable,seg_55,definition 3.1: a random variable is a function that associates a real number with each element in the sample space.
461,1,"['case', 'random variable', 'variable', 'random']", Concept of a Random Variable,seg_55,"we shall use a capital letter, say x, to denote a random variable and its corresponding small letter, x in this case, for one of its values. in the electronic component testing illustration above, we notice that the random variable x assumes the value 2 for all elements in the subset"
462,1,"['sample', 'experiment', 'event', 'sample space']", Concept of a Random Variable,seg_55,"of the sample space s. that is, each possible value of x represents an event that is a subset of the sample space for the given experiment."
463,1,"['outcomes', 'without replacement', 'replacement', 'random variable', 'variable', 'random']", Concept of a Random Variable,seg_55,"example 3.1: two balls are drawn in succession without replacement from an urn containing 4 red balls and 3 black balls. the possible outcomes and the values y of the random variable y , where y is the number of red balls, are"
464,1,"['sample', 'variable', 'random variable', 'random']", Concept of a Random Variable,seg_55,"example 3.2: a stockroom clerk returns three safety helmets at random to three steel mill employees who had previously checked them. if smith, jones, and brown, in that order, receive one of the three hats, list the sample points for the possible orders of returning the helmets, and find the value m of the random variable m that represents the number of correct matches."
465,0,[], Concept of a Random Variable,seg_55,"solution : if s, j , and b stand for smith’s, jones’s, and brown’s helmets, respectively, then the possible arrangements in which the helmets may be returned and the number of correct matches are"
466,1,"['sample', 'sample space']", Concept of a Random Variable,seg_55,"in each of the two preceding examples, the sample space contains a finite number of elements. on the other hand, when a die is thrown until a 5 occurs, we obtain a sample space with an unending sequence of elements,"
467,1,['experiment'], Concept of a Random Variable,seg_55,"where f and n represent, respectively, the occurrence and nonoccurrence of a 5. but even in this experiment, the number of elements can be equated to the number of whole numbers so that there is a first element, a second element, a third element, and so on, and in this sense can be counted."
468,1,"['categorical', 'variables', 'case', 'cases', 'random variable', 'variable', 'random', 'dummy variables']", Concept of a Random Variable,seg_55,"there are cases where the random variable is categorical in nature. variables, often called dummy variables, are used. a good illustration is the case in which the random variable is binary in nature, as shown in the following example."
469,1,"['condition', 'random variable', 'variable', 'random']", Concept of a Random Variable,seg_55,example 3.3: consider the simple condition in which components are arriving from the production line and they are stipulated to be defective or not defective. define the random variable x by
470,0,[], Concept of a Random Variable,seg_55,"1, if the component is defective,"
471,0,[], Concept of a Random Variable,seg_55,"x = {0, if the component is not defective."
472,1,"['bernoulli', 'random variable', 'variable', 'random', 'bernoulli random variable']", Concept of a Random Variable,seg_55,clearly the assignment of 1 or 0 is arbitrary though quite convenient. this will become clear in later chapters. the random variable for which 0 and 1 are chosen to describe the two possible values is called a bernoulli random variable.
473,1,"['random variables', 'variables', 'random']", Concept of a Random Variable,seg_55,further illustrations of random variables are revealed in the following examples.
474,1,"['statisticians', 'sampling']", Concept of a Random Variable,seg_55,example 3.4: statisticians use sampling plans to either accept or reject batches or lots of material. suppose one of these sampling plans involves sampling independently 10 items from a lot of 100 items in which 12 are defective.
475,1,"['sample', 'case', 'variable', 'random variable', 'random']", Concept of a Random Variable,seg_55,"let x be the random variable defined as the number of items found defective in the sample of 10. in this case, the random variable takes on the values 0, 1, 2, . . . , 9, 10."
476,1,"['sample', 'sample spaces', 'process', 'sampling', 'variable', 'random variable', 'random']", Concept of a Random Variable,seg_55,"example 3.5: suppose a sampling plan involves sampling items from a process until a defective is observed. the evaluation of the process will depend on how many consecutive items are observed. in that regard, let x be a random variable defined by the number of items observed before a defective is found. with n a nondefective and d a defective, sample spaces are s = {d} given x = 1, s = {nd} given x = 2, s = {nnd} given x = 3, and so on."
477,1,"['random variable', 'variable', 'random']", Concept of a Random Variable,seg_55,example 3.6: interest centers around the proportion of people who respond to a certain mail order solicitation. let x be that proportion. x is a random variable that takes on all values x for which 0 ≤ x ≤ 1.
478,1,"['random variable', 'variable', 'random']", Concept of a Random Variable,seg_55,"example 3.7: let x be the random variable defined by the waiting time, in hours, between successive speeders spotted by a radar unit. the random variable x takes on all values x for which x ≥ 0."
479,1,"['sample', 'sample space', 'discrete']", Concept of a Random Variable,seg_55,"definition 3.2: if a sample space contains a finite number of possibilities or an unending sequence with as many elements as there are whole numbers, it is called a discrete sample space."
480,1,"['sample', 'sample spaces', 'outcomes', 'sample space', 'case', 'discrete', 'intervals', 'variable', 'statistical', 'measuring', 'experiments', 'test']", Concept of a Random Variable,seg_55,"the outcomes of some statistical experiments may be neither finite nor countable. such is the case, for example, when one conducts an investigation measuring the distances that a certain make of automobile will travel over a prescribed test course on 5 liters of gasoline. assuming distance to be a variable measured to any degree of accuracy, then clearly we have an infinite number of possible distances in the sample space that cannot be equated to the number of whole numbers. or, if one were to record the length of time for a chemical reaction to take place, once again the possible time intervals making up our sample space would be infinite in number and uncountable. we see now that all sample spaces need not be discrete."
481,1,"['continuous', 'sample', 'sample space']", Concept of a Random Variable,seg_55,"definition 3.3: if a sample space contains an infinite number of possibilities equal to the number of points on a line segment, it is called a continuous sample space."
482,1,"['discrete random variable', 'outcomes', 'random variables', 'interval', 'variables', 'discrete random variables', 'discrete', 'random variable', 'variable', 'set', 'random']", Concept of a Random Variable,seg_55,a random variable is called a discrete random variable if its set of possible outcomes is countable. the random variables in examples 3.1 to 3.5 are discrete random variables. but a random variable whose set of possible values is an entire interval of numbers is not discrete. when a random variable can take on values
483,1,"['sample', 'continuous random variables', 'random variables', 'variables', 'continuous random variable', 'variable', 'random variable', 'random', 'continuous', 'sample space']", Concept of a Random Variable,seg_55,"on a continuous scale, it is called a continuous random variable. often the possible values of a continuous random variable are precisely the same values that are contained in the continuous sample space. obviously, the random variables described in examples 3.6 and 3.7 are continuous random variables."
484,1,"['sample', 'continuous random variables', 'random variables', 'variables', 'discrete random variables', 'data', 'discrete', 'random', 'continuous']", Concept of a Random Variable,seg_55,"in most practical problems, continuous random variables represent measured data, such as all possible heights, weights, temperatures, distance, or life periods, whereas discrete random variables represent count data, such as the number of defectives in a sample of k items or the number of highway fatalities per year in a given state. note that the random variables y and m of examples 3.1 and 3.2 both represent count data, y the number of red balls and m the number of correct hat matches."
485,1,"['sample', 'discrete random variable', 'random', 'probabilities', 'simple events', 'case', 'discrete', 'random variable', 'events', 'variable', 'probability', 'tail']", Discrete Probability Distributions,seg_57,"a discrete random variable assumes each of its values with a certain probability. in the case of tossing a coin three times, the variable x, representing the number of heads, assumes the value 2 with probability 3/8, since 3 of the 8 equally likely sample points result in two heads and one tail. if one assumes equal weights for the simple events in example 3.2, the probability that no employee gets back the right helmet, that is, the probability that m assumes the value 0, is 1/3. the possible values m of m and their probabilities are"
486,1,"['cases', 'probabilities']", Discrete Probability Distributions,seg_57,m 0 1 3 1 1 1 p(m = m) 3 2 6 note that the values of m exhaust all possible cases and hence the probabilities add to 1.
487,1,"['probability mass function', 'discrete', 'set', 'probability', 'random', 'function', 'numerical', 'mass function', 'random variable', 'distribution', 'ordered pairs', 'probability function', 'discrete random variable', 'probabilities', 'variable', 'probability distribution']", Discrete Probability Distributions,seg_57,"frequently, it is convenient to represent all the probabilities of a random variable x by a formula. such a formula would necessarily be a function of the numerical values x that we shall denote by f(x), g(x), r(x), and so forth. therefore, we write f(x) = p (x = x); that is, f(3) = p (x = 3). the set of ordered pairs (x, f(x)) is called the probability function, probability mass function, or probability distribution of the discrete random variable x."
488,1,"['probability mass function', 'discrete', 'set', 'probability', 'random', 'function', 'mass function', 'random variable', 'distribution', 'ordered pairs', 'outcome', 'probability function', 'discrete random variable', 'variable', 'probability distribution']", Discrete Probability Distributions,seg_57,"definition 3.4: the set of ordered pairs (x, f(x)) is a probability function, probability mass function, or probability distribution of the discrete random variable x if, for each possible outcome x,"
489,1,"['distribution', 'probability distribution', 'probability', 'random']", Discrete Probability Distributions,seg_57,"example 3.8: a shipment of 20 similar laptop computers to a retail outlet contains 3 that are defective. if a school makes a random purchase of 2 of these computers, find the probability distribution for the number of defectives."
490,1,"['random variable', 'variable', 'random']", Discrete Probability Distributions,seg_57,"solution : let x be a random variable whose values x are the possible numbers of defective computers purchased by the school. then x can only take the numbers 0, 1, and"
491,1,"['distribution', 'probability distribution', 'probability']", Discrete Probability Distributions,seg_57,"thus, the probability distribution of x is"
492,1,"['distribution', 'probability distribution', 'probability']", Discrete Probability Distributions,seg_57,"example 3.9: if a car agency sells 50% of its inventory of a certain foreign car equipped with side airbags, find a formula for the probability distribution of the number of cars with side airbags among the next 4 cars sold by the agency."
493,1,"['sample', 'model', 'probabilities', 'probability', 'function', 'sample space', 'outcomes']", Discrete Probability Distributions,seg_57,"solution : since the probability of selling an automobile with side airbags is 0.5, the 24 = 16 points in the sample space are equally likely to occur. therefore, the denominator for all probabilities, and also for our function, is 16. to obtain the number of ways of selling 3 cars with side airbags, we need to consider the number of ways of partitioning 4 outcomes into two cells, with 3 cars with side airbags assigned to one cell and the model without side airbags assigned to the other. this can be"
494,1,['event'], Discrete Probability Distributions,seg_57,"4)= 4 ways. in general, the event of selling x models with side airbags"
495,0,[], Discrete Probability Distributions,seg_57,and 4− x models without side airbags can occur in (
496,1,"['distribution', 'probability distribution', 'probability']", Discrete Probability Distributions,seg_57,"2, 3, or 4. thus, the probability distribution f(x) = p (x = x) is"
497,1,"['cumulative distribution function', 'distribution function', 'distribution', 'random variable', 'variable', 'probability', 'random', 'function']", Discrete Probability Distributions,seg_57,"there are many problems where we may wish to compute the probability that the observed value of a random variable x will be less than or equal to some real number x. writing f (x) = p (x ≤ x) for every real number x, we define f (x) to be the cumulative distribution function of the random variable x."
498,1,"['discrete random variable', 'cumulative distribution function', 'distribution function', 'discrete', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'function']", Discrete Probability Distributions,seg_57,definition 3.5: the cumulative distribution function f (x) of a discrete random variable x with probability distribution f(x) is
499,1,"['random variable', 'variable', 'random']", Discrete Probability Distributions,seg_57,"for the random variable m , the number of correct matches in example 3.2, we"
500,1,"['cumulative distribution function', 'distribution function', 'distribution', 'function']", Discrete Probability Distributions,seg_57,the cumulative distribution function of m is
501,1,"['distribution', 'random variable', 'variable', 'random', 'function']", Discrete Probability Distributions,seg_57,one should pay particular notice to the fact that the cumulative distribution function is a monotone nondecreasing function defined not only for the values assumed by the given random variable but for all real numbers.
502,1,"['cumulative distribution function', 'distribution function', 'distribution', 'random variable', 'variable', 'random', 'function']", Discrete Probability Distributions,seg_57,"example 3.10: find the cumulative distribution function of the random variable x in example 3.9. using f (x), verify that f(2) = 3/8."
503,1,"['distribution', 'probability distribution', 'probability']", Discrete Probability Distributions,seg_57,"solution : direct calculations of the probability distribution of example 3.9 give f(0)= 1/16, f(1) = 1/4, f(2)= 3/8, f(3)= 1/4, and f(4)= 1/16. therefore,"
504,1,"['plot', 'mass function', 'probability mass function', 'symmetric', 'case', 'distribution', 'probability distribution', 'probability', 'function']", Discrete Probability Distributions,seg_57,"it is often helpful to look at a probability distribution in graphic form. one might plot the points (x, f(x)) of example 3.9 to obtain figure 3.1. by joining the points to the x axis either with a dashed or with a solid line, we obtain a probability mass function plot. figure 3.1 makes it easy to see what values of x are most likely to occur, and it also indicates a perfectly symmetric situation in this case."
505,1,"['histogram', 'probabilities', 'probability', 'plotting']", Discrete Probability Distributions,seg_57,"instead of plotting the points (x, f(x)), we more frequently construct rectangles, as in figure 3.2. here the rectangles are constructed so that their bases of equal width are centered at each value x and their heights are equal to the corresponding probabilities given by f(x). the bases are constructed so as to leave no space between the rectangles. figure 3.2 is called a probability histogram."
506,0,[], Discrete Probability Distributions,seg_57,"since each base in figure 3.2 has unit width, p (x = x) is equal to the area of the rectangle centered at x. even if the bases were not of unit width, we could adjust the heights of the rectangles to give areas that would still equal the probabilities of x assuming any of its values x. this concept of using areas to represent"
507,1,"['plot', 'histogram', 'mass function', 'probability mass function', 'probability', 'function']", Discrete Probability Distributions,seg_57,figure 3.1: probability mass function plot. figure 3.2: probability histogram.
508,1,"['continuous random variable', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'continuous']", Discrete Probability Distributions,seg_57,probabilities is necessary for our consideration of the probability distribution of a continuous random variable.
509,1,"['cumulative distribution function', 'distribution function', 'distribution', 'step function', 'function', 'plotting']", Discrete Probability Distributions,seg_57,"the graph of the cumulative distribution function of example 3.9, which appears as a step function in figure 3.3, is obtained by plotting the points (x, f (x))."
510,1,"['experimental', 'probability distributions', 'discrete', 'discrete distributions', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'distributions']", Discrete Probability Distributions,seg_57,"certain probability distributions are applicable to more than one physical situation. the probability distribution of example 3.9, for example, also applies to the random variable y , where y is the number of heads when a coin is tossed 4 times, or to the random variable w , where w is the number of red cards that occur when 4 cards are drawn at random from a deck in succession with each card replaced and the deck shuffled before the next drawing. special discrete distributions that can be applied to many different experimental situations will be considered in chapter 5."
511,1,"['cumulative distribution function', 'distribution function', 'discrete', 'distribution', 'function']", Discrete Probability Distributions,seg_57,figure 3.3: discrete cumulative distribution function.
512,1,"['continuous random variable', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'continuous']", Continuous Probability Distributions,seg_59,"a continuous random variable has a probability of 0 of assuming exactly any of its values. consequently, its probability distribution cannot be given in tabular form."
513,1,"['interval', 'case', 'random variable', 'variable', 'set', 'probability', 'random', 'event']", Continuous Probability Distributions,seg_59,"at first this may seem startling, but it becomes more plausible when we consider a particular example. let us discuss a random variable whose values are the heights of all people over 21 years of age. between any two values, say 163.5 and 164.5 centimeters, or even 163.99 and 164.01 centimeters, there are an infinite number of heights, one of which is 164 centimeters. the probability of selecting a person at random who is exactly 164 centimeters tall and not one of the infinitely large set of heights so close to 164 centimeters that you cannot humanly measure the difference is remote, and thus we assign a probability of 0 to the event. this is not the case, however, if we talk about the probability of selecting a person who is at least 163 centimeters but not more than 165 centimeters tall. now we are dealing with an interval rather than a point value of our random variable."
514,1,"['continuous random variables', 'random variables', 'probabilities', 'variables', 'intervals', 'random', 'continuous']", Continuous Probability Distributions,seg_59,"we shall concern ourselves with computing probabilities for various intervals of continuous random variables such as p (a < x < b), p (w ≥ c), and so forth. note that when x is continuous,"
515,1,"['interval', 'discrete']", Continuous Probability Distributions,seg_59,"that is, it does not matter whether we include an endpoint of the interval or not. this is not true, though, when x is discrete."
516,1,"['probability', 'random', 'function', 'numerical', 'sample', 'data', 'random variable', 'sample space', 'statistical', 'functions', 'density function', 'probability density function', 'density functions', 'distribution', 'continuous', 'continuous random variable', 'probabilities', 'variables', 'variable', 'probability distribution']", Continuous Probability Distributions,seg_59,"although the probability distribution of a continuous random variable cannot be presented in tabular form, it can be stated as a formula. such a formula would necessarily be a function of the numerical values of the continuous random variable x and as such will be represented by the functional notation f(x). in dealing with continuous variables, f(x) is usually called the probability density function, or simply the density function, of x. since x is defined over a continuous sample space, it is possible for f(x) to have a finite number of discontinuities. however, most density functions that have practical applications in the analysis of statistical data are continuous and their graphs may take any of several forms, some of which are shown in figure 3.4. because areas will be used to represent probabilities and probabilities are positive numerical values, the density function must lie entirely above the x axis."
517,1,"['functions', 'density functions']", Continuous Probability Distributions,seg_59,figure 3.4: typical density functions.
518,1,"['density function', 'curve', 'probability density function', 'probability', 'function']", Continuous Probability Distributions,seg_59,a probability density function is constructed so that the area under its curve
519,1,"['density function', 'interval', 'range', 'set', 'probability', 'function']", Continuous Probability Distributions,seg_59,"bounded by the x axis is equal to 1 when computed over the range of x for which f(x) is defined. should this range of x be a finite interval, it is always possible to extend the interval to include the entire set of real numbers by defining f(x) to be zero at all points in the extended portions of the interval. in figure 3.5, the probability that x assumes a value between a and b is equal to the shaded area under the density function between the ordinates at x = a and x = b, and from integral calculus is given by"
520,1,"['density function', 'probability density function', 'continuous', 'continuous random variable', 'random variable', 'variable', 'set', 'probability', 'random', 'function']", Continuous Probability Distributions,seg_59,"definition 3.6: the function f(x) is a probability density function (pdf) for the continuous random variable x, defined over the set of real numbers, if"
521,1,"['density function', 'probability density function', 'continuous', 'experiment', 'continuous random variable', 'random variable', 'variable', 'probability', 'random', 'function', 'error']", Continuous Probability Distributions,seg_59,"example 3.11: suppose that the error in the reaction temperature, in ◦c, for a controlled laboratory experiment is a continuous random variable x having the probability density function"
522,1,"['function', 'density function']", Continuous Probability Distributions,seg_59,(a) verify that f(x) is a density function.
523,0,[], Continuous Probability Distributions,seg_59,solution : we use definition 3.6.
524,1,['condition'], Continuous Probability Distributions,seg_59,"(a) obviously, f(x) ≥ 0. to verify condition 2 in definition 3.6, we have"
525,0,[], Continuous Probability Distributions,seg_59,"(b) using formula 3 in definition 3.6, we obtain"
526,1,"['density function', 'cumulative distribution function', 'continuous', 'distribution function', 'continuous random variable', 'distribution', 'random variable', 'variable', 'random', 'function']", Continuous Probability Distributions,seg_59,definition 3.7: the cumulative distribution function f (x) of a continuous random variable x with density function f(x) is
527,1,['results'], Continuous Probability Distributions,seg_59,"as an immediate consequence of definition 3.7, one can write the two results"
528,1,"['function', 'density function']", Continuous Probability Distributions,seg_59,"example 3.12: for the density function of example 3.11, find f (x), and use it to evaluate p (0 < x ≤ 1)."
529,1,"['cumulative distribution function', 'distribution function', 'distribution', 'function']", Continuous Probability Distributions,seg_59,the cumulative distribution function f (x) is expressed in figure 3.6. now
530,1,"['function', 'density function']", Continuous Probability Distributions,seg_59,which agrees with the result obtained by using the density function in example 3.11.
531,1,"['density function', 'estimate', 'estimates', 'function']", Continuous Probability Distributions,seg_59,example 3.13: the department of energy (doe) puts projects out on bid and generally estimates what a reasonable bid should be. call the estimate b. the doe has determined that the density function of the winning (low) bid is
532,1,"['probability', 'estimate']", Continuous Probability Distributions,seg_59,find f (y) and use it to determine the probability that the winning bid is less than the doe’s preliminary estimate b.
533,1,"['cumulative distribution function', 'continuous', 'distribution function', 'distribution', 'function']", Continuous Probability Distributions,seg_59,figure 3.6: continuous cumulative distribution function.
534,1,"['probability', 'estimate']", Continuous Probability Distributions,seg_59,"to determine the probability that the winning bid is less than the preliminary bid estimate b, we have"
535,1,"['sample spaces', 'probability', 'random', 'sample', 'experiment', 'probability distributions', 'random variable', 'distributions', 'outcomes', 'random variables', 'variables', 'variable']", Joint Probability Distributions,seg_63,"our study of random variables and their probability distributions in the preceding sections is restricted to one-dimensional sample spaces, in that we recorded outcomes of an experiment as values assumed by a single random variable. there will be situations, however, where we may find it desirable to record the simulta-"
536,1,"['sample', 'random variables', 'experiment', 'variables', 'likelihood', 'sample space', 'data', 'success', 'random', 'test', 'average', 'outcomes']", Joint Probability Distributions,seg_63,"neous outcomes of several random variables. for example, we might measure the amount of precipitate p and volume v of gas released from a controlled chemical experiment, giving rise to a two-dimensional sample space consisting of the outcomes (p, v), or we might be interested in the hardness h and tensile strength t of cold-drawn copper, resulting in the outcomes (h, t). in a study to determine the likelihood of success in college based on high school data, we might use a threedimensional sample space and record for each individual his or her aptitude test score, high school class rank, and grade-point average at the end of freshman year in college."
537,1,"['random variables', 'variables', 'range', 'discrete random variables', 'joint probability distribution', 'discrete', 'distribution', 'probability distribution', 'joint', 'probability', 'random', 'function', 'joint probability']", Joint Probability Distributions,seg_63,"if x and y are two discrete random variables, the probability distribution for their simultaneous occurrence can be represented by a function with values f(x, y) for any pair of values (x, y) within the range of the random variables x and y . it is customary to refer to this function as the joint probability distribution of x and y ."
538,1,"['case', 'discrete']", Joint Probability Distributions,seg_63,"hence, in the discrete case,"
539,1,"['outcomes', 'probability']", Joint Probability Distributions,seg_63,"that is, the values f(x, y) give the probability that outcomes x and y occur at the same time. for example, if an 18-wheeler is to have its tires serviced and x represents the number of miles these tires have been driven and y represents the number of tires that need to be replaced, then f(30000, 5) is the probability that the tires are used over 30,000 miles and the truck needs 5 new tires."
540,1,"['random variables', 'mass function', 'variables', 'probability mass function', 'joint probability distribution', 'discrete random variables', 'discrete', 'distribution', 'probability distribution', 'joint', 'probability', 'random', 'function', 'joint probability']", Joint Probability Distributions,seg_63,"definition 3.8: the function f(x, y) is a joint probability distribution or probability mass function of the discrete random variables x and y if"
541,1,['random'], Joint Probability Distributions,seg_63,"example 3.14: two ballpoint pens are selected at random from a box that contains 3 blue pens, 2 red pens, and 3 green pens. if x is the number of blue pens selected and y is the number of red pens selected, find"
542,1,"['joint', 'probability', 'function', 'probability function', 'joint probability']", Joint Probability Distributions,seg_63,"(a) the joint probability function f(x, y),"
543,1,['probability'], Joint Probability Distributions,seg_63,"(a) now, f(0, 1), for example, represents the probability that a red and a green"
544,0,[], Joint Probability Distributions,seg_63,pens are selected. the total number of equally likely ways of selecting any 2
545,0,[], Joint Probability Distributions,seg_63,8) = 28. the number of ways of selecting 1 red from 2
546,0,[], Joint Probability Distributions,seg_63,red pens and 1 green from 3 green pens is (1
547,1,"['cases', 'probabilities', 'table']", Joint Probability Distributions,seg_63,"= 3/14. similar calculations yield the probabilities for the other cases, which are presented in table 3.1. note that the probabilities sum to 1. in chapter"
548,1,"['table', 'joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'joint probability']", Joint Probability Distributions,seg_63,"5, it will become clear that the joint probability distribution of table 3.1 can be represented by the formula"
549,1,['probability'], Joint Probability Distributions,seg_63,"(b) the probability that (x,y ) fall in the region a is"
550,1,"['joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'joint probability']", Joint Probability Distributions,seg_63,table 3.1: joint probability distribution for example 3.14
551,1,"['density function', 'continuous random variables', 'random variables', 'continuous', 'variables', 'joint', 'random', 'function']", Joint Probability Distributions,seg_63,"when x and y are continuous random variables, the joint density function f(x, y) is a surface lying above the xy plane, and p [(x,y ) ∈ a], where a is any region in the xy plane, is equal to the volume of the right cylinder bounded by the base a and the surface."
552,1,"['density function', 'continuous random variables', 'random variables', 'continuous', 'variables', 'joint', 'random', 'function']", Joint Probability Distributions,seg_63,"definition 3.9: the function f(x, y) is a joint density function of the continuous random variables x and y if"
553,1,"['density function', 'random variables', 'variables', 'joint', 'random', 'function']", Joint Probability Distributions,seg_63,"example 3.15: a privately owned business operates both a drive-in facility and a walk-in facility. on a randomly selected day, let x and y , respectively, be the proportions of the time that the drive-in and the walk-in facilities are in use, and suppose that the joint density function of these random variables is"
554,1,['condition'], Joint Probability Distributions,seg_63,(a) verify condition 2 of definition 3.9.
555,0,[], Joint Probability Distributions,seg_63,"solution : (a) the integration of f(x, y) over the whole region is"
556,1,['probability'], Joint Probability Distributions,seg_63,"(b) to calculate the probability, we use"
557,1,"['marginal', 'discrete', 'probability', 'random', 'distributions', 'continuous random variables', 'joint probability distribution', 'distribution', 'continuous', 'random variables', 'variables', 'discrete random variables', 'marginal distributions', 'probability distribution', 'joint', 'joint probability']", Joint Probability Distributions,seg_63,"given the joint probability distribution f(x, y) of the discrete random variables x and y , the probability distribution g(x) of x alone is obtained by summing f(x, y) over the values of y . similarly, the probability distribution h(y) of y alone is obtained by summing f(x, y) over the values of x. we define g(x) and h(y) to be the marginal distributions of x and y , respectively. when x and y are continuous random variables, summations are replaced by integrals. we can now make the following general definition."
558,1,"['marginal', 'marginal distributions', 'distributions']", Joint Probability Distributions,seg_63,definition 3.10: the marginal distributions of x alone and of y alone are
559,1,"['case', 'discrete']", Joint Probability Distributions,seg_63,"for the discrete case, and"
560,1,"['continuous', 'case']", Joint Probability Distributions,seg_63,for the continuous case.
561,1,"['marginal', 'case', 'discrete', 'table']", Joint Probability Distributions,seg_63,"the term marginal is used here because, in the discrete case, the values of g(x) and h(y) are just the marginal totals of the respective columns and rows when the values of f(x, y) are displayed in a rectangular table."
562,1,"['table', 'marginal', 'row totals', 'distribution', 'marginal distribution']", Joint Probability Distributions,seg_63,example 3.16: show that the column and row totals of table 3.1 give the marginal distribution of x alone and of y alone.
563,1,"['random variable', 'variable', 'random']", Joint Probability Distributions,seg_63,"solution : for the random variable x, we see that"
564,1,"['column totals', 'table', 'marginal', 'row totals', 'marginal distributions', 'distributions']", Joint Probability Distributions,seg_63,"which are just the column totals of table 3.1. in a similar manner we could show that the values of h(y) are given by the row totals. in tabular form, these marginal distributions may be written as follows:"
565,1,"['function', 'density function', 'joint']", Joint Probability Distributions,seg_63,"example 3.17: find g(x) and h(y) for the joint density function of example 3.15. solution : by definition,"
566,1,"['variables', 'marginal', 'case', 'marginal distributions', 'continuous', 'distributions']", Joint Probability Distributions,seg_63,"the fact that the marginal distributions g(x) and h(y) are indeed the probability distributions of the individual variables x and y alone can be verified by showing that the conditions of definition 3.4 or definition 3.6 are satisfied. for example, in the continuous case"
567,1,"['sample', 'conditional probability', 'conditional', 'variable', 'random variable', 'probability', 'random', 'sample space', 'event']", Joint Probability Distributions,seg_63,"in section 3.1, we stated that the value x of the random variable x represents an event that is a subset of the sample space. if we use the definition of conditional probability as stated in chapter 2,"
568,1,['events'], Joint Probability Distributions,seg_63,"where a and b are now the events defined by x = x and y = y, respectively, then"
569,1,"['random variables', 'variables', 'discrete random variables', 'discrete', 'random']", Joint Probability Distributions,seg_63,where x and y are discrete random variables.
570,1,"['marginal', 'conditional', 'probability', 'random', 'function', 'conditional probabilities', 'conditional probability distribution', 'continuous random variables', 'distribution', 'continuous', 'random variables', 'probabilities', 'variables', 'probability distribution', 'joint', 'conditional probability', 'marginal distribution']", Joint Probability Distributions,seg_63,"it is not difficult to show that the function f(x, y)/g(x), which is strictly a function of y with x fixed, satisfies all the conditions of a probability distribution. this is also true when f(x, y) and g(x) are the joint density and marginal distribution, respectively, of continuous random variables. as a result, it is extremely important that we make use of the special type of distribution of the form f(x, y)/g(x) in order to be able to effectively compute conditional probabilities. this type of distribution is called a conditional probability distribution; the formal definition follows."
571,1,"['random variables', 'variables', 'conditional', 'discrete', 'distribution', 'random variable', 'variable', 'random', 'continuous', 'conditional distribution']", Joint Probability Distributions,seg_63,"definition 3.11: let x and y be two random variables, discrete or continuous. the conditional distribution of the random variable y given that x = x is"
572,1,"['distribution', 'conditional distribution', 'conditional']", Joint Probability Distributions,seg_63,"similarly, the conditional distribution of x given that y = y is"
573,1,"['discrete random variable', 'discrete', 'random variable', 'variable', 'probability', 'random']", Joint Probability Distributions,seg_63,"if we wish to find the probability that the discrete random variable x falls between a and b when it is known that the discrete variable y = y, we evaluate"
574,1,"['continuous', 'summation']", Joint Probability Distributions,seg_63,"where the summation extends over all values of x between a and b. when x and y are continuous, we evaluate"
575,1,"['distribution', 'conditional distribution', 'conditional']", Joint Probability Distributions,seg_63,"example 3.18: referring to example 3.14, find the conditional distribution ofx, given that y = 1, and use it to determine p (x = 0 | y = 1)."
576,1,"['distribution', 'conditional distribution', 'conditional']", Joint Probability Distributions,seg_63,"and the conditional distribution of x, given that y = 1, is"
577,1,['probability'], Joint Probability Distributions,seg_63,"therefore, if it is known that 1 of the 2 pen refills selected is red, we have a probability equal to 1/2 that the other refill is not blue."
578,1,"['random variables', 'variables', 'joint', 'random']", Joint Probability Distributions,seg_63,"example 3.19: the joint density for the random variables (x,y ), where x is the unit temperature change and y is the proportion of spectrum shift that a certain atomic particle"
579,1,"['densities', 'conditional density', 'marginal', 'conditional']", Joint Probability Distributions,seg_63,"(a) find the marginal densities g(x), h(y), and the conditional density f(y|x)."
580,1,['probability'], Joint Probability Distributions,seg_63,(b) find the probability that the spectrum shifts more than half of the total
581,0,[], Joint Probability Distributions,seg_63,"observations, given that the temperature is increased by 0.25 unit."
582,0,[], Joint Probability Distributions,seg_63,"solution : (a) by definition,"
583,1,"['function', 'density function', 'joint']", Joint Probability Distributions,seg_63,example 3.20: given the joint density function
584,1,"['marginal', 'marginal density']", Joint Probability Distributions,seg_63,"4 < x < 1 2 | y = 1 3 ). solution : by definition of the marginal density. for 0 < x < 2,"
585,1,"['conditional density', 'conditional']", Joint Probability Distributions,seg_63,"therefore, using the conditional density definition, for 0 < x < 2,"
586,1,['case'], Joint Probability Distributions,seg_63,"if f(x|y) does not depend on y, as is the case for example 3.20, then f(x|y) = g(x) and f(x, y) = g(x)h(y). the proof follows by substituting"
587,1,"['distribution', 'marginal', 'marginal distribution']", Joint Probability Distributions,seg_63,"into the marginal distribution of x. that is,"
588,1,"['density function', 'probability density function', 'probability', 'function']", Joint Probability Distributions,seg_63,"since h(y) is the probability density function of y . therefore,"
589,1,"['random variables', 'independent', 'variables', 'independence', 'random variable', 'variable', 'independent random variables', 'random', 'outcome', 'statistical']", Joint Probability Distributions,seg_63,"it should make sense to the reader that if f(x|y) does not depend on y, then of course the outcome of the random variable y has no impact on the outcome of the random variable x. in other words, we say that x and y are independent random variables. we now offer the following formal definition of statistical independence."
590,1,"['random variables', 'independent', 'variables', 'marginal', 'discrete', 'marginal distributions', 'distribution', 'joint', 'random', 'continuous', 'distributions']", Joint Probability Distributions,seg_63,"definition 3.12: let x and y be two random variables, discrete or continuous, with joint probability distribution f(x, y) and marginal distributions g(x) and h(y), respectively. the random variables x and y are said to be statistically independent if and only if"
591,1,['range'], Joint Probability Distributions,seg_63,"for all (x, y) within their range."
592,1,"['marginal', 'case', 'discrete', 'independence', 'probability', 'random', 'function', 'statistical', 'distributions', 'density function', 'continuous random variables', 'joint probability distribution', 'distribution', 'continuous', 'random variables', 'independent', 'variables', 'discrete random variables', 'marginal distributions', 'probability distribution', 'joint', 'combinations', 'joint probability']", Joint Probability Distributions,seg_63,"the continuous random variables of example 3.20 are statistically independent, since the product of the two marginal distributions gives the joint density function. this is obviously not the case, however, for the continuous variables of example 3.19. checking for statistical independence of discrete random variables requires a more thorough investigation, since it is possible to have the product of the marginal distributions equal to the joint probability distribution for some but not all combinations of (x, y). if you can find any point (x, y) for which f(x, y) is defined such that f(x, y) = g(x)h(y), the discrete variables x and y are not statistically independent."
593,1,"['random variables', 'probabilities', 'independent', 'table', 'variables', 'random']", Joint Probability Distributions,seg_63,"example 3.21: show that the random variables of example 3.14 are not statistically independent. proof : let us consider the point (0, 1). from table 3.1 we find the three probabilities f(0, 1), g(0), and h(1) to be"
594,1,['independent'], Joint Probability Distributions,seg_63,and therefore x and y are not statistically independent.
595,1,"['random variables', 'variables', 'marginal', 'case', 'distribution', 'joint', 'probability', 'random', 'function', 'marginal distribution', 'probability function', 'joint probability']", Joint Probability Distributions,seg_63,"all the preceding definitions concerning two random variables can be generalized to the case of n random variables. let f(x1, x2, . . . , xn) be the joint probability function of the random variables x1, x2, . . . , xn. the marginal distribution of x1, for example, is"
596,1,"['case', 'discrete']", Joint Probability Distributions,seg_63,"for the discrete case, and"
597,1,"['marginal', 'case', 'marginal distributions', 'joint', 'continuous', 'distributions']", Joint Probability Distributions,seg_63,"for the continuous case. we can now obtain joint marginal distributions such as g(x1, x2), where"
598,1,"['case', 'discrete']", Joint Probability Distributions,seg_63,"⎧ ∑ · · ·∑ f(x1, x2, . . . , xn) (discrete case), g(x1, x2) = ⎨x3∞ xn ∞"
599,1,"['continuous', 'case']", Joint Probability Distributions,seg_63,"⎩∫−∞ · · ·∫−∞ f(x1, x2, . . . , xn) dx3 dx4 · · · dxn (continuous case)."
600,1,"['conditional', 'conditional distributions', 'distribution', 'joint', 'conditional distribution', 'distributions']", Joint Probability Distributions,seg_63,"we could consider numerous conditional distributions. for example, the joint conditional distribution of x1, x2, and x3, given that x4 = x4, x5 = x5, . . . , xn = xn, is written"
601,1,"['random variables', 'variables', 'marginal', 'distribution', 'joint', 'random', 'marginal distribution']", Joint Probability Distributions,seg_63,"where g(x4, x5, . . . , xn) is the joint marginal distribution of the random variables x4, x5, . . . , xn."
602,1,"['variables', 'independence', 'statistical']", Joint Probability Distributions,seg_63,"a generalization of definition 3.12 leads to the following definition for the mutual statistical independence of the variables x1, x2, . . . , xn."
603,1,"['random variables', 'independent', 'variables', 'marginal', 'joint probability distribution', 'discrete', 'distribution', 'probability distribution', 'joint', 'probability', 'random', 'continuous', 'marginal distribution', 'joint probability']", Joint Probability Distributions,seg_63,"definition 3.13: let x1, x2, . . . , xn be n random variables, discrete or continuous, with joint probability distribution f(x1, x2, . . . , xn) and marginal distribution f1(x1), f2(x2), . . . , fn(xn), respectively. the random variablesx1, x2, . . . , xn are said to be mutually statistically independent if and only if"
604,1,['range'], Joint Probability Distributions,seg_63,"for all (x1, x2, . . . , xn) within their range."
605,1,"['density function', 'probability density function', 'random variable', 'variable', 'probability', 'random', 'function']", Joint Probability Distributions,seg_63,"example 3.22: suppose that the shelf life, in years, of a certain perishable food product packaged in cardboard containers is a random variable whose probability density function is given by"
606,0,[], Joint Probability Distributions,seg_63,"let x1, x2, and x3 represent the shelf lives for three of these containers selected independently and find p (x1 < 2, 1 < x2 < 3, x3 > 2)."
607,1,"['random variables', 'independent', 'variables', 'joint', 'probability', 'random', 'joint probability']", Joint Probability Distributions,seg_63,"solution : since the containers were selected independently, we can assume that the random variables x1, x2, and x3 are statistically independent, having the joint probability density"
608,1,"['random variables', 'probability distributions', 'variables', 'probability', 'random', 'distributions']", Joint Probability Distributions,seg_63,104 chapter 3 random variables and probability distributions
609,1,"['continuous distribution', 'observations', 'discrete', 'frequency', 'probability', 'function', 'binomial distribution', 'exponential distribution', 'estimate', 'probability distributions', 'data', 'cases', 'exponential', 'standard', 'histograms', 'probability density functions', 'historical data', 'distributions', 'functions', 'density functions', 'failure', 'distribution', 'binomial', 'continuous', 'probability function', 'estimated', 'independent']", Joint Probability Distributions,seg_63,"this is an important point in the text to provide the reader with a transition into the next three chapters. we have given illustrations in both examples and exercises of practical scientific and engineering situations in which probability distributions and their properties are used to solve important problems. these probability distributions, either discrete or continuous, were introduced through phrases like “it is known that” or “suppose that” or even in some cases “historical evidence suggests that.” these are situations in which the nature of the distribution and even a good estimate of the probability structure can be determined through historical data, data from long-term studies, or even large amounts of planned data. the reader should remember the discussion of the use of histograms in chapter 1 and from that recall how frequency distributions are estimated from the histograms. however, not all probability functions and probability density functions are derived from large amounts of historical data. there are a substantial number of situations in which the nature of the scientific scenario suggests a distribution type. indeed, many of these are reflected in exercises in both chapter 2 and this chapter. when independent repeated observations are binary in nature (e.g., defective or not, survive or not, allergic or not) with value 0 or 1, the distribution covering this situation is called the binomial distribution and the probability function is known and will be demonstrated in its generality in chapter 5. exercise 3.34 in section 3.3 and review exercise 3.80 are examples, and there are others that the reader should recognize. the scenario of a continuous distribution in time to failure, as in review exercise 3.69 or exercise 3.27 on page 93, often suggests a distribution type called the exponential distribution. these types of illustrations are merely two of many so-called standard distributions that are used extensively in real-world problems because the scientific scenario that gives rise to each of them is recognizable and occurs often in practice. chapters 5 and 6 cover many of these types along with some underlying theory concerning their use."
610,1,"['distributional parameters', 'case', 'discrete', 'probability', 'function', 'data', 'information', 'mean', 'population', 'density function', 'probability density function', 'parameters', 'continuous', 'variance', 'probability function', 'population mean']", Joint Probability Distributions,seg_63,"a second part of this transition to material in future chapters deals with the notion of population parameters or distributional parameters. recall in chapter 1 we discussed the need to use data to provide information about these parameters. we went to some length in discussing the notions of a mean and variance and provided a vision for the concepts in the context of a population. indeed, the population mean and variance are easily found from the probability function for the discrete case or probability density function for the continuous case. these parameters and their importance in the solution of many types of real-world problems will provide much of the material in chapters 8 through 17."
611,1,"['probabilities', 'probability distributions', 'distribution', 'probability distribution', 'probability', 'process', 'distributions']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_69,"in future chapters it will become apparent that probability distributions represent the structure through which probabilities that are computed aid in the evaluation and understanding of a process. for example, in review exercise 3.65, the probability distribution that quantifies the probability of a heavy load during certain time periods can be very useful in planning for any changes in the system. review exercise 3.69 describes a scenario in which the life span of an electronic component is studied. knowledge of the probability structure for the component will contribute significantly to an understanding of the reliability of a large system of which the component is a part. in addition, an understanding of the general nature of probability distributions will enhance understanding of the concept of a p-value, which was introduced briefly in chapter 1 and will play a major role beginning in chapter 10 and extending throughout the balance of the text."
612,1,"['density function', 'probability density function', 'parameters', 'probability distributions', 'failure', 'distribution', 'probability', 'function', 'probability function', 'distributions']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_69,"chapters 4, 5, and 6 depend heavily on the material in this chapter. in chapter 4, we discuss the meaning of important parameters in probability distributions. these important parameters quantify notions of central tendency and variability in a system. in fact, knowledge of these quantities themselves, quite apart from the complete distribution, can provide insight into the nature of the system. chapters 5 and 6 will deal with engineering, biological, or general scientific scenarios that identify special types of distributions. for example, the structure of the probability function in review exercise 3.65 will easily be identified under certain assumptions discussed in chapter 5. the same holds for the scenario of review exercise 3.69. this is a special type of time to failure problem for which the probability density function will be discussed in chapter 6."
613,1,"['distribution', 'probability distribution', 'probability']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_69,"as far as potential hazards with the use of material in this chapter, the warning to the reader is not to read more into the material than is evident. the general nature of the probability distribution for a specific scientific phenomenon is not obvious from what is learned in this chapter. the purpose of this chapter is for readers to learn how to manipulate a probability distribution, not to learn how to identify a specific type. chapters 5 and 6 go a long way toward identification according to the general nature of the scientific system."
614,1,"['sample', 'sample mean', 'experiment', 'arithmetic mean', 'data', 'mean', 'average']", Mean of a Random Variable,seg_73,"in chapter 1, we discussed the sample mean, which is the arithmetic mean of the data. now consider the following. if two coins are tossed 16 times and x is the number of heads that occur per toss, then the values of x are 0, 1, and 2. suppose that the experiment yields no heads, one head, and two heads a total of 4, 7, and 5 times, respectively. the average number of heads per toss of the two coins is then"
615,1,"['experiment', 'data', 'outcome', 'average']", Mean of a Random Variable,seg_73,"this is an average value of the data and yet it is not a possible outcome of {0, 1, 2}. hence, an average is not necessarily a possible outcome for the experiment. for instance, a salesman’s average monthly income is not likely to be equal to any of his monthly paychecks."
616,1,['average'], Mean of a Random Variable,seg_73,let us now restructure our computation for the average number of heads so as to have the following equivalent form:
617,1,"['experiment', 'frequencies', 'observations', 'data', 'relative frequencies', 'set', 'mean', 'number of observations', 'average']", Mean of a Random Variable,seg_73,"the numbers 4/16, 7/16, and 5/16 are the fractions of the total tosses resulting in 0, 1, and 2 heads, respectively. these fractions are also the relative frequencies for the different values of x in our experiment. in fact, then, we can calculate the mean, or average, of a set of data by knowing the distinct values that occur and their relative frequencies, without any knowledge of the total number of observations in our set of data. therefore, if 4/16, or 1/4, of the tosses result in no heads, 7/16 of the tosses result in one head, and 5/16 of the tosses result in two heads, the mean number of heads per toss would be 1.06 no matter whether the total number of tosses were 16, 1000, or even 10,000."
618,1,"['method', 'frequencies', 'relative frequencies', 'distribution', 'random variable', 'probability distribution', 'variable', 'mean', 'probability', 'random', 'average']", Mean of a Random Variable,seg_73,this method of relative frequencies is used to calculate the average number of heads per toss of two coins that we might expect in the long run. we shall refer to this average value as the mean of the random variable x or the mean of the probability distribution of x and write it as μx or simply as μ when it is
619,1,"['mathematical expectation', 'statisticians', 'expected value', 'random variable', 'variable', 'expectation', 'mean', 'random']", Mean of a Random Variable,seg_73,"clear to which random variable we refer. it is also common among statisticians to refer to this mean as the mathematical expectation, or the expected value of the random variable x, and denote it as e(x)."
620,1,"['sample', 'experiment', 'sample space']", Mean of a Random Variable,seg_73,"assuming that 1 fair coin was tossed twice, we find that the sample space for our experiment is"
621,1,['sample'], Mean of a Random Variable,seg_73,"since the 4 sample points are all equally likely, it follows that"
622,1,"['probabilities', 'frequencies', 'relative frequencies', 'events', 'tail']", Mean of a Random Variable,seg_73,"where a typical element, say th, indicates that the first toss resulted in a tail followed by a head on the second toss. now, these probabilities are just the relative frequencies for the given events in the long run. therefore,"
623,1,['average'], Mean of a Random Variable,seg_73,"this result means that a person who tosses 2 coins over and over again will, on the average, get 1 head per toss."
624,1,"['discrete random variable', 'continuous random variables', 'random variables', 'method', 'variables', 'case', 'discrete', 'expected value', 'random variable', 'variable', 'mean', 'probability', 'random', 'continuous']", Mean of a Random Variable,seg_73,"the method described above for calculating the expected number of heads per toss of 2 coins suggests that the mean, or expected value, of any discrete random variable may be obtained by multiplying each of the values x1, x2, . . . , xn of the random variable x by its corresponding probability f(x1), f(x2), . . . , f(xn) and summing the products. this is true, however, only if the random variable is discrete. in the case of continuous random variables, the definition of an expected value is essentially the same with summations replaced by integrations."
625,1,"['distribution', 'random variable', 'probability distribution', 'expected value', 'variable', 'mean', 'probability', 'random']", Mean of a Random Variable,seg_73,"definition 4.1: let x be a random variable with probability distribution f(x). the mean, or expected value, of x is"
626,1,['discrete'], Mean of a Random Variable,seg_73,"if x is discrete, and"
627,1,['continuous'], Mean of a Random Variable,seg_73,if x is continuous.
628,1,"['mathematical expectation', 'sample', 'sample mean', 'data', 'distribution', 'expected value', 'probability distribution', 'expectation', 'mean', 'probability']", Mean of a Random Variable,seg_73,"the reader should note that the way to calculate the expected value, or mean, shown here is different from the way to calculate the sample mean described in chapter 1, where the sample mean is obtained by using data. in mathematical expectation, the expected value is calculated by using the probability distribution."
629,1,"['distribution', 'mean', 'expected value']", Mean of a Random Variable,seg_73,"however, the mean is usually understood as a “center” value of the underlying distribution if we use the expected value, as in definition 4.1."
630,1,"['expected value', 'sample']", Mean of a Random Variable,seg_73,example 4.1: a lot containing 7 components is sampled by a quality inspector; the lot contains 4 good components and 3 defective components. a sample of 3 is taken by the inspector. find the expected value of the number of good components in this sample.
631,1,"['sample', 'distribution', 'probability distribution', 'probability']", Mean of a Random Variable,seg_73,solution : let x represent the number of good components in the sample. the probability distribution of x is
632,1,"['sample', 'average', 'random']", Mean of a Random Variable,seg_73,"thus, if a sample of size 3 is selected at random over and over again from a lot of 4 good components and 3 defective components, it will contain, on average, 1.7 good components."
633,1,"['independent', 'results', 'successful', 'probability']", Mean of a Random Variable,seg_73,"example 4.2: a salesperson for a medical device company has two appointments on a given day. at the first appointment, he believes that he has a 70% chance to make the deal, from which he can earn $1000 commission if successful. on the other hand, he thinks he only has a 40% chance to make the deal at the second appointment, from which, if successful, he can make $1500. what is his expected commission based on his own probability belief? assume that the appointment results are independent of each other."
634,1,"['independence', 'probabilities', 'associated']", Mean of a Random Variable,seg_73,"solution : first, we know that the salesperson, for the two appointments, can have 4 possible commission totals: $0, $1000, $1500, and $2500. we then need to calculate their associated probabilities. by independence, we obtain"
635,0,[], Mean of a Random Variable,seg_73,"therefore, the expected commission for the salesperson is"
636,1,"['random variables', 'variables', 'continuous random variable', 'failure', 'discrete', 'expected value', 'random variable', 'variable', 'cases', 'mean', 'parameter', 'random', 'continuous']", Mean of a Random Variable,seg_73,"examples 4.1 and 4.2 are designed to allow the reader to gain some insight into what we mean by the expected value of a random variable. in both cases the random variables are discrete. we follow with an example involving a continuous random variable, where an engineer is interested in the mean life of a certain type of electronic device. this is an illustration of a time to failure problem that occurs often in practice. the expected value of the life of a device is an important parameter for its evaluation."
637,1,"['density function', 'probability density function', 'random variable', 'variable', 'probability', 'random', 'function']", Mean of a Random Variable,seg_73,example 4.3: let x be the random variable that denotes the life in hours of a certain electronic device. the probability density function is
638,0,[], Mean of a Random Variable,seg_73,find the expected life of this type of device.
639,0,[], Mean of a Random Variable,seg_73,"solution : using definition 4.1, we have"
640,1,['average'], Mean of a Random Variable,seg_73,"therefore, we can expect this type of device to last, on average, 200 hours."
641,1,"['discrete random variable', 'discrete', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random']", Mean of a Random Variable,seg_73,"now let us consider a new random variable g(x), which depends on x; that is, each value of g(x) is determined by the value of x. for instance, g(x) might be x2 or 3x − 1, and whenever x assumes the value 2, g(x) assumes the value g(2). in particular, if x is a discrete random variable with probability distribution f(x), for x = −1, 0, 1, 2, and g(x) = x2, then"
642,1,"['distribution', 'probability distribution', 'probability']", Mean of a Random Variable,seg_73,and so the probability distribution of g(x) may be written
643,1,"['expected value', 'random variable', 'variable', 'random']", Mean of a Random Variable,seg_73,"by the definition of the expected value of a random variable, we obtain"
644,1,"['continuous random variables', 'random variables', 'variables', 'discrete', 'random', 'continuous']", Mean of a Random Variable,seg_73,this result is generalized in theorem 4.1 for both discrete and continuous random variables.
645,1,"['distribution', 'random variable', 'probability distribution', 'variable', 'expected value', 'probability', 'random']", Mean of a Random Variable,seg_73,theorem 4.1: let x be a random variable with probability distribution f(x). the expected value of the random variable g(x) is
646,1,['discrete'], Mean of a Random Variable,seg_73,"if x is discrete, and"
647,1,['continuous'], Mean of a Random Variable,seg_73,if x is continuous.
648,1,"['distribution', 'probability distribution', 'probability']", Mean of a Random Variable,seg_73,example 4.4: suppose that the number of cars x that pass through a car wash between 4:00 p.m. and 5:00 p.m. on any sunny friday has the following probability distribution:
649,0,[], Mean of a Random Variable,seg_73,"x 4 5 6 7 8 9 1 1 1 1 1 1 p (x = x) 12 12 4 4 6 6 let g(x) = 2x−1 represent the amount of money, in dollars, paid to the attendant by the manager. find the attendant’s expected earnings for this particular time period."
650,0,[], Mean of a Random Variable,seg_73,"solution : by theorem 4.1, the attendant can expect to receive"
651,1,"['density function', 'random variable', 'variable', 'random', 'function']", Mean of a Random Variable,seg_73,example 4.5: let x be a random variable with density function
652,1,['expected value'], Mean of a Random Variable,seg_73,find the expected value of g(x) = 4x + 3.
653,0,[], Mean of a Random Variable,seg_73,"solution : by theorem 4.1, we have"
654,1,"['mathematical expectation', 'random variables', 'variables', 'joint probability distribution', 'case', 'distribution', 'probability distribution', 'expectation', 'joint', 'probability', 'random', 'joint probability']", Mean of a Random Variable,seg_73,"we shall now extend our concept of mathematical expectation to the case of two random variables x and y with joint probability distribution f(x, y)."
655,1,"['random variables', 'variables', 'joint probability distribution', 'distribution', 'random variable', 'probability distribution', 'expected value', 'variable', 'mean', 'joint', 'random', 'probability', 'joint probability']", Mean of a Random Variable,seg_73,"definition 4.2: let x and y be random variables with joint probability distribution f(x, y). the mean, or expected value, of the random variable g(x,y ) is"
656,1,['discrete'], Mean of a Random Variable,seg_73,"if x and y are discrete, and"
657,1,['continuous'], Mean of a Random Variable,seg_73,if x and y are continuous.
658,1,"['functions', 'random variables', 'variables', 'random', 'expectations']", Mean of a Random Variable,seg_73,generalization of definition 4.2 for the calculation of mathematical expectations of functions of several random variables is straightforward.
659,1,"['random variables', 'table', 'variables', 'joint probability distribution', 'distribution', 'probability distribution', 'expected value', 'joint', 'probability', 'random', 'joint probability']", Mean of a Random Variable,seg_73,"example 4.6: let x and y be the random variables with joint probability distribution indicated in table 3.1 on page 96. find the expected value of g(x,y ) = xy . the table is reprinted here for convenience."
660,0,[], Mean of a Random Variable,seg_73,"solution : by definition 4.2, we write"
661,1,"['function', 'density function']", Mean of a Random Variable,seg_73,example 4.7: find e(y/x) for the density function
662,0,[], Mean of a Random Variable,seg_73,solution : we have
663,1,"['case', 'discrete']", Mean of a Random Variable,seg_73,"⎧ xf(x, y) = xg(x) (discrete case), e(x) = ⎨∑"
664,1,"['continuous', 'case']", Mean of a Random Variable,seg_73,"∑ x ∞ xf(x, y) dy dx = xg(x) dx (continuous case), ⎩∫−∞ ∫−∞ ∫−∞"
665,1,"['marginal', 'joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'marginal distribution', 'joint probability']", Mean of a Random Variable,seg_73,"where g(x) is the marginal distribution of x. therefore, in calculating e(x) over a two-dimensional space, one may use either the joint probability distribution of x and y or the marginal distribution of x. similarly, we define"
666,1,"['case', 'discrete']", Mean of a Random Variable,seg_73,"⎧ yf(x, y) = yh(y) (discrete case), e(y ) = ⎨∑"
667,1,"['continuous', 'case']", Mean of a Random Variable,seg_73,"∑ y ∞ ⎩∫−∞ ∫−∞ yf(x, y) dxdy = ∫−∞ yh(y) dy (continuous case),"
668,1,"['marginal', 'distribution', 'random variable', 'variable', 'marginal distribution', 'random']", Mean of a Random Variable,seg_73,where h(y) is the marginal distribution of the random variable y .
669,1,"['observations', 'statistics', 'discrete', 'probability', 'random', 'probability distributions', 'random variable', 'mean', 'histograms', 'distributions', 'distribution', 'expected value', 'discrete probability distributions', 'variability', 'variable', 'probability distribution', 'dispersion']", Variance and Covariance of Random Variables,seg_77,"the mean, or expected value, of a random variable x is of special importance in statistics because it describes where the probability distribution is centered. by itself, however, the mean does not give an adequate description of the shape of the distribution. we also need to characterize the variability in the distribution. in figure 4.1, we have the histograms of two discrete probability distributions that have the same mean, μ = 2, but differ considerably in variability, or the dispersion of their observations about the mean."
670,1,"['dispersions', 'distributions']", Variance and Covariance of Random Variables,seg_77,figure 4.1: distributions with equal means and unequal dispersions.
671,1,"['variability', 'random variable', 'variable', 'probability', 'random', 'variance']", Variance and Covariance of Random Variables,seg_77,the most important measure of variability of a random variable x is obtained by applying theorem 4.1 with g(x) = (x − μ)2. the quantity is referred to as the variance of the random variable x or the variance of the probability
672,0,[], Variance and Covariance of Random Variables,seg_77,distribution of x and is denoted by var(x) or the symbol σx
673,1,"['random variable', 'variable', 'random']", Variance and Covariance of Random Variables,seg_77,"2 , or simply by σ2 when it is clear to which random variable we refer."
674,1,"['distribution', 'random variable', 'probability distribution', 'variable', 'mean', 'probability', 'random', 'variance']", Variance and Covariance of Random Variables,seg_77,definition 4.3: let x be a random variable with probability distribution f(x) and mean μ. the variance of x is
675,1,"['continuous', 'discrete']", Variance and Covariance of Random Variables,seg_77,"σ2 = e[(x − μ)2] =∑(x− μ)2f(x), if x is discrete, and x ∞ σ2 = e[(x − μ)2] =∫−∞ (x− μ)2f(x) dx, if x is continuous."
676,1,"['deviation', 'standard', 'standard deviation', 'variance']", Variance and Covariance of Random Variables,seg_77,"the positive square root of the variance, σ, is called the standard deviation of x."
677,1,"['deviation', 'observation', 'deviations', 'set', 'mean', 'vary']", Variance and Covariance of Random Variables,seg_77,"the quantity x−μ in definition 4.3 is called the deviation of an observation from its mean. since the deviations are squared and then averaged, σ2 will be much smaller for a set of x values that are close to μ than it will be for a set of values that vary considerably from μ."
678,1,"['distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random']", Variance and Covariance of Random Variables,seg_77,example 4.8: let the random variable x represent the number of automobiles that are used for official business purposes on any given workday. the probability distribution for company a [figure 4.1(a)] is
679,1,"['distribution', 'probability distribution', 'variance', 'probability']", Variance and Covariance of Random Variables,seg_77,show that the variance of the probability distribution for company b is greater than that for company a.
680,0,[], Variance and Covariance of Random Variables,seg_77,"solution : for company a, we find that"
681,0,[], Variance and Covariance of Random Variables,seg_77,"for company b, we have"
682,1,['variance'], Variance and Covariance of Random Variables,seg_77,"clearly, the variance of the number of automobiles that are used for official business purposes is greater for company b than for company a."
683,0,[], Variance and Covariance of Random Variables,seg_77,"an alternative and preferred formula for finding σ2, which often simplifies the calculations, is stated in the following theorem."
684,1,"['random variable', 'variable', 'random', 'variance']", Variance and Covariance of Random Variables,seg_77,the variance of a random variable x is
685,1,"['case', 'discrete']", Variance and Covariance of Random Variables,seg_77,"proof : for the discrete case, we can write"
686,1,"['discrete', 'probability']", Variance and Covariance of Random Variables,seg_77,"σ2 =∑(x− μ)2f(x) =∑(x2 − 2μx+ μ2)f(x) x x ∑ ∑ ∑ = x2f(x)− 2μ xf(x) + μ2 f(x). x x x ∑ ∑ since μ = xf(x) by definition, and f(x) = 1 for any discrete probability"
687,1,['distribution'], Variance and Covariance of Random Variables,seg_77,"x x distribution, it follows that"
688,1,"['continuous', 'case']", Variance and Covariance of Random Variables,seg_77,"σ2 =∑x2f(x)− μ2 = e(x2)− μ2. x for the continuous case the proof is step by step the same, with summations replaced by integrations."
689,1,"['distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random']", Variance and Covariance of Random Variables,seg_77,example 4.9: let the random variable x represent the number of defective parts for a machine when 3 parts are sampled from a production line and tested. the following is the probability distribution of x.
690,0,[], Variance and Covariance of Random Variables,seg_77,"solution : first, we compute"
691,1,"['continuous', 'continuous random variable', 'random variable', 'variable', 'probability', 'random', 'efficiency']", Variance and Covariance of Random Variables,seg_77,"example 4.10: the weekly demand for a drinking-water product, in thousands of liters, from a local chain of efficiency stores is a continuous random variable x having the probability density"
692,1,"['variance', 'mean']", Variance and Covariance of Random Variables,seg_77,find the mean and variance of x.
693,1,"['deviation', 'measurement', 'variances', 'observations', 'distribution', 'variable', 'scores', 'standard', 'standard deviation', 'variance', 'distributions']", Variance and Covariance of Random Variables,seg_77,"at this point, the variance or standard deviation has meaning only when we compare two or more distributions that have the same units of measurement. therefore, we could compare the variances of the distributions of contents, measured in liters, of bottles of orange juice from two companies, and the larger value would indicate the company whose product was more variable or less uniform. it would not be meaningful to compare the variance of a distribution of heights to the variance of a distribution of aptitude scores. in section 4.4, we show how the standard deviation can be used to describe a single distribution of observations."
694,1,"['random variables', 'variables', 'random variable', 'variable', 'random', 'variance']", Variance and Covariance of Random Variables,seg_77,"we shall now extend our concept of the variance of a random variable x to include random variables related to x. for the random variable g(x), the variance is denoted by σg2(x) and is calculated by means of the following theorem."
695,1,"['distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'variance']", Variance and Covariance of Random Variables,seg_77,let x be a random variable with probability distribution f(x). the variance of
696,1,"['random variable', 'variable', 'random']", Variance and Covariance of Random Variables,seg_77,theorem 4.3: the random variable g(x) is
697,1,['discrete'], Variance and Covariance of Random Variables,seg_77,"if x is discrete, and"
698,1,['continuous'], Variance and Covariance of Random Variables,seg_77,if x is continuous.
699,1,"['random variable', 'variable', 'mean', 'random']", Variance and Covariance of Random Variables,seg_77,"proof : since g(x) is itself a random variable with mean μg(x) as defined in theorem 4.1, it follows from definition 4.3 that"
700,1,"['random variable', 'variable', 'random']", Variance and Covariance of Random Variables,seg_77,"now, applying theorem 4.1 again to the random variable [g(x)−μg(x)]2 completes the proof."
701,1,"['distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'variance']", Variance and Covariance of Random Variables,seg_77,"example 4.11: calculate the variance of g(x) = 2x + 3, where x is a random variable with probability distribution"
702,1,"['random variable', 'variable', 'mean', 'random']", Variance and Covariance of Random Variables,seg_77,"solution : first, we find the mean of the random variable 2x+3. according to theorem 4.1,"
703,1,"['density function', 'random variable', 'variable', 'random', 'function', 'variance']", Variance and Covariance of Random Variables,seg_77,example 4.12: let x be a random variable having the density function given in example 4.5 on page 115. find the variance of the random variable g(x) = 4x + 3.
704,1,"['expected value', 'covariance']", Variance and Covariance of Random Variables,seg_77,"if g(x,y ) = (x−μx)(y −μy ), where μx = e(x) and μy = e(y ), definition 4.2 yields an expected value called the covariance of x and y , which we denote by σxy or cov(x,y )."
705,1,"['random variables', 'variables', 'joint probability distribution', 'distribution', 'probability distribution', 'joint', 'covariance', 'probability', 'random', 'joint probability']", Variance and Covariance of Random Variables,seg_77,"definition 4.4: let x and y be random variables with joint probability distribution f(x, y). the covariance of x and y is"
706,1,['discrete'], Variance and Covariance of Random Variables,seg_77,"if x and y are discrete, and"
707,1,['continuous'], Variance and Covariance of Random Variables,seg_77,if x and y are continuous.
708,1,"['linear', 'random variables', 'independent', 'association', 'variables', 'nonlinear', 'dependent', 'covariance', 'random']", Variance and Covariance of Random Variables,seg_77,"the covariance between two random variables is a measure of the nature of the association between the two. if large values of x often result in large values of y or small values of x result in small values of y , positive x−μx will often result in positive y −μy and negative x−μx will often result in negative y −μy . thus, the product (x − μx)(y − μy ) will tend to be positive. on the other hand, if large x values often result in small y values, the product (x−μx)(y −μy ) will tend to be negative. the sign of the covariance indicates whether the relationship between two dependent random variables is positive or negative. whenx and y are statistically independent, it can be shown that the covariance is zero (see corollary 4.5). the converse, however, is not generally true. two variables may have zero covariance and still not be statistically independent. note that the covariance only describes the linear relationship between two random variables. therefore, if a covariance between x and y is zero, x and y may have a nonlinear relationship, which means that they are not necessarily independent."
709,0,[], Variance and Covariance of Random Variables,seg_77,the alternative and preferred formula for σxy is stated by theorem 4.4.
710,1,"['random variables', 'variables', 'covariance', 'random']", Variance and Covariance of Random Variables,seg_77,"theorem 4.4: the covariance of two random variables x and y with means μx and μy , respectively, is given by"
711,1,"['case', 'discrete']", Variance and Covariance of Random Variables,seg_77,"proof : for the discrete case, we can write"
712,1,"['distribution', 'discrete distribution', 'joint', 'discrete']", Variance and Covariance of Random Variables,seg_77,"for any joint discrete distribution, it follows that"
713,1,"['continuous', 'case']", Variance and Covariance of Random Variables,seg_77,"for the continuous case, the proof is identical with summations replaced by integrals."
714,1,"['joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'random', 'joint probability']", Variance and Covariance of Random Variables,seg_77,"example 4.13: example 3.14 on page 95 describes a situation involving the number of blue refills x and the number of red refills y . two refills for a ballpoint pen are selected at random from a certain box, and the following is the joint probability distribution:"
715,1,['covariance'], Variance and Covariance of Random Variables,seg_77,find the covariance of x and y .
716,1,"['function', 'density function', 'joint']", Variance and Covariance of Random Variables,seg_77,example 4.14: the fraction x of male runners and the fraction y of female runners who compete in marathon races are described by the joint density function
717,1,['covariance'], Variance and Covariance of Random Variables,seg_77,find the covariance of x and y .
718,1,"['functions', 'density functions', 'marginal', 'marginal density']", Variance and Covariance of Random Variables,seg_77,solution : we first compute the marginal density functions. they are
719,1,"['functions', 'density functions', 'marginal', 'marginal density']", Variance and Covariance of Random Variables,seg_77,"from these marginal density functions, we compute"
720,1,"['function', 'density function', 'joint']", Variance and Covariance of Random Variables,seg_77,"from the joint density function given above, we have"
721,1,"['random variables', 'variables', 'statistics', 'correlation', 'covariance', 'correlation coefficient', 'random', 'coefficient']", Variance and Covariance of Random Variables,seg_77,"although the covariance between two random variables does provide information regarding the nature of the relationship, the magnitude of σxy does not indicate anything regarding the strength of the relationship, since σxy is not scale-free. its magnitude will depend on the units used to measure both x and y . there is a scale-free version of the covariance called the correlation coefficient that is used widely in statistics."
722,1,"['random variables', 'random', 'standard deviations', 'variables', 'coefficient', 'correlation', 'deviations', 'covariance', 'correlation coefficient', 'standard']", Variance and Covariance of Random Variables,seg_77,"definition 4.5: let x and y be random variables with covariance σxy and standard deviations σx and σy , respectively. the correlation coefficient of x and y is"
723,1,"['linear', 'correlation', 'correlation coefficient', 'coefficient', 'inequality']", Variance and Covariance of Random Variables,seg_77,"it should be clear to the reader that ρxy is free of the units of x and y . the correlation coefficient satisfies the inequality −1 ≤ ρxy ≤ 1. it assumes a value of zero when σxy = 0. where there is an exact linear dependency, say y ≡ a+ bx,"
724,1,"['linear', 'regression', 'correlation', 'linear regression', 'correlation coefficient', 'coefficient']", Variance and Covariance of Random Variables,seg_77,"ρxy = 1 if b > 0 and ρxy = −1 if b < 0. (see exercise 4.48.) the correlation coefficient is the subject of more discussion in chapter 12, where we deal with linear regression."
725,1,"['coefficient', 'correlation coefficient', 'correlation']", Variance and Covariance of Random Variables,seg_77,example 4.15: find the correlation coefficient between x and y in example 4.13. solution : since
726,1,"['coefficient', 'correlation coefficient', 'correlation']", Variance and Covariance of Random Variables,seg_77,"therefore, the correlation coefficient between x and y is"
727,1,"['coefficient', 'correlation coefficient', 'correlation']", Variance and Covariance of Random Variables,seg_77,example 4.16: find the correlation coefficient of x and y in example 4.14. solution : because
728,1,"['correlation coefficients', 'correlation', 'covariance', 'coefficients']", Variance and Covariance of Random Variables,seg_77,"note that although the covariance in example 4.15 is larger in magnitude (disregarding the sign) than that in example 4.16, the relationship of the magnitudes of the correlation coefficients in these two examples is just the reverse. this is evidence that we cannot look at the magnitude of the covariance to decide on how strong the relationship is."
729,1,"['continuous random variables', 'random variables', 'parameters', 'variables', 'results', 'case', 'discrete', 'random', 'continuous', 'expectations', 'variances']", Means and Variances of Linear Combinations of Random Variables,seg_81,"we now develop some useful properties that will simplify the calculations of means and variances of random variables that appear in later chapters. these properties will permit us to deal with expectations in terms of other parameters that are either known or easily computed. all the results that we present here are valid for both discrete and continuous random variables. proofs are given only for the continuous case. we begin with a theorem and two corollaries that should be, intuitively, reasonable to the reader."
730,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,"if a and b are constants, then"
731,1,['expected value'], Means and Variances of Linear Combinations of Random Variables,seg_81,"proof : by the definition of expected value,"
732,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,"the first integral on the right is e(x) and the second integral equals 1. therefore, we have"
733,1,"['discrete random variable', 'discrete', 'random variable', 'variable', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,"example 4.17: applying theorem 4.5 to the discrete random variable f(x) = 2x − 1, rework example 4.4 on page 115."
734,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,"solution : according to theorem 4.5, we can write"
735,1,"['continuous random variable', 'random variable', 'variable', 'random', 'continuous']", Means and Variances of Linear Combinations of Random Variables,seg_81,"example 4.18: applying theorem 4.5 to the continuous random variable g(x) = 4x + 3, rework example 4.5 on page 115."
736,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,"solution : for example 4.5, we may use theorem 4.5 to write"
737,1,"['expected value', 'functions', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,the expected value of the sum or difference of two or more functions of a random
738,1,"['functions', 'variable', 'expected values']", Means and Variances of Linear Combinations of Random Variables,seg_81,"theorem 4.6: variable x is the sum or difference of the expected values of the functions. that is,"
739,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,"proof : by definition,"
740,1,"['distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,example 4.19: let x be a random variable with probability distribution as follows: x 0 1 2 3 1 1 1 f(x) 0 3 2 6
741,1,['expected value'], Means and Variances of Linear Combinations of Random Variables,seg_81,find the expected value of y = (x − 1)2.
742,1,['function'], Means and Variances of Linear Combinations of Random Variables,seg_81,"solution : applying theorem 4.6 to the function y = (x − 1)2, we can write"
743,1,"['density function', 'continuous', 'continuous random variable', 'random variable', 'variable', 'random', 'function']", Means and Variances of Linear Combinations of Random Variables,seg_81,"example 4.20: the weekly demand for a certain drink, in thousands of liters, at a chain of convenience stores is a continuous random variable g(x) = x2 +x − 2, where x has the density function"
744,1,['expected value'], Means and Variances of Linear Combinations of Random Variables,seg_81,find the expected value of the weekly demand for the drink.
745,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,"solution : by theorem 4.6, we write"
746,1,"['efficiency', 'average']", Means and Variances of Linear Combinations of Random Variables,seg_81,so the average weekly demand for the drink from this chain of efficiency stores is 2500 liters.
747,1,"['functions', 'random variables', 'variables', 'expected value', 'joint', 'probability', 'random', 'expected values', 'joint probability']", Means and Variances of Linear Combinations of Random Variables,seg_81,"suppose that we have two random variables x and y with joint probability distribution f(x, y). two additional properties that will be very useful in succeeding chapters involve the expected values of the sum, difference, and product of these two random variables. first, however, let us prove a theorem on the expected value of the sum or difference of functions of the given variables. this, of course, is merely an extension of theorem 4.6."
748,1,"['expected value', 'functions', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,the expected value of the sum or difference of two or more functions of the random
749,1,"['functions', 'variables', 'expected values']", Means and Variances of Linear Combinations of Random Variables,seg_81,"theorem 4.7: variables x and y is the sum or difference of the expected values of the functions. that is,"
750,1,"['states', 'average']", Means and Variances of Linear Combinations of Random Variables,seg_81,"if x represents the daily production of some item from machine a and y the daily production of the same kind of item from machine b, then x +y represents the total number of items produced daily by both machines. corollary 4.4 states that the average daily production for both machines is equal to the sum of the average daily production of each machine."
751,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,let x and y be two independent random variables. then
752,1,['independent'], Means and Variances of Linear Combinations of Random Variables,seg_81,"since x and y are independent, we may write"
753,1,"['marginal', 'marginal distributions', 'distributions']", Means and Variances of Linear Combinations of Random Variables,seg_81,"where g(x) and h(y) are the marginal distributions ofx and y , respectively. hence,"
754,1,"['variables', 'discrete', 'random variable', 'variable', 'random', 'outcome', 'average']", Means and Variances of Linear Combinations of Random Variables,seg_81,"= e(x)e(y ). theorem 4.8 can be illustrated for discrete variables by considering the experiment of tossing a green die and a red die. let the random variable x represent the outcome on the green die and the random variable y represent the outcome on the red die. then xy represents the product of the numbers that occur on the pair of dice. in the long run, the average of the products of the numbers is equal to the product of the average number that occurs on the green die and the average number that occurs on the red die."
755,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,corollary 4.5: let x and y be two independent random variables. then σxy = 0.
756,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,proof : the proof can be carried out by using theorems 4.4 and 4.8.
757,1,"['density function', 'random variables', 'independent', 'variables', 'independent random variables', 'joint', 'random', 'function']", Means and Variances of Linear Combinations of Random Variables,seg_81,"example 4.21: it is known that the ratio of gallium to arsenide does not affect the functioning of gallium-arsenide wafers, which are the main components of microchips. let x denote the ratio of gallium to arsenide and y denote the functional wafers retrieved during a 1-hour period. x and y are independent random variables with the joint density function"
758,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,"solution : by definition,"
759,1,"['standard deviations', 'deviations', 'standard', 'variances']", Means and Variances of Linear Combinations of Random Variables,seg_81,we conclude this section by proving one theorem and presenting several corollaries that are useful for calculating variances or standard deviations.
760,1,"['random variables', 'variables', 'joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'random', 'joint probability']", Means and Variances of Linear Combinations of Random Variables,seg_81,"if x and y are random variables with joint probability distribution f(x, y) and a,"
761,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,"by using corollary 4.4 followed by corollary 4.2. therefore,"
762,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,"using theorem 4.9, we have the following corollaries."
763,1,"['variability', 'random variable', 'variable', 'random', 'variance']", Means and Variances of Linear Combinations of Random Variables,seg_81,"corollaries 4.6 and 4.7 state that the variance is unchanged if a constant is added to or subtracted from a random variable. the addition or subtraction of a constant simply shifts the values of x to the right or to the left but does not change their variability. however, if a random variable is multiplied or divided by a constant, then corollaries 4.6 and 4.8 state that the variance is multiplied or divided by the square of the constant."
764,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,"if x and y are independent random variables, then"
765,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,the result stated in corollary 4.9 is obtained from theorem 4.9 by invoking corollary 4.5.
766,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,"if x and y are independent random variables, then"
767,1,"['linear', 'random variables', 'linear combination', 'independent', 'combination', 'variables', 'independent random variables', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,"corollary 4.10 follows when b in corollary 4.9 is replaced by −b. generalizing to a linear combination of n independent random variables, we have corollary 4.11."
768,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,"corollary 4.11: if x1, x2, . . . , xn are independent random variables, then"
769,1,"['random variables', 'variables', 'random', 'variances']", Means and Variances of Linear Combinations of Random Variables,seg_81,example 4.22: if x and y are random variables with variances σx
770,1,"['random variable', 'variable', 'covariance', 'random', 'variance']", Means and Variances of Linear Combinations of Random Variables,seg_81,"2 = 2 and σy 2 = 4 and covariance σxy = −2, find the variance of the random variable z = 3x − 4y + 8."
771,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random', 'variances']", Means and Variances of Linear Combinations of Random Variables,seg_81,example 4.23: let x and y denote the amounts of two different types of impurities in a batch of a certain chemical product. suppose that x and y are independent random variables with variances σx
772,1,"['random variable', 'variable', 'random', 'variance']", Means and Variances of Linear Combinations of Random Variables,seg_81,2 = 2 and σy 2 = 3. find the variance of the random variable z = 3x − 2y + 5.
773,1,"['case', 'set', 'random', 'function', 'linear', 'data', 'standard', 'statistical', 'expected values', 'functions', 'model', 'linear model', 'nonlinear', 'linear combinations', 'data set', 'random variables', 'variables', 'combinations', 'variances']", Means and Variances of Linear Combinations of Random Variables,seg_81,"in that which has preceded this section, we have dealt with properties of linear functions of random variables for very important reasons. chapters 8 through 15 will discuss and illustrate practical real-world problems in which the analyst is constructing a linear model to describe a data set and thus to describe or explain the behavior of a certain scientific phenomenon. thus, it is natural that expected values and variances of linear combinations of random variables are encountered. however, there are situations in which properties of nonlinear functions of random variables become important. certainly there are many scientific phenomena that are nonlinear, and certainly statistical modeling using nonlinear functions is very important. in fact, in chapter 12, we deal with the modeling of what have become standard nonlinear models. indeed, even a simple function of random variables, such as z = x/y , occurs quite frequently in practice, and yet unlike in the case of"
774,1,"['linear', 'random variables', 'combinations', 'variables', 'expected value', 'linear combinations', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,"the expected value of linear combinations of random variables, there is no simple general rule. for example,"
775,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,except in very special circumstances.
776,1,"['functions', 'independence', 'probability', 'variance', 'distributions']", Means and Variances of Linear Combinations of Random Variables,seg_81,"the material provided by theorems 4.5 through 4.9 and the various corollaries is extremely useful in that there are no restrictions on the form of the density or probability functions, apart from the property of independence when it is required as in the corollaries following theorems 4.9. to illustrate, consider example 4.23; the variance of z = 3x−2y +5 does not require restrictions on the distributions of the amounts x and y of the two types of impurities. only independence between x and y is required. now, we do have at our disposal the capacity to find μg(x)"
777,1,['function'], Means and Variances of Linear Combinations of Random Variables,seg_81,and σg2(x) for any function g(·) from first principles established in theorems 4.1
778,1,"['density function', 'nonlinear', 'case', 'discrete', 'distribution', 'probability', 'function', 'probability function']", Means and Variances of Linear Combinations of Random Variables,seg_81,"and 4.3, where it is assumed that the corresponding distribution f(x) is known. exercises 4.40, 4.41, and 4.42, among others, illustrate the use of these theorems. thus, if the function g(x) is nonlinear and the density function (or probability function in the discrete case) is known, μg(x) and σg2(x) can be evaluated exactly."
779,1,"['functions', 'linear', 'random variables', 'combinations', 'variables', 'nonlinear', 'distribution', 'linear combinations', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,"but, similar to the rules given for linear combinations, are there rules for nonlinear functions that can be used when the form of the distribution of the pertinent random variables is not known?"
780,1,"['linear', 'approximation', 'random variable', 'variable', 'random', 'function']", Means and Variances of Linear Combinations of Random Variables,seg_81,"in general, suppose x is a random variable and y = g(x). the general solution for e(y ) or var(y ) can be difficult to find and depends on the complexity of the function g(·). however, there are approximations available that depend on a linear approximation of the function g(x). for example, suppose we denote e(x) as μ and var(x) = σx"
781,1,"['approximation', 'taylor series']", Means and Variances of Linear Combinations of Random Variables,seg_81,2 . then a taylor series approximation of g(x) around x = μx gives
782,1,"['linear', 'taylor series', 'approximation', 'expected value', 'cases']", Means and Variances of Linear Combinations of Random Variables,seg_81,"as a result, if we truncate after the linear term and take the expected value of both sides, we obtain e[g(x)] ≈ g(μx), which is certainly intuitive and in some cases gives a reasonable approximation. however, if we include the second-order term of the taylor series, then we have a second-order adjustment for this first-order approximation as follows:"
783,1,"['random variable', 'variable', 'mean', 'random', 'variance']", Means and Variances of Linear Combinations of Random Variables,seg_81,example 4.24: given the random variable x with mean μx and variance σx
784,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,"2 , give the second-order"
785,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,approximation to e(ex).
786,1,"['approximation', 'variance', 'taylor series']", Means and Variances of Linear Combinations of Random Variables,seg_81,"similarly, we can develop an approximation for var[g(x)] by taking the variance of both sides of the first-order taylor series expansion of g(x)."
787,1,"['random variable', 'variable', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,"example 4.25: given the random variable x as in example 4.24, give an approximate formula for var[g(x)]."
788,1,"['functions', 'nonlinear', 'random variable', 'variable', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,these approximations can be extended to nonlinear functions of more than one random variable.
789,1,"['random variables', 'independent', 'variables', 'independent random variables', 'set', 'random', 'variances']", Means and Variances of Linear Combinations of Random Variables,seg_81,"given a set of independent random variables x1, x2, . . . , xk with means μ1, μ2, . . . , μk and variances σ12, σ22, . . . , σk2, respectively, let"
790,1,"['function', 'nonlinear']", Means and Variances of Linear Combinations of Random Variables,seg_81,be a nonlinear function; then the following are approximations for e(y ) and var(y ):
791,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random', 'variances']", Means and Variances of Linear Combinations of Random Variables,seg_81,example 4.26: consider two independent random variables x and z with means μx and μz and variances σx
792,1,"['random variable', 'variable', 'random']", Means and Variances of Linear Combinations of Random Variables,seg_81,"2 and σz 2 , respectively. consider a random variable"
793,0,[], Means and Variances of Linear Combinations of Random Variables,seg_81,give approximations for e(y ) and var(y ).
794,1,"['approximation', 'variance']", Means and Variances of Linear Combinations of Random Variables,seg_81,and the approximation for the variance of y is given by
795,1,"['deviation', 'continuous distribution', 'random', 'interval', 'variability', 'observations', 'distribution', 'random variable', 'variable', 'mean', 'probability', 'standard', 'continuous', 'standard deviation', 'variance']", Chebyshevs Theorem,seg_83,"in section 4.2 we stated that the variance of a random variable tells us something about the variability of the observations about the mean. if a random variable has a small variance or standard deviation, we would expect most of the values to be grouped around the mean. therefore, the probability that the random variable assumes a value within a certain interval about the mean is greater than for a similar random variable with a larger standard deviation. if we think of probability in terms of area, we would expect a continuous distribution with a large value of σ to indicate a greater variability, and therefore we should expect the area to be more spread out, as in figure 4.2(a). a distribution with a small standard deviation should have most of its area close to μ, as in figure 4.2(b)."
796,1,"['variability', 'observations', 'mean', 'continuous']", Chebyshevs Theorem,seg_83,figure 4.2: variability of continuous observations about the mean.
797,1,"['variability', 'observations', 'discrete', 'mean']", Chebyshevs Theorem,seg_83,figure 4.3: variability of discrete observations about the mean.
798,1,"['histogram', 'outcomes', 'discrete', 'distribution', 'variable', 'measurements', 'discrete distribution']", Chebyshevs Theorem,seg_83,we can argue the same way for a discrete distribution. the area in the probability histogram in figure 4.3(b) is spread out much more than that in figure 4.3(a) indicating a more variable distribution of measurements or outcomes.
799,1,"['deviation', 'curve', 'histogram', 'random', 'symmetric', 'distribution', 'random variable', 'probability distribution', 'variable', 'mean', 'probability', 'standard', 'standard deviation']", Chebyshevs Theorem,seg_83,"the russian mathematician p. l. chebyshev (1821–1894) discovered that the fraction of the area between any two values symmetric about the mean is related to the standard deviation. since the area under a probability distribution curve or in a probability histogram adds to 1, the area between any two numbers is the probability of the random variable assuming a value between these numbers."
800,1,"['random', 'standard deviations', 'estimate', 'deviations', 'variable', 'random variable', 'mean', 'probability', 'standard']", Chebyshevs Theorem,seg_83,"the following theorem, due to chebyshev, gives a conservative estimate of the probability that a random variable assumes a value within k standard deviations of its mean for any real number k."
801,1,"['random variable', 'variable', 'probability', 'random']", Chebyshevs Theorem,seg_83,(chebyshev’s theorem) the probability that any random variable x will as-
802,1,"['standard deviations', 'deviations', 'mean', 'standard']", Chebyshevs Theorem,seg_83,"theorem 4.10: sume a value within k standard deviations of the mean is at least 1− 1/k2. that is,"
803,1,"['random', 'interval', 'standard deviations', 'states', 'observations', 'distribution', 'random variable', 'variable', 'deviations', 'mean', 'probability', 'standard']", Chebyshevs Theorem,seg_83,"for k = 2, the theorem states that the random variable x has a probability of at least 1−1/22 = 3/4 of falling within two standard deviations of the mean. that is, three-fourths or more of the observations of any distribution lie in the interval μ ± 2σ. similarly, the theorem says that at least eight-ninths of the observations of any distribution fall in the interval μ± 3σ."
804,1,"['distribution', 'random variable', 'probability distribution', 'variable', 'mean', 'probability', 'random', 'variance']", Chebyshevs Theorem,seg_83,"example 4.27: a random variable x has a mean μ = 8, a variance σ2 = 9, and an unknown probability distribution. find"
805,1,"['random', 'probabilities', 'standard deviations', 'observations', 'results', 'distribution', 'random variable', 'probability distribution', 'variable', 'deviations', 'mean', 'probability', 'standard', 'distributions']", Chebyshevs Theorem,seg_83,"1 = 1− p [8− (2)(3) < x < 8 + (2)(3)] ≤ . 4 chebyshev’s theorem holds for any distribution of observations, and for this reason the results are usually weak. the value given by the theorem is a lower bound only. that is, we know that the probability of a random variable falling within two standard deviations of the mean can be no less than 3/4, but we never know how much more it might actually be. only when the probability distribution is known can we determine exact probabilities. for this reason we call the theorem a distribution-free result. when specific distributions are assumed, as in future chapters, the results will be less conservative. the use of chebyshev’s theorem is relegated to situations where the form of the distribution is unknown."
806,1,"['deviation', 'random variables', 'random', 'parameters', 'variables', 'distribution', 'covariance', 'mean', 'standard', 'standard deviation', 'variance']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_89,"the material in this chapter is extremely fundamental in nature, much like that in chapter 3. whereas in chapter 3 we focused on general characteristics of a probability distribution, in this chapter we defined important quantities or parameters that characterize the general nature of the system. the mean of a distribution reflects central tendency, and the variance or standard deviation reflects variability in the system. in addition, covariance reflects the tendency for two random variables to “move together” in a system. these important parameters will remain fundamental to all that follows in this text."
807,1,"['experimental', 'estimated', 'estimation', 'case', 'data', 'distribution', 'mean', 'parameter']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_89,"the reader should understand that the distribution type is often dictated by the scientific scenario. however, the parameter values need to be estimated from scientific data. for example, in the case of review exercise 4.85, the manufacturer of the compressor may know (material that will be presented in chapter 6) from experience and knowledge of the type of compressor that the nature of the distribution is as indicated in the exercise. but the mean μ = 900 would be estimated from experimentation on the machine. though the parameter value of 900 is given as known here, it will not be known in real-life situations without the use of experimental data. chapter 9 is dedicated to estimation."
808,1,"['histogram', 'observations', 'discrete', 'probability', 'random', 'associated', 'probability distributions', 'random variable', 'statistical', 'distributions', 'distribution', 'random variables', 'variables', 'discrete random variables', 'variable', 'probability distribution', 'discrete probability distribution', 'experiments']", Introduction and Motivation,seg_93,"no matter whether a discrete probability distribution is represented graphically by a histogram, in tabular form, or by means of a formula, the behavior of a random variable is described. often, the observations generated by different statistical experiments have the same general type of behavior. consequently, discrete random variables associated with these experiments can be described by essentially the same probability distribution and therefore can be represented by a single formula. in fact, one needs only a handful of important probability distributions to describe many of the discrete random variables encountered in practice."
809,1,"['case', 'negative binomial', 'hypergeometric', 'random', 'binomial distribution', 'process', 'geometric', 'sample', 'data', 'random variable', 'samples', 'mean', 'statistical', 'quality control', 'distributions', 'geometric distribution', 'distribution', 'hypergeometric random variable', 'binomial', 'poisson distribution', 'control', 'observational data', 'poisson', 'negative binomial distribution', 'variable']", Introduction and Motivation,seg_93,"such a handful of distributions describe several real-life random phenomena. for instance, in a study involving testing the effectiveness of a new drug, the number of cured patients among all the patients who use the drug approximately follows a binomial distribution (section 5.2). in an industrial example, when a sample of items selected from a batch of production is tested, the number of defective items in the sample usually can be modeled as a hypergeometric random variable (section 5.3). in a statistical quality control problem, the experimenter will signal a shift of the process mean when observational data exceed certain limits. the number of samples required to produce a false alarm follows a geometric distribution which is a special case of the negative binomial distribution (section 5.4). on the other hand, the number of white cells from a fixed amount of an individual’s blood sample is usually random and may be described by a poisson distribution (section 5.5). in this chapter, we present these commonly used distributions with various examples."
810,1,"['outcomes', 'experiment', 'failure', 'trials', 'success']", Binomial and Multinomial Distributions,seg_95,"an experiment often consists of repeated trials, each with two possible outcomes that may be labeled success or failure. the most obvious application deals with"
811,1,"['process', 'probabilities', 'conditional probability', 'conditional', 'bernoulli', 'trials', 'set', 'success', 'bernoulli trials', 'probability', 'bernoulli trial', 'outcome', 'trial']", Binomial and Multinomial Distributions,seg_95,"the testing of items as they come off an assembly line, where each trial may indicate a defective or a nondefective item. we may choose to define either outcome as a success. the process is referred to as a bernoulli process. each trial is called a bernoulli trial. observe, for example, if one were drawing cards from a deck, the probabilities for repeated trials change if the cards are not replaced. that is, the probability of selecting a heart on the first draw is 1/4, but on the second draw it is a conditional probability having a value of 13/51 or 12/51, depending on whether a heart appeared on the first draw: this, then, would no longer be considered a set of bernoulli trials."
812,1,"['bernoulli', 'process']", Binomial and Multinomial Distributions,seg_95,"strictly speaking, the bernoulli process must possess the following properties:"
813,1,"['trials', 'experiment']", Binomial and Multinomial Distributions,seg_95,1. the experiment consists of repeated trials.
814,1,"['results', 'failure', 'success', 'outcome', 'trial']", Binomial and Multinomial Distributions,seg_95,2. each trial results in an outcome that may be classified as a success or a failure.
815,1,"['success', 'probability of success', 'trial', 'probability']", Binomial and Multinomial Distributions,seg_95,"3. the probability of success, denoted by p, remains constant from trial to trial."
816,1,"['trials', 'independent']", Binomial and Multinomial Distributions,seg_95,4. the repeated trials are independent.
817,1,"['outcomes', 'successes', 'bernoulli', 'variable', 'random variable', 'set', 'bernoulli trials', 'success', 'random', 'trials', 'process']", Binomial and Multinomial Distributions,seg_95,"consider the set of bernoulli trials where three items are selected at random from a manufacturing process, inspected, and classified as defective or nondefective. a defective item is designated a success. the number of successes is a random variable x assuming integral values from 0 through 3. the eight possible outcomes and the corresponding values of x are"
818,0,[], Binomial and Multinomial Distributions,seg_95,outcome nnn ndn nnd dnn ndd dnd ddn ddd
819,1,['process'], Binomial and Multinomial Distributions,seg_95,"since the items are selected independently and we assume that the process produces 25% defectives, we have"
820,1,"['probabilities', 'distribution', 'probability distribution', 'probability', 'outcomes']", Binomial and Multinomial Distributions,seg_95,similar calculations yield the probabilities for the other possible outcomes. the probability distribution of x is therefore
821,1,"['discrete', 'bernoulli trials', 'probability', 'random', 'binomial distribution', 'successes', 'trials', 'random variable', 'success', 'binomial random variable', 'probability of a success', 'trial', 'distribution', 'binomial', 'discrete random variable', 'bernoulli', 'variable', 'probability distribution']", Binomial and Multinomial Distributions,seg_95,"the number x of successes in n bernoulli trials is called a binomial random variable. the probability distribution of this discrete random variable is called the binomial distribution, and its values will be denoted by b(x;n, p) since they depend on the number of trials and the probability of a success on a given trial. thus, for the probability distribution of x, the number of defectives is"
822,1,"['probability', 'sample', 'experiment', 'partitions', 'successes', 'trials', 'success', 'binomial experiment', 'failure', 'binomial', 'outcomes', 'independent', 'probabilities', 'failures']", Binomial and Multinomial Distributions,seg_95,"let us now generalize the above illustration to yield a formula for b(x;n, p). that is, we wish to find a formula that gives the probability of x successes in n trials for a binomial experiment. first, consider the probability of x successes and n − x failures in a specified order. since the trials are independent, we can multiply all the probabilities corresponding to the different outcomes. each success occurs with probability p and each failure with probability q = 1 − p. therefore, the probability for the specified order is pxqn−x. we must now determine the total number of sample points in the experiment that have x successes and n−x failures. this number is equal to the number of partitions of n outcomes into two groups with x in one group and n−x in the other and is written (n"
823,1,"['partitions', 'mutually exclusive', 'probabilities']", Binomial and Multinomial Distributions,seg_95,"x) as introduced in section 2.3. because these partitions are mutually exclusive, we add the probabilities of all the different partitions to obtain the general formula, or simply multiply pxqn−x"
824,1,"['probability', 'random', 'successes', 'trials', 'random variable', 'success', 'binomial random variable', 'distribution probability', 'bernoulli trial', 'trial', 'failure', 'distribution', 'binomial', 'independent', 'bernoulli', 'variable', 'probability distribution']", Binomial and Multinomial Distributions,seg_95,"binomial a bernoulli trial can result in a success with probability p and a failure with distribution probability q = 1−p. then the probability distribution of the binomial random variable x, the number of successes in n independent trials, is"
825,1,"['distribution', 'probability distribution', 'probability']", Binomial and Multinomial Distributions,seg_95,"note that when n = 3 and p = 1/4, the probability distribution of x, the number of defectives, may be written as"
826,0,[], Binomial and Multinomial Distributions,seg_95,rather than in the tabular form on page 144.
827,1,"['test', 'probability']", Binomial and Multinomial Distributions,seg_95,example 5.1: the probability that a certain kind of component will survive a shock test is 3/4. find the probability that exactly 2 of the next 4 components tested survive.
828,1,"['tests', 'independent']", Binomial and Multinomial Distributions,seg_95,"solution : assuming that the tests are independent and p = 3/4 for each of the 4 tests, we obtain"
829,1,"['distribution', 'binomial distribution', 'binomial']", Binomial and Multinomial Distributions,seg_95,"the binomial distribution derives its name from the fact that the n + 1 terms in the binomial expansion of (q+p)n correspond to the various values of b(x;n, p) for x = 0, 1, 2, . . . , n. that is,"
830,1,"['distribution', 'probability distribution', 'probability', 'condition']", Binomial and Multinomial Distributions,seg_95,a condition that must hold for any probability distribution.
831,1,['binomial'], Binomial and Multinomial Distributions,seg_95,"frequently, we are interested in problems where it is necessary to find p (x < r) or p (a ≤ x ≤ b). binomial sums"
832,1,['table'], Binomial and Multinomial Distributions,seg_95,"are given in table a.1 of the appendix for n = 1, 2, . . . , 20 for selected values of p from 0.1 to 0.9. we illustrate the use of table a.1 with the following example."
833,1,['probability'], Binomial and Multinomial Distributions,seg_95,"example 5.2: the probability that a patient recovers from a rare blood disease is 0.4. if 15 people are known to have contracted this disease, what is the probability that (a) at least 10 survive, (b) from 3 to 8 survive, and (c) exactly 5 survive?"
834,0,[], Binomial and Multinomial Distributions,seg_95,solution : let x be the number of people who survive.
835,1,['rate'], Binomial and Multinomial Distributions,seg_95,example 5.3: a large chain retailer purchases a certain kind of electronic device from a manufacturer. the manufacturer indicates that the defective rate of the device is 3%.
836,0,[], Binomial and Multinomial Distributions,seg_95,(a) the inspector randomly picks 20 items from a shipment. what is the proba-
837,0,[], Binomial and Multinomial Distributions,seg_95,bility that there will be at least one defective item among these 20?
838,0,[], Binomial and Multinomial Distributions,seg_95,(b) suppose that the retailer receives 10 shipments in a month and the inspector
839,1,"['tests', 'probability']", Binomial and Multinomial Distributions,seg_95,randomly tests 20 devices per shipment. what is the probability that there will be exactly 3 shipments each containing at least one defective device among the 20 that are selected and tested from the shipment?
840,1,['distribution'], Binomial and Multinomial Distributions,seg_95,"solution : (a) denote by x the number of defective devices among the 20. then x follows a b(x; 20, 0.03) distribution. hence,"
841,1,['case'], Binomial and Multinomial Distributions,seg_95,"(b) in this case, each shipment can either contain at least one defective item or"
842,1,"['bernoulli', 'independence', 'bernoulli trial', 'trial']", Binomial and Multinomial Distributions,seg_95,"not. hence, testing of each shipment can be viewed as a bernoulli trial with p = 0.4562 from part (a). assuming independence from shipment to shipment"
843,1,"['distribution', 'binomial distribution', 'binomial']", Binomial and Multinomial Distributions,seg_95,"and denoting by y the number of shipments containing at least one defective item, y follows another binomial distribution b(y; 10, 0.4562). therefore,"
844,1,"['probability', 'binomial distribution', 'process', 'results', 'success', 'trial', 'quality control', 'failure', 'distribution', 'binomial', 'outcome', 'control', 'processes', 'independent', 'sampling', 'probability of success']", Binomial and Multinomial Distributions,seg_95,"from examples 5.1 through 5.3, it should be clear that the binomial distribution finds applications in many scientific fields. an industrial engineer is keenly interested in the “proportion defective” in an industrial process. often, quality control measures and sampling schemes for processes are based on the binomial distribution. this distribution applies to any industrial situation where an outcome of a process is dichotomous and the results of the process are independent, with the probability of success being constant from trial to trial. the binomial distribution is also used extensively for medical and military applications. in both fields, a success or failure result is important. for example, “cure” or “no cure” is important in pharmaceutical work, and “hit” or “miss” is often the interpretation of the result of firing a guided missile."
845,1,"['functions', 'parameters', 'distribution', 'random variable', 'probability distribution', 'variable', 'binomial random variable', 'mean', 'binomial', 'random', 'probability', 'variance']", Binomial and Multinomial Distributions,seg_95,"since the probability distribution of any binomial random variable depends only on the values assumed by the parameters n, p, and q, it would seem reasonable to assume that the mean and variance of a binomial random variable also depend on the values assumed by these parameters. indeed, this is true, and in the proof of theorem 5.1 we derive general formulas that can be used to compute the mean and variance of any binomial random variable as functions of n, p, and q."
846,1,"['distribution', 'binomial', 'mean', 'binomial distribution', 'variance']", Binomial and Multinomial Distributions,seg_95,"theorem 5.1: the mean and variance of the binomial distribution b(x;n, p) are μ = np and σ2 = npq."
847,1,"['random', 'experiment', 'successes', 'random variable', 'indicator', 'trial', 'bernoulli random variable', 'binomial experiment', 'binomial', 'indicator variables', 'outcome', 'independent', 'probabilities', 'variables', 'bernoulli', 'variable']", Binomial and Multinomial Distributions,seg_95,"proof : let the outcome on the jth trial be represented by a bernoulli random variable ij , which assumes the values 0 and 1 with probabilities q and p, respectively. therefore, in a binomial experiment the number of successes can be written as the sum of the n independent indicator variables. hence,"
848,1,"['distribution', 'binomial', 'mean', 'binomial distribution']", Binomial and Multinomial Distributions,seg_95,"the mean of any ij is e(ij) = (0)(q) + (1)(p) = p. therefore, using corollary 4.4 on page 131, the mean of the binomial distribution is"
849,1,['variance'], Binomial and Multinomial Distributions,seg_95,the variance of any ij is σi2
850,1,"['independent', 'variables', 'case', 'distribution', 'bernoulli', 'binomial', 'binomial distribution', 'variance']", Binomial and Multinomial Distributions,seg_95,j = e(ij2)−p2 = (0)2(q)+(1)2(p)−p2 = p(1−p) = pq. extending corollary 4.11 to the case of n independent bernoulli variables gives the variance of the binomial distribution as
851,1,['test'], Binomial and Multinomial Distributions,seg_95,"example 5.4: it is conjectured that an impurity exists in 30% of all drinking wells in a certain rural community. in order to gain some insight into the true extent of the problem, it is determined that some testing is necessary. it is too expensive to test all of the wells in the area, so 10 are randomly selected for testing."
852,1,"['distribution', 'binomial', 'probability', 'binomial distribution']", Binomial and Multinomial Distributions,seg_95,"(a) using the binomial distribution, what is the probability that exactly 3 wells"
853,0,[], Binomial and Multinomial Distributions,seg_95,"have the impurity, assuming that the conjecture is correct?"
854,1,['probability'], Binomial and Multinomial Distributions,seg_95,(b) what is the probability that more than 3 wells are impure?
855,1,['case'], Binomial and Multinomial Distributions,seg_95,"(b) in this case, p (x > 3) = 1− 0.6496 = 0.3504."
856,1,"['interval', 'random variable', 'variable', 'binomial random variable', 'mean', 'binomial', 'random', 'variance']", Binomial and Multinomial Distributions,seg_95,"example 5.5: find the mean and variance of the binomial random variable of example 5.2, and then use chebyshev’s theorem (on page 137) to interpret the interval μ± 2σ."
857,1,"['binomial experiment', 'experiment', 'binomial']", Binomial and Multinomial Distributions,seg_95,"solution : since example 5.2 was a binomial experiment with n = 15 and p = 0.4, by theorem 5.1, we have"
858,1,"['interval', 'states', 'discrete', 'data', 'probability']", Binomial and Multinomial Distributions,seg_95,"taking the square root of 3.6, we find that σ = 1.897. hence, the required interval is 6±(2)(1.897), or from 2.206 to 9.794. chebyshev’s theorem states that the number of recoveries among 15 patients who contracted the disease has a probability of at least 3/4 of falling between 2.206 and 9.794 or, because the data are discrete, between 2 and 10 inclusive."
859,1,"['probabilities', 'data', 'binomial', 'population']", Binomial and Multinomial Distributions,seg_95,there are solutions in which the computation of binomial probabilities may allow us to draw a scientific inference about population after data are collected. an illustration is given in the next example.
860,1,['probability'], Binomial and Multinomial Distributions,seg_95,example 5.6: consider the situation of example 5.4. the notion that 30% of the wells are impure is merely a conjecture put forth by the area water board. suppose 10 wells are randomly selected and 6 are found to contain the impurity. what does this imply about the conjecture? use a probability statement.
861,0,[], Binomial and Multinomial Distributions,seg_95,"solution : we must first ask: “if the conjecture is correct, is it likely that we would find 6 or more impure wells?”"
862,0,[], Binomial and Multinomial Distributions,seg_95,"as a result, it is very unlikely (4.7% chance) that 6 or more wells would be found impure if only 30% of all are impure. this casts considerable doubt on the conjecture and suggests that the impurity problem is much more severe."
863,1,"['categories', 'outcomes']", Binomial and Multinomial Distributions,seg_95,"as the reader should realize by now, in many applications there are more than two possible outcomes. to borrow an example from the field of genetics, the color of guinea pigs produced as offspring may be red, black, or white. often the “defective” or “not defective” dichotomy is truly an oversimplification in engineering situations. indeed, there are often more than two categories that characterize items or parts coming off an assembly line."
864,1,"['binomial experiment', 'outcomes', 'multinomial', 'experiment', 'with replacement', 'intersection', 'replacement', 'binomial', 'multinomial experiment', 'trial']", Binomial and Multinomial Distributions,seg_95,"the binomial experiment becomes a multinomial experiment if we let each trial have more than two possible outcomes. the classification of a manufactured product as being light, heavy, or acceptable and the recording of accidents at a certain intersection according to the day of the week constitute multinomial experiments. the drawing of a card from a deck with replacement is also a multinomial experiment if the 4 suits are the outcomes of interest."
865,1,"['outcomes', 'independent', 'probabilities', 'multinomial', 'multinomial distribution', 'distribution', 'trials', 'probability', 'trial']", Binomial and Multinomial Distributions,seg_95,"in general, if a given trial can result in any one of k possible outcomes e1, e2, . . . , ek with probabilities p1, p2, . . . , pk, then the multinomial distribution will give the probability that e1 occurs x1 times, e2 occurs x2 times, . . . , and ek occurs xk times in n independent trials, where"
866,1,"['joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'joint probability']", Binomial and Multinomial Distributions,seg_95,we shall denote this joint probability distribution by
867,1,"['outcomes', 'trial']", Binomial and Multinomial Distributions,seg_95,"clearly, p1 + p2 + · · · + pk = 1, since the result of each trial must be one of the k possible outcomes."
868,1,"['outcomes', 'independent', 'case', 'trials', 'binomial', 'probability']", Binomial and Multinomial Distributions,seg_95,"to derive the general formula, we proceed as in the binomial case. since the trials are independent, any specified order yielding x1 outcomes for e1, x2 for e2, . . . , xk for ek will occur with probability px"
869,1,"['partitions', 'trials', 'outcomes']", Binomial and Multinomial Distributions,seg_95,"22 · · · px kk . the total number of orders yielding similar outcomes for the n trials is equal to the number of partitions of n items into k groups with x1 in the first group, x2 in the second group, . . . , and xk in the kth group. this can be done in"
870,1,"['mutually exclusive', 'multinomial', 'partitions', 'multinomial distribution', 'distribution', 'probability']", Binomial and Multinomial Distributions,seg_95,"ways. since all the partitions are mutually exclusive and occur with equal probability, we obtain the multinomial distribution by multiplying the probability for a specified order by the total number of partitions."
871,1,"['outcomes', 'random variables', 'probabilities', 'variables', 'distribution', 'probability distribution', 'trials', 'probability', 'random', 'trial']", Binomial and Multinomial Distributions,seg_95,"multinomial if a given trial can result in the k outcomes e1, e2, . . . , ek with probabilities distribution p1, p2, . . . , pk, then the probability distribution of the random variables x1, x2, . . . , xk, representing the number of occurrences for e1, e2, . . . , ek in n independent trials, is"
872,1,"['distribution', 'multinomial distribution', 'multinomial']", Binomial and Multinomial Distributions,seg_95,"the multinomial distribution derives its name from the fact that the terms of the multinomial expansion of (p1 + p2 + · · · + pk)n correspond to all the possible values of f(x1, x2, . . . , xk; p1, p2, . . . , pk, n)."
873,1,"['probability distributions', 'discrete', 'probability', 'discrete probability distributions', 'distributions']", Binomial and Multinomial Distributions,seg_95,150 chapter 5 some discrete probability distributions
874,1,"['model', 'probabilities', 'simulation']", Binomial and Multinomial Distributions,seg_95,"example 5.7: the complexity of arrivals and departures of planes at an airport is such that computer simulation is often used to model the “ideal” conditions. for a certain airport with three runways, it is known that in the ideal setting the following are the probabilities that the individual runways are accessed by a randomly arriving commercial jet:"
875,1,['probability'], Binomial and Multinomial Distributions,seg_95,"runway 1: p1 = 2/9, runway 2: p2 = 1/6, runway 3: p3 = 11/18. what is the probability that 6 randomly arriving airplanes are distributed in the following fashion?"
876,1,"['distribution', 'multinomial distribution', 'multinomial']", Binomial and Multinomial Distributions,seg_95,"runway 1: 2 airplanes, runway 2: 1 airplane, runway 3: 3 airplanes solution : using the multinomial distribution, we have"
877,1,"['observations', 'without replacement', 'case', 'independence', 'hypergeometric', 'binomial distribution', 'replacement', 'trials', 'with replacement', 'distribution', 'binomial', 'hypergeometric distribution', 'probabilities', 'sampling', 'number of observations']", Hypergeometric Distribution,seg_99,"the simplest way to view the distinction between the binomial distribution of section 5.2 and the hypergeometric distribution is to note the way the sampling is done. the types of applications for the hypergeometric are very similar to those for the binomial distribution. we are interested in computing probabilities for the number of observations that fall into a particular category. but in the case of the binomial distribution, independence among trials is required. as a result, if that distribution is applied to, say, sampling from a lot of items (deck of cards, batch of production items), the sampling must be done with replacement of each item after it is observed. on the other hand, the hypergeometric distribution does not require independence and is based on sampling done without replacement."
878,1,"['sample', 'without replacement', 'replacement', 'sampling', 'distribution', 'hypergeometric', 'hypergeometric distribution', 'acceptance sampling']", Hypergeometric Distribution,seg_99,"applications for the hypergeometric distribution are found in many areas, with heavy use in acceptance sampling, electronic testing, and quality assurance. obviously, in many of these fields, testing is done at the expense of the item being tested. that is, the item is destroyed and hence cannot be replaced in the sample. thus, sampling without replacement is necessary. a simple example with playing"
879,0,[], Hypergeometric Distribution,seg_99,cards will serve as our first illustration.
880,1,"['without replacement', 'replacement', 'sampling', 'distribution', 'binomial', 'probability', 'random', 'binomial distribution']", Hypergeometric Distribution,seg_99,"if we wish to find the probability of observing 3 red cards in 5 draws from an ordinary deck of 52 playing cards, the binomial distribution of section 5.2 does not apply unless each card is replaced and the deck reshuffled before the next draw is made. to solve the problem of sampling without replacement, let us restate the problem. if 5 cards are drawn at random, we are interested in the probability of selecting 3 red cards from the 26 available in the deck and 2 black cards from the 26 available in the deck. there are (2"
881,0,[], Hypergeometric Distribution,seg_99,"6) ways of selecting 3 red cards, and for each of"
882,0,[], Hypergeometric Distribution,seg_99,these ways we can choose 2 black cards in (2
883,0,[], Hypergeometric Distribution,seg_99,"6) ways. therefore, the total number"
884,0,[], Hypergeometric Distribution,seg_99,of ways to select 3 red and 2 black cards in 5 draws is the product (2
885,0,[], Hypergeometric Distribution,seg_99,total number of ways to select any 5 cards from the 52 that are available is (5
886,1,"['probability', 'without replacement', 'replacement']", Hypergeometric Distribution,seg_99,"hence, the probability of selecting 5 cards without replacement of which 3 are red and 2 are black is given by"
887,1,"['sample', 'experiment', 'failures', 'random sample', 'successes', 'hypergeometric', 'random', 'probability']", Hypergeometric Distribution,seg_99,"in general, we are interested in the probability of selecting x successes from the k items labeled successes and n − x failures from the n − k items labeled failures when a random sample of size n is selected from n items. this is known as a hypergeometric experiment, that is, one that possesses the following two properties:"
888,1,"['sample', 'random', 'without replacement', 'replacement', 'random sample']", Hypergeometric Distribution,seg_99,1. a random sample of size n is selected without replacement from n items.
889,1,['successes'], Hypergeometric Distribution,seg_99,"2. of the n items, k may be classified as successes and n − k are classified as"
890,1,"['experiment', 'successes', 'distribution', 'random variable', 'probability distribution', 'variable', 'set', 'hypergeometric', 'random', 'probability', 'hypergeometric distribution']", Hypergeometric Distribution,seg_99,"the number x of successes of a hypergeometric experiment is called a hypergeometric random variable. accordingly, the probability distribution of the hypergeometric variable is called the hypergeometric distribution, and its values are denoted by h(x;n,n, k), since they depend on the number of successes k in the set n from which we select n items."
891,1,"['sampling', 'distribution', 'binomial', 'hypergeometric', 'binomial distribution', 'hypergeometric distribution', 'acceptance sampling']", Hypergeometric Distribution,seg_99,"like the binomial distribution, the hypergeometric distribution finds applications in acceptance sampling, where lots of materials or parts are sampled in order to determine whether or not the entire lot is accepted."
892,1,"['utility', 'sampling', 'random', 'random sampling']", Hypergeometric Distribution,seg_99,"example 5.8: a particular part that is used as an injection device is sold in lots of 10. the producer deems a lot acceptable if no more than one defective is in the lot. a sampling plan involves random sampling and testing 3 of the parts out of 10. if none of the 3 is defective, the lot is accepted. comment on the utility of this plan."
893,1,"['sampling', 'probability']", Hypergeometric Distribution,seg_99,"solution : let us assume that the lot is truly unacceptable (i.e., that 2 out of 10 parts are defective). the probability that the sampling plan finds the lot acceptable is"
894,1,['sampling'], Hypergeometric Distribution,seg_99,"thus, if the lot is truly unacceptable, with 2 defective parts, this sampling plan will allow acceptance roughly 47% of the time. as a result, this plan should be considered faulty."
895,1,['samples'], Hypergeometric Distribution,seg_99,"let us now generalize in order to find a formula for h(x;n,n, k). the total number of samples of size n chosen from n items is (n"
896,1,['samples'], Hypergeometric Distribution,seg_99,n). these samples are
897,0,[], Hypergeometric Distribution,seg_99,assumed to be equally likely. there are (
898,1,['successes'], Hypergeometric Distribution,seg_99,k) ways of selecting x successes from the
899,1,['failures'], Hypergeometric Distribution,seg_99,"k that are available, and for each of these ways we can choose the n− x failures in"
900,1,['samples'], Hypergeometric Distribution,seg_99,"k) ways. thus, the total number of favorable samples among the (n n) possible"
901,0,[], Hypergeometric Distribution,seg_99,samples is given by (x
902,0,[], Hypergeometric Distribution,seg_99,"k). hence, we have the following definition."
903,1,"['sample', 'random sample', 'failure', 'successes', 'distribution', 'probability distribution', 'success', 'hypergeometric', 'random', 'probability']", Hypergeometric Distribution,seg_99,"hypergeometric the probability distribution of the hypergeometric random variablex, the numdistribution ber of successes in a random sample of size n selected from n items of which k are labeled success and n − k labeled failure, is"
904,1,"['range', 'hypergeometric', 'random', 'coefficients', 'sample', 'successes', 'random variable', 'sample size', 'binomial coefficients', 'hypergeometric random variable', 'binomial', 'failures', 'variable']", Hypergeometric Distribution,seg_99,"the range of x can be determined by the three binomial coefficients in the definition, where x and n−x are no more than k and n−k, respectively, and both of them cannot be less than 0. usually, when both k (the number of successes) and n − k (the number of failures) are larger than the sample size n, the range of a hypergeometric random variable will be x = 0, 1, . . . , n."
905,1,"['sample', 'sampling', 'probability', 'random']", Hypergeometric Distribution,seg_99,example 5.9: lots of 40 components each are deemed unacceptable if they contain 3 or more defectives. the procedure for sampling a lot is to select 5 components at random and to reject the lot if a defective is found. what is the probability that exactly 1 defective is found in the sample if there are 3 defectives in the entire lot?
906,1,"['distribution', 'probability', 'hypergeometric distribution', 'hypergeometric']", Hypergeometric Distribution,seg_99,"solution : using the hypergeometric distribution with n = 5, n = 40, k = 3, and x = 1, we find the probability of obtaining 1 defective to be"
907,0,[], Hypergeometric Distribution,seg_99,"once again, this plan is not desirable since it detects a bad lot (3 defectives) only about 30% of the time."
908,1,"['distribution', 'mean', 'variance', 'hypergeometric', 'hypergeometric distribution']", Hypergeometric Distribution,seg_99,"theorem 5.2: the mean and variance of the hypergeometric distribution h(x;n,n, k) are"
909,1,['mean'], Hypergeometric Distribution,seg_99,the proof for the mean is shown in appendix a.24.
910,1,"['sample', 'variable', 'random variable', 'probability', 'random', 'sample space']", Hypergeometric Distribution,seg_99,"example 5.10: let us now reinvestigate example 3.4 on page 83. the purpose of this example was to illustrate the notion of a random variable and the corresponding sample space. in the example, we have a lot of 100 items of which 12 are defective. what is the probability that in a sample of 10, 3 are defective?"
911,1,"['probability', 'hypergeometric', 'function', 'probability function']", Hypergeometric Distribution,seg_99,"solution : using the hypergeometric probability function, we have"
912,1,"['interval', 'random variable', 'variable', 'mean', 'random', 'variance']", Hypergeometric Distribution,seg_99,example 5.11: find the mean and variance of the random variable of example 5.9 and then use chebyshev’s theorem to interpret the interval μ± 2σ.
913,1,"['hypergeometric', 'experiment']", Hypergeometric Distribution,seg_99,"solution : since example 5.9 was a hypergeometric experiment with n = 40, n = 5, and k = 3, by theorem 5.2, we have"
914,1,"['states', 'interval', 'probability', 'random']", Hypergeometric Distribution,seg_99,"taking the square root of 0.3113, we find that σ = 0.558. hence, the required interval is 0.375 ± (2)(0.558), or from −0.741 to 1.491. chebyshev’s theorem states that the number of defectives obtained when 5 components are selected at random from a lot of 40 components of which 3 are defective has a probability of at least 3/4 of falling between −0.741 and 1.491. that is, at least three-fourths of the time, the 5 components include fewer than 2 defectives."
915,1,"['approximation', 'discrete', 'discrete distributions', 'distribution', 'binomial', 'hypergeometric', 'binomial distribution', 'hypergeometric distribution', 'distributions']", Hypergeometric Distribution,seg_99,"in this chapter, we discuss several important discrete distributions that have wide applicability. many of these distributions relate nicely to each other. the beginning student should gain a clear understanding of these relationships. there is an interesting relationship between the hypergeometric and the binomial distribution. as one might expect, if n is small compared ton , the nature of then items changes very little in each draw. so a binomial distribution can be used to approximate the hypergeometric distribution when n is small compared to n . in fact, as a rule of thumb, the approximation is good when n/n ≤ 0.05."
916,1,"['distribution', 'mean', 'variance', 'hypergeometric', 'binomial', 'binomial distribution', 'parameter', 'hypergeometric distribution']", Hypergeometric Distribution,seg_99,"thus, the quantity k/n plays the role of the binomial parameter p. as a result, the binomial distribution may be viewed as a large-population version of the hypergeometric distribution. the mean and variance then come from the formulas"
917,1,"['factor', 'variance', 'mean', 'correction factor']", Hypergeometric Distribution,seg_99,"comparing these formulas with those of theorem 5.2, we see that the mean is the same but the variance differs by a correction factor of (n − n)/(n − 1), which is negligible when n is small relative to n ."
918,1,"['probability', 'random']", Hypergeometric Distribution,seg_99,"example 5.12: a manufacturer of automobile tires reports that among a shipment of 5000 sent to a local distributor, 1000 are slightly blemished. if one purchases 10 of these tires at random from the distributor, what is the probability that exactly 3 are blemished?"
919,1,"['sample size', 'sample', 'distribution', 'binomial', 'probability', 'binomial distribution']", Hypergeometric Distribution,seg_99,"solution : since n = 5000 is large relative to the sample size n = 10, we shall approximate the desired probability by using the binomial distribution. the probability of obtaining a blemished tire is 0.2. therefore, the probability of obtaining exactly 3 blemished tires is"
920,1,['probability'], Hypergeometric Distribution,seg_99,"on the other hand, the exact probability is h(3; 5000, 10, 1000) = 0.2015."
921,1,"['sample', 'random sample', 'case', 'distribution', 'hypergeometric', 'random', 'probability', 'hypergeometric distribution']", Hypergeometric Distribution,seg_99,"the hypergeometric distribution can be extended to treat the case where the n items can be partitioned into k cells a1, a2, . . . , ak with a1 elements in the first cell, a2 elements in the second cell, . . . , ak elements in the kth cell. we are now interested in the probability that a random sample of size n yields x1 elements from a1, x2 elements from a2, . . . , and xk elements from ak. let us represent this probability by"
922,1,['samples'], Hypergeometric Distribution,seg_99,"to obtain a general formula, we note that the total number of samples of size n that can be chosen from n items is still (n"
923,0,[], Hypergeometric Distribution,seg_99,"a1 1) ways of selecting x1 items from the items in a1, and for each of these we can choose x2 items from"
924,0,[], Hypergeometric Distribution,seg_99,"a2 2) ways. therefore, we can select x1 items from a1 and x2"
925,0,['n'], Hypergeometric Distribution,seg_99,"a2 2) ways. continuing in this way, we can select all n items consisting of x1 from a1, x2 from a2, . . . , and xk from ak in"
926,1,"['distribution', 'probability distribution', 'probability']", Hypergeometric Distribution,seg_99,the required probability distribution is now defined as follows.
927,1,"['sample', 'random sample', 'distribution', 'probability distribution', 'hypergeometric', 'random', 'probability']", Hypergeometric Distribution,seg_99,"multivariate if n items can be partitioned into the k cells a1, a2, . . . , ak with a1, a2, . . . , ak hypergeometric elements, respectively, then the probability distribution of the random varidistribution ables x1, x2, . . . , xk, representing the number of elements selected from a1, a2, . . . , ak in a random sample of size n, is"
928,1,"['sample', 'random', 'case', 'probability', 'random sample']", Hypergeometric Distribution,seg_99,"example 5.13: a group of 10 individuals is used for a biological case study. the group contains 3 people with blood type o, 4 with blood type a, and 3 with blood type b. what is the probability that a random sample of 5 will contain 1 person with blood type o, 2 people with blood type a, and 2 people with blood type b?"
929,1,"['distribution', 'probability', 'hypergeometric distribution', 'hypergeometric']", Hypergeometric Distribution,seg_99,"solution : using the extension of the hypergeometric distribution with x1 = 1, x2 = 2, x3 = 2, a1 = 3, a2 = 4, a3 = 3, n = 10, and n = 5, we find that the desired probability is"
930,1,"['binomial experiment', 'experiment', 'binomial experiments', 'successes', 'negative binomial experiments', 'negative binomial', 'success', 'trials', 'binomial', 'probability', 'experiments', 'trial']", Negative Binomial and Geometric Distributions,seg_103,"let us consider an experiment where the properties are the same as those listed for a binomial experiment, with the exception that the trials will be repeated until a fixed number of successes occur. therefore, instead of the probability of x successes in n trials, where n is fixed, we are now interested in the probability that the kth success occurs on the xth trial. experiments of this kind are called negative binomial experiments."
931,1,"['failure', 'cases', 'success', 'probability']", Negative Binomial and Geometric Distributions,seg_103,"as an illustration, consider the use of a drug that is known to be effective in 60% of the cases where it is used. the drug will be considered a success if it is effective in bringing some degree of relief to the patient. we are interested in finding the probability that the fifth patient to experience relief is the seventh patient to receive the drug during a given week. designating a success by s and a failure by f , a possible order of achieving the desired result is sfsssfs, which occurs with probability"
932,1,"['failures', 'partitions', 'successes', 'trials', 'success', 'outcome']", Negative Binomial and Geometric Distributions,seg_103,"we could list all possible orders by rearranging the f ’s and s’s except for the last outcome, which must be the fifth success. the total number of possible orders is equal to the number of partitions of the first six trials into two groups with 2 failures assigned to the one group and 4 successes assigned to the other group."
933,1,"['outcome', 'success', 'mutually exclusive']", Negative Binomial and Geometric Distributions,seg_103,"6) = 15 mutually exclusive ways. hence, if x represents the outcome on which the fifth success occurs, then"
934,1,"['negative binomial experiment', 'binomial experiment', 'experiment', 'negative binomial random variable', 'successes', 'trials', 'negative binomial', 'variable', 'random variable', 'binomial random variable', 'binomial', 'probability', 'random']", Negative Binomial and Geometric Distributions,seg_103,"the number x of trials required to produce k successes in a negative binomial experiment is called a negative binomial random variable, and its probability"
935,1,"['independent', 'probabilities', 'failures', 'successes', 'failure', 'negative binomial distribution', 'trial', 'distribution', 'negative binomial', 'success', 'trials', 'binomial', 'probability', 'binomial distribution', 'outcome', 'probability of a success']", Negative Binomial and Geometric Distributions,seg_103,"distribution is called the negative binomial distribution. since its probabilities depend on the number of successes desired and the probability of a success on a given trial, we shall denote them by b∗(x; k, p). to obtain the general formula for b∗(x; k, p), consider the probability of a success on the xth trial preceded by k − 1 successes and x − k failures in some specified order. since the trials are independent, we can multiply all the probabilities corresponding to each desired outcome. each success occurs with probability p and each failure with probability q = 1− p. therefore, the probability for the specified order ending in success is"
936,1,"['sample', 'failures', 'experiment', 'partitions', 'successes', 'trials', 'success']", Negative Binomial and Geometric Distributions,seg_103,"the total number of sample points in the experiment ending in a success, after the occurrence of k−1 successes and x−k failures in any order, is equal to the number of partitions of x−1 trials into two groups with k−1 successes corresponding to one group and x−k failures corresponding to the other group. this number is specified"
937,1,"['mutually exclusive', 'probability']", Negative Binomial and Geometric Distributions,seg_103,"1), each mutually exclusive and occurring with equal probability"
938,0,[], Negative Binomial and Geometric Distributions,seg_103,pkqx−k. we obtain the general formula by multiplying pkqx−k by (x
939,1,"['independent', 'failure', 'trial', 'distribution', 'random variable', 'probability distribution', 'variable', 'success', 'binomial', 'probability', 'random', 'trials']", Negative Binomial and Geometric Distributions,seg_103,"negative if repeated independent trials can result in a success with probability p and binomial a failure with probability q = 1 − p, then the probability distribution of the distribution random variable x, the number of the trial on which the kth success occurs, is"
940,1,"['association', 'probability']", Negative Binomial and Geometric Distributions,seg_103,"example 5.14: in an nba (national basketball association) championship series, the team that wins four games out of seven is the winner. suppose that teams a and b face each other in the championship games and that team a has probability 0.55 of winning a game over team b."
941,1,['probability'], Negative Binomial and Geometric Distributions,seg_103,(a) what is the probability that team a will win the series in 6 games?
942,1,['probability'], Negative Binomial and Geometric Distributions,seg_103,(b) what is the probability that team a will win the series?
943,0,[], Negative Binomial and Geometric Distributions,seg_103,"(c) if teams a and b were facing each other in a regional playoff series, which is"
944,1,['probability'], Negative Binomial and Geometric Distributions,seg_103,"decided by winning three out of five games, what is the probability that team a would win the series?"
945,0,[], Negative Binomial and Geometric Distributions,seg_103,(b) p (team a wins the championship series) is
946,1,"['case', 'negative binomial distribution', 'distribution', 'negative binomial', 'probability distribution', 'success', 'trials', 'binomial', 'probability', 'binomial distribution']", Negative Binomial and Geometric Distributions,seg_103,"the negative binomial distribution derives its name from the fact that each term in the expansion of pk(1 − q)−k corresponds to the values of b∗(x; k, p) for x = k, k + 1, k + 2, . . . . if we consider the special case of the negative binomial distribution where k = 1, we have a probability distribution for the number of trials required for a single success. an example would be the tossing of a coin until a head occurs. we might be interested in the probability that the first head occurs on the fourth toss. the negative binomial distribution reduces to the form"
947,1,"['geometric distribution', 'case', 'distribution', 'geometric']", Negative Binomial and Geometric Distributions,seg_103,"since the successive terms constitute a geometric progression, it is customary to refer to this special case as the geometric distribution and denote its values by g(x; p)."
948,1,"['independent', 'failure', 'trial', 'distribution', 'random variable', 'probability distribution', 'variable', 'success', 'trials', 'probability', 'random']", Negative Binomial and Geometric Distributions,seg_103,"geometric if repeated independent trials can result in a success with probability p and distribution a failure with probability q = 1 − p, then the probability distribution of the random variable x, the number of the trial on which the first success occurs, is"
949,1,"['average', 'process', 'probability']", Negative Binomial and Geometric Distributions,seg_103,"example 5.15: for a certain manufacturing process, it is known that, on the average, 1 in every 100 items is defective. what is the probability that the fifth item inspected is the first defective item found?"
950,1,"['distribution', 'geometric distribution', 'geometric']", Negative Binomial and Geometric Distributions,seg_103,"solution : using the geometric distribution with x = 5 and p = 0.01, we have"
951,1,"['successful', 'probability']", Negative Binomial and Geometric Distributions,seg_103,"example 5.16: at a “busy time,” a telephone exchange is very near capacity, so callers have difficulty placing their calls. it may be of interest to know the number of attempts necessary in order to make a connection. suppose that we let p = 0.05 be the probability of a connection during a busy time. we are interested in knowing the probability that 5 attempts are necessary for a successful call."
952,1,"['distribution', 'geometric distribution', 'geometric']", Negative Binomial and Geometric Distributions,seg_103,solution : using the geometric distribution with x = 5 and p = 0.05 yields
953,1,"['states', 'geometric distribution', 'distribution', 'mean', 'variance', 'geometric']", Negative Binomial and Geometric Distributions,seg_103,"quite often, in applications dealing with the geometric distribution, the mean and variance are important. for example, in example 5.16, the expected number of calls necessary to make a connection is quite important. the following theorem states without proof the mean and variance of the geometric distribution."
954,1,"['geometric', 'geometric distribution', 'distribution', 'random variable', 'variable', 'mean', 'random', 'variance']", Negative Binomial and Geometric Distributions,seg_103,the mean and variance of a random variable following the geometric distribution
955,1,"['negative binomial and geometric distributions', 'geometric distribution', 'geometric distributions', 'case', 'distribution', 'negative binomial', 'success', 'trials', 'binomial', 'probability', 'geometric', 'distributions']", Negative Binomial and Geometric Distributions,seg_103,"areas of application for the negative binomial and geometric distributions become obvious when one focuses on the examples in this section and the exercises devoted to these distributions at the end of section 5.5. in the case of the geometric distribution, example 5.16 depicts a situation where engineers or managers are attempting to determine how inefficient a telephone exchange system is during busy times. clearly, in this case, trials occurring prior to a success represent a cost. if there is a high probability of several attempts being required prior to making a connection, then plans should be made to redesign the system."
956,1,"['experienced', 'level', 'successes', 'negative binomial distribution', 'distribution', 'negative binomial', 'success', 'locations', 'binomial', 'probability', 'binomial distribution']", Negative Binomial and Geometric Distributions,seg_103,"applications of the negative binomial distribution are similar in nature. suppose attempts are costly in some sense and are occurring in sequence. a high probability of needing a “large” number of attempts to experience a fixed number of successes is not beneficial to the scientist or engineer. consider the scenarios of review exercises 5.90 and 5.91. in review exercise 5.91, the oil driller defines a certain level of success from sequentially drilling locations for oil. if only 6 attempts have been made at the point where the second success is experienced, the profits appear to dominate substantially the investment incurred by the drilling."
957,1,"['interval', 'observations', 'random', 'process', 'numerical', 'experiment', 'random variable', 'poisson process', 'poisson experiment', 'outcomes', 'poisson', 'errors', 'variable', 'experiments']", Poisson Distribution and the Poisson Process,seg_105,"experiments yielding numerical values of a random variable x, the number of outcomes occurring during a given time interval or in a specified region, are called poisson experiments. the given time interval may be of any length, such as a minute, a day, a week, a month, or even a year. for example, a poisson experiment can generate observations for the random variable x representing the number of telephone calls received per hour by an office, the number of days school is closed due to snow during the winter, or the number of games postponed due to rain during a baseball season. the specified region could be a line segment, an area, a volume, or perhaps a piece of material. in such instances, x might represent the number of field mice per acre, the number of bacteria in a given culture, or the number of typing errors per page. a poisson experiment is derived from the poisson process and possesses the following properties."
958,1,"['outcomes', 'interval']", Poisson Distribution and the Poisson Process,seg_105,1. the number of outcomes occurring in one time interval or specified region of
959,1,"['poisson', 'disjoint', 'memory', 'independent', 'poisson process', 'process']", Poisson Distribution and the Poisson Process,seg_105,space is independent of the number that occur in any other disjoint time interval or region. in this sense we say that the poisson process has no memory.
960,1,"['outcome', 'probability']", Poisson Distribution and the Poisson Process,seg_105,2. the probability that a single outcome will occur during a very short time
961,1,"['outcomes', 'interval']", Poisson Distribution and the Poisson Process,seg_105,interval or in a small region is proportional to the length of the time interval or the size of the region and does not depend on the number of outcomes occurring outside this time interval or region.
962,1,"['outcome', 'probability']", Poisson Distribution and the Poisson Process,seg_105,3. the probability that more than one outcome will occur in such a short time
963,0,[], Poisson Distribution and the Poisson Process,seg_105,interval or fall in such a small region is negligible.
964,1,"['poisson', 'outcomes', 'poisson experiment', 'experiment', 'poisson random variable', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random']", Poisson Distribution and the Poisson Process,seg_105,"the number x of outcomes occurring during a poisson experiment is called a poisson random variable, and its probability distribution is called the poisson"
965,1,"['poisson', 'rate', 'poisson process', 'probabilities', 'mean', 'process', 'outcomes']", Poisson Distribution and the Poisson Process,seg_105,"distribution. the mean number of outcomes is computed from μ = λt, where t is the specific “time,” “distance,” “area,” or “volume” of interest. since the probabilities depend on λ, the rate of occurrence of outcomes, we shall denote them by p(x;λt). the derivation of the formula for p(x;λt), based on the three properties of a poisson process listed above, is beyond the scope of this book. the following formula is used for computing poisson probabilities."
966,1,"['poisson', 'outcomes', 'interval', 'poisson random variable', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random']", Poisson Distribution and the Poisson Process,seg_105,"poisson the probability distribution of the poisson random variable x, representing distribution the number of outcomes occurring in a given time interval or specified region"
967,1,"['average', 'outcomes']", Poisson Distribution and the Poisson Process,seg_105,"p(x;λt) = , x = 0, 1, 2, . . . , x! where λ is the average number of outcomes per unit time, distance, area, or volume and e = 2.71828 . . . ."
968,1,"['poisson', 'probability']", Poisson Distribution and the Poisson Process,seg_105,"table a.2 contains poisson probability sums,"
969,1,['table'], Poisson Distribution and the Poisson Process,seg_105,for selected values of λt ranging from 0.1 to 18.0. we illustrate the use of this table with the following two examples.
970,1,"['average', 'probability', 'experiment']", Poisson Distribution and the Poisson Process,seg_105,"example 5.17: during a laboratory experiment, the average number of radioactive particles passing through a counter in 1 millisecond is 4. what is the probability that 6 particles enter the counter in a given millisecond?"
971,1,"['poisson', 'distribution', 'table', 'poisson distribution']", Poisson Distribution and the Poisson Process,seg_105,"solution : using the poisson distribution with x = 6 and λt = 4 and referring to table a.2, we have"
972,1,"['average', 'probability']", Poisson Distribution and the Poisson Process,seg_105,example 5.18: ten is the average number of oil tankers arriving each day at a certain port. the facilities at the port can handle at most 15 tankers per day. what is the probability that on a given day tankers have to be turned away?
973,1,['table'], Poisson Distribution and the Poisson Process,seg_105,"solution : let x be the number of tankers arriving each day. then, using table a.2, we have"
974,1,"['random', 'binomial distribution', 'process', 'poisson random variable', 'random variable', 'distributions', 'poisson process', 'distribution', 'binomial', 'continuous', 'poisson distribution', 'poisson', 'sampling', 'variable', 'acceptance sampling', 'continuous distributions']", Poisson Distribution and the Poisson Process,seg_105,"15 p (x > 15) = 1− p (x ≤ 15) = 1−∑ p(x; 10) = 1− 0.9513 = 0.0487. x=0 like the binomial distribution, the poisson distribution is used for quality control, quality assurance, and acceptance sampling. in addition, certain important continuous distributions used in reliability theory and queuing theory depend on the poisson process. some of these distributions are discussed and developed in chapter 6. the following theorem concerning the poisson random variable is given in appendix a.25."
975,1,"['poisson', 'distribution', 'mean', 'poisson distribution', 'variance']", Poisson Distribution and the Poisson Process,seg_105,theorem 5.4: both the mean and the variance of the poisson distribution p(x;λt) are λt.
976,1,"['plots', 'discrete', 'probability', 'function', 'binomial distribution', 'symmetric', 'symmetry', 'mean', 'distributions', 'condition', 'distribution', 'binomial', 'continuous', 'probability function', 'poisson', 'continuous distributions']", Poisson Distribution and the Poisson Process,seg_105,"like so many discrete and continuous distributions, the form of the poisson distribution becomes more and more symmetric, even bell-shaped, as the mean grows large. figure 5.1 illustrates this, showing plots of the probability function for μ = 0.1, μ = 2, and μ = 5. note the nearness to symmetry when μ becomes as large as 5. a similar condition exists for the binomial distribution, as will be illustrated later in the text."
977,1,"['poisson', 'functions', 'density functions']", Poisson Distribution and the Poisson Process,seg_105,figure 5.1: poisson density functions for different means.
978,1,"['case', 'bernoulli trials', 'binomial distribution', 'process', 'trials', 'success', 'parameter', 'poisson process', 'failure', 'distribution', 'binomial', 'continuous', 'poisson distribution', 'poisson', 'probabilities', 'bernoulli']", Poisson Distribution and the Poisson Process,seg_105,"it should be evident from the three principles of the poisson process that the poisson distribution is related to the binomial distribution. although the poisson usually finds applications in space and time problems, as illustrated by examples 5.17 and 5.18, it can be viewed as a limiting form of the binomial distribution. in the case of the binomial, if n is quite large and p is small, the conditions begin to simulate the continuous space or time implications of the poisson process. the independence among bernoulli trials in the binomial case is consistent with principle 2 of the poisson process. allowing the parameter p to be close to 0 relates to principle 3 of the poisson process. indeed, if n is large and p is close to 0, the poisson distribution can be used, with μ = np, to approximate binomial probabilities. if p is close to 1, we can still use the poisson distribution to approximate binomial probabilities by interchanging what we have defined to be a success and a failure, thereby changing p to a value close to 0."
979,1,"['distribution', 'random variable', 'probability distribution', 'variable', 'binomial random variable', 'binomial', 'probability', 'random']", Poisson Distribution and the Poisson Process,seg_105,"theorem 5.5: letx be a binomial random variable with probability distribution b(x;n, p). when n→∞ n → ∞, p → 0, and np −→ μ remains constant,"
980,1,"['probability distributions', 'discrete', 'probability', 'discrete probability distributions', 'distributions']", Poisson Distribution and the Poisson Process,seg_105,164 chapter 5 some discrete probability distributions
981,1,"['independent', 'probability']", Poisson Distribution and the Poisson Process,seg_105,"example 5.19: in a certain industrial facility, accidents occur infrequently. it is known that the probability of an accident on any given day is 0.005 and accidents are independent of each other."
982,1,['probability'], Poisson Distribution and the Poisson Process,seg_105,(a) what is the probability that in any given period of 400 days there will be an
983,0,[], Poisson Distribution and the Poisson Process,seg_105,accident on one day?
984,1,['probability'], Poisson Distribution and the Poisson Process,seg_105,(b) what is the probability that there are at most three days with an accident?
985,1,"['poisson', 'approximation', 'random variable', 'variable', 'binomial random variable', 'binomial', 'random']", Poisson Distribution and the Poisson Process,seg_105,"solution : let x be a binomial random variable with n = 400 and p = 0.005. thus, np = 2. using the poisson approximation,"
986,1,"['sample', 'process', 'random', 'probability', 'random sample', 'average']", Poisson Distribution and the Poisson Process,seg_105,"example 5.20: in a manufacturing process where glass products are made, defects or bubbles occur, occasionally rendering the piece undesirable for marketing. it is known that, on average, 1 in every 1000 of these items produced has one or more bubbles. what is the probability that a random sample of 8000 will yield fewer than 7 items possessing bubbles?"
987,1,"['poisson', 'binomial experiment', 'experiment', 'distribution', 'binomial', 'poisson distribution']", Poisson Distribution and the Poisson Process,seg_105,"solution : this is essentially a binomial experiment with n = 8000 and p = 0.001. since p is very close to 0 and n is quite large, we shall approximate with the poisson distribution using"
988,0,[], Poisson Distribution and the Poisson Process,seg_105,"hence, if x represents the number of bubbles, we have"
989,1,"['discrete', 'negative binomial', 'frequency', 'binomial distributions', 'random', 'geometric', 'poisson distributions', 'gamma distribution', 'negative binomial distributions', 'random variable', 'binomial random variable', 'distributions', 'gamma', 'discrete distributions', 'distribution', 'binomial', 'poisson', 'random variables', 'variables', 'sampling', 'variable', 'gamma random variables']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_111,"the discrete distributions discussed in this chapter occur with great frequency in engineering and the biological and physical sciences. the exercises and examples certainly suggest this. industrial sampling plans and many engineering judgments are based on the binomial and poisson distributions as well as on the hypergeometric distribution. while the geometric and negative binomial distributions are used to a somewhat lesser extent, they also find applications. in particular, a negative binomial random variable can be viewed as a mixture of poisson and gamma random variables (the gamma distribution will be discussed in chapter 6)."
990,1,"['poisson', 'process', 'factors', 'data', 'distribution', 'probability', 'parameter', 'poisson distribution', 'control', 'historical data', 'distributions']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_111,"despite the rich heritage that these distributions find in real life, they can be misused unless the scientific practitioner is prudent and cautious. of course, any probability calculation for the distributions discussed in this chapter is made under the assumption that the parameter value is known. real-world applications often result in a parameter value that may “move around” due to factors that are difficult to control in the process or because of interventions in the process that have not been taken into account. for example, in review exercise 5.77, “historical information” is used. but is the process that exists now the same as that under which the historical data were collected? the use of the poisson distribution can suffer even more from this kind of difficulty. for example, in review exercise 5.80, the questions in parts (a), (b), and (c) are based on the use of μ = 2.7 calls per minute. based on historical records, this is the number of calls that occur “on average.” but in this and many other applications of the poisson distribution, there are slow times and busy times and so there are times in which the conditions"
991,1,"['poisson', 'independent', 'poisson process', 'case', 'bernoulli', 'independence', 'trials', 'bernoulli trials', 'binomial', 'probability', 'process']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_111,"for the poisson process may appear to hold when in fact they do not. thus, the probability calculations may be incorrect. in the case of the binomial, the assumption that may fail in certain applications (in addition to nonconstancy of p) is the independence assumption, stating that the bernoulli trials are independent."
992,1,"['estimate', 'prediction', 'probability theory', 'distribution', 'frequency', 'binomial', 'probability', 'binomial distribution', 'error']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_111,"one of the most famous misuses of the binomial distribution occurred in the 1961 baseball season, when mickey mantle and roger maris were engaged in a friendly battle to break babe ruth’s all-time record of 60 home runs. a famous magazine article made a prediction, based on probability theory, that mantle would break the record. the prediction was based on probability calculation with the use of the binomial distribution. the classic error made was to estimate the parameter p (one for each player) based on relative historical frequency of home runs throughout the players’ careers. maris, unlike mantle, had not been a prodigious home run hitter prior to 1961 so his estimate of p was quite low. as a result, the calculated probability of breaking the record was quite high for mantle and low for maris. the end result: mantle failed to break the record and maris succeeded."
993,1,"['density function', 'interval', 'continuous', 'uniform distribution', 'statistics', 'continuous distributions', 'distribution', 'probability', 'function', 'distributions']", Continuous Uniform Distribution,seg_115,"one of the simplest continuous distributions in all of statistics is the continuous uniform distribution. this distribution is characterized by a density function that is “flat,” and thus the probability is uniform in a closed interval, say [a, b]. although applications of the continuous uniform distribution are not as abundant as those for other distributions discussed in this chapter, it is appropriate for the novice to begin this introduction to continuous distributions with the uniform distribution."
994,1,"['density function', 'continuous', 'random variable', 'variable', 'random', 'function']", Continuous Uniform Distribution,seg_115,"uniform the density function of the continuous uniform random variable x on the indistribution terval [a, b] is"
995,1,"['density function', 'interval', 'uniform distribution', 'distribution', 'random variable', 'variable', 'rectangular distribution', 'random', 'function']", Continuous Uniform Distribution,seg_115,"1 the density function forms a rectangle with base b−a and constant height b−a . as a result, the uniform distribution is often called the rectangular distribution. note, however, that the interval may not always be closed: [a,b]. it can be (a,b) as well. the density function for a uniform random variable on the interval [1, 3] is shown in figure 6.1."
996,1,"['density function', 'interval', 'uniform distribution', 'distribution', 'probability', 'function']", Continuous Uniform Distribution,seg_115,"probabilities are simple to calculate for the uniform distribution because of the simple nature of the density function. however, note that the application of this distribution is based on the assumption that the probability of falling in an interval of fixed length within [a, b] is constant."
997,1,"['distribution', 'uniform distribution', 'interval']", Continuous Uniform Distribution,seg_115,"example 6.1: suppose that a large conference room at a certain company can be reserved for no more than 4 hours. both long and short conferences occur quite often. in fact, it can be assumed that the length x of a conference has a uniform distribution on the interval [0, 4]."
998,1,"['density function', 'interval', 'random variable', 'variable', 'random', 'function']", Continuous Uniform Distribution,seg_115,"figure 6.1: the density function for a random variable on the interval [1, 3]."
999,1,"['density function', 'probability density function', 'probability', 'function']", Continuous Uniform Distribution,seg_115,(a) what is the probability density function?
1000,1,['probability'], Continuous Uniform Distribution,seg_115,(b) what is the probability that any given conference lasts at least 3 hours?
1001,1,"['density function', 'random', 'function', 'uniformly distributed']", Continuous Uniform Distribution,seg_115,(a) the appropriate density function for the uniformly distributed random vari-
1002,0,[], Continuous Uniform Distribution,seg_115,solution : able x in this situation is
1003,1,"['distribution', 'uniform distribution', 'variance', 'mean']", Continuous Uniform Distribution,seg_115,the mean and variance of the uniform distribution are
1004,0,[], Continuous Uniform Distribution,seg_115,the proofs of the theorems are left to the reader. see exercise 6.1 on page 185.
1005,1,"['curve', 'statistics', 'errors', 'gaussian distribution', 'distribution', 'probability distribution', 'normal', 'measurements', 'probability', 'continuous probability distribution', 'continuous', 'experiments', 'normal distribution']", Normal Distribution,seg_117,"the most important continuous probability distribution in the entire field of statistics is the normal distribution. its graph, called the normal curve, is the bell-shaped curve of figure 6.2, which approximately describes many phenomena that occur in nature, industry, and research. for example, physical measurements in areas such as meteorological experiments, rainfall studies, and measurements of manufactured parts are often more than adequately explained with a normal distribution. in addition, errors in scientific measurements are extremely well approximated by a normal distribution. in 1733, abraham demoivre developed the mathematical equation of the normal curve. it provided a basis from which much of the theory of inductive statistics is founded. the normal distribution is often referred to as the gaussian distribution, in honor of karl friedrich gauss"
1006,1,"['curve', 'normal']", Normal Distribution,seg_117,figure 6.2: the normal curve.
1007,1,['errors'], Normal Distribution,seg_117,"(1777–1855), who also derived its equation from a study of errors in repeated measurements of the same quantity."
1008,1,"['deviation', 'random', 'parameters', 'normal random variable', 'continuous random variable', 'distribution', 'random variable', 'probability distribution', 'normal', 'variable', 'mean', 'probability', 'standard', 'continuous', 'standard deviation']", Normal Distribution,seg_117,"a continuous random variable x having the bell-shaped distribution of figure 6.2 is called a normal random variable. the mathematical equation for the probability distribution of the normal variable depends on the two parameters μ and σ, its mean and standard deviation, respectively. hence, we denote the values of the density of x by n(x;μ, σ)."
1009,1,"['random variable', 'variable', 'normal', 'mean', 'random', 'normal random variable', 'variance']", Normal Distribution,seg_117,"normal the density of the normal random variable x, with mean μ and variance σ2, is"
1010,1,"['deviation', 'curve', 'normal', 'standard', 'standard deviation']", Normal Distribution,seg_117,"once μ and σ are specified, the normal curve is completely determined. for example, if μ = 50 and σ = 5, then the ordinates n(x; 50, 5) can be computed for various values of x and the curve drawn. in figure 6.3, we have sketched two normal curves having the same standard deviation but different means. the two curves are identical in form but are centered at different positions along the horizontal axis."
1011,1,['normal'], Normal Distribution,seg_117,figure 6.3: normal curves with μ1 < μ2 and σ1 = σ2.
1012,1,['normal'], Normal Distribution,seg_117,figure 6.4: normal curves with μ1 = μ2 and σ1 < σ2.
1013,1,"['deviation', 'curve', 'standard deviations', 'observations', 'deviations', 'variable', 'set', 'normal', 'mean', 'probability', 'standard', 'standard deviation']", Normal Distribution,seg_117,"in figure 6.4, we have sketched two normal curves with the same mean but different standard deviations. this time we see that the two curves are centered at exactly the same position on the horizontal axis, but the curve with the larger standard deviation is lower and spreads out farther. remember that the area under a probability curve must be equal to 1, and therefore the more variable the set of observations, the lower and wider the corresponding curve will be."
1014,1,"['deviations', 'normal']", Normal Distribution,seg_117,"figure 6.5 shows two normal curves having different means and different standard deviations. clearly, they are centered at different positions on the horizontal axis and their shapes reflect the two different values of σ."
1015,1,['normal'], Normal Distribution,seg_117,figure 6.5: normal curves with μ1 < μ2 and σ1 < σ2.
1016,1,"['curve', 'normal']", Normal Distribution,seg_117,"based on inspection of figures 6.2 through 6.5 and examination of the first and second derivatives of n(x;μ, σ), we list the following properties of the normal curve:"
1017,1,['curve'], Normal Distribution,seg_117,"1. the mode, which is the point on the horizontal axis where the curve is a"
1018,1,"['symmetric', 'curve', 'mean']", Normal Distribution,seg_117,2. the curve is symmetric about a vertical axis through the mean μ.
1019,1,['curve'], Normal Distribution,seg_117,3. the curve has its points of inflection at x = μ± σ; it is concave downward if
1020,1,"['curve', 'normal']", Normal Distribution,seg_117,4. the normal curve approaches the horizontal axis asymptotically as we proceed
1021,1,['mean'], Normal Distribution,seg_117,in either direction away from the mean.
1022,1,['curve'], Normal Distribution,seg_117,5. the total area under the curve and above the horizontal axis is equal to 1.
1023,1,"['deviation', 'variance', 'mean']", Normal Distribution,seg_117,"theorem 6.2: the mean and variance of n(x;μ, σ) are μ and σ2, respectively. hence, the standard deviation is σ."
1024,1,['mean'], Normal Distribution,seg_117,"proof : to evaluate the mean, we first calculate"
1025,1,['function'], Normal Distribution,seg_117,"since the integrand above is an odd function of z. using theorem 4.5 on page 128, we conclude that"
1026,1,"['distribution', 'normal', 'variance', 'normal distribution']", Normal Distribution,seg_117,the variance of the normal distribution is given by
1027,0,[], Normal Distribution,seg_117,"again setting z = (x− μ)/σ and dx = σ dz, we obtain"
1028,1,"['probability', 'random', 'probability distributions', 'data', 'statistical', 'distributions', 'curve', 'experimental', 'parameters', 'random variables', 'estimated', 'variables', 'normal']", Normal Distribution,seg_117,"many random variables have probability distributions that can be described adequately by the normal curve once μ and σ2 are specified. in this chapter, we shall assume that these two parameters are known, perhaps from previous investigations. later, we shall make statistical inferences when μ and σ2 are unknown and have been estimated from the available experimental data."
1029,1,"['case', 'hypergeometric', 'sample', 'approximation', 'data', 'sampling distributions', 'hypergeometric distributions', 'statistical', 'distributions', 'statistical inference', 'distribution', 'binomial', 'continuous', 'variables', 'binomial and hypergeometric distributions', 'sampling', 'normal', 'experiments', 'normal distribution']", Normal Distribution,seg_117,"we pointed out earlier the role that the normal distribution plays as a reasonable approximation of scientific variables in real-life experiments. there are other applications of the normal distribution that the reader will appreciate as he or she moves on in the book. the normal distribution finds enormous application as a limiting distribution. under certain conditions, the normal distribution provides a good continuous approximation to the binomial and hypergeometric distributions. the case of the approximation to the binomial is covered in section 6.5. in chapter 8, the reader will learn about sampling distributions. it turns out that the limiting distribution of sample averages is normal. this provides a broad base for statistical inference that proves very valuable to the data analyst interested in"
1030,1,"['distribution', 'hypothesis testing', 'normal', 'control', 'analysis of variance', 'variance', 'quality control', 'normal distribution', 'hypothesis']", Normal Distribution,seg_117,"estimation and hypothesis testing. theory in the important areas such as analysis of variance (chapters 13, 14, and 15) and quality control (chapter 17) is based on assumptions that make use of the normal distribution."
1031,1,"['distribution', 'tables', 'normal', 'normal distribution']", Normal Distribution,seg_117,"in section 6.3, examples demonstrate the use of tables of the normal distribution. section 6.4 follows with examples of applications of the normal distribution."
1032,1,"['density function', 'curve', 'continuous', 'distribution', 'random variable', 'probability distribution', 'normal', 'variable', 'continuous probability distribution', 'probability', 'random', 'function']", Areas under the Normal Curve,seg_119,"the curve of any continuous probability distribution or density function is constructed so that the area under the curve bounded by the two ordinates x = x1 and x = x2 equals the probability that the random variable x assumes a value between x = x1 and x = x2. thus, for the normal curve in figure 6.6,"
1033,0,[], Areas under the Normal Curve,seg_119,is represented by the area of the shaded region.
1034,1,"['deviation', 'curve', 'random', 'dependent', 'distribution', 'random variable', 'variable', 'normal', 'associated', 'mean', 'probability', 'standard', 'standard deviation', 'variances']", Areas under the Normal Curve,seg_119,"in figures 6.3, 6.4, and 6.5 we saw how the normal curve is dependent on the mean and the standard deviation of the distribution under investigation. the area under the curve between any two ordinates must then also depend on the values μ and σ. this is evident in figure 6.7, where we have shaded regions corresponding to p (x1 < x < x2) for two curves with different means and variances. p (x1 < x < x2), where x is the random variable describing distribution a, is indicated by the shaded area below the curve of a. if x is the random variable describing distribution b, then p (x1 < x < x2) is given by the entire shaded region. obviously, the two shaded regions are different in size; therefore, the probability associated with each distribution will be different for the two given values of x."
1035,1,"['observations', 'set', 'random', 'random variable', 'normal random variable', 'statistical', 'functions', 'curve', 'density functions', 'variable', 'tables', 'normal', 'transform']", Areas under the Normal Curve,seg_119,"there are many types of statistical software that can be used in calculating areas under the normal curve. the difficulty encountered in solving integrals of normal density functions necessitates the tabulation of normal curve areas for quick reference. however, it would be a hopeless task to attempt to set up separate tables for every conceivable value of μ and σ. fortunately, we are able to transform all the observations of any normal random variable x into a new set of observations"
1036,1,['normal'], Areas under the Normal Curve,seg_119,figure 6.7: p (x1 < x < x2) for different normal curves.
1037,1,"['random variable', 'variable', 'normal', 'mean', 'random', 'transformation', 'normal random variable', 'variance']", Areas under the Normal Curve,seg_119,of a normal random variable z with mean 0 and variance 1. this can be done by means of the transformation
1038,1,"['random variable', 'variable', 'random']", Areas under the Normal Curve,seg_119,"whenever x assumes a value x, the corresponding value of z is given by z = (x − μ)/σ. therefore, if x falls between the values x = x1 and x = x2, the random variable z will fall between the corresponding values z1 = (x1 − μ)/σ and z2 = (x2 − μ)/σ. consequently, we may write"
1039,1,"['random variable', 'variable', 'normal', 'mean', 'random', 'normal random variable', 'variance']", Areas under the Normal Curve,seg_119,where z is seen to be a normal random variable with mean 0 and variance 1.
1040,1,"['random', 'standard normal', 'distribution', 'random variable', 'variable', 'normal', 'mean', 'standard', 'normal random variable', 'standard normal distribution', 'variance', 'normal distribution']", Areas under the Normal Curve,seg_119,definition 6.1: the distribution of a normal random variable with mean 0 and variance 1 is called a standard normal distribution.
1041,1,"['transformed', 'distributions']", Areas under the Normal Curve,seg_119,"the original and transformed distributions are illustrated in figure 6.8. since all the values of x falling between x1 and x2 have corresponding z values between z1 and z2, the area under the x-curve between the ordinates x = x1 and x = x2 in figure 6.8 equals the area under the z-curve between the transformed ordinates z = z1 and z = z2."
1042,1,"['curve', 'table', 'standard normal', 'distribution', 'tables', 'normal', 'probability', 'standard', 'standard normal distribution', 'process', 'normal distribution']", Areas under the Normal Curve,seg_119,"we have now reduced the required number of tables of normal-curve areas to one, that of the standard normal distribution. table a.3 indicates the area under the standard normal curve corresponding to p (z < z) for values of z ranging from −3.49 to 3.49. to illustrate the use of this table, let us find the probability that z is less than 1.74. first, we locate a value of z equal to 1.7 in the left column; then we move across the row to the column under 0.04, where we read 0.9591. therefore, p (z < 1.74) = 0.9591. to find a z value corresponding to a given probability, the process is reversed. for example, the z value leaving an area of 0.2148 under the curve to the left of z is seen to be −0.79."
1043,1,"['normal distributions', 'transformed', 'normal', 'distributions']", Areas under the Normal Curve,seg_119,figure 6.8: the original and transformed normal distributions.
1044,1,"['curve', 'standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Areas under the Normal Curve,seg_119,"example 6.2: given a standard normal distribution, find the area under the curve that lies"
1045,0,[], Areas under the Normal Curve,seg_119,solution : see figure 6.9 for the specific areas.
1046,1,['table'], Areas under the Normal Curve,seg_119,"in table a.3 to the left of z = 1.84, namely, 1− 0.9671 = 0.0329."
1047,1,['table'], Areas under the Normal Curve,seg_119,area to the left of z = 0.86 minus the area to the left of z = −1.97. from table a.3 we find the desired area to be 0.8051− 0.0244 = 0.7807.
1048,1,"['standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Areas under the Normal Curve,seg_119,"example 6.3: given a standard normal distribution, find the value of k such that"
1049,1,['distributions'], Areas under the Normal Curve,seg_119,solution : distributions and the desired areas are shown in figure 6.10.
1050,1,['table'], Areas under the Normal Curve,seg_119,right must then leave an area of 0.6985 to the left. from table a.3 it follows that k = 0.52.
1051,1,['table'], Areas under the Normal Curve,seg_119,(b) from table a.3 we note that the total area to the left of −0.18 is equal to
1052,1,['table'], Areas under the Normal Curve,seg_119,"0.4286. in figure 6.10(b), we see that the area between k and −0.18 is 0.4197, so the area to the left of k must be 0.4286 − 0.4197 = 0.0089. hence, from table a.3, we have k = −2.37."
1053,1,"['distribution', 'random variable', 'variable', 'normal', 'probability', 'random', 'normal distribution']", Areas under the Normal Curve,seg_119,"example 6.4: given a random variable x having a normal distribution with μ = 50 and σ = 10, find the probability that x assumes a value between 45 and 62."
1054,0,[], Areas under the Normal Curve,seg_119,solution : the z values corresponding to x1 = 45 and x2 = 62 are
1055,1,['table'], Areas under the Normal Curve,seg_119,"p (−0.5 < z < 1.2) is shown by the area of the shaded region in figure 6.11. this area may be found by subtracting the area to the left of the ordinate z = −0.5 from the entire area to the left of z = 1.2. using table a.3, we have"
1056,1,"['distribution', 'probability', 'normal', 'normal distribution']", Areas under the Normal Curve,seg_119,"example 6.5: given that x has a normal distribution with μ = 300 and σ = 50, find the probability that x assumes a value greater than 362."
1057,1,"['curve', 'table', 'distribution', 'normal probability distribution', 'probability distribution', 'normal', 'probability', 'transforming']", Areas under the Normal Curve,seg_119,"solution : the normal probability distribution with the desired area shaded is shown in figure 6.12. to find p (x > 362), we need to evaluate the area under the normal curve to the right of x = 362. this can be done by transforming x = 362 to the corresponding z value, obtaining the area to the left of z from table a.3, and then subtracting this area from 1. we find that"
1058,1,"['random', 'standard deviations', 'distribution', 'random variable', 'variable', 'normal', 'deviations', 'mean', 'probability', 'standard', 'normal distribution']", Areas under the Normal Curve,seg_119,"according to chebyshev’s theorem on page 137, the probability that a random variable assumes a value within 2 standard deviations of the mean is at least 3/4. if the random variable has a normal distribution, the z values corresponding to x1 = μ− 2σ and x2 = μ+ 2σ are easily computed to be"
1059,0,[], Areas under the Normal Curve,seg_119,which is a much stronger statement than that given by chebyshev’s theorem.
1060,1,"['probability', 'table']", Areas under the Normal Curve,seg_119,"sometimes, we are required to find the value of z corresponding to a specified probability that falls between values listed in table a.3 (see example 6.6). for convenience, we shall always choose the z value corresponding to the tabular probability that comes closest to the specified probability."
1061,1,"['process', 'probability']", Areas under the Normal Curve,seg_119,"the preceding two examples were solved by going first from a value of x to a z value and then computing the desired area. in example 6.6, we reverse the process and begin with a known area or probability, find the z value, and then determine x by rearranging the formula"
1062,1,"['distribution', 'normal', 'normal distribution']", Areas under the Normal Curve,seg_119,"example 6.6: given a normal distribution with μ = 40 and σ = 6, find the value of x that has"
1063,1,['table'], Areas under the Normal Curve,seg_119,solution : we require a z value that leaves an area of 0.45 to the left. from table a.3
1064,1,['table'], Areas under the Normal Curve,seg_119,"x value. this time we require a z value that leaves 0.14 of the area to the right and hence an area of 0.86 to the left. again, from table a.3, we find p (z < 1.08) = 0.86, so the desired z value is 1.08 and"
1065,1,"['curve', 'probabilities', 'distribution', 'normal', 'binomial', 'normal distribution']", Applications of the Normal Distribution,seg_121,some of the many problems for which the normal distribution is applicable are treated in the following examples. the use of the normal curve to approximate binomial probabilities is considered in section 6.5.
1066,1,"['deviation', 'normally distributed', 'probability', 'standard', 'standard deviation', 'average']", Applications of the Normal Distribution,seg_121,"example 6.7: a certain type of storage battery lasts, on average, 3.0 years with a standard deviation of 0.5 year. assuming that battery life is normally distributed, find the probability that a given battery will last less than 2.3 years."
1067,1,"['distribution', 'curve', 'normal']", Applications of the Normal Distribution,seg_121,"solution : first construct a diagram such as figure 6.14, showing the given distribution of battery lives and the desired area. to find p (x < 2.3), we need to evaluate the area under the normal curve to the left of 2.3. this is accomplished by finding the area to the left of the corresponding z value. hence, we find that"
1068,1,['table'], Applications of the Normal Distribution,seg_121,"and then, using table a.3, we have"
1069,1,"['deviation', 'normally distributed', 'mean', 'probability', 'standard', 'standard deviation']", Applications of the Normal Distribution,seg_121,"example 6.8: an electrical firm manufactures light bulbs that have a life, before burn-out, that is normally distributed with mean equal to 800 hours and a standard deviation of 40 hours. find the probability that a bulb burns between 778 and 834 hours."
1070,1,['distribution'], Applications of the Normal Distribution,seg_121,solution : the distribution of light bulb life is illustrated in figure 6.15. the z values corresponding to x1 = 778 and x2 = 834 are
1071,1,"['sets', 'process']", Applications of the Normal Distribution,seg_121,"example 6.9: in an industrial process, the diameter of a ball bearing is an important measurement. the buyer sets specifications for the diameter to be 3.0 ± 0.01 cm. the"
1072,1,"['deviation', 'process', 'normal', 'mean', 'standard', 'standard deviation', 'average']", Applications of the Normal Distribution,seg_121,"implication is that no part falling outside these specifications will be accepted. it is known that in the process the diameter of a ball bearing has a normal distribution with mean μ = 3.0 and standard deviation σ = 0.005. on average, how many manufactured ball bearings will be scrapped?"
1073,1,['distribution'], Applications of the Normal Distribution,seg_121,solution : the distribution of diameters is illustrated by figure 6.16. the values corresponding to the specification limits are x1 = 2.99 and x2 = 3.01. the corresponding z values are
1074,1,"['symmetry', 'normal', 'table']", Applications of the Normal Distribution,seg_121,"from table a.3, p (z < −2.0) = 0.0228. due to symmetry of the normal distribution, we find that"
1075,1,['average'], Applications of the Normal Distribution,seg_121,"as a result, it is anticipated that, on average, 4.56% of manufactured ball bearings will be scrapped."
1076,1,"['deviation', 'measurement', 'normally distributed', 'measurements', 'mean', 'standard', 'standard deviation']", Applications of the Normal Distribution,seg_121,example 6.10: gauges are used to reject all components for which a certain dimension is not within the specification 1.50 ± d. it is known that this measurement is normally distributed with mean 1.50 and standard deviation 0.2. determine the value d such that the specifications “cover” 95% of the measurements.
1077,1,['table'], Applications of the Normal Distribution,seg_121,solution : from table a.3 we know that
1078,0,[], Applications of the Normal Distribution,seg_121,from which we obtain
1079,0,[], Applications of the Normal Distribution,seg_121,an illustration of the specifications is shown in figure 6.17.
1080,1,"['deviation', 'percentage', 'distribution', 'normal', 'mean', 'standard', 'standard deviation', 'normal distribution']", Applications of the Normal Distribution,seg_121,"example 6.11: a certain machine makes electrical resistors having a mean resistance of 40 ohms and a standard deviation of 2 ohms. assuming that the resistance follows a normal distribution and can be measured to any degree of accuracy, what percentage of resistors will have a resistance exceeding 43 ohms?"
1081,1,"['percentage', 'interval', 'table', 'relative frequency', 'frequency', 'probability', 'transforming']", Applications of the Normal Distribution,seg_121,"solution : a percentage is found by multiplying the relative frequency by 100%. since the relative frequency for an interval is equal to the probability of a value falling in the interval, we must find the area to the right of x = 43 in figure 6.18. this can be done by transforming x = 43 to the corresponding z value, obtaining the area to the left of z from table a.3, and then subtracting this area from 1. we find"
1082,0,[], Applications of the Normal Distribution,seg_121,"hence, 6.68% of the resistors will have a resistance exceeding 43 ohms."
1083,1,['percentage'], Applications of the Normal Distribution,seg_121,example 6.12: find the percentage of resistances exceeding 43 ohms for example 6.11 if resistance is measured to the nearest ohm.
1084,1,"['discrete', 'distribution', 'normal', 'continuous', 'discrete distribution', 'normal distribution']", Applications of the Normal Distribution,seg_121,solution : this problem differs from that in example 6.11 in that we now assign a measurement of 43 ohms to all resistors whose resistances are greater than 42.5 and less than 43.5. we are actually approximating a discrete distribution by means of a continuous normal distribution. the required area is the region shaded to the right of 43.5 in figure 6.19. we now find that
1085,0,[], Applications of the Normal Distribution,seg_121,"therefore, 4.01% of the resistances exceed 43 ohms when measured to the nearest ohm. the difference 6.68% − 4.01% = 2.67% between this answer and that of example 6.11 represents all those resistance values greater than 43 and less than 43.5 that are now being recorded as 43 ohms."
1086,1,"['deviation', 'distribution', 'normal', 'standard', 'standard deviation', 'average', 'normal distribution']", Applications of the Normal Distribution,seg_121,"example 6.13: the average grade for an exam is 74, and the standard deviation is 7. if 12% of the class is given as, and the grades are curved to follow a normal distribution, what is the lowest possible a and the highest possible b?"
1087,1,"['probability', 'table']", Applications of the Normal Distribution,seg_121,"solution : in this example, we begin with a known area of probability, find the z value, and then determine x from the formula x = σz + μ. an area of 0.12, corresponding to the fraction of students receiving as, is shaded in figure 6.20. we require a z value that leaves 0.12 of the area to the right and, hence, an area of 0.88 to the left. from table a.3, p (z < 1.18) has the closest value to 0.88, so the desired z value is 1.18. hence,"
1088,0,[], Applications of the Normal Distribution,seg_121,"therefore, the lowest a is 83 and the highest b is 82."
1089,1,['table'], Applications of the Normal Distribution,seg_121,"example 6.14: refer to example 6.13 and find the sixth decile. solution : the sixth decile, written d6, is the x value that leaves 60% of the area to the left, as shown in figure 6.21. from table a.3 we find p (z < 0.25) ≈ 0.6, so the desired z value is 0.25. now x = (7)(0.25) + 74 = 75.75. hence, d6 = 75.75. that is, 60% of the grades are 75 or less."
1090,1,"['poisson', 'probabilities', 'table', 'binomial experiments', 'distribution', 'normal', 'binomial', 'associated', 'poisson distribution', 'binomial distribution', 'experiments', 'poisson distributions', 'normal distribution', 'distributions']", Normal Approximation to the Binomial,seg_125,"probabilities associated with binomial experiments are readily obtainable from the formula b(x;n, p) of the binomial distribution or from table a.1 when n is small. in addition, binomial probabilities are readily available in many computer software packages. however, it is instructive to learn the relationship between the binomial and the normal distribution. in section 5.5, we illustrated how the poisson distribution can be used to approximate binomial probabilities when n is quite large and p is very close to 0 or 1. both the binomial and the poisson distributions"
1091,1,"['distribution function', 'discrete', 'continuous probability distribution', 'probability', 'function', 'binomial distribution', 'discrete distribution', 'sample', 'cumulative distribution function', 'approximation', 'symmetric', 'sample space', 'distributions', 'curve', 'parameters', 'distribution', 'binomial', 'continuous', 'probabilities', 'probability distribution', 'normal', 'normal distribution']", Normal Approximation to the Binomial,seg_125,"are discrete. the first application of a continuous probability distribution to approximate probabilities over a discrete sample space was demonstrated in example 6.12, where the normal curve was used. the normal distribution is often a good approximation to a discrete distribution when the latter takes on a symmetric bell shape. from a theoretical point of view, some distributions converge to the normal as their parameters approach certain limits. the normal distribution is a convenient approximating distribution because the cumulative distribution function is so easily tabled. the binomial distribution is nicely approximated by the normal in practical problems when one works with the cumulative distribution function. we now state a theorem that allows us to use areas under the normal curve to approximate binomial properties when n is sufficiently large."
1092,1,"['distribution', 'random variable', 'variable', 'binomial random variable', 'mean', 'binomial', 'random', 'variance']", Normal Approximation to the Binomial,seg_125,"theorem 6.3: if x is a binomial random variable with mean μ = np and variance σ2 = npq, then the limiting form of the distribution of"
1093,1,"['standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Normal Approximation to the Binomial,seg_125,"as n → ∞, is the standard normal distribution n(z; 0, 1)."
1094,1,"['approximation', 'distribution', 'normal', 'binomial', 'binomial distribution', 'normal distribution']", Normal Approximation to the Binomial,seg_125,it turns out that the normal distribution with μ = np and σ2 = np(1− p) not only provides a very accurate approximation to the binomial distribution when n is large and p is not extremely close to 0 or 1 but also provides a fairly good approximation even when n is small and p is reasonably close to 1/2.
1095,1,"['curve', 'normal approximation', 'histogram', 'approximation', 'distribution', 'variable', 'normal', 'mean', 'binomial', 'binomial distribution', 'variance']", Normal Approximation to the Binomial,seg_125,"to illustrate the normal approximation to the binomial distribution, we first draw the histogram for b(x; 15, 0.4) and then superimpose the particular normal curve having the same mean and variance as the binomial variable x. hence, we draw a normal curve with"
1096,1,"['histogram', 'curve', 'normal', 'mean', 'variance']", Normal Approximation to the Binomial,seg_125,"the histogram of b(x; 15, 0.4) and the corresponding superimposed normal curve, which is completely determined by its mean and variance, are illustrated in figure 6.22."
1097,1,"['normal approximation', 'approximation', 'normal']", Normal Approximation to the Binomial,seg_125,"figure 6.22: normal approximation of b(x; 15, 0.4)."
1098,1,"['table', 'random variable', 'variable', 'binomial random variable', 'binomial', 'probability', 'random']", Normal Approximation to the Binomial,seg_125,"the exact probability that the binomial random variable x assumes a given value x is equal to the area of the bar whose base is centered at x. for example, the exact probability that x assumes the value 4 is equal to the area of the rectangle with base centered at x = 4. using table a.1, we find this area to be"
1099,1,"['curve', 'normal']", Normal Approximation to the Binomial,seg_125,"which is approximately equal to the area of the shaded region under the normal curve between the two ordinates x1 = 3.5 and x2 = 4.5 in figure 6.23. converting to z values, we have"
1100,1,"['standard', 'standard normal', 'random variable', 'variable', 'normal', 'binomial random variable', 'binomial', 'random']", Normal Approximation to the Binomial,seg_125,"if x is a binomial random variable and z a standard normal variable, then"
1101,0,[], Normal Approximation to the Binomial,seg_125,this agrees very closely with the exact value of 0.1268.
1102,1,"['normal approximation', 'approximation', 'normal', 'binomial', 'probability']", Normal Approximation to the Binomial,seg_125,"the normal approximation is most useful in calculating binomial sums for large values of n. referring to figure 6.23, we might be interested in the probability that x assumes a value from 7 to 9 inclusive. the exact probability is given by"
1103,1,"['normal approximation', 'curve', 'approximation', 'normal']", Normal Approximation to the Binomial,seg_125,"which is equal to the sum of the areas of the rectangles with bases centered at x = 7, 8, and 9. for the normal approximation, we find the area of the shaded region under the curve between the ordinates x1 = 6.5 and x2 = 9.5 in figure 6.23. the corresponding z values are"
1104,1,"['curve', 'histogram', 'symmetric', 'approximation', 'normal', 'histograms']", Normal Approximation to the Binomial,seg_125,"once again, the normal curve approximation provides a value that agrees very closely with the exact value of 0.3564. the degree of accuracy, which depends on how well the curve fits the histogram, will increase as n increases. this is particularly true when p is not very close to 1/2 and the histogram is no longer symmetric. figures 6.24 and 6.25 show the histograms for b(x; 6, 0.2) and b(x; 15, 0.2), respectively. it is evident that a normal curve would fit the histogram considerably better when n = 15 than when n = 6."
1105,1,['histogram'], Normal Approximation to the Binomial,seg_125,"figure 6.24: histogram for b(x; 6, 0.2). figure 6.25: histogram for b(x; 15, 0.2)."
1106,1,"['curve', 'normal approximation', 'continuous distribution', 'approximation', 'discrete', 'distribution', 'normal', 'binomial', 'continuous', 'discrete distribution', 'continuity correction']", Normal Approximation to the Binomial,seg_125,"in our illustrations of the normal approximation to the binomial, it became apparent that if we seek the area under the normal curve to the left of, say, x, it is more accurate to use x + 0.5. this is a correction to accommodate the fact that a discrete distribution is being approximated by a continuous distribution. the correction +0.5 is called a continuity correction. the foregoing discussion leads to the following formal normal approximation to the binomial."
1107,1,"['parameters', 'approximation', 'distribution', 'random variable', 'variable', 'normal', 'binomial random variable', 'binomial', 'random', 'normal distribution']", Normal Approximation to the Binomial,seg_125,"normal let x be a binomial random variable with parameters n and p. for large n, x approximation to has approximately a normal distribution with μ = np and σ2 = npq = np(1−p) the binomial and"
1108,1,"['curve', 'normal']", Normal Approximation to the Binomial,seg_125,"p (x ≤ x) = ∑ b(k;n, p) k=0 ≈ area under normal curve to the left of x+ 0.5"
1109,1,['approximation'], Normal Approximation to the Binomial,seg_125,and the approximation will be good if np and n(1− p) are greater than or equal to 5.
1110,1,"['sample size', 'sample', 'table', 'approximation']", Normal Approximation to the Binomial,seg_125,"as we indicated earlier, the quality of the approximation is quite good for large n. if p is close to 1/2, a moderate or small sample size will be sufficient for a reasonable approximation. we offer table 6.1 as an indication of the quality of the"
1111,1,"['normal approximation', 'probabilities', 'approximation', 'normal', 'binomial']", Normal Approximation to the Binomial,seg_125,"approximation. both the normal approximation and the true binomial cumulative probabilities are given. notice that at p = 0.05 and p = 0.10, the approximation is fairly crude for n = 10. however, even for n = 10, note the improvement for p = 0.50. on the other hand, when p is fixed at p = 0.05, note the improvement of the approximation as we go from n = 20 to n = 100."
1112,1,"['normal approximation', 'probabilities', 'approximation', 'normal', 'binomial']", Normal Approximation to the Binomial,seg_125,table 6.1: normal approximation and true cumulative binomial probabilities
1113,1,"['normal', 'binomial']", Normal Approximation to the Binomial,seg_125,"p = 0.05, n = 10 p = 0.10, n = 10 p = 0.50, n = 10 r binomial normal binomial normal binomial normal 0 0.5987 0.5000 0.3487 0.2981 0.0010 0.0022 1 0.9139 0.9265 0.7361 0.7019 0.0107 0.0136 2 0.9885 0.9981 0.9298 0.9429 0.0547 0.0571 3 0.9990 1.0000 0.9872 0.9959 0.1719 0.1711 4 1.0000 1.0000 0.9984 0.9999 0.3770 0.3745 5 1.0000 1.0000 0.6230 0.6255 6 0.8281 0.8289 7 0.9453 0.9429 8 0.9893 0.9864 9 0.9990 0.9978 10 1.0000 0.9997 p = 0.05 n = 20 n = 50 n = 100 r binomial normal binomial normal binomial normal 0 0.3585 0.3015 0.0769 0.0968 0.0059 0.0197 1 0.7358 0.6985 0.2794 0.2578 0.0371 0.0537 2 0.9245 0.9382 0.5405 0.5000 0.1183 0.1251 3 0.9841 0.9948 0.7604 0.7422 0.2578 0.2451 4 0.9974 0.9998 0.8964 0.9032 0.4360 0.4090 5 0.9997 1.0000 0.9622 0.9744 0.6160 0.5910 6 1.0000 1.0000 0.9882 0.9953 0.7660 0.7549 7 0.9968 0.9994 0.8720 0.8749 8 0.9992 0.9999 0.9369 0.9463 9 0.9998 1.0000 0.9718 0.9803 10 1.0000 1.0000 0.9885 0.9941"
1114,1,['probability'], Normal Approximation to the Binomial,seg_125,"example 6.15: the probability that a patient recovers from a rare blood disease is 0.4. if 100 people are known to have contracted this disease, what is the probability that fewer than 30 survive?"
1115,1,"['results', 'variable', 'binomial']", Normal Approximation to the Binomial,seg_125,"solution : let the binomial variable x represent the number of patients who survive. since n = 100, we should obtain fairly accurate results using the normal-curve approximation with"
1116,1,['probability'], Normal Approximation to the Binomial,seg_125,"to obtain the desired probability, we have to find the area to the left of x = 29.5."
1117,0,[], Normal Approximation to the Binomial,seg_125,the z value corresponding to 29.5 is
1118,1,['probability'], Normal Approximation to the Binomial,seg_125,"and the probability of fewer than 30 of the 100 patients surviving is given by the shaded region in figure 6.26. hence,"
1119,1,['probability'], Normal Approximation to the Binomial,seg_125,"example 6.16: a multiple-choice quiz has 200 questions, each with 4 possible answers of which only 1 is correct. what is the probability that sheer guesswork yields from 25 to 30 correct answers for the 80 of the 200 problems about which the student has no knowledge?"
1120,1,['probability'], Normal Approximation to the Binomial,seg_125,"solution : the probability of guessing a correct answer for each of the 80 questions is p = 1/4. if x represents the number of correct answers resulting from guesswork, then"
1121,1,"['curve', 'approximation', 'normal']", Normal Approximation to the Binomial,seg_125,using the normal curve approximation with
1122,0,[], Normal Approximation to the Binomial,seg_125,we need the area between x1 = 24.5 and x2 = 30.5. the corresponding z values are
1123,1,"['probability', 'table']", Normal Approximation to the Binomial,seg_125,the probability of correctly guessing from 25 to 30 questions is given by the shaded region in figure 6.27. from table a.3 we find that
1124,1,"['functions', 'gamma', 'density functions', 'gamma and exponential distributions', 'distribution', 'exponential distributions', 'normal', 'exponential', 'normal distribution', 'distributions']", Gamma and Exponential Distributions,seg_129,"although the normal distribution can be used to solve many problems in engineering and science, there are still numerous situations that require different types of density functions. two such density functions, the gamma and exponential distributions, are discussed in this section."
1125,1,"['exponential distribution', 'exponential and gamma distributions', 'gamma', 'failure', 'case', 'distribution', 'gamma distributions', 'exponential', 'gamma distribution', 'distributions']", Gamma and Exponential Distributions,seg_129,it turns out that the exponential distribution is a special case of the gamma distribution. both find a large number of applications. the exponential and gamma distributions play an important role in both queuing theory and reliability problems. time between arrivals at service facilities and time to failure of component parts and electrical systems often are nicely modeled by the exponential distribution. the relationship between the gamma and the exponential allows the gamma to be used in similar types of problems. more details and illustrations will be supplied later in the section.
1126,1,"['gamma', 'distribution', 'function', 'gamma distribution']", Gamma and Exponential Distributions,seg_129,"the gamma distribution derives its name from the well-known gamma function, studied in many areas of mathematics. before we proceed to the gamma distribution, let us review this function and some of its important properties."
1127,1,"['function', 'gamma', 'gamma function']", Gamma and Exponential Distributions,seg_129,definition 6.2: the gamma function is defined by
1128,1,"['function', 'gamma', 'gamma function']", Gamma and Exponential Distributions,seg_129,the following are a few simple properties of the gamma function.
1129,0,[], Gamma and Exponential Distributions,seg_129,"for α > 1, which yields the recursion formula"
1130,0,[], Gamma and Exponential Distributions,seg_129,"the result follows after repeated application of the recursion formula. using this result, we can easily show the following two properties."
1131,0,[], Gamma and Exponential Distributions,seg_129,"furthermore, we have the following property of γ(α), which is left for the reader to verify (see exercise 6.39 on page 206)."
1132,1,"['distribution', 'gamma distribution', 'gamma']", Gamma and Exponential Distributions,seg_129,the following is the definition of the gamma distribution.
1133,1,"['density function', 'gamma', 'continuous', 'continuous random variable', 'distribution', 'random variable', 'variable', 'random', 'function', 'gamma distribution']", Gamma and Exponential Distributions,seg_129,"gamma the continuous random variable x has a gamma distribution, with paramdistribution eters α and β, if its density function is given by"
1134,1,"['exponential distribution', 'gamma', 'parameters', 'distribution', 'gamma distributions', 'exponential', 'gamma distribution', 'distributions']", Gamma and Exponential Distributions,seg_129,graphs of several gamma distributions are shown in figure 6.28 for certain specified values of the parameters α and β. the special gamma distribution for which α = 1 is called the exponential distribution.
1135,1,"['gamma', 'gamma distributions', 'distributions']", Gamma and Exponential Distributions,seg_129,figure 6.28: gamma distributions.
1136,1,"['density function', 'exponential distribution', 'continuous', 'continuous random variable', 'distribution', 'random variable', 'variable', 'exponential', 'parameter', 'random', 'function']", Gamma and Exponential Distributions,seg_129,"exponential the continuous random variable x has an exponential distribution, with distribution parameter β, if its density function is given by"
1137,1,"['gamma', 'gamma and exponential distributions', 'exponential distributions', 'exponential', 'mean', 'variance', 'distributions']", Gamma and Exponential Distributions,seg_129,the following theorem and corollary give the mean and variance of the gamma and exponential distributions.
1138,1,"['gamma', 'distribution', 'mean', 'variance', 'gamma distribution']", Gamma and Exponential Distributions,seg_129,the mean and variance of the gamma distribution are
1139,0,[], Gamma and Exponential Distributions,seg_129,the proof of this theorem is found in appendix a.26.
1140,1,"['exponential distribution', 'distribution', 'exponential', 'mean', 'variance']", Gamma and Exponential Distributions,seg_129,corollary 6.1: the mean and variance of the exponential distribution are
1141,1,"['discrete', 'probability', 'random', 'process', 'gamma distribution', 'exponential distribution', 'random variable', 'exponential', 'event', 'poisson process', 'gamma', 'distribution', 'poisson distribution', 'poisson', 'intersection', 'variable']", Gamma and Exponential Distributions,seg_129,"we shall pursue applications of the exponential distribution and then return to the gamma distribution. the most important applications of the exponential distribution are situations where the poisson process applies (see section 5.5). the reader should recall that the poisson process allows for the use of the discrete distribution called the poisson distribution. recall that the poisson distribution is used to compute the probability of specific numbers of “events” during a particular period of time or span of space. in many applications, the time period or span of space is the random variable. for example, an industrial engineer may be interested in modeling the time t between arrivals at a congested intersection during rush hour in a large city. an arrival represents the poisson event."
1142,1,"['poisson', 'random', 'process', 'exponential distribution', 'poisson process', 'distribution', 'random variable', 'events', 'variable', 'exponential', 'mean', 'probability', 'event', 'poisson distribution', 'parameter']", Gamma and Exponential Distributions,seg_129,"the relationship between the exponential distribution (often called the negative exponential) and the poisson process is quite simple. in chapter 5, the poisson distribution was developed as a single-parameter distribution with parameter λ, where λ may be interpreted as the mean number of events per unit “time.” consider now the random variable described by the time required for the first event to occur. using the poisson distribution, we find that the probability of no events occurring in the span up to time t is given by"
1143,1,"['poisson', 'events', 'probability', 'event']", Gamma and Exponential Distributions,seg_129,"we can now make use of the above and let x be the time to the first poisson event. the probability that the length of time until the first event will exceed x is the same as the probability that no poisson events will occur in x. the latter, of"
1144,1,"['cumulative distribution function', 'distribution function', 'distribution', 'function']", Gamma and Exponential Distributions,seg_129,"thus, the cumulative distribution function for x is given by"
1145,1,"['cumulative distribution function', 'exponential distribution', 'distribution function', 'distribution', 'exponential', 'function']", Gamma and Exponential Distributions,seg_129,"now, in order that we may recognize the presence of the exponential distribution, we differentiate the cumulative distribution function above to obtain the density"
1146,1,"['density function', 'exponential distribution', 'distribution', 'exponential', 'function']", Gamma and Exponential Distributions,seg_129,which is the density function of the exponential distribution with λ = 1/β.
1147,1,"['memory', 'process', 'gamma distribution', 'exponential distribution', 'events', 'exponential', 'mean', 'parameter', 'event', 'poisson process', 'gamma', 'failure', 'distribution', 'poisson distribution', 'response', 'poisson', 'independent', 'failures', 'experiments']", Gamma and Exponential Distributions,seg_129,"in the foregoing, we provided the foundation for the application of the exponential distribution in “time to arrival” or time to poisson event problems. we will illustrate some applications here and then proceed to discuss the role of the gamma distribution in these modeling applications. notice that the mean of the exponential distribution is the parameter β, the reciprocal of the parameter in the poisson distribution. the reader should recall that it is often said that the poisson distribution has no memory, implying that occurrences in successive time periods are independent. the important parameter β is the mean time between events. in reliability theory, where equipment failure often conforms to this poisson process, β is called mean time between failures. many equipment breakdowns do follow the poisson process, and thus the exponential distribution does apply. other applications include survival times in biomedical experiments and computer response time."
1148,1,"['distribution', 'exponential', 'binomial', 'binomial distribution']", Gamma and Exponential Distributions,seg_129,"in the following example, we show a simple application of the exponential distribution to a problem in reliability. the binomial distribution also plays a role in the solution."
1149,1,"['exponential distribution', 'failure', 'distribution', 'random variable', 'variable', 'exponential', 'mean', 'probability', 'random']", Gamma and Exponential Distributions,seg_129,"example 6.17: suppose that a system contains a certain type of component whose time, in years, to failure is given by t . the random variable t is modeled nicely by the exponential distribution with mean time to failure β = 5. if 5 of these components are installed in different systems, what is the probability that at least 2 are still functioning at the end of 8 years?"
1150,1,['probability'], Gamma and Exponential Distributions,seg_129,solution : the probability that a given component is still functioning after 8 years is given by
1151,1,"['distribution', 'binomial', 'binomial distribution']", Gamma and Exponential Distributions,seg_129,"let x represent the number of components functioning after 8 years. then using the binomial distribution, we have"
1152,1,"['distribution', 'exponential', 'exponential distribution']", Gamma and Exponential Distributions,seg_129,"5 1 p (x ≥ 2) =∑ b(x; 5, 0.2) = 1−∑ b(x; 5, 0.2) = 1− 0.7373 = 0.2627. x=2 x=0 there are exercises and examples in chapter 3 where the reader has already encountered the exponential distribution. others involving waiting time and reliability include example 6.24 and some of the exercises and review exercises at the end of this chapter."
1153,1,"['memory', 'exponential distribution', 'case', 'distribution', 'exponential']", Gamma and Exponential Distributions,seg_129,"the types of applications of the exponential distribution in reliability and component or machine lifetime problems are influenced by the memoryless (or lack-of- memory) property of the exponential distribution. for example, in the case of,"
1154,1,"['exponential distribution', 'conditional', 'distribution', 'exponential', 'probability', 'conditional probability']", Gamma and Exponential Distributions,seg_129,"say, an electronic component where lifetime has an exponential distribution, the probability that the component lasts, say, t hours, that is, p (x ≥ t), is the same as the conditional probability"
1155,1,"['exponential distribution', 'gamma', 'memoryless property', 'failure', 'distribution', 'exponential', 'probability', 'weibull distribution', 'weibull']", Gamma and Exponential Distributions,seg_129,"so if the component “makes it” to t0 hours, the probability of lasting an additional t hours is the same as the probability of lasting t hours. there is no “punish- ment” through wear that may have ensued for lasting the first t0 hours. thus, the exponential distribution is more appropriate when the memoryless property is justified. but if the failure of the component is a result of gradual or slow wear (as in mechanical wear), then the exponential does not apply and either the gamma or the weibull distribution (section 6.10) may be more appropriate."
1156,1,"['case', 'random', 'function', 'process', 'gamma distribution', 'numerical', 'exponential distribution', 'cases', 'random variable', 'events', 'exponential', 'event', 'distributions', 'density function', 'poisson process', 'gamma', 'distribution', 'poisson', 'variable']", Gamma and Exponential Distributions,seg_129,"the importance of the gamma distribution lies in the fact that it defines a family of which other distributions are special cases. but the gamma itself has important applications in waiting time and reliability theory. whereas the exponential distribution describes the time until the occurrence of a poisson event (or the time between poisson events), the time (or space) occurring until a specified number of poisson events occur is a random variable whose density function is described by the gamma distribution. this specific number of events is the parameter α in the gamma density function. thus, it becomes easy to understand that when α = 1, the special case of the exponential distribution occurs. the gamma density can be developed from its relationship to the poisson process in much the same manner as we developed the exponential density. the details are left to the reader. the following is a numerical example of the use of the gamma distribution in a waiting-time application."
1157,1,"['poisson', 'process', 'poisson process', 'probability', 'average']", Gamma and Exponential Distributions,seg_129,example 6.18: suppose that telephone calls arriving at a particular switchboard follow a poisson process with an average of 5 calls coming per minute. what is the probability that up to a minute will elapse by the time 2 calls have come in to the switchboard?
1158,1,"['poisson', 'poisson process', 'gamma', 'distribution', 'events', 'probability', 'process', 'gamma distribution']", Gamma and Exponential Distributions,seg_129,"solution : the poisson process applies, with time until 2 poisson events following a gamma distribution with β = 1/5 and α = 2. denote by x the time in minutes that transpires before 2 calls come. the required probability is given by"
1159,1,"['poisson', 'gamma', 'distribution', 'events', 'gamma distribution']", Gamma and Exponential Distributions,seg_129,"while the origin of the gamma distribution deals in time (or space) until the occurrence of α poisson events, there are many instances where a gamma distribution works very well even though there is no clear poisson structure. this is particularly true for survival time problems in both engineering and biomedical applications."
1160,1,"['distribution', 'gamma', 'gamma distribution', 'probability']", Gamma and Exponential Distributions,seg_129,"example 6.19: in a biomedical study with rats, a dose-response investigation is used to determine the effect of the dose of a toxicant on their survival time. the toxicant is one that is frequently discharged into the atmosphere from jet fuel. for a certain dose of the toxicant, the study determines that the survival time, in weeks, has a gamma distribution with α = 5 and β = 10. what is the probability that a rat survives no longer than 60 weeks?"
1161,1,"['random variable', 'variable', 'probability', 'random']", Gamma and Exponential Distributions,seg_129,solution : let the random variable x be the survival time (time to death). the required probability is
1162,1,"['cumulative distribution function', 'gamma', 'distribution function', 'distribution', 'function', 'gamma function']", Gamma and Exponential Distributions,seg_129,"the integral above can be solved through the use of the incomplete gamma function, which becomes the cumulative distribution function for the gamma distribution. this function is written as"
1163,1,"['probabilities', 'gamma', 'table', 'distribution', 'probability', 'function', 'gamma distribution', 'gamma function']", Gamma and Exponential Distributions,seg_129,"which is denoted as f (6; 5) in the table of the incomplete gamma function in appendix a.23. note that this allows a quick computation of probabilities for the gamma distribution. indeed, for this problem, the probability that the rat survives no longer than 60 days is given by"
1164,1,"['gamma', 'data', 'distribution', 'control', 'quality control', 'gamma distribution']", Gamma and Exponential Distributions,seg_129,"example 6.20: it is known, from previous data, that the length of time in months between customer complaints about a certain product is a gamma distribution with α = 2 and β = 4. changes were made to tighten quality control requirements. following these changes, 20 months passed before the first complaint. does it appear as if the quality control tightening was effective?"
1165,1,"['distribution', 'gamma', 'gamma distribution']", Gamma and Exponential Distributions,seg_129,"solution : let x be the time to the first complaint, which, under conditions prior to the changes, followed a gamma distribution with α = 2 and β = 4. the question centers around how rare x ≥ 20 is, given that α and β remain at values 2 and 4, respectively. in other words, under the prior conditions is a “time to complaint” as large as 20 months reasonable? thus, following the solution to example 6.19,"
1166,1,['table'], Gamma and Exponential Distributions,seg_129,where f (5; 2) = 0.96 is found from table a.23.
1167,1,"['gamma', 'data', 'distribution', 'control', 'quality control', 'gamma distribution']", Gamma and Exponential Distributions,seg_129,"as a result, we could conclude that the conditions of the gamma distribution with α = 2 and β = 4 are not supported by the data that an observed time to complaint is as large as 20 months. thus, it is reasonable to conclude that the quality control work was effective."
1168,1,"['function', 'density function']", Gamma and Exponential Distributions,seg_129,"example 6.21: consider exercise 3.31 on page 94. based on extensive testing, it is determined that the time y in years before a major repair is required for a certain washing machine is characterized by the density function"
1169,1,"['random variable', 'variable', 'exponential', 'probability', 'random']", Gamma and Exponential Distributions,seg_129,note that y is an exponential random variable with μ = 4 years. the machine is considered a bargain if it is unlikely to require a major repair before the sixth year. what is the probability p (y > 6)? what is the probability that a major repair is required in the first year?
1170,1,"['cumulative distribution function', 'exponential distribution', 'distribution function', 'distribution', 'exponential', 'function']", Gamma and Exponential Distributions,seg_129,"solution : consider the cumulative distribution function f (y) for the exponential distribution,"
1171,1,['probability'], Gamma and Exponential Distributions,seg_129,"thus, the probability that the washing machine will require major repair after year six is 0.223. of course, it will require repair before year six with probability 0.777. thus, one might conclude the machine is not really a bargain. the probability that a major repair is necessary in the first year is"
1172,1,"['degrees of freedom', 'gamma', 'case', 'distribution', 'parameter', 'gamma distribution']", ChiSquared Distribution,seg_131,"another very important special case of the gamma distribution is obtained by letting α = v/2 and β = 2, where v is a positive integer. the result is called the chi-squared distribution. the distribution has a single parameter, v, called the degrees of freedom."
1173,1,"['density function', 'degrees of freedom', 'continuous', 'continuous random variable', 'distribution', 'random variable', 'variable', 'random', 'function']", ChiSquared Distribution,seg_131,"chi-squared the continuous random variable x has a chi-squared distribution, with v distribution degrees of freedom, if its density function is given by"
1174,0,[], ChiSquared Distribution,seg_131,where v is a positive integer.
1175,1,"['statistical inference', 'estimation', 'statistical hypothesis', 'distribution', 'statistical hypothesis testing', 'statistical', 'hypothesis testing', 'hypothesis']", ChiSquared Distribution,seg_131,"the chi-squared distribution plays a vital role in statistical inference. it has considerable applications in both methodology and theory. while we do not discuss applications in detail in this chapter, it is important to understand that chapters 8, 9, and 16 contain important applications. the chi-squared distribution is an important component of statistical hypothesis testing and estimation."
1176,1,"['statistics', 'sampling', 'distribution', 'analysis of variance', 'sampling distributions', 'variance', 'distributions']", ChiSquared Distribution,seg_131,"topics dealing with sampling distributions, analysis of variance, and nonparametric statistics involve extensive use of the chi-squared distribution."
1177,1,"['distribution', 'mean', 'variance']", ChiSquared Distribution,seg_131,the mean and variance of the chi-squared distribution are
1178,1,"['beta function', 'uniform distribution', 'distribution', 'function', 'beta distribution']", Beta Distribution,seg_133,an extension to the uniform distribution is a beta distribution. let us start by defining a beta function.
1179,1,"['function', 'beta function']", Beta Distribution,seg_133,definition 6.3: a beta function is defined by
1180,1,"['function', 'gamma', 'gamma function']", Beta Distribution,seg_133,where γ(α) is the gamma function.
1181,1,"['density function', 'parameters', 'continuous', 'continuous random variable', 'distribution', 'random variable', 'variable', 'beta distribution', 'random', 'function']", Beta Distribution,seg_133,beta distribution the continuous random variable x has a beta distribution with parameters α > 0 and β > 0 if its density function is given by
1182,1,"['parameters', 'uniform distribution', 'distribution', 'beta distribution']", Beta Distribution,seg_133,"note that the uniform distribution on (0, 1) is a beta distribution with parameters α = 1 and β = 1."
1183,1,"['parameters', 'distribution', 'mean', 'variance', 'beta distribution']", Beta Distribution,seg_133,the mean and variance of a beta distribution with parameters α and β are
1184,1,"['distribution', 'uniform distribution', 'variance', 'mean']", Beta Distribution,seg_133,"for the uniform distribution on (0, 1), the mean and variance are"
1185,1,"['lognormal', 'results', 'lognormal distribution', 'distribution', 'cases', 'normal', 'transformation', 'normal distribution']", Lognormal Distribution,seg_135,the lognormal distribution is used for a wide variety of applications. the distribution applies in cases where a natural log transformation results in a normal distribution.
1186,1,"['lognormal', 'lognormal distribution', 'random', 'function', 'random variable', 'mean', 'standard', 'standard deviation', 'density function', 'distribution', 'continuous', 'continuous random variable', 'deviation', 'variable', 'normal', 'normal distribution']", Lognormal Distribution,seg_135,lognormal the continuous random variable x has a lognormal distribution if the randistribution dom variable y = ln(x) has a normal distribution with mean μ and standard deviation σ. the resulting density function of x is
1187,1,"['lognormal', 'lognormal distributions', 'distributions']", Lognormal Distribution,seg_135,figure 6.29: lognormal distributions.
1188,1,"['lognormal', 'lognormal distributions', 'distributions']", Lognormal Distribution,seg_135,the graphs of the lognormal distributions are illustrated in figure 6.29.
1189,1,"['lognormal', 'lognormal distribution', 'distribution', 'mean', 'variance']", Lognormal Distribution,seg_135,the mean and variance of the lognormal distribution are
1190,1,"['cumulative distribution function', 'distribution function', 'distribution', 'normal', 'function', 'normal distribution']", Lognormal Distribution,seg_135,the cumulative distribution function is quite simple due to its relationship to the normal distribution. the use of the distribution function is illustrated by the following example.
1191,1,"['lognormal', 'parameters', 'lognormal distribution', 'distribution', 'probability', 'concentration']", Lognormal Distribution,seg_135,"example 6.22: concentrations of pollutants produced by chemical plants historically are known to exhibit behavior that resembles a lognormal distribution. this is important when one considers issues regarding compliance with government regulations. suppose it is assumed that the concentration of a certain pollutant, in parts per million, has a lognormal distribution with parameters μ = 3.2 and σ = 1. what is the probability that the concentration exceeds 8 parts per million?"
1192,1,"['random variable', 'variable', 'random', 'concentration']", Lognormal Distribution,seg_135,solution : let the random variable x be pollutant concentration. then
1193,1,"['deviation', 'distribution', 'normal', 'mean', 'standard', 'standard deviation', 'normal distribution']", Lognormal Distribution,seg_135,"since ln(x) has a normal distribution with mean μ = 3.2 and standard deviation σ = 1,"
1194,1,"['cumulative distribution function', 'standard normal', 'distribution function', 'concentration', 'distribution', 'normal', 'probability', 'standard', 'function', 'standard normal distribution', 'normal distribution']", Lognormal Distribution,seg_135,"here, we use φ to denote the cumulative distribution function of the standard normal distribution. as a result, the probability that the pollutant concentration exceeds 8 parts per million is 0.1314."
1195,1,"['lognormal', 'percentile', 'lognormal distribution', 'distribution', 'control']", Lognormal Distribution,seg_135,"example 6.23: the life, in thousands of miles, of a certain type of electronic control for locomotives has an approximately lognormal distribution with μ = 5.149 and σ = 0.737. find the 5th percentile of the life of such an electronic control."
1196,1,"['percentile', 'table', 'distribution', 'normal', 'mean', 'control', 'normal distribution']", Lognormal Distribution,seg_135,"solution : from table a.3, we know that p (z < −1.645) = 0.05. denote by x the life of such an electronic control. since ln(x) has a normal distribution with mean μ = 5.149 and σ = 0.737, the 5th percentile of x can be calculated as"
1197,0,[], Lognormal Distribution,seg_135,"hence, x = 51.265. this means that only 5% of the controls will have lifetimes less than 51,265 miles."
1198,1,"['gamma', 'design', 'gamma and exponential distributions', 'distribution', 'exponential distributions', 'exponential', 'weibull distribution', 'weibull', 'distributions']", Weibull Distribution Optional,seg_137,"modern technology has enabled engineers to design many complicated systems whose operation and safety depend on the reliability of the various components making up the systems. for example, a fuse may burn out, a steel column may buckle, or a heat-sensing device may fail. identical components subjected to identical environmental conditions will fail at different and unpredictable times. we have seen the role that the gamma and exponential distributions play in these types of problems. another distribution that has been used extensively in recent years to deal with such problems is the weibull distribution, introduced by the swedish physicist waloddi weibull in 1939."
1199,1,"['density function', 'continuous', 'continuous random variable', 'distribution', 'random variable', 'variable', 'random', 'function', 'weibull distribution', 'weibull']", Weibull Distribution Optional,seg_137,"weibull the continuous random variable x has a weibull distribution, with paramdistribution eters α and β, if its density function is given by"
1200,1,"['curve', 'exponential distribution', 'weibull distribution', 'distribution', 'normal', 'exponential', 'parameter', 'skewness', 'weibull']", Weibull Distribution Optional,seg_137,"the graphs of the weibull distribution for α = 1 and various values of the parameter β are illustrated in figure 6.30. we see that the curves change considerably in shape for different values of the parameter β. if we let β = 1, the weibull distribution reduces to the exponential distribution. for values of β > 1, the curves become somewhat bell shaped and resemble the normal curve but display some skewness."
1201,1,"['distribution', 'mean', 'variance', 'weibull distribution', 'weibull']", Weibull Distribution Optional,seg_137,the mean and variance of the weibull distribution are stated in the following theorem. the reader is asked to provide the proof in exercise 6.52 on page 206.
1202,1,"['distribution', 'mean', 'variance', 'weibull distribution', 'weibull']", Weibull Distribution Optional,seg_137,the mean and variance of the weibull distribution are
1203,1,"['gamma', 'gamma and exponential distributions', 'failure', 'distribution', 'exponential distributions', 'exponential', 'weibull distribution', 'weibull', 'distributions']", Weibull Distribution Optional,seg_137,"like the gamma and exponential distributions, the weibull distribution is also applied to reliability and life-testing problems such as the time to failure or"
1204,1,"['weibull', 'distributions']", Weibull Distribution Optional,seg_137,figure 6.30: weibull distributions (α = 1).
1205,1,"['memory', 'distribution function', 'probability', 'random', 'function', 'cumulative distribution function', 'exponential distribution', 'random variable', 'exponential', 'density function', 'probability density function', 'failure', 'distribution', 'continuous', 'continuous random variable', 'weibull distribution', 'weibull', 'probabilities', 'variable']", Weibull Distribution Optional,seg_137,"life length of a component, measured from some specified time until it fails. let us represent this time to failure by the continuous random variable t , with probability density function f(t), where f(t) is the weibull distribution. the weibull distribution has inherent flexibility in that it does not require the lack of memory property of the exponential distribution. the cumulative distribution function (cdf) for the weibull can be written in closed form and certainly is useful in computing probabilities."
1206,1,"['cumulative distribution function', 'distribution function', 'distribution', 'function', 'weibull distribution', 'weibull']", Weibull Distribution Optional,seg_137,cdf for weibull the cumulative distribution function for the weibull distribution is distribution given by
1207,1,"['weibull', 'probability']", Weibull Distribution Optional,seg_137,"example 6.24: the length of life x, in hours, of an item in a machine shop has a weibull distribution with α = 0.01 and β = 2. what is the probability that it fails before eight hours of usage?"
1208,1,"['rate', 'experimental', 'distribution', 'probability', 'function', 'weibull distribution', 'weibull']", Weibull Distribution Optional,seg_137,"when the weibull distribution applies, it is often helpful to determine the failure rate (sometimes called the hazard rate) in order to get a sense of wear or deterioration of the component. let us first define the reliability of a component or product as the probability that it will function properly for at least a specified time under specified experimental conditions. therefore, if r(t) is defined to be"
1209,0,[], Weibull Distribution Optional,seg_137,"the reliability of the given component at time t, we may write"
1210,1,"['cumulative distribution function', 'interval', 'distribution function', 'conditional', 'distribution', 'probability', 'conditional probability', 'function']", Weibull Distribution Optional,seg_137,"where f (t) is the cumulative distribution function of t . the conditional probability that a component will fail in the interval from t = t to t = t +δt, given that it survived to time t, is"
1211,1,"['rate', 'failure rate', 'failure', 'limit']", Weibull Distribution Optional,seg_137,"dividing this ratio by δt and taking the limit as δt → 0, we get the failure rate, denoted by z(t). hence,"
1212,1,"['rate', 'failure rate', 'failure', 'distribution']", Weibull Distribution Optional,seg_137,which expresses the failure rate in terms of the distribution of the time to failure.
1213,1,"['rate', 'failure rate', 'failure']", Weibull Distribution Optional,seg_137,"since z(t) = f(t)/[1− f (t)], the failure rate is given as follows:"
1214,1,"['rate', 'failure rate', 'failure', 'distribution', 'weibull distribution', 'weibull']", Weibull Distribution Optional,seg_137,failure rate for the failure rate at time t for the weibull distribution is given by
1215,1,['distribution'], Weibull Distribution Optional,seg_137,"weibull z(t) = αβtβ−1, t > 0. distribution"
1216,1,"['rate', 'failure rate', 'failure', 'conditional', 'probability', 'conditional probability']", Weibull Distribution Optional,seg_137,the quantity z(t) is aptly named as a failure rate since it does quantify the rate of change over time of the conditional probability that the component lasts an additional δt given that it has lasted to time t. the rate of decrease (or increase) with time is important. the following are crucial points.
1217,1,"['rate', 'failure rate', 'rate ', 'failure']", Weibull Distribution Optional,seg_137,"(a) if β = 1, the failure rate = α, a constant. this, as indicated earlier, is the"
1218,1,"['memory', 'exponential distribution', 'case', 'distribution', 'exponential']", Weibull Distribution Optional,seg_137,special case of the exponential distribution in which lack of memory prevails.
1219,1,['function'], Weibull Distribution Optional,seg_137,"(b) if β > 1, z(t) is an increasing function of time t, which indicates that the"
1220,0,[], Weibull Distribution Optional,seg_137,component wears over time.
1221,1,['function'], Weibull Distribution Optional,seg_137,"(c) if β < 1, z(t) is a decreasing function of time t and hence the component"
1222,0,[], Weibull Distribution Optional,seg_137,strengthens or hardens over time.
1223,1,"['rate', 'parameters', 'failure rate', 'failure', 'case', 'failure rate function', 'rate function', 'function']", Weibull Distribution Optional,seg_137,"for example, the item in the machine shop in example 6.24 has β = 2, and hence it wears over time. in fact, the failure rate function is given by z(t) = 0.02t. on the other hand, suppose the parameters were β = 3/4 and α = 2. in that case, z(t) = 1.5/t1/4 and hence the component gets stronger over time."
1224,1,"['probability distributions', 'probability', 'continuous probability distributions', 'continuous', 'distributions']", Weibull Distribution Optional,seg_137,206 chapter 6 some continuous probability distributions
1225,1,"['tests of hypotheses', 'statistical inference', 'statistics', 'distribution', 'normality', 'normal', 'hypotheses', 'statistical', 'tests', 'normal distribution']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_143,"many of the hazards in the use of material in this chapter are quite similar to those of chapter 5. one of the biggest misuses of statistics is the assumption of an underlying normal distribution in carrying out a type of statistical inference when indeed it is not normal. the reader will be exposed to tests of hypotheses in chapters 10 through 15 in which the normality assumption is made. in addition,"
1226,1,"['goodness of fit', 'graphical', 'data', 'normality', 'tests']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_143,"however, the reader will be reminded that there are tests of goodness of fit as well as graphical routines discussed in chapters 8 and 10 that allow for checks on data to determine if the normality assumption is reasonable."
1227,1,"['exponential distribution', 'probabilities', 'estimates', 'estimation', 'parameters', 'failure', 'data', 'distribution', 'normal', 'exponential', 'probability', 'parameter', 'distributions']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_143,"similar warnings should be conveyed regarding assumptions that are often made concerning other distributions, apart from the normal. this chapter has presented examples in which one is required to calculate probabilities to failure of a certain item or the probability that one observes a complaint during a certain time period. assumptions are made concerning a certain distribution type as well as values of parameters of the distributions. note that parameter values (for example, the value of β for the exponential distribution) were given in the example problems. however, in real-life problems, parameter values must be estimates from real-life experience or data. note the emphasis placed on estimation in the projects that appear in chapters 1, 5, and 6. note also the reference in chapter 5 to parameter estimation, which will be discussed extensively beginning in chapter 9."
1228,1,"['discrete', 'random', 'linear', 'quality control', 'distributions', 'functions', 'continuous', 'control', 'random variables', 'variables', 'sampling', 'acceptance sampling']", Introduction,seg_147,"this chapter contains a broad spectrum of material. chapters 5 and 6 deal with specific types of distributions, both discrete and continuous. these are distributions that find use in many subject matter applications, including reliability, quality control, and acceptance sampling. in the present chapter, we begin with a more general topic, that of distributions of functions of random variables. general techniques are introduced and illustrated by examples. this discussion is followed by coverage of a related concept, moment-generating functions, which can be helpful in learning about distributions of linear functions of random variables."
1229,1,"['statistical hypothesis', 'random', 'linear', 'hypothesis testing', 'random variable', 'standard', 'statistical', 'distributions', 'functions', 'graphics', 'statistical graphics', 'statistical inference', 'distribution', 'linear combinations', 'statistical hypothesis testing', 'analysis of variance', 'variance', 'hypothesis', 'random variables', 'variables', 'variable', 'combinations']", Introduction,seg_147,"in standard statistical methods, the result of statistical hypothesis testing, estimation, or even statistical graphics does not involve a single random variable but, rather, functions of one or more random variables. as a result, statistical inference requires the distributions of these functions. for example, the use of averages of random variables is common. in addition, sums and more general linear combinations are important. we are often interested in the distribution of sums of squares of random variables, particularly in the use of analysis of variance techniques discussed in chapters 11–14."
1230,1,"['discrete random variable', 'random variables', 'variables', 'statistics', 'discrete', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'function', 'transformation']", Transformations of Variables,seg_149,"frequently in statistics, one encounters the need to derive the probability distribution of a function of one or more random variables. for example, suppose that x is a discrete random variable with probability distribution f(x), and suppose further that y = u(x) defines a one-to-one transformation between the values of x and y . we wish to find the probability distribution of y . it is important to note that the one-to-one transformation implies that each value x is related to one, and only one, value y = u(x) and that each value y is related to one, and only one, value x = w(y), where w(y) is obtained by solving y = u(x) for x in terms of y."
1231,1,"['probability distributions', 'discrete', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'discrete probability distributions', 'distributions']", Transformations of Variables,seg_149,"from our discussion of discrete probability distributions in chapter 3, it is clear that the random variable y assumes the value y when x assumes the value w(y). consequently, the probability distribution of y is given by"
1232,1,"['discrete random variable', 'discrete', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'transformation']", Transformations of Variables,seg_149,"theorem 7.1: suppose that x is a discrete random variable with probability distribution f(x). let y = u(x) define a one-to-one transformation between the values of x and y so that the equation y = u(x) can be uniquely solved for x in terms of y, say x = w(y). then the probability distribution of y is"
1233,1,"['distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'geometric']", Transformations of Variables,seg_149,example 7.1: let x be a geometric random variable with probability distribution
1234,1,"['distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random']", Transformations of Variables,seg_149,find the probability distribution of the random variable y = x2.
1235,1,['transformation'], Transformations of Variables,seg_149,"solution : since the values of x are all positive, the transformation defines a one-to-one correspondence between the x and y values, y = x2 and x = √y. hence"
1236,1,['transformation'], Transformations of Variables,seg_149,"similarly, for a two-dimension transformation, we have the result in theorem 7.2."
1237,1,"['random variables', 'variables', 'discrete random variables', 'joint probability distribution', 'discrete', 'distribution', 'probability distribution', 'joint', 'probability', 'random', 'transformation', 'joint probability']", Transformations of Variables,seg_149,"theorem 7.2: suppose that x1 and x2 are discrete random variables with joint probability distribution f(x1, x2). let y1 = u1(x1, x2) and y2 = u2(x1, x2) define a one-to- one transformation between the points (x1, x2) and (y1, y2) so that the equations"
1238,1,"['joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'joint probability']", Transformations of Variables,seg_149,"may be uniquely solved for x1 and x2 in terms of y1 and y2, say x1 = w1(y1, y2) and x2 = w2(y1, y2). then the joint probability distribution of y1 and y2 is"
1239,1,"['marginal', 'discrete', 'probability', 'random', 'function', 'random variable', 'joint probability distribution', 'distribution', 'random variables', 'variables', 'discrete random variables', 'variable', 'probability distribution', 'joint', 'marginal distribution', 'joint probability']", Transformations of Variables,seg_149,"theorem 7.2 is extremely useful for finding the distribution of some random variable y1 = u1(x1, x2), where x1 and x2 are discrete random variables with joint probability distribution f(x1, x2). we simply define a second function, say y2 = u2(x1, x2), maintaining a one-to-one correspondence between the points (x1, x2) and (y1, y2), and obtain the joint probability distribution g(y1, y2). the distribution of y1 is just the marginal distribution of g(y1, y2), found by summing over the y2 values. denoting the distribution of y1 by h(y1), we can then write"
1240,1,"['poisson', 'random variables', 'parameters', 'independent', 'variables', 'distribution', 'random variable', 'variable', 'independent random variables', 'random', 'poisson distributions', 'distributions']", Transformations of Variables,seg_149,"example 7.2: let x1 and x2 be two independent random variables having poisson distributions with parameters μ1 and μ2, respectively. find the distribution of the random variable y1 = x1 +x2."
1241,1,['independent'], Transformations of Variables,seg_149,"solution : since x1 and x2 are independent, we can write"
1242,1,"['functions', 'joint probability distribution', 'distribution', 'random variable', 'probability distribution', 'variable', 'joint', 'probability', 'random', 'joint probability']", Transformations of Variables,seg_149,"where x1 = 0, 1, 2, . . . and x2 = 0, 1, 2, . . . . let us now define a second random variable, say y2 = x2. the inverse functions are given by x1 = y1−y2 and x2 = y2. using theorem 7.2, we find the joint probability distribution of y1 and y2 to be"
1243,1,"['marginal probability distribution', 'marginal', 'distribution', 'probability distribution', 'probability', 'marginal probability']", Transformations of Variables,seg_149,"where y1 = 0, 1, 2, . . . and y2 = 0, 1, 2, . . . , y1. note that since x1 > 0, the transformation x1 = y1 − x2 implies that y2 and hence x2 must always be less than or equal to y1. consequently, the marginal probability distribution of y1 is"
1244,1,['binomial'], Transformations of Variables,seg_149,recognizing this sum as the binomial expansion of (μ1 + μ2)y1 we obtain
1245,1,"['poisson', 'random variables', 'parameters', 'independent', 'variables', 'distribution', 'independent random variables', 'parameter', 'random', 'poisson distribution', 'poisson distributions', 'distributions']", Transformations of Variables,seg_149,"from which we conclude that the sum of the two independent random variables having poisson distributions, with parameters μ1 and μ2, has a poisson distribution with parameter μ1 + μ2."
1246,1,"['continuous random variable', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'continuous', 'transformation']", Transformations of Variables,seg_149,"to find the probability distribution of the random variable y = u(x) when x is a continuous random variable and the transformation is one-to-one, we shall need theorem 7.3. the proof of the theorem is left to the reader."
1247,1,"['continuous random variable', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'continuous']", Transformations of Variables,seg_149,suppose that x is a continuous random variable with probability distribution
1248,1,"['distribution', 'probability distribution', 'probability']", Transformations of Variables,seg_149,"theorem 7.3: f(x). let y = u(x) define a one-to-one correspondence between the values of x and y so that the equation y = u(x) can be uniquely solved for x in terms of y, say x = w(y). then the probability distribution of y is"
1249,1,"['transformation', 'jacobian']", Transformations of Variables,seg_149,where j = w′(y) and is called the jacobian of the transformation.
1250,1,"['continuous random variable', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'continuous']", Transformations of Variables,seg_149,example 7.3: let x be a continuous random variable with probability distribution
1251,1,"['distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random']", Transformations of Variables,seg_149,find the probability distribution of the random variable y = 2x − 3.
1252,1,"['function', 'density function']", Transformations of Variables,seg_149,"solution : the inverse solution of y = 2x − 3 yields x = (y + 3)/2, from which we obtain j = w′(y) = dx/dy = 1/2. therefore, using theorem 7.3, we find the density function of y to be"
1253,1,"['random variables', 'variables', 'joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'random', 'continuous', 'transformation', 'joint probability']", Transformations of Variables,seg_149,"to find the joint probability distribution of the random variables y1 = u1(x1, x2) and y2 = u2(x1, x2) when x1 and x2 are continuous and the transformation is one-to-one, we need an additional theorem, analogous to theorem 7.2, which we state without proof."
1254,1,"['continuous random variables', 'random variables', 'variables', 'joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'random', 'continuous', 'transformation', 'joint probability']", Transformations of Variables,seg_149,"theorem 7.4: suppose that x1 and x2 are continuous random variables with joint probability distribution f(x1, x2). let y1 = u1(x1, x2) and y2 = u2(x1, x2) define a one-to- one transformation between the points (x1, x2) and (y1, y2) so that the equations y1 = u1(x1, x2) and y2 = u2(x1, x2) may be uniquely solved for x1 and x2 in terms of y1 and y2, say x1 = w1(yl, y2) and x2 = w2(y1, y2). then the joint probability distribution of y1 and y2 is"
1255,1,"['jacobian', 'determinant']", Transformations of Variables,seg_149,where the jacobian is the 2 × 2 determinant
1256,0,[], Transformations of Variables,seg_149,"1 is simply the derivative of x1 = w1(y1, y2) with respect to y1 with y2 held constant, referred to in calculus as the partial derivative of x1 with respect to y1. the other partial derivatives are defined in a similar manner."
1257,1,"['continuous random variables', 'random variables', 'variables', 'joint', 'probability', 'random', 'continuous', 'joint probability']", Transformations of Variables,seg_149,example 7.4: let x1 and x2 be two continuous random variables with joint probability distribution
1258,1,"['joint probability distribution', 'distribution', 'probability distribution', 'joint', 'probability', 'joint probability']", Transformations of Variables,seg_149,find the joint probability distribution of y1 = x12 and y2 = x1x2.
1259,0,[], Transformations of Variables,seg_149,"solution : the inverse solutions of y1 = x21 and y2 = x1x2 are x1 = √y1 and x2 = y2/√y1, from which we obtain"
1260,1,['set'], Transformations of Variables,seg_149,"to determine the set b of points in the y1y2 plane into which the set a of points in the x1x2 plane is mapped, we write"
1261,1,"['transformed', 'joint probability distribution', 'distribution', 'probability distribution', 'set', 'joint', 'probability', 'transformation', 'joint probability']", Transformations of Variables,seg_149,"then setting x1 = 0, x2 = 0, x1 = 1, and x2 = 1, the boundaries of set a are transformed to y1 = 0, y2 = 0, y1 = 1, and y2 = √y1, or y22 = y1. the two regions are illustrated in figure 7.1. clearly, the transformation is one-to- one, mapping the set a = {(x1, x2) | 0 < x1 < 1, 0 < x2 < 1} into the set b = {(y1, y2) | y22 < y1 < 1, 0 < y2 < 1}. from theorem 7.4 the joint probability distribution of y1 and y2 is"
1262,1,['set'], Transformations of Variables,seg_149,figure 7.1: mapping set a into set b.
1263,1,"['interval', 'continuous random variable', 'case', 'distribution', 'random variable', 'probability distribution', 'variable', 'probability', 'random', 'continuous', 'transformation']", Transformations of Variables,seg_149,"problems frequently arise when we wish to find the probability distribution of the random variable y = u(x) when x is a continuous random variable and the transformation is not one-to-one. that is, to each value x there corresponds exactly one value y, but to each y value there corresponds more than one x value. for example, suppose that f(x) is positive over the interval −1 < x < 2 and zero elsewhere. consider the transformation y = x2. in this case, x = ±√y for 0 < y < 1 and x = √y for 1 < y < 4. for the interval 1 < y < 4, the probability distribution of y is found as before, using theorem 7.3. that is,"
1264,1,"['functions', 'interval']", Transformations of Variables,seg_149,"however, when 0 < y < 1, we may partition the interval −1 < x < 1 to obtain the two inverse functions"
1265,0,[], Transformations of Variables,seg_149,then to every y value there corresponds a single x value for each partition. from figure 7.2 we see that
1266,1,['function'], Transformations of Variables,seg_149,figure 7.2: decreasing and increasing function.
1267,1,['variable'], Transformations of Variables,seg_149,"changing the variable of integration from x to y, we obtain"
1268,0,[], Transformations of Variables,seg_149,"hence, we can write"
1269,1,"['distribution', 'probability distribution', 'probability']", Transformations of Variables,seg_149,the probability distribution of y for 0 < y < 4 may now be written
1270,1,"['functions', 'variables', 'statistics', 'transformations']", Transformations of Variables,seg_149,"this procedure for finding g(y) when 0 < y < 1 is generalized in theorem 7.5 for k inverse functions. for transformations not one-to-one of functions of several variables, the reader is referred to introduction to mathematical statistics by hogg, mckean, and craig (2005; see the bibliography)."
1271,1,"['functions', 'disjoint', 'interval', 'continuous', 'continuous random variable', 'distribution', 'random variable', 'probability distribution', 'variable', 'sets', 'probability', 'random', 'transformation', 'disjoint sets']", Transformations of Variables,seg_149,theorem 7.5: suppose that x is a continuous random variable with probability distribution f(x). let y = u(x) define a transformation between the values of x and y that is not one-to-one. if the interval over which x is defined can be partitioned into k mutually disjoint sets such that each of the inverse functions
1272,1,"['distribution', 'probability distribution', 'probability']", Transformations of Variables,seg_149,"of y = u(x) defines a one-to-one correspondence, then the probability distribution of y is"
1273,1,"['distribution', 'normal', 'degree of freedom', 'mean', 'variance', 'normal distribution']", Transformations of Variables,seg_149,example 7.5: show that y = (x−μ)2/σ2 has a chi-squared distribution with 1 degree of freedom when x has a normal distribution with mean μ and variance σ2.
1274,1,"['standard', 'standard normal', 'random variable', 'variable', 'normal', 'random']", Transformations of Variables,seg_149,"solution : let z = (x − μ)/σ, where the random variable z has the standard normal distribution"
1275,1,"['distribution', 'variable', 'random variable', 'random']", Transformations of Variables,seg_149,"we shall now find the distribution of the random variable y = z2. the inverse solutions of y = z2 are z = ±√y. if we designate z1 = −√y and z2 = √y, then j1 = −1/2√y and j2 = 1/2√y. hence, by theorem 7.5, we have"
1276,1,"['function', 'density function']", Transformations of Variables,seg_149,"since g(y) is a density function, it follows that"
1277,1,"['curve', 'parameters', 'gamma', 'probability']", Transformations of Variables,seg_149,"the integral being the area under a gamma probability curve with parameters α = 1/2 and β = 2. hence, √π = γ(1/2) and the density of y is given by"
1278,1,"['distribution', 'degree of freedom']", Transformations of Variables,seg_149,which is seen to be a chi-squared distribution with 1 degree of freedom.
1279,1,"['functions', 'random variables', 'variables', 'moments', 'random', 'function', 'distributions']", Moments and MomentGenerating Functions,seg_151,"in this section, we concentrate on applications of moment-generating functions. the obvious purpose of the moment-generating function is in determining moments of random variables. however, the most important contribution is to establish distributions of functions of random variables."
1280,1,"['moment', 'expected value', 'random variable', 'variable', 'random']", Moments and MomentGenerating Functions,seg_151,"if g(x) = xr for r = 0, 1, 2, 3, . . . , definition 7.1 yields an expected value called the rth moment about the origin of the random variable x, which we denote by μ′r."
1281,1,"['moment', 'random variable', 'variable', 'random']", Moments and MomentGenerating Functions,seg_151,definition 7.1: the rth moment about the origin of the random variable x is given by
1282,1,"['continuous', 'discrete']", Moments and MomentGenerating Functions,seg_151,"⎧∑xrf(x), if x is discrete, μ′r = e(xr) = ∫x x f(x) dx, if x is continuous. ⎩⎨"
1283,1,"['random variable', 'variable', 'moments', 'mean', 'random', 'variance']", Moments and MomentGenerating Functions,seg_151,"since the first and second moments about the origin are given by μ′1 = e(x) and μ2′ = e(x2), we can write the mean and variance of a random variable as"
1284,1,"['random variable', 'variable', 'moments', 'random', 'function']", Moments and MomentGenerating Functions,seg_151,"although the moments of a random variable can be determined directly from definition 7.1, an alternative procedure exists. this procedure requires us to utilize a moment-generating function."
1285,1,"['random', 'function']", Moments and MomentGenerating Functions,seg_151,"definition 7.2: themoment-generating function of the random variablex is given by e(etx) and is denoted by mx(t). hence,"
1286,1,"['continuous', 'discrete']", Moments and MomentGenerating Functions,seg_151,"⎧ ∑ etxf(x), if x is discrete, mx(t) = e(etx) = ⎨ x∞ tx e f(x) dx, if x is continuous. ⎩∫−∞"
1287,1,"['functions', 'method', 'random variable', 'variable', 'moments', 'random', 'function']", Moments and MomentGenerating Functions,seg_151,"moment-generating functions will exist only if the sum or integral of definition 7.2 converges. if a moment-generating function of a random variable x does exist, it can be used to generate all the moments of that variable. the method is described in theorem 7.6 without proof."
1288,1,"['random variable', 'variable', 'random', 'function']", Moments and MomentGenerating Functions,seg_151,theorem 7.6: let x be a random variable with moment-generating function mx(t). then
1289,1,"['random variable', 'variable', 'binomial random variable', 'binomial', 'random', 'function']", Moments and MomentGenerating Functions,seg_151,example 7.6: find the moment-generating function of the binomial random variable x and then use it to verify that μ = np and σ2 = npq.
1290,0,[], Moments and MomentGenerating Functions,seg_151,solution : from definition 7.2 we have
1291,1,['binomial'], Moments and MomentGenerating Functions,seg_151,"recognizing this last sum as the binomial expansion of (pet + q)n, we obtain"
1292,1,['results'], Moments and MomentGenerating Functions,seg_151,which agrees with the results obtained in chapter 5.
1293,1,"['distribution', 'random variable', 'probability distribution', 'normal', 'variable', 'mean', 'random', 'function', 'probability', 'normal probability distribution', 'variance']", Moments and MomentGenerating Functions,seg_151,example 7.7: show that the moment-generating function of the random variable x having a normal probability distribution with mean μ and variance σ2 is given by
1294,1,"['random variable', 'variable', 'normal', 'random', 'function', 'normal random variable']", Moments and MomentGenerating Functions,seg_151,solution : from definition 7.2 the moment-generating function of the normal random variable x is
1295,0,[], Moments and MomentGenerating Functions,seg_151,"completing the square in the exponent, we can write"
1296,1,"['density curve', 'curve', 'standard normal', 'normal', 'standard']", Moments and MomentGenerating Functions,seg_151,since the last integral represents the area under a standard normal density curve and hence equals 1.
1297,1,"['functions', 'linear', 'random variables', 'method', 'linear combination', 'variables', 'independent', 'combination', 'distribution', 'independent random variables', 'random', 'function', 'transforming']", Moments and MomentGenerating Functions,seg_151,"although the method of transforming variables provides an effective way of finding the distribution of a function of several variables, there is an alternative and often preferred procedure when the function in question is a linear combination of independent random variables. this procedure utilizes the properties of momentgenerating functions discussed in the following four theorems. in keeping with the mathematical scope of this book, we state theorem 7.7 without proof."
1298,1,"['functions', 'random variables', 'variables', 'distribution', 'probability distribution', 'probability', 'random']", Moments and MomentGenerating Functions,seg_151,"theorem 7.7: (uniqueness theorem) let x and y be two random variables with momentgenerating functions mx(t) and my (t), respectively. if mx(t) = my (t) for all values of t, then x and y have the same probability distribution."
1299,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random']", Moments and MomentGenerating Functions,seg_151,"theorem 7.10: ifx1, x2, . . . , xn are independent random variables with moment-generating functions mx1(t),mx2(t), . . . ,mxn(t), respectively, and y = x1+x2+ · · ·+xn, then"
1300,0,[], Moments and MomentGenerating Functions,seg_151,the proof of theorem 7.10 is left for the reader.
1301,1,"['poisson', 'functions', 'random variables', 'independent', 'variables', 'distribution', 'random']", Moments and MomentGenerating Functions,seg_151,theorems 7.7 through 7.10 are vital for understanding moment-generating functions. an example follows to illustrate. there are many situations in which we need to know the distribution of the sum of random variables. we may use theorems 7.7 and 7.10 and the result of exercise 7.19 on page 224 to find the distribution of a sum of two independent poisson random variables with moment-generating functions given by
1302,1,"['random variable', 'variable', 'random', 'function']", Moments and MomentGenerating Functions,seg_151,"respectively. according to theorem 7.10, the moment-generating function of the random variable y1 = x1 +x2 is"
1303,1,"['random', 'function', 'poisson distributions', 'independent random variables', 'random variable', 'parameter', 'distributions', 'parameters', 'distribution', 'poisson distribution', 'poisson', 'random variables', 'independent', 'variables', 'variable']", Moments and MomentGenerating Functions,seg_151,"which we immediately identify as the moment-generating function of a random variable having a poisson distribution with the parameter μ1 +μ2. hence, according to theorem 7.7, we again conclude that the sum of two independent random variables having poisson distributions, with parameters μ1 and μ2, has a poisson distribution with parameter μ1 + μ2."
1304,1,"['statistics', 'probability', 'random', 'linear', 'random variable', 'mean', 'combination', 'distribution', 'variance', 'random variables', 'linear combination', 'independent', 'variables', 'variable', 'probability distribution', 'normal']", Moments and MomentGenerating Functions,seg_151,"in applied statistics one frequently needs to know the probability distribution of a linear combination of independent normal random variables. let us obtain the distribution of the random variable y = a1x1+a2x2 when x1 is a normal variable with mean μ1 and variance σ12 and x2 is also a normal variable but independent of x1 with mean μ2 and variance σ22. first, by theorem 7.10, we find"
1305,1,"['distribution', 'normal', 'function', 'normal distribution']", Moments and MomentGenerating Functions,seg_151,"substituting a1t for t and then a2t for t in a moment-generating function of the normal distribution derived in example 7.7, we have"
1306,1,"['distribution', 'normal', 'mean', 'function', 'variance']", Moments and MomentGenerating Functions,seg_151,which we recognize as the moment-generating function of a distribution that is normal with mean a1μ1 + a2μ2 and variance a21σ12 + a22σ22.
1307,1,"['independent', 'variables', 'case', 'normal']", Moments and MomentGenerating Functions,seg_151,"generalizing to the case of n independent normal variables, we state the following result."
1308,1,"['normal distributions', 'random variables', 'independent', 'variances', 'variables', 'normal', 'independent random variables', 'random', 'distributions']", Moments and MomentGenerating Functions,seg_151,"theorem 7.11: if x1, x2, . . . , xn are independent random variables having normal distributions with means μ1, μ2, . . . , μn and variances σ12, σ22, . . . , σn"
1309,0,[], Moments and MomentGenerating Functions,seg_151,"2 , respectively, then the ran-"
1310,1,['variable'], Moments and MomentGenerating Functions,seg_151,dom variable
1311,1,"['distribution', 'normal', 'mean', 'normal distribution']", Moments and MomentGenerating Functions,seg_151,has a normal distribution with mean
1312,1,['variance'], Moments and MomentGenerating Functions,seg_151,and variance
1313,1,"['poisson', 'random variables', 'independent', 'variables', 'distribution', 'random variable', 'variable', 'normal', 'independent random variables', 'random', 'poisson distribution', 'normal distribution', 'distributions']", Moments and MomentGenerating Functions,seg_151,it is now evident that the poisson distribution and the normal distribution possess a reproductive property in that the sum of independent random variables having either of these distributions is a random variable that also has the same type of distribution. the chi-squared distribution also has this reproductive property.
1314,1,"['degrees of freedom', 'random variables', 'independent', 'variables', 'mutually independent', 'random variable', 'variable', 'independent random variables', 'random', 'distributions']", Moments and MomentGenerating Functions,seg_151,"theorem 7.12: if x1, x2, . . . , xn are mutually independent random variables that have, respectively, chi-squared distributions with v1, v2, . . . , vn degrees of freedom, then the random variable"
1315,1,"['degrees of freedom', 'distribution']", Moments and MomentGenerating Functions,seg_151,has a chi-squared distribution with v = v1 + v2 + · · ·+ vn degrees of freedom.
1316,1,"['functions', 'random variables', 'variables', 'random']", Moments and MomentGenerating Functions,seg_151,222 chapter 7 functions of random variables (optional)
1317,1,"['degrees of freedom', 'distribution', 'function']", Moments and MomentGenerating Functions,seg_151,which we recognize as the moment-generating function of a chi-squared distribution with v = v1 + v2 + · · ·+ vn degrees of freedom.
1318,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random variable', 'variable', 'normal', 'mean', 'random', 'variance']", Moments and MomentGenerating Functions,seg_151,"corollary 7.1: if x1, x2, . . . , xn are independent random variables having identical normal distributions with mean μ and variance σ2, then the random variable"
1319,1,"['degrees of freedom', 'distribution']", Moments and MomentGenerating Functions,seg_151,has a chi-squared distribution with v = n degrees of freedom.
1320,1,"['degrees of freedom', 'distribution', 'normal', 'mean', 'parameter', 'normal distribution']", Moments and MomentGenerating Functions,seg_151,"this corollary is an immediate consequence of example 7.5. it establishes a relationship between the very important chi-squared distribution and the normal distribution. it also should provide the reader with a clear idea of what we mean by the parameter that we call degrees of freedom. in future chapters, the notion of degrees of freedom will play an increasingly important role."
1321,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random variable', 'variable', 'normal', 'mean', 'random', 'variance']", Moments and MomentGenerating Functions,seg_151,"corollary 7.2: if x1, x2, . . . , xn are independent random variables and xi follows a normal distribution with mean μi and variance σi2 for i = 1, 2, . . . , n, then the random variable"
1322,1,"['degrees of freedom', 'distribution']", Moments and MomentGenerating Functions,seg_151,has a chi-squared distribution with v = n degrees of freedom.
1323,1,"['tests', 'experiment', 'outcome of interest', 'outcome', 'statistical', 'numerical']", Random Sampling,seg_157,"the outcome of a statistical experiment may be recorded either as a numerical value or as a descriptive representation. when a pair of dice is tossed and the total is the outcome of interest, we record a numerical value. however, if the students of a certain school are given blood tests and the type of blood is of interest, then a descriptive representation might be more useful. a person’s blood can be classified in 8 ways: ab, a, b, or o, each with a plus or minus sign, depending on the presence or absence of the rh antigen."
1324,1,"['sample', 'graphical', 'statistical inference', 'sample mean', 'sampling', 'statistical', 'mean', 'populations', 'sample variance', 'variance', 'distributions']", Random Sampling,seg_157,"in this chapter, we focus on sampling from distributions or populations and study such important quantities as the sample mean and sample variance, which will be of vital importance in future chapters. in addition, we attempt to give the reader an introduction to the role that the sample mean and variance will play in statistical inference in later chapters. the use of modern high-speed computers allows the scientist or engineer to greatly enhance his or her use of formal statistical inference with graphical techniques. much of the time, formal inference appears quite dry and perhaps even abstract to the practitioner or to the manager who wishes to let statistical analysis be a guide to decision-making."
1325,1,"['random variables', 'variables', 'observations', 'statisticians', 'populations', 'samples', 'population', 'random', 'statistical', 'outcomes']", Random Sampling,seg_157,"we begin this section by discussing the notions of populations and samples. both are mentioned in a broad fashion in chapter 1. however, much more needs to be presented about them here, particularly in the context of the concept of random variables. the totality of observations with which we are concerned, whether their number be finite or infinite, constitutes what we call a population. there was a time when the word population referred to observations obtained from statistical studies about people. today, statisticians use the term to refer to observations relevant to anything of interest, whether it be groups of people, animals, or all possible outcomes from some complicated biological or engineering system."
1326,1,"['observations', 'population']", Random Sampling,seg_157,definition 8.1: a population consists of the totality of the observations with which we are concerned.
1327,1,"['observations', 'case', 'distribution', 'measurements', 'population', 'populations', 'number of observations', 'measuring']", Random Sampling,seg_157,"the number of observations in the population is defined to be the size of the population. if there are 600 students in the school whom we classified according to blood type, we say that we have a population of size 600. the numbers on the cards in a deck, the heights of residents in a certain city, and the lengths of fish in a particular lake are examples of populations with finite size. in each case, the total number of observations is a finite number. the observations obtained by measuring the atmospheric pressure every day, from the past on into the future, or all measurements of the depth of a lake, from any conceivable position, are examples of populations whose sizes are infinite. some finite populations are so large that in theory we assume them to be infinite. this is true in the case of the population of lifetimes of a certain type of storage battery being manufactured for mass distribution throughout the country."
1328,1,"['observation', 'distribution', 'random variable', 'probability distribution', 'variable', 'bernoulli', 'population', 'probability', 'random', 'bernoulli random variable']", Random Sampling,seg_157,"each observation in a population is a value of a random variablex having some probability distribution f(x). if one is inspecting items coming off an assembly line for defects, then each observation in the population might be a value 0 or 1 of the bernoulli random variable x with probability distribution"
1329,1,"['observations', 'discrete', 'probability', 'random', 'binomial distribution', 'experiment', 'random variable', 'mean', 'population', 'trial', 'distribution', 'binomial', 'continuous', 'continuous random variable', 'variance', 'discrete random variable', 'variable', 'probability distribution', 'normal', 'normal distribution']", Random Sampling,seg_157,"where 0 indicates a nondefective item and 1 indicates a defective item. of course, it is assumed that p, the probability of any item being defective, remains constant from trial to trial. in the blood-type experiment, the random variable x represents the type of blood and is assumed to take on values from 1 to 8. each student is given one of the values of the discrete random variable. the lives of the storage batteries are values assumed by a continuous random variable having perhaps a normal distribution. when we refer hereafter to a “binomial population,” a “nor- mal population,” or, in general, the “population f(x),” we shall mean a population whose observations are values of a random variable having a binomial distribution, a normal distribution, or the probability distribution f(x). hence, the mean and variance of a random variable or probability distribution are also referred to as the mean and variance of the corresponding population."
1330,1,"['statistical inference', 'factor', 'observations', 'statisticians', 'sampling', 'statistical', 'set', 'population', 'test', 'average']", Random Sampling,seg_157,"in the field of statistical inference, statisticians are interested in arriving at conclusions concerning a population when it is impossible or impractical to observe the entire set of observations that make up the population. for example, in attempting to determine the average length of life of a certain brand of light bulb, it would be impossible to test all such bulbs if we are to have any left to sell. exorbitant costs can also be a prohibitive factor in studying an entire population. therefore, we must depend on a subset of observations from the population to help us make inferences concerning that same population. this brings us to consider the notion of sampling."
1331,1,"['sample', 'population']", Random Sampling,seg_157,definition 8.2: a sample is a subset of a population.
1332,1,"['sample', 'samples', 'population', 'representative']", Random Sampling,seg_157,"if our inferences from the sample to the population are to be valid, we must obtain samples that are representative of the population. all too often we are"
1333,1,"['sample', 'bias', 'random', 'observations', 'biased', 'sampling', 'population', 'random sample']", Random Sampling,seg_157,"tempted to choose a sample by selecting the most convenient members of the population. such a procedure may lead to erroneous inferences concerning the population. any sampling procedure that produces inferences that consistently overestimate or consistently underestimate some characteristic of the population is said to be biased. to eliminate any possibility of bias in the sampling procedure, it is desirable to choose a random sample in the sense that the observations are made independently and at random."
1334,1,"['measurements', 'probability', 'random sample', 'random', 'numerical', 'sample', 'experiment', 'probability distributions', 'random variable', 'population', 'distributions', 'joint probability distribution', 'distribution', 'random variables', 'measurement', 'independent', 'variables', 'variable', 'probability distribution', 'joint', 'joint probability']", Random Sampling,seg_157,"in selecting a random sample of size n from a population f(x), let us define the random variable xi, i = 1, 2, . . . , n, to represent the ith measurement or sample value that we observe. the random variables x1, x2, . . . , xn will then constitute a random sample from the population f(x) with numerical values x1, x2, . . . , xn if the measurements are obtained by repeating the experiment n independent times under essentially the same conditions. because of the identical conditions under which the elements of the sample are selected, it is reasonable to assume that the n random variablesx1, x2, . . . , xn are independent and that each has the same probability distribution f(x). that is, the probability distributions of x1, x2, . . . , xn are, respectively, f(x1), f(x2), . . . , f(xn), and their joint probability distribution is f(x1, x2, . . . , xn) = f(x1)f(x2) · · · f(xn). the concept of a random sample is described formally by the following definition."
1335,1,"['sample', 'random variables', 'independent', 'variables', 'joint probability distribution', 'random sample', 'distribution', 'probability distribution', 'joint', 'population', 'probability', 'random', 'independent random variables', 'joint probability']", Random Sampling,seg_157,"definition 8.3: let x1, x2, . . . , xn be n independent random variables, each having the same probability distribution f(x). define x1, x2, . . . , xn to be a random sample of size n from the population f(x) and write its joint probability distribution as"
1336,1,"['sample', 'measurement', 'random sample', 'distribution', 'normal', 'population', 'random', 'process', 'normal distribution']", Random Sampling,seg_157,"if one makes a random selection of n = 8 storage batteries from a manufacturing process that has maintained the same specification throughout and records the length of life for each battery, with the first measurement x1 being a value of x1, the second measurement x2 a value of x2, and so forth, then x1, x2, . . . , x8 are the values of the random sample x1, x2, . . . , x8. if we assume the population of battery lives to be normal, the possible values of any xi, i = 1, 2, . . . , 8, will be precisely the same as those in the original population, and hence xi has the same identical normal distribution as x."
1337,1,"['sample', 'random', 'parameters', 'random samples', 'states', 'information', 'samples', 'population', 'parameter', 'random sample']", Some Important Statistics,seg_159,"our main purpose in selecting random samples is to elicit information about the unknown population parameters. suppose, for example, that we wish to arrive at a conclusion concerning the proportion of coffee-drinkers in the united states who prefer a certain brand of coffee. it would be impossible to question every coffeedrinking american in order to compute the value of the parameter p representing the population proportion. instead, a large random sample is selected and the proportion p̂ of people in this sample favoring the brand of coffee in question is calculated. the value p̂ is now used to make an inference concerning the true proportion p."
1338,1,"['sample', 'random', 'random sample', 'function']", Some Important Statistics,seg_159,"now, p̂ is a function of the observed values in the random sample; since many"
1339,1,"['sample', 'variable', 'random variable', 'samples', 'statistic', 'population', 'random', 'vary']", Some Important Statistics,seg_159,"random samples are possible from the same population, we would expect p̂ to vary somewhat from sample to sample. that is, p̂ is a value of a random variable that we represent by p . such a random variable is called a statistic."
1340,1,"['sample', 'random variables', 'variables', 'random sample', 'statistic', 'random', 'function']", Some Important Statistics,seg_159,definition 8.4: any function of the random variables constituting a random sample is called a statistic.
1341,1,"['observations', 'statistics', 'location', 'set', 'probability', 'random sample', 'random', 'sample', 'data', 'mean', 'population', 'parameters', 'median', 'distribution', 'random variables', 'variability', 'variables', 'probability distribution', 'measuring']", Some Important Statistics,seg_159,"in chapter 4 we introduced the two parameters μ and σ2, which measure the center of location and the variability of a probability distribution. these are constant population parameters and are in no way affected or influenced by the observations of a random sample. we shall, however, define some important statistics that describe corresponding measures of a random sample. the most commonly used statistics for measuring the center of a set of data, arranged in order of magnitude, are the mean, median, and mode. although the first two of these statistics were defined in chapter 1, we repeat the definitions here. let x1, x2, . . . , xn represent n random variables."
1342,1,"['sample', 'mean', 'sample mean']", Some Important Statistics,seg_159,(a) sample mean:
1343,1,['statistic'], Some Important Statistics,seg_159,note that the statistic x̄ assumes the value x̄ = n
1344,1,"['sample', 'sample mean', 'statistic', 'mean']", Some Important Statistics,seg_159,"1 ∑ xi when x1 assumes the i=1 value x1, x2 assumes the value x2, and so forth. the term sample mean is applied to both the statistic x̄ and its computed value x̄."
1345,1,"['sample median', 'sample', 'median']", Some Important Statistics,seg_159,(b) sample median:
1346,1,"['sample median', 'sample', 'sample mean', 'median', 'location', 'mean', 'location measure']", Some Important Statistics,seg_159,the sample median is also a location measure that shows the middle value of the sample. examples for both the sample mean and the sample median can be found in section 1.3. the sample mode is defined as follows.
1347,1,['sample'], Some Important Statistics,seg_159,(c) the sample mode is the value of the sample that occurs most often.
1348,1,"['observations', 'data', 'set', 'data set']", Some Important Statistics,seg_159,example 8.1: suppose a data set consists of the following observations:
1349,1,['sample'], Some Important Statistics,seg_159,"the sample mode is 0.43, since this value occurs more than any other value."
1350,1,"['sample', 'variability', 'location']", Some Important Statistics,seg_159,"as we suggested in chapter 1, a measure of location or central tendency in a sample does not by itself give a clear indication of the nature of the sample. thus, a measure of variability in the sample must also be considered."
1351,1,"['sample', 'variability', 'median', 'observations', 'sets', 'measurements', 'mean', 'average']", Some Important Statistics,seg_159,the variability in a sample displays how the observations spread out from the average. the reader is referred to chapter 1 for more discussion. it is possible to have two sets of observations with the same mean or median that differ considerably in the variability of their measurements about the average.
1352,1,"['measurements', 'samples']", Some Important Statistics,seg_159,"consider the following measurements, in liters, for two samples of orange juice bottled by companies a and b:"
1353,1,['sample'], Some Important Statistics,seg_159,sample a 0.97 1.00 0.94 1.03 1.06 sample b 1.06 1.01 0.88 0.91 1.14
1354,1,"['sample', 'variability', 'observations', 'confident', 'samples', 'mean', 'average', 'dispersion']", Some Important Statistics,seg_159,"both samples have the same mean, 1.00 liter. it is obvious that company a bottles orange juice with a more uniform content than company b. we say that the variability, or the dispersion, of the observations from the average is less for sample a than for sample b. therefore, in buying orange juice, we would feel more confident that the bottle we select will be close to the advertised average if we buy from company a."
1355,1,"['deviation', 'sample', 'sample variance', 'random variables', 'random', 'variability', 'variables', 'range', 'sample standard deviation', 'sample variability', 'standard', 'standard deviation', 'variance']", Some Important Statistics,seg_159,"in chapter 1 we introduced several measures of sample variability, including the sample variance, sample standard deviation, and sample range. in this chapter, we will focus mainly on the sample variance. again, let x1, . . . , xn represent n random variables."
1356,1,"['sample', 'variance', 'sample variance']", Some Important Statistics,seg_159,(a) sample variance: n
1357,1,"['sample', 'observations', 'deviations', 'mean', 'average']", Some Important Statistics,seg_159,the computed value of s2 for a given sample is denoted by s2. note that s2 is essentially defined to be the average of the squares of the deviations of the observations from their mean. the reason for using n− 1 as a divisor rather than the more obvious choice n will become apparent in chapter 9.
1358,1,"['sample', 'random', 'random sample', 'variance']", Some Important Statistics,seg_159,"example 8.2: a comparison of coffee prices at 4 randomly selected grocery stores in san diego showed increases from the previous month of 12, 15, 17, and 20 cents for a 1-pound bag. find the variance of this random sample of price increases."
1359,1,"['sample', 'mean', 'sample mean']", Some Important Statistics,seg_159,"solution : calculating the sample mean, we get"
1360,1,"['sample', 'variability', 'sample variance', 'variance']", Some Important Statistics,seg_159,"whereas the expression for the sample variance best illustrates that s2 is a measure of variability, an alternative expression does have some merit and thus the reader should be aware of it. the following theorem contains this expression."
1361,1,"['data', 'sampling', 'sampling distributions', 'distributions']", Some Important Statistics,seg_159,230 chapter 8 fundamental sampling distributions and data descriptions
1362,1,"['sample', 'random', 'random sample', 'variance']", Some Important Statistics,seg_159,"theorem 8.1: if s2 is the variance of a random sample of size n, we may write"
1363,0,[], Some Important Statistics,seg_159,"proof : by definition,"
1364,1,"['deviation', 'sample', 'range', 'sample standard deviation', 'standard', 'standard deviation']", Some Important Statistics,seg_159,"as in chapter 1, the sample standard deviation and the sample range are defined below."
1365,1,"['deviation', 'sample', 'sample standard deviation', 'standard', 'standard deviation']", Some Important Statistics,seg_159,(b) sample standard deviation:
1366,1,"['sample', 'variance', 'sample variance']", Some Important Statistics,seg_159,where s2 is the sample variance.
1367,0,[], Some Important Statistics,seg_159,let xmax denote the largest of the xi values and xmin the smallest.
1368,1,"['sample', 'range']", Some Important Statistics,seg_159,(c) sample range:
1369,1,"['sample', 'random', 'data', 'random sample', 'variance']", Some Important Statistics,seg_159,"example 8.3: find the variance of the data 3, 4, 5, 6, 6, and 7, representing the number of trout caught by a random sample of 6 fishermen on june 19, 1996, at lake muskoka."
1370,1,"['deviation', 'sample', 'range', 'sample standard deviation', 'standard', 'standard deviation']", Some Important Statistics,seg_159,"thus, the sample standard deviation s = √13/6 = 1.47 and the sample range is 7− 3 = 4."
1371,1,"['sample', 'statistical inference', 'estimates', 'random sample', 'case', 'statistical', 'mean', 'population', 'random', 'average', 'predictions']", Sampling Distributions,seg_163,"the field of statistical inference is basically concerned with generalizations and predictions. for example, we might claim, based on the opinions of several people interviewed on the street, that in a forthcoming election 60% of the eligible voters in the city of detroit favor a certain candidate. in this case, we are dealing with a random sample of opinions from a very large finite population. as a second illustration we might state that the average cost to build a residence in charleston, south carolina, is between $330,000 and $335,000, based on the estimates of 3 contractors selected at random from the 30 now building in this city. the population being sampled here is again finite but very small. finally, let us consider a soft-drink machine designed to dispense, on average, 240 milliliters per drink. a company official who computes the mean of 40 drinks obtains x̄ = 236 milliliters and, on the basis of this value, decides that the machine is still dispensing drinks with an average content of μ = 240 milliliters. the 40 drinks represent a sample from the infinite population of possible drinks that will be dispensed by this machine."
1372,1,"['sample', 'parameters', 'sample mean', 'sampling', 'statistic', 'mean', 'population', 'tests', 'average']", Sampling Distributions,seg_163,"in each of the examples above, we computed a statistic from a sample selected from the population, and from this statistic we made various statements concerning the values of population parameters that may or may not be true. the company official made the decision that the soft-drink machine dispenses drinks with an average content of 240 milliliters, even though the sample mean was 236 milliliters, because he knows from sampling theory that, if μ = 240 milliliters, such a sample value could easily occur. in fact, if he ran similar tests, say every hour, he would expect the values of the statistic x̄ to fluctuate above and below μ = 240 milliliters. only when the value of x̄ is substantially different from 240 milliliters will the company official initiate action to adjust the machine."
1373,1,"['sample', 'distribution', 'random variable', 'probability distribution', 'variable', 'statistic', 'probability', 'random']", Sampling Distributions,seg_163,"since a statistic is a random variable that depends only on the observed sample, it must have a probability distribution."
1374,1,"['sampling', 'distribution', 'probability distribution', 'statistic', 'probability', 'sampling distribution']", Sampling Distributions,seg_163,definition 8.5: the probability distribution of a statistic is called a sampling distribution.
1375,1,"['method', 'sampling', 'distribution', 'samples', 'statistic', 'sampling distribution']", Sampling Distributions,seg_163,"the sampling distribution of a statistic depends on the distribution of the population, the size of the samples, and the method of choosing the samples. in the"
1376,1,"['statistical inference', 'statistics', 'sampling', 'distribution', 'probability distribution', 'mean', 'probability', 'sampling distributions', 'statistical', 'sampling distribution', 'distributions']", Sampling Distributions,seg_163,remainder of this chapter we study several of the important sampling distributions of frequently used statistics. applications of these sampling distributions to problems of statistical inference are considered throughout most of the remaining chapters. the probability distribution of x̄ is called the sampling distribution of the mean.
1377,1,"['case', 'sample', 'experiment', 'results', 'information', 'mean', 'population', 'sampling distributions', 'distributions', 'sample size', 'parameters', 'distribution', 'sampling distribution', 'variability', 'population mean', 'sampling', 'experiments']", Sampling Distributions,seg_163,"we should view the sampling distributions of x̄ and s2 as the mechanisms from which we will be able to make inferences on the parameters μ and σ2. the sampling distribution of x̄ with sample size n is the distribution that results when an experiment is conducted over and over (always with sample size n) and the many values of x̄ result. this sampling distribution, then, describes the variability of sample averages around the population mean μ. in the case of the soft-drink machine, knowledge of the sampling distribution of x̄ arms the analyst with the knowledge of a “typical” discrepancy between an observed x̄ value and true μ. the same principle applies in the case of the distribution of s2. the sampling distribution produces information about the variability of s2 values around σ2 in repeated experiments."
1378,1,"['sample', 'observations', 'random sample', 'observation', 'sampling', 'distribution', 'normal', 'mean', 'population', 'random', 'variance', 'sampling distribution', 'normal distribution']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"the first important sampling distribution to be considered is that of the mean x̄. suppose that a random sample of n observations is taken from a normal population with mean μ and variance σ2. each observation xi, i = 1, 2, . . . , n, of the random sample will then have the same normal distribution as the population being sampled. hence, by the reproductive property of the normal distribution established in theorem 7.11, we conclude that"
1379,1,"['distribution', 'normal', 'mean', 'normal distribution']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,has a normal distribution with mean
1380,1,['variance'], Sampling Distribution of Means and the Central Limit Theorem,seg_165,1 1 σ2 μx̄ = n (μ+ μ+ · · ·+ μ) = μ and variance σx̄
1381,1,"['sample size', 'sample', 'sampling', 'distribution', 'central limit theorem', 'normal', 'mean', 'population', 'variance', 'sampling distribution', 'limit']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"if we are sampling from a population with unknown distribution, either finite or infinite, the sampling distribution of x̄ will still be approximately normal with mean μ and variance σ2/n, provided that the sample size is large. this amazing result is an immediate consequence of the following theorem, called the central limit theorem."
1382,1,"['sample', 'random sample', 'distribution', 'central limit theorem', 'mean', 'population', 'random', 'variance', 'limit']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"theorem 8.2: central limit theorem: if x̄ is the mean of a random sample of size n taken from a population with mean μ and finite variance σ2, then the limiting form of the distribution of"
1383,1,"['standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"as n → ∞, is the standard normal distribution n(z; 0, 1)."
1384,1,"['normal approximation', 'approximation', 'skewed', 'sampling', 'distribution', 'samples', 'normal', 'population', 'sampling distribution', 'normal distribution']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"the normal approximation for x̄ will generally be good if n ≥ 30, provided the population distribution is not terribly skewed. if n < 30, the approximation is good only if the population is not too different from a normal distribution and, as stated above, if the population is known to be normal, the sampling distribution of x̄ will follow a normal distribution exactly, no matter how small the size of the samples."
1385,1,"['sample size', 'sample', 'observation', 'distribution', 'normality', 'normal', 'mean', 'central limit theorem', 'variance', 'limit']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"the sample size n = 30 is a guideline to use for the central limit theorem. however, as the statement of the theorem implies, the presumption of normality on the distribution of x̄ becomes more accurate as n grows larger. in fact, figure 8.1 illustrates how the theorem works. it shows how the distribution of x̄ becomes closer to normal as n grows larger, beginning with the clearly nonsymmetric distribution of an individual observation (n = 1). it also illustrates that the mean of x̄ remains μ for any sample size and the variance of x̄ gets smaller as n increases."
1386,1,"['distribution', 'limit', 'central limit theorem']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"figure 8.1: illustration of the central limit theorem (distribution of x̄ for n = 1, moderate n, and large n)."
1387,1,"['deviation', 'sample', 'random', 'random sample', 'normally distributed', 'mean', 'probability', 'standard', 'standard deviation', 'average']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"example 8.4: an electrical firm manufactures light bulbs that have a length of life that is approximately normally distributed, with mean equal to 800 hours and a standard deviation of 40 hours. find the probability that a random sample of 16 bulbs will have an average life of less than 775 hours."
1388,1,"['sampling', 'distribution', 'normal', 'probability', 'sampling distribution']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"solution : the sampling distribution of x̄ will be approximately normal, with μx̄ = 800 and σx̄ = 40/√16 = 10. the desired probability is given by the area of the shaded"
1389,0,[], Sampling Distribution of Means and the Central Limit Theorem,seg_165,"corresponding to x̄ = 775, we find that"
1390,1,"['estimation', 'population mean', 'hypothesis testing', 'central limit theorem', 'mean', 'population', 'control', 'quality control', 'limit', 'hypothesis']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"one very important application of the central limit theorem is the determination of reasonable values of the population mean μ. topics such as hypothesis testing, estimation, quality control, and many others make use of the central limit theorem. the following example illustrates the use of the central limit theorem with regard to its relationship with μ, the mean of the population, although the formal application to the foregoing topics is relegated to future chapters."
1391,1,"['case', 'data', 'sampling', 'distribution', 'parameter of interest', 'hypothesis testing', 'central limit theorem', 'mean', 'hypothesis', 'parameter', 'sampling distributions', 'sampling distribution', 'limit', 'distributions']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"in the following case study, an illustration is given which draws an inference that makes use of the sampling distribution of x̄. in this simple illustration, μ and σ are both known. the central limit theorem and the general notion of sampling distributions are often used to produce evidence about some important aspect of a distribution such as a parameter of the distribution. in the case of the central limit theorem, the parameter of interest is the mean μ. the inference made concerning μ may take one of many forms. often there is a desire on the part of the analyst that the data (in the form of x̄) support (or not) some predetermined conjecture concerning the value of μ. the use of what we know about the sampling distribution can contribute to answering this type of question. in the following case study, the concept of hypothesis testing leads to a formal objective that we will highlight in future chapters."
1392,1,['process'], Sampling Distribution of Means and the Central Limit Theorem,seg_165,case study 8.1: automobile parts:an important manufacturing process produces cylindrical component parts for the automotive industry. it is important that the process produce
1393,1,"['deviation', 'sample', 'process', 'population standard deviation', 'experiment', 'population mean', 'information', 'sample average', 'mean', 'population', 'standard', 'standard deviation', 'average']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,parts having a mean diameter of 5.0 millimeters. the engineer involved conjectures that the population mean is 5.0 millimeters. an experiment is conducted in which 100 parts produced by the process are selected randomly and the diameter measured on each. it is known that the population standard deviation is σ = 0.1 millimeter. the experiment indicates a sample average diameter of x̄ = 5.027 millimeters. does this sample information appear to support or refute the engineer’s conjecture?
1394,1,"['hypothesis testing', 'associated', 'hypothesis']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"solution : this example reflects the kind of problem often posed and solved with hypothesis testing machinery introduced in future chapters. we will not use the formality associated with hypothesis testing here, but we will illustrate the principles and logic used."
1395,1,"['experiment', 'population mean', 'data', 'mean', 'probability', 'population']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"whether the data support or refute the conjecture depends on the probability that data similar to those obtained in this experiment (x̄ = 5.027) can readily occur when in fact μ = 5.0 (figure 8.3). in other words, how likely is it that one can obtain x̄ ≥ 5.027 with n = 100 if the population mean is μ = 5.0? if this probability suggests that x̄ = 5.027 is not unreasonable, the conjecture is not refuted. if the probability is quite low, one can certainly argue that the data do not support the conjecture that μ = 5.0. the probability that we choose to compute is given by p (|x̄ − 5| ≥ 0.027)."
1396,1,['case'], Sampling Distribution of Means and the Central Limit Theorem,seg_165,figure 8.3: area for case study 8.1.
1397,1,['mean'], Sampling Distribution of Means and the Central Limit Theorem,seg_165,"in other words, if the mean μ is 5, what is the chance that x̄ will deviate by as much as 0.027 millimeter?"
1398,1,"['central limit theorem', 'limit']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,here we are simply standardizing x̄ according to the central limit theorem. if
1399,1,"['experiments', 'mean', 'experiment']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"therefore, one would experience by chance that an x̄ would be 0.027 millimeter from the mean in only 7 in 1000 experiments. as a result, this experiment with x̄ = 5.027 certainly does not give supporting evidence to the conjecture that μ = 5.0. in fact, it strongly refutes the conjecture!"
1400,1,"['deviation', 'mean', 'probability', 'standard', 'standard deviation', 'average']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"example 8.5: traveling between two campuses of a university in a city via shuttle bus takes, on average, 28 minutes with a standard deviation of 5 minutes. in a given week, a bus transported passengers 40 times. what is the probability that the average transport time was more than 30 minutes? assume the mean time is measured to the nearest minute."
1401,1,"['continuous', 'case', 'probability']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"solution : in this case, μ = 28 and σ = 3. we need to calculate the probability p (x̄ > 30) with n = 40. since the time is measured on a continuous scale to the nearest minute, an x̄ greater than 30 is equivalent to x̄ ≥ 30.5. hence,"
1402,1,['average'], Sampling Distribution of Means and the Central Limit Theorem,seg_165,there is only a slight chance that the average time of one bus trip will exceed 30 minutes. an illustrative graph is shown in figure 8.4.
1403,1,"['statistical inference', 'experiment', 'population mean', 'case', 'mean', 'population', 'statistical']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"the illustration in case study 8.1 deals with notions of statistical inference on a single mean μ. the engineer was interested in supporting a conjecture regarding a single population mean. a far more important application involves two populations. a scientist or engineer may be interested in a comparative experiment in which two manufacturing methods, 1 and 2, are to be compared. the basis for that comparison is μ1 − μ2, the difference in the population means."
1404,1,"['sample', 'random', 'populations', 'statistic', 'mean', 'population', 'random sample', 'variance']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"suppose that we have two populations, the first with mean μ1 and variance σ12, and the second with mean μ2 and variance σ22. let the statistic x̄1 represent the mean of a random sample of size n1 selected from the first population, and the statistic x̄2 represent the mean of a random sample of size n2 selected from"
1405,1,"['sample', 'independent', 'variables', 'approximation', 'normally distributed', 'sampling', 'distribution', 'samples', 'mean', 'population', 'populations', 'sampling distribution', 'variances']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"the second population, independent of the sample from the first population. what can we say about the sampling distribution of the difference x̄1 − x̄2 for repeated samples of size n1 and n2? according to theorem 8.2, the variables x̄1 and x̄2 are both approximately normally distributed with means μ1 and μ2 and variances σ12/n1 and σ22/n2, respectively. this approximation improves as n1 and n2 increase. by choosing independent samples from the two populations we ensure that the variables x̄1 and x̄2 will be independent, and then using theorem 7.11, with a1 = 1 and a2 = −1, we can conclude that x̄1 − x̄2 is approximately normally distributed with mean"
1406,1,['variance'], Sampling Distribution of Means and the Central Limit Theorem,seg_165,and variance
1407,1,"['central limit theorem', 'case', 'limit']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"the central limit theorem can be easily extended to the two-sample, two-population case."
1408,1,"['independent', 'discrete', 'normally distributed', 'sampling', 'distribution', 'samples', 'mean', 'random', 'continuous', 'variance', 'sampling distribution', 'variances']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"theorem 8.3: if independent samples of size n1 and n2 are drawn at random from two populations, discrete or continuous, with means μ1 and μ2 and variances σ12 and σ22, respectively, then the sampling distribution of the differences of means, x̄1 − x̄2, is approximately normally distributed with mean and variance given by"
1409,1,"['standard normal', 'variable', 'normal', 'standard']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,is approximately a standard normal variable.
1410,1,"['normal approximation', 'approximation', 'distribution', 'normal', 'populations', 'normal distribution', 'distributions']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"if both n1 and n2 are greater than or equal to 30, the normal approximation for the distribution of x̄1 − x̄2 is very good when the underlying distributions are not too far away from normal. however, even when n1 and n2 are less than 30, the normal approximation is reasonably good except when the populations are decidedly nonnormal. of course, if both populations are normal, then x̄1− x̄2 has a normal distribution no matter what the sizes of n1 and n2 are."
1411,1,"['sample', 'utility', 'sample means', 'case', 'sampling', 'distribution', 'mean', 'population', 'sampling distribution']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,the utility of the sampling distribution of the difference between two sample averages is very similar to that described in case study 8.1 on page 235 for the case of a single mean. case study 8.2 that follows focuses on the use of the difference between two sample means to support (or not) the conjecture that two population means are the same.
1412,1,"['independent', 'standard deviations', 'deviations', 'population', 'standard', 'experiments']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"case study 8.2: paint drying time: two independent experiments are run in which two different types of paint are compared. eighteen specimens are painted using type a, and the drying time, in hours, is recorded for each. the same is done with type b. the population standard deviations are both known to be 1.0."
1413,1,"['samples', 'average', 'mean']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"assuming that the mean drying time is equal for the two types of paint, find p (x̄a− x̄b > 1.0), where x̄a and x̄b are average drying times for samples of size na = nb = 18."
1414,1,"['sampling', 'distribution', 'normal', 'mean', 'sampling distribution']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"solution : from the sampling distribution of x̄a − x̄b , we know that the distribution is approximately normal with mean"
1415,1,['variance'], Sampling Distribution of Means and the Central Limit Theorem,seg_165,and variance
1416,1,['case'], Sampling Distribution of Means and the Central Limit Theorem,seg_165,figure 8.5: area for case study 8.2.
1417,1,['probability'], Sampling Distribution of Means and the Central Limit Theorem,seg_165,"the desired probability is given by the shaded region in figure 8.5. corresponding to the value x̄a − x̄b = 1.0, we have"
1418,1,"['experiment', 'population mean', 'mean', 'population']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"the machinery in the calculation is based on the presumption that μa = μb . suppose, however, that the experiment is actually conducted for the purpose of drawing an inference regarding the equality of μa and μb , the two population mean drying times. if the two averages differ by as much as 1 hour (or more), this clearly is evidence that would lead one to conclude that the population mean drying time is not equal for the two types of paint. on the other hand, suppose"
1419,1,['sample'], Sampling Distribution of Means and the Central Limit Theorem,seg_165,"that the difference in the two sample averages is as small as, say, 15 minutes. if μa = μb,"
1420,1,"['sample', 'sample means', 'probability', 'average']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"since this probability is not low, one would conclude that a difference in sample means of 15 minutes can happen by chance (i.e., it happens frequently even though μa = μb). as a result, that type of difference in average drying times certainly is not a clear signal that μa = μb."
1421,1,"['statistical inference', 'sampling', 'statistical', 'hypothesis testing', 'hypothesis', 'central limit theorem', 'sampling distributions', 'limit', 'distributions']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"as we indicated earlier, a more detailed formalism regarding this and other types of statistical inference (e.g., hypothesis testing) will be supplied in future chapters. the central limit theorem and sampling distributions discussed in the next three sections will also play a vital role."
1422,1,"['deviation', 'sample', 'standard', 'random sample', 'mean', 'probability', 'random', 'standard deviation']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"example 8.6: the television picture tubes of manufacturer a have a mean lifetime of 6.5 years and a standard deviation of 0.9 year, while those of manufacturer b have a mean lifetime of 6.0 years and a standard deviation of 0.8 year. what is the probability that a random sample of 36 tubes from manufacturer a will have a mean lifetime that is at least 1 year more than the mean lifetime of a sample of 49 tubes from manufacturer b?"
1423,1,['information'], Sampling Distribution of Means and the Central Limit Theorem,seg_165,solution : we are given the following information:
1424,1,['population'], Sampling Distribution of Means and the Central Limit Theorem,seg_165,population 1 population 2 μ1 = 6.5 μ2 = 6.0 σ1 = 0.9 σ2 = 0.8 n1 = 36 n2 = 49
1425,1,"['deviation', 'sampling', 'distribution', 'normal', 'mean', 'standard', 'standard deviation', 'sampling distribution']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"if we use theorem 8.3, the sampling distribution of x̄1 − x̄2 will be approximately normal and will have a mean and standard deviation"
1426,1,"['mean', 'probability']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"the probability that the mean lifetime for 36 tubes from manufacturer a will be at least 1 year longer than the mean lifetime for 49 tubes from manufacturer b is given by the area of the shaded region in figure 8.6. corresponding to the value x̄1 − x̄2 = 1.0, we find that"
1427,1,"['normal approximation', 'set', 'random', 'binomial distribution', 'experiment', 'approximation', 'successes', 'trials', 'random variable', 'binomial random variable', 'trial', 'limit', 'parameters', 'distribution', 'binomial', 'central limit theorem', 'outcome', 'independent', 'variable', 'normal', 'average', 'normal distribution']", Sampling Distribution of Means and the Central Limit Theorem,seg_165,"section 6.5 presented the normal approximation to the binomial distribution at length. conditions were given on the parameters n and p for which the distribution of a binomial random variable can be approximated by the normal distribution. examples and exercises reflected the importance of the concept of the “normal approximation.” it turns out that the central limit theorem sheds even more light on how and why this approximation works. we certainly know that a binomial random variable is the number x of successes in n independent trials, where the outcome of each trial is binary. we also illustrated in chapter 1 that the proportion computed in such an experiment is an average of a set of 0s and 1s. indeed, while the proportion x/n is an average, x is the sum of this set of 0s and 1s, and both x and x/n are approximately normal if n is sufficiently large. of course, from what we learned in chapter 6, we know that there are conditions on n and p that affect the quality of the approximation, namely np ≥ 5 and nq ≥ 5."
1428,1,"['sampling', 'distribution', 'central limit theorem', 'sampling distribution', 'limit']", Sampling Distribution of S,seg_169,in the preceding section we learned about the sampling distribution of x̄. the central limit theorem allowed us to make use of the fact that
1429,1,"['statistics', 'population variance', 'sample', 'information', 'mean', 'population', 'sampling distributions', 'distributions', 'sample size', 'parameters', 'distribution', 'variance', 'sampling distribution', 'variability', 'population mean', 'sampling']", Sampling Distribution of S,seg_169,"tends toward n(0, 1) as the sample size grows large. sampling distributions of important statistics allow us to learn information about parameters. usually, the parameters are the counterpart to the statistics in question. for example, if an engineer is interested in the population mean resistance of a certain type of resistor, the sampling distribution of x̄ will be exploited once the sample information is gathered. on the other hand, if the variability in resistance is to be studied, clearly the sampling distribution of s2 will be used in learning about the parametric counterpart, the population variance σ2."
1430,1,"['sample', 'random sample', 'distribution', 'statistic', 'normal', 'mean', 'population', 'random', 'sample variance', 'variance']", Sampling Distribution of S,seg_169,"if a random sample of size n is drawn from a normal population with mean μ and variance σ2, and the sample variance is computed, we obtain a value of the statistic s2. we shall proceed to consider the distribution of the statistic (n− 1)s2/σ2."
1431,1,"['sample', 'mean', 'sample mean']", Sampling Distribution of S,seg_169,"by tnhe addition andnsubtraction of the sample mean x̄, it is easy to see that"
1432,0,[], Sampling Distribution of S,seg_169,"now, according to corollary 7.1 on page 222, we know that"
1433,1,"['degrees of freedom', 'gamma', 'case', 'distribution', 'random variable', 'variable', 'degree of freedom', 'random', 'gamma distribution']", Sampling Distribution of S,seg_169,"is a chi-squared random variable with n degrees of freedom. we have a chi-squared random variable with n degrees of freedom partitioned into two components. note that in section 6.7 we showed that a chi-squared distribution is a special case of a gamma distribution. the second term on the right-hand side is z2, which is a chi-squared random variable with 1 degree of freedom, and it turns out that (n − 1)s2/σ2 is a chi-squared random variable with n − 1 degree of freedom. we formalize this in the following theorem."
1434,1,"['sample', 'random', 'statistic', 'normal', 'population', 'random sample', 'variance']", Sampling Distribution of S,seg_169,"theorem 8.4: if s2 is the variance of a random sample of size n taken from a normal population having the variance σ2, then the statistic"
1435,1,"['degrees of freedom', 'distribution']", Sampling Distribution of S,seg_169,has a chi-squared distribution with v = n− 1 degrees of freedom.
1436,1,"['sample', 'variable', 'random variable', 'random']", Sampling Distribution of S,seg_169,the values of the random variable χ2 are calculated from each sample by the
1437,1,"['sample', 'curve', 'random', 'probability', 'random sample']", Sampling Distribution of S,seg_169,the probability that a random sample produces a χ2 value greater than some specified value is equal to the area under the curve to the right of this value. it is customary to let χα2 represent the χ2 value above which we find an area of α. this is illustrated by the shaded region in figure 8.7.
1438,1,['distribution'], Sampling Distribution of S,seg_169,figure 8.7: the chi-squared distribution.
1439,1,"['degrees of freedom', 'lack of symmetry', 'table', 'symmetry', 'tables']", Sampling Distribution of S,seg_169,"table a.5 gives values of χ2α for various values of α and v. the areas, α, are the column headings; the degrees of freedom, v, are given in the left column; and the table entries are the χ2 values. hence, the χ2 value with 7 degrees of freedom, leaving an area of 0.05 to the right, is χ20.05 = 14.067. owing to lack of symmetry, we must also use the tables to find χ20.95 = 2.167 for v = 7."
1440,1,"['distribution', 'error']", Sampling Distribution of S,seg_169,"exactly 95% of a chi-squared distribution lies between χ20.975 and χ20.025. a χ2 value falling to the right of χ20.025 is not likely to occur unless our assumed value of σ2 is too small. similarly, a χ2 value falling to the left of χ20.975 is unlikely unless our assumed value of σ2 is too large. in other words, it is possible to have a χ2 value to the left of χ20.975 or to the right of χ20.025 when σ2 is correct, but if this should occur, it is more probable that the assumed value of σ2 is in error."
1441,1,"['deviation', 'distribution', 'normal', 'standard', 'standard deviation', 'average', 'normal distribution']", Sampling Distribution of S,seg_169,"example 8.7: a manufacturer of car batteries guarantees that the batteries will last, on average, 3 years with a standard deviation of 1 year. if five of these batteries have lifetimes of 1.9, 2.4, 3.0, 3.5, and 4.2 years, should the manufacturer still be convinced that the batteries have a standard deviation of 1 year? assume that the battery lifetime follows a normal distribution."
1442,1,"['sample', 'variance', 'sample variance']", Sampling Distribution of S,seg_169,"solution : we first find the sample variance using theorem 8.1,"
1443,1,"['deviation', 'degrees of freedom', 'distribution', 'standard', 'standard deviation']", Sampling Distribution of S,seg_169,"is a value from a chi-squared distribution with 4 degrees of freedom. since 95% of the χ2 values with 4 degrees of freedom fall between 0.484 and 11.143, the computed value with σ2 = 1 is reasonable, and therefore the manufacturer has no reason to suspect that the standard deviation is other than 1 year."
1444,0,[], Sampling Distribution of S,seg_169,recall from corollary 7.1 in section 7.3 that
1445,1,"['degrees of freedom', 'random variable', 'variable', 'random']", Sampling Distribution of S,seg_169,"has a χ2-distribution with n degrees of freedom. note also theorem 8.4, which indicates that the random variable"
1446,1,['degrees of freedom'], Sampling Distribution of S,seg_169,"has a χ2-distribution with n−1 degrees of freedom. the reader may also recall that the term degrees of freedom, used in this identical context, is discussed in chapter 1."
1447,1,['distribution'], Sampling Distribution of S,seg_169,"as we indicated earlier, the proof of theorem 8.4 will not be given. however, the reader can view theorem 8.4 as indicating that when μ is not known and one considers the distribution of"
1448,1,"['sample', 'independent', 'estimate', 'estimation', 'random sample', 'data', 'information', 'distribution', 'normal', 'mean', 'degree of freedom', 'random', 'normal distribution']", Sampling Distribution of S,seg_169,"there is 1 less degree of freedom, or a degree of freedom is lost in the estimation of μ (i.e., when μ is replaced by x̄). in other words, there are n degrees of freedom, or independent pieces of information, in the random sample from the normal distribution. when the data (the values in the sample) are used to compute the mean, there is 1 less degree of freedom in the information used to estimate σ2."
1449,1,"['statistic', 'process', 'sample', 'estimate', 'information', 'mean', 'population', 'standard', 'standard deviation', 'limit', 'experimental', 'distribution', 'sample average', 'central limit theorem', 'deviation', 'utility', 'population mean', 'normal', 'population standard deviation', 'average', 'normal distribution']", tDistribution,seg_171,"in section 8.4, we discussed the utility of the central limit theorem. its applications revolve around inferences on a population mean or the difference between two population means. use of the central limit theorem and the normal distribution is certainly helpful in this context. however, it was assumed that the population standard deviation is known. this assumption may not be unreasonable in situations where the engineer is quite familiar with the system or process. however, in many experimental scenarios, knowledge of σ is certainly no more reasonable than knowledge of the population mean μ. often, in fact, an estimate of σ must be supplied by the same sample information that produced the sample average x̄. as a result, a natural statistic to consider to deal with inferences on μ is"
1450,1,"['sample size', 'sample', 'standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", tDistribution,seg_171,"since s is the sample analog to σ. if the sample size is small, the values of s2 fluctuate considerably from sample to sample (see exercise 8.43 on page 259) and the distribution of t deviates appreciably from that of a standard normal distribution."
1451,1,"['sample size', 'sample', 'random', 'standard normal', 'random sample', 'sampling', 'distribution', 'normal', 'population', 'standard', 'sampling distribution']", tDistribution,seg_171,"if the sample size is large enough, say n ≥ 30, the distribution of t does not differ considerably from the standard normal. however, for n < 30, it is useful to deal with the exact distribution of t . in developing the sampling distribution of t , we shall assume that our random sample was selected from a normal population. we can then write"
1452,1,"['standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", tDistribution,seg_171,has the standard normal distribution and
1453,1,"['degrees of freedom', 'random', 'function', 'random variable', 'populations', 'standard', 'density function', 'standard normal', 'distribution', 'independent', 'sampling', 'variable', 'normal']", tDistribution,seg_171,"has a chi-squared distribution with v = n−1 degrees of freedom. in sampling from normal populations, we can show that x̄ and s2 are independent, and consequently so are z and v . the following theorem gives the definition of a random variable t as a function of z (standard normal) and χ2. for completeness, the density function of the t-distribution is given."
1454,1,"['standard', 'standard normal', 'random variable', 'variable', 'normal', 'standard normal random variable', 'random', 'normal random variable']", tDistribution,seg_171,let z be a standard normal random variable and v a chi-squared random variable
1455,1,"['degrees of freedom', 'independent', 'distribution', 'random variable', 'variable', 'random']", tDistribution,seg_171,"theorem 8.5: with v degrees of freedom. if z and v are independent, then the distribution of the random variable t , where"
1456,1,"['function', 'density function']", tDistribution,seg_171,is given by the density function
1457,1,['degrees of freedom'], tDistribution,seg_171,this is known as the t-distribution with v degrees of freedom.
1458,0,[], tDistribution,seg_171,from the foregoing and the theorem above we have the following corollary.
1459,1,"['deviation', 'random variables', 'random', 'independent', 'variables', 'independent random variables', 'normal', 'mean', 'standard', 'standard deviation']", tDistribution,seg_171,"corollary 8.1: let x1, x2, . . . , xn be independent random variables that are all normal with mean μ and standard deviation σ. let"
1460,1,"['random variable', 'variable', 'random']", tDistribution,seg_171,x̄−μ then the random variable t = has a t-distribution with v = n − 1 degrees
1461,1,"['distribution', 'samples', 'probability distribution', 'normal', 'population', 'probability', 'populations', 'distributions']", tDistribution,seg_171,"the probability distribution of t was first published in 1908 in a paper written by w. s. gosset. at the time, gosset was employed by an irish brewery that prohibited publication of research by members of its staff. to circumvent this restriction, he published his work secretly under the name “student.” consequently, the distribution of t is usually called the student t-distribution or simply the tdistribution. in deriving the equation of this distribution, gosset assumed that the samples were selected from a normal population. although this would seem to be a very restrictive assumption, it can be shown that nonnormal populations possessing nearly bell-shaped distributions will still provide values of t that approximate the t-distribution very closely."
1462,1,"['degrees of freedom', 'standard normal distribution', 'sample', 'symmetric', 'mean', 'standard', 'distributions', 'sample size', 'standard normal', 'distribution', 'variance', 'percentage', 'table', 'variable', 'normal', 'normal distribution']", tDistribution,seg_171,"the distribution of t is similar to the distribution of z in that they both are symmetric about a mean of zero. both distributions are bell shaped, but the tdistribution is more variable, owing to the fact that the t -values depend on the fluctuations of two quantities, x̄ and s2, whereas the z-values depend only on the changes in x̄ from sample to sample. the distribution of t differs from that of z in that the variance of t depends on the sample size n and is always greater than 1. only when the sample size n → ∞ will the two distributions become the same. in figure 8.8, we show the relationship between a standard normal distribution (v = ∞) and t-distributions with 2 and 5 degrees of freedom. the percentage points of the t-distribution are given in table a.4."
1463,1,['symmetry'], tDistribution,seg_171,"figure 8.8: the t-distribution curves for v = 2, 5, figure 8.9: symmetry property (about 0) of the and ∞. t-distribution."
1464,1,"['degrees of freedom', 'symmetric', 'distribution', 'mean', 'tail']", tDistribution,seg_171,"it is customary to let tα represent the t-value above which we find an area equal to α. hence, the t-value with 10 degrees of freedom leaving an area of 0.025 to the right is t = 2.228. since the t-distribution is symmetric about a mean of zero, we have t1−α = −tα; that is, the t-value leaving an area of 1− α to the right and therefore an area of α to the left is equal to the negative t-value that leaves an area of α in the right tail of the distribution (see figure 8.9). that is, t0.95 = −t0.05, t0.99 = −t0.01, and so forth."
1465,1,['degrees of freedom'], tDistribution,seg_171,"example 8.8: the t-value with v = 14 degrees of freedom that leaves an area of 0.025 to the left, and therefore an area of 0.975 to the right, is"
1466,1,"['sample', 'random sample', 'distribution', 'normal', 'random', 'normal distribution']", tDistribution,seg_171,example 8.10: find k such that p (k < t < −1.761) = 0.045 for a random sample of size 15 x−μ selected from a normal distribution and .
1467,1,"['probability', 'table']", tDistribution,seg_171,"solution : from table a.4 we note that 1.761 corresponds to t0.05 when v = 14. therefore, −t0.05 = −1.761. since k in the original probability statement is to the left of −t0.05 = −1.761, let k = −tα. then, from figure 8.10, we have"
1468,1,['table'], tDistribution,seg_171,"hence, from table a.4 with v = 14,"
1469,1,"['degrees of freedom', 'interval', 'table', 'tails', 'distribution', 'parameter', 'event', 'error']", tDistribution,seg_171,"exactly 95% of the values of a t-distribution with v = n− 1 degrees of freedom lie between −t0.025 and t0.025. of course, there are other t-values that contain 95% of the distribution, such as −t0.02 and t0.03, but these values do not appear in table a.4, and furthermore, the shortest possible interval is obtained by choosing t-values that leave exactly the same area in the two tails of our distribution. a t-value that falls below −t0.025 or above t0.025 would tend to make us believe either that a very rare event has taken place or that our assumption about μ is in error. should this happen, we shall make the the decision that our assumed value of μ is in error. in fact, a t-value falling below −t0.01 or above t0.01 would provide even stronger evidence that our assumed value of μ is quite unlikely. general procedures for testing claims concerning the value of the parameter μ will be treated in chapter 10. a preliminary look into the foundation of these procedure is illustrated by the following example."
1470,1,"['deviation', 'sample', 'sample standard deviation', 'population mean', 'distribution', 'samples', 'normal', 'mean', 'population', 'standard', 'standard deviation', 'process']", tDistribution,seg_171,"example 8.11: a chemical engineer claims that the population mean yield of a certain batch process is 500 grams per milliliter of raw material. to check this claim he samples 25 batches each month. if the computed t-value falls between −t0.05 and t0.05, he is satisfied with this claim. what conclusion should he draw from a sample that has a mean x̄ = 518 grams per milliliter and a sample standard deviation s = 40 grams? assume the distribution of yields to be approximately normal."
1471,1,"['sample', 'degrees of freedom', 'table']", tDistribution,seg_171,"solution : from table a.4 we find that t0.05 = 1.711 for 24 degrees of freedom. therefore, the engineer can be satisfied with his claim if a sample of 25 batches yields a t-value between −1.711 and 1.711. if μ= 500, then"
1472,1,"['sample', 'process', 'probability']", tDistribution,seg_171,"a value well above 1.711. the probability of obtaining a t-value, with v = 24, equal to or greater than 2.25 is approximately 0.02. if μ > 500, the value of t computed from the sample is more reasonable. hence, the engineer is likely to conclude that the process produces a better product than he thought."
1473,1,"['population mean', 'distribution', 'samples', 'statistic', 'cases', 'mean', 'population']", tDistribution,seg_171,"the t-distribution is used extensively in problems that deal with inference about the population mean (as illustrated in example 8.11) or in problems that involve comparative samples (i.e., in cases where one is trying to determine if means from two samples are significantly different). the use of the distribution will be extended in chapters 9, 10, 11, and 12. the reader should note that use of the t-distribution for the statistic"
1474,1,"['sample size', 'sample', 'standard normal', 'case', 'distribution', 'normal', 'central limit theorem', 'standard', 'standard normal distribution', 'estimator', 'limit', 'normal distribution']", tDistribution,seg_171,"requires that x1, x2, . . . , xn be normal. the use of the t-distribution and the sample size consideration do not relate to the central limit theorem. the use of the standard normal distribution rather than t for n ≥ 30 merely implies that s is a sufficiently good estimator of σ in this case. in chapters that follow the t-distribution finds extensive usage."
1475,1,"['case', 'sample', 'sample variances', 'sample means', 'data', 'information', 'samples', 'population', 'variability', 'sampling', 'variances']", F Distribution,seg_173,"we have motivated the t-distribution in part by its application to problems in which there is comparative sampling (i.e., a comparison between two sample means). for example, some of our examples in future chapters will take a more formal approach, chemical engineer collects data on two catalysts, biologist collects data on two growth media, or chemist gathers data on two methods of coating material to inhibit corrosion. while it is of interest to let sample information shed light on two population means, it is often the case that a comparison of variability is equally important, if not more so. the f -distribution finds enormous application in comparing sample variances. applications of the f -distribution are found in problems involving two or more samples."
1476,1,"['degrees of freedom', 'random variables', 'independent', 'variables', 'statistic', 'random']", F Distribution,seg_173,"the statistic f is defined to be the ratio of two independent chi-squared random variables, each divided by its number of degrees of freedom. hence, we can write"
1477,1,"['degrees of freedom', 'random variables', 'independent', 'variables', 'sampling', 'distribution', 'independent random variables', 'random', 'sampling distribution', 'distributions']", F Distribution,seg_173,"where u and v are independent random variables having chi-squared distributions with v1 and v2 degrees of freedom, respectively. we shall now state the sampling distribution of f ."
1478,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random', 'distributions']", F Distribution,seg_173,let u and v be two independent random variables having chi-squared distributions
1479,1,"['distribution', 'degrees of freedom']", F Distribution,seg_173,"theorem 8.6: with v1 and v2 degrees of freedom, respectively. then the distribution of the"
1480,1,"['function', 'variable', 'density function']", F Distribution,seg_173,random variable f = u/v1 is given by the density function
1481,1,['degrees of freedom'], F Distribution,seg_173,this is known as the f-distribution with v1 and v2 degrees of freedom (d.f.).
1482,1,"['density function', 'curve', 'parameters', 'random variable', 'variable', 'random', 'function']", F Distribution,seg_173,"we will make considerable use of the random variable f in future chapters. however, the density function will not be used and is given only for completeness. the curve of the f -distribution depends not only on the two parameters v1 and v2 but also on the order in which we state them. once these two values are given, we can identify the curve. typical f -distributions are shown in figure 8.11."
1483,1,"['degrees of freedom', 'table', 'combinations']", F Distribution,seg_173,"let fα be the f -value above which we find an area equal to α. this is illustrated by the shaded region in figure 8.12. table a.6 gives values of fα only for α = 0.05 and α = 0.01 for various combinations of the degrees of freedom v1 and v2. hence, the f -value with 6 and 10 degrees of freedom, leaving an area of 0.05 to the right, is f0.05 = 3.22. by means of the following theorem, table a.6 can also be used to find values of f0.95 and f0.99. the proof is left for the reader."
1484,1,['distribution'], F Distribution,seg_173,figure 8.11: typical f -distributions. figure 8.12: illustration of the fα for the f - distribution.
1485,1,['degrees of freedom'], F Distribution,seg_173,"theorem 8.7: writing fα(v1, v2) for fα with v1 and v2 degrees of freedom, we obtain"
1486,1,['degrees of freedom'], F Distribution,seg_173,"thus, the f -value with 6 and 10 degrees of freedom, leaving an area of 0.95 to the right, is"
1487,1,"['random samples', 'populations', 'samples', 'normal', 'random', 'variances']", F Distribution,seg_173,"suppose that random samples of size n1 and n2 are selected from two normal populations with variances σ12 and σ22, respectively. from theorem 8.4, we know that"
1488,1,"['degrees of freedom', 'random variables', 'independent', 'variables', 'samples', 'independent random variables', 'random', 'distributions']", F Distribution,seg_173,"are random variables having chi-squared distributions with v1 = n1 − 1 and v2 = n2 − 1 degrees of freedom. furthermore, since the samples are selected at random, we are dealing with independent random variables. then, using theorem 8.6 with χ21 = u and χ22 = v , we obtain the following result."
1489,1,"['independent', 'random samples', 'populations', 'samples', 'normal', 'random', 'variances']", F Distribution,seg_173,"theorem 8.8: if s12 and s22 are the variances of independent random samples of size n1 and n2 taken from normal populations with variances σ12 and σ22, respectively, then"
1490,1,['degrees of freedom'], F Distribution,seg_173,has an f -distribution with v1 = n1 − 1 and v2 = n2 − 1 degrees of freedom.
1491,1,"['experiment', 'variance ratio distribution', 'case', 'information', 'distribution', 'normal', 'mean', 'population', 'variance', 'normal distribution', 'variances']", F Distribution,seg_173,"we answered this question, in part, at the beginning of this section. the f - distribution is used in two-sample situations to draw inferences about the population variances. this involves the application of theorem 8.8. however, the f -distribution can also be applied to many other types of problems involving sample variances. in fact, the f -distribution is called the variance ratio distribution. as an illustration, consider case study 8.2, in which two paints, a and b, were compared with regard to mean drying time. the normal distribution applies nicely (assuming that σa and σb are known). however, suppose that there are three types of paints to compare, say a, b, and c. we wish to determine if the population means are equivalent. suppose that important summary information from the experiment is as follows:"
1492,1,"['sample size', 'sample', 'sample mean', 'mean sample', 'variance sample', 'mean', 'sample variance', 'variance']", F Distribution,seg_173,paint sample mean sample variance sample size
1493,1,"['plot', 'sample', 'graphics', 'variability', 'data', 'information', 'samples']", F Distribution,seg_173,"the problem centers around whether or not the sample averages (x̄a, x̄b , x̄c) are far enough apart. the implication of “far enough apart” is very important. it would seem reasonable that if the variability between sample averages is larger than what one would expect by chance, the data do not support the conclusion that μa = μb = μc . whether these sample averages could have occurred by chance depends on the variability within samples, as quantified by s2a, s2b , and s2c . the notion of the important components of variability is best seen through some simple graphics. consider the plot of raw data from samples a, b, and c, shown in figure 8.13. these data could easily have generated the above summary information."
1494,1,"['samples', 'data']", F Distribution,seg_173,figure 8.13: data from three distinct samples.
1495,1,"['sample', 'variability', 'data', 'samples', 'jointly', 'mean', 'population', 'populations', 'distributions']", F Distribution,seg_173,"it appears evident that the data came from distributions with different population means, although there is some overlap between the samples. an analysis that involves all of the data would attempt to determine if the variability between the sample averages and the variability within the samples could have occurred jointly if in fact the populations have a common mean. notice that the key to this analysis centers around the two following sources of variability."
1496,1,"['variability', 'observations', 'samples']", F Distribution,seg_173,(1) variability within samples (between observations in distinct samples)
1497,1,"['sample', 'variability', 'samples']", F Distribution,seg_173,(2) variability between samples (between sample averages)
1498,1,"['sample', 'data', 'variability']", F Distribution,seg_173,"clearly, if the variability in (1) is considerably larger than that in (2), there will be considerable overlap in the sample data, a signal that the data could all have come"
1499,1,"['sample', 'variability', 'data', 'distribution', 'samples', 'set', 'data set', 'mean', 'distributions']", F Distribution,seg_173,"from a common distribution. an example is found in the data set shown in figure 8.14. on the other hand, it is very unlikely that data from distributions with a common mean could have variability between sample averages that is considerably larger than the variability within samples."
1500,1,"['data', 'population']", F Distribution,seg_173,figure 8.14: data that easily could have come from the same population.
1501,1,"['sample', 'sample variances', 'variability', 'analysis of variance', 'variance', 'variances']", F Distribution,seg_173,"the sources of variability in (1) and (2) above generate important ratios of sample variances, and ratios are used in conjunction with the f -distribution. the general procedure involved is called analysis of variance. it is interesting that in the paint example described here, we are dealing with inferences on three population means, but two sources of variability are used. we will not supply details here, but in chapters 13 through 15 we make extensive use of analysis of variance, and, of course, the f -distribution plays an important role."
1502,1,"['plots', 'set', 'sample', 'data', 'information', 'samples', 'symmetry', 'populations', 'distributions', 'experimental', 'distribution', 'variability']", Quantile and Probability Plots,seg_175,"in chapter 1 we introduced the reader to empirical distributions. the motivation is to use creative displays to extract information about properties of a set of data. for example, stem-and-leaf plots provide the viewer with a look at symmetry and other properties of the data. in this chapter we deal with samples, which, of course, are collections of experimental data from which we draw conclusions about populations. often the appearance of the sample provides information about the distribution from which the data are taken. for example, in chapter 1 we illustrated the general nature of pairs of samples with point plots that displayed a relative comparison between central tendency and variability in two samples."
1503,1,"['probability plots', 'plots', 'frequency', 'probability', 'varying', 'graphical', 'data', 'information', 'histograms', 'distribution', 'normal probability plots', 'quantile', 'normal', 'quantile plots', 'normal distribution']", Quantile and Probability Plots,seg_175,"in chapters that follow, we often make the assumption that a distribution is normal. graphical information regarding the validity of this assumption can be retrieved from displays like stem-and-leaf plots and frequency histograms. in addition, we will introduce the notion of normal probability plots and quantile plots in this section. these plots are used in studies that have varying degrees of complexity, with the main objective of the plots being to provide a diagnostic check on the assumption that the data came from a normal distribution."
1504,1,"['sample', 'graphical', 'method', 'variability', 'statistics', 'location', 'information', 'samples', 'statistical', 'process']", Quantile and Probability Plots,seg_175,"we can characterize statistical analysis as the process of drawing conclusions about systems in the presence of system variability. for example, an engineer’s attempt to learn about a chemical process is often clouded by process variability. a study involving the number of defective items in a production process is often made more difficult by variability in the method of manufacture of the items. in what has preceded, we have learned about samples and statistics that express center of location and variability in the sample. these statistics provide single measures, whereas a graphical display adds additional information through a picture."
1505,1,"['plot', 'quantile', 'case', 'data', 'set', 'data set', 'quantile plot']", Quantile and Probability Plots,seg_175,one type of plot that can be particularly useful in characterizing the nature of a data set is the quantile plot. as in the case of the box-and-whisker plot (section
1506,1,"['plot', 'quantile', 'graphics', 'statistical inference', 'plots', 'case', 'data', 'samples', 'set', 'associated', 'data set', 'quantile plots', 'statistical', 'quantile plot']", Quantile and Probability Plots,seg_175,"1.6), one can use the basic ideas in the quantile plot to compare samples of data, where the goal of the analyst is to draw distinctions. further illustrations of this type of usage of quantile plots will be given in future chapters where the formal statistical inference associated with comparing samples is discussed. at that point, case studies will expose the reader to both the formal inference and the diagnostic graphics for the same data set."
1507,1,"['plot', 'sample', 'quantile', 'cumulative distribution function', 'distribution function', 'distribution', 'function', 'quantile plot']", Quantile and Probability Plots,seg_175,"the purpose of the quantile plot is to depict, in sample form, the cumulative distribution function discussed in chapter 3."
1508,1,"['sample', 'quantile', 'data']", Quantile and Probability Plots,seg_175,"definition 8.6: a quantile of a sample, q(f), is a value for which a specified fraction f of the data values is less than or equal to q(f)."
1509,1,"['sample median', 'sample', 'quantile', 'percentile', 'median', 'estimate', 'distribution', 'quartile', 'population', 'upper quartile']", Quantile and Probability Plots,seg_175,"obviously, a quantile represents an estimate of a characteristic of a population, or rather, the theoretical distribution. the sample median is q(0.5). the 75th percentile (upper quartile) is q(0.75) and the lower quartile is q(0.25)."
1510,1,"['plot', 'quantile', 'observations', 'plots', 'data', 'quantile plot']", Quantile and Probability Plots,seg_175,"a quantile plot simply plots the data values on the vertical axis against an empirical assessment of the fraction of observations exceeded by the data value. for theoretical purposes, this fraction is computed as"
1511,1,['observations'], Quantile and Probability Plots,seg_175,"where i is the order of the observations when they are ranked from low to high. in other words, if we denote the ranked observations as"
1512,1,"['plot', 'quantile', 'data', 'quantile plot']", Quantile and Probability Plots,seg_175,"then the quantile plot depicts a plot of y(i) against fi. in figure 8.15, the quantile plot is given for the paint can ear data discussed previously."
1513,1,"['plot', 'quantile', 'median', 'quantiles', 'data', 'quartile', 'samples', 'slopes', 'upper quartile', 'plotting', 'quantile plotting', 'quantile plot']", Quantile and Probability Plots,seg_175,"unlike the box-and-whisker plot, the quantile plot actually shows all observations. all quantiles, including the median and the upper and lower quantile, can be approximated visually. for example, we readily observe a median of 35 and an upper quartile of about 36. relatively large clusters around specific values are indicated by slopes near zero, while sparse data in certain areas produce steeper slopes. figure 8.15 depicts sparsity of data from the values 28 through 30 but relatively high density at 36 through 38. in chapters 9 and 10 we pursue quantile plotting further by illustrating useful ways of comparing distinct samples."
1514,1,"['observations', 'set', 'normal distribution', 'random', 'data', 'information', 'tests', 'distribution', 'data set', 'test', 'plot', 'random variables', 'independent', 'variables', 'normal']", Quantile and Probability Plots,seg_175,"it should be somewhat evident to the reader that detection of whether or not a data set came from a normal distribution can be an important tool for the data analyst. as we indicated earlier in this section, we often make the assumption that all or subsets of observations in a data set are realizations of independent identically distributed normal random variables. once again, the diagnostic plot can often nicely augment (for display purposes) a formal goodness-of-fit test on the data. goodness-of-fit tests are discussed in chapter 10. readers of a scientific paper or report tend to find diagnostic information much clearer, less dry, and perhaps less boring than a formal analysis. in later chapters (chapters 9 through 13), we focus"
1515,1,"['plot', 'quantile', 'data', 'quantile plot']", Quantile and Probability Plots,seg_175,figure 8.15: quantile plot for paint data.
1516,1,"['model building', 'design of experiments', 'design', 'plots', 'statistical', 'model', 'statistical inference', 'distribution', 'plot', 'quantile', 'deviations', 'normality', 'normal', 'quantile plots', 'experiments']", Quantile and Probability Plots,seg_175,"again on methods of detecting deviations from normality as an augmentation of formal statistical inference. quantile plots are useful in detection of distribution types. there are also situations in both model building and design of experiments in which the plots are used to detect important model terms or effects that are active. in other situations, they are used to determine whether or not the underlying assumptions made by the scientist or engineer in building the model are reasonable. many examples with illustrations will be encountered in chapters 11, 12, and 13. the following subsection provides a discussion and illustration of a diagnostic plot called the normal quantile-quantile plot."
1517,1,"['plot', 'quantile', 'approximation', 'quantiles', 'distribution', 'random variable', 'variable', 'normal', 'random', 'normal distribution']", Quantile and Probability Plots,seg_175,"the normal quantile-quantile plot takes advantage of what is known about the quantiles of the normal distribution. the methodology involves a plot of the empirical quantiles recently discussed against the corresponding quantile of the normal distribution. now, the expression for a quantile of an n(μ, σ) random variable is very complicated. however, a good approximation is given by"
1518,1,"['quantile', 'approximation', 'random variable', 'variable', 'random']", Quantile and Probability Plots,seg_175,"the expression in braces (the multiple of σ) is the approximation for the corresponding quantile for the n(0, 1) random variable, that is,"
1519,1,"['plot', 'observations', 'normal']", Quantile and Probability Plots,seg_175,"definition 8.7: the normal quantile-quantile plot is a plot of y(i) (ordered observations) 3 i− against q0,1(fi), where fi = n+"
1520,1,"['estimate', 'data', 'mean', 'population', 'standard', 'standard deviation', 'distribution', 'plot', 'deviation', 'population mean', 'intercept', 'normal', 'normal distribution', 'slope']", Quantile and Probability Plots,seg_175,a nearly straight-line relationship suggests that the data came from a normal distribution. the intercept on the vertical axis is an estimate of the population mean μ and the slope is an estimate of the standard deviation σ. figure 8.16 shows a normal quantile-quantile plot for the paint can data.
1521,1,"['standard normal', 'quantile', 'normal', 'standard']", Quantile and Probability Plots,seg_175,"−2 2 1 −2 2 standard normal quantile, q 0 , 1 (f)"
1522,1,"['plot', 'data', 'normal']", Quantile and Probability Plots,seg_175,figure 8.16: normal quantile-quantile plot for paint data.
1523,1,"['plot', 'deviation', 'results', 'data', 'normality', 'slope']", Quantile and Probability Plots,seg_175,notice how the deviation from normality becomes clear from the appearance of the plot. the asymmetry exhibited in the data results in changes in the slope.
1524,1,"['observations', 'plots', 'probability', 'graphical', 'probability plot', 'results', 'data', 'samples', 'expected values', 'distribution', 'expected value', 'normal probability plot', 'plotting', 'plot', 'normal', 'probability plotting', 'normal distribution']", Quantile and Probability Plots,seg_175,"the ideas of probability plotting are manifested in plots other than the normal quantile-quantile plot discussed here. for example, much attention is given to the so-called normal probability plot, in which f is plotted against the ordered data values on special paper and the scale used results in a straight line. in addition, an alternative plot makes use of the expected values of the ranked observations for the normal distribution and plots the ranked observations against their expected value, under the assumption of data from n(μ, σ). once again, the straight line is the graphical yardstick used. we continue to suggest that the foundation in graphical analytical methods developed in this section will aid in understanding formal methods of distinguishing between distinct samples of data."
1525,1,"['table', 'data', 'distribution', 'samples', 'response']", Quantile and Probability Plots,seg_175,"example 8.12: consider the data in exercise 10.41 on page 358 in chapter 10. in a study “nu- trient retention and macro invertebrate community response to sewage stress in a stream ecosystem,” conducted in the department of zoology at the virginia polytechnic institute and state university, data were collected on density measurements (number of organisms per square meter) at two different collecting stations. details are given in chapter 10 regarding analytical methods of comparing samples to determine if both are from the same n(μ, σ) distribution. the data are given in table 8.1."
1526,1,['data'], Quantile and Probability Plots,seg_175,table 8.1: data for example 8.12
1527,1,"['plot', 'distribution', 'samples', 'normal']", Quantile and Probability Plots,seg_175,"construct a normal quantile-quantile plot and draw conclusions regarding whether or not it is reasonable to assume that the two samples are from the same n(x;μ, σ) distribution."
1528,1,"['plot', 'data', 'normal']", Quantile and Probability Plots,seg_175,figure 8.17: normal quantile-quantile plot for density data of example 8.12.
1529,1,"['plot', 'observations', 'data', 'distribution', 'samples', 'normal', 'measurements', 'tail']", Quantile and Probability Plots,seg_175,"solution : figure 8.17 shows the normal quantile-quantile plot for the density measurements. the plot is far from a single straight line. in fact, the data from station 1 reflect a few values in the lower tail of the distribution and several in the upper tail. the “clustering” of observations would make it seem unlikely that the two samples came from a common n(μ, σ) distribution."
1530,1,"['distribution', 'normal', 'probability', 'probability plotting', 'plotting', 'normal distribution']", Quantile and Probability Plots,seg_175,"although we have concentrated our development and illustration on probability plotting for the normal distribution, we could focus on any distribution. we would merely need to compute quantities analytically for the theoretical distribution in question."
1531,1,"['statistics', 'information', 'central limit theorem', 'limit']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_181,"the central limit theorem is one of the most powerful tools in all of statistics, and even though this chapter is relatively short, it contains a wealth of fundamental information about tools that will be used throughout the balance of the text."
1532,1,"['sample', 'sampling distributions', 'population mean', 'statistics', 'sampling', 'distribution', 'samples', 'statistic', 'mean', 'population', 'central limit theorem', 'sampling distribution', 'limit', 'distributions']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_181,"the notion of a sampling distribution is one of the most important fundamental concepts in all of statistics, and the student at this point in his or her training should gain a clear understanding of it before proceeding beyond this chapter. all chapters that follow will make considerable use of sampling distributions. suppose one wants to use the statistic x̄ to draw inferences about the population mean μ. this will be done by using the observed value x̄ from a single sample of size n. then any inference made must be accomplished by taking into account not just the single value but rather the theoretical structure, or distribution of all x̄ values that could be observed from samples of size n. thus, the concept of a sampling distribution comes to the surface. this distribution is the basis for the central limit theorem. the t, χ2, and f-distributions are also used in the context of sampling distributions. for example, the t-distribution, pictured in figure 8.8,"
1533,0,[], Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_181,"x̄−μ represents the structure that occurs if all of the values of are formed, where"
1534,1,"['sample', 'statistics', 'information', 'distribution', 'samples', 'normal', 'normal distribution', 'distributions']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_181,"s/√n x̄ and s are taken from samples of size n from a n(x;μ, σ) distribution. similar remarks can be made about χ2 and f , and the reader should not forget that the sample information forming the statistics for all of these distributions is the normal. so it can be said that where there is a t, f, or χ2, the source was a sample from a normal distribution."
1535,1,['distributions'], Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_181,"the three distributions described above may appear to have been introduced in a rather self-contained fashion with no indication of what they are about. however, they will appear in practical problem-solving throughout the balance of the text."
1536,1,"['sampling', 'set', 'sampling distributions', 'distributions']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_181,"now, there are three things that one must bear in mind, lest confusion set in regarding these fundamental sampling distributions:"
1537,1,"['limit', 'central limit theorem']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_181,(i) one cannot use the central limit theorem unless σ is known. when σ is not
1538,1,"['deviation', 'sample', 'sample standard deviation', 'central limit theorem', 'standard', 'standard deviation', 'limit']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_181,"known, it should be replaced by s, the sample standard deviation, in order to use the central limit theorem."
1539,1,"['statistic', 'limit', 'central limit theorem']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_181,"(ii) the t statistic is not a result of the central limit theorem and x1, x2, . . . , xn"
1540,1,"['distribution', 'estimate']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_181,"must come from a n(x;μ, σ) distribution in order for x̄−μ to be a t-distribution; s/√n s is, of course, merely an estimate of σ."
1541,1,['degrees of freedom'], Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_181,"(iii) while the notion of degrees of freedom is new at this point, the concept"
1542,1,"['sample', 'information']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_181,"should be very intuitive, since it is reasonable that the nature of the distribution of s and also t should depend on the amount of information in the sample x1, x2, . . . , xn."
1543,0,[], Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_181,this page intentionally left blank
1544,1,"['sample', 'sample mean', 'data', 'information', 'mean', 'population', 'limit', 'experimental', 'parameters', 'distribution', 'sample average', 'central limit theorem', 'variance', 'sampling distribution', 'population mean', 'sampling', 'normal', 'average', 'normal distribution']", Introduction,seg_185,"in previous chapters, we emphasized sampling properties of the sample mean and variance. we also emphasized displays of data in various forms. the purpose of these presentations is to build a foundation that allows us to draw conclusions about the population parameters from experimental data. for example, the central limit theorem provides information about the distribution of the sample mean x̄. the distribution involves the population mean μ. thus, any conclusions concerning μ drawn from an observed sample average must depend on knowledge of this sampling distribution. similar comments apply to s2 and σ2. clearly, any conclusions we draw about the variance of a normal distribution will likely involve the sampling distribution of s2."
1545,1,"['parameters', 'estimation', 'samples', 'population', 'statistical']", Introduction,seg_185,"in this chapter, we begin by formally outlining the purpose of statistical inference. we follow this by discussing the problem of estimation of population parameters. we confine our formal developments of specific estimation procedures to problems involving one and two samples."
1546,1,"['bayesian', 'statistics', 'probability', 'random sample', 'random', 'subjective', 'sample', 'estimate', 'data', 'information', 'mean', 'parameter', 'population', 'statistical', 'parameters', 'statistical inference', 'distribution', 'variance', 'method', 'probability distribution']", Statistical Inference,seg_187,"in chapter 1, we discussed the general philosophy of formal statistical inference. statistical inference consists of those methods by which one makes inferences or generalizations about a population. the trend today is to distinguish between the classical method of estimating a population parameter, whereby inferences are based strictly on information obtained from a random sample selected from the population, and the bayesian method, which utilizes prior subjective knowledge about the probability distribution of the unknown parameters in conjunction with the information provided by the sample data. throughout most of this chapter, we shall use classical methods to estimate unknown population parameters such as the mean, the proportion, and the variance by computing statistics from random"
1547,1,"['bayesian', 'estimation', 'sampling', 'sampling distributions', 'distributions']", Statistical Inference,seg_187,"samples and applying the theory of sampling distributions, much of which was covered in chapter 8. bayesian estimation will be discussed in chapter 18."
1548,1,"['sample', 'tests of hypotheses', 'estimate', 'estimation', 'random sample', 'sampling', 'distribution', 'hypothesis testing', 'hypotheses', 'population', 'random', 'tests', 'sampling distribution', 'hypothesis']", Statistical Inference,seg_187,"statistical inference may be divided into two major areas: estimation and tests of hypotheses. we treat these two areas separately, dealing with theory and applications of estimation in this chapter and hypothesis testing in chapter 10. to distinguish clearly between the two areas, consider the following examples. a candidate for public office may wish to estimate the true proportion of voters favoring him by obtaining opinions from a random sample of 100 eligible voters. the fraction of voters in the sample favoring the candidate could be used as an estimate of the true proportion in the population of voters. a knowledge of the sampling distribution of a proportion enables one to establish the degree of accuracy of such an estimate. this problem falls in the area of estimation."
1549,1,"['estimate', 'case', 'data', 'dependent', 'sampling', 'parameter', 'hypothesis']", Statistical Inference,seg_187,"now consider the case in which one is interested in finding out whether brand a floor wax is more scuff-resistant than brand b floor wax. he or she might hypothesize that branda is better than brandb and, after proper testing, accept or reject this hypothesis. in this example, we do not attempt to estimate a parameter, but instead we try to arrive at a correct decision about a prestated hypothesis. once again we are dependent on sampling theory and the use of data to provide us with some measure of accuracy for our decision."
1550,1,"['sample', 'binomial experiment', 'experiment', 'estimate', 'statistic', 'binomial', 'point estimate', 'population', 'parameter']", Classical Methods of Estimation,seg_189,"a point estimate of some population parameter θ is a single value θ̂ of a statistic θ̂. for example, the value x̄ of the statistic x̄, computed from a sample of size n, is a point estimate of the population parameter μ. similarly, p̂ = x/n is a point estimate of the true proportion p for a binomial experiment."
1551,1,"['sample median', 'sample', 'true parameter', 'random', 'sample mean', 'median', 'estimate', 'case', 'mean', 'population', 'random sample', 'parameter', 'estimator', 'error']", Classical Methods of Estimation,seg_189,"an estimator is not expected to estimate the population parameter without error. we do not expect x̄ to estimate μ exactly, but we certainly hope that it is not far off. for a particular sample, it is possible to obtain a closer estimate of μ by using the sample median x̃ as an estimator. consider, for instance, a sample consisting of the values 2, 5, and 11 from a population whose mean is 4 but is supposedly unknown. we would estimate μ to be x̄ = 6, using the sample mean as our estimate, or x̃ = 5, using the sample median as our estimate. in this case, the estimator x̃ produces an estimate closer to the true parameter than does the estimator x̄. on the other hand, if our random sample contains the values 2, 6, and 7, then x̄ = 5 and x̃ = 6, so x̄ is the better estimator. not knowing the true value of μ, we must decide in advance whether to use x̄ or x̃ as our estimator."
1552,1,"['estimated', 'estimate', 'sampling', 'distribution', 'mean', 'point estimate', 'population', 'function', 'parameter', 'estimator', 'sampling distribution', 'unbiased']", Classical Methods of Estimation,seg_189,"what are the desirable properties of a “good” decision function that would influence us to choose one estimator rather than another? let θ̂ be an estimator whose value θ̂ is a point estimate of some unknown population parameter θ. certainly, we would like the sampling distribution of θ̂ to have a mean equal to the parameter estimated. an estimator possessing this property is said to be unbiased."
1553,1,"['unbiased estimator', 'statistic', 'parameter', 'estimator', 'unbiased']", Classical Methods of Estimation,seg_189,definition 9.1: a statistic θ̂ is said to be an unbiased estimator of the parameter θ if
1554,1,"['unbiased estimator', 'estimator', 'parameter', 'unbiased']", Classical Methods of Estimation,seg_189,"example 9.1: show that s2 is an unbiased estimator of the parameter σ2. solution : in section 8.5 on page 244, we showed that"
1555,1,"['unbiased estimator', 'bias', 'estimated', 'biased', 'samples', 'biased estimator', 'variance', 'estimator', 'unbiased']", Classical Methods of Estimation,seg_189,"although s2 is an unbiased estimator of σ2, s, on the other hand, is usually a biased estimator of σ, with the bias becoming insignificant for large samples. this example illustrates why we divide by n − 1 rather than n when the variance is estimated."
1556,1,"['efficient', 'unbiased estimators', 'estimators', 'sampling', 'distribution', 'parameter', 'population', 'efficient estimator', 'variance', 'estimator', 'sampling distribution', 'unbiased']", Classical Methods of Estimation,seg_189,"if θ̂1 and θ̂2 are two unbiased estimators of the same population parameter θ, we want to choose the estimator whose sampling distribution has the smaller variance. hence, if σ2 < σ2 , we say that θ̂1 is a more efficient estimator of θ than θ̂2."
1557,1,"['efficient', 'unbiased estimators', 'estimators', 'parameter', 'efficient estimator', 'variance', 'estimator', 'unbiased']", Classical Methods of Estimation,seg_189,"definition 9.2: if we consider all possible unbiased estimators of some parameter θ, the one with the smallest variance is called the most efficient estimator of θ."
1558,1,"['efficient', 'estimators', 'sampling', 'sampling distributions', 'variance', 'estimator', 'unbiased', 'distributions']", Classical Methods of Estimation,seg_189,"figure 9.1 illustrates the sampling distributions of three different estimators, θ̂1, θ̂2, and θ̂3, all estimating θ. it is clear that only θ̂1 and θ̂2 are unbiased, since their distributions are centered at θ. the estimator θ̂1 has a smaller variance than θ̂2 and is therefore more efficient. hence, our choice for an estimator of θ, among the three considered, would be θ̂1."
1559,1,"['population mean', 'normal', 'mean', 'population', 'populations', 'variance', 'unbiased']", Classical Methods of Estimation,seg_189,"for normal populations, one can show that both x̄ and x̃ are unbiased estimators of the population mean μ, but the variance of x̄ is smaller than the variance"
1560,1,"['estimators', 'sampling', 'sampling distributions', 'distributions']", Classical Methods of Estimation,seg_189,figure 9.1: sampling distributions of different estimators of θ.
1561,1,"['sample', 'efficient', 'estimates', 'population mean', 'mean', 'population', 'average']", Classical Methods of Estimation,seg_189,"of x̃. thus, both estimates x̄ and x̃ will, on average, equal the population mean μ, but x̄ is likely to be closer to μ for a given sample, and thus x̄ is more efficient than x̃."
1562,1,"['efficient unbiased estimator', 'sample', 'unbiased estimator', 'efficient', 'interval', 'estimate', 'estimation', 'samples', 'interval estimate', 'point estimate', 'population', 'parameter', 'estimator', 'unbiased']", Classical Methods of Estimation,seg_189,"even the most efficient unbiased estimator is unlikely to estimate the population parameter exactly. it is true that estimation accuracy increases with large samples, but there is still no reason we should expect a point estimate from a given sample to be exactly equal to the population parameter it is supposed to estimate. there are many situations in which it is preferable to determine an interval within which we would expect to find the value of the parameter. such an interval is called an interval estimate."
1563,1,"['interval', 'statistic', 'random sample', 'random', 'sample', 'sample mean', 'estimate', 'mean', 'scores', 'population', 'parameter', 'sample size', 'distribution', 'interval estimate', 'sampling distribution', 'sampling', 'average']", Classical Methods of Estimation,seg_189,"an interval estimate of a population parameter θ is an interval of the form θ̂l < θ < θ̂u , where θ̂l and θ̂u depend on the value of the statistic θ̂ for a particular sample and also on the sampling distribution of θ̂. for example, a random sample of sat verbal scores for students in the entering freshman class might produce an interval from 530 to 550, within which we expect to find the true average of all sat verbal scores for the freshman class. the values of the endpoints, 530 and 550, will depend on the computed sample mean x̄ and the sampling distribution of x̄. as the sample size increases, we know that σx̄"
1564,1,"['sample', 'interval', 'estimate', 'interval estimate', 'point estimate', 'population', 'parameter']", Classical Methods of Estimation,seg_189,"decreases, and consequently our estimate is likely to be closer to the parameter μ, resulting in a shorter interval. thus, the interval estimate indicates, by its length, the accuracy of the point estimate. an engineer will gain some insight into the population proportion defective by taking a sample and computing the sample proportion defective. but an interval estimate might be more informative."
1565,1,"['random variables', 'interval', 'variables', 'sampling', 'distribution', 'samples', 'random', 'sampling distribution']", Classical Methods of Estimation,seg_189,"since different samples will generally yield different values of θ̂ and, therefore, different values for θ̂l and θ̂u , these endpoints of the interval are values of corresponding random variables θ̂l and θ̂u . from the sampling distribution of θ̂ we shall be able to determine θ̂l and θ̂u such that p (θ̂l < θ < θ̂u ) is equal to any"
1566,0,[], Classical Methods of Estimation,seg_189,"positive fractional value we care to specify. if, for instance, we find θ̂l and θ̂u such that"
1567,1,"['sample', 'confidence coefficient', 'random', 'interval', 'confidence limits', 'confident', 'intervals', 'probability', 'random sample', 'parameter', 'coefficient', 'confidence', 'average', 'confidence interval']", Classical Methods of Estimation,seg_189,"for 0 < α < 1, then we have a probability of 1−α of selecting a random sample that will produce an interval containing θ. the interval θ̂l < θ < θ̂u , computed from the selected sample, is called a 100(1 − α)% confidence interval, the fraction 1 − α is called the confidence coefficient or the degree of confidence, and the endpoints, θ̂l and θ̂u , are called the lower and upper confidence limits. thus, when α = 0.05, we have a 95% confidence interval, and when α = 0.01, we obtain a wider 99% confidence interval. the wider the confidence interval is, the more confident we can be that the interval contains the unknown parameter. of course, it is better to be 95% confident that the average life of a certain television transistor is between 6 and 7 years than to be 99% confident that it is between 3 and 10 years. ideally, we prefer a short interval with a high degree of confidence. sometimes, restrictions on the size of our sample prevent us from achieving short intervals without sacrificing some degree of confidence."
1568,1,"['interval', 'estimation', 'case', 'estimators', 'sampling', 'interval estimator', 'distribution', 'information', 'parameter', 'interval estimation', 'point and interval estimation', 'confidence', 'estimator', 'sampling distribution', 'point estimator', 'confidence interval']", Classical Methods of Estimation,seg_189,"in the sections that follow, we pursue the notions of point and interval estimation, with each section presenting a different special case. the reader should notice that while point and interval estimation represent different approaches to gaining information regarding a parameter, they are related in the sense that confidence interval estimators are based on point estimators. in the following section, for example, we will see that x̄ is a very reasonable point estimator of μ. as a result, the important confidence interval estimator of μ depends on knowledge of the sampling distribution of x̄."
1569,1,"['interval', 'estimation', 'results', 'case', 'information', 'associated', 'mean', 'interval estimation', 'confidence', 'confidence interval']", Classical Methods of Estimation,seg_189,"we begin the following section with the simplest case of a confidence interval. the scenario is simple and yet unrealistic. we are interested in estimating a population mean μ and yet σ is known. clearly, if μ is unknown, it is quite unlikely that σ is known. any historical results that produced enough information to allow the assumption that σ is known would likely have produced similar information about μ. despite this argument, we begin with this case because the concepts and indeed the resulting mechanics associated with confidence interval estimation remain the same for the more realistic situations presented later in section 9.4 and beyond."
1570,1,"['sample', 'estimate', 'sample mean', 'population mean', 'estimators', 'sampling', 'distribution', 'mean', 'point estimate', 'population', 'variance', 'sampling distribution']", Single Sample Estimating the Mean,seg_191,"the sampling distribution of x̄ is centered at μ, and in most applications the variance is smaller than that of any other estimators of μ. thus, the sample mean x̄ will be used as a point estimate for the population mean μ. recall that σx̄ 2 = σ2/n, so a large sample will yield a value of x̄ that comes from a sampling distribution with a small variance. hence, x̄ is likely to be a very accurate estimate of μ when n is large."
1571,1,"['sample', 'estimate', 'interval', 'sampling', 'distribution', 'normal', 'interval estimate', 'population', 'confidence', 'sampling distribution', 'confidence interval']", Single Sample Estimating the Mean,seg_191,"let us now consider the interval estimate of μ. if our sample is selected from a normal population or, failing this, if n is sufficiently large, we can establish a confidence interval for μ by considering the sampling distribution of x̄."
1572,1,"['normally distributed', 'sampling', 'mean', 'central limit theorem', 'limit']", Single Sample Estimating the Mean,seg_191,"according to the central limit theorem, we can expect the sampling distribution of x̄ to be approximately normally distributed with mean μx̄ = μ and"
1573,1,"['deviation', 'curve', 'normal']", Single Sample Estimating the Mean,seg_191,"standard deviation σx̄ = σ/√n. writing zα/2 for the z-value above which we find an area of α/2 under the normal curve, we can see from figure 9.2 that"
1574,1,['inequality'], Single Sample Estimating the Mean,seg_191,"multiplying each term in the inequality by σ/√n and then subtracting x̄ from each term and multiplying by −1 (reversing the sense of the inequalities), we obtain"
1575,1,"['sample', 'interval', 'random sample', 'central limit theorem', 'mean', 'population', 'random', 'confidence', 'variance', 'limit', 'confidence interval']", Single Sample Estimating the Mean,seg_191,"a random sample of size n is selected from a population whose variance σ2 is known, and the mean x̄ is computed to give the 100(1−α)% confidence interval below. it is important to emphasize that we have invoked the central limit theorem above. as a result, it is important to note the conditions for applications that follow."
1576,1,"['sample', 'random', 'interval', 'mean', 'population', 'random sample', 'confidence', 'variance', 'confidence interval']", Single Sample Estimating the Mean,seg_191,"confidence if x̄ is the mean of a random sample of size n from a population with known interval on μ, σ2 variance σ2, a 100(1− α)% confidence interval for μ is given by"
1577,1,"['populations', 'samples', 'confidence']", Single Sample Estimating the Mean,seg_191,"for small samples selected from nonnormal populations, we cannot expect our degree of confidence to be accurate. however, for samples of size n ≥ 30, with"
1578,1,"['results', 'skewed', 'sampling', 'distributions']", Single Sample Estimating the Mean,seg_191,"the shape of the distributions not too skewed, sampling theory guarantees good results."
1579,1,"['random variables', 'variables', 'confidence limits', 'random', 'confidence']", Single Sample Estimating the Mean,seg_191,"clearly, the values of the random variables θ̂l and θ̂u , defined in section 9.3, are the confidence limits"
1580,1,"['sample', 'random', 'interval estimates', 'estimate', 'estimates', 'interval', 'confident', 'intervals', 'samples', 'point estimate', 'random sample', 'parameter']", Single Sample Estimating the Mean,seg_191,"different samples will yield different values of x̄ and therefore produce different interval estimates of the parameter μ, as shown in figure 9.3. the dot at the center of each interval indicates the position of the point estimate x̄ for that random sample. note that all of these intervals are of the same width, since their widths depend only on the choice of zα/2 once x̄ is determined. the larger the value we choose for zα/2, the wider we make all the intervals and the more confident we can be that the particular sample selected will produce an interval that contains the unknown parameter μ. in general, for a selection of zα/2, 100(1 − α)% of the intervals will cover μ."
1581,1,"['interval estimates', 'estimates', 'interval', 'samples']", Single Sample Estimating the Mean,seg_191,figure 9.3: interval estimates of μ for different samples.
1582,1,"['confidence intervals', 'locations', 'measurements', 'sample', 'intervals', 'mean', 'population', 'standard', 'standard deviation', 'confidence', 'concentration', 'deviation', 'population standard deviation', 'average']", Single Sample Estimating the Mean,seg_191,example 9.2: the average zinc concentration recovered from a sample of measurements taken in 36 different locations in a river is found to be 2.6 grams per milliliter. find the 95% and 99% confidence intervals for the mean zinc concentration in the river. assume that the population standard deviation is 0.3 gram per milliliter.
1583,1,"['interval', 'estimate', 'table', 'point estimate', 'confidence', 'confidence interval']", Single Sample Estimating the Mean,seg_191,"solution : the point estimate of μ is x̄ = 2.6. the z-value leaving an area of 0.025 to the right, and therefore an area of 0.975 to the left, is z0.025 = 1.96 (table a.3). hence, the 95% confidence interval is"
1584,1,"['confidence', 'interval', 'confidence interval', 'table']", Single Sample Estimating the Mean,seg_191,"which reduces to 2.50 < μ < 2.70. to find a 99% confidence interval, we find the z-value leaving an area of 0.005 to the right and 0.995 to the left. from table a.3 again, z0.005 = 2.575, and the 99% confidence interval is"
1585,1,"['confidence', 'interval', 'estimate']", Single Sample Estimating the Mean,seg_191,we now see that a longer interval is required to estimate μ with a higher degree of confidence.
1586,1,"['absolute value', 'interval', 'estimate', 'estimates', 'confident', 'point estimate', 'confidence', 'error', 'confidence interval']", Single Sample Estimating the Mean,seg_191,"the 100(1−α)% confidence interval provides an estimate of the accuracy of our point estimate. if μ is actually the center value of the interval, then x̄ estimates μ without error. most of the time, however, x̄ will not be exactly equal to μ and the point estimate will be in error. the size of this error will be the absolute value of the difference between μ and x̄, and we can be 100(1− α)% confident that this"
1587,0,[], Single Sample Estimating the Mean,seg_191,σ difference will not exceed zα/2 . we can readily see this if we draw a diagram of
1588,1,"['confidence', 'interval', 'confidence interval']", Single Sample Estimating the Mean,seg_191,"√n a hypothetical confidence interval, as in figure 9.4."
1589,1,['error'], Single Sample Estimating the Mean,seg_191,figure 9.4: error in estimating μ by x̄.
1590,1,"['error', 'confident', 'estimate']", Single Sample Estimating the Mean,seg_191,"theorem 9.1: if x̄ is used as an estimate of μ, we can be 100(1 − α)% confident that the error σ will not exceed zα/2 ."
1591,1,"['sample', 'sample mean', 'confident', 'mean']", Single Sample Estimating the Mean,seg_191,"in example 9.2, we are 95% confident that the sample mean x̄ = 2.6 differs from the true mean μ by an amount less than (1.96)(0.3)/√36 = 0.1 and 99% confident that the difference is less than (2.575)(0.3)/√36 = 0.13."
1592,1,"['sample', 'error']", Single Sample Estimating the Mean,seg_191,"frequently, we wish to know how large a sample is necessary to ensure that the error in estimating μ will be less than a specified amount e. by theorem 9.1,"
1593,0,"['n', 'e']", Single Sample Estimating the Mean,seg_191,σ we must choose n such that zα/2 = e. solving this equation gives the following
1594,1,"['sample size', 'sample', 'estimate', 'confident', 'error']", Single Sample Estimating the Mean,seg_191,"theorem 9.2: if x̄ is used as an estimate of μ, we can be 100(1 − α)% confident that the error will not exceed a specified amount e when the sample size is"
1595,1,"['sample size', 'sample', 'confidence']", Single Sample Estimating the Mean,seg_191,"when solving for the sample size, n, we round all fractional values up to the next whole number. by adhering to this principle, we can be sure that our degree of confidence never falls below 100(1− α)%."
1596,1,"['sample', 'estimate', 'approximation', 'observations', 'information', 'population', 'variance']", Single Sample Estimating the Mean,seg_191,"strictly speaking, the formula in theorem 9.2 is applicable only if we know the variance of the population from which we select our sample. lacking this information, we could take a preliminary sample of size n ≥ 30 to provide an estimate of σ. then, using s as an approximation for σ in theorem 9.2, we could determine approximately how many observations are needed to provide the desired degree of accuracy."
1597,1,"['sample', 'confident', 'estimate']", Single Sample Estimating the Mean,seg_191,example 9.3: how large a sample is required if we want to be 95% confident that our estimate of μ in example 9.2 is off by less than 0.05?
1598,1,"['deviation', 'population standard deviation', 'standard', 'population', 'standard deviation']", Single Sample Estimating the Mean,seg_191,"solution : the population standard deviation is σ = 0.3. then, by theorem 9.2,"
1599,1,"['sample', 'random', 'estimate', 'confident', 'random sample']", Single Sample Estimating the Mean,seg_191,"therefore, we can be 95% confident that a random sample of size 139 will provide an estimate x̄ differing from μ by an amount less than 0.05."
1600,1,"['measurement', 'confidence intervals', 'confidence bound', 'case', 'intervals', 'information', 'mean', 'confidence']", Single Sample Estimating the Mean,seg_191,"the confidence intervals and resulting confidence bounds discussed thus far are two-sided (i.e., both upper and lower bounds are given). however, there are many applications in which only one bound is sought. for example, if the measurement of interest is tensile strength, the engineer receives better information from a lower bound only. this bound communicates the worst-case scenario. on the other hand, if the measurement is something for which a relatively large value of μ is not profitable or desirable, then an upper confidence bound is of interest. an example would be a case in which inferences need to be made concerning the mean mercury composition in a river. an upper bound is very informative in this case."
1601,1,"['intervals', 'probability', 'central limit theorem', 'confidence', 'limit']", Single Sample Estimating the Mean,seg_191,"one-sided confidence bounds are developed in the same fashion as two-sided intervals. however, the source is a one-sided probability statement that makes use of the central limit theorem:"
1602,1,['probability'], Single Sample Estimating the Mean,seg_191,one can then manipulate the probability statement much as before and obtain
1603,0,[], Single Sample Estimating the Mean,seg_191,similar manipulation of p (σ
1604,0,[], Single Sample Estimating the Mean,seg_191,"as a result, the upper and lower one-sided bounds follow."
1605,1,"['sample', 'random', 'mean', 'population', 'random sample', 'confidence', 'variance']", Single Sample Estimating the Mean,seg_191,"one-sided if x̄ is the mean of a random sample of size n from a population with variance confidence σ2, the one-sided 100(1− α)% confidence bounds for μ are given by bounds on μ, σ2 upper one-sided bound: x̄+ zασ/√n; known"
1606,1,"['experiment', 'distribution', 'normal', 'mean', 'variance', 'average']", Single Sample Estimating the Mean,seg_191,"example 9.4: in a psychological testing experiment, 25 subjects are selected randomly and their reaction time, in seconds, to a particular stimulus is measured. past experience suggests that the variance in reaction times to these types of stimuli is 4 sec2 and that the distribution of reaction times is approximately normal. the average time for the subjects is 6.2 seconds. give an upper 95% bound for the mean reaction time."
1607,0,[], Single Sample Estimating the Mean,seg_191,solution : the upper 95% bound is given by
1608,1,"['mean', 'confident']", Single Sample Estimating the Mean,seg_191,"hence, we are 95% confident that the mean reaction time is less than 6.858 seconds."
1609,1,"['sample', 'estimate', 'random sample', 'distribution', 'random variable', 'variable', 'normal', 'mean', 'population', 'random', 'normal distribution']", Single Sample Estimating the Mean,seg_191,"frequently, we must attempt to estimate the mean of a population when the variance is unknown. the reader should recall learning in chapter 8 that if we have a random sample from a normal distribution, then the random variable"
1610,1,"['confidence interval', 'sample', 'degrees of freedom', 'deviation', 'standard normal distribution', 'interval', 'standard normal', 'sample standard deviation', 'distribution', 'normal', 'standard', 'standard deviation', 'confidence', 'normal distribution']", Single Sample Estimating the Mean,seg_191,"has a student t-distribution with n − 1 degrees of freedom. here s is the sample standard deviation. in this situation, with σ unknown, t can be used to construct a confidence interval on μ. the procedure is the same as that with σ known except that σ is replaced by s and the standard normal distribution is replaced by the t-distribution. referring to figure 9.5, we can assert that"
1611,1,"['symmetry', 'degrees of freedom']", Single Sample Estimating the Mean,seg_191,"where tα/2 is the t-value with n−1 degrees of freedom, above which we find an area of α/2. because of symmetry, an equal area of α/2 will fall to the left of −tα/2. substituting for t , we write"
1612,1,['inequality'], Single Sample Estimating the Mean,seg_191,"multiplying each term in the inequality by s/√n, and then subtracting x̄ from each term and multiplying by −1, we obtain"
1613,1,"['deviation', 'sample', 'standard', 'interval', 'random sample', 'mean', 'random', 'standard deviation', 'confidence', 'confidence interval']", Single Sample Estimating the Mean,seg_191,"for a particular random sample of size n, the mean x̄ and standard deviation s are computed and the following 100(1− α)% confidence interval for μ is obtained."
1614,1,"['deviation', 'sample', 'random', 'interval', 'random sample', 'normal', 'mean', 'population', 'standard', 'standard deviation', 'confidence', 'variance', 'confidence interval']", Single Sample Estimating the Mean,seg_191,"confidence if x̄ and s are the mean and standard deviation of a random sample from a interval on μ, σ2 normal population with unknown variance σ2, a 100(1−α)% confidence interval unknown for μ is"
1615,1,['degrees of freedom'], Single Sample Estimating the Mean,seg_191,"where tα/2 is the t-value with v = n − 1 degrees of freedom, leaving an area of α/2 to the right."
1616,1,"['confidence intervals', 'interval', 'estimates', 'random', 'interval estimates', 'results', 'intervals', 'cases', 'random variable', 'confidence', 'limit', 'confidence interval', 'distribution', 'central limit theorem', 'sampling distribution', 'sampling', 'variable', 'normal', 'normal distribution']", Single Sample Estimating the Mean,seg_191,"we have made a distinction between the cases of σ known and σ unknown in computing confidence interval estimates. we should emphasize that for σ known we exploited the central limit theorem, whereas for σ unknown we made use of the sampling distribution of the random variable t . however, the use of the tdistribution is based on the premise that the sampling is from a normal distribution. as long as the distribution is approximately bell shaped, confidence intervals can be computed when σ2 is unknown by using the t-distribution and we may expect very good results."
1617,1,['confidence'], Single Sample Estimating the Mean,seg_191,"computed one-sided confidence bounds for μ with σ unknown are as the reader would expect, namely"
1618,0,[], Single Sample Estimating the Mean,seg_191,"they are the upper and lower 100(1 − α)% bounds, respectively. here tα is the t-value having an area of α to the right."
1619,1,"['interval', 'distribution', 'normal distribution', 'normal', 'mean', 'confidence', 'confidence interval']", Single Sample Estimating the Mean,seg_191,"example 9.5: the contents of seven similar containers of sulfuric acid are 9.8, 10.2, 10.4, 9.8, 10.0, 10.2, and 9.6 liters. find a 95% confidence interval for the mean contents of all such containers, assuming an approximately normal distribution."
1620,1,"['deviation', 'sample', 'sample mean', 'data', 'mean', 'standard', 'standard deviation']", Single Sample Estimating the Mean,seg_191,solution : the sample mean and standard deviation for the given data are
1621,1,"['degrees of freedom', 'table']", Single Sample Estimating the Mean,seg_191,"using table a.4, we find t0.025 = 2.447 for v = 6 degrees of freedom. hence, the"
1622,1,"['confidence', 'interval', 'confidence interval']", Single Sample Estimating the Mean,seg_191,95% confidence interval for μ is
1623,1,"['interval', 'statisticians', 'normality', 'confidence', 'confidence interval']", Single Sample Estimating the Mean,seg_191,"often statisticians recommend that even when normality cannot be assumed, σ is unknown, and n ≥ 30, s can replace σ and the confidence interval"
1624,1,"['sample size', 'sample', 'interval', 'approximation', 'skewed', 'distribution', 'central limit theorem', 'population', 'confidence', 'confidence interval', 'limit']", Single Sample Estimating the Mean,seg_191,"may be used. this is often referred to as a large-sample confidence interval. the justification lies only in the presumption that with a sample as large as 30 and the population distribution not too skewed, s will be very close to the true σ and thus the central limit theorem prevails. it should be emphasized that this is only an approximation and the quality of the result becomes better as the sample size grows larger."
1625,1,"['deviation', 'sample', 'random', 'sample mean', 'interval', 'random sample', 'mean', 'scores', 'standard', 'standard deviation', 'confidence', 'test', 'confidence interval']", Single Sample Estimating the Mean,seg_191,"example 9.6: scholastic aptitude test (sat) mathematics scores of a random sample of 500 high school seniors in the state of texas are collected, and the sample mean and standard deviation are found to be 501 and 112, respectively. find a 99% confidence interval on the mean sat mathematics score for seniors in the state of texas."
1626,1,"['sample size', 'sample', 'normal approximation', 'interval', 'table', 'approximation', 'normal', 'confidence', 'confidence interval']", Single Sample Estimating the Mean,seg_191,"solution : since the sample size is large, it is reasonable to use the normal approximation. using table a.3, we find z0.005 = 2.575. hence, a 99% confidence interval for μ is"
1627,1,"['experimental', 'interval', 'estimate', 'data', 'intervals', 'set', 'interval estimate', 'point estimate', 'parameter', 'confidence', 'confidence interval']", Standard Error of a Point Estimate,seg_193,"we have made a rather sharp distinction between the goal of a point estimate and that of a confidence interval estimate. the former supplies a single number extracted from a set of experimental data, and the latter provides an interval that is reasonable for the parameter, given the experimental data; that is, 100(1− α)% of such computed intervals “cover” the parameter."
1628,1,"['unbiased estimator', 'estimation', 'sampling', 'distribution', 'point estimator', 'variance', 'estimator', 'sampling distribution', 'unbiased']", Standard Error of a Point Estimate,seg_193,"these two approaches to estimation are related to each other. the common thread is the sampling distribution of the point estimator. consider, for example, the estimator x̄ of μ with σ known. we indicated earlier that a measure of the quality of an unbiased estimator is its variance. the variance of x̄ is"
1629,1,"['deviation', 'error', 'standard error', 'standard', 'standard deviation', 'confidence', 'estimator', 'limit']", Standard Error of a Point Estimate,seg_193,"thus, the standard deviation of x̄, or standard error of x̄, is σ/√n. simply put, the standard error of an estimator is its standard deviation. for x̄, the computed confidence limit"
1630,1,"['interval', 'case', 'dependent', 'estimator', 'standard', 'point estimator', 'confidence', 'error', 'confidence interval', 'distribution', 'estimated', 'standard error', 'confidence limits', 'sampling', 'normal', 'normal distribution']", Standard Error of a Point Estimate,seg_193,"where “s.e.” is the “standard error.” the important point is that the width of the confidence interval on μ is dependent on the quality of the point estimator through its standard error. in the case where σ is unknown and sampling is from a normal distribution, s replaces σ and the estimated standard error s/√n is involved. thus, the confidence limits on μ are"
1631,1,"['error', 'estimated', 'interval', 'estimate', 'standard error', 'case', 'errors', 'point estimate', 'standard', 'confidence', 'standard errors', 'confidence interval']", Standard Error of a Point Estimate,seg_193,"confidence s limits on μ, σ2 x̄± tα/2 = x̄± tα/2 s.e.(x̄) √n unknown again, the confidence interval is no better (in terms of width) than the quality of the point estimate, in this case through its estimated standard error. computer packages often refer to estimated standard errors simply as “standard errors.”"
1632,1,"['precision', 'confidence intervals', 'interval', 'estimate', 'intervals', 'point estimate', 'confidence', 'confidence interval']", Standard Error of a Point Estimate,seg_193,"as we move to more complex confidence intervals, there is a prevailing notion that widths of confidence intervals become shorter as the quality of the corresponding point estimate becomes better, although it is not always quite as simple as we have illustrated here. it can be argued that a confidence interval is merely an augmentation of the point estimate to take into account the precision of the point estimate."
1633,1,"['interval', 'prediction', 'case', 'observation', 'process', 'prediction interval', 'sample', 'uncertainty', 'data', 'information', 'mean', 'parameter', 'population', 'confidence', 'quality control', 'confidence interval', 'distribution', 'control', 'population mean', 'normal', 'normal distribution']", Prediction Intervals,seg_195,"the point and interval estimations of the mean in sections 9.4 and 9.5 provide good information about the unknown parameter μ of a normal distribution or a nonnormal distribution from which a large sample is drawn. sometimes, other than the population mean, the experimenter may also be interested in predicting the possible value of a future observation. for instance, in quality control, the experimenter may need to use the observed data to predict a new observation. a process that produces a metal part may be evaluated on the basis of whether the part meets specifications on tensile strength. on certain occasions, a customer may be interested in purchasing a single part. in this case, a confidence interval on the mean tensile strength does not capture the required information. the customer requires a statement regarding the uncertainty of a single observation. this type of requirement is nicely fulfilled by the construction of a prediction interval."
1634,1,"['sample', 'point estimator', 'error', 'interval', 'prediction', 'random sample', 'observation', 'variation', 'normal', 'mean', 'population', 'random', 'random error', 'variance', 'estimator', 'prediction interval']", Prediction Intervals,seg_195,"it is quite simple to obtain a prediction interval for the situations we have considered so far. assume that the random sample comes from a normal population with unknown mean μ and known variance σ2. a natural point estimator of a new observation is x̄. it is known, from section 8.4, that the variance of x̄ is σ2/n. however, to predict a new observation, not only do we need to account for the variation due to estimating the mean, but also we should account for the variation of a future observation. from the assumption, we know that the variance of the random error in a new observation is σ2. the development of a"
1635,1,"['sample', 'independent', 'interval', 'observation', 'variable', 'random variable', 'normal', 'random', 'normal random variable']", Prediction Intervals,seg_195,"prediction interval is best illustrated by beginning with a normal random variable x0 − x̄, where x0 is the new observation and x̄ comes from the sample. since x0 and x̄ are independent, we know that"
1636,1,['probability'], Prediction Intervals,seg_195,"is n(z; 0, 1). as a result, if we use the probability statement"
1637,1,"['probability', 'event']", Prediction Intervals,seg_195,"with the z-statistic above and place x0 in the center of the probability statement, we have the following event occurring with probability 1− α:"
1638,1,"['prediction', 'prediction interval', 'interval']", Prediction Intervals,seg_195,"as a result, computation of the prediction interval is formalized as follows."
1639,1,"['interval', 'prediction', 'observation', 'distribution', 'normal', 'measurements', 'mean', 'variance', 'prediction interval', 'normal distribution']", Prediction Intervals,seg_195,"prediction for a normal distribution of measurements with unknown mean μ and known interval of a variance σ2, a 100(1− α)% prediction interval of a future observation x0 is future observation, σ2 x̄− zα/2σ√1 + 1/n < x0 < x̄+ zα/2σ√1 + 1/n,"
1640,1,"['deviation', 'sample', 'population standard deviation', 'interval', 'prediction', 'population', 'standard', 'standard deviation', 'average', 'prediction interval', 'rates']", Prediction Intervals,seg_195,"example 9.7: due to the decrease in interest rates, the first citizens bank received a lot of mortgage applications. a recent sample of 50 mortgage loans resulted in an average loan amount of $257,300. assume a population standard deviation of $25,000. for the next customer who fills out a mortgage application, find a 95% prediction interval for the loan amount."
1641,1,"['prediction', 'prediction interval', 'interval', 'point prediction']", Prediction Intervals,seg_195,"solution : the point prediction of the next customer’s loan amount is x̄ = $257, 300. the z-value here is z0.025 = 1.96. hence, a 95% prediction interval for the future loan amount is"
1642,1,['interval'], Prediction Intervals,seg_195,"which gives the interval ($207,812.43, $306,787.57)."
1643,1,"['sample', 'sample mean', 'estimate', 'interval', 'estimation', 'prediction', 'case', 'observation', 'location', 'distribution', 'normal', 'mean', 'variation', 'variance', 'prediction interval', 'normal distribution']", Prediction Intervals,seg_195,"the prediction interval provides a good estimate of the location of a future observation, which is quite different from the estimate of the sample mean value. it should be noted that the variation of this prediction is the sum of the variation due to an estimation of the mean and the variation of a single observation. however, as in the past, we first consider the case with known variance. it is also important to deal with the prediction interval of a future observation in the situation where the variance is unknown. indeed a student t-distribution may be used in this case, as described in the following result. the normal distribution is merely replaced by the t-distribution."
1644,1,"['interval', 'prediction', 'observation', 'distribution', 'normal', 'measurements', 'mean', 'variance', 'prediction interval', 'normal distribution']", Prediction Intervals,seg_195,"prediction for a normal distribution of measurements with unknown mean μ and unknown interval of a variance σ2, a 100(1− α)% prediction interval of a future observation x0 is future observation, σ2 x̄− tα/2s√1 + 1/n < x0 < x̄+ tα/2s√1 + 1/n,"
1645,1,['degrees of freedom'], Prediction Intervals,seg_195,"unknown where tα/2 is the t-value with v = n − 1 degrees of freedom, leaving an area of α/2 to the right."
1646,1,"['prediction intervals', 'prediction', 'observations', 'intervals', 'cases']", Prediction Intervals,seg_195,one-sided prediction intervals can also be constructed. upper prediction bounds apply in cases where focus must be placed on future large observations. concern over future small observations calls for the use of lower prediction bounds. the upper bound is given by
1647,0,[], Prediction Intervals,seg_195,and the lower bound by
1648,1,"['deviation', 'sample', 'interval', 'prediction', 'sample standard deviation', 'normality', 'mean', 'standard', 'standard deviation', 'prediction interval']", Prediction Intervals,seg_195,example 9.8: a meat inspector has randomly selected 30 packs of 95% lean beef. the sample resulted in a mean of 96.2% with a sample standard deviation of 0.8%. find a 99% prediction interval for the leanness of a new pack. assume normality.
1649,1,"['degrees of freedom', 'interval', 'prediction', 'observation', 'prediction interval']", Prediction Intervals,seg_195,"solution : for v = 29 degrees of freedom, t0.005 = 2.756. hence, a 99% prediction interval for a new observation x0 is"
1650,1,"['outliers', 'prediction intervals', 'prediction', 'outlying', 'observations', 'intervals', 'outlier']", Prediction Intervals,seg_195,"to this point in the text very little attention has been paid to the concept of outliers, or aberrant observations. the majority of scientific investigators are keenly sensitive to the existence of outlying observations or so-called faulty or “bad data.” we deal with the concept of outlier detection extensively in chapter 12. however, it is certainly of interest here since there is an important relationship between outlier detection and prediction intervals."
1651,1,"['sample', 'interval', 'prediction', 'outlying', 'observation', 'outlier', 'mean', 'probability', 'population', 'prediction interval']", Prediction Intervals,seg_195,"it is convenient for our purposes to view an outlying observation as one that comes from a population with a mean that is different from the mean that governs the rest of the sample of size n being studied. the prediction interval produces a bound that “covers” a future single observation with probability 1− α if it comes from the population from which the sample was drawn. as a result, a methodology for outlier detection involves the rule that an observation is an outlier if it falls outside the prediction interval computed without including the questionable observation in the sample. as a result, for the prediction interval of example 9.8, if a new pack of beef is measured and its leanness is outside the interval (93.96, 98.44), that observation can be viewed as an outlier."
1652,1,"['measurement', 'prediction intervals', 'interval', 'parameters', 'prediction', 'probabilistic', 'intervals', 'observation', 'mean', 'population', 'process']", Tolerance Limits,seg_197,"as discussed in section 9.6, the scientist or engineer may be less interested in estimating parameters than in gaining a notion about where an individual observation or measurement might fall. such situations call for the use of prediction intervals. however, there is yet a third type of interval that is of interest in many applications. once again, suppose that interest centers around the manufacturing of a component part and specifications exist on a dimension of that part. in addition, there is little concern about the mean of the dimension. but unlike in the scenario in section 9.6, one may be less interested in a single observation and more interested in where the majority of the population falls. if process specifications are important, the manager of the process is concerned about long-range performance, not the next observation. one must attempt to determine bounds that, in some probabilistic sense, “cover” values in the population (i.e., the measured values of the dimension)."
1653,1,"['confidence interval', 'method', 'interval', 'observations', 'sampling', 'distribution', 'normal', 'measurements', 'mean', 'population', 'random', 'confidence', 'variance', 'random sampling', 'normal distribution']", Tolerance Limits,seg_197,"one method of establishing the desired bounds is to determine a confidence interval on a fixed proportion of the measurements. this is best motivated by visualizing a situation in which we are doing random sampling from a normal distribution with known mean μ and variance σ2. clearly, a bound that covers the middle 95% of the population of observations is"
1654,1,"['observations', 'tolerance interval', 'interval']", Tolerance Limits,seg_197,"this is called a tolerance interval, and indeed its coverage of 95% of measured observations is exact. however, in practice, μ and σ are seldom known; thus, the user must apply"
1655,1,"['interval', 'random variable', 'variable', 'population', 'random', 'confidence', 'confidence interval']", Tolerance Limits,seg_197,"now, of course, the interval is a random variable, and hence the coverage of a proportion of the population by the interval is not exact. as a result, a 100(1−γ)% confidence interval must be used since x̄ ± ks cannot be expected to cover any specified proportion all the time. as a result, we have the following definition."
1656,1,"['deviation', 'distribution', 'tolerance limits', 'normal', 'measurements', 'mean', 'standard', 'standard deviation', 'confidence', 'normal distribution']", Tolerance Limits,seg_197,"tolerance limits for a normal distribution of measurements with unknown mean μ and unknown standard deviation σ, tolerance limits are given by x̄ ± ks, where k is determined such that one can assert with 100(1 − γ)% confidence that the given limits contain at least the proportion 1− α of the measurements."
1657,1,"['interval', 'data', 'information', 'distribution', 'normal', 'tolerance interval', 'normal distribution']", Tolerance Limits,seg_197,"example 9.9: consider example 9.8. with the information given, find a tolerance interval that gives two-sided 95% bounds on 90% of the distribution of packages of 95% lean beef. assume the data came from an approximately normal distribution."
1658,1,"['deviation', 'sample', 'sample mean', 'table', 'sample standard deviation', 'mean', 'standard', 'standard deviation']", Tolerance Limits,seg_197,"solution : recall from example 9.8 that n = 30, the sample mean is 96.2%, and the sample standard deviation is 0.8%. from table a.7, k = 2.14. using"
1659,0,[], Tolerance Limits,seg_197,we find that the lower and upper bounds are 94.5 and 97.9.
1660,1,"['range', 'confident']", Tolerance Limits,seg_197,we are 95% confident that the above range covers the central 90% of the distribution of 95% lean beef packages.
1661,1,['intervals'], Tolerance Limits,seg_197,"it is important to reemphasize the difference among the three types of intervals discussed and illustrated in the preceding sections. the computations are straightforward, but interpretation can be confusing. in real-life applications, these intervals are not interchangeable because their interpretations are quite distinct."
1662,1,"['confidence intervals', 'population mean', 'case', 'intervals', 'set', 'tolerance limits', 'mean', 'parameter', 'population', 'confidence', 'process']", Tolerance Limits,seg_197,"in the case of confidence intervals, one is attentive only to the population mean. for example, exercise 9.13 on page 283 deals with an engineering process that produces shearing pins. a specification will be set on rockwell hardness, below which a customer will not accept any pins. here, a population parameter must take a backseat. it is important that the engineer know where the majority of the values of rockwell hardness are going to be. thus, tolerance limits should be used. surely, when tolerance limits on any process output are tighter than process specifications, that is good news for the process manager."
1663,1,"['interval', 'tolerance limit', 'case', 'distribution', 'normal distribution', 'normal', 'tolerance limits', 'confidence', 'tolerance interval', 'limit', 'confidence interval']", Tolerance Limits,seg_197,"it is true that the tolerance limit interpretation is somewhat related to the confidence interval. the 100(1−α)% tolerance interval on, say, the proportion 0.95 can be viewed as a confidence interval on the middle 95% of the corresponding normal distribution. one-sided tolerance limits are also relevant. in the case of the rockwell hardness problem, it is desirable to have a lower bound of the form x̄ − ks such that there is 99% confidence that at least 99% of rockwell hardness values will exceed the computed value."
1664,1,"['observation', 'intervals', 'location', 'mean', 'population']", Tolerance Limits,seg_197,"prediction intervals are applicable when it is important to determine a bound on a single value. the mean is not the issue here, nor is the location of the majority of the population. rather, the location of a single new observation is required."
1665,1,"['deviation', 'sample', 'sample mean', 'interval', 'data', 'distribution', 'normal', 'mean', 'standard', 'standard deviation', 'normal distribution']", Tolerance Limits,seg_197,"case study 9.1: machine quality: a machine produces metal pieces that are cylindrical in shape. a sample of these pieces is taken and the diameters are found to be 1.01, 0.97, 1.03, 1.04, 0.99, 0.98, 0.99, 1.01, and 1.03 centimeters. use these data to calculate three interval types and draw interpretations that illustrate the distinction between them in the context of the system. for all computations, assume an approximately normal distribution. the sample mean and standard deviation for the given data are x̄ = 1.0056 and s = 0.0246."
1666,1,"['confidence', 'interval', 'mean', 'confidence interval']", Tolerance Limits,seg_197,(a) find a 99% confidence interval on the mean diameter.
1667,1,"['prediction', 'prediction interval', 'interval']", Tolerance Limits,seg_197,(b) compute a 99% prediction interval on a measured diameter of a single metal
1668,0,[], Tolerance Limits,seg_197,piece taken from the machine.
1669,1,['tolerance limits'], Tolerance Limits,seg_197,(c) find the 99% tolerance limits that will contain 95% of the metal pieces pro-
1670,0,[], Tolerance Limits,seg_197,duced by this machine.
1671,1,"['confidence', 'interval', 'mean', 'confidence interval']", Tolerance Limits,seg_197,solution : (a) the 99% confidence interval for the mean diameter is given by
1672,1,['estimation'], Tolerance Limits,seg_197,282 chapter 9 oneand two-sample estimation problems
1673,1,['confidence'], Tolerance Limits,seg_197,"thus, the 99% confidence bounds are 0.9781 and 1.0331."
1674,1,"['prediction', 'prediction interval', 'interval', 'observation']", Tolerance Limits,seg_197,(b) the 99% prediction interval for a future observation is given by
1675,1,['table'], Tolerance Limits,seg_197,"(c) from table a.7, for n = 9, 1− γ = 0.99, and 1− α = 0.95, we find k = 4.550"
1676,1,['tolerance limits'], Tolerance Limits,seg_197,"for two-sided limits. hence, the 99% tolerance limits are given by"
1677,1,"['distribution', 'tolerance interval', 'confident', 'interval']", Tolerance Limits,seg_197,with the bounds being 0.8937 and 1.1175. we are 99% confident that the tolerance interval from 0.8937 to 1.1175 will contain the central 95% of the distribution of diameters produced.
1678,1,"['interval', 'prediction', 'range', 'case', 'confident', 'process', 'results', 'intervals', 'mean', 'population', 'confidence', 'confidence interval', 'distribution', 'tolerance limits', 'population mean']", Tolerance Limits,seg_197,"this case study illustrates that the three types of limits can give appreciably different results even though they are all 99% bounds. in the case of the confidence interval on the mean, 99% of such intervals cover the population mean diameter. thus, we say that we are 99% confident that the mean diameter produced by the process is between 0.9781 and 1.0331 centimeters. emphasis is placed on the mean, with less concern about a single reading or the general nature of the distribution of diameters in the population. in the case of the prediction limits, the bounds 0.9186 and 1.0926 are based on the distribution of a single “new” metal piece taken from the process, and again 99% of such limits will cover the diameter of a new measured piece. on the other hand, the tolerance limits, as suggested in the previous section, give the engineer a sense of where the “majority,” say the central 95%, of the diameters of measured pieces in the population reside. the 99% tolerance limits, 0.8937 and 1.1175, are numerically quite different from the other two bounds. if these bounds appear alarmingly wide to the engineer, it reflects negatively on process quality. on the other hand, if the bounds represent a desirable result, the engineer may conclude that a majority (95% in here) of the diameters are in a desirable range. again, a confidence interval interpretation may be used: namely, 99% of such calculated bounds will cover the middle 95% of the population of diameters."
1679,1,"['random samples', 'statistic', 'random', 'estimator', 'sample', 'estimate', 'sample means', 'samples', 'populations', 'population', 'point estimator', 'distribution', 'sampling distribution', 'independent', 'sampling', 'point estimate', 'variances']", Two Samples Estimating the Difference between Two Means,seg_201,"if we have two populations with means μ1 and μ2 and variances σ12 and σ22, respectively, a point estimator of the difference between μ1 and μ2 is given by the statistic x̄1 − x̄2. therefore, to obtain a point estimate of μ1 − μ2, we shall select two independent random samples, one from each population, of sizes n1 and n2, and compute x̄1−x̄2, the difference of the sample means. clearly, we must consider the sampling distribution of x̄1 − x̄2."
1680,1,"['normally distributed', 'sampling', 'distribution', 'mean', 'sampling distribution']", Two Samples Estimating the Difference between Two Means,seg_201,"according to theorem 8.3, we can expect the sampling distribution of x̄1 − x̄2 to be approximately normally distributed with mean μx̄1−x̄2 = μ1 − μ2 and"
1681,1,"['deviation', 'standard normal', 'variable', 'normal', 'probability', 'standard']", Two Samples Estimating the Difference between Two Means,seg_201,"standard deviation σx̄1−x̄2 = √σ12/n1 + σ22/n2. therefore, we can assert with a probability of 1− α that the standard normal variable"
1682,0,[], Two Samples Estimating the Difference between Two Means,seg_201,"will fall between −zα/2 and zα/2. referring once again to figure 9.2, we write"
1683,0,[], Two Samples Estimating the Difference between Two Means,seg_201,"substituting for z, we state equivalently that"
1684,1,"['confidence', 'interval', 'confidence interval']", Two Samples Estimating the Difference between Two Means,seg_201,which leads to the following 100(1− α)% confidence interval for μ1 − μ2.
1685,1,"['independent', 'interval', 'random samples', 'populations', 'samples', 'random', 'confidence', 'confidence interval', 'variances']", Two Samples Estimating the Difference between Two Means,seg_201,"confidence if x̄1 and x̄2 are means of independent random samples of sizes n1 and n2 interval for from populations with known variances σ12 and σ22, respectively, a 100(l − α)% μ1 − μ2, σ12 and confidence interval for μ1 − μ2 is given by σ22 known"
1686,1,"['approximation', 'samples', 'central limit theorem', 'normal', 'populations', 'confidence', 'limit']", Two Samples Estimating the Difference between Two Means,seg_201,"the degree of confidence is exact when samples are selected from normal populations. for nonnormal populations, the central limit theorem allows for a good approximation for reasonable size samples."
1687,1,"['interval', 'random samples', 'case', 'random', 'process', 'population variance', 'bias', 'experiment', 'results', 'hypothesis testing', 'samples', 'populations', 'population', 'confidence', 'error', 'confidence interval', 'distributions', 'experimental', 'estimation', 'experimental unit', 'experimental units', 'variance', 'hypothesis', 'plot', 'experimental error', 'independent', 'randomization', 'interval estimation']", Two Samples Estimating the Difference between Two Means,seg_201,"for the case of confidence interval estimation on the difference between two means, we need to consider the experimental conditions in the data-taking process. it is assumed that we have two independent random samples from distributions with means μ1 and μ2, respectively. it is important that experimental conditions emulate this ideal described by these assumptions as closely as possible. quite often, the experimenter should plan the strategy of the experiment accordingly. for almost any study of this type, there is a so-called experimental unit, which is that part of the experiment that produces experimental error and is responsible for the population variance we refer to as σ2. in a drug study, the experimental unit is the patient or subject. in an agricultural experiment, it may be a plot of ground. in a chemical experiment, it may be a quantity of raw materials. it is important that differences between the experimental units have minimal impact on the results. the experimenter will have a degree of insurance that experimental units will not bias results if the conditions that define the two populations are randomly assigned to the experimental units. we shall again focus on randomization in future chapters that deal with hypothesis testing."
1688,1,"['standard deviations', 'interval', 'population mean', 'deviations', 'confidence', 'mean', 'population', 'standard', 'experiments', 'average', 'confidence interval']", Two Samples Estimating the Difference between Two Means,seg_201,"example 9.10: a study was conducted in which two types of engines, a and b, were compared. gas mileage, in miles per gallon, was measured. fifty experiments were conducted using engine type a and 75 experiments were done with engine type b. the gasoline used and other conditions were held constant. the average gas mileage was 36 miles per gallon for engine a and 42 miles per gallon for engine b. find a 96% confidence interval on μb − μa, where μa and μb are population mean gas mileages for engines a and b, respectively. assume that the population standard deviations are 6 and 8 for engines a and b, respectively."
1689,1,"['interval', 'estimate', 'table', 'point estimate', 'confidence', 'confidence interval']", Two Samples Estimating the Difference between Two Means,seg_201,"solution : the point estimate of μb − μa is x̄b − x̄a = 42− 36 = 6. using α = 0.04, we find z0.02 = 2.05 from table a.3. hence, with substitution in the formula above, the 96% confidence interval is"
1690,1,"['sample', 'interval', 'variances', 'case', 'normality', 'samples', 'normal', 'confidence', 'confidence interval', 'distributions']", Two Samples Estimating the Difference between Two Means,seg_201,"this procedure for estimating the difference between two means is applicable if σ12 and σ22 are known. if the variances are not known and the two distributions involved are approximately normal, the t-distribution becomes involved, as in the case of a single sample. if one is not willing to assume normality, large samples (say greater than 30) will allow the use of s1 and s2 in place of σ1 and σ2, respectively, with the rationale that s1 ≈ σ1 and s2 ≈ σ2. again, of course, the confidence interval is an approximate one."
1691,1,"['standard normal', 'case', 'variable', 'normal', 'standard']", Two Samples Estimating the Difference between Two Means,seg_201,"consider the case where σ12 and σ22 are unknown. if σ12 = σ22 = σ2, we obtain a standard normal variable of the form"
1692,1,"['random variables', 'variables', 'random']", Two Samples Estimating the Difference between Two Means,seg_201,"according to theorem 8.4, the two random variables"
1693,1,"['degrees of freedom', 'independent', 'random samples', 'variables', 'samples', 'random', 'distributions']", Two Samples Estimating the Difference between Two Means,seg_201,"have chi-squared distributions with n1 − 1 and n2 − 1 degrees of freedom, respectively. furthermore, they are independent chi-squared variables, since the random samples were selected independently. consequently, their sum"
1694,1,"['degrees of freedom', 'distribution']", Two Samples Estimating the Difference between Two Means,seg_201,has a chi-squared distribution with v = n1 + n2 − 2 degrees of freedom.
1695,1,"['statistic', 'independent']", Two Samples Estimating the Difference between Two Means,seg_201,"since the preceding expressions for z and v can be shown to be independent, it follows from theorem 8.5 that the statistic"
1696,1,['degrees of freedom'], Two Samples Estimating the Difference between Two Means,seg_201,has the t-distribution with v = n1 + n2 − 2 degrees of freedom.
1697,1,"['sample', 'sample variances', 'estimate', 'point estimate', 'variance', 'estimator', 'variances']", Two Samples Estimating the Difference between Two Means,seg_201,"a point estimate of the unknown common variance σ2 can be obtained by pooling the sample variances. denoting the pooled estimator by sp2, we have the following."
1698,1,"['variance', 'estimate']", Two Samples Estimating the Difference between Two Means,seg_201,pooled estimate (n1 − 1)s2 + (n2 − 1)s2 of variance sp2 = 1 2 . n1 + n2 − 2
1699,1,['statistic'], Two Samples Estimating the Difference between Two Means,seg_201,"substituting sp2 in the t statistic, we obtain the less cumbersome form"
1700,1,['statistic'], Two Samples Estimating the Difference between Two Means,seg_201,"using the t statistic, we have"
1701,1,"['degrees of freedom', 'inequality']", Two Samples Estimating the Difference between Two Means,seg_201,"where tα/2 is the t-value with n1 + n2 − 2 degrees of freedom, above which we find an area of α/2. substituting for t in the inequality, we write"
1702,1,"['sample', 'interval', 'sample means', 'pooled variance', 'confidence', 'variance', 'confidence interval']", Two Samples Estimating the Difference between Two Means,seg_201,"after the usual mathematical manipulations, the difference of the sample means x̄1 − x̄2 and the pooled variance are computed and then the following 100(1−α)% confidence interval for μ1 − μ2 is obtained."
1703,1,"['sample', 'degrees of freedom', 'sample variances', 'weighted average', 'average', 'variances']", Two Samples Estimating the Difference between Two Means,seg_201,"the value of s2p is easily seen to be a weighted average of the two sample variances s21 and s22, where the weights are the degrees of freedom."
1704,1,"['independent', 'interval', 'random samples', 'populations', 'samples', 'normal', 'random', 'confidence', 'confidence interval', 'variances']", Two Samples Estimating the Difference between Two Means,seg_201,"confidence if x̄1 and x̄2 are the means of independent random samples of sizes n1 and n2, interval for respectively, from approximately normal populations with unknown but equal μ1 − μ2, σ12 = σ22 variances, a 100(1− α)% confidence interval for μ1 − μ2 is given by but both 1 1 1 1 unknown (x̄1 − x̄2)− tα/2sp√n1 + n2"
1705,1,"['deviation', 'degrees of freedom', 'population standard deviation', 'standard', 'estimate', 'population', 'standard deviation']", Two Samples Estimating the Difference between Two Means,seg_201,"where sp is the pooled estimate of the population standard deviation and tα/2 is the t-value with v = n1 + n2 − 2 degrees of freedom, leaving an area of α/2 to the right."
1706,1,"['indicator', 'numerical', 'parameters']", Two Samples Estimating the Difference between Two Means,seg_201,"example 9.11: the article “macroinvertebrate community structure as an indicator of acid mine pollution,” published in the journal of environmental pollution, reports on an investigation undertaken in cane creek, alabama, to determine the relationship between selected physiochemical parameters and different measures of macroinvertebrate community structure. one facet of the investigation was an evaluation of the effectiveness of a numerical species diversity index to indicate aquatic degradation due to acid mine drainage. conceptually, a high index of macroinvertebrate species diversity should indicate an unstressed aquatic system, while a low diversity index should indicate a stressed aquatic system."
1707,1,"['interval', 'locations', 'normally distributed', 'samples', 'mean', 'populations', 'population', 'standard', 'standard deviation', 'confidence', 'confidence interval', 'deviation', 'independent', 'sampling', 'variances']", Two Samples Estimating the Difference between Two Means,seg_201,"two independent sampling stations were chosen for this study, one located downstream from the acid mine discharge point and the other located upstream. for 12 monthly samples collected at the downstream station, the species diversity index had a mean value x̄1 = 3.11 and a standard deviation s1 = 0.771, while 10 monthly samples collected at the upstream station had a mean index value x̄2 = 2.04 and a standard deviation s2 = 0.448. find a 90% confidence interval for the difference between the population means for the two locations, assuming that the populations are approximately normally distributed with equal variances."
1708,1,"['interval', 'estimate', 'point estimate', 'population', 'confidence', 'confidence interval']", Two Samples Estimating the Difference between Two Means,seg_201,"solution : let μ1 and μ2 represent the population means, respectively, for the species diversity indices at the downstream and upstream stations. we wish to find a 90% confidence interval for μ1 − μ2. our point estimate of μ1 − μ2 is"
1709,1,"['variance', 'estimate']", Two Samples Estimating the Difference between Two Means,seg_201,"the pooled estimate, sp2, of the common variance, σ2, is"
1710,1,"['degrees of freedom', 'interval', 'table', 'confidence', 'confidence interval']", Two Samples Estimating the Difference between Two Means,seg_201,"taking the square root, we obtain sp = 0.646. using α = 0.1, we find in table a.4 that t0.05 = 1.725 for v = n1+n2− 2 = 20 degrees of freedom. therefore, the 90% confidence interval for μ1 − μ2 is"
1711,1,"['risk', 'error', 'experimental', 'interval', 'case', 'data', 'confidence limits', 'confident', 'parameter', 'population', 'confidence', 'average', 'confidence interval']", Two Samples Estimating the Difference between Two Means,seg_201,"for the case of a single parameter, the confidence interval simply provides error bounds on the parameter. values contained in the interval should be viewed as reasonable values given the experimental data. in the case of a difference between two means, the interpretation can be extended to one of comparing the two means. for example, if we have high confidence that a difference μ1 − μ2 is positive, we would certainly infer that μ1 > μ2 with little risk of being in error. for example, in example 9.11, we are 90% confident that the interval from 0.593 to 1.547 contains the difference of the population means for values of the species diversity index at the two stations. the fact that both confidence limits are positive indicates that, on the average, the index for the station located downstream from the discharge point is greater than the index for the station located upstream."
1712,1,"['confidence intervals', 'interval', 'sample', 'sample variances', 'experiment', 'results', 'intervals', 'information', 'samples', 'population', 'populations', 'confidence', 'variance', 'normality', 'normal', 'variances']", Two Samples Estimating the Difference between Two Means,seg_201,"the procedure for constructing confidence intervals for μ1 − μ2 with σ1 = σ2 = σ unknown requires the assumption that the populations are normal. slight departures from either the equal variance or the normality assumption do not seriously alter the degree of confidence for our interval. (a procedure is presented in chapter 10 for testing the equality of two unknown population variances based on the information provided by the sample variances.) if the population variances are considerably different, we still obtain reasonable results when the populations are normal, provided that n1 = n2. therefore, in planning an experiment, one should make every effort to equalize the size of the samples."
1713,1,"['interval', 'estimate', 'case', 'statistic', 'interval estimate', 'population', 'variances']", Two Samples Estimating the Difference between Two Means,seg_201,let us now consider the problem of finding an interval estimate of μ1 − μ2 when the unknown population variances are not likely to be equal. the statistic most often used in this case is
1714,1,['degrees of freedom'], Two Samples Estimating the Difference between Two Means,seg_201,"which has approximately a t-distribution with v degrees of freedom, where"
1715,1,"['degrees of freedom', 'approximation', 'estimate']", Two Samples Estimating the Difference between Two Means,seg_201,"since v is seldom an integer, we round it down to the nearest whole number. the above estimate of the degrees of freedom is called the satterthwaite approximation (satterthwaite, 1946, in the bibliography)."
1716,1,['statistic'], Two Samples Estimating the Difference between Two Means,seg_201,"using the statistic t ′, we write"
1717,1,"['degrees of freedom', 'inequality']", Two Samples Estimating the Difference between Two Means,seg_201,"where tα/2 is the value of the t-distribution with v degrees of freedom, above which we find an area of α/2. substituting for t ′ in the inequality and following the same steps as before, we state the final result."
1718,1,"['independent', 'interval', 'populations', 'samples', 'normal', 'random interval', 'random', 'confidence', 'variances']", Two Samples Estimating the Difference between Two Means,seg_201,"confidence if x̄1 and s21 and x̄2 and s22 are the means and variances of independent random interval for samples of sizes n1 and n2, respectively, from approximately normal populations μ1 − μ2, σ12 = σ22 with unknown and unequal variances, an approximate 100(1 − α)% confidence and both interval for μ1 − μ2 is given by unknown"
1719,0,[], Two Samples Estimating the Difference between Two Means,seg_201,"degrees of freedom, leaving an area of α/2 to the right."
1720,1,"['degrees of freedom', 'random variables', 'estimate', 'variables', 'random', 'confidence']", Two Samples Estimating the Difference between Two Means,seg_201,"note that the expression for v above involves random variables, and thus v is an estimate of the degrees of freedom. in applications, this estimate will not result in a whole number, and thus the analyst must round down to the nearest integer to achieve the desired confidence."
1721,1,"['confidence intervals', 'interval', 'intervals', 'mean', 'confidence', 'confidence interval']", Two Samples Estimating the Difference between Two Means,seg_201,"before we illustrate the above confidence interval with an example, we should point out that all the confidence intervals on μ1 − μ2 are of the same general form as those on a single mean; namely, they can be written as"
1722,1,"['point estimate', 'estimate']", Two Samples Estimating the Difference between Two Means,seg_201,point estimate ± tα/2 ŝ.e.(point estimate)
1723,1,"['point estimate', 'estimate']", Two Samples Estimating the Difference between Two Means,seg_201,point estimate ± zα/2 s.e.(point estimate).
1724,1,"['estimated', 'standard error', 'case', 'standard', 'error']", Two Samples Estimating the Difference between Two Means,seg_201,"for example, in the case where σ1 = σ2 = σ, the estimated standard error of x̄1 − x̄2 is sp√1/n1 + 1/n2. for the case where σ12 = σ22,"
1725,1,"['deviation', 'interval', 'estimate', 'observations', 'populations', 'samples', 'normal', 'standard', 'standard deviation', 'confidence', 'average', 'confidence interval', 'variances']", Two Samples Estimating the Difference between Two Means,seg_201,"example 9.12: a study was conducted by the department of zoology at the virginia tech to estimate the difference in the amounts of the chemical orthophosphorus measured at two different stations on the james river. orthophosphorus was measured in milligrams per liter. fifteen samples were collected from station 1, and 12 samples were obtained from station 2. the 15 samples from station 1 had an average orthophosphorus content of 3.84 milligrams per liter and a standard deviation of 3.07 milligrams per liter, while the 12 samples from station 2 had an average content of 1.49 milligrams per liter and a standard deviation of 0.80 milligram per liter. find a 95% confidence interval for the difference in the true average orthophosphorus contents at these two stations, assuming that the observations came from normal populations with different variances."
1726,1,"['confidence', 'interval', 'confidence interval']", Two Samples Estimating the Difference between Two Means,seg_201,"solution : for station 1, we have x̄1 = 3.84, s1 = 3.07, and n1 = 15. for station 2, x̄2 = 1.49, s2 = 0.80, and n2 = 12. we wish to find a 95% confidence interval for μ1 − μ2."
1727,1,"['degrees of freedom', 'interval', 'population', 'confidence', 'confidence interval', 'variances']", Two Samples Estimating the Difference between Two Means,seg_201,"since the population variances are assumed to be unequal, we can only find an approximate 95% confidence interval based on the t-distribution with v degrees of freedom, where"
1728,1,"['point estimate', 'estimate']", Two Samples Estimating the Difference between Two Means,seg_201,our point estimate of μ1 − μ2 is
1729,1,"['degrees of freedom', 'interval', 'table', 'confidence', 'confidence interval']", Two Samples Estimating the Difference between Two Means,seg_201,"using α = 0.05, we find in table a.4 that t0.025 = 2.120 for v = 16 degrees of freedom. therefore, the 95% confidence interval for μ1 − μ2 is"
1730,1,"['interval', 'confident', 'locations', 'average']", Two Samples Estimating the Difference between Two Means,seg_201,"which simplifies to 0.60 < μ1 − μ2 < 4.10. hence, we are 95% confident that the interval from 0.60 to 4.10 milligrams per liter contains the difference of the true average orthophosphorus contents for these two locations."
1731,1,"['variance', 'population', 'variances']", Two Samples Estimating the Difference between Two Means,seg_201,"when two population variances are unknown, the assumption of equal variances or unequal variances may be precarious. in section 10.10, a procedure will be introduced that will aid in discriminating between the equal variance and the unequal variance situation."
1732,1,"['observations', 'normally distributed', 'random sample', 'random', 'paired', 'sample', 'information', 'samples', 'mean', 'populations', 'population', 'experimental', 'condition', 'estimation', 'paired observations', 'experimental unit', 'experimental units', 'test', 'variance', 'independent', 'homogeneous', 'variances']", Paired Observations,seg_203,"at this point, we shall consider estimation procedures for the difference of two means when the samples are not independent and the variances of the two populations are not necessarily equal. the situation considered here deals with a very special experimental condition, namely that of paired observations. unlike in the situation described earlier, the conditions of the two populations are not assigned randomly to experimental units. rather, each homogeneous experimental unit receives both population conditions; as a result, each experimental unit has a pair of observations, one for each population. for example, if we run a test on a new diet using 15 individuals, the weights before and after going on the diet form the information for our two samples. the two populations are “before” and “after,” and the experimental unit is the individual. obviously, the observations in a pair have something in common. to determine if the diet is effective, we consider the differences d1, d2, . . . , dn in the paired observations. these differences are the values of a random sample d1, d2, . . . , dn from a population of differences that we shall assume to be normally distributed with mean μd = μ1 −μ2 and variance σd"
1733,1,['estimate'], Paired Observations,seg_203,we estimate σd
1734,1,"['sample', 'point estimator', 'variance', 'estimator']", Paired Observations,seg_203,"2 by s2d, the variance of the differences that constitute our sample. the point estimator of μd is given by d̄."
1735,1,"['observations', 'experiment']", Paired Observations,seg_203,pairing observations in an experiment is a strategy that can be employed in many fields of application. the reader will be exposed to this concept in material related
1736,1,"['experimental error', 'experimental', 'design', 'case', 'homogeneous', 'hypothesis testing', 'population', 'experimental units', 'variance', 'error', 'hypothesis']", Paired Observations,seg_203,"to hypothesis testing in chapter 10 and experimental design issues in chapters 13 and 15. selecting experimental units that are relatively homogeneous (within the units) and allowing each unit to experience both population conditions reduces the effective experimental error variance (in this case, σd"
1737,0,[], Paired Observations,seg_203,2 ). the reader may visualize
1738,0,[], Paired Observations,seg_203,the ith pair difference as
1739,1,"['sample', 'experimental', 'independent', 'observations', 'experimental unit']", Paired Observations,seg_203,"since the two observations are taken on the sample experimental unit, they are not independent and, in fact,"
1740,0,[], Paired Observations,seg_203,"now, intuitively, we expect that σd"
1741,1,"['experimental', 'interval', 'homogeneity', 'standard error', 'observations', 'homogeneous', 'covariance', 'experimental unit', 'standard', 'confidence', 'error', 'confidence interval']", Paired Observations,seg_203,"2 should be reduced because of the similarity in nature of the “errors” of the two observations within a given experimental unit, and this comes through in the expression above. one certainly expects that if the unit is homogeneous, the covariance is positive. as a result, the gain in quality of the confidence interval over that obtained without pairing will be greatest when there is homogeneity within units and large differences as one goes from unit to unit. one should keep in mind that the performance of the confidence interval will depend on the standard error of d̄, which is, of course, σd/√n, where n is the number of pairs. as we indicated earlier, the intent of pairing is to reduce σd."
1742,1,"['degrees of freedom', 'experienced', 'confidence intervals', 'estimate', 'standard error', 'case', 'intervals', 'adjusted', 'point estimate', 'standard', 'confidence', 'variance', 'error']", Paired Observations,seg_203,"comparing the confidence intervals obtained with and without pairing makes apparent that there is a tradeoff involved. although pairing should indeed reduce variance and hence reduce the standard error of the point estimate, the degrees of freedom are reduced by reducing the problem to a one-sample problem. as a result, the tα/2 point attached to the standard error is adjusted accordingly. thus, pairing may be counterproductive. this would certainly be the case if one experienced only a modest reduction in variance (through σd"
1743,1,"['case', 'random']", Paired Observations,seg_203,"another illustration of pairing involves choosing n pairs of subjects, with each pair having a similar characteristic such as iq, age, or breed, and then selecting one member of each pair at random to yield a value of x1, leaving the other member to provide the value of x2. in this case, x1 and x2 might represent the grades obtained by two individuals of equal iq when one of the individuals is assigned at random to a class using the conventional lecture approach while the other individual is assigned to a class using programmed materials."
1744,1,"['confidence', 'interval', 'confidence interval']", Paired Observations,seg_203,a 100(1− α)% confidence interval for μd can be established by writing
1745,0,[], Paired Observations,seg_203,"where t = d̄−μd and tα/2, as before, is a value of the t-distribution with n − 1"
1746,1,['degrees of freedom'], Paired Observations,seg_203,sd/√n degrees of freedom.
1747,1,"['confidence', 'interval', 'confidence interval', 'inequality']", Paired Observations,seg_203,it is now a routine procedure to replace t by its definition in the inequality above and carry out the mathematical steps that lead to the following 100(1−α)% confidence interval for μ1 − μ2 = μd.
1748,1,"['deviation', 'standard', 'interval', 'measurements', 'mean', 'random', 'standard deviation']", Paired Observations,seg_203,"confidence if d̄ and sd are the mean and standard deviation, respectively, of the normally interval for distributed differences of n random pairs of measurements, a 100(1− α)% conμd = μ1 − μ2 for fidence interval for μd = μ1 − μ2 is"
1749,1,['observations'], Paired Observations,seg_203,"paired sd sd observations d̄− tα/2 < μd < d̄+ tα/2 , √n √n"
1750,1,['degrees of freedom'], Paired Observations,seg_203,"where tα/2 is the t-value with v = n − 1 degrees of freedom, leaving an area of α/2 to the right."
1751,1,"['levels', 'table']", Paired Observations,seg_203,example 9.13: a study published in chemosphere reported the levels of the dioxin tcdd of 20 massachusetts vietnam veterans who were possibly exposed to agent orange. the tcdd levels in plasma and in fat tissue are listed in table 9.1.
1752,1,"['levels', 'interval', 'distribution', 'normal', 'mean', 'confidence', 'confidence interval']", Paired Observations,seg_203,"find a 95% confidence interval for μ1 − μ2, where μ1 and μ2 represent the true mean tcdd levels in plasma and in fat tissue, respectively. assume the distribution of the differences to be approximately normal."
1753,1,['data'], Paired Observations,seg_203,table 9.1: data for example 9.13
1754,1,['levels'], Paired Observations,seg_203,tcdd tcdd tcdd tcdd levels in levels in levels in levels in veteran plasma fat tissue di veteran plasma fat tissue di 1 2.5 4.9 −2.4 11 6.9 7.0 −0.1 2 3.1 5.9 −2.8 12 3.3 2.9 0.4 3 2.1 4.4 −2.3 13 4.6 4.6 0.0 4 3.5 6.9 −3.4 14 1.6 1.4 0.2 5 3.1 7.0 −3.9 15 7.2 7.7 −0.5 6 1.8 4.2 −2.4 16 1.8 1.1 0.7 7 6.0 10.0 −4.0 17 20.0 11.0 9.0 8 3.0 5.5 −2.5 18 2.0 2.5 −0.5 9 36.0 41.0 −5.0 19 2.5 2.3 0.2 10 4.7 4.4 0.3 20 4.1 2.5 1.6
1755,1,"['deviation', 'sample', 'interval', 'estimate', 'observations', 'point estimate', 'standard', 'standard deviation', 'confidence', 'confidence interval', 'paired']", Paired Observations,seg_203,"solution : we wish to find a 95% confidence interval for μ1 − μ2. since the observations are paired, μ1 − μ2 = μd. the point estimate of μd is d̄ = −0.87. the standard deviation, sd, of the sample differences is"
1756,1,"['degrees of freedom', 'interval', 'table', 'confidence', 'confidence interval']", Paired Observations,seg_203,"using α = 0.05, we find in table a.4 that t0.025 = 2.093 for v = n−1 = 19 degrees of freedom. therefore, the 95% confidence interval is"
1757,1,['estimation'], Paired Observations,seg_203,294 chapter 9 oneand two-sample estimation problems
1758,1,"['level', 'mean']", Paired Observations,seg_203,"or simply −2.2634 < μd < 0.5234, from which we can conclude that there is no significant difference between the mean tcdd level in plasma and the mean tcdd level in fat tissue."
1759,1,"['sample', 'binomial experiment', 'experiment', 'estimate', 'successes', 'trials', 'statistic', 'binomial', 'point estimate', 'point estimator', 'parameter', 'estimator']", Single Sample Estimating a Proportion,seg_207,"a point estimator of the proportion p in a binomial experiment is given by the statistic p̂ = x/n, where x represents the number of successes in n trials. therefore, the sample proportion p̂ = x/n will be used as the point estimate of the parameter p."
1760,1,"['interval', 'normally distributed', 'sample', 'sample mean', 'successes', 'success', 'mean', 'confidence', 'trial', 'limit', 'confidence interval', 'failure', 'distribution', 'binomial', 'central limit theorem', 'sampling distribution', 'sampling']", Single Sample Estimating a Proportion,seg_207,"if the unknown proportion p is not expected to be too close to 0 or 1, we can establish a confidence interval for p by considering the sampling distribution of p̂ . designating a failure in each binomial trial by the value 0 and a success by the value 1, the number of successes, x, can be interpreted as the sum of n values consisting only of 0 and 1s, and p̂ is just the sample mean of these n values. hence, by the central limit theorem, for n sufficiently large, p̂ is approximately normally distributed with mean"
1761,1,['variance'], Single Sample Estimating a Proportion,seg_207,and variance
1762,0,[], Single Sample Estimating a Proportion,seg_207,"therefore, we can assert that"
1763,1,"['standard normal', 'curve', 'normal', 'standard']", Single Sample Estimating a Proportion,seg_207,"and zα/2 is the value above which we find an area of α/2 under the standard normal curve. substituting for z, we write"
1764,1,"['error', 'point estimate', 'estimate']", Single Sample Estimating a Proportion,seg_207,"when n is large, very little error is introduced by substituting the point estimate p̂ = x/n for the p under the radical sign. then we can write"
1765,1,['inequality'], Single Sample Estimating a Proportion,seg_207,"on the other hand, by solving for p in the quadratic inequality above,"
1766,1,"['confidence', 'interval', 'confidence interval']", Single Sample Estimating a Proportion,seg_207,we obtain another form of the confidence interval for p with limits
1767,1,"['sample', 'random', 'confidence intervals', 'intervals', 'random sample', 'confidence']", Single Sample Estimating a Proportion,seg_207,"for a random sample of size n, the sample proportion p̂ = x/n is computed, and the following approximate 100(1−α)% confidence intervals for p can be obtained."
1768,1,"['sample', 'random', 'method', 'interval', 'successes', 'intervals', 'binomial', 'parameter', 'random sample', 'confidence', 'confidence interval']", Single Sample Estimating a Proportion,seg_207,"large-sample if p̂ is the proportion of successes in a random sample of size n and q̂ = 1 − p̂, confidence an approximate 100(1− α)% confidence interval, for the binomial parameter p intervals for p is given by (method 1)"
1769,1,['method'], Single Sample Estimating a Proportion,seg_207,or by (method 2)
1770,1,"['interval', 'distribution', 'binomial', 'hypergeometric', 'parameter', 'binomial distribution', 'confidence', 'hypergeometric distribution', 'confidence interval']", Single Sample Estimating a Proportion,seg_207,"when n is small and the unknown proportion p is believed to be close to 0 or to 1, the confidence-interval procedure established here is unreliable and, therefore, should not be used. to be on the safe side, one should require both np̂ and nq̂ to be greater than or equal to 5. the methods for finding a confidence interval for the binomial parameter p are also applicable when the binomial distribution is being used to approximate the hypergeometric distribution, that is, when n is small relative to n , as illustrated by example 9.14."
1771,1,"['sample size', 'sample', 'method', 'results']", Single Sample Estimating a Proportion,seg_207,"note that although method 2 yields more accurate results, it is more complicated to calculate, and the gain in accuracy that it provides diminishes when the sample size is large enough. hence, method 1 is commonly used in practice."
1772,1,"['sample', 'random', 'interval', 'sets', 'random sample', 'confidence', 'confidence interval']", Single Sample Estimating a Proportion,seg_207,"example 9.14: in a random sample of n = 500 families owning television sets in the city of hamilton, canada, it is found that x = 340 subscribe to hbo. find a 95% confidence interval for the actual proportion of families with television sets in this city that subscribe to hbo."
1773,1,"['method', 'estimate', 'interval', 'table', 'point estimate', 'confidence', 'confidence interval']", Single Sample Estimating a Proportion,seg_207,"solution : the point estimate of p is p̂ = 340/500 = 0.68. using table a.3, we find that z0.025 = 1.96. therefore, using method 1, the 95% confidence interval for p is"
1774,1,['method'], Single Sample Estimating a Proportion,seg_207,"if we use method 2, we can obtain"
1775,1,['results'], Single Sample Estimating a Proportion,seg_207,"which simplifies to 0.6378 < p < 0.7194. apparently, when n is large (500 here), both methods yield very similar results."
1776,1,"['method', 'estimate', 'estimates', 'interval', 'confident', 'point estimate', 'confidence', 'error', 'confidence interval']", Single Sample Estimating a Proportion,seg_207,"if p is the center value of a 100(1−α)% confidence interval, then p̂ estimates p without error. most of the time, however, p̂ will not be exactly equal to p and the point estimate will be in error. the size of this error will be the positive difference that separates p and p̂, and we can be 100(1 − α)% confident that this difference will not exceed zα/2√p̂q̂/n. we can readily see this if we draw a diagram of a typical confidence interval, as in figure 9.6. here we use method 1 to estimate the error."
1777,1,['error'], Single Sample Estimating a Proportion,seg_207,figure 9.6: error in estimating p by p̂.
1778,1,"['error', 'confident', 'estimate']", Single Sample Estimating a Proportion,seg_207,"theorem 9.3: if p̂ is used as an estimate of p, we can be 100(1 − α)% confident that the error will not exceed zα/2√p̂q̂/n."
1779,1,"['sample', 'confident']", Single Sample Estimating a Proportion,seg_207,"in example 9.14, we are 95% confident that the sample proportion p̂ = 0.68 differs from the true proportion p by an amount not exceeding 0.04."
1780,1,"['sample', 'error']", Single Sample Estimating a Proportion,seg_207,"let us now determine how large a sample is necessary to ensure that the error in estimating p will be less than a specified amount e. by theorem 9.3, we must choose n such that zα/2√p̂q̂/n = e."
1781,1,"['sample size', 'sample', 'estimate', 'confident', 'error']", Single Sample Estimating a Proportion,seg_207,"theorem 9.4: if p̂ is used as an estimate of p, we can be 100(1 − α)% confident that the error will be less than a specified amount e when the sample size is approximately"
1782,1,"['sample size', 'sample', 'estimate', 'observations']", Single Sample Estimating a Proportion,seg_207,"theorem 9.4 is somewhat misleading in that we must use p̂ to determine the sample size n, but p̂ is computed from the sample. if a crude estimate of p can be made without taking a sample, this value can be used to determine n. lacking such an estimate, we could take a preliminary sample of size n ≥ 30 to provide an estimate of p. using theorem 9.4, we could determine approximately how many observations are needed to provide the desired degree of accuracy. note that fractional values of n are rounded up to the next whole number."
1783,1,"['sample', 'confident', 'estimate']", Single Sample Estimating a Proportion,seg_207,example 9.15: how large a sample is required if we want to be 95% confident that our estimate of p in example 9.14 is within 0.02 of the true value?
1784,1,"['sample', 'estimate']", Single Sample Estimating a Proportion,seg_207,"solution : let us treat the 500 families as a preliminary sample, providing an estimate p̂ = 0.68. then, by theorem 9.4,"
1785,1,"['sample', 'random', 'estimate', 'confident', 'random sample']", Single Sample Estimating a Proportion,seg_207,"therefore, if we base our estimate of p on a random sample of size 2090, we can be 95% confident that our sample proportion will not differ from the true proportion by more than 0.02."
1786,1,"['sample size', 'sample', 'estimate', 'confidence']", Single Sample Estimating a Proportion,seg_207,"occasionally, it will be impractical to obtain an estimate of p to be used for determining the sample size for a specified degree of confidence. if this happens, an upper bound for n is established by noting that p̂q̂ = p̂(1 − p̂), which must be at most 1/4, since p̂ must lie between 0 and 1. this fact may be verified by completing the square. hence"
1787,1,['confidence'], Single Sample Estimating a Proportion,seg_207,"which is always less than 1/4 except when p̂ = 1/2, and then p̂q̂ = 1/4. therefore, if we substitute p̂ = 1/2 into the formula for n in theorem 9.4 when, in fact, p actually differs from l/2, n will turn out to be larger than necessary for the specified degree of confidence; as a result, our degree of confidence will increase."
1788,1,"['sample size', 'sample', 'estimate', 'confident', 'error']", Single Sample Estimating a Proportion,seg_207,"theorem 9.5: if p̂ is used as an estimate of p, we can be at least 100(1 − α)% confident that the error will not exceed a specified amount e when the sample size is"
1789,1,"['sample', 'confident', 'estimate']", Single Sample Estimating a Proportion,seg_207,example 9.16: how large a sample is required if we want to be at least 95% confident that our estimate of p in example 9.14 is within 0.02 of the true value?
1790,1,"['sample', 'confident', 'estimate']", Single Sample Estimating a Proportion,seg_207,"solution : unlike in example 9.15, we shall now assume that no preliminary sample has been taken to provide an estimate of p. consequently, we can be at least 95% confident that our sample proportion will not differ from the true proportion by more than 0.02 if we choose a sample of size"
1791,1,"['sample', 'results', 'information']", Single Sample Estimating a Proportion,seg_207,"comparing the results of examples 9.15 and 9.16, we see that information concerning p, provided by a preliminary sample or from experience, enables us to choose a smaller sample while maintaining our required degree of accuracy."
1792,1,"['sample', 'independent', 'parameters', 'random samples', 'estimate', 'samples', 'binomial', 'random', 'estimator', 'point estimator', 'variances']", Two Samples Estimating the Difference between Two Proportions,seg_209,"consider the problem where we wish to estimate the difference between two binomial parameters p1 and p2. for example, p1 might be the proportion of smokers with lung cancer and p2 the proportion of nonsmokers with lung cancer, and the problem is to estimate the difference between these two proportions. first, we select independent random samples of sizes n1 and n2 from the two binomial populations with means n1p1 and n2p2 and variances n1p1q1 and n2p2q2, respectively; then we determine the numbers x1 and x2 of people in each sample with lung cancer and form the proportions p̂1 = x1/n and p̂2 = x2/n. a point estimator of the"
1793,1,"['sample', 'estimate', 'statistic', 'point estimate']", Two Samples Estimating the Difference between Two Proportions,seg_209,"difference between the two proportions, p1 − p2, is given by the statistic p̂1 − p̂2. therefore, the difference of the sample proportions, p̂1 − p̂2, will be used as the point estimate of p1 − p2."
1794,1,"['independent', 'interval', 'normally distributed', 'distribution', 'samples', 'confidence', 'confidence interval', 'variances']", Two Samples Estimating the Difference between Two Proportions,seg_209,"a confidence interval for p1 − p2 can be established by considering the sampling distribution of p̂1 − p̂2. from section 9.10 we know that p̂1 and p̂2 are each approximately normally distributed, with means p1 and p2 and variances p1q1/n1 and p2q2/n2, respectively. choosing independent samples from the two popula-"
1795,1,"['independent', 'variables', 'normally distributed', 'distribution', 'normal', 'mean', 'normal distribution']", Two Samples Estimating the Difference between Two Proportions,seg_209,"tions ensures that the variables p̂1 and p̂2 will be independent, and then by the reproductive property of the normal distribution established in theorem 7.11, we conclude that p̂1 − p̂2 is approximately normally distributed with mean"
1796,1,['variance'], Two Samples Estimating the Difference between Two Proportions,seg_209,and variance
1797,0,[], Two Samples Estimating the Difference between Two Proportions,seg_209,"therefore, we can assert that"
1798,1,"['standard normal', 'curve', 'normal', 'standard']", Two Samples Estimating the Difference between Two Proportions,seg_209,"and zα/2 is the value above which we find an area of α/2 under the standard normal curve. substituting for z, we write"
1799,1,"['interval', 'estimates', 'confidence', 'confidence interval']", Two Samples Estimating the Difference between Two Proportions,seg_209,"after performing the usual mathematical manipulations, we replace p1, p2, q1, and q2 under the radical sign by their estimates p̂1 = x1/n1, p̂2 = x2/n2, q̂1 = 1 − p̂1, and q̂2 = 1 − p̂2, provided that n1p̂1, n1q̂1, n2p̂2, and n2q̂2 are all greater than or equal to 5, and the following approximate 100(1− α)% confidence interval for p1 − p2 is obtained."
1800,1,"['random', 'parameters', 'interval', 'random samples', 'successes', 'samples', 'binomial', 'confidence', 'confidence interval']", Two Samples Estimating the Difference between Two Proportions,seg_209,"large-sample if p̂1 and p̂2 are the proportions of successes in random samples of sizes n1 and confidence n2, respectively, q̂1 = 1 − p̂1, and q̂2 = 1 − p̂2, an approximate 100(1 − α)% interval for confidence interval for the difference of two binomial parameters, p1 − p2, is p1 − p2 given by"
1801,1,"['interval', 'results', 'samples', 'confidence', 'process', 'confidence interval']", Two Samples Estimating the Difference between Two Proportions,seg_209,"example 9.17: a certain change in a process for manufacturing component parts is being considered. samples are taken under both the existing and the new process so as to determine if the new process results in an improvement. if 75 of 1500 items from the existing process are found to be defective and 80 of 2000 items from the new process are found to be defective, find a 90% confidence interval for the true difference in the proportion of defectives between the existing and the new process."
1802,1,"['point estimate', 'estimate']", Two Samples Estimating the Difference between Two Proportions,seg_209,"solution : let p1 and p2 be the true proportions of defectives for the existing and new processes, respectively. hence, p̂1 = 75/1500 = 0.05 and p̂2 = 80/2000 = 0.04, and the point estimate of p1 − p2 is"
1803,1,['table'], Two Samples Estimating the Difference between Two Proportions,seg_209,"using table a.3, we find z0.05 = 1.645. therefore, substituting into the formula, with"
1804,1,"['interval', 'method', 'confidence', 'process', 'confidence interval']", Two Samples Estimating the Difference between Two Proportions,seg_209,"we find the 90% confidence interval to be −0.0017 < p1 − p2 < 0.0217. since the interval contains the value 0, there is no reason to believe that the new process produces a significant decrease in the proportion of defectives over the existing method."
1805,1,"['confidence intervals', 'intervals', 'confidence']", Two Samples Estimating the Difference between Two Proportions,seg_209,"up to this point, all confidence intervals presented were of the form"
1806,1,"['point estimate', 'estimate']", Two Samples Estimating the Difference between Two Proportions,seg_209,"point estimate ± k s.e.(point estimate),"
1807,1,"['variances', 'symmetry', 'normal', 'mean', 'parameter', 'percent']", Two Samples Estimating the Difference between Two Proportions,seg_209,"where k is a constant (either t or normal percent point). this form is valid when the parameter is a mean, a difference between means, a proportion, or a difference between proportions, due to the symmetry of the tand z-distributions. however, it does not extend to variances and ratios of variances, which will be discussed in sections 9.12 and 9.13."
1808,1,['estimation'], Two Samples Estimating the Difference between Two Proportions,seg_209,302 chapter 9 oneand two-sample estimation problems
1809,1,"['sample', 'estimate', 'statistic', 'normal', 'point estimate', 'population', 'sample variance', 'variance', 'estimator']", Single Sample Estimating the Variance,seg_213,"if a sample of size n is drawn from a normal population with variance σ2 and the sample variance s2 is computed, we obtain a value of the statistic s2. this computed sample variance is used as a point estimate of σ2. hence, the statistic s2 is called an estimator of σ2."
1810,1,"['statistic', 'interval estimate', 'interval', 'estimate']", Single Sample Estimating the Variance,seg_213,an interval estimate of σ2 can be established by using the statistic
1811,1,"['degrees of freedom', 'distribution', 'samples', 'statistic', 'normal', 'population']", Single Sample Estimating the Variance,seg_213,"according to theorem 8.4, the statistic x2 has a chi-squared distribution with n− 1 degrees of freedom when samples are chosen from a normal population. we may write (see figure 9.7)"
1812,1,['distribution'], Single Sample Estimating the Variance,seg_213,where χ21−α/2 and χ2α/2 are values of the chi-squared distribution with n−1 degrees
1813,0,[], Single Sample Estimating the Variance,seg_213,"of freedom, leaving areas of 1−α/2 and α/2, respectively, to the right. substituting for x2, we write"
1814,1,['inequality'], Single Sample Estimating the Variance,seg_213,"dividing each term in the inequality by (n − 1)s2 and then inverting each term (thereby changing the sense of the inequalities), we obtain"
1815,1,"['sample', 'random', 'interval', 'normal', 'population', 'random sample', 'sample variance', 'confidence', 'variance', 'confidence interval']", Single Sample Estimating the Variance,seg_213,"for a random sample of size n from a normal population, the sample variance s2 is computed, and the following 100(1−α)% confidence interval for σ2 is obtained."
1816,1,"['sample', 'random', 'interval', 'normal', 'population', 'random sample', 'confidence', 'variance', 'confidence interval']", Single Sample Estimating the Variance,seg_213,"confidence if s2 is the variance of a random sample of size n from a normal population, a interval for σ2 100(1− α)% confidence interval for σ2 is"
1817,1,['degrees of freedom'], Single Sample Estimating the Variance,seg_213,"where χ2α/2 and χ21−α/2 are χ2-values with v = n−1 degrees of freedom, leaving"
1818,1,"['confidence', 'interval', 'confidence interval']", Single Sample Estimating the Variance,seg_213,an approximate 100(1 − α)% confidence interval for σ is obtained by taking the square root of each endpoint of the interval for σ2.
1819,1,"['interval', 'normal', 'population', 'confidence', 'variance', 'confidence interval']", Single Sample Estimating the Variance,seg_213,"example 9.18: the following are the weights, in decagrams, of 10 packages of grass seed distributed by a certain company: 46.4, 46.1, 45.8, 47.0, 46.1, 45.9, 45.8, 46.9, 45.2, and 46.0. find a 95% confidence interval for the variance of the weights of all such packages of grass seed distributed by this company, assuming a normal population."
1820,0,[], Single Sample Estimating the Variance,seg_213,solution : first we find
1821,1,"['degrees of freedom', 'interval', 'table', 'confidence', 'confidence interval']", Single Sample Estimating the Variance,seg_213,"to obtain a 95% confidence interval, we choose α = 0.05. then, using table a.5 with v = 9 degrees of freedom, we find χ20.025 = 19.023 and χ20.975 = 2.700. therefore, the 95% confidence interval for σ2 is"
1822,1,"['sample', 'sample variances', 'estimate', 'statistic', 'point estimate', 'population', 'estimator', 'variances']", Two Samples Estimating the Ratio of Two Variances,seg_215,"a point estimate of the ratio of two population variances σ12/σ22 is given by the ratio s21/s22 of the sample variances. hence, the statistic s12/s22 is called an estimator of σ12/σ22."
1823,1,"['interval', 'estimate', 'statistic', 'normal', 'interval estimate', 'populations', 'variances']", Two Samples Estimating the Ratio of Two Variances,seg_215,"if σ12 and σ22 are the variances of normal populations, we can establish an interval estimate of σ12/σ22 by using the statistic"
1824,1,"['degrees of freedom', 'random variable', 'variable', 'random']", Two Samples Estimating the Ratio of Two Variances,seg_215,"according to theorem 8.8, the random variable f has an f -distribution with v1 = n1 − 1 and v2 = n2 − 1 degrees of freedom. therefore, we may write (see figure 9.8)"
1825,1,['degrees of freedom'], Two Samples Estimating the Ratio of Two Variances,seg_215,"where f1−α/2(v1, v2) and fα/2(v1, v2) are the values of the f -distribution with v1 and v2 degrees of freedom, leaving areas of 1 − α/2 and α/2, respectively, to the right."
1826,0,[], Two Samples Estimating the Ratio of Two Variances,seg_215,"substituting for f , we write"
1827,1,['inequality'], Two Samples Estimating the Ratio of Two Variances,seg_215,"multiplying each term in the inequality by s22/s12 and then inverting each term, we obtain"
1828,1,['results'], Two Samples Estimating the Ratio of Two Variances,seg_215,"the results of theorem 8.7 enable us to replace the quantity f1−α/2(v1, v2) by 1/fα/2(v2, v1). therefore,"
1829,1,"['sample', 'independent', 'sample variances', 'interval', 'random samples', 'populations', 'samples', 'normal', 'random', 'confidence', 'confidence interval', 'variances']", Two Samples Estimating the Ratio of Two Variances,seg_215,"for any two independent random samples of sizes n1 and n2 selected from two normal populations, the ratio of the sample variances s21/s22 is computed, and the following 100(1− α)% confidence interval for σ12/σ22 is obtained."
1830,1,"['independent', 'interval', 'samples', 'normal', 'populations', 'confidence', 'confidence interval', 'variances']", Two Samples Estimating the Ratio of Two Variances,seg_215,"confidence if s12 and s22 are the variances of independent samples of sizes n1 and n2, reinterval for σ12/σ22 spectively, from normal populations, then a 100(1−α)% confidence interval for σ12/σ22 is"
1831,1,['degrees of freedom'], Two Samples Estimating the Ratio of Two Variances,seg_215,"where fα/2(v1, v2) is an f -value with v1 = n1 − 1 and v2 = n2 − 1 degrees of freedom, leaving an area of α/2 to the right, and fα/2(v2, v1) is a similar f -value with v2 = n2 − 1 and v1 = n1 − 1 degrees of freedom."
1832,1,"['confidence', 'interval', 'confidence interval']", Two Samples Estimating the Ratio of Two Variances,seg_215,"as in section 9.12, an approximate 100(1− α)% confidence interval for σ1/σ2 is obtained by taking the square root of each endpoint of the interval for σ12/σ22."
1833,1,"['population variance', 'confidence intervals', 'interval', 'intervals', 'normal', 'mean', 'population', 'populations', 'confidence', 'variance', 'confidence interval', 'variances']", Two Samples Estimating the Ratio of Two Variances,seg_215,"example 9.19: a confidence interval for the difference in the mean orthophosphorus contents, measured in milligrams per liter, at two stations on the james river was constructed in example 9.12 on page 290 by assuming the normal population variance to be unequal. justify this assumption by constructing 98% confidence intervals for σ12/σ22 and for σ1/σ2, where σ12 and σ22 are the variances of the populations of orthophosphorus contents at station 1 and station 2, respectively."
1834,1,"['confidence', 'interval', 'confidence interval', 'table']", Two Samples Estimating the Ratio of Two Variances,seg_215,"solution : from example 9.12, we have n1 = 15, n2 = 12, s1 = 3.07, and s2 = 0.80. for a 98% confidence interval, α = 0.02. interpolating in table a.6, we find f0.01(14, 11) ≈ 4.30 and f0.01(11, 14) ≈ 3.87. therefore, the 98% confidence interval for σ12/σ22 is"
1835,1,['confidence'], Two Samples Estimating the Ratio of Two Variances,seg_215,2 which simplifies to 3.425 < σ1 < 56.991. taking square roots of the confidence
1836,1,"['confidence', 'interval', 'confidence interval']", Two Samples Estimating the Ratio of Two Variances,seg_215,"σ22 limits, we find that a 98% confidence interval for σ1/σ2 is"
1837,1,['interval'], Two Samples Estimating the Ratio of Two Variances,seg_215,"since this interval does not allow for the possibility of σ1/σ2 being equal to 1, we were correct in assuming that σ1 = σ2 or σ12 = σ22 in example 9.12."
1838,1,"['method of maximum likelihood', 'statistics', 'estimators', 'estimator', 'sample', 'mean', 'parameter', 'population', 'parameters', 'estimation', 'binomial', 'method', 'population mean', 'likelihood', 'maximum likelihood', 'average']", Maximum Likelihood Estimation Optional,seg_219,"often the estimators of parameters have been those that appeal to intuition. the estimator x̄ certainly seems reasonable as an estimator of a population mean μ. the virtue of s2 as an estimator of σ2 is underscored through the discussion of unbiasedness in section 9.3. the estimator for a binomial parameter p is merely a sample proportion, which, of course, is an average and appeals to common sense. but there are many situations in which it is not at all obvious what the proper estimator should be. as a result, there is much to be learned by the student of statistics concerning different philosophies that produce different methods of estimation. in this section, we deal with the method of maximum likelihood."
1839,1,"['statistical inference', 'method', 'estimation', 'likelihood', 'maximum likelihood', 'statistical']", Maximum Likelihood Estimation Optional,seg_219,"maximum likelihood estimation is one of the most important approaches to estimation in all of statistical inference. we will not give a thorough development of the method. rather, we will attempt to communicate the philosophy of maximum likelihood and illustrate with examples that relate to other estimation problems discussed in this chapter."
1840,1,"['random variables', 'method', 'method of maximum likelihood', 'variables', 'independent', 'likelihood', 'discrete', 'distribution', 'maximum likelihood', 'independent random variables', 'likelihood function', 'parameter', 'random', 'function', 'discrete distribution']", Maximum Likelihood Estimation Optional,seg_219,"as the name implies, the method of maximum likelihood is that for which the likelihood function is maximized. the likelihood function is best illustrated through the use of an example with a discrete distribution and a single parameter. denote by x1, x2, . . . , xn the independent random variables taken from a discrete probability distribution represented by f(x, θ), where θ is a single parameter of the distribution. now"
1841,1,"['case', 'discrete', 'likelihood function', 'probability', 'random', 'function', 'sample', 'random variable', 'distribution', 'discrete random variable', 'random variables', 'variables', 'likelihood', 'variable', 'joint', 'joint probability']", Maximum Likelihood Estimation Optional,seg_219,"is the joint distribution of the random variables, often referred to as the likelihood function. note that the variable of the likelihood function is θ, not x. denote by x1, x2, . . . , xn the observed values in a sample. in the case of a discrete random variable, the interpretation is very clear. the quantity l(x1, x2, . . . , xn; θ), the likelihood of the sample, is the following joint probability:"
1842,1,"['sample', 'results', 'likelihood', 'case', 'discrete', 'joint', 'maximum likelihood', 'maximum likelihood estimator', 'probability', 'estimator', 'joint probability']", Maximum Likelihood Estimation Optional,seg_219,"which is the probability of obtaining the sample values x1, x2, . . . , xn. for the discrete case, the maximum likelihood estimator is one that results in a maximum value for this joint probability or maximizes the likelihood of the sample."
1843,1,"['sample', 'process', 'estimate', 'results', 'likelihood', 'bernoulli']", Maximum Likelihood Estimation Optional,seg_219,"consider a fictitious example where three items from an assembly line are inspected. the items are ruled either defective or nondefective, and thus the bernoulli process applies. testing the three items results in two nondefective items followed by a defective item. it is of interest to estimate p, the proportion nondefective in the process. the likelihood of the sample for this illustration is given by"
1844,1,"['maximum likelihood estimation', 'estimate', 'estimation', 'likelihood', 'set', 'maximum likelihood']", Maximum Likelihood Estimation Optional,seg_219,"where q = 1 − p. maximum likelihood estimation would give an estimate of p for which the likelihood is maximized. it is clear that if we differentiate the likelihood with respect to p, set the derivative to zero, and solve, we obtain the value"
1845,1,"['sample', 'likelihood', 'case', 'discrete', 'information', 'jointly', 'maximum likelihood', 'probability', 'parameter', 'estimator']", Maximum Likelihood Estimation Optional,seg_219,"now, of course, in this situation p̂ = 2/3 is the sample proportion defective and is thus a reasonable estimator of the probability of a defective. the reader should attempt to understand that the philosophy of maximum likelihood estimation evolves from the notion that the reasonable estimator of a parameter based on sample information is that parameter value that produces the largest probability of obtaining the sample. this is, indeed, the interpretation for the discrete case, since the likelihood is the probability of jointly observing the values in the sample."
1846,1,"['maximum likelihood estimation', 'continuous distribution', 'parameters', 'continuous', 'estimation', 'likelihood', 'case', 'discrete', 'distribution', 'joint', 'maximum likelihood', 'likelihood function', 'probability', 'function', 'joint probability']", Maximum Likelihood Estimation Optional,seg_219,"now, while the interpretation of the likelihood function as a joint probability is confined to the discrete case, the notion of maximum likelihood extends to the estimation of parameters of a continuous distribution. we now present a formal definition of maximum likelihood estimation."
1847,1,"['independent', 'continuous', 'mass function', 'probability mass function', 'observations', 'case', 'discrete', 'probability', 'function']", Maximum Likelihood Estimation Optional,seg_219,"definition 9.3: given independent observations x1, x2, . . . , xn from a probability density function (continuous case) or probability mass function (discrete case) f(x; θ), the"
1848,1,"['function', 'estimator', 'likelihood', 'likelihood function']", Maximum Likelihood Estimation Optional,seg_219,maximum likelihood estimator θ̂ is that which maximizes the likelihood function
1849,1,"['poisson', 'likelihood', 'distribution', 'likelihood function', 'parameter', 'function', 'poisson distribution']", Maximum Likelihood Estimation Optional,seg_219,quite often it is convenient to work with the natural log of the likelihood function in finding the maximum of that function. consider the following example dealing with the parameter μ of a poisson distribution.
1850,1,"['poisson', 'mass function', 'probability mass function', 'distribution', 'function', 'probability', 'poisson distribution']", Maximum Likelihood Estimation Optional,seg_219,example 9.20: consider a poisson distribution with probability mass function
1851,1,"['sample', 'random', 'maximum likelihood estimate', 'estimate', 'likelihood', 'distribution', 'maximum likelihood', 'random sample']", Maximum Likelihood Estimation Optional,seg_219,"suppose that a random sample x1, x2, . . . , xn is taken from the distribution. what is the maximum likelihood estimate of μ?"
1852,1,"['function', 'likelihood', 'likelihood function']", Maximum Likelihood Estimation Optional,seg_219,solution : the likelihood function is
1853,1,"['likelihood', 'maximum likelihood estimator', 'maximum likelihood', 'parameter', 'estimator']", Maximum Likelihood Estimation Optional,seg_219,"solving for μ̂, the maximum likelihood estimator, involves setting the derivative to zero and solving for the parameter. thus,"
1854,1,"['poisson', 'sample', 'distribution', 'function', 'sample average', 'mean', 'poisson distribution', 'average', 'estimator']", Maximum Likelihood Estimation Optional,seg_219,"the second derivative of the log-likelihood function is negative, which implies that the solution above indeed is a maximum. since μ is the mean of the poisson distribution (chapter 5), the sample average would certainly seem like a reasonable estimator."
1855,1,"['method', 'estimates', 'method of maximum likelihood', 'parameters', 'likelihood', 'jointly', 'maximum likelihood', 'likelihood function', 'function']", Maximum Likelihood Estimation Optional,seg_219,the following example shows the use of the method of maximum likelihood for finding estimates of two parameters. we simply find the values of the parameters that maximize (jointly) the likelihood function.
1856,1,"['sample', 'maximum likelihood estimators', 'likelihood', 'random sample', 'estimators', 'distribution', 'maximum likelihood', 'normal', 'random', 'normal distribution']", Maximum Likelihood Estimation Optional,seg_219,"example 9.21: consider a random sample x1, x2, . . . , xn from a normal distribution n(μ, σ). find the maximum likelihood estimators for μ and σ2."
1857,1,"['likelihood', 'distribution', 'normal', 'likelihood function', 'function', 'normal distribution']", Maximum Likelihood Estimation Optional,seg_219,solution : the likelihood function for the normal distribution is
1858,0,[], Maximum Likelihood Estimation Optional,seg_219,taking logarithms gives us
1859,0,[], Maximum Likelihood Estimation Optional,seg_219,"setting both derivatives equal to 0, we obtain"
1860,1,"['maximum likelihood estimator', 'maximum likelihood', 'estimator', 'likelihood']", Maximum Likelihood Estimation Optional,seg_219,"thus, the maximum likelihood estimator of μ is given by"
1861,1,"['estimate', 'likelihood', 'maximum likelihood', 'maximum likelihood estimator', 'point estimate', 'estimator']", Maximum Likelihood Estimation Optional,seg_219,"which is a pleasing result since x̄ has played such an important role in this chapter as a point estimate of μ. on the other hand, the maximum likelihood estimator of σ2 is"
1862,1,"['results', 'likelihood', 'likelihood function', 'function']", Maximum Likelihood Estimation Optional,seg_219,checking the second-order partial derivative matrix confirms that the solution results in a maximum of the likelihood function.
1863,1,"['degrees of freedom', 'unbiased estimator', 'asymptotic', 'likelihood', 'estimators', 'maximum likelihood', 'maximum likelihood estimator', 'estimator', 'unbiased']", Maximum Likelihood Estimation Optional,seg_219,"it is interesting to note the distinction between the maximum likelihood estimator of σ2 and the unbiased estimator s2 developed earlier in this chapter. the numerators are identical, of course, and the denominator is the degrees of freedom n−1 for the unbiased estimator and n for the maximum likelihood estimator. maximum likelihood estimators do not necessarily enjoy the property of unbiasedness. however, they do have very important asymptotic properties."
1864,1,['rate'], Maximum Likelihood Estimation Optional,seg_219,"example 9.22: suppose 10 rats are used in a biomedical study where they are injected with cancer cells and then given a cancer drug that is designed to increase their survival rate. the survival times, in months, are 14, 17, 27, 18, 12, 8, 22, 13, 19, and 12. assume"
1865,1,"['exponential distribution', 'maximum likelihood estimate', 'estimate', 'likelihood', 'distribution', 'maximum likelihood', 'exponential', 'mean']", Maximum Likelihood Estimation Optional,seg_219,that the exponential distribution applies. give a maximum likelihood estimate of the mean survival time.
1866,1,"['density function', 'probability density function', 'random variable', 'variable', 'exponential', 'probability', 'random', 'function']", Maximum Likelihood Estimation Optional,seg_219,"solution : from chapter 6, we know that the probability density function for the exponential random variable x is"
1867,1,"['function', 'data']", Maximum Likelihood Estimation Optional,seg_219,"thus, the log-likelihood function for the data, given n = 10, is"
1868,1,"['sample', 'sample average', 'mean', 'parameter', 'function', 'average', 'estimator']", Maximum Likelihood Estimation Optional,seg_219,"evaluating the second derivative of the log-likelihood function at the value β̂ above yields a negative value. as a result, the estimator of the parameter β, the population mean, is the sample average x̄."
1869,1,"['maximum likelihood estimator', 'maximum likelihood', 'estimator', 'likelihood']", Maximum Likelihood Estimation Optional,seg_219,the following example shows the maximum likelihood estimator for a distribution that does not appear in previous chapters.
1870,1,"['sample', 'density function', 'population', 'function']", Maximum Likelihood Estimation Optional,seg_219,"example 9.23: it is known that a sample consisting of the values 12, 11.2, 13.5, 12.3, 13.8, and 11.9 comes from a population with the density function"
1871,1,"['maximum likelihood', 'likelihood', 'maximum likelihood estimate', 'estimate']", Maximum Likelihood Estimation Optional,seg_219,where θ > 0. find the maximum likelihood estimate of θ.
1872,1,"['observations', 'likelihood', 'likelihood function', 'population', 'function']", Maximum Likelihood Estimation Optional,seg_219,solution : the likelihood function of n observations from this population can be written as
1873,1,['estimation'], Maximum Likelihood Estimation Optional,seg_219,312 chapter 9 oneand two-sample estimation problems
1874,1,['likelihood'], Maximum Likelihood Estimation Optional,seg_219,"since the second derivative of l is −n/θ2, which is always negative, the likelihood"
1875,0,[], Maximum Likelihood Estimation Optional,seg_219,function does achieve its maximum value at θ̂.
1876,1,"['maximum likelihood estimation', 'method of maximum likelihood', 'estimators', 'estimator', 'sample', 'bias', 'maximum likelihood estimator', 'efficiency', 'statistical', 'limit', 'sample size', 'statistical inference', 'estimation', 'distribution', 'variance', 'unbiased', 'method', 'likelihood', 'maximum likelihood']", Maximum Likelihood Estimation Optional,seg_219,"a thorough discussion of the properties of maximum likelihood estimation is beyond the scope of this book and is usually a major topic of a course in the theory of statistical inference. the method of maximum likelihood allows the analyst to make use of knowledge of the distribution in determining an appropriate estimator. the method of maximum likelihood cannot be applied without knowledge of the underlying distribution. we learned in example 9.21 that the maximum likelihood estimator is not necessarily unbiased. the maximum likelihood estimator is unbiased asymptotically or in the limit; that is, the amount of bias approaches zero as the sample size becomes large. earlier in this chapter the notion of efficiency was discussed, efficiency being linked to the variance property of an estimator. maximum likelihood estimators possess desirable variance properties in the limit. the reader should consult lehmann and d’abrera (1998) for details."
1877,1,"['interval', 'distribution', 'normal', 'population', 'confidence', 'confidence interval']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_225,"the concept of a large-sample confidence interval on a population is often confusing to the beginning student. it is based on the notion that even when σ is unknown and one is not convinced that the distribution being sampled is normal, a confidence interval on μ can be computed from"
1878,1,"['sample', 'interval', 'estimate', 'approximation', 'symmetric', 'case', 'distribution', 'normality', 'central limit theorem', 'limit']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_225,"in practice, this formula is often used when the sample is too small. the genesis of this large sample interval is, of course, the central limit theorem (clt), under which normality is not necessary. here the clt requires a known σ, of which s is only an estimate. thus, n must be at least as large as 30 and the underlying distribution must be close to symmetric, in which case the interval is still an approximation."
1879,1,"['interval', 'distribution', 'normality', 'normal', 'confidence', 'confidence interval']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_225,"there are instances in which the appropriateness of the practical application of material in this chapter depends very much on the specific context. one very important illustration is the use of the t-distribution for the confidence interval on μ when σ is unknown. strictly speaking, the use of the t-distribution requires that the distribution sampled from be normal. however, it is well known that any application of the t-distribution is reasonably insensitive (i.e., robust) to the normality assumption. this represents one of those fortunate situations which"
1880,1,"['sample', 'probability plots', 'plots', 'statistics', 'normal', 'probability', 'population', 'tests', 'normal probability plots']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_225,"occur often in the field of statistics in which a basic assumption does not hold and yet “everything turns out all right!” however, one population from which the sample is drawn cannot deviate substantially from normal. thus, the normal probability plots discussed in chapter 8 and the goodness-of-fit tests introduced in chapter 10 often need be called upon to ascertain some sense of “nearness to normality.” this idea of “robustness to normality” will reappear in chapter 10."
1881,1,"['bayesian', 'interval', 'observation', 'intervals', 'information', 'frequency', 'mean', 'probability', 'parameter', 'confidence', 'statistical', 'confidence interval', 'posterior']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_225,"it is our experience that one of the most serious “misuses of statistics” in practice evolves from confusion about distinctions in the interpretation of the types of statistical intervals. thus, the subsection in this chapter where differences among the three types of intervals are discussed is important. it is very likely that in practice the confidence interval is heavily overused. that is, it is used when there is really no interest in the mean; rather, the question is “where is the next observation going to fall?” or often, more importantly, “where is the large bulk of the distribution?” these are crucial questions that are not answered by computing an interval on the mean. the interpretation of a confidence interval is often misunderstood. it is tempting to conclude that the parameter falls inside the interval with probability 0.95. while this is a correct interpretation of a bayesian posterior interval (readers are referred to chapter 18 for more information on bayesian inference), it is not the proper frequency interpretation."
1882,1,"['true parameter', 'interval', 'experiment', 'statistics', 'data', 'intervals', 'parameter', 'confidence', 'statistical', 'confidence interval']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_225,"a confidence interval merely suggests that if the experiment is conducted and data are observed again and again, about 95% of such intervals will contain the true parameter. any beginning student of practical statistics should be very clear on the difference among these statistical intervals."
1883,1,"['sample', 'interval', 'statistics', 'distribution', 'normality', 'χ2 test', 'confidence', 'test', 'variance', 'confidence interval']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_225,"another potential serious misuse of statistics centers around the use of the χ2-distribution for a confidence interval on a single variance. again, normality of the distribution from which the sample is drawn is assumed. unlike the use of the t-distribution, the use of the χ2 test for this application is not robust to the nor-"
1884,1,"['sampling', 'sampling distribution', 'distribution']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_225,"mality assumption (i.e., the sampling distribution of (n−1)s2 deviates far from"
1885,1,"['information', 'distribution', 'normal', 'probability', 'plotting', 'probability plotting', 'tests']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_225,"σ2 χ2 if the underlying distribution is not normal). thus, strict use of goodness-of-fit (chapter 10) tests and/or normal probability plotting can be extremely important in such contexts. more information about this general issue will be given in future chapters."
1886,0,[], Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_225,this page intentionally left blank
1887,1,"['risk', 'statistical hypothesis', 'case', 'sample', 'data', 'cases', 'mean', 'parameter', 'population', 'statistical', 'experimental', 'statistical inference', 'hypothesis', 'independent', 'variables']", Statistical Hypotheses General Concepts,seg_229,"often, the problem confronting the scientist or engineer is not so much the estimation of a population parameter, as discussed in chapter 9, but rather the formation of a data-based decision procedure that can produce a conclusion about some scientific system. for example, a medical researcher may decide on the basis of experimental evidence whether coffee drinking increases the risk of cancer in humans; an engineer might have to decide on the basis of sample data whether there is a difference between the accuracy of two kinds of gauges; or a sociologist might wish to collect appropriate data to enable him or her to decide whether a person’s blood type and eye color are independent variables. in each of these cases, the scientist or engineer postulates or conjectures something about a system. in addition, each must make use of experimental data and make a decision based on the data. in each case, the conjecture can be put in the form of a statistical hypothesis. procedures that lead to the acceptance or rejection of statistical hypotheses such as these comprise a major area of statistical inference. first, let us define precisely what we mean by a statistical hypothesis."
1888,1,"['statistical hypothesis', 'populations', 'statistical', 'hypothesis']", Statistical Hypotheses General Concepts,seg_229,definition 10.1: a statistical hypothesis is an assertion or conjecture concerning one or more populations.
1889,1,"['sample', 'random', 'statistical hypothesis', 'data', 'stated hypothesis', 'population', 'random sample', 'statistical', 'hypothesis']", Statistical Hypotheses General Concepts,seg_229,"the truth or falsity of a statistical hypothesis is never known with absolute certainty unless we examine the entire population. this, of course, would be impractical in most situations. instead, we take a random sample from the population of interest and use the data contained in this sample to provide evidence that either supports or does not support the hypothesis. evidence from the sample that is inconsistent with the stated hypothesis leads to a rejection of the hypothesis."
1890,1,"['risk', 'probability', 'random sample', 'random', 'process', 'sample', 'experiment', 'data', 'information', 'parameter', 'condition', 'failure', 'binomial', 'hypothesis']", Statistical Hypotheses General Concepts,seg_229,"it should be made clear to the reader that the decision procedure must include an awareness of the probability of a wrong conclusion. for example, suppose that the hypothesis postulated by the engineer is that the fraction defective p in a certain process is 0.10. the experiment is to observe a random sample of the product in question. suppose that 100 items are tested and 12 items are found defective. it is reasonable to conclude that this evidence does not refute the condition that the binomial parameter p = 0.10, and thus it may lead one not to reject the hypothesis. however, it also does not refute p = 0.12 or perhaps even p = 0.15. as a result, the reader must be accustomed to understanding that rejection of a hypothesis implies that the sample evidence refutes it. put another way, rejection means that there is a small probability of obtaining the sample information observed when, in fact, the hypothesis is true. for example, for our proportion-defective hypothesis, a sample of 100 revealing 20 defective items is certainly evidence for rejection. why? if, indeed, p = 0.10, the probability of obtaining 20 or more defectives is approximately 0.002. with the resulting small risk of a wrong conclusion, it would seem safe to reject the hypothesis that p = 0.10. in other words, rejection of a hypothesis tends to all but “rule out” the hypothesis. on the other hand, it is very important to emphasize that acceptance or, rather, failure to reject does not rule out other possibilities. as a result, the firm conclusion is established by the data analyst when a hypothesis is rejected."
1891,1,"['risk', 'hypothesis tested', 'probability', 'tests', 'hypothesis']", Statistical Hypotheses General Concepts,seg_229,"the formal statement of a hypothesis is often influenced by the structure of the probability of a wrong conclusion. if the scientist is interested in strongly supporting a contention, he or she hopes to arrive at the contention in the form of rejection of a hypothesis. if the medical researcher wishes to show strong evidence in favor of the contention that coffee drinking increases the risk of cancer, the hypothesis tested should be of the form “there is no increase in cancer risk produced by drinking coffee.” as a result, the contention is reached via a rejection. similarly, to support the claim that one kind of gauge is more accurate than another, the engineer tests the hypothesis that there is no difference in the accuracy of the two kinds of gauges."
1892,1,"['experimental', 'data', 'hypothesis testing', 'hypothesis']", Statistical Hypotheses General Concepts,seg_229,"the foregoing implies that when the data analyst formalizes experimental evidence on the basis of hypothesis testing, the formal statement of the hypothesis is very important."
1893,1,"['alternative hypothesis', 'hypothesis testing', 'complement', 'test', 'null hypothesis', 'hypothesis']", Statistical Hypotheses General Concepts,seg_229,"the structure of hypothesis testing will be formulated with the use of the term null hypothesis, which refers to any hypothesis we wish to test and is denoted by h0. the rejection of h0 leads to the acceptance of an alternative hypothesis, denoted by h1. an understanding of the different roles played by the null hypothesis (h0) and the alternative hypothesis (h1) is crucial to one’s understanding of the rudiments of hypothesis testing. the alternative hypothesis h1 usually represents the question to be answered or the theory to be tested, and thus its specification is crucial. the null hypothesis h0 nullifies or opposes h1 and is often the logical complement to h1. as the reader gains more understanding of hypothesis testing, he or she should note that the analyst arrives at one of the two following"
1894,1,['data'], Statistical Hypotheses General Concepts,seg_229,reject h0 in favor of h1 because of sufficient evidence in the data or
1895,1,['data'], Statistical Hypotheses General Concepts,seg_229,fail to reject h0 because of insufficient evidence in the data.
1896,1,"['binomial', 'failure', 'probability']", Statistical Hypotheses General Concepts,seg_229,"note that the conclusions do not involve a formal and literal “accept h0.” the statement of h0 often represents the “status quo” in opposition to the new idea, conjecture, and so on, stated in h1, while failure to reject h0 represents the proper conclusion. in our binomial example, the practical issue may be a concern that the historical defective probability of 0.10 no longer is true. indeed, the conjecture may be that p exceeds 0.10. we may then state"
1897,1,['data'], Statistical Hypotheses General Concepts,seg_229,"now 12 defective items out of 100 does not refute p = 0.10, so the conclusion is “fail to reject h0.” however, if the data produce 20 out of 100 defective items, then the conclusion is “reject h0” in favor of h1: p > 0.10."
1898,1,"['null and alternative hypotheses', 'hypothesis testing', 'hypotheses', 'trial', 'alternative hypotheses', 'hypothesis']", Statistical Hypotheses General Concepts,seg_229,"though the applications of hypothesis testing are quite abundant in scientific and engineering work, perhaps the best illustration for a novice lies in the predicament encountered in a jury trial. the null and alternative hypotheses are"
1899,0,[], Statistical Hypotheses General Concepts,seg_229,"h0: defendant is innocent,"
1900,0,[], Statistical Hypotheses General Concepts,seg_229,h1: defendant is guilty.
1901,1,"['case', 'hypothesis']", Statistical Hypotheses General Concepts,seg_229,"the indictment comes because of suspicion of guilt. the hypothesis h0 (the status quo) stands in opposition to h1 and is maintained unless h1 is supported by evidence “beyond a reasonable doubt.” however, “failure to reject h0” in this case does not imply innocence, but merely that the evidence was insufficient to convict. so the jury does not necessarily accept h0 but fails to reject h0."
1902,1,"['statistical hypothesis', 'statistical test', 'random', 'test', 'statistical', 'null hypothesis', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"to illustrate the concepts used in testing a statistical hypothesis about a population, we present the following example. a certain type of cold vaccine is known to be only 25% effective after a period of 2 years. to determine if a new and somewhat more expensive vaccine is superior in providing protection against the same virus for a longer period of time, suppose that 20 people are chosen at random and inoculated. (in an actual study of this type, the participants receiving the new vaccine might number several thousand. the number 20 is being used here only to demonstrate the basic steps in carrying out a statistical test.) if more than 8 of those receiving the new vaccine surpass the 2-year period without contracting the virus, the new vaccine will be considered superior to the one presently in use. the requirement that the number exceed 8 is somewhat arbitrary but appears reasonable in that it represents a modest gain over the 5 people who could be expected to receive protection if the 20 people had been inoculated with the vaccine already in use. we are essentially testing the null hypothesis that the new vaccine is equally effective after a period of 2 years as the one now commonly used. the alternative"
1903,1,"['trial', 'success', 'binomial', 'probability', 'parameter', 'probability of a success', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,hypothesis is that the new vaccine is in fact superior. this is equivalent to testing the hypothesis that the binomial parameter for the probability of a success on a given trial is p = 1/4 against the alternative that p > 1/4. this is usually written as follows:
1904,1,"['alternative hypothesis', 'critical region', 'test statistic', 'statistic', 'scores', 'test', 'critical value', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"the test statistic on which we base our decision is x, the number of individuals in our test group who receive protection from the new vaccine for a period of at least 2 years. the possible values of x, from 0 to 20, are divided into two groups: those numbers less than or equal to 8 and those greater than 8. all possible scores greater than 8 constitute the critical region. the last number that we observe in passing into the critical region is called the critical value. in our illustration, the critical value is the number 8. therefore, if x > 8, we reject h0 in favor of the alternative hypothesis h1. if x ≤ 8, we fail to reject h0. this decision criterion is illustrated in figure 10.1."
1905,0,[], Testing a Statistical Hypothesis,seg_231,figure 10.1: decision criterion for testing p = 0.25 versus p > 0.25.
1906,1,"['type i error', 'error']", Testing a Statistical Hypothesis,seg_231,"the decision procedure just described could lead to either of two wrong conclusions. for instance, the new vaccine may be no better than the one now in use (h0 true) and yet, in this particular randomly selected group of individuals, more than 8 surpass the 2-year period without contracting the virus. we would be committing an error by rejecting h0 in favor of h1 when, in fact, h0 is true. such an error is called a type i error."
1907,1,"['error', 'type i error', 'null hypothesis', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,definition 10.2: rejection of the null hypothesis when it is true is called a type i error.
1908,1,"['type ii error', 'case', 'type ii', 'error']", Testing a Statistical Hypothesis,seg_231,"a second kind of error is committed if 8 or fewer of the group surpass the 2-year period successfully and we are unable to conclude that the vaccine is better when it actually is better (h1 true). thus, in this case, we fail to reject h0 when in fact h0 is false. this is called a type ii error."
1909,1,"['type ii error', 'null hypothesis', 'type ii', 'error', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,definition 10.3: nonrejection of the null hypothesis when it is false is called a type ii error.
1910,1,"['statistical hypothesis', 'statistical', 'error', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"in testing any statistical hypothesis, there are four possible situations that determine whether our decision is correct or in error. these four situations are"
1911,1,['table'], Testing a Statistical Hypothesis,seg_231,summarized in table 10.1.
1912,1,"['statistical hypothesis', 'statistical', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,table 10.1: possible situations for testing a statistical hypothesis
1913,1,"['type ii error', 'type i error', 'error', 'type ii']", Testing a Statistical Hypothesis,seg_231,h0 is true h0 is false do not reject h0 correct decision type ii error reject h0 type i error correct decision
1914,1,"['level', 'type i error', 'error', 'probability']", Testing a Statistical Hypothesis,seg_231,"the probability of committing a type i error, also called the level of significance, is denoted by the greek letter α. in our illustration, a type i error will occur when more than 8 individuals inoculated with the new vaccine surpass the 2-year period without contracting the virus and researchers conclude that the new vaccine is better when it is actually equivalent to the one in use. hence, if x is the number of individuals who remain free of the virus for at least 2 years,"
1915,1,"['type i error', 'error']", Testing a Statistical Hypothesis,seg_231,"20∑ 1 1 α = p (type i error) = p (x > 8 when p = 4) = b( x; 20, 4) x=9"
1916,1,"['error', 'level of significance', 'null hypothesis', 'level', 'significance', 'test', 'type i error', 'critical region', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"we say that the null hypothesis, p = 1/4, is being tested at the α = 0.0409 level of significance. sometimes the level of significance is called the size of the test. a critical region of size 0.0409 is very small, and therefore it is unlikely that a type i error will be committed. consequently, it would be most unusual for more than 8 individuals to remain immune to a virus for a 2-year period using a new vaccine that is essentially equivalent to the one now on the market."
1917,1,"['alternative hypothesis', 'type ii error', 'error', 'case', 'probability', 'type ii', 'test', 'null hypothesis', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"the probability of committing a type ii error, denoted by β, is impossible to compute unless we have a specific alternative hypothesis. if we test the null hypothesis that p = 1/4 against the alternative hypothesis that p = 1/2, then we are able to compute the probability of not rejecting h0 when it is false. we simply find the probability of obtaining 8 or fewer in the group that surpass the 2-year period when p = 1/2. in this case,"
1918,1,"['type ii error', 'error', 'type ii']", Testing a Statistical Hypothesis,seg_231,1 β = p (type ii error) = p ( x ≤ 8 when p = 2)
1919,1,"['type ii error', 'probabilities', 'probability', 'type ii', 'test', 'error']", Testing a Statistical Hypothesis,seg_231,"this is a rather high probability, indicating a test procedure in which it is quite likely that we shall reject the new vaccine when, in fact, it is superior to what is now in use. ideally, we like to use a test procedure for which the type i and type ii error probabilities are both small."
1920,1,"['type ii error', 'error', 'type ii']", Testing a Statistical Hypothesis,seg_231,"it is possible that the director of the testing program is willing to make a type ii error if the more expensive vaccine is not significantly superior. in fact, the only"
1921,1,"['type ii error', 'test', 'error', 'type ii']", Testing a Statistical Hypothesis,seg_231,"time he wishes to guard against the type ii error is when the true value of p is at least 0.7. if p = 0.7, this test procedure gives"
1922,1,"['type ii error', 'error', 'type ii']", Testing a Statistical Hypothesis,seg_231,β = p (type ii error) = p (x ≤ 8 when p = 0.7)
1923,1,"['alternative hypothesis', 'type ii error', 'probability', 'type ii', 'error', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"with such a small probability of committing a type ii error, it is extremely unlikely that the new vaccine would be rejected when it was 70% effective after a period of 2 years. as the alternative hypothesis approaches unity, the value of β diminishes to zero."
1924,1,"['alternative hypothesis', 'type ii error', 'error', 'critical region', 'scores', 'probability', 'type ii', 'critical value', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"let us assume that the director of the testing program is unwilling to commit a type ii error when the alternative hypothesis p = 1/2 is true, even though we have found the probability of such an error to be β = 0.2517. it is always possible to reduce β by increasing the size of the critical region. for example, consider what happens to the values of α and β when we change our critical value to 7 so that all scores greater than 7 fall in the critical region and those less than or equal to 7 fall in the nonrejection region. now, in testing p = 1/4 against the alternative hypothesis that p = 1/2, we find that"
1925,1,"['sample size', 'sample', 'alternative hypothesis', 'error', 'type ii error', 'acceptance region', 'critical region', 'null hypothesis', 'random sample', 'scores', 'probability', 'random', 'type ii', 'type i error', 'critical value', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"β =∑ b(x; 20, 2) = 0.1316. x=0 by adopting a new decision procedure, we have reduced the probability of committing a type ii error at the expense of increasing the probability of committing a type i error. for a fixed sample size, a decrease in the probability of one error will usually result in an increase in the probability of the other error. fortunately, the probability of committing both types of error can be reduced by increasing the sample size. consider the same problem using a random sample of 100 individuals. if more than 36 of the group surpass the 2-year period, we reject the null hypothesis that p = 1/4 and accept the alternative hypothesis that p > 1/4. the critical value is now 36. all possible scores above 36 constitute the critical region, and all possible scores less than or equal to 36 fall in the acceptance region."
1926,1,"['curve', 'approximation', 'normal', 'probability', 'type i error', 'error']", Testing a Statistical Hypothesis,seg_231,"to determine the probability of committing a type i error, we shall use the normal curve approximation with"
1927,1,"['curve', 'normal']", Testing a Statistical Hypothesis,seg_231,"referring to figure 10.2, we need the area under the normal curve to the right of x = 36.5. the corresponding z-value is"
1928,1,"['type i error', 'error', 'probability']", Testing a Statistical Hypothesis,seg_231,figure 10.2: probability of a type i error.
1929,1,"['type i error', 'error', 'table']", Testing a Statistical Hypothesis,seg_231,from table a.3 we find that 1 α = p (type i error) = p (x > 36 when p = 4) ≈ p (z > 2.66)
1930,1,"['type ii error', 'curve', 'approximation', 'normal', 'probability', 'type ii', 'error']", Testing a Statistical Hypothesis,seg_231,"if h0 is false and the true value of h1 is p = 1/2, we can determine the probability of a type ii error using the normal curve approximation with"
1931,1,['probability'], Testing a Statistical Hypothesis,seg_231,the probability of a value falling in the nonrejection region when h0 is true is given by the area of the shaded region to the left of x = 36.5 in figure 10.3. the z-value corresponding to x = 36.5 is
1932,1,"['type ii error', 'type ii', 'error', 'probability']", Testing a Statistical Hypothesis,seg_231,figure 10.3: probability of a type ii error.
1933,1,"['type ii error', 'error', 'type ii']", Testing a Statistical Hypothesis,seg_231,1 β = p (type ii error) = p (
1934,1,"['experiment', 'type i and type ii errors', 'errors', 'type ii', 'type ii errors']", Testing a Statistical Hypothesis,seg_231,"obviously, the type i and type ii errors will rarely occur if the experiment consists of 100 individuals."
1935,1,"['sample size', 'sample', 'deviation', 'null and alternative hypotheses', 'hypothesis testing', 'hypotheses', 'mean', 'probability', 'sensitivity', 'test', 'alternative hypotheses', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"the illustration above underscores the strategy of the scientist in hypothesis testing. after the null and alternative hypotheses are stated, it is important to consider the sensitivity of the test procedure. by this we mean that there should be a determination, for a fixed α, of a reasonable value for the probability of wrongly accepting h0 (i.e., the value of β) when the true situation represents some important deviation from h0. a value for the sample size can usually be determined for which there is a reasonable balance between the values of α and β computed in this fashion. the vaccine problem provides an illustration."
1936,1,"['alternative hypothesis', 'continuous random variables', 'random variables', 'variables', 'discrete', 'population', 'random', 'continuous', 'test', 'average', 'null hypothesis', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"the concepts discussed here for a discrete population can be applied equally well to continuous random variables. consider the null hypothesis that the average weight of male students in a certain college is 68 kilograms against the alternative hypothesis that it is unequal to 68. that is, we wish to test"
1937,1,"['alternative hypothesis', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,the alternative hypothesis allows for the possibility that μ < 68 or μ > 68.
1938,1,"['sample', 'sample mean', 'interval', 'test statistic', 'case', 'intervals', 'statistic', 'mean', 'test', 'critical region']", Testing a Statistical Hypothesis,seg_231,"a sample mean that falls close to the hypothesized value of 68 would be considered evidence in favor ofh0. on the other hand, a sample mean that is considerably less than or more than 68 would be evidence inconsistent with h0 and therefore favoring h1. the sample mean is the test statistic in this case. a critical region for the test statistic might arbitrarily be chosen to be the two intervals x̄ < 67 and x̄ > 69. the nonrejection region will then be the interval 67 ≤ x̄ ≤ 69. this decision criterion is illustrated in figure 10.4."
1939,1,['critical region'], Testing a Statistical Hypothesis,seg_231,figure 10.4: critical region (in blue).
1940,1,"['probabilities', 'type i and type ii errors', 'errors', 'type ii', 'null hypothesis', 'type ii errors', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,let us now use the decision criterion of figure 10.4 to calculate the probabilities of committing type i and type ii errors when testing the null hypothesis that μ = 68 kilograms against the alternative that μ = 68 kilograms.
1941,1,"['efficient', 'statistic', 'random', 'random sample', 'estimator', 'sample', 'estimate', 'samples', 'population', 'standard', 'efficient estimator', 'standard deviation', 'limit', 'distribution', 'central limit theorem', 'sampling distribution', 'deviation', 'sampling', 'normal']", Testing a Statistical Hypothesis,seg_231,"assume the standard deviation of the population of weights to be σ = 3.6. for large samples, we may substitute s for σ if no other estimate of σ is available. our decision statistic, based on a random sample of size n = 36, will be x̄, the most efficient estimator of μ. from the central limit theorem, we know that the sampling distribution of x̄ is approximately normal with standard deviation σx̄ = σ/√n = 3.6/6 = 0.6."
1942,1,"['level of significance', 'distribution', 'tail', 'probability', 'level', 'significance', 'test', 'type i error', 'error']", Testing a Statistical Hypothesis,seg_231,"the probability of committing a type i error, or the level of significance of our test, is equal to the sum of the areas that have been shaded in each tail of the distribution in figure 10.5. therefore,"
1943,1,['critical region'], Testing a Statistical Hypothesis,seg_231,figure 10.5: critical region for testing μ = 68 versus μ = 68.
1944,1,"['sample size', 'sample', 'samples']", Testing a Statistical Hypothesis,seg_231,"thus, 9.5% of all samples of size 36 would lead us to reject μ = 68 kilograms when, in fact, it is true. to reduce α, we have a choice of increasing the sample size or widening the fail-to-reject region. suppose that we increase the sample size to n = 64. then σx̄ = 3.6/8 = 0.45. now"
1945,1,"['type ii error', 'sample', 'error', 'sample mean', 'symmetry', 'hypotheses', 'mean', 'probability', 'type ii', 'null hypothesis', 'alternative hypotheses', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"the reduction in α is not sufficient by itself to guarantee a good testing procedure. we must also evaluate β for various alternative hypotheses. if it is important to reject h0 when the true mean is some value μ ≥ 70 or μ ≤ 66, then the probability of committing a type ii error should be computed and examined for the alternatives μ = 66 and μ = 70. because of symmetry, it is only necessary to consider the probability of not rejecting the null hypothesis that μ = 68 when the alternative μ = 70 is true. a type ii error will result when the sample mean x̄ falls between 67 and 69 when h1 is true. therefore, referring to figure 10.6, we find that"
1946,1,"['type ii error', 'type ii', 'probability', 'error']", Testing a Statistical Hypothesis,seg_231,figure 10.6: probability of type ii error for testing μ = 68 versus μ = 70.
1947,0,['n'], Testing a Statistical Hypothesis,seg_231,"if the true value of μ is the alternative μ = 66, the value of β will again be 0.0132. for all possible values of μ < 66 or μ > 70, the value of β will be even smaller when n = 64, and consequently there would be little chance of not rejecting h0 when it is false."
1948,1,"['alternative hypothesis', 'type ii error', 'probability', 'type ii', 'error', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"the probability of committing a type ii error increases rapidly when the true value of μ approaches, but is not equal to, the hypothesized value. of course, this is usually the situation where we do not mind making a type ii error. for example, if the alternative hypothesis μ = 68.5 is true, we do not mind committing a type ii error by concluding that the true answer is μ = 68. the probability of making such an error will be high when n = 64. referring to figure 10.7, we have"
1949,0,[], Testing a Statistical Hypothesis,seg_231,the preceding examples illustrate the following important properties:
1950,1,"['type ii error', 'error', 'type ii']", Testing a Statistical Hypothesis,seg_231,figure 10.7: type ii error for testing μ = 68 versus μ = 68.5.
1951,1,"['type ii error', 'error', 'critical region', 'results', 'probability', 'type ii', 'test', 'type i error', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"important 1. the type i error and type ii error are related. a decrease in the probability properties of a of one generally results in an increase in the probability of the other. test of hypothesis 2. the size of the critical region, and therefore the probability of committing a type i error, can always be reduced by adjusting the critical value(s)."
1952,1,"['sample size', 'sample']", Testing a Statistical Hypothesis,seg_231,3. an increase in the sample size n will reduce α and β simultaneously.
1953,1,"['null hypothesis', 'parameter', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"4. if the null hypothesis is false, β is a maximum when the true value of a parameter approaches the hypothesized value. the greater the distance between the true value and the hypothesized value, the smaller β will be."
1954,1,"['power of a test', 'error', 'test', 'probabilities']", Testing a Statistical Hypothesis,seg_231,one very important concept that relates to error probabilities is the notion of the power of a test.
1955,1,"['power of a test', 'test', 'probability']", Testing a Statistical Hypothesis,seg_231,definition 10.4: the power of a test is the probability of rejecting h0 given that a specific alternative is true.
1956,1,"['sample size', 'sample', 'type ii error', 'power of the test', 'case', 'power of a test', 'power is a more succinct measure of how sensitive the test', 'mean', 'probability', 'sensitivity', 'type ii', 'tests', 'test', 'error']", Testing a Statistical Hypothesis,seg_231,"the power of a test can be computed as 1 − β. often different types of tests are compared by contrasting power properties. consider the previous illustration, in which we were testing h0 : μ = 68 and h1 : μ = 68. as before, suppose we are interested in assessing the sensitivity of the test. the test is governed by the rule that we do not reject h0 if 67 ≤ x̄ ≤ 69. we seek the capability of the test to properly reject h0 when indeed μ = 68.5. we have seen that the probability of a type ii error is given by β = 0.8661. thus, the power of the test is 1 − 0.8661 = 0.1339. in a sense, the power is a more succinct measure of how sensitive the test is for detecting differences between a mean of 68 and a mean of 68.5. in this case, if μ is truly 68.5, the test as described will properly reject h0 only 13.39% of the time. as a result, the test would not be a good one if it was important that the analyst have a reasonable chance of truly distinguishing between a mean of 68.0 (specified by h0) and a mean of 68.5. from the foregoing, it is clear that to produce a desirable power (say, greater than 0.8), one must either increase α or increase the sample size."
1957,1,"['hypothesis testing', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"so far in this chapter, much of the discussion of hypothesis testing has focused on foundations and definitions. in the sections that follow, we get more specific"
1958,1,"['tests of hypotheses', 'parameters', 'hypotheses', 'categories', 'tests', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,and put hypotheses in categories as well as discuss tests of hypotheses on various parameters of interest. we begin by drawing the distinction between a one-sided and a two-sided hypothesis.
1959,1,"['statistical hypothesis', 'test', 'statistical', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"a test of any statistical hypothesis where the alternative is one sided, such as"
1960,1,"['alternative hypothesis', 'experiment', 'inequality', 'test statistic', 'distribution', 'statistic', 'binomial', 'binomial distribution', 'tail', 'test', 'critical region', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"is called a one-tailed test. earlier in this section, we referred to the test statistic for a hypothesis. generally, the critical region for the alternative hypothesis θ > θ0 lies in the right tail of the distribution of the test statistic, while the critical region for the alternative hypothesis θ < θ0 lies entirely in the left tail. (in a sense, the inequality symbol points in the direction of the critical region.) a one-tailed test was used in the vaccine experiment to test the hypothesis p = 1/4 against the one-sided alternative p > 1/4 for the binomial distribution. the one-tailed critical region is usually obvious; the reader should visualize the behavior of the test statistic and notice the obvious signal that would produce evidence supporting the alternative hypothesis."
1961,1,"['statistical hypothesis', 'test', 'statistical', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"a test of any statistical hypothesis where the alternative is two sided, such as"
1962,1,"['alternative hypothesis', 'probabilities', 'states', 'null hypothesis', 'test statistic', 'distribution', 'statistic', 'population', 'continuous', 'tail', 'test', 'critical region', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"is called a two-tailed test, since the critical region is split into two parts, often having equal probabilities, in each tail of the distribution of the test statistic. the alternative hypothesis θ = θ0 states that either θ < θ0 or θ > θ0. a two-tailed test was used to test the null hypothesis that μ = 68 kilograms against the twosided alternative μ = 68 kilograms in the example of the continuous population of student weights."
1963,1,"['alternative hypothesis', 'error', 'case', 'complement', 'probability', 'parameter', 'tests', 'type i error', 'null hypothesis', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"the null hypothesis h0 will often be stated using the equality sign. with this approach, it is clear how the probability of type i error is controlled. however, there are situations in which “do not reject h0” implies that the parameter θ might be any value defined by the natural complement to the alternative hypothesis. for example, in the vaccine example, where the alternative hypothesis is h1: p > 1/4, it is quite possible that nonrejection of h0 cannot rule out a value of p less than 1/4. clearly though, in the case of one-tailed tests, the statement of the alternative is the most important consideration."
1964,1,"['alternative hypothesis', 'tests', 'location', 'distribution', 'sets', 'statistic', 'tails', 'tail', 'test', 'critical region', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"whether one sets up a one-tailed or a two-tailed test will depend on the conclusion to be drawn if h0 is rejected. the location of the critical region can be determined only after h1 has been stated. for example, in testing a new drug, one sets up the hypothesis that it is no better than similar drugs now on the market and tests this against the alternative hypothesis that the new drug is superior. such an alternative hypothesis will result in a one-tailed test with the critical region in the right tail. however, if we wish to compare a new teaching technique with the conventional classroom procedure, the alternative hypothesis should allow for the new approach to be either inferior or superior to the conventional procedure. hence, the test is two-tailed with the critical region divided equally so as to fall in the extreme left and right tails of the distribution of our statistic."
1965,1,"['null and alternative hypotheses', 'hypotheses', 'critical region', 'alternative hypotheses', 'average']", Testing a Statistical Hypothesis,seg_231,example 10.1: a manufacturer of a certain brand of rice cereal claims that the average saturated fat content does not exceed 1.5 grams per serving. state the null and alternative hypotheses to be used in testing this claim and determine where the critical region is located.
1966,1,['test'], Testing a Statistical Hypothesis,seg_231,solution : the manufacturer’s claim should be rejected only if μ is greater than 1.5 milligrams and should not be rejected if μ is less than or equal to 1.5 milligrams. we test
1967,1,"['test statistic', 'distribution', 'statistic', 'tail', 'test', 'critical region']", Testing a Statistical Hypothesis,seg_231,"nonrejection of h0 does not rule out values less than 1.5 milligrams. since we have a one-tailed test, the greater than symbol indicates that the critical region lies entirely in the right tail of the distribution of our test statistic x̄."
1968,1,"['sample', 'test statistic', 'location', 'null and alternative hypotheses', 'statistic', 'hypotheses', 'test', 'critical region', 'alternative hypotheses']", Testing a Statistical Hypothesis,seg_231,"example 10.2: a real estate agent claims that 60% of all private residences being built today are 3-bedroom homes. to test this claim, a large sample of new residences is inspected; the proportion of these homes with 3 bedrooms is recorded and used as the test statistic. state the null and alternative hypotheses to be used in this test and determine the location of the critical region."
1969,1,"['test statistic', 'statistic', 'test', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"solution : if the test statistic were substantially higher or lower than p = 0.6, we would reject the agent’s claim. hence, we should make the hypothesis"
1970,1,"['alternative hypothesis', 'test statistic', 'tails', 'distribution', 'statistic', 'test', 'critical region', 'hypothesis']", Testing a Statistical Hypothesis,seg_231,"the alternative hypothesis implies a two-tailed test with the critical region divided equally in both tails of the distribution of p̂ , our test statistic."
1971,1,"['critical value', 'test statistic', 'discrete', 'statistic', 'hypotheses', 'test', 'critical region']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"in testing hypotheses in which the test statistic is discrete, the critical region may be chosen arbitrarily and its size determined. if α is too large, it can be reduced by making an adjustment in the critical value. it may be necessary to increase the"
1972,1,"['test', 'power of the test']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,sample size to offset the decrease that occurs automatically in the power of the test.
1973,1,"['standard normal', 'test statistic', 'data', 'distribution', 'statistical', 'statistic', 'normal', 'set', 'standard', 'level', 'standard normal distribution', 'significance', 'test', 'level of significance', 'critical region', 'normal distribution']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"over a number of generations of statistical analysis, it had become customary to choose an α of 0.05 or 0.01 and select the critical region accordingly. then, of course, strict rejection or nonrejection of h0 would depend on that critical region. for example, if the test is two tailed and α is set at the 0.05 level of significance and the test statistic involves, say, the standard normal distribution, then a z-value is observed from the data and the critical region is"
1974,1,"['table', 'test statistic', 'statistic', 'test', 'critical region', 'hypothesis']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"where the value 1.96 is found as z0.025 in table a.3. a value of z in the critical region prompts the statement “the value of the test statistic is significant,” which we can then translate into the user’s language. for example, if the hypothesis is given by"
1975,1,['mean'], The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"one might say, “the mean differs significantly from the value 10.”"
1976,1,"['risk', 'error', 'statistics', 'case', 'test statistics', 'type i error', 'level', 'significance', 'test', 'significance level', 'critical region']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"this preselection of a significance level α has its roots in the philosophy that the maximum risk of making a type i error should be controlled. however, this approach does not account for values of test statistics that are “close” to the critical region. suppose, for example, in the illustration with h0 : μ = 10 versus h1: μ = 10, a value of z = 1.87 is observed; strictly speaking, with α = 0.05, the value is not significant. but the risk of committing a type i error if one rejects h0 in this case could hardly be considered severe. in fact, in a two-tailed scenario, one can quantify this risk as"
1977,1,"['statistics', 'information', 'probability', 'level', 'critical region']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"as a result, 0.0614 is the probability of obtaining a value of z as large as or larger (in magnitude) than 1.87 when in fact μ = 10. although this evidence against h0 is not as strong as that which would result from rejection at an α = 0.05 level, it is important information to the user. indeed, continued use of α = 0.05 or 0.01 is only a result of what standards have been passed down through the generations. the p-value approach has been adopted extensively by users of applied statistics. the approach is designed to give the user an alternative (in terms of a probability) to a mere “reject” or “do not reject” conclusion. the p -value computation also gives the user important information when the z-value falls well into the ordinary critical region. for example, if z is 2.73, it is informative for the user to observe that"
1978,1,"['level', 'condition', 'event', 'experiments']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"and thus the z-value is significant at a level considerably less than 0.05. it is important to know that under the condition of h0, a value of z = 2.73 is an extremely rare event. that is, a value at least that large in magnitude would only occur 64 times in 10,000 experiments."
1979,1,"['sample', 'population variance', 'samples', 'mean', 'hypothesis', 'percent', 'population', 'variance', 'distributions']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"one very simple way of explaining a p -value graphically is to consider two distinct samples. suppose that two materials are being considered for coating a particular type of metal in order to inhibit corrosion. specimens are obtained, and one collection is coated with material 1 and one collection coated with material 2. the sample sizes are n1 = n2 = 10, and corrosion is measured in percent of surface area affected. the hypothesis is that the samples came from common distributions with mean μ = 10. let us assume that the population variance is 1.0. then we are testing"
1980,1,"['plot', 'data', 'distribution', 'samples', 'probability', 'population', 'null hypothesis', 'hypothesis']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"let figure 10.8 represent a point plot of the data; the data are placed on the distribution stated by the null hypothesis. let us assume that the “×” data refer to material 1 and the “◦” data refer to material 2. now it seems clear that the data do refute the null hypothesis. but how can this be summarized in one number? the p-value can be viewed as simply the probability of obtaining these data given that both samples come from the same distribution. clearly, this probability is quite small, say 0.00000001! thus, the small p -value clearly refutes h0, and the conclusion is that the population means are significantly different."
1981,1,"['data', 'populations']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,figure 10.8: data that are likely generated from populations having two different means.
1982,1,"['test statistic', 'statistic', 'test']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"use of the p -value approach as an aid in decision-making is quite natural, and nearly all computer packages that provide hypothesis-testing computation print out p -values along with values of the appropriate test statistic. the following is a formal definition of a p -value."
1983,1,"['test statistic', 'statistic', 'level', 'significance', 'test']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,definition 10.5: a p -value is the lowest level (of significance) at which the observed value of the test statistic is significant.
1984,1,['associated'], The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"it is tempting at this point to summarize the procedures associated with testing, say, h0 : θ = θ0. however, the student who is a novice in this area should understand that there are differences in approach and philosophy between the classic"
1985,1,"['tests', 'tests of hypotheses', 'hypotheses']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,334 chapter 10 oneand two-sample tests of hypotheses
1986,1,['subjective'], The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"fixed α approach that is climaxed with either a “reject h0” or a “do not reject h0” conclusion and the p -value approach. in the latter, no fixed α is determined and conclusions are drawn on the basis of the size of the p -value in harmony with the subjective judgment of the engineer or scientist. while modern computer software will output p -values, nevertheless it is important that readers understand both approaches in order to appreciate the totality of the concepts. thus, we offer a brief list of procedural steps for both the classical and the p -value approach."
1987,1,"['error', 'test statistic', 'null and alternative hypotheses', 'significance level', 'statistic', 'hypotheses', 'probability', 'level', 'significance', 'test', 'type i error', 'critical region', 'alternative hypotheses', 'hypothesis']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"approach to 1. state the null and alternative hypotheses. hypothesis 2. choose a fixed significance level α. testing with 3. choose an appropriate test statistic and establish the critical region based fixed probability on α. of type i error 4. reject h0 if the computed test statistic is in the critical region. otherwise, do not reject. 5. draw scientific or engineering conclusions."
1988,1,"['test statistic', 'null and alternative hypotheses', 'statistic', 'hypotheses', 'test', 'alternative hypotheses']", The Use of P Values for Decision Making in Testing Hypotheses,seg_233,significance 1. state null and alternative hypotheses. testing (p -value 2. choose an appropriate test statistic. approach) 3. compute the p -value based on the computed value of the test statistic. 4. use judgment based on the p -value and knowledge of the scientific system.
1989,0,[], The Use of P Values for Decision Making in Testing Hypotheses,seg_233,"in later sections of this chapter and chapters that follow, many examples and exercises emphasize the p -value approach to drawing scientific conclusions."
1990,1,"['tests of hypotheses', 'population mean', 'hypotheses', 'mean', 'population', 'tests']", Single Sample Tests Concerning a Single Mean,seg_237,"in this section, we formally consider tests of hypotheses on a single population mean. many of the illustrations from previous sections involved tests on the mean, so the reader should already have insight into some of the details that are outlined here."
1991,1,"['sample', 'model', 'experiment', 'random sample', 'distribution', 'mean', 'random', 'variance', 'hypothesis']", Single Sample Tests Concerning a Single Mean,seg_237,"we should first describe the assumptions on which the experiment is based. the model for the underlying situation centers around an experiment with x1, x2, . . . , xn representing a random sample from a distribution with mean μ and variance σ2 > 0. consider first the hypothesis"
1992,1,"['sample', 'states', 'test statistic', 'distribution', 'random variable', 'variable', 'normal', 'statistic', 'mean', 'random', 'central limit theorem', 'test', 'variance', 'limit', 'normal distribution']", Single Sample Tests Concerning a Single Mean,seg_237,"the appropriate test statistic should be based on the random variable x̄. in chapter 8, the central limit theorem was introduced, which essentially states that despite the distribution of x, the random variable x̄ has approximately a normal distribution with mean μ and variance σ2/n for reasonably large sample sizes. so, μx̄ = μ and σx̄"
1993,1,"['sample', 'sample average', 'test', 'critical region', 'average']", Single Sample Tests Concerning a Single Mean,seg_237,"2 = σ2/n. we can then determine a critical region based on the computed sample average, x̄. it should be clear to the reader by now that there will be a two-tailed critical region for the test."
1994,1,"['standard', 'standard normal', 'random variable', 'variable', 'normal', 'standard normal random variable', 'random', 'normal random variable']", Single Sample Tests Concerning a Single Mean,seg_237,"it is convenient to standardize x̄ and formally involve the standard normal random variable z, where"
1995,1,['distribution'], Single Sample Tests Concerning a Single Mean,seg_237,"we know that under h0, that is, if μ = μ0, √n(x̄ − μ0)/σ follows an n(x; 0, 1) distribution, and hence the expression"
1996,1,"['error', 'test statistic', 'statistic', 'probability', 'control', 'test', 'type i error', 'critical region']", Single Sample Tests Concerning a Single Mean,seg_237,"can be used to write an appropriate nonrejection region. the reader should keep in mind that, formally, the critical region is designed to control α, the probability of type i error. it should be obvious that a two-tailed signal of evidence is needed to support h1. thus, given a computed value x̄, the formal test involves rejecting h0 if the computed test statistic z falls in the critical region described next."
1997,1,"['alternative hypothesis', 'mean', 'probability', 'variance', 'critical region', 'hypothesis']", Single Sample Tests Concerning a Single Mean,seg_237,"test procedure x̄− μ0 x̄− μ0 for a single mean z = > zα/2 or z = < −zα/2 σ/√n σ/√n (variance known) if −zα/2 < z < zα/2, do not reject h0. rejection of h0, of course, implies acceptance of the alternative hypothesis μ = μ0. with this definition of the critical region, it should be clear that there will be probability α of rejecting h0 (falling into the critical region) when, indeed, μ = μ0."
1998,1,"['average', 'critical region']", Single Sample Tests Concerning a Single Mean,seg_237,"although it is easier to understand the critical region written in terms of z, we can write the same critical region in terms of the computed average x̄. the following can be written as an identical decision procedure:"
1999,1,"['critical values', 'significance level', 'random variable', 'variable', 'random', 'level', 'significance']", Single Sample Tests Concerning a Single Mean,seg_237,"hence, for a significance level α, the critical values of the random variable z and x̄ are both depicted in figure 10.9."
2000,1,"['alternative hypothesis', 'critical region', 'hypothesis']", Single Sample Tests Concerning a Single Mean,seg_237,figure 10.9: critical region for the alternative hypothesis μ = μ0.
2001,1,"['standard normal', 'case', 'distribution', 'statistic', 'normal', 'hypotheses', 'mean', 'standard', 'standard normal distribution', 'tail', 'test', 'critical region', 'normal distribution']", Single Sample Tests Concerning a Single Mean,seg_237,"tests of one-sided hypotheses on the mean involve the same statistic described in the two-sided case. the difference, of course, is that the critical region is only in one tail of the standard normal distribution. for example, suppose that we seek to test"
2002,1,"['null hypothesis', 'results', 'case', 'tail', 'critical region', 'hypothesis']", Single Sample Tests Concerning a Single Mean,seg_237,"the signal that favorsh1 comes from large values of z. thus, rejection ofh0 results when the computed z > zα. obviously, if the alternative is h1: μ < μ0, the critical region is entirely in the lower tail and thus rejection results from z < −zα. although in a one-sided testing case the null hypothesis can be written as h0 : μ ≤ μ0 or h0: μ ≥ μ0, it is usually written as h0: μ = μ0."
2003,1,"['tests', 'case']", Single Sample Tests Concerning a Single Mean,seg_237,the following two examples illustrate tests on means for the case in which σ is known.
2004,1,"['deviation', 'sample', 'random', 'population standard deviation', 'states', 'random sample', 'mean', 'population', 'standard', 'level', 'standard deviation', 'significance', 'average', 'level of significance']", Single Sample Tests Concerning a Single Mean,seg_237,"example 10.3: a random sample of 100 recorded deaths in the united states during the past year showed an average life span of 71.8 years. assuming a population standard deviation of 8.9 years, does this seem to indicate that the mean life span today is greater than 70 years? use a 0.05 level of significance."
2005,1,['critical region'], Single Sample Tests Concerning a Single Mean,seg_237,"4. critical region: z > 1.645, where z = σ"
2006,1,['mean'], Single Sample Tests Concerning a Single Mean,seg_237,6. decision: reject h0 and conclude that the mean life span today is greater
2007,0,[], Single Sample Tests Concerning a Single Mean,seg_237,than 70 years.
2008,0,[], Single Sample Tests Concerning a Single Mean,seg_237,the p -value corresponding to z = 2.02 is given by the area of the shaded region in figure 10.10.
2009,1,['table'], Single Sample Tests Concerning a Single Mean,seg_237,"using table a.3, we have"
2010,1,"['level', 'significance', 'level of significance']", Single Sample Tests Concerning a Single Mean,seg_237,"as a result, the evidence in favor of h1 is even stronger than that suggested by a 0.05 level of significance."
2011,1,"['deviation', 'sample', 'standard', 'random sample', 'mean', 'random', 'level', 'standard deviation', 'significance', 'test', 'level of significance', 'hypothesis']", Single Sample Tests Concerning a Single Mean,seg_237,example 10.4: a manufacturer of sports equipment has developed a new synthetic fishing line that the company claims has a mean breaking strength of 8 kilograms with a standard deviation of 0.5 kilogram. test the hypothesis that μ = 8 kilograms against the alternative that μ = 8 kilograms if a random sample of 50 lines is tested and found to have a mean breaking strength of 7.8 kilograms. use a 0.01 level of significance.
2012,1,['critical region'], Single Sample Tests Concerning a Single Mean,seg_237,"4. critical region: z < −2.575 and z > 2.575, where z = σ"
2013,1,['average'], Single Sample Tests Concerning a Single Mean,seg_237,6. decision: reject h0 and conclude that the average breaking strength is not
2014,0,[], Single Sample Tests Concerning a Single Mean,seg_237,"equal to 8 but is, in fact, less than 8 kilograms."
2015,1,"['test', 'table']", Single Sample Tests Concerning a Single Mean,seg_237,"since the test in this example is two tailed, the desired p -value is twice the area of the shaded region in figure 10.11 to the left of z = −2.83. therefore, using table a.3, we have"
2016,1,"['level', 'significance', 'level of significance', 'null hypothesis', 'hypothesis']", Single Sample Tests Concerning a Single Mean,seg_237,which allows us to reject the null hypothesis that μ = 8 kilograms at a level of significance smaller than 0.01.
2017,1,"['interval', 'case', 'random', 'hypothesis testing', 'random variable', 'mean', 'parameter', 'population', 'confidence', 'statistical', 'confidence interval', 'statistical inference', 'estimation', 'hypothesis', 'population mean', 'variable', 'interval estimation']", Single Sample Tests Concerning a Single Mean,seg_237,"the reader should realize by now that the hypothesis-testing approach to statistical inference in this chapter is very closely related to the confidence interval approach in chapter 9. confidence interval estimation involves computation of bounds within which it is “reasonable” for the parameter in question to lie. for the case of a single population mean μ with σ2 known, the structure of both hypothesis testing and confidence interval estimation is based on the random variable"
2018,1,"['interval', 'failure', 'confidence', 'level', 'significance', 'significance level', 'confidence interval', 'hypothesis']", Single Sample Tests Concerning a Single Mean,seg_237,"it turns out that the testing of h0: μ = μ0 against h1: μ = μ0 at a significance level α is equivalent to computing a 100(1− α)% confidence interval on μ and rejecting h0 if μ0 is outside the confidence interval. if μ0 is inside the confidence interval, the hypothesis is not rejected. the equivalence is very intuitive and quite simple to illustrate. recall that with an observed value x̄, failure to reject h0 at significance level α implies that"
2019,0,[], Single Sample Tests Concerning a Single Mean,seg_237,which is equivalent to
2020,1,"['interval', 'statistics', 'sample', 'information', 'hypothesis testing', 'mean', 'confidence', 'statistical', 'confidence interval', 'statistical inference', 'estimation', 'hypothesis', 'interval estimation', 'variances']", Single Sample Tests Concerning a Single Mean,seg_237,"the equivalence of confidence interval estimation to hypothesis testing extends to differences between two means, variances, ratios of variances, and so on. as a result, the student of statistics should not consider confidence interval estimation and hypothesis testing as separate forms of statistical inference. for example, consider example 9.2 on page 271. the 95% confidence interval on the mean is given by the bounds (2.50, 2.70). thus, with the same sample information, a twosided hypothesis on μ involving any hypothesized value between 2.50 and 2.70 will not be rejected. as we turn to different areas of hypothesis testing, the equivalence to the confidence interval estimation will continue to be exploited."
2021,1,"['degrees of freedom', 'confidence intervals', 'interval', 'case', 'statistic', 'random', 'random sample', 'standard normal distribution', 'sample', 'estimate', 'intervals', 'hypothesis testing', 'random variable', 'mean', 'standard', 'population', 'confidence', 'tests', 'confidence interval', 'estimation', 'standard normal', 'test statistic', 'distribution', 'test', 'hypothesis', 'random variables', 'variables', 'population mean', 'variable', 'normal', 'interval estimation', 'normal distribution']", Single Sample Tests Concerning a Single Mean,seg_237,"one would certainly suspect that tests on a population mean μ with σ2 unknown, like confidence interval estimation, should involve the use of student t-distribution. strictly speaking, the application of student t for both confidence intervals and hypothesis testing is developed under the following assumptions. the random variables x1, x2, . . . , xn represent a random sample from a normal distribution with unknown μ and σ2. then the random variable √n(x̄ − μ)/s has a student t-distribution with n−1 degrees of freedom. the structure of the test is identical to that for the case of σ known, with the exception that the value σ in the test statistic is replaced by the computed estimate s and the standard normal distribution is replaced by a t-distribution."
2022,1,"['test', 'mean', 'hypothesis']", Single Sample Tests Concerning a Single Mean,seg_237,"the t-statistic for the two-sided hypothesis for a test on a single mean h0: μ = μ0,"
2023,1,"['level', 'significance', 'variance', 'significance level']", Single Sample Tests Concerning a Single Mean,seg_237,"(variance h1: μ = μ0, unknown) we reject h0 at significance level α when the computed t-statistic"
2024,1,"['symmetric', 'results', 'case', 'level', 'significance', 'significance level', 'critical region', 'hypothesis']", Single Sample Tests Concerning a Single Mean,seg_237,"the reader should recall from chapters 8 and 9 that the t-distribution is symmetric around the value zero. thus, this two-tailed critical region applies in a fashion similar to that for the case of known σ. for the two-sided hypothesis at significance level α, the two-tailed critical regions apply. for h1: μ > μ0, rejection results when t > tα,n−1. for h1: μ < μ0, the critical region is given by t < −tα,n−1."
2025,1,"['deviation', 'sample', 'random', 'random sample', 'normal', 'population', 'standard', 'level', 'standard deviation', 'significance', 'average', 'level of significance']", Single Sample Tests Concerning a Single Mean,seg_237,"example 10.5: the edison electric institute has published figures on the number of kilowatt hours used annually by various home appliances. it is claimed that a vacuum cleaner uses an average of 46 kilowatt hours per year. if a random sample of 12 homes included in a planned study indicates that vacuum cleaners use an average of 42 kilowatt hours per year with a standard deviation of 11.9 kilowatt hours, does this suggest at the 0.05 level of significance that vacuum cleaners use, on average, less than 46 kilowatt hours annually? assume the population of kilowatt hours to be normal."
2026,1,"['degrees of freedom', 'critical region']", Single Sample Tests Concerning a Single Mean,seg_237,"4. critical region: t < −1.796, where t = x̄−μ0 with 11 degrees of freedom."
2027,1,['average'], Single Sample Tests Concerning a Single Mean,seg_237,6. decision: do not reject h0 and conclude that the average number of kilowatt
2028,0,[], Single Sample Tests Concerning a Single Mean,seg_237,hours used annually by home vacuum cleaners is not significantly less than 46.
2029,1,"['sample', 'interval', 'population mean', 'information', 'mean', 'population', 'confidence', 'confidence interval', 'hypothesis']", Single Sample Tests Concerning a Single Mean,seg_237,"the reader has probably noticed that the equivalence of the two-tailed t-test for a single mean and the computation of a confidence interval on μ with σ replaced by s is maintained. for example, consider example 9.5 on page 275. essentially, we can view that computation as one in which we have found all values of μ0, the hypothesized mean volume of containers of sulfuric acid, for which the hypothesis h0: μ = μ0 will not be rejected at α = 0.05. again, this is consistent with the statement “based on the sample information, values of the population mean volume between 9.74 and 10.26 liters are not unreasonable.”"
2030,1,"['confidence intervals', 'interval', 'statistics', 'statistic', 'random', 'standard normal distribution', 'sample', 'estimate', 'intervals', 'random variable', 'standard', 'population', 'normal random variable', 'confidence', 'tests', 'limit', 'confidence interval', 'standard normal', 'test statistic', 'distribution', 'standard normal random variable', 'central limit theorem', 'test', 'percentage', 'normality', 'variable', 'normal', 'experiments', 'normal distribution']", Single Sample Tests Concerning a Single Mean,seg_237,"comments regarding the normality assumption are worth emphasizing at this point. we have indicated that when σ is known, the central limit theorem allows for the use of a test statistic or a confidence interval which is based on z, the standard normal random variable. strictly speaking, of course, the central limit theorem, and thus the use of the standard normal distribution, does not apply unless σ is known. in chapter 8, the development of the t-distribution was given. there we pointed out that normality on x1, x2, . . . , xn was an underlying assumption. thus, strictly speaking, the student’s t-tables of percentage points for tests or confidence intervals should not be used unless it is known that the sample comes from a normal population. in practice, σ can rarely be assumed to be known. however, a very good estimate may be available from previous experiments. many statistics textbooks suggest that one can safely replace σ by s in the test statistic"
2031,1,"['probability plots', 'interval', 'plots', 'set', 'probability', 'sample', 'estimate', 'results', 'data', 'population', 'confidence', 'limit', 'confidence interval', 'critical region', 'distribution', 'data set', 'central limit theorem', 'normal probability plots', 'deviation', 'percentage', 'normality', 'normal', 'critical value', 'normal distribution']", Single Sample Tests Concerning a Single Mean,seg_237,"when n ≥ 30 with a bell-shaped population and still use the z-tables for the appropriate critical region. the implication here is that the central limit theorem is indeed being invoked and one is relying on the fact that s ≈ σ. obviously, when this is done, the results must be viewed as approximate. thus, a computed p - value (from the z-distribution) of 0.15 may be 0.12 or perhaps 0.17, or a computed confidence interval may be a 93% confidence interval rather than a 95% interval as desired. now what about situations where n ≤ 30? the user cannot rely on s being close to σ, and in order to take into account the inaccuracy of the estimate, the confidence interval should be wider or the critical value larger in magnitude. the t-distribution percentage points accomplish this but are correct only when the sample is from a normal distribution. of course, normal probability plots can be used to ascertain some sense of the deviation of normality in a data set."
2032,1,"['confidence intervals', 'random', 'results', 'intervals', 'samples', 'tests', 'confidence', 'distributions', 'distribution', 'random variables', 'variables', 'deviations', 'normal']", Single Sample Tests Concerning a Single Mean,seg_237,"for small samples, it is often difficult to detect deviations from a normal distribution. (goodness-of-fit tests are discussed in a later section of this chapter.) for bell-shaped distributions of the random variables x1, x2, . . . , xn, the use of the t-distribution for tests or confidence intervals is likely to produce quite good results. when in doubt, the user should resort to nonparametric procedures, which are presented in chapter 16."
2033,1,"['sample', 'bias', 'data', 'measurements']", Single Sample Tests Concerning a Single Mean,seg_237,it should be of interest for the reader to see an annotated computer printout showing the result of a single-sample t-test. suppose that an engineer is interested in testing the bias in a ph meter. data are collected on a neutral substance (ph = 7.0). a sample of the measurements were taken with the data as follows:
2034,1,['test'], Single Sample Tests Concerning a Single Mean,seg_237,"it is, then, of interest to test"
2035,1,"['deviation', 'sample', 'estimated', 'standard error', 'sample standard deviation', 'data', 'set', 'data set', 'mean', 'standard', 'standard deviation', 'error']", Single Sample Tests Concerning a Single Mean,seg_237,"in this illustration, we use the computer package minitab to illustrate the analysis of the data set above. notice the key components of the printout shown in figure 10.12. of course, the mean ȳ is 7.0250, stdev is simply the sample standard deviation s = 0.044, and se mean is the estimated standard error of the mean and is computed as s/√n = 0.0139. the t-value is the ratio"
2036,1,['test'], Single Sample Tests Concerning a Single Mean,seg_237,ph-meter 7.07 7.00 7.10 6.97 7.00 7.03 7.01 7.01 6.98 7.08 mtb > onet ’ph-meter’; subc> test 7.
2037,1,"['variable', 'test', 'mean']", Single Sample Tests Concerning a Single Mean,seg_237,"one-sample t: ph-meter test of mu = 7 vs not = 7 variable n mean stdev se mean 95% ci t p ph-meter 10 7.02500 0.04403 0.01392 (6.99350, 7.05650) 1.80 0.106"
2038,1,['sample'], Single Sample Tests Concerning a Single Mean,seg_237,figure 10.12: minitab printout for one sample t-test for ph meter.
2039,1,"['sample size', 'sample', 'experiment', 'results', 'unbiased']", Single Sample Tests Concerning a Single Mean,seg_237,"the p -value of 0.106 suggests results that are inconclusive. there is no evidence suggesting a strong rejection of h0 (based on an α of 0.05 or 0.10), yet one certainly cannot truly conclude that the ph meter is unbiased. notice that the sample size of 10 is rather small. an increase in sample size (perhaps another experiment) may sort things out. a discussion regarding appropriate sample size appears in section 10.6."
2040,1,"['tests', 'experimental', 'independent', 'interval', 'random samples', 'confidence intervals', 'intervals', 'samples', 'set', 'random', 'confidence', 'confidence interval']", Two Samples Tests on Two Means,seg_239,"the reader should now understand the relationship between tests and confidence intervals, and can only heavily rely on details supplied by the confidence interval material in chapter 9. tests concerning two means represent a set of very important analytical tools for the scientist or engineer. the experimental setting is very much like that described in section 9.8. two independent random samples of sizes"
2041,1,"['populations', 'random variable', 'variable', 'random', 'variances']", Two Samples Tests on Two Means,seg_239,"n1 and n2, respectively, are drawn from two populations with means μ1 and μ2 and variances σ12 and σ22. we know that the random variable"
2042,1,"['standard normal', 'populations', 'distribution', 'statistic', 'normal', 'central limit theorem', 'standard', 'standard normal distribution', 'limit', 'normal distribution']", Two Samples Tests on Two Means,seg_239,"has a standard normal distribution. here we are assuming that n1 and n2 are sufficiently large that the central limit theorem applies. of course, if the two populations are normal, the statistic above has a standard normal distribution even for small n1 and n2. obviously, if we can assume that σ1 = σ2 = σ, the statistic above reduces to"
2043,1,"['tests', 'confidence intervals', 'statistics', 'intervals', 'mean', 'confidence', 'test']", Two Samples Tests on Two Means,seg_239,"the two statistics above serve as a basis for the development of the test procedures involving two means. the equivalence between tests and confidence intervals, along with the technical detail involving tests on one mean, allow a simple transition to tests on two means."
2044,1,['hypothesis'], Two Samples Tests on Two Means,seg_239,the two-sided hypothesis on two means can be written generally as
2045,1,"['distribution', 'test statistic', 'statistic', 'test']", Two Samples Tests on Two Means,seg_239,"obviously, the alternative can be two sided or one sided. again, the distribution used is the distribution of the test statistic under h0. values x̄1 and x̄2 are computed and, for σ1 and σ2 known, the test statistic is given by"
2046,1,"['test statistic', 'case', 'statistic', 'test', 'critical region']", Two Samples Tests on Two Means,seg_239,"with a two-tailed critical region in the case of a two-sided alternative. that is, reject h0 in favor of h1: μ1 −μ2 = d0 if z > zα/2 or z < −zα/2. one-tailed critical regions are used in the case of the one-sided alternatives. the reader should, as before, study the test statistic and be satisfied that for, say, h1: μ1 − μ2 > d0, the signal favoring h1 comes from large values of z. thus, the upper-tailed critical region applies."
2047,1,"['tests', 'variances', 'test statistic', 'statistic', 'normal', 'test', 'distributions']", Two Samples Tests on Two Means,seg_239,"the more prevalent situations involving tests on two means are those in which variances are unknown. if the scientist involved is willing to assume that both distributions are normal and that σ1 = σ2 = σ, the pooled t-test (often called the two-sample t-test) may be used. the test statistic (see section 9.8) is given by the following test procedure."
2048,1,['hypothesis'], Two Samples Tests on Two Means,seg_239,two-sample for the two-sided hypothesis
2049,0,[], Two Samples Tests on Two Means,seg_239,"pooled t-test h0: μ1 = μ2,"
2050,1,"['level', 'significance', 'significance level']", Two Samples Tests on Two Means,seg_239,we reject h0 at significance level α when the computed t-statistic
2051,1,"['degrees of freedom', 'estimate', 'information', 'samples']", Two Samples Tests on Two Means,seg_239,"recall from chapter 9 that the degrees of freedom for the t-distribution are a result of pooling of information from the two samples to estimate σ2. one-sided alternatives suggest one-sided critical regions, as one might expect. for example, for h1: μ1 − μ2 > d0, reject h1: μ1 − μ2 = d0 when t > tα,n1+n2−2."
2052,1,"['case', 'significance', 'sample', 'experiment', 'samples', 'populations', 'standard', 'standard deviation', 'level', 'level of significance', 'deviation', 'sample standard deviation', 'normal', 'average', 'measuring', 'variances']", Two Samples Tests on Two Means,seg_239,"example 10.6: an experiment was performed to compare the abrasive wear of two different laminated materials. twelve pieces of material 1 were tested by exposing each piece to a machine measuring wear. ten pieces of material 2 were similarly tested. in each case, the depth of wear was observed. the samples of material 1 gave an average (coded) wear of 85 units with a sample standard deviation of 4, while the samples of material 2 gave an average of 81 with a sample standard deviation of 5. can we conclude at the 0.05 level of significance that the abrasive wear of material 1 exceeds that of material 2 by more than 2 units? assume the populations to be approximately normal with equal variances."
2053,1,['population'], Two Samples Tests on Two Means,seg_239,"solution : let μ1 and μ2 represent the population means of the abrasive wear for material 1 and material 2, respectively."
2054,1,['critical region'], Two Samples Tests on Two Means,seg_239,"4. critical region: t > 1.725, where t = (x̄1−x̄2)−d0 with v = 20 degrees of"
2055,0,[], Two Samples Tests on Two Means,seg_239,5. computations:
2056,1,['table'], Two Samples Tests on Two Means,seg_239,p = p (t > 1.04) ≈ 0.16. (see table a.4.)
2057,0,[], Two Samples Tests on Two Means,seg_239,6. decision: do not reject h0. we are unable to conclude that the abrasive wear
2058,0,[], Two Samples Tests on Two Means,seg_239,of material 1 exceeds that of material 2 by more than 2 units.
2059,1,"['statistic', 'normal', 'populations']", Two Samples Tests on Two Means,seg_239,"there are situations where the analyst is not able to assume that σ1 = σ2. recall from section 9.8 that, if the populations are normal, the statistic"
2060,1,['degrees of freedom'], Two Samples Tests on Two Means,seg_239,has an approximate t-distribution with approximate degrees of freedom
2061,1,['test'], Two Samples Tests on Two Means,seg_239,"as a result, the test procedure is to not reject h0 when"
2062,1,['case'], Two Samples Tests on Two Means,seg_239,"with v given as above. again, as in the case of the pooled t-test, one-sided alternatives suggest one-sided critical regions."
2063,1,"['interval', 'design', 'random', 'results', 'biased', 'populations', 'confidence', 'confidence interval', 'experimental', 'experimental units']", Two Samples Tests on Two Means,seg_239,"a study of the two-sample t-test or confidence interval on the difference between means should suggest the need for experimental design. recall the discussion of experimental units in chapter 9, where it was suggested that the conditions of the two populations (often referred to as the two treatments) should be assigned randomly to the experimental units. this is done to avoid biased results due to systematic differences between experimental units. in other words, in hypothesistesting jargon, it is important that any significant difference found between means be due to the different conditions of the populations and not due to the experimental units in the study. for example, consider exercise 9.40 in section 9.9. the 20 seedlings play the role of the experimental units. ten of them are to be treated with nitrogen and 10 with no nitrogen. it may be very important that this assignment to the “nitrogen” and “no-nitrogen” treatments be random to ensure that systematic differences between the seedlings do not interfere with a valid comparison between the means."
2064,1,"['measurement', 'random']", Two Samples Tests on Two Means,seg_239,"in example 10.6, time of measurement is the most likely choice for the experimental unit. the 22 pieces of material should be measured in random order. we"
2065,1,"['experimental', 'results', 'measurements', 'random', 'experimental units']", Two Samples Tests on Two Means,seg_239,"need to guard against the possibility that wear measurements made close together in time might tend to give similar results. systematic (nonrandom) differences in experimental units are not expected. however, random assignments guard against the problem."
2066,1,"['sample size', 'sample', 'randomization', 'data', 'experiments']", Two Samples Tests on Two Means,seg_239,"references to planning of experiments, randomization, choice of sample size, and so on, will continue to influence much of the development in chapters 13, 14, and 15. any scientist or engineer whose interest lies in analysis of real data should study this material. the pooled t-test is extended in chapter 13 to cover more than two means."
2067,1,"['interval', 'observations', 'populations', 'data', 'paired observations', 'homogeneous', 'random variable', 'variable', 'random', 'confidence', 'confidence interval', 'paired']", Two Samples Tests on Two Means,seg_239,"testing of two means can be accomplished when data are in the form of paired observations, as discussed in chapter 9. in this pairing structure, the conditions of the two populations (treatments) are assigned randomly within homogeneous units. computation of the confidence interval for μ1 − μ2 in the situation with paired observations is based on the random variable"
2068,1,"['observations', 'case', 'random', 'sample', 'sample mean', 'mean', 'standard', 'standard deviation', 'experimental', 'experimental units', 'hypothesis', 'deviation', 'random variables', 'variables', 'normal']", Two Samples Tests on Two Means,seg_239,"where d̄ and sd are random variables representing the sample mean and standard deviation of the differences of the observations in the experimental units. as in the case of the pooled t-test, the assumption is that the observations from each population are normal. this two-sample problem is essentially reduced to a one-sample problem by using the computed differences d1, d2, . . . , dn. thus, the hypothesis reduces to"
2069,1,"['test statistic', 'statistic', 'test']", Two Samples Tests on Two Means,seg_239,the computed test statistic is then given by
2070,0,[], Two Samples Tests on Two Means,seg_239,critical regions are constructed using the t-distribution with n− 1 degrees of freedom.
2071,1,"['experimental', 'factors', 'interaction', 'case', 'experimental units', 'statistical', 'paired']", Two Samples Tests on Two Means,seg_239,not only will the case study that follows illustrate the use of the paired t-test but the discussion will shed considerable light on the difficulties that arise when there is an interaction between the treatments and the experimental units in the paired t structure. recall that interaction between factors was introduced in section 1.7 in a discussion of general types of statistical studies. the concept of interaction will be an important issue from chapter 13 through chapter 15.
2072,1,"['interval', 'paired', 'results', 'statistical tests', 'standard', 'standard deviation', 'tests', 'statistical', 'confidence', 'confidence interval', 'experimental', 'experimental units', 'deviation', 'interaction', 'homogeneous']", Two Samples Tests on Two Means,seg_239,"there are some types of statistical tests in which the existence of interaction results in difficulty. the paired t-test is one such example. in section 9.9, the paired structure was used in the computation of a confidence interval on the difference between two means, and the advantage in pairing was revealed for situations in which the experimental units are homogeneous. the pairing results in a reduction in σd, the standard deviation of a difference di = x1i − x2i, as discussed in"
2073,1,"['experimental', 'levels', 'interaction', 'data', 'mean', 'experimental units']", Two Samples Tests on Two Means,seg_239,"section 9.9. if interaction exists between treatments and experimental units, the advantage gained in pairing may be substantially reduced. thus, in example 9.13 on page 293, the no interaction assumption allowed the difference in mean tcdd levels (plasma vs. fat tissue) to be the same across veterans. a quick glance at the data would suggest that there is no significant violation of the assumption of no interaction."
2074,1,"['experimental', 'independent', 'results', 'interaction', 'homogeneous', 'errors', 'covariance', 'experimental unit', 'paired']", Two Samples Tests on Two Means,seg_239,"in order to demonstrate how interaction influences var(d) and hence the quality of the paired t-test, it is instructive to revisit the ith difference given by di = x1i− x2i = (μ1 − μ2) + ( 1 − 2), where x1i and x2i are taken on the ith experimental unit. if the pairing unit is homogeneous, the errors in x1i and in x2i should be similar and not independent. we noted in chapter 9 that the positive covariance between the errors results in a reduced var(d). thus, the size of the difference in the treatments and the relationship between the errors in x1i and x2i contributed by the experimental unit will tend to allow a significant difference to be detected."
2075,1,"['experimental', 'random variables', 'variables', 'homogeneous', 'errors', 'experimental unit', 'random', 'experimental units']", Two Samples Tests on Two Means,seg_239,"let us consider a situation in which the experimental units are not homogeneous. rather, consider the ith experimental unit with random variables x1i and x2i that are not similar. let 1i and 2i be random variables representing the errors in the values x1i and x2i, respectively, at the ith unit. thus, we may write"
2076,1,"['case', 'vary', 'model', 'experimental', 'parameters', 'expectation', 'experimental unit', 'experimental units', 'variance', 'response', 'interaction', 'homogeneous', 'errors']", Two Samples Tests on Two Means,seg_239,"the errors with expectation zero may tend to cause the response values x1i and x2i to move in opposite directions, resulting in a negative value for cov( 1i, 2i) and hence negative cov(x1i, x2i). in fact, the model may be complicated even more by the fact that σ12 = var( 1i) = σ22 = var( 2i). the variance and covariance parameters may vary among the n experimental units. thus, unlike in the homogeneous case, di will tend to be quite different across experimental units due to the heterogeneous nature of the difference in 1 − 2 among the units. this produces the interaction between treatments and units. in addition, for a specific experimental unit (see theorem 4.9),"
2077,1,"['estimate', 'treatment', 'case', 'homogeneous', 'cases', 'covariance', 'vary', 'variance']", Two Samples Tests on Two Means,seg_239,"is inflated by the negative covariance term, and thus the advantage gained in pairing in the homogeneous unit case is lost in the case described here. while the inflation in var(d) will vary from case to case, there is a danger in some cases that the increase in variance may neutralize any difference that exists between μ1 and μ2. of course, a large value of d̄ in the t-statistic may reflect a treatment difference that overcomes the inflated variance estimate, s2d."
2078,1,"['sample', 'levels', 'data', 'samples']", Two Samples Tests on Two Means,seg_239,"case study 10.1: blood sample data: in a study conducted in the forestry and wildlife department at virginia tech, j. a. wesson examined the influence of the drug succinylcholine on the circulation levels of androgens in the blood. blood samples were taken from wild, free-ranging deer immediately after they had received an intramuscular injection of succinylcholine administered using darts and a capture gun. a second blood sample was obtained from each deer 30 minutes after the"
2079,1,"['sample', 'levels', 'table']", Two Samples Tests on Two Means,seg_239,"first sample, after which the deer was released. the levels of androgens at time of capture and 30 minutes later, measured in nanograms per milliliter (ng/ml), for 15 deer are given in table 10.2."
2080,1,"['levels', 'normally distributed', 'populations', 'level', 'significance', 'test', 'level of significance']", Two Samples Tests on Two Means,seg_239,"assuming that the populations of androgen levels at time of injection and 30 minutes later are normally distributed, test at the 0.05 level of significance whether the androgen concentrations are altered after 30 minutes."
2081,1,"['case', 'data']", Two Samples Tests on Two Means,seg_239,table 10.2: data for case study 10.1
2082,1,"['average', 'concentration']", Two Samples Tests on Two Means,seg_239,"solution : let μ1 and μ2 be the average androgen concentration at the time of injection and 30 minutes later, respectively. we proceed as follows:"
2083,1,['critical region'], Two Samples Tests on Two Means,seg_239,"4. critical region: t < −2.145 and t > 2.145, where t = d−d0 with v = 14"
2084,1,['degrees of freedom'], Two Samples Tests on Two Means,seg_239,sd/√n degrees of freedom.
2085,1,"['deviation', 'sample', 'sample mean', 'mean', 'standard', 'standard deviation']", Two Samples Tests on Two Means,seg_239,5. computations: the sample mean and standard deviation for the di are
2086,1,"['level', 'table']", Two Samples Tests on Two Means,seg_239,"6. though the t-statistic is not significant at the 0.05 level, from table a.4,"
2087,1,"['levels', 'mean']", Two Samples Tests on Two Means,seg_239,"as a result, there is some evidence that there is a difference in mean circulating levels of androgen."
2088,1,"['experimental', 'levels', 'factors', 'interaction', 'homogeneous', 'data', 'mean', 'combinations', 'level', 'experimental units']", Two Samples Tests on Two Means,seg_239,"the assumption of no interaction would imply that the effect on androgen levels of the deer is roughly the same in the data for both treatments, i.e., at the time of injection of succinylcholine and 30 minutes following injection. this can be expressed with the two factors switching roles; for example, the difference in treatments is roughly the same across the units (i.e., the deer). there certainly are some deer/treatment combinations for which the no interaction assumption seems to hold, but there is hardly any strong evidence that the experimental units are homogeneous. however, the nature of the interaction and the resulting increase in var(d̄) appear to be dominated by a substantial difference in the treatments. this is further demonstrated by the fact that 11 of the 15 deer exhibited positive signs for the computed di and the negative di (for deer 2, 10, 11, and 12) are small in magnitude compared to the 12 positive ones. thus, it appears that the mean level of androgen is significantly higher 30 minutes following injection than at injection, and the conclusions may be stronger than p = 0.06 would suggest."
2089,1,"['sample', 'case', 'data', 'test', 'paired']", Two Samples Tests on Two Means,seg_239,"figure 10.13 displays a sas computer printout for a paired t-test using the data of case study 10.1. notice that the printout looks like that for a single sample t-test and, of course, that is exactly what is accomplished, since the test seeks to determine if d is significantly different from zero."
2090,1,"['variable', 'variable ']", Two Samples Tests on Two Means,seg_239,analysis variable : diff
2091,1,"['error', 'mean']", Two Samples Tests on Two Means,seg_239,n mean std error t value pr > |t| --------------------------------------------------------- 15 9.8480000 4.7698699 2.06 0.0580 ---------------------------------------------------------
2092,1,"['case', 'data', 'paired']", Two Samples Tests on Two Means,seg_239,figure 10.13: sas printout of paired t-test for data of case study 10.1.
2093,1,"['tests', 'variances', 'table', 'cases', 'statistic', 'normal', 'mean', 'population', 'test', 'distributions']", Two Samples Tests on Two Means,seg_239,"as we complete the formal development of tests on population means, we offer table 10.3, which summarizes the test procedure for the cases of a single mean and two means. notice the approximate procedure when distributions are normal and variances are unknown but not assumed to be equal. this statistic was introduced in chapter 9."
2094,1,"['sample size', 'sample', 'power of the test', 'process', 'experiment', 'standard', 'level', 'significance', 'test', 'significance level']", Choice of Sample Size for Testing Means,seg_241,"in section 10.2, we demonstrated how the analyst can exploit relationships among the sample size, the significance level α, and the power of the test to achieve a certain standard of quality. in most practical circumstances, the experiment should be planned, with a choice of sample size made prior to the data-taking process if possible. the sample size is usually determined to achieve good power for a fixed α and fixed specific alternative. this fixed alternative may be in the"
2095,1,['tests'], Choice of Sample Size for Testing Means,seg_241,table 10.3: tests concerning means
2096,1,"['test statistic', 'statistic', 'test', 'critical region']", Choice of Sample Size for Testing Means,seg_241,h0 value of test statistic h1 critical region
2097,1,"['observations', 'paired']", Choice of Sample Size for Testing Means,seg_241,μd = d0 d− d0 μd < d0 t < −tα t = ; paired sd/√n μd > d0 t > tα observations v = n− 1 μd = d0 t < −tα/2 or t > tα/2
2098,1,"['cases', 'mean', 'case', 'hypothesis']", Choice of Sample Size for Testing Means,seg_241,form of μ−μ0 in the case of a hypothesis involving a single mean or μ1−μ2 in the case of a problem involving two means. specific cases will provide illustrations.
2099,1,"['test', 'hypothesis']", Choice of Sample Size for Testing Means,seg_241,suppose that we wish to test the hypothesis
2100,1,"['variance', 'level', 'significance', 'test', 'significance level']", Choice of Sample Size for Testing Means,seg_241,"with a significance level α, when the variance σ2 is known. for a specific alternative, say μ = μ0 + δ, the power of our test is shown in figure 10.14 to be"
2101,0,[], Choice of Sample Size for Testing Means,seg_241,figure 10.14: testing μ = μ0 versus μ = μ0 + δ.
2102,1,"['alternative hypothesis', 'statistic', 'hypothesis']", Choice of Sample Size for Testing Means,seg_241,"under the alternative hypothesis μ = μ0 + δ, the statistic"
2103,1,"['standard normal', 'variable', 'normal', 'standard']", Choice of Sample Size for Testing Means,seg_241,is the standard normal variable z. so
2104,0,[], Choice of Sample Size for Testing Means,seg_241,from which we conclude that
2105,1,"['sample size', 'sample']", Choice of Sample Size for Testing Means,seg_241,"(zα + zβ)2σ2 choice of sample size: n = , δ2"
2106,1,"['alternative hypothesis', 'hypothesis']", Choice of Sample Size for Testing Means,seg_241,a result that is also true when the alternative hypothesis is μ < μ0.
2107,1,"['test', 'case']", Choice of Sample Size for Testing Means,seg_241,"in the case of a two-tailed test, we obtain the power 1 − β for a specified alternative when"
2108,1,"['test', 'hypothesis']", Choice of Sample Size for Testing Means,seg_241,example 10.7: suppose that we wish to test the hypothesis
2109,1,"['sample size', 'sample', 'mean', 'level', 'significance', 'test', 'level of significance']", Choice of Sample Size for Testing Means,seg_241,"for the weights of male students at a certain college, using an α = 0.05 level of significance, when it is known that σ = 5. find the sample size required if the power of our test is to be 0.95 when the true mean is 69 kilograms."
2110,1,"['observations', 'test', 'null hypothesis', 'hypothesis']", Choice of Sample Size for Testing Means,seg_241,"therefore, 271 observations are required if the test is to reject the null hypothesis 95% of the time when, in fact, μ is as large as 69 kilograms."
2111,1,"['sample size', 'sample', 'power of the test', 'population', 'test', 'hypothesis']", Choice of Sample Size for Testing Means,seg_241,"a similar procedure can be used to determine the sample size n = n1 = n2 required for a specific power of the test in which two population means are being compared. for example, suppose that we wish to test the hypothesis"
2112,1,['test'], Choice of Sample Size for Testing Means,seg_241,"when σ1 and σ2 are known. for a specific alternative, say μ1 − μ2 = d0 + δ, the power of our test is shown in figure 10.15 to be"
2113,0,[], Choice of Sample Size for Testing Means,seg_241,figure 10.15: testing μ1 − μ2 = d0 versus μ1 − μ2 = d0 + δ.
2114,1,"['alternative hypothesis', 'statistic', 'hypothesis']", Choice of Sample Size for Testing Means,seg_241,"under the alternative hypothesis μ1 − μ2 = d0 + δ, the statistic"
2115,1,"['standard normal', 'variable', 'normal', 'standard']", Choice of Sample Size for Testing Means,seg_241,"is the standard normal variable z. now, writing"
2116,0,[], Choice of Sample Size for Testing Means,seg_241,from which we conclude that
2117,1,"['sample size', 'sample', 'test']", Choice of Sample Size for Testing Means,seg_241,"for the one-tailed test, the expression for the required sample size when n = n1 = n2 is"
2118,1,"['sample size', 'sample']", Choice of Sample Size for Testing Means,seg_241,(zα + zβ)2(σ12 + σ22) choice of sample size: n = . δ2
2119,1,"['sample size', 'sample', 'population variance', 'variances', 'statistic', 'population', 'variance', 'hypothesis']", Choice of Sample Size for Testing Means,seg_241,"when the population variance (or variances, in the two-sample situation) is unknown, the choice of sample size is not straightforward. in testing the hypothesis μ = μ0 when the true value is μ = μ0 + δ, the statistic"
2120,1,"['sample size', 'sample', 'estimate', 'table', 'tables', 'control', 'noncentral', 'charts']", Choice of Sample Size for Testing Means,seg_241,"does not follow the t-distribution, as one might expect, but instead follows the noncentral t-distribution. however, tables or charts based on the noncentral t-distribution do exist for determining the appropriate sample size if some estimate of σ is available or if δ is a multiple of σ. table a.8 gives the sample sizes needed to control the values of α and β for various values of"
2121,1,"['sample', 'tests', 'case', 'control', 'variances']", Choice of Sample Size for Testing Means,seg_241,"for both oneand two-tailed tests. in the case of the two-sample t-test in which the variances are unknown but assumed equal, we obtain the sample sizes n = n1 = n2 needed to control the values of α and β for various values of"
2122,1,['table'], Choice of Sample Size for Testing Means,seg_241,from table a.9.
2123,1,['variances'], Choice of Sample Size for Testing Means,seg_241,"example 10.8: in comparing the performance of two catalysts on the effect of a reaction yield, a two-sample t-test is to be conducted with α = 0.05. the variances in the yields"
2124,1,"['sample', 'test', 'hypothesis']", Choice of Sample Size for Testing Means,seg_241,are considered to be the same for the two catalysts. how large a sample for each catalyst is needed to test the hypothesis
2125,1,['probability'], Choice of Sample Size for Testing Means,seg_241,if it is essential to detect a difference of 0.8σ between the catalysts with probability 0.9?
2126,1,"['test', 'table']", Choice of Sample Size for Testing Means,seg_241,"solution : from table a.9, with α = 0.05 for a two-tailed test, β = 0.1, and"
2127,1,"['sample size', 'sample']", Choice of Sample Size for Testing Means,seg_241,we find the required sample size to be n = 34.
2128,1,"['sample size', 'sample', 'information', 'statistical']", Choice of Sample Size for Testing Means,seg_241,"in practical situations, it might be difficult to force a scientist or engineer to make a commitment on information from which a value of δ can be found. the reader is reminded that the δ-value quantifies the kind of difference between the means that the scientist considers important, that is, a difference considered significant from a scientific, not a statistical, point of view. example 10.8 illustrates how this choice is often made, namely, by selecting a fraction of σ. obviously, if the sample size is based on a choice of |δ| that is a small fraction of σ, the resulting sample size may be quite large compared to what the study allows."
2129,1,"['plots', 'set', 'graphical', 'data', 'experimental', 'regression', 'analysis of variance', 'variance', 'quantile', 'regression analysis', 'normal']", Graphical Methods for Comparing Means,seg_243,"in chapter 1, considerable attention was directed to displaying data in graphical form, such as stem-and-leaf plots and box-and-whisker plots. in section 8.8, quantile plots and quantile-quantile normal plots were used to provide a “picture” to summarize a set of experimental data. many computer software packages produce graphical displays. as we proceed to other forms of data analysis (e.g., regression analysis and analysis of variance), graphical methods become even more informative."
2130,1,"['graphical', 'test statistic', 'failure', 'replacement', 'statistic', 'test']", Graphical Methods for Comparing Means,seg_243,"graphical aids cannot be used as a replacement for the test procedure itself. certainly, the value of the test statistic indicates the proper type of evidence in support of h0 or h1. however, a pictorial display provides a good illustration and is often a better communicator of evidence to the beneficiary of the analysis. also, a picture will often clarify why a significant difference was found. failure of an important assumption may be exposed by a summary type of graphical tool."
2131,1,"['levels', 'plots', 'case', 'set', 'sample', 'sample means', 'whiskers', 'data', 'percentile', 'median', 'data set', 'variability']", Graphical Methods for Comparing Means,seg_243,"for the comparison of means, side-by-side box-and-whisker plots provide a telling display. the reader should recall that these plots display the 25th percentile, 75th percentile, and the median in a data set. in addition, the whiskers display the extremes in a data set. consider exercise 10.40 at the end of this section. plasma ascorbic acid levels were measured in two groups of pregnant women, smokers and nonsmokers. figure 10.16 shows the box-and-whisker plots for both groups of women. two things are very apparent. taking into account variability, there appears to be a negligible difference in the sample means. in addition, the variability in the two groups appears to be somewhat different. of course, the analyst must keep in mind the rather sizable differences between the sample sizes in this case."
2132,1,"['plots', 'data']", Graphical Methods for Comparing Means,seg_243,figure 10.16: two box-and-whisker plots of figure 10.17: two box-and-whisker plots of plasma ascorbic acid in smokers and nonsmokers. seedling data.
2133,1,"['plot', 'variability', 'plots', 'data', 'mean', 'box plots']", Graphical Methods for Comparing Means,seg_243,"consider exercise 9.40 in section 9.9. figure 10.17 shows the multiple boxand-whisker plot for the data on 10 seedlings, half given nitrogen and half given no nitrogen. the display reveals a smaller variability for the group containing no nitrogen. in addition, the lack of overlap of the box plots suggests a significant difference between the mean stem weights for the two groups. it would appear that the presence of nitrogen increases the stem weights and perhaps increases the variability in the weights."
2134,1,"['sample', 'percentile', 'median', 'plots']", Graphical Methods for Comparing Means,seg_243,"there are no certain rules of thumb regarding when two box-and-whisker plots give evidence of significant difference between the means. however, a rough guideline is that if the 25th percentile line for one sample exceeds the median line for the other sample, there is strong evidence of a difference between means."
2135,1,"['graphical', 'case']", Graphical Methods for Comparing Means,seg_243,more emphasis is placed on graphical methods in a real-life case study presented later in this chapter.
2136,1,"['test', 'data']", Graphical Methods for Comparing Means,seg_243,"consider once again exercise 9.40 on page 294, where seedling data under conditions of nitrogen and no nitrogen were collected. test"
2137,1,"['plot', 'sample', 'deviation', 'standard error', 'sample standard deviation', 'case', 'information', 'samples', 'mean', 'population', 'standard', 'standard deviation', 'variance', 'error']", Graphical Methods for Comparing Means,seg_243,"where the population means indicate mean weights. figure 10.18 is an annotated computer printout generated using the sas package. notice that sample standard deviation and standard error are shown for both samples. the t-statistics under the assumption of equal variance and unequal variance are both given. from the boxand-whisker plot of figure 10.17 it would certainly appear that the equal variance assumption is violated. a p -value of 0.0229 suggests a conclusion of unequal means. this concurs with the diagnostic information given in figure 10.18. incidentally, notice that t and t′ are equal in this case, since n1 = n2."
2138,1,"['tests', 'tests of hypotheses', 'hypotheses']", Graphical Methods for Comparing Means,seg_243,356 chapter 10 oneand two-sample tests of hypotheses
2139,1,['variable'], Graphical Methods for Comparing Means,seg_243,ttest procedure variable weight
2140,1,['mean'], Graphical Methods for Comparing Means,seg_243,mineral n mean std dev std err no nitrogen 10 0.3990 0.0728 0.0230
2141,1,"['variable', 'variances']", Graphical Methods for Comparing Means,seg_243,test the equality of variances variable num df den df f value pr > f
2142,0,[], Graphical Methods for Comparing Means,seg_243,figure 10.18: sas printout for two-sample t-test.
2143,1,"['hypotheses', 'outcomes']", One Sample Test on a Single Proportion,seg_247,tests of hypotheses concerning proportions are required in many areas. politicians are certainly interested in knowing what fraction of the voters will favor them in the next election. all manufacturing firms are concerned about the proportion of defective items when a shipment is made. gamblers depend on a knowledge of the proportion of outcomes that they consider favorable.
2144,1,"['alternative hypothesis', 'binomial experiment', 'experiment', 'successes', 'distribution', 'binomial', 'parameter', 'binomial distribution', 'null hypothesis', 'hypothesis']", One Sample Test on a Single Proportion,seg_247,"we shall consider the problem of testing the hypothesis that the proportion of successes in a binomial experiment equals some specified value. that is, we are testing the null hypothesis h0 that p = p0, where p is the parameter of the binomial distribution. the alternative hypothesis may be one of the usual one-sided"
2145,0,[], One Sample Test on a Single Proportion,seg_247,or two-sided alternatives:
2146,1,"['null hypothesis', 'discrete', 'variable', 'random variable', 'samples', 'statistic', 'mean', 'binomial', 'random', 'binomial random variable', 'test', 'critical region', 'hypothesis']", One Sample Test on a Single Proportion,seg_247,"the appropriate random variable on which we base our decision criterion is the binomial random variable x, although we could just as well use the statistic p̂ = x/n. values of x that are far from the mean μ = np0 will lead to the rejection of the null hypothesis. because x is a discrete binomial variable, it is unlikely that a critical region can be established whose size is exactly equal to a prespecified value of α. for this reason it is preferable, in dealing with small samples, to base our decisions on p -values. to test the hypothesis"
2147,1,"['distribution', 'binomial distribution', 'binomial']", One Sample Test on a Single Proportion,seg_247,we use the binomial distribution to compute the p -value
2148,1,"['sample', 'successes', 'level', 'test', 'hypothesis']", One Sample Test on a Single Proportion,seg_247,"the value x is the number of successes in our sample of size n. if this p -value is less than or equal to α, our test is significant at the α level and we reject h0 in favor of h1. similarly, to test the hypothesis"
2149,1,['significance'], One Sample Test on a Single Proportion,seg_247,"at the α-level of significance, we compute"
2150,1,"['test', 'hypothesis']", One Sample Test on a Single Proportion,seg_247,"and reject h0 in favor of h1 if this p -value is less than or equal to α. finally, to test the hypothesis"
2151,1,['significance'], One Sample Test on a Single Proportion,seg_247,"h1: p = p0, at the α-level of significance, we compute"
2152,0,[], One Sample Test on a Single Proportion,seg_247,and reject h0 in favor of h1 if the computed p -value is less than or equal to α.
2153,1,"['probabilities', 'table', 'binomial', 'null hypothesis', 'hypothesis']", One Sample Test on a Single Proportion,seg_247,the steps for testing a null hypothesis about a proportion against various alternatives using the binomial probabilities of table a.1 are as follows:
2154,1,"['level of significance', 'test statistic', 'successes', 'variable', 'samples', 'statistic', 'binomial', 'level', 'significance', 'test']", One Sample Test on a Single Proportion,seg_247,"testing a 1. h0: p = p0. proportion 2. one of the alternatives h1: p < p0, p > p0, or p = p0. (small samples) 3. choose a level of significance equal to α. 4. test statistic: binomial variable x with p = p0. 5. computations: find x, the number of successes, and compute the appropriate p -value. 6. decision: draw appropriate conclusions based on the p -value."
2155,1,"['level', 'significance', 'level of significance', 'random']", One Sample Test on a Single Proportion,seg_247,"example 10.9: a builder claims that heat pumps are installed in 70% of all homes being constructed today in the city of richmond, virginia. would you agree with this claim if a random survey of new homes in this city showed that 8 out of 15 had heat pumps installed? use a 0.10 level of significance."
2156,1,"['test statistic', 'variable', 'statistic', 'binomial', 'test']", One Sample Test on a Single Proportion,seg_247,4. test statistic: binomial variable x with p = 0.7 and n = 15.
2157,1,['table'], One Sample Test on a Single Proportion,seg_247,"5. computations: x = 8 and np0 = (15)(0.7) = 10.5. therefore, from table a.1,"
2158,0,[], One Sample Test on a Single Proportion,seg_247,the computed p -value is
2159,0,[], One Sample Test on a Single Proportion,seg_247,6. decision: do not reject h0. conclude that there is insufficient reason to
2160,0,[], One Sample Test on a Single Proportion,seg_247,doubt the builder’s claim.
2161,1,"['poisson', 'curve', 'normal approximation', 'probabilities', 'parameters', 'table', 'approximation', 'distribution', 'normal', 'binomial', 'parameter', 'poisson distribution']", One Sample Test on a Single Proportion,seg_247,"in section 5.2, we saw that binomial probabilities can be obtained from the actual binomial formula or from table a.1 when n is small. for large n, approximation procedures are required. when the hypothesized value p0 is very close to 0 or 1, the poisson distribution, with parameter μ = np0, may be used. however, the normal curve approximation, with parameters μ = np0 and σ2 = np0q0, is usually preferred for large n and is very accurate as long as p0 is not extremely close to 0 or to 1. if we use the normal approximation, the z-value for testing p = p0 is given by"
2162,1,"['standard normal', 'variable', 'normal', 'standard', 'significance', 'test', 'critical region']", One Sample Test on a Single Proportion,seg_247,"which is a value of the standard normal variable z. hence, for a two-tailed test at the α-level of significance, the critical region is z < −zα/2 or z > zα/2. for the one-sided alternative p < p0, the critical region is z < −zα, and for the alternative p > p0, the critical region is z > zα."
2163,1,"['sample', 'experimental', 'random', 'results', 'random sample', 'level', 'significance', 'level of significance']", One Sample Test on a Single Proportion,seg_247,example 10.10: a commonly prescribed drug for relieving nervous tension is believed to be only 60% effective. experimental results with a new drug administered to a random sample of 100 adults who were suffering from nervous tension show that 70 received relief. is this sufficient evidence to conclude that the new drug is superior to the one commonly prescribed? use a 0.05 level of significance.
2164,1,['critical region'], One Sample Test on a Single Proportion,seg_247,4. critical region: z > 1.645.
2165,0,[], One Sample Test on a Single Proportion,seg_247,6. decision: reject h0 and conclude that the new drug is superior.
2166,1,"['test', 'hypothesis']", Two Samples Tests on Two Proportions,seg_249,"situations often arise where we wish to test the hypothesis that two proportions are equal. for example, we might want to show evidence that the proportion of doctors who are pediatricians in one state is equal to the proportion in another state. a person may decide to give up smoking only if he or she is convinced that the proportion of smokers with lung cancer exceeds the proportion of nonsmokers with lung cancer."
2167,1,"['statistic', 'random', 'null hypothesis', 'successes', 'random variable', 'samples', 'populations', 'parameters', 'binomial', 'test', 'hypothesis', 'independent', 'variable']", Two Samples Tests on Two Proportions,seg_249,"in general, we wish to test the null hypothesis that two proportions, or binomial parameters, are equal. that is, we are testing p1 = p2 against one of the alternatives p1 < p2, p1 > p2, or p1 = p2. of course, this is equivalent to testing the null hypothesis that p1 − p2 = 0 against one of the alternatives p1 − p2 < 0, p1 − p2 > 0, or p1 − p2 = 0. the statistic on which we base our decision is the random variable p̂1 − p̂2. independent samples of sizes n1 and n2 are selected at random from two binomial populations and the proportions of successes p̂1 and p̂2 for the two samples are computed."
2168,1,"['confidence intervals', 'intervals', 'normally distributed', 'mean', 'point estimator', 'confidence', 'estimator']", Two Samples Tests on Two Proportions,seg_249,"in our construction of confidence intervals for p1 and p2 we noted, for n1 and n2 sufficiently large, that the point estimator p̂1 minus p̂2 was approximately normally distributed with mean"
2169,1,['variance'], Two Samples Tests on Two Proportions,seg_249,and variance
2170,1,"['standard normal', 'variable', 'normal', 'standard']", Two Samples Tests on Two Proportions,seg_249,"therefore, our critical region(s) can be established by using the standard normal variable"
2171,0,[], Two Samples Tests on Two Proportions,seg_249,"when h0 is true, we can substitute p1 = p2 = p and q1 = q2 = q (where p and q are the common values) in the preceding formula for z to give the form"
2172,1,"['parameters', 'estimate', 'data', 'samples']", Two Samples Tests on Two Proportions,seg_249,"to compute a value of z, however, we must estimate the parameters p and q that appear in the radical. upon pooling the data from both samples, the pooled estimate of the proportion p is"
2173,1,"['successes', 'samples']", Two Samples Tests on Two Proportions,seg_249,"where x1 and x2 are the numbers of successes in each of the two samples. substituting p̂ for p and q̂ = 1− p̂ for q, the z-value for testing p1= p2 is determined from the formula"
2174,1,"['curve', 'standard normal', 'set', 'normal', 'hypotheses', 'standard', 'significance', 'test', 'critical region', 'alternative hypotheses']", Two Samples Tests on Two Proportions,seg_249,"the critical regions for the appropriate alternative hypotheses are set up as before, using critical points of the standard normal curve. hence, for the alternative p1 = p2 at the α-level of significance, the critical region is z < −zα/2 or z > zα/2. for a test where the alternative is p1 < p2, the critical region is z < −zα, and when the alternative is p1 > p2, the critical region is z > zα."
2175,1,"['county', 'level', 'significance', 'level of significance']", Two Samples Tests on Two Proportions,seg_249,"example 10.11: a vote is to be taken among the residents of a town and the surrounding county to determine whether a proposed chemical plant should be constructed. the construction site is within the town limits, and for this reason many voters in the county believe that the proposal will pass because of the large proportion of town voters who favor the construction. to determine if there is a significant difference in the proportions of town voters and county voters favoring the proposal, a poll is taken. if 120 of 200 town voters favor the proposal and 240 of 500 county residents favor it, would you agree that the proportion of town voters favoring the proposal is higher than the proportion of county voters? use an α = 0.05 level of significance."
2176,1,['county'], Two Samples Tests on Two Proportions,seg_249,"solution : let p1 and p2 be the true proportions of voters in the town and county, respectively, favoring the proposal."
2177,1,['critical region'], Two Samples Tests on Two Proportions,seg_249,4. critical region: z > 1.645.
2178,0,[], Two Samples Tests on Two Proportions,seg_249,5. computations:
2179,0,[], Two Samples Tests on Two Proportions,seg_249,6. decision: reject h0 and agree that the proportion of town voters favoring
2180,1,['county'], Two Samples Tests on Two Proportions,seg_249,the proposal is higher than the proportion of county voters.
2181,1,"['set', 'measurements', 'process', 'population', 'standard', 'tests', 'variance', 'test', 'processes', 'standard deviations', 'variability', 'deviations', 'hypotheses', 'experiments', 'variances']", One and TwoSample Tests Concerning Variances,seg_253,"in this section, we are concerned with testing hypotheses concerning population variances or standard deviations. applications of oneand two-sample tests on variances are certainly not difficult to motivate. engineers and scientists are confronted with studies in which they are required to demonstrate that measurements involving products or processes adhere to specifications set by consumers. the specifications are often met if the process variance is sufficiently small. attention is also focused on comparative experiments between methods or processes, where inherent reproducibility or variability must formally be compared. in addition, to determine if the equal variance assumption is violated, a test comparing two variances is often applied prior to conducting a t-test on two means."
2182,1,"['population variance', 'interval', 'distribution', 'statistic', 'normal', 'confidence', 'population', 'variance', 'null hypothesis', 'confidence interval', 'hypothesis']", One and TwoSample Tests Concerning Variances,seg_253,"let us first consider the problem of testing the null hypothesis h0 that the population variance σ2 equals a specified value σ02 against one of the usual alternatives σ2 < σ02, σ2 > σ02, or σ2 = σ02. the appropriate statistic on which to base our decision is the chi-squared statistic of theorem 8.4, which was used in chapter 9 to construct a confidence interval for σ2. therefore, if we assume that the distribution of the population being sampled is normal, the chi-squared value for testing σ2 = σ02 is given by"
2183,1,"['sample size', 'sample', 'degrees of freedom', 'null hypothesis', 'significance', 'distribution', 'sample variance', 'test', 'variance', 'critical region', 'hypothesis']", One and TwoSample Tests Concerning Variances,seg_253,"where n is the sample size, s2 is the sample variance, and σ02 is the value of σ2 given by the null hypothesis. if h0 is true, χ2 is a value of the chi-squared distribution with v = n − 1 degrees of freedom. hence, for a two-tailed test at the α-level of significance, the critical region is χ2 < χ21−α/2 or χ2 > χ2α/2. for the one-"
2184,1,['critical region'], One and TwoSample Tests Concerning Variances,seg_253,"sided alternative σ2 < σ02, the critical region is χ2 < χ12−α, and for the one-sided alternative σ2 > σ02, the critical region is χ2 > χα2 ."
2185,1,"['tests', 'distribution', 'normality', 'statistically significant', 'normal', 'population', 'vary', 'success', 'variance', 'normal distribution']", One and TwoSample Tests Concerning Variances,seg_253,"the reader may have discerned that various tests depend, at least theoretically, on the assumption of normality. in general, many procedures in applied statistics have theoretical underpinnings that depend on the normal distribution. these procedures vary in the degree of their dependency on the assumption of normality. a procedure that is reasonably insensitive to the assumption is called a robust procedure (i.e., robust to normality). the χ2-test on a single variance is very nonrobust to normality (i.e., the practical success of the procedure depends on normality). as a result, the p -value computed may be appreciably different from the actual p -value if the population sampled is not normal. indeed, it is quite feasible that a statistically significant p -value may not truly signal h1: σ = σ0; rather, a significant value may be a result of the violation of the normality assumptions. therefore, the analyst should approach the use of this particular χ2-test with caution."
2186,1,"['deviation', 'normally distributed', 'standard', 'standard deviation']", One and TwoSample Tests Concerning Variances,seg_253,example 10.12: a manufacturer of car batteries claims that the life of the company’s batteries is approximately normally distributed with a standard deviation equal to 0.9 year.
2187,1,"['deviation', 'sample', 'random', 'standard', 'random sample', 'level', 'standard deviation', 'significance', 'level of significance']", One and TwoSample Tests Concerning Variances,seg_253,"if a random sample of 10 of these batteries has a standard deviation of 1.2 years, do you think that σ > 0.9 year? use a 0.05 level of significance. 1. h0: σ2 = 0.81."
2188,1,"['null hypothesis', 'critical region', 'hypothesis']", One and TwoSample Tests Concerning Variances,seg_253,4. critical region: from figure 10.19 we see that the null hypothesis is rejected
2189,1,['degrees of freedom'], One and TwoSample Tests Concerning Variances,seg_253,"when χ2 > 16.919, where χ2 = (n−1)s2 , with v = 9 degrees of freedom. σ02"
2190,1,"['alternative hypothesis', 'critical region', 'hypothesis']", One and TwoSample Tests Concerning Variances,seg_253,figure 10.19: critical region for the alternative hypothesis σ > 0.9.
2191,1,['level'], One and TwoSample Tests Concerning Variances,seg_253,"6. decision: the χ2-statistic is not significant at the 0.05 level. however, based"
2192,1,"['variances', 'populations', 'test', 'null hypothesis', 'hypothesis']", One and TwoSample Tests Concerning Variances,seg_253,"now let us consider the problem of testing the equality of the variances σ12 and σ22 of two populations. that is, we shall test the null hypothesis h0 that σ12 = σ22 against one of the usual alternatives"
2193,1,"['independent', 'random samples', 'populations', 'samples', 'random']", One and TwoSample Tests Concerning Variances,seg_253,"for independent random samples of sizes n1 and n2, respectively, from the two populations, the f-value for testing σ12 = σ22 is the ratio"
2194,1,"['degrees of freedom', 'null hypothesis', 'normally distributed', 'samples', 'hypothesis', 'populations', 'variances']", One and TwoSample Tests Concerning Variances,seg_253,"where s21 and s22 are the variances computed from the two samples. if the two populations are approximately normally distributed and the null hypothesis is true, according to theorem 8.8 the ratio f = s21/s22 is a value of the f -distribution with v1 = n1 − 1 and v2 = n2 − 1 degrees of freedom. therefore, the critical regions"
2195,1,['critical region'], One and TwoSample Tests Concerning Variances,seg_253,"of size α corresponding to the one-sided alternatives σ12 < σ22 and σ12 > σ22 are, respectively, f < f1−α(v1, v2) and f > fα(v1, v2). for the two-sided alternative σ12 = σ22, the critical region is f < f1−α/2(v1, v2) or f > fα/2(v1, v2)."
2196,1,"['population', 'level', 'significance', 'level of significance', 'variances']", One and TwoSample Tests Concerning Variances,seg_253,"example 10.13: in testing for the difference in the abrasive wear of the two materials in example 10.6, we assumed that the two unknown population variances were equal. were we justified in making this assumption? use a 0.10 level of significance."
2197,1,"['population', 'variances']", One and TwoSample Tests Concerning Variances,seg_253,"solution : let σ12 and σ22 be the population variances for the abrasive wear of material 1 and material 2, respectively."
2198,1,['critical region'], One and TwoSample Tests Concerning Variances,seg_253,"4. critical region: from figure 10.20, we see that f0.05(11, 9) = 3.11, and, by"
2199,1,"['degrees of freedom', 'null hypothesis', 'hypothesis']", One and TwoSample Tests Concerning Variances,seg_253,"therefore, the null hypothesis is rejected when f < 0.34 or f > 3.11, where f = s21/s22 with v1 = 11 and v2 = 9 degrees of freedom."
2200,0,[], One and TwoSample Tests Concerning Variances,seg_253,25 6. decision: do not reject h0. conclude that there is insufficient evidence that
2201,1,['variances'], One and TwoSample Tests Concerning Variances,seg_253,the variances differ.
2202,1,"['alternative hypothesis', 'critical region', 'hypothesis']", One and TwoSample Tests Concerning Variances,seg_253,figure 10.20: critical region for the alternative hypothesis σ12 = σ22.
2203,1,"['plots', 'homogeneous', 'data', 'variances']", One and TwoSample Tests Concerning Variances,seg_253,"figure 10.18 on page 356 displays the printout of a two-sample t-test where two means from the seedling data in exercise 9.40 were compared. box-and-whisker plots in figure 10.17 on page 355 suggest that variances are not homogeneous, and thus the t′-statistic and its corresponding p -value are relevant. note also that"
2204,1,"['condition', 'variability']", One and TwoSample Tests Concerning Variances,seg_253,"the printout displays the f -statistic for h0: σ1 = σ2 with a p -value of 0.0098, additional evidence that more variability is to be expected when nitrogen is used than under the no-nitrogen condition."
2205,1,"['sample', 'parameters', 'frequencies', 'observations', 'statistical hypotheses', 'distribution', 'statistical', 'hypotheses', 'frequency', 'population', 'test']", GoodnessofFit Test,seg_257,"throughout this chapter, we have been concerned with the testing of statistical hypotheses about single population parameters such as μ, σ2, and p. now we shall consider a test to determine if a population has a specified theoretical distribution. the test is based on how good a fit we have between the frequency of occurrence of observations in an observed sample and the expected frequencies obtained from the hypothesized distribution."
2206,1,"['uniform distribution', 'discrete', 'distribution', 'discrete uniform distribution', 'outcomes', 'hypothesis']", GoodnessofFit Test,seg_257,"to illustrate, we consider the tossing of a die. we hypothesize that the die is honest, which is equivalent to testing the hypothesis that the distribution of outcomes is the discrete uniform distribution"
2207,1,"['outcome', 'results', 'table']", GoodnessofFit Test,seg_257,"suppose that the die is tossed 120 times and each outcome is recorded. theoretically, if the die is balanced, we would expect each face to occur 20 times. the results are given in table 10.4."
2208,1,['frequencies'], GoodnessofFit Test,seg_257,table 10.4: observed and expected frequencies of 120 tosses of a die
2209,1,"['experiment', 'frequencies', 'sampling', 'distribution', 'statistic', 'outcome', 'outcomes']", GoodnessofFit Test,seg_257,"by comparing the observed frequencies with the corresponding expected frequencies, we must decide whether these discrepancies are likely to occur as a result of sampling fluctuations and the die is balanced or whether the die is not honest and the distribution of outcomes is not uniform. it is common practice to refer to each possible outcome of an experiment as a cell. in our illustration, we have 6 cells. the appropriate statistic on which we base our decision criterion for an experiment involving k cells is defined by the following."
2210,1,"['frequencies', 'test']", GoodnessofFit Test,seg_257,a goodness-of-fit test between observed and expected frequencies is based on the quantity
2211,1,['test'], GoodnessofFit Test,seg_257,"goodness-of-fit k∑ (oi − ei)2 test χ2 = , ei i=1"
2212,1,"['degrees of freedom', 'frequencies', 'sampling', 'random variable', 'variable', 'distribution', 'random', 'sampling distribution']", GoodnessofFit Test,seg_257,"where χ2 is a value of a random variable whose sampling distribution is approximated very closely by the chi-squared distribution with v = k − 1 degrees of freedom. the symbols oi and ei represent the observed and expected frequencies, respectively, for the ith cell."
2213,1,"['degrees of freedom', 'frequencies', 'distribution', 'frequency', 'associated']", GoodnessofFit Test,seg_257,"the number of degrees of freedom associated with the chi-squared distribution used here is equal to k − 1, since there are only k − 1 freely determined cell frequencies. that is, once k − 1 cell frequencies are determined, so is the frequency for the kth cell."
2214,1,"['degrees of freedom', 'table', 'critical region', 'frequencies', 'significance', 'distribution', 'level', 'tail', 'level of significance', 'critical value']", GoodnessofFit Test,seg_257,"if the observed frequencies are close to the corresponding expected frequencies, the χ2-value will be small, indicating a good fit. if the observed frequencies differ considerably from the expected frequencies, the χ2-value will be large and the fit is poor. a good fit leads to the acceptance of h0, whereas a poor fit leads to its rejection. the critical region will, therefore, fall in the right tail of the chi-squared distribution. for a level of significance equal to α, we find the critical value χα2 from table a.5, and then χ2 > χα2 constitutes the critical region. the decision criterion described here should not be used unless each of the expected frequencies is at least equal to 5. this restriction may require the combining of adjacent cells, resulting in a reduction in the number of degrees of freedom."
2215,1,['table'], GoodnessofFit Test,seg_257,"from table 10.4, we find the χ2-value to be"
2216,1,"['degrees of freedom', 'critical value', 'table']", GoodnessofFit Test,seg_257,"using table a.5, we find χ20.05 = 11.070 for v = 5 degrees of freedom. since 1.7 is less than the critical value, we fail to reject h0. we conclude that there is insufficient evidence that the die is not balanced."
2217,1,"['deviation', 'curve', 'table', 'frequencies', 'distribution', 'frequency', 'normal', 'mean', 'standard', 'standard deviation', 'test', 'normal distribution', 'hypothesis']", GoodnessofFit Test,seg_257,"as a second illustration, let us test the hypothesis that the frequency distribution of battery lives given in table 1.7 on page 23 may be approximated by a normal distribution with mean μ = 3.5 and standard deviation σ = 0.7. the expected frequencies for the 7 classes (cells), listed in table 10.5, are obtained by computing the areas under the hypothesized normal curve that fall between the various class boundaries."
2218,1,"['normality', 'frequencies']", GoodnessofFit Test,seg_257,"table 10.5: observed and expected frequencies of battery lives, assuming normality"
2219,0,[], GoodnessofFit Test,seg_257,"for example, the z-values corresponding to the boundaries of the fourth class are"
2220,1,['table'], GoodnessofFit Test,seg_257,from table a.3 we find the area between z1 = −0.79 and z2 = −0.07 to be
2221,1,['frequency'], GoodnessofFit Test,seg_257,"hence, the expected frequency for the fourth class is"
2222,1,['frequencies'], GoodnessofFit Test,seg_257,it is customary to round these frequencies to one decimal.
2223,1,"['degrees of freedom', 'curve', 'class interval', 'method', 'interval', 'table', 'frequencies', 'intervals', 'frequency', 'normal', 'test']", GoodnessofFit Test,seg_257,"the expected frequency for the first class interval is obtained by using the total area under the normal curve to the left of the boundary 1.95. for the last class interval, we use the total area to the right of the boundary 4.45. all other expected frequencies are determined by the method described for the fourth class. note that we have combined adjacent classes in table 10.5 where the expected frequencies are less than 5 (a rule of thumb in the goodness-of-fit test). consequently, the total number of intervals is reduced from 7 to 4, resulting in v = 3 degrees of freedom. the χ2-value is then given by"
2224,1,"['degrees of freedom', 'distribution', 'normal', 'null hypothesis', 'normal distribution', 'hypothesis']", GoodnessofFit Test,seg_257,"since the computed χ2-value is less than χ20.05 = 7.815 for 3 degrees of freedom, we have no reason to reject the null hypothesis and conclude that the normal distribution with μ = 3.5 and σ = 0.7 provides a good fit for the distribution of battery lives."
2225,1,"['tests', 'confidence intervals', 'data', 'intervals', 'distribution', 'normality', 'statistical', 'confidence', 'test']", GoodnessofFit Test,seg_257,"the chi-squared goodness-of-fit test is an important resource, particularly since so many statistical procedures in practice depend, in a theoretical sense, on the assumption that the data gathered come from a specific type of distribution. as we have already seen, the normality assumption is often made. in the chapters that follow, we shall continue to make normality assumptions in order to provide a theoretical basis for certain tests and confidence intervals."
2226,1,"['estimators', 'statistic', 'random', 'random sample', 'sample', 'population', 'standard', 'standard deviation', 'tests', 'distribution', 'test', 'deviation', 'normality', 'normal', 'population standard deviation', 'normal distribution']", GoodnessofFit Test,seg_257,"there are tests in the literature that are more powerful than the chi-squared test for testing normality. one such test is called geary’s test. this test is based on a very simple statistic which is a ratio of two estimators of the population standard deviation σ. suppose that a random sample x1, x2, . . . , xn is taken from a normal distribution, n(μ, σ). consider the ratio"
2227,1,"['distribution', 'normality', 'normal', 'estimator', 'hypothesis']", GoodnessofFit Test,seg_257,"the reader should recognize that the denominator is a reasonable estimator of σ whether the distribution is normal or not. the numerator is a good estimator of σ if the distribution is normal but may overestimate or underestimate σ when there are departures from normality. thus, values of u differing considerably from 1.0 represent the signal that the hypothesis of normality should be rejected."
2228,1,"['test statistic', 'standardization', 'normality', 'samples', 'statistic', 'test']", GoodnessofFit Test,seg_257,"for large samples, a reasonable test is based on approximate normality of u . the test statistic is then a standardization of u , given by"
2229,1,"['data', 'normality', 'test', 'critical region', 'hypothesis']", GoodnessofFit Test,seg_257,"of course, the test procedure involves the two-sided critical region. we compute a value of z from the data and do not reject the hypothesis of normality when"
2230,1,['test'], GoodnessofFit Test,seg_257,"a paper dealing with geary’s test is cited in the bibliography (geary, 1947)."
2231,1,"['levels', 'contingency table', 'independence', 'random sample', 'random', 'sample', 'frequencies', 'test', 'hypothesis', 'independent', 'table', 'variables']", Test for Independence Categorical Data,seg_259,"the chi-squared test procedure discussed in section 10.11 can also be used to test the hypothesis of independence of two variables of classification. suppose that we wish to determine whether the opinions of the voting residents of the state of illinois concerning a new tax reform are independent of their levels of income. members of a random sample of 1000 registered voters from the state of illinois are classified as to whether they are in a low, medium, or high income bracket and whether or not they favor the tax reform. the observed frequencies are presented in table 10.6, which is known as a contingency table."
2232,1,"['contingency table', 'table']", Test for Independence Categorical Data,seg_259,table 10.6: 2 × 3 contingency table
2233,1,['level'], Test for Independence Categorical Data,seg_259,income level tax reform low medium high total for 182 213 203 598 against 154 138 110 402 total 336 351 313 1000
2234,1,"['column totals', 'row and column totals', 'marginal', 'frequencies', 'table', 'contingency table', 'independence', 'events', 'level', 'null hypothesis', 'hypothesis']", Test for Independence Categorical Data,seg_259,"a contingency table with r rows and c columns is referred to as an r × c table (“r × c” is read “r by c”). the row and column totals in table 10.6 are called marginal frequencies. our decision to accept or reject the null hypothesis, h0, of independence between a voter’s opinion concerning the tax reform and his or her level of income is based upon how good a fit we have between the observed frequencies in each of the 6 cells of table 10.6 and the frequencies that we would expect for each cell under the assumption that h0 is true. to find these expected frequencies, let us define the following events:"
2235,1,['level'], Test for Independence Categorical Data,seg_259,l: a person selected is in the low-income level.
2236,1,['level'], Test for Independence Categorical Data,seg_259,m : a person selected is in the medium-income level.
2237,1,['level'], Test for Independence Categorical Data,seg_259,h: a person selected is in the high-income level.
2238,0,[], Test for Independence Categorical Data,seg_259,f : a person selected is for the tax reform.
2239,0,[], Test for Independence Categorical Data,seg_259,a: a person selected is against the tax reform.
2240,1,"['marginal', 'frequencies', 'probability']", Test for Independence Categorical Data,seg_259,"by using the marginal frequencies, we can list the following probability estimates:"
2241,1,"['variables', 'independent']", Test for Independence Categorical Data,seg_259,"now, if h0 is true and the two variables are independent, we should have"
2242,1,"['sample', 'estimated', 'frequencies', 'observations', 'probability', 'number of observations']", Test for Independence Categorical Data,seg_259,"the expected frequencies are obtained by multiplying each cell probability by the total number of observations. as before, we round these frequencies to one decimal. thus, the expected number of low-income voters in our sample who favor the tax reform is estimated to be"
2243,1,['frequency'], Test for Independence Categorical Data,seg_259,when h0 is true. the general rule for obtaining the expected frequency of any cell is given by the following formula:
2244,1,"['frequency ', 'frequency', 'row total']", Test for Independence Categorical Data,seg_259,(column total)× (row total) expected frequency = . grand total
2245,1,"['degrees of freedom', 'table', 'marginal', 'frequencies', 'frequency', 'associated', 'test']", Test for Independence Categorical Data,seg_259,"the expected frequency for each cell is recorded in parentheses beside the actual observed value in table 10.7. note that the expected frequencies in any row or column add up to the appropriate marginal total. in our example, we need to compute only two expected frequencies in the top row of table 10.7 and then find the others by subtraction. the number of degrees of freedom associated with the chi-squared test used here is equal to the number of cell frequencies that may be filled in freely when we are given the marginal totals and the grand total, and in this illustration that number is 2. a simple formula providing the correct number of degrees of freedom is"
2246,1,['frequencies'], Test for Independence Categorical Data,seg_259,table 10.7: observed and expected frequencies
2247,1,['level'], Test for Independence Categorical Data,seg_259,income level tax reform low medium high total for 182 (200.9) 213 (209.9) 203 (187.2) 598 against 154 (135.1) 138 (141.1) 110 (125.8) 402 total 336 351 313 1000
2248,1,"['degrees of freedom', 'independence', 'test', 'null hypothesis', 'hypothesis']", Test for Independence Categorical Data,seg_259,"hence, for our example, v = (2− 1)(3− 1) = 2 degrees of freedom. to test the null hypothesis of independence, we use the following decision criterion."
2249,1,"['contingency table', 'summation', 'table']", Test for Independence Categorical Data,seg_259,where the summation extends over all rc cells in the r × c contingency table.
2250,1,"['degrees of freedom', 'independence', 'significance', 'null hypothesis', 'hypothesis']", Test for Independence Categorical Data,seg_259,"if χ2 > χα2 with v = (r − 1)(c − 1) degrees of freedom, reject the null hypothesis of independence at the α-level of significance; otherwise, fail to reject the null hypothesis."
2251,0,[], Test for Independence Categorical Data,seg_259,"applying this criterion to our example, we find that"
2252,1,"['degrees of freedom', 'independent', 'table', 'level', 'null hypothesis', 'hypothesis']", Test for Independence Categorical Data,seg_259,from table a.5 we find that χ20.05 = 5.991 for v = (2 − 1)(3 − 1) = 2 degrees of freedom. the null hypothesis is rejected and we conclude that a voter’s opinion concerning the tax reform and his or her level of income are not independent.
2253,1,"['degrees of freedom', 'table', 'contingency table', 'frequencies', 'discrete', 'distribution', 'statistic', 'degree of freedom', 'continuous']", Test for Independence Categorical Data,seg_259,"it is important to remember that the statistic on which we base our decision has a distribution that is only approximated by the chi-squared distribution. the computed χ2-values depend on the cell frequencies and consequently are discrete. the continuous chi-squared distribution seems to approximate the discrete sampling distribution of χ2 very well, provided that the number of degrees of freedom is greater than 1. in a 2 × 2 contingency table, where we have only 1 degree of freedom, a correction called yates’ correction for continuity is applied. the corrected formula then becomes"
2254,1,"['sample', 'frequencies', 'results', 'statistics', 'probability', 'test']", Test for Independence Categorical Data,seg_259,"if the expected cell frequencies are large, the corrected and uncorrected results are almost the same. when the expected frequencies are between 5 and 10, yates’ correction should be applied. for expected frequencies less than 5, the fisher-irwin exact test should be used. a discussion of this test may be found in basic concepts of probability and statistics by hodges and lehmann (2005; see the bibliography). the fisher-irwin test may be avoided, however, by choosing a larger sample."
2255,1,"['sample', 'column totals', 'method', 'row and column totals', 'table', 'contingency table', 'random sample', 'independence', 'random']", Test for Homogeneity,seg_261,"when we tested for independence in section 10.12, a random sample of 1000 voters was selected and the row and column totals for our contingency table were determined by chance. another type of problem for which the method of section 10.12 applies is one in which either the row or column totals are predetermined. suppose, for example, that we decide in advance to select 200 democrats, 150 republicans, and 150 independents from the voters of the state of north carolina and record whether they are for a proposed abortion law, against it, or undecided. the observed responses are given in table 10.8."
2256,1,['frequencies'], Test for Homogeneity,seg_261,table 10.8: observed frequencies
2257,1,['independent'], Test for Homogeneity,seg_261,political affiliation abortion law democrat republican independent total for 82 70 62 214 against 93 62 67 222 undecided 25 18 21 64 total 200 150 150 500
2258,1,"['homogeneity', 'homogeneous', 'independence', 'categories', 'test', 'hypothesis']", Test for Homogeneity,seg_261,"now, rather than test for independence, we test the hypothesis that the population proportions within each row are the same. that is, we test the hypothesis that the proportions of democrats, republicans, and independents favoring the abortion law are the same; the proportions of each political affiliation against the law are the same; and the proportions of each political affiliation that are undecided are the same. we are basically interested in determining whether the three categories of voters are homogeneous with respect to their opinions concerning the proposed abortion law. such a test is called a test for homogeneity."
2259,1,"['column totals', 'homogeneity', 'row and column totals', 'frequencies']", Test for Homogeneity,seg_261,"assuming homogeneity, we again find the expected cell frequencies by multiplying the corresponding row and column totals and then dividing by the grand"
2260,1,"['table', 'data', 'statistic', 'process']", Test for Homogeneity,seg_261,total. the analysis then proceeds using the same chi-squared statistic as before. we illustrate this process for the data of table 10.8 in the following example.
2261,1,"['table', 'data', 'level', 'significance', 'test', 'level of significance', 'hypothesis']", Test for Homogeneity,seg_261,"example 10.14: referring to the data of table 10.8, test the hypothesis that opinions concerning the proposed abortion law are the same within each political affiliation. use a 0.05 level of significance."
2262,0,[], Test for Homogeneity,seg_261,"solution : 1. h0: for each opinion, the proportions of democrats, republicans, and independents are the same."
2263,0,[], Test for Homogeneity,seg_261,"2. h1: for at least one opinion, the proportions of democrats, republicans, and"
2264,0,[], Test for Homogeneity,seg_261,independents are not the same.
2265,1,"['degrees of freedom', 'critical region']", Test for Homogeneity,seg_261,4. critical region: χ2 > 9.488 with v = 4 degrees of freedom.
2266,1,['frequency'], Test for Homogeneity,seg_261,"5. computations: using the expected cell frequency formula on page 375, we"
2267,1,"['frequencies', 'table']", Test for Homogeneity,seg_261,need to compute 4 cell frequencies. all other frequencies are found by subtraction. the observed and expected cell frequencies are displayed in table 10.9.
2268,1,['frequencies'], Test for Homogeneity,seg_261,table 10.9: observed and expected frequencies
2269,1,['independent'], Test for Homogeneity,seg_261,political affiliation abortion law democrat republican independent total for 82 (85.6) 70 (64.2) 62 (64.2) 214 against 93 (88.8) 62 (66.6) 67 (66.6) 222 undecided 25 (25.6) 18 (19.2) 21 (19.2) 64 total 200 150 150 500
2270,0,[], Test for Homogeneity,seg_261,6. decision: do not reject h0. there is insufficient evidence to conclude that
2271,0,[], Test for Homogeneity,seg_261,"the proportions of democrats, republicans, and independents differ for each stated opinion."
2272,1,"['parameters', 'homogeneity', 'statistic', 'binomial', 'test', 'null hypothesis', 'hypothesis']", Test for Homogeneity,seg_261,"the chi-squared statistic for testing for homogeneity is also applicable when testing the hypothesis that k binomial parameters have the same value. this is, therefore, an extension of the test presented in section 10.9 for determining differences between two proportions to a test for determining differences among k proportions. hence, we are interested in testing the null hypothesis"
2273,1,"['alternative hypothesis', 'independent', 'random samples', 'table', 'contingency table', 'populations', 'data', 'samples', 'population', 'random', 'test', 'hypothesis']", Test for Homogeneity,seg_261,"against the alternative hypothesis, h1, that the population proportions are not all equal. to perform this test, we first observe independent random samples of size n1, n2, . . . , nk from the k populations and arrange the data in a 2 × k contingency table, table 10.10."
2274,1,"['independent', 'samples', 'binomial']", Test for Homogeneity,seg_261,table 10.10: k independent binomial samples
2275,1,"['successes', 'failures']", Test for Homogeneity,seg_261,sample: 1 2 · · · k successes x1 x2 · · · xk failures n1 − x1 n2 − x2 · · · nk − xk
2276,1,"['random samples', 'homogeneity', 'frequencies', 'independence', 'samples', 'statistic', 'random', 'test']", Test for Homogeneity,seg_261,"depending on whether the sizes of the random samples were predetermined or occurred at random, the test procedure is identical to the test for homogeneity or the test for independence. therefore, the expected cell frequencies are calculated as before and substituted, together with the observed frequencies, into the chi-squared statistic"
2277,0,[], Test for Homogeneity,seg_261,degrees of freedom.
2278,1,['critical region'], Test for Homogeneity,seg_261,"by selecting the appropriate upper-tail critical region of the form χ2 > χα2 , we can now reach a decision concerning h0."
2279,1,"['set', 'data', 'table']", Test for Homogeneity,seg_261,"example 10.15: in a shop study, a set of data was collected to determine whether or not the proportion of defectives produced was the same for workers on the day, evening, and night shifts. the data collected are shown in table 10.11."
2280,1,['data'], Test for Homogeneity,seg_261,table 10.11: data for example 10.15
2281,0,[], Test for Homogeneity,seg_261,shift: day evening night defectives 45 55 70 nondefectives 905 890 870
2282,1,"['level', 'significance', 'level of significance']", Test for Homogeneity,seg_261,use a 0.025 level of significance to determine if the proportion of defectives is the same for all three shifts.
2283,0,[], Test for Homogeneity,seg_261,"solution : let p1, p2, and p3 represent the true proportions of defectives for the day, evening, and night shifts, respectively."
2284,1,"['degrees of freedom', 'critical region']", Test for Homogeneity,seg_261,4. critical region: χ2 > 7.378 for v = 2 degrees of freedom.
2285,1,['frequencies'], Test for Homogeneity,seg_261,5. computations: corresponding to the observed frequencies o1 = 45 and o2 =
2286,1,"['frequencies', 'table']", Test for Homogeneity,seg_261,all other expected frequencies are found by subtraction and are displayed in table 10.12.
2287,1,['frequencies'], Test for Homogeneity,seg_261,table 10.12: observed and expected frequencies
2288,0,[], Test for Homogeneity,seg_261,"6. decision: we do not reject h0 at α = 0.025. nevertheless, with the above"
2289,0,[], Test for Homogeneity,seg_261,"p -value computed, it would certainly be dangerous to conclude that the proportion of defectives produced is the same for all shifts."
2290,1,"['graphics', 'statistical graphics', 'statistics', 'hypothesis testing', 'statistical', 'test statistics', 'test', 'numerical', 'hypothesis']", Test for Homogeneity,seg_261,"often a complete study involving the use of statistical methods in hypothesis testing can be illustrated for the scientist or engineer using both test statistics, complete with p -values and statistical graphics. the graphics supplement the numerical diagnostics with pictures that show intuitively why the p -values appear as they do, as well as how reasonable (or not) the operative assumptions are."
2291,1,"['graphical', 'statistics', 'data', 'consistency']", TwoSample Case Study,seg_263,"in this section, we consider a study involving a thorough graphical and formal analysis, along with annotated computer printout and conclusions. in a data analysis study conducted by personnel at the statistics consulting center at virginia tech, two different materials, alloy a and alloy b, were compared in terms of breaking strength. alloy b is more expensive, but it should certainly be adopted if it can be shown to be stronger than alloy a. the consistency of performance of the two alloys should also be taken into account."
2292,1,"['samples', 'data', 'table']", TwoSample Case Study,seg_263,"random samples of beams made from each alloy were selected, and strength was measured in units of 0.001-inch deflection as a fixed force was applied at both ends of the beam. twenty specimens were used for each of the two alloys. the data are given in table 10.13."
2293,1,['average'], TwoSample Case Study,seg_263,it is important that the engineer compare the two alloys. of concern is average strength and reproducibility. it is of interest to determine if there is a severe
2294,1,"['case', 'data']", TwoSample Case Study,seg_263,table 10.13: data for two-sample case study
2295,1,"['plots', 'samples', 'normality', 'normal']", TwoSample Case Study,seg_263,violation of the normality assumption required of both the tand f -tests. figures 10.21 and 10.22 are normal quantile-quantile plots of the samples of the two alloys.
2296,1,"['sample', 'standard deviations', 'sample means', 'plots', 'deviations', 'normality', 'mean', 'standard']", TwoSample Case Study,seg_263,"there does not appear to be any serious violation of the normality assumption. in addition, figure 10.23 shows two box-and-whisker plots on the same graph. the box-and-whisker plots suggest that there is no appreciable difference in the variability of deflection for the two alloys. however, it seems that the mean deflection for alloy b is significantly smaller, suggesting, at least graphically, that alloy b is stronger. the sample means and standard deviations are"
2297,1,"['test', 'variances']", TwoSample Case Study,seg_263,"the sas printout for the proc ttest is shown in figure 10.24. the f -test suggests no significant difference in variances (p = 0.4709), and the two-sample t-statistic for testing"
2298,1,"['graphical', 'results', 'information', 'variances']", TwoSample Case Study,seg_263,"(t = 3.59, p = 0.0009) rejects h0 in favor of h1 and thus confirms what the graphical information suggests. here we use the t-test that pools the two-sample variances together in light of the results of the f -test. on the basis of this analysis, the adoption of alloy b would seem to be in order."
2299,1,"['sample', 'statistician', 'results', 'statisticians', 'data', 'case', 'statistically significant', 'statistical', 'statistical significance', 'significance', 'average']", TwoSample Case Study,seg_263,"while the statistician may feel quite comfortable with the results of the comparison between the two alloys in the case study above, a dilemma remains for the engineer. the analysis demonstrated a statistically significant improvement with the use of alloy b. however, is the difference found really worth pursuing, since alloy b is more expensive? this illustration highlights a very important issue often overlooked by statisticians and data analysts—the distinction between statistical significance and engineering or scientific significance. here the average difference in deflection is ȳa − ȳb = 0.00385 inch. in a complete analysis, the engineer must determine if the difference is sufficient to justify the extra cost in the long run. this is an economic and engineering issue. the reader should understand that a statistically significant difference merely implies that the difference in the sample"
2300,1,"['plot', 'data', 'normal']", TwoSample Case Study,seg_263,figure 10.21: normal quantile-quantile plot of figure 10.22: normal quantile-quantile plot of data for alloy a. data for alloy b.
2301,1,['plots'], TwoSample Case Study,seg_263,figure 10.23: box-and-whisker plots for both alloys.
2302,1,"['sample', 'variability', 'observations', 'biased', 'data', 'deviations', 'mean', 'population']", TwoSample Case Study,seg_263,"means found in the data could hardly have occurred by chance. it does not imply that the difference in the population means is profound or particularly significant in the context of the problem. for example, in section 10.4, an annotated computer printout was used to show evidence that a ph meter was, in fact, biased. that is, it does not demonstrate a mean ph of 7.00 for the material on which it was tested. but the variability among the observations in the sample is very small. the engineer may decide that the small deviations from 7.0 render the ph meter adequate."
2303,1,"['tests', 'tests of hypotheses', 'hypotheses']", TwoSample Case Study,seg_263,382 chapter 10 oneand two-sample tests of hypotheses
2304,1,['mean'], TwoSample Case Study,seg_263,the ttest procedure alloy n mean std dev std err alloy a 20 83.55 3.6631 0.8191 alloy b 20 79.7 3.0967 0.6924
2305,1,['variances'], TwoSample Case Study,seg_263,variances df t value pr > |t| equal 38 3.59 0.0009 unequal 37 3.59 0.0010 equality of variances num df den df f value pr > f 19 19 1.40 0.4709
2306,1,['data'], TwoSample Case Study,seg_263,figure 10.24: annotated sas printout for alloy data.
2307,1,"['alternative hypothesis', 'statistics', 'case', 'null hypothesis', 'hypothesis']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_269,"one of the easiest ways to misuse statistics relates to the final scientific conclusion drawn when the analyst does not reject the null hypothesis h0. in this text, we have attempted to make clear what the null hypothesis means and what the alternative means, and to stress that, in a large sense, the alternative hypothesis is much more important. put in the form of an example, if an engineer is attempting to compare two gauges using a two-sample t-test, and h0 is “the gauges are equivalent” while h1 is “the gauges are not equivalent,” not rejecting h0 does not lead to the conclusion of equivalent gauges. in fact, a case can be made for never writing or saying “accept h0”! not rejecting h0 merely implies insufficient evidence. depending on the nature of the hypothesis, a lot of possibilities are still not ruled out."
2308,1,"['confidence', 'interval', 'case', 'confidence interval']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_269,"in chapter 9, we considered the case of the large-sample confidence interval"
2309,1,"['confidence intervals', 'case', 'probability', 'sample', 'graphical', 'intervals', 'hypothesis testing', 'confidence', 'tests', 'limit', 'distribution', 'central limit theorem', 'plotting', 'hypothesis', 'normality', 'normal', 'probability plotting']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_269,"in hypothesis testing, replacing σ by s for n < 30 is risky. if n ≥ 30 and the distribution is not normal but somehow close to normal, the central limit theorem is being called upon and one is relying on the fact that with n ≥ 30, s ≈ σ. of course, any t-test is accompanied by the concomitant assumption of normality. as in the case of confidence intervals, the t-test is relatively robust to normality. however, one should still use normal probability plotting, goodness-of-fit tests, or other graphical procedures when the sample is not too small."
2310,1,['estimation'], Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_269,most of the chapters in this text include discussions whose purpose is to relate the chapter in question to other material that will follow. the topics of estimation
2311,1,"['prediction', 'associated', 'process', 'sample', 'estimate', 'data', 'hypothesis testing', 'statistical model', 'tests', 'statistical', 'sample size', 'model', 'parameters', 'regression', 'hypothesis']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_269,"and hypothesis testing are both used in a major way in nearly all of the techniques that fall under the umbrella of “statistical methods.” this will be readily noted by students who advance to chapters 11 through 16. it will be obvious that these chapters depend heavily on statistical modeling. students will be exposed to the use of modeling in a wide variety of applications in many scientific and engineering fields. it will become obvious quite quickly that the framework of a statistical model is useless unless data are available with which to estimate parameters in the formulated model. this will become particularly apparent in chapters 11 and 12 as we introduce the notion of regression models. the concepts and theory associated with chapter 9 will carry over. as far as material in the present chapter is concerned, the framework of hypothesis testing, p -values, power of tests, and choice of sample size will collectively play a major role. since initial model formulation quite often must be supplemented by model editing before the analyst is sufficiently comfortable to use the model for either process understanding or prediction, chapters 11, 12, and 15 make major use of hypothesis testing to supplement diagnostic measures that are used to assess model quality."
2312,0,[], Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_269,this page intentionally left blank
2313,1,"['linear', 'experimental', 'levels', 'independent', 'method', 'variables', 'prediction', 'dependent', 'dependent variables', 'sets', 'process', 'response']", Introduction to Linear Regression,seg_273,"often, in practice, one is called upon to solve problems involving sets of variables when it is known that there exists some inherent relationship among the variables. for example, in an industrial situation it may be known that the tar content in the outlet stream in a chemical process is related to the inlet temperature. it may be of interest to develop a method of prediction, that is, a procedure for estimating the tar content for various levels of the inlet temperature from experimental information. now, of course, it is highly likely that for many example runs in which the inlet temperature is the same, say 130◦c, the outlet tar content will not be the same. this is much like what happens when we study several automobiles with the same engine volume. they will not all have the same gas mileage. houses in the same part of the country that have the same square footage of living space will not all be sold for the same price. tar content, gas mileage (mpg), and the price of houses (in thousands of dollars) are natural dependent variables, or responses, in these three scenarios. inlet temperature, engine volume (cubic feet), and square feet of living space are, respectively, natural independent variables, or regressors. a reasonable form of a relationship between the response y and the regressor x is the linear relationship"
2314,1,"['intercept', 'slope']", Introduction to Linear Regression,seg_273,"where, of course, β0 is the intercept and β1 is the slope. the relationship is illustrated in figure 11.1."
2315,1,"['random', 'variables', 'regression analysis', 'regression', 'probabilistic']", Introduction to Linear Regression,seg_273,"if the relationship is exact, then it is a deterministic relationship between two scientific variables and there is no random or probabilistic component to it. however, in the examples listed above, as well as in countless other scientific and engineering phenomena, the relationship is not deterministic (i.e., a given x does not always give the same value for y ). as a result, important problems here are probabilistic in nature since the relationship above cannot be viewed as being exact. the concept of regression analysis deals with finding the best relationship"
2316,1,"['linear', 'intercept', 'slope']", Introduction to Linear Regression,seg_273,figure 11.1: a linear relationship; β0: intercept; β1: slope.
2317,1,"['prediction', 'response']", Introduction to Linear Regression,seg_273,"between y and x, quantifying the strength of that relationship, and using methods that allow for prediction of the response values given values of the regressor x."
2318,1,"['independent', 'case', 'independent variable', 'regression', 'multiple regression', 'variable', 'response']", Introduction to Linear Regression,seg_273,"in many applications, there will be more than one regressor (i.e., more than one independent variable that helps to explain y ). for example, in the case where the response is the price of a house, one would expect the age of the house to contribute to the explanation of the price, so in this case the multiple regression structure might be written"
2319,1,"['case', 'regression', 'multiple regression', 'samples', 'loss', 'response']", Introduction to Linear Regression,seg_273,"where y is price, x1 is square footage, and x2 is age in years. in the next chapter, we will consider problems with multiple regressors. the resulting analysis is termed multiple regression, while the analysis of the single regressor case is called simple regression. as a second illustration of multiple regression, a chemical engineer may be concerned with the amount of hydrogen lost from samples of a particular metal when the material is placed in storage. in this case, there may be two inputs, storage time x1 in hours and storage temperature x2 in degrees centigrade. the response would then be hydrogen loss y in parts per million."
2320,1,"['sample', 'linear', 'simple linear regression', 'ordered pair', 'random sample', 'case', 'regression', 'variable', 'random variable', 'samples', 'set', 'linear regression', 'random', 'vary']", Introduction to Linear Regression,seg_273,"in this chapter, we deal with the topic of simple linear regression, treating only the case of a single regressor variable in which the relationship between y and x is linear. for the case of more than one regressor variable, the reader is referred to chapter 12. denote a random sample of size n by the set {(xi, yi); i = 1, 2, . . . , n}. if additional samples were taken using exactly the same values of x, we should expect the y values to vary. hence, the value yi in the ordered pair (xi, yi) is a value of some random variable yi."
2321,1,"['variables', 'regression analysis', 'regression', 'random']", The Simple Linear Regression SLR Model,seg_275,"we have already confined the terminology regression analysis to situations in which relationships among variables are not deterministic (i.e., not exact). in other words, there must be a random component to the equation that relates the variables."
2322,1,"['model', 'linear', 'parameters', 'range', 'approximation', 'case', 'regression', 'random', 'linear equation', 'response']", The Simple Linear Regression SLR Model,seg_275,"this random component takes into account considerations that are not being measured or, in fact, are not understood by the scientists or engineers. indeed, in most applications of regression, the linear equation, say y = β0 + β1x, is an approximation that is a simplification of something unknown and much more complicated. for example, in our illustration involving the response y= tar content and x = inlet temperature, y = β0 + β1x is likely a reasonable approximation that may be operative within a confined range on x. more often than not, the models that are simplifications of more complicated and unknown structures are linear in nature (i.e., linear in the parameters β0 and β1 or, in the case of the model involving the price, size, and age of the house, linear in the parameters β0, β1, and β2). these linear structures are simple and empirical in nature and are thus called empirical models."
2323,1,"['simple linear regression', 'set', 'random', 'linear', 'data', 'random variable', 'statistical model', 'statistical', 'model', 'statistician', 'regression', 'linear regression', 'response', 'independent', 'independent variable', 'variable']", The Simple Linear Regression SLR Model,seg_275,"an analysis of the relationship between y and x requires the statement of a statistical model. a model is often used by a statistician as a representation of an ideal that essentially defines how we perceive that the data were generated by the system in question. the model must include the set {(xi, yi); i = 1, 2, . . . , n} of data involving n pairs of (x, y) values. one must bear in mind that the value yi depends on xi via a linear structure that also has the random component involved. the basis for the use of a statistical model relates to how the random variable y moves with x and the random component. the model also includes what is assumed about the statistical properties of the random component. the statistical model for simple linear regression is given below. the response y is related to the independent variable x through the equation"
2324,1,"['linear', 'model', 'regression model', 'regression']", The Simple Linear Regression SLR Model,seg_275,simple linear y = β0 + β1x+ . regression model
2325,1,"['error', 'parameters', 'residual', 'intercept', 'random variable', 'variable', 'random', 'variance', 'residual variance', 'slope']", The Simple Linear Regression SLR Model,seg_275,"in the above, β0 and β1 are unknown intercept and slope parameters, respectively, and is a random variable that is assumed to be distributed with e( ) = 0 and var( ) = σ2. the quantity σ2 is often called the error variance or residual variance."
2326,1,"['observations', 'case', 'random', 'linear', 'approximation', 'data', 'random variable', 'population', 'true regression line', 'error', 'model', 'regression', 'variance', 'regression line', 'estimated', 'homogeneous', 'errors', 'variable', 'random error']", The Simple Linear Regression SLR Model,seg_275,"from the model above, several things become apparent. the quantity y is a random variable since is random. the value x of the regressor variable is not random and, in fact, is measured with negligible error. the quantity , often called a random error or random disturbance, has constant variance. this portion of the assumptions is often called the homogeneous variance assumption. the presence of this random error, , keeps the model from becoming simply a deterministic equation. now, the fact that e( ) = 0 implies that at a specific x the y-values are distributed around the true, or population, regression line y = β0 + β1x. if the model is well chosen (i.e., there are no additional important regressors and the linear approximation is good within the ranges of the data), then positive and negative errors around the true regression are reasonable. we must keep in mind that in practice β0 and β1 are not known and must be estimated from data. in addition, the model described above is conceptual in nature. as a result, we never observe the actual values in practice and thus we can never draw the true regression line (but we assume it is there). we can only draw an estimated line. figure 11.2 depicts the nature of hypothetical (x, y) data scattered around a true regression line for a case in which only n = 5 observations are available. let us emphasize that what we see in figure 11.2 is not the line that is used by the"
2327,1,"['mean', 'regression']", The Simple Linear Regression SLR Model,seg_275,"scientist or engineer. rather, the picture merely describes what the assumptions mean! the regression that the user has at his or her disposal will now be described."
2328,1,"['true regression line', 'regression line', 'data', 'regression']", The Simple Linear Regression SLR Model,seg_275,"figure 11.2: hypothetical (x, y) data scattered around the true regression line for n = 5."
2329,1,"['estimated', 'method', 'estimate', 'estimation', 'regression coefficients', 'regression analysis', 'regression', 'regression line', 'coefficients']", The Simple Linear Regression SLR Model,seg_275,"an important aspect of regression analysis is, very simply, to estimate the parameters β0 and β1 (i.e., estimate the so-called regression coefficients). the method of estimation will be discussed in the next section. suppose we denote the estimates b0 for β0 and b1 for β1. then the estimated or fitted regression line is given by"
2330,1,"['estimate', 'predicted', 'data', 'regression', 'true regression line', 'regression line', 'fitted line']", The Simple Linear Regression SLR Model,seg_275,"where ŷ is the predicted or fitted value. obviously, the fitted line is an estimate of the true regression line. we expect that the fitted line should be closer to the true regression line when a large amount of data are available. in the following example, we illustrate the fitted line for a real-life pollution study."
2331,1,"['experimental', 'table', 'data', 'samples', 'control', 'percent']", The Simple Linear Regression SLR Model,seg_275,"one of the more challenging problems confronting the water pollution control field is presented by the tanning industry. tannery wastes are chemically complex. they are characterized by high values of chemical oxygen demand, volatile solids, and other pollution measures. consider the experimental data in table 11.1, which were obtained from 33 samples of chemically treated waste in a study conducted at virginia tech. readings on x, the percent reduction in total solids, and y, the percent reduction in chemical oxygen demand, were recorded."
2332,1,"['table', 'variables', 'data', 'scatter diagram']", The Simple Linear Regression SLR Model,seg_275,"the data of table 11.1 are plotted in a scatter diagram in figure 11.3. from an inspection of this scatter diagram, it can be seen that the points closely follow a straight line, indicating that the assumption of linearity between the two variables appears to be reasonable."
2333,0,[], The Simple Linear Regression SLR Model,seg_275,table 11.1: measures of reduction in solids and oxygen demand
2334,1,"['scatter diagram', 'regression']", The Simple Linear Regression SLR Model,seg_275,figure 11.3: scatter diagram with regression lines.
2335,1,"['method', 'estimation', 'regression', 'true regression line', 'regression line', 'scatter diagram']", The Simple Linear Regression SLR Model,seg_275,"the fitted regression line and a hypothetical true regression line are shown on the scatter diagram of figure 11.3. this example will be revisited as we move on to the method of estimation, discussed in section 11.3."
2336,1,"['simple linear regression', 'linear', 'model', 'graphical', 'regression model', 'regression', 'normality', 'linear regression', 'linear regression model']", The Simple Linear Regression SLR Model,seg_275,it may be instructive to revisit the simple linear regression model presented previously and discuss in a graphical sense how it relates to the so-called true regression. let us expand on figure 11.2 by illustrating not merely where the i fall on a graph but also what the implication is of the normality assumption on the i.
2337,1,"['simple linear regression', 'observations', 'linear', 'mean', 'true regression line', 'distributions', 'model', 'regression', 'distribution', 'linear regression', 'variance', 'response', 'regression line', 'deviation', 'normal', 'normal distribution']", The Simple Linear Regression SLR Model,seg_275,"suppose we have a simple linear regression with n = 6 evenly spaced values of x and a single y-value at each x. consider the graph in figure 11.4. this illustration should give the reader a clear representation of the model and the assumptions involved. the line in the graph is the true regression line. the points plotted are actual (y, x) points which are scattered about the line. each point is on its own normal distribution with the center of the distribution (i.e., the mean of y) falling on the line. this is certainly expected since e(y ) = β0 + β1x. as a result, the true regression line goes through the means of the response, and the actual observations are on the distribution around the means. note also that all distributions have the same variance, which we referred to as σ2. of course, the deviation between an individual y and the point on the line will be its individual value. this is clear since"
2338,1,['variance'], The Simple Linear Regression SLR Model,seg_275,"thus, at a given x, y and the corresponding both have variance σ2."
2339,1,"['observations', 'regression', 'true regression line', 'regression line']", The Simple Linear Regression SLR Model,seg_275,figure 11.4: individual observations around true regression line.
2340,1,"['regression line', 'regression', 'random variable', 'variable', 'mean', 'random', 'true regression line']", The Simple Linear Regression SLR Model,seg_275,note also that we have written the true regression line here as μy |x = β0+β1x in order to reaffirm that the line goes through the mean of the y random variable.
2341,1,"['estimated', 'method', 'estimates', 'data', 'regression', 'estimated regression line', 'regression line']", Least Squares and the Fitted Model,seg_277,"in this section, we discuss the method of fitting an estimated regression line to the data. this is tantamount to the determination of estimates b0 for β0 and b1"
2342,1,"['model', 'method', 'residual', 'estimation', 'predicted', 'least squares', 'information', 'method of least squares', 'error', 'fitted line']", Least Squares and the Fitted Model,seg_277,"for β1. this of course allows for the computation of predicted values from the fitted line ŷ = b0 + b1x and other types of analyses and diagnostic information that will ascertain the strength of the relationship and the adequacy of the fitted model. before we discuss the method of least squares estimation, it is important to introduce the concept of a residual. a residual is essentially an error in the fit of the model ŷ = b0 + b1x."
2343,1,"['model', 'residual', 'data', 'regression', 'set', 'error']", Least Squares and the Fitted Model,seg_277,"residual: error in given a set of regression data {(xi, yi); i = 1, 2, . . . , n} and a fitted model, ŷi = fit b0 + b1xi, the ith residual ei is given by"
2344,1,"['model', 'residuals', 'set']", Least Squares and the Fitted Model,seg_277,"obviously, if a set of n residuals is large, then the fit of the model is not good. small residuals are a sign of a good fit. another interesting relationship which is useful at times is the following:"
2345,1,"['model', 'residuals', 'errors']", Least Squares and the Fitted Model,seg_277,"the use of the above equation should result in clarification of the distinction between the residuals, ei, and the conceptual model errors, i. one must bear in mind that whereas the i are not observed, the ei not only are observed but also play an important role in the total analysis."
2346,1,"['model', 'parameters', 'estimate', 'data', 'set', 'statistical model', 'statistical', 'fitted line']", Least Squares and the Fitted Model,seg_277,"figure 11.5 depicts the line fit to this set of data, namely ŷ = b0+ b1x, and the line reflecting the model μy |x = β0 + β1x. now, of course, β0 and β1 are unknown parameters. the fitted line is an estimate of the line produced by the statistical model. keep in mind that the line μy |x = β0 + β1x is not known."
2347,1,['residual'], Least Squares and the Fitted Model,seg_277,"figure 11.5: comparing i with the residual, ei."
2348,1,"['estimates', 'residual', 'residuals', 'errors', 'regression', 'sum of squares', 'residual sum of squares', 'regression line']", Least Squares and the Fitted Model,seg_277,"we shall find b0 and b1, the estimates of β0 and β1, so that the sum of the squares of the residuals is a minimum. the residual sum of squares is often called the sum of squares of the errors about the regression line and is denoted by sse. this"
2349,1,"['parameters', 'method', 'least squares', 'method of least squares']", Least Squares and the Fitted Model,seg_277,"minimization procedure for estimating the parameters is called the method of least squares. hence, we shall find a and b so as to minimize"
2350,1,"['normal equations', 'normal']", Least Squares and the Fitted Model,seg_277,"n n ∂(sse) ∂(sse) = −2∑(yi − b0 − b1xi), = −2∑(yi − b0 − b1xi)xi. ∂b0 ∂b1 i=1 i=1 setting the partial derivatives equal to zero and rearranging the terms, we obtain the equations (called the normal equations)"
2351,0,[], Least Squares and the Fitted Model,seg_277,which may be solved simultaneously to yield computing formulas for b0 and b1.
2352,1,"['sample', 'estimates', 'regression coefficients', 'least squares', 'regression', 'least squares estimates', 'coefficients']", Least Squares and the Fitted Model,seg_277,"estimating the given the sample {(xi, yi); i = 1, 2, . . . , n}, the least squares estimates b0 and b1 regression of the regression coefficients β0 and β1 are computed from the formulas coefficients"
2353,1,"['data', 'table']", Least Squares and the Fitted Model,seg_277,"the calculations of b0 and b1, using the data of table 11.1, are illustrated by the following example."
2354,1,"['table', 'estimate', 'data', 'regression', 'regression line']", Least Squares and the Fitted Model,seg_277,example 11.1: estimate the regression line for the pollution data of table 11.1. solution : 33 33 33 33
2355,1,"['estimated', 'regression', 'estimated regression line', 'regression line']", Least Squares and the Fitted Model,seg_277,"thus, the estimated regression line is given by"
2356,1,"['regression line', 'regression']", Least Squares and the Fitted Model,seg_277,"using the regression line of example 11.1, we would predict a 31% reduction in the chemical oxygen demand when the reduction in the total solids is 30%. the"
2357,1,"['estimate', 'estimates', 'experiment', 'table', 'population mean', 'data', 'observation', 'measurements', 'mean', 'population', 'error']", Least Squares and the Fitted Model,seg_277,"31% reduction in the chemical oxygen demand may be interpreted as an estimate of the population mean μy |30 or as an estimate of a new observation when the reduction in total solids is 30%. such estimates, however, are subject to error. even if the experiment were controlled so that the reduction in total solids was 30%, it is unlikely that we would measure a reduction in the chemical oxygen demand exactly equal to 31%. in fact, the original data recorded in table 11.1 show that measurements of 25% and 35% were recorded for the reduction in oxygen demand when the reduction in total solids was kept at 30%."
2358,1,"['least squares criterion', 'results', 'least squares', 'measuring', 'fitted line']", Least Squares and the Fitted Model,seg_277,"it should be noted that the least squares criterion is designed to provide a fitted line that results in a “closeness” between the line and the plotted points. there are many ways of measuring closeness. for example, one may wish to determine b0"
2359,1,"['predicted', 'residuals', 'least squares', 'sum of squares', 'deviations', 'set', 'fitted line']", Least Squares and the Fitted Model,seg_277,"i=1 i=1 these are both viable and reasonable methods. note that both of these, as well as the least squares procedure, result in forcing residuals to be “small” in some sense. one should remember that the residuals are the empirical counterpart to the values. figure 11.6 illustrates a set of residuals. one should note that the fitted line has predicted values as points on the line and hence the residuals are vertical deviations from points to the line. as a result, the least squares procedure produces a line that minimizes the sum of squares of vertical deviations from the points to the line."
2360,1,"['residuals', 'deviations']", Least Squares and the Fitted Model,seg_277,figure 11.6: residuals as vertical deviations.
2361,1,"['simple linear regression', 'linear', 'regression', 'correlation', 'linear regression']", Least Squares and the Fitted Model,seg_277,398 chapter 11 simple linear regression and correlation
2362,1,"['model', 'error']", Properties of the Least Squares Estimators,seg_281,in addition to the assumptions that the error term in the model
2363,1,"['independent', 'experiment', 'estimators', 'random variable', 'variable', 'mean', 'random', 'variance', 'variances']", Properties of the Least Squares Estimators,seg_281,"is a random variable with mean 0 and constant variance σ2, suppose that we make the further assumption that 1, 2, . . . , n are independent from run to run in the experiment. this provides a foundation for finding the means and variances for the estimators of β0 and β1."
2364,1,"['sample', 'random variables', 'parameters', 'experiment', 'variables', 'estimates', 'observations', 'random']", Properties of the Least Squares Estimators,seg_281,"it is important to remember that our values of b0 and b1, based on a given sample of n observations, are only estimates of true parameters β0 and β1. if the experiment is repeated over and over again, each time using the same fixed values of x, the resulting estimates of β0 and β1 will most likely differ from experiment to experiment. these different estimates may be viewed as values assumed by the random variables b0 and b1, while b0 and b1 are specific realizations."
2365,1,"['random variables', 'variables', 'random']", Properties of the Least Squares Estimators,seg_281,"since the values of x remain fixed, the values of b0 and b1 depend on the variations in the values of y or, more precisely, on the values of the random variables,"
2366,1,['mean'], Properties of the Least Squares Estimators,seg_281,"y1, y2, . . . , yn. the distributional assumptions imply that the yi, i = 1, 2, . . . , n, are also independently distributed, with mean μy |xi = β0 + β1xi and equal variances σ2; that is,"
2367,1,"['confidence interval', 'interval', 'variances', 'estimation', 'slope', 'intercept', 'hypothesis testing', 'interval estimation', 'confidence', 'estimator', 'unbiased', 'hypothesis']", Properties of the Least Squares Estimators,seg_281,"in what follows, we show that the estimator b1 is unbiased for β1 and demonstrate the variances of both b0 and b1. this will begin a series of developments that lead to hypothesis testing and confidence interval estimation on the intercept and slope."
2368,1,['estimator'], Properties of the Least Squares Estimators,seg_281,since the estimator
2369,1,['distribution'], Properties of the Least Squares Estimators,seg_281,"we may conclude from theorem 7.11 that b1 has a n(μb1 , σb1) distribution with"
2370,1,"['normally distributed', 'random variable', 'variable', 'random']", Properties of the Least Squares Estimators,seg_281,it can also be shown (review exercise 11.60 on page 438) that the random variable b0 is normally distributed with
2371,1,"['variance', 'mean']", Properties of the Least Squares Estimators,seg_281,n ∑ x2i mean μb0 = β0 and variance σb
2372,1,"['unbiased estimators', 'results', 'least squares', 'estimators', 'least squares estimators', 'unbiased']", Properties of the Least Squares Estimators,seg_281,"from the foregoing results, it is apparent that the least squares estimators for β0 and β1 are both unbiased estimators."
2373,1,"['model', 'estimate', 'random variation', 'variation', 'parameter', 'random', 'variance', 'error']", Properties of the Least Squares Estimators,seg_281,"to draw inferences on β0 and β1, it becomes necessary to arrive at an estimate of the parameter σ2 appearing in the two preceding variance formulas for b0 and b1. the parameter σ2, the model error variance, reflects random variation or"
2374,1,"['regression', 'regression line', 'error', 'variation']", Properties of the Least Squares Estimators,seg_281,"experimental error variation around the regression line. in much of what follows, it is advantageous to use the notation"
2375,1,"['sum of squares', 'error sum of squares', 'error']", Properties of the Least Squares Estimators,seg_281,now we may write the error sum of squares as follows:
2376,0,[], Properties of the Least Squares Estimators,seg_281,the final step following from the fact that b1 = sxy/sxx.
2377,1,"['unbiased', 'estimate']", Properties of the Least Squares Estimators,seg_281,theorem 11.1: an unbiased estimate of σ2 is
2378,0,[], Properties of the Least Squares Estimators,seg_281,the proof of theorem 11.1 is left as an exercise (see review exercise 11.59).
2379,1,"['degrees of freedom', 'estimates', 'observation', 'associated', 'estimator', 'mean', 'parameter', 'standard', 'fitted line', 'parameters', 'standard normal', 'regression', 'variance', 'deviation', 'estimated', 'sampling', 'deviations', 'normal', 'degree of freedom']", Properties of the Least Squares Estimators,seg_281,"one should observe the result of theorem 11.1 in order to gain some intuition about the estimator of σ2. the parameter σ2 measures variance or squared deviations between y values and their mean given by μy |x (i.e., squared deviations between y and β0 + β1x). of course, β0 + β1x is estimated by ŷ = b0 + b1x. thus, it would make sense that the variance σ2 is best depicted as a squared deviation of the typical observation yi from the estimated mean, ŷi, which is the corresponding point on the fitted line. thus, (yi − ŷi)2 values reveal the appropriate variance, much like the way (yi − ȳ)2 values measure variance when one is sampling in a nonregression scenario. in other words, ȳ estimates the mean in the latter simple situation, whereas ŷi estimates the mean of yi in a regression structure. now, what about the divisor n−2? in future sections, we shall note that these are the degrees of freedom associated with the estimator s2 of σ2. whereas in the standard normal i.i.d. scenario, one degree of freedom is subtracted from n in the denominator and a reasonable explanation is that one parameter is estimated, namely the mean μ by, say, ȳ, but in the regression problem, two parameters are estimated, namely β0 and β1 by b0 and b1. thus, the important parameter σ2, estimated by"
2380,1,"['residuals', 'mean', 'mean squared error', 'error']", Properties of the Least Squares Estimators,seg_281,"is called a mean squared error, depicting a type of mean (division by n− 2) of the squared residuals."
2381,1,"['linear', 'confidence intervals', 'prediction', 'intercept', 'intervals', 'distribution', 'normally distributed', 'probability distribution', 'hypotheses', 'probability', 'confidence', 'slope']", Inferences Concerning the Regression Coefficients,seg_283,"aside from merely estimating the linear relationship between x and y for purposes of prediction, the experimenter may also be interested in drawing certain inferences about the slope and intercept. in order to allow for the testing of hypotheses and the construction of confidence intervals on β0 and β1, one must be willing to make the further assumption that each i, i = 1, 2, . . . , n, is normally distributed. this assumption implies that y1, y2, . . . , yn are also normally distributed, each with probability distribution n(yi;β0 + β1xi, σ)."
2382,1,"['degrees of freedom', 'independent', 'distribution', 'normality', 'random variable', 'normal', 'variable', 'statistic', 'random', 'normal distribution']", Inferences Concerning the Regression Coefficients,seg_283,"from section 11.4 we know that b1 follows a normal distribution. it turns out that under the normality assumption, a result very much analogous to that given in theorem 8.4 allows us to conclude that (n − 2)s2/σ2 is a chi-squared variable with n − 2 degrees of freedom, independent of the random variable b1. theorem 8.5 then assures us that the statistic"
2383,1,"['degrees of freedom', 'interval', 'statistic', 'coefficient', 'confidence', 'confidence interval']", Inferences Concerning the Regression Coefficients,seg_283,has a t-distribution with n− 2 degrees of freedom. the statistic t can be used to construct a 100(1− α)% confidence interval for the coefficient β1.
2384,1,"['interval', 'regression', 'parameter', 'confidence', 'regression line', 'confidence interval']", Inferences Concerning the Regression Coefficients,seg_283,confidence interval a 100(1 − α)% confidence interval for the parameter β1 in the regression line for β1 μy |x = β0 + β1x is
2385,1,['degrees of freedom'], Inferences Concerning the Regression Coefficients,seg_283,where tα/2 is a value of the t-distribution with n− 2 degrees of freedom.
2386,1,"['interval', 'table', 'data', 'regression', 'confidence', 'regression line', 'confidence interval']", Inferences Concerning the Regression Coefficients,seg_283,"example 11.2: find a 95% confidence interval for β1 in the regression line μy |x = β0+β1x, based on the pollution data of table 11.1."
2387,1,['results'], Inferences Concerning the Regression Coefficients,seg_283,"solution : from the results given in example 11.1 we find that sxx = 4152.18 and sxy = 3752.09. in addition, we find that syy = 3713.88. recall that b1 = 0.903643. hence,"
2388,1,"['degrees of freedom', 'interval', 'table', 'confidence', 'confidence interval']", Inferences Concerning the Regression Coefficients,seg_283,"therefore, taking the square root, we obtain s = 3.2295. using table a.4, we find t0.025 ≈ 2.045 for 31 degrees of freedom. therefore, a 95% confidence interval for β1 is"
2389,1,"['degrees of freedom', 'critical region', 'test', 'null hypothesis', 'hypothesis']", Inferences Concerning the Regression Coefficients,seg_283,"to test the null hypothesis h0 that β1 = β10 against a suitable alternative, we again use the t-distribution with n − 2 degrees of freedom to establish a critical region and then base our decision on the value of"
2390,1,['method'], Inferences Concerning the Regression Coefficients,seg_283,the method is illustrated by the following example.
2391,1,"['test', 'estimated', 'hypothesis']", Inferences Concerning the Regression Coefficients,seg_283,"example 11.3: using the estimated value b1 = 0.903643 of example 11.1, test the hypothesis that β1 = 1.0 against the alternative that β1 < 1.0."
2392,1,['hypotheses'], Inferences Concerning the Regression Coefficients,seg_283,solution : the hypotheses are h0: β1 = 1.0 and h1: β1 < 1.0. so
2393,1,['degrees of freedom'], Inferences Concerning the Regression Coefficients,seg_283,with n− 2 = 31 degrees of freedom (p ≈ 0.03).
2394,1,['level'], Inferences Concerning the Regression Coefficients,seg_283,"decision: the t-value is significant at the 0.03 level, suggesting strong evidence that β1 < 1.0."
2395,1,"['test', 'slope', 'hypothesis']", Inferences Concerning the Regression Coefficients,seg_283,one important t-test on the slope is the test of the hypothesis
2396,1,"['plot', 'linear', 'independent', 'independent variable', 'data', 'regression', 'information', 'variable', 'linear regression', 'null hypothesis', 'hypothesis']", Inferences Concerning the Regression Coefficients,seg_283,"when the null hypothesis is not rejected, the conclusion is that there is no significant linear relationship between e(y) and the independent variable x. the plot of the data for example 11.1 would suggest that a linear relationship exists. however, in some applications in which σ2 is large and thus considerable “noise” is present in the data, a plot, while useful, may not produce clear information for the researcher. rejection of h0 above implies that a significant linear regression exists."
2397,0,[], Inferences Concerning the Regression Coefficients,seg_283,figure 11.7 displays a minitab printout showing the t-test for
2398,1,"['linear', 'error', 'standard error', 'data', 'regression', 'regression coefficient', 'mean', 'standard', 'coefficient', 'null hypothesis', 'hypothesis']", Inferences Concerning the Regression Coefficients,seg_283,"for the data of example 11.1. note the regression coefficient (coef), standard error (se coef), t-value (t), and p -value (p). the null hypothesis is rejected. clearly, there is a significant linear relationship between mean chemical oxygen demand reduction and solids reduction. note that the t-statistic is computed as"
2399,1,"['standard error', 'error', 'standard']", Inferences Concerning the Regression Coefficients,seg_283,coefficient b1 t = = . standard error s/√sxx
2400,1,"['nonlinear', 'linear', 'failure', 'mean']", Inferences Concerning the Regression Coefficients,seg_283,"the failure to reject h0: β1 = 0 suggests that there is no linear relationship between y and x. figure 11.8 is an illustration of the implication of this result. it may mean that changing x has little impact on changes in y , as seen in (a). however, it may also indicate that the true relationship is nonlinear, as indicated by (b)."
2401,1,"['linear', 'model', 'variability']", Inferences Concerning the Regression Coefficients,seg_283,"when h0: β1 = 0 is rejected, there is an implication that the linear term in x residing in the model explains a significant portion of variability in y . the two"
2402,1,['regression'], Inferences Concerning the Regression Coefficients,seg_283,regression analysis: cod versus per_red the regression equation is cod = 3.83 + 0.904 per_red
2403,1,"['residual', 'regression', 'analysis of variance', 'variance', 'error']", Inferences Concerning the Regression Coefficients,seg_283,s = 3.22954 r-sq = 91.3% r-sq(adj) = 91.0% analysis of variance source df ss ms f p regression 1 3390.6 3390.6 325.08 0.000 residual error 31 323.3 10.4 total 32 3713.9
2404,1,['data'], Inferences Concerning the Regression Coefficients,seg_283,figure 11.7: minitab printout for t-test for data of example 11.1.
2405,1,['hypothesis'], Inferences Concerning the Regression Coefficients,seg_283,figure 11.8: the hypothesis h0: β1 = 0 is not rejected.
2406,1,"['linear', 'model']", Inferences Concerning the Regression Coefficients,seg_283,"plots in figure 11.9 illustrate possible scenarios. as depicted in (a) of the figure, rejection of h0 may suggest that the relationship is, indeed, linear. as indicated in (b), it may suggest that while the model does contain a linear effect, a better representation may be found by including a polynomial (perhaps quadratic) term (i.e., terms that supplement the linear term)."
2407,1,"['intervals', 'hypothesis testing', 'normally distributed', 'coefficient', 'hypothesis']", Inferences Concerning the Regression Coefficients,seg_283,confidence intervals and hypothesis testing on the coefficient β0 may be established from the fact that b0 is also normally distributed. it is not difficult to show that
2408,1,['hypothesis'], Inferences Concerning the Regression Coefficients,seg_283,figure 11.9: the hypothesis h0: β1 = 0 is rejected.
2409,1,"['degrees of freedom', 'interval', 'confidence', 'confidence interval']", Inferences Concerning the Regression Coefficients,seg_283,has a t-distribution with n− 2 degrees of freedom from which we may construct a 100(1− α)% confidence interval for α.
2410,1,"['interval', 'regression', 'parameter', 'confidence', 'regression line', 'confidence interval']", Inferences Concerning the Regression Coefficients,seg_283,confidence interval a 100(1 − α)% confidence interval for the parameter β0 in the regression line for β0 μy |x = β0 + β1x is
2411,1,['degrees of freedom'], Inferences Concerning the Regression Coefficients,seg_283,where tα/2 is a value of the t-distribution with n− 2 degrees of freedom.
2412,1,"['interval', 'table', 'data', 'regression', 'confidence', 'regression line', 'confidence interval']", Inferences Concerning the Regression Coefficients,seg_283,"example 11.4: find a 95% confidence interval for β0 in the regression line μy |x = β0+β1x, based on the data of table 11.1."
2413,0,[], Inferences Concerning the Regression Coefficients,seg_283,"solution : in examples 11.1 and 11.2, we found that"
2414,0,[], Inferences Concerning the Regression Coefficients,seg_283,from example 11.1 we had
2415,1,"['degrees of freedom', 'interval', 'table', 'confidence', 'confidence interval']", Inferences Concerning the Regression Coefficients,seg_283,"using table a.4, we find t0.025 ≈ 2.045 for 31 degrees of freedom. therefore, a 95% confidence interval for β0 is"
2416,1,"['degrees of freedom', 'critical region', 'test', 'null hypothesis', 'hypothesis']", Inferences Concerning the Regression Coefficients,seg_283,"to test the null hypothesis h0 that β0 = β00 against a suitable alternative, we can use the t-distribution with n − 2 degrees of freedom to establish a critical region and then base our decision on the value of"
2417,1,"['estimated', 'level', 'significance', 'test', 'level of significance', 'hypothesis']", Inferences Concerning the Regression Coefficients,seg_283,"example 11.5: using the estimated value b0 = 3.829633 of example 11.1, test the hypothesis that β0 = 0 at the 0.05 level of significance against the alternative that β0 = 0."
2418,1,['hypotheses'], Inferences Concerning the Regression Coefficients,seg_283,solution : the hypotheses are h0: β0 = 0 and h1: β0 = 0. so
2419,1,"['degrees of freedom', 'estimated', 'standard error', 'intercept', 'standard', 'error']", Inferences Concerning the Regression Coefficients,seg_283,"with 31 degrees of freedom. thus, p = p -value ≈ 0.038 and we conclude that β0 = 0. note that this is merely coef/stdev, as we see in the minitab printout in figure 11.7. the se coef is the standard error of the estimated intercept."
2420,1,"['model', 'variability', 'regression', 'hypothesis testing', 'coefficient', 'coefficient of determination', 'hypothesis']", Inferences Concerning the Regression Coefficients,seg_283,"note in figure 11.7 that an item denoted by r-sq is given with a value of 91.3%. this quantity, r2, is called the coefficient of determination. this quantity is a measure of the proportion of variability explained by the fitted model. in section 11.8, we shall introduce the notion of an analysis-of-variance approach to hypothesis testing in regression. the analysis-of-variance approach makes use"
2421,1,"['sum of squares', 'error sum of squares', 'error']", Inferences Concerning the Regression Coefficients,seg_283,n of the error sum of squares sse = ∑ (yi − ŷi)2 and the total corrected sum of
2422,1,"['model', 'error', 'variation']", Inferences Concerning the Regression Coefficients,seg_283,"i=1 values that ideally would be explained by the model. the sse value is the variation due to error, or variation unexplained. clearly, if sse = 0, all variation is explained. the quantity that represents variation explained is sst − sse. the r2 is"
2423,0,[], Inferences Concerning the Regression Coefficients,seg_283,sse coeff. of determination: r2 = 1− . sst
2424,1,"['model', 'variability', 'residuals', 'data', 'response', 'coefficient', 'coefficient of determination']", Inferences Concerning the Regression Coefficients,seg_283,"note that if the fit is perfect, all residuals are zero, and thus r2 = 1.0. but if sse is only slightly smaller than sst , r2 ≈ 0. note from the printout in figure 11.7 that the coefficient of determination suggests that the model fit to the data explains 91.3% of the variability observed in the response, the reduction in chemical oxygen demand."
2425,1,['plot'], Inferences Concerning the Regression Coefficients,seg_283,figure 11.10 provides an illustration of a good fit (r2 ≈ 1.0) in plot (a) and a poor fit (r2 ≈ 0) in plot (b).
2426,1,['function'], Inferences Concerning the Regression Coefficients,seg_283,"analysts quote values of r2 quite often, perhaps due to its simplicity. however, there are pitfalls in its interpretation. the reliability of r2 is a function of the"
2427,1,['plots'], Inferences Concerning the Regression Coefficients,seg_283,figure 11.10: plots depicting a very good fit and a poor fit.
2428,1,"['model', 'linear', 'precision', 'experienced', 'variability', 'residuals', 'data', 'regression', 'set', 'data set']", Inferences Concerning the Regression Coefficients,seg_283,"size of the regression data set and the type of application. clearly, 0 ≤ r2 ≤ 1 and the upper bound is achieved when the fit to the data is perfect (i.e., all of the residuals are zero). what is an acceptable value for r2? this is a difficult question to answer. a chemist, charged with doing a linear calibration of a highprecision piece of equipment, certainly expects to experience a very high r2-value (perhaps exceeding 0.99), while a behavioral scientist, dealing in data impacted by variability in human behavior, may feel fortunate to experience an r2 as large as 0.70. an experienced model fitter senses when a value is large enough, given the situation confronted. clearly, some scientific phenomena lend themselves to modeling with more precision than others."
2429,1,"['model', 'prediction', 'data', 'model selection', 'set', 'data set', 'process', 'response']", Inferences Concerning the Regression Coefficients,seg_283,"the r2 criterion is dangerous to use for comparing competing models for the same data set. adding additional terms to the model (e.g., an additional regressor) decreases sse and thus increases r2 (or at least does not decrease it). this implies that r2 can be made artificially high by an unwise practice of overfitting (i.e., the inclusion of too many model terms). thus, the inevitable increase in r2 enjoyed by adding an additional term does not imply the additional term was needed. in fact, the simple model may be superior for predicting response values. the role of overfitting and its influence on prediction capability will be discussed at length in chapter 12 as we visit the notion of models involving more than a single regressor. suffice it to say at this point that one should not subscribe to a model selection process that solely involves the consideration of r2."
2430,1,"['linear', 'independent', 'independent variable', 'regression', 'variable', 'response', 'linear regression']", Prediction,seg_285,"there are several reasons for building a linear regression. one, of course, is to predict response values at one or more values of the independent variable. in this"
2431,1,"['prediction', 'errors', 'associated']", Prediction,seg_285,"section, the focus is on errors associated with prediction."
2432,1,"['estimate', 'prediction', 'predicted', 'case', 'intervals', 'variable', 'response', 'mean', 'error']", Prediction,seg_285,"the equation ŷ = b0 + b1x may be used to predict or estimate the mean response μy |x0 at x = x0, where x0 is not necessarily one of the prechosen values, or it may be used to predict a single value y0 of the variable y0, when x = x0. we would expect the error of prediction to be higher in the case of a single predicted value than in the case where a mean is predicted. this, then, will affect the width of our intervals for the values being predicted."
2433,1,"['interval', 'estimate', 'point estimator', 'confidence', 'estimator', 'confidence interval']", Prediction,seg_285,suppose that the experimenter wishes to construct a confidence interval for μy |x0 . we shall use the point estimator ŷ0 = b0 + b1x0 to estimate μy |x0 =
2434,1,"['sampling', 'distribution', 'normal', 'mean', 'sampling distribution']", Prediction,seg_285,β0 + β1x. it can be shown that the sampling distribution of ŷ0 is normal with mean
2435,1,['variance'], Prediction,seg_285,and variance
2436,1,"['interval', 'statistic', 'mean', 'confidence', 'response', 'confidence interval']", Prediction,seg_285,"the latter following from the fact that cov(ȳ , b1) = 0 (see review exercise 11.61 on page 438). thus, a 100(1−α)% confidence interval on the mean response μy |x0 can now be constructed from the statistic"
2437,1,['degrees of freedom'], Prediction,seg_285,which has a t-distribution with n− 2 degrees of freedom.
2438,1,"['interval', 'mean', 'confidence', 'response', 'confidence interval']", Prediction,seg_285,confidence interval a 100(1− α)% confidence interval for the mean response μy |x0 is for μy |x0
2439,1,['degrees of freedom'], Prediction,seg_285,where tα/2 is a value of the t-distribution with n− 2 degrees of freedom.
2440,1,"['table', 'confidence limits', 'data', 'mean', 'confidence', 'response']", Prediction,seg_285,"example 11.6: using the data of table 11.1, construct 95% confidence limits for the mean response"
2441,1,['regression'], Prediction,seg_285,"solution : from the regression equation we find for x0 = 20% solids reduction, say,"
2442,1,"['degrees of freedom', 'interval', 'confidence', 'confidence interval']", Prediction,seg_285,"in addition, x̄ = 33.4545, sxx = 4152.18, s = 3.2295, and t0.025 ≈ 2.045 for 31 degrees of freedom. therefore, a 95% confidence interval for μy |20 is"
2443,1,"['estimated', 'confidence limits', 'data', 'regression', 'confidence', 'mean', 'estimated regression line', 'regression line']", Prediction,seg_285,"repeating the previous calculations for each of several different values of x0, one can obtain the corresponding confidence limits on each μy |x0 . figure 11.11 displays the data points, the estimated regression line, and the upper and lower confidence limits on the mean of y |x."
2444,1,"['confidence', 'mean', 'confidence limits']", Prediction,seg_285,figure 11.11: confidence limits for the mean value of y |x.
2445,1,"['population mean', 'mean', 'confident', 'population']", Prediction,seg_285,"in example 11.6, we are 95% confident that the population mean reduction in chemical oxygen demand is between 20.1071% and 23.6979% when solid reduction is 20%."
2446,1,"['error', 'interval', 'prediction', 'mean', 'confidence', 'response', 'prediction interval', 'confidence interval']", Prediction,seg_285,"another type of interval that is often misinterpreted and confused with that given for μy |x is the prediction interval for a future observed response. actually in many instances, the prediction interval is more relevant to the scientist or engineer than the confidence interval on the mean. in the tar content and inlet temperature example cited in section 11.1, there would certainly be interest not only in estimating the mean tar content at a specific temperature but also in constructing an interval that reflects the error in predicting a future observed amount of tar content at the given temperature."
2447,1,"['sampling distribution', 'estimate', 'interval', 'prediction', 'regression', 'sampling', 'distribution', 'random variable', 'normal', 'variable', 'mean', 'random', 'variance', 'prediction interval']", Prediction,seg_285,"to obtain a prediction interval for any single value y0 of the variable y0, it is necessary to estimate the variance of the differences between the ordinates ŷ0, obtained from the computed regression lines in repeated sampling when x = x0, and the corresponding true ordinate y0. we can think of the difference ŷ0 − y0 as a value of the random variable ŷ0 − y0, whose sampling distribution can be shown to be normal with mean"
2448,1,['variance'], Prediction,seg_285,and variance
2449,1,"['interval', 'prediction', 'predicted', 'statistic', 'prediction interval']", Prediction,seg_285,"thus, a 100(1 − α)% prediction interval for a single predicted value y0 can be constructed from the statistic"
2450,1,['degrees of freedom'], Prediction,seg_285,which has a t-distribution with n− 2 degrees of freedom.
2451,1,"['prediction', 'response', 'prediction interval', 'interval']", Prediction,seg_285,prediction interval a 100(1− α)% prediction interval for a single response y0 is given by for y0
2452,1,['degrees of freedom'], Prediction,seg_285,where tα/2 is a value of the t-distribution with n− 2 degrees of freedom.
2453,1,"['confidence intervals', 'interval', 'prediction', 'intervals', 'random variable', 'variable', 'population', 'parameter', 'random', 'confidence', 'prediction interval', 'confidence interval']", Prediction,seg_285,"clearly, there is a distinction between the concept of a confidence interval and the prediction interval described previously. the interpretation of the confidence interval is identical to that described for all confidence intervals on population parameters discussed throughout the book. indeed, μy |x0 is a population parameter. the computed prediction interval, however, represents an interval that has a probability equal to 1 − α of containing not a parameter but a future value y0 of the random variable y0."
2454,1,"['interval', 'table', 'prediction', 'data', 'prediction interval']", Prediction,seg_285,"example 11.7: using the data of table 11.1, construct a 95% prediction interval for y0 when x0 = 20%."
2455,1,"['degrees of freedom', 'interval', 'prediction', 'prediction interval']", Prediction,seg_285,"solution : we have n = 33, x0 = 20, x̄ = 33.4545, ŷ0 = 21.9025, sxx = 4152.18, s = 3.2295, and t0.025 ≈ 2.045 for 31 degrees of freedom. therefore, a 95% prediction interval for y0 is"
2456,1,"['plot', 'interval', 'prediction', 'case', 'data', 'regression', 'response', 'mean', 'confidence', 'regression line', 'prediction interval', 'confidence interval']", Prediction,seg_285,"figure 11.12 shows another plot of the chemical oxygen demand reduction data, with both the confidence interval on the mean response and the prediction interval on an individual response plotted. the plot reflects a much tighter interval around the regression line in the case of the mean response."
2457,1,"['simple linear regression', 'linear', 'regression', 'correlation', 'linear regression']",Exercises,seg_287,412 chapter 11 simple linear regression and correlation
2458,1,"['prediction intervals', 'prediction', 'confidence limits', 'data', 'intervals', 'mean', 'confidence']",Exercises,seg_287,figure 11.12: confidence and prediction intervals for the chemical oxygen demand reduction data; inside bands indicate the confidence limits for the mean responses and outside bands indicate the prediction limits for the future responses.
2459,0,[],Exercises,seg_287,figure 11.13: sas printout for exercise 11.27.
2460,1,"['model', 'parameters', 'variables', 'range', 'prediction', 'nonlinear', 'regression', 'variable', 'response', 'varying']", Choice of a Regression Model,seg_289,"much of what has been presented thus far on regression involving a single independent variable depends on the assumption that the model chosen is correct, the presumption that μy |x is related to x linearly in the parameters. certainly, one cannot expect the prediction of the response to be good if there are several independent variables, not considered in the model, that are affecting the response and are varying in the system. in addition, the prediction will certainly be inadequate if the true structure relating μy |x to x is extremely nonlinear in the range of the variables considered."
2461,1,"['simple linear regression', 'model', 'linear', 'regression model', 'range', 'results', 'regression', 'linear regression', 'function', 'linear regression model']", Choice of a Regression Model,seg_289,"often the simple linear regression model is used even though it is known that the model is something other than linear or that the true structure is unknown. this approach is often sound, particularly when the range of x is narrow. thus, the model used becomes an approximating function that one hopes is an adequate representation of the true picture in the region of interest. one should note, however, the effect of an inadequate model on the results presented thus far. for example, if the true model, unknown to the experimenter, is linear in more than one x, say"
2462,1,"['bias', 'experiment', 'estimate', 'biased', 'least squares', 'least squares estimate', 'variable', 'function', 'coefficient']", Choice of a Regression Model,seg_289,"then the ordinary least squares estimate b1 = sxy/sxx, calculated by only considering x1 in the experiment, is, under general circumstances, a biased estimate of the coefficient β1, the bias being a function of the additional coefficient β2 (see review exercise 11.65 on page 438). also, the estimate s2 for σ2 is biased due to the additional variable."
2463,1,"['dependent variable', 'estimated', 'regression line', 'dependent', 'regression', 'variable', 'analysis of variance', 'variation', 'estimated regression line', 'variance', 'anova']", AnalysisofVariance Approach,seg_291,"often the problem of analyzing the quality of the estimated regression line is handled by an analysis-of-variance (anova) approach: a procedure whereby the total variation in the dependent variable is subdivided into meaningful components that are then observed and treated in a systematic fashion. the analysis of variance, discussed in chapter 13, is a powerful resource that is used for many applications."
2464,1,"['experimental', 'estimated', 'estimation', 'data', 'regression', 'regression line']", AnalysisofVariance Approach,seg_291,"suppose that we have n experimental data points in the usual form (xi, yi) and that the regression line is estimated. in our estimation of σ2 in section 11.4, we established the identity"
2465,0,[], AnalysisofVariance Approach,seg_291,an alternative and perhaps more informative formulation is
2466,1,['sum of squares'], AnalysisofVariance Approach,seg_291,we have achieved a partitioning of the total corrected sum of squares of y into two components that should reflect particular meaning to the experimenter. we shall indicate this partitioning symbolically as
2467,1,"['model', 'case', 'regression', 'sum of squares', 'error sum of squares', 'variation', 'regression line', 'error']", AnalysisofVariance Approach,seg_291,"the first component on the right, ssr, is called the regression sum of squares, and it reflects the amount of variation in the y-values explained by the model, in this case the postulated straight line. the second component is the familiar error sum of squares, which reflects variation about the regression line."
2468,1,['hypothesis'], AnalysisofVariance Approach,seg_291,suppose that we are interested in testing the hypothesis
2469,1,"['degrees of freedom', 'model', 'independent', 'condition', 'variables', 'results', 'variation', 'variable', 'random', 'test', 'null hypothesis', 'hypothesis']", AnalysisofVariance Approach,seg_291,"where the null hypothesis says essentially that the model is μy |x = β0. that is, the variation in y results from chance or random fluctuations which are independent of the values of x. this condition is reflected in figure 11.10(b). under the conditions of this null hypothesis, it can be shown that ssr/σ2 and sse/σ2 are values of independent chi-squared variables with 1 and n−2 degrees of freedom, respectively, and then by theorem 7.12 it follows that sst/σ2 is also a value of a chi-squared variable with n− 1 degrees of freedom. to test the hypothesis above, we compute"
2470,1,['significance'], AnalysisofVariance Approach,seg_291,"and reject h0 at the α-level of significance when f > fα(1, n− 2)."
2471,1,"['degrees of freedom', 'table', 'mean squares', 'mean']", AnalysisofVariance Approach,seg_291,"the computations are usually summarized by means of an analysis-of-variance table, as in table 11.2. it is customary to refer to the various sums of squares divided by their respective degrees of freedom as the mean squares."
2472,1,"['variance', 'analysis of variance']", AnalysisofVariance Approach,seg_291,table 11.2: analysis of variance for testing β1 = 0
2473,1,"['regression', 'error', 'mean', 'variation']", AnalysisofVariance Approach,seg_291,source of sum of degrees of mean computed variation squares freedom square f ssr regression ssr 1 ssr s2 error sse n− 2 s2 = sse n−2 total sst n− 1
2474,1,"['model', 'null hypothesis', 'data', 'variation', 'function', 'response', 'critical value', 'hypothesis']", AnalysisofVariance Approach,seg_291,"when the null hypothesis is rejected, that is, when the computed f -statistic exceeds the critical value fα(1, n − 2), we conclude that there is a significant amount of variation in the response accounted for by the postulated model, the straight-line function. if the f -statistic is in the fail to reject region, we conclude that the data did not reflect sufficient evidence to support the model postulated."
2475,1,['statistic'], AnalysisofVariance Approach,seg_291,"in section 11.5, a procedure was given whereby the statistic"
2476,1,"['test', 'hypothesis']", AnalysisofVariance Approach,seg_291,is used to test the hypothesis
2477,1,"['degrees of freedom', 'significance', 'hypothesis']", AnalysisofVariance Approach,seg_291,where t follows the t-distribution with n − 2 degrees of freedom. the hypothesis is rejected if |t| > tα/2 for an α-level of significance. it is interesting to note that
2478,1,['case'], AnalysisofVariance Approach,seg_291,in the special case in which we are testing
2479,0,[], AnalysisofVariance Approach,seg_291,the value of our t -statistic becomes
2480,1,"['table', 'states', 'analysis of variance', 'variance', 'variation', 'response', 'null hypothesis', 'hypothesis']", AnalysisofVariance Approach,seg_291,"and the hypothesis under consideration is identical to that being tested in table 11.2. namely, the null hypothesis states that the variation in the response is due merely to chance. the analysis of variance uses the f -distribution rather than the t-distribution. for the two-sided alternative, the two approaches are identical. this we can see by writing"
2481,1,"['degrees of freedom', 'analysis of variance', 'variance']", AnalysisofVariance Approach,seg_291,which is identical to the f -value used in the analysis of variance. the basic relationship between the t-distribution with v degrees of freedom and the f -distribution with 1 and v degrees of freedom is
2482,1,['test'], AnalysisofVariance Approach,seg_291,"of course, the t-test allows for testing against a one-sided alternative while the f -test is restricted to testing against a two-sided alternative."
2483,1,"['confidence intervals', 'prediction intervals', 'prediction', 'observation', 'data', 'intervals', 'mean', 'parameter', 'tests', 'confidence', 'regression', 'response', 'table', 'hypotheses', 'fitted values', 'variation']", AnalysisofVariance Approach,seg_291,"consider again the chemical oxygen demand reduction data of table 11.1. figures 11.14 and 11.15 show more complete annotated computer printouts. again we illustrate it with minitab software. the t-ratio column indicates tests for null hypotheses of zero values on the parameter. the term “fit” denotes ŷ-values, often called fitted values. the term “se fit” is used in computing confidence intervals on mean response. the item r2 is computed as (ssr/sst )×100 and signifies the proportion of variation in y explained by the straight-line regression. also shown are confidence intervals on the mean response and prediction intervals on a new observation."
2484,1,"['model', 'experimental', 'quantitative', 'estimate', 'observations', 'information', 'significance', 'test', 'response']", Test for Linearity of Regression Data with Repeated Observations,seg_293,"in certain kinds of experimental situations, the researcher has the capability of obtaining repeated observations on the response for each value of x. although it is not necessary to have these repetitions in order to estimate β0 and β1, nevertheless repetitions enable the experimenter to obtain quantitative information concerning the appropriateness of the model. in fact, if repeated observations are generated, the experimenter can make a significance test to aid in determining whether or not the model is adequate."
2485,1,"['predictor', 'regression']", Test for Linearity of Regression Data with Repeated Observations,seg_293,the regression equation is cod = 3.83 + 0.904 per_red predictor coef se coef t p
2486,1,"['residual', 'variance', 'error', 'regression']", Test for Linearity of Regression Data with Repeated Observations,seg_293,analysis of variance source df ss ms f p regression 1 3390.6 3390.6 325.08 0.000 residual error 31 323.3 10.4 total 32 3713.9
2487,1,['residual'], Test for Linearity of Regression Data with Repeated Observations,seg_293,obs per_red cod fit se fit residual st resid
2488,1,"['simple linear regression', 'linear', 'data', 'regression', 'linear regression']", Test for Linearity of Regression Data with Repeated Observations,seg_293,figure 11.14: minitab printout of simple linear regression for chemical oxygen demand reduction data; part i.
2489,1,"['sample', 'observations', 'random sample', 'variable', 'random variable', 'random']", Test for Linearity of Regression Data with Repeated Observations,seg_293,"let us select a random sample of n observations using k distinct values of x, say x1, x2, . . . , xn, such that the sample contains n1 observed values of the random variable y1 corresponding to x1, n2 observed values of y2 corresponding to x2, . . . ,"
2490,0,['n'], Test for Linearity of Regression Data with Repeated Observations,seg_293,"k nk observed values of yk corresponding to xk. of necessity, n = ∑ ni."
2491,1,"['simple linear regression', 'linear', 'data', 'regression', 'linear regression']", Test for Linearity of Regression Data with Repeated Observations,seg_293,figure 11.15: minitab printout of simple linear regression for chemical oxygen demand reduction data; part ii.
2492,1,"['random variable', 'variable', 'random']", Test for Linearity of Regression Data with Repeated Observations,seg_293,"yij = the jth value of the random variable yi, ni"
2493,1,"['observations', 'measurements']", Test for Linearity of Regression Data with Repeated Observations,seg_293,"hence, if n4 = 3 measurements of y were made corresponding to x = x4, we would indicate these observations by y41, y42, and y43. then"
2494,1,"['sum of squares', 'error sum of squares', 'variation', 'error']", Test for Linearity of Regression Data with Repeated Observations,seg_293,the error sum of squares consists of two parts: the amount due to the variation between the values of y within given values of x and a component that is normally
2495,1,"['observations', 'case', 'sum of squares', 'random', 'linear', 'estimate', 'biased', 'random variation', 'data', 'error', 'model', 'experimental', 'systematic variation', 'linear model', 'error sum of squares', 'unbiased', 'experimental error', 'random errors', 'errors', 'variation']", Test for Linearity of Regression Data with Repeated Observations,seg_293,"called the lack-of-fit contribution. the first component reflects mere random variation, or pure experimental error, while the second component is a measure of the systematic variation brought about by higher-order terms. in our case, these are terms in x other than the linear, or first-order, contribution. note that in choosing a linear model we are essentially assuming that this second component does not exist and hence our error sum of squares is completely due to random errors. if this should be the case, then s2 = sse/(n− 2) is an unbiased estimate of σ2. however, if the model does not adequately fit the data, then the error sum of squares is inflated and produces a biased estimate of σ2. whether or not the model fits the data, an unbiased estimate of σ2 can always be obtained when we have repeated observations simply by computing"
2496,1,['variances'], Test for Linearity of Regression Data with Repeated Observations,seg_293,for each of the k distinct values of x and then pooling these variances to get
2497,1,"['experimental error', 'experimental', 'sum of squares', 'error sum of squares', 'lack of fit', 'error']", Test for Linearity of Regression Data with Repeated Observations,seg_293,the numerator of s2 is a measure of the pure experimental error. a computational procedure for separating the error sum of squares into the two components representing pure error and lack of fit is as follows:
2498,1,"['sum of squares', 'error sum of squares', 'error']", Test for Linearity of Regression Data with Repeated Observations,seg_293,computation of 1. compute the pure error sum of squares lack-of-fit sum of squares k ni ∑∑(yij − ȳi.)2. i=1 j=1
2499,1,"['degrees of freedom', 'estimate', 'sum of squares', 'mean square', 'error sum of squares', 'mean', 'associated', 'lack of fit', 'error', 'unbiased']", Test for Linearity of Regression Data with Repeated Observations,seg_293,"this sum of squares has n − k degrees of freedom associated with it, and the resulting mean square is our unbiased estimate s2 of σ2. 2. subtract the pure error sum of squares from the error sum of squares sse, thereby obtaining the sum of squares due to lack of fit. the degrees of freedom for lack of fit are obtained by simply subtracting (n− 2)− (n− k) = k − 2."
2500,1,"['table', 'regression', 'measurements', 'hypotheses', 'response']", Test for Linearity of Regression Data with Repeated Observations,seg_293,the computations required for testing hypotheses in a regression problem with repeated measurements on the response may be summarized as shown in table 11.3.
2501,1,"['sample', 'linear', 'model', 'linear model', 'observations', 'regression', 'variation', 'lack of fit', 'regression line', 'error']", Test for Linearity of Regression Data with Repeated Observations,seg_293,"figures 11.16 and 11.17 display the sample points for the “correct model” and “incorrect model” situations. in figure 11.16, where the μy |x fall on a straight line, there is no lack of fit when a linear model is assumed, so the sample variation around the regression line is a pure error resulting from the variation that occurs among repeated observations. in figure 11.17, where the μy |x clearly do not fall on a straight line, the lack of fit from erroneously choosing a linear model accounts for a large portion of the variation around the regression line, supplementing the pure error."
2502,1,"['regression', 'analysis of variance', 'variance']", Test for Linearity of Regression Data with Repeated Observations,seg_293,table 11.3: analysis of variance for testing linearity of regression
2503,1,"['regression', 'mean', 'variation', 'lack of fit', 'error']", Test for Linearity of Regression Data with Repeated Observations,seg_293,source of sum of degrees of mean variation squares freedom square computed f ssr regression ssr 1 ssr s2 error sse n− 2 sse−sse(pure) sse−sse(pure) lack of fit sse − sse (pure) k − 2 k−2 s2(k−2)
2504,1,['error'], Test for Linearity of Regression Data with Repeated Observations,seg_293,sse(pure) pure error {sse (pure) {n − k s2 = n−k
2505,1,"['linear', 'model', 'linear model']", Test for Linearity of Regression Data with Repeated Observations,seg_293,figure 11.16: correct linear model with no lack-of- figure 11.17: incorrect linear model with lack-of-fit fit component. component.
2506,1,"['design', 'case', 'sum of squares', 'significance', 'linear', 'experiment', 'mean', 'error', 'linear regression model', 'model', 'regression model', 'regression', 'error sum of squares', 'linear regression', 'regression analysis', 'mean square', 'lack of fit']", Test for Linearity of Regression Data with Repeated Observations,seg_293,"the concept of lack of fit is extremely important in applications of regression analysis. in fact, the need to construct or design an experiment that will account for lack of fit becomes more critical as the problem and the underlying mechanism involved become more complicated. surely, one cannot always be certain that his or her postulated structure, in this case the linear regression model, is correct or even an adequate representation. the following example shows how the error sum of squares is partitioned into the two components representing pure error and lack of fit. the adequacy of the model is tested at the α-level of significance by comparing the lack-of-fit mean square divided by s2 with fα(k − 2, n− k)."
2507,1,"['linear', 'model', 'linear model', 'estimate', 'table', 'observations', 'lack of fit', 'test']", Test for Linearity of Regression Data with Repeated Observations,seg_293,example 11.8: observations of the yield of a chemical reaction taken at various temperatures were recorded in table 11.4. estimate the linear model μy |x = β0 + β1x and test for lack of fit.
2508,1,"['linear', 'model', 'experimental', 'linear model', 'table', 'results', 'data', 'variation', 'lack of fit', 'null hypothesis', 'hypothesis']", Test for Linearity of Regression Data with Repeated Observations,seg_293,"solution : results of the computations are shown in table 11.5. conclusion: the partitioning of the total variation in this manner reveals a significant variation accounted for by the linear model and an insignificant amount of variation due to lack of fit. thus, the experimental data do not seem to suggest the need to consider terms higher than first order in the model, and the null hypothesis is not rejected."
2509,1,['data'], Test for Linearity of Regression Data with Repeated Observations,seg_293,table 11.4: data for example 11.8
2510,1,"['variance', 'analysis of variance', 'data']", Test for Linearity of Regression Data with Repeated Observations,seg_293,table 11.5: analysis of variance on yield-temperature data
2511,1,"['variation', 'mean']", Test for Linearity of Regression Data with Repeated Observations,seg_293,source of sum of degrees of mean variation squares freedom square computed f p-values
2512,1,['error'], Test for Linearity of Regression Data with Repeated Observations,seg_293,regression 509.2507 1 509.2507 1531.58 <0.0001 error 3.8660 10 l p
2513,1,"['degrees of freedom', 'linear', 'model', 'data']", Test for Linearity of Regression Data with Repeated Observations,seg_293,"figure 11.18 is an annotated computer printout showing analysis of the data of example 11.8 with sas. note the “lof” with 2 degrees of freedom, representing the quadratic and cubic contribution to the model, and the p -value of 0.22, suggesting that the linear (first-order) model is adequate."
2514,1,['data'], Test for Linearity of Regression Data with Repeated Observations,seg_293,"figure 11.18: sas printout, showing analysis of data of example 11.8."
2515,1,"['simple linear regression', 'model', 'linear', 'variables', 'plots', 'nonlinear', 'data', 'regression', 'case', 'variable', 'linear regression', 'plotting', 'transformation']", Data Plots and Transformations,seg_297,"in this chapter, we deal with building regression models where there is one independent, or regressor, variable. in addition, we are assuming, through model formulation, that both x and y enter the model in a linear fashion. often it is advisable to work with an alternative model in which either x or y (or both) enters in a nonlinear way. a transformation of the data may be indicated because of theoretical considerations inherent in the scientific study, or a simple plotting of the data may suggest the need to reexpress the variables in the model. the need to perform a transformation is rather simple to diagnose in the case of simple linear regression because two-dimensional plots give a true pictorial display of how each variable enters the model."
2516,1,"['model', 'linear', 'transformed', 'parameters', 'regression model', 'variables', 'nonlinear regression', 'nonlinear', 'data', 'regression', 'information', 'transformation']", Data Plots and Transformations,seg_297,"a model in which x or y is transformed should not be viewed as a nonlinear regression model. we normally refer to a regression model as linear when it is linear in the parameters. in other words, suppose the complexion of the data or other scientific information suggests that we should regress y* against x*, where each is a transformation on the natural variables x and y. then the model of the form"
2517,1,"['linear', 'model', 'linear model', 'parameters']", Data Plots and Transformations,seg_297,"is a linear model since it is linear in the parameters β0 and β1. the material given in sections 11.2 through 11.9 remains intact, with yi∗ and x∗i replacing yi and xi. a simple and useful example is the log-log model"
2518,1,"['linear', 'model', 'linear model', 'parameters', 'nonlinear']", Data Plots and Transformations,seg_297,"although this model is not linear in x and y, it is linear in the parameters and is thus treated as a linear model. on the other hand, an example of a truly nonlinear model is"
2519,1,"['linear', 'model', 'estimated', 'parameter']", Data Plots and Transformations,seg_297,where the parameter β2 (as well as β0 and β1) is to be estimated. the model is not linear in β2.
2520,1,"['functions', 'model', 'linear', 'table', 'transformations', 'regression', 'linear regression', 'transformation']", Data Plots and Transformations,seg_297,"transformations that may enhance the fit and predictability of a model are many in number. for a thorough discussion of transformations, the reader is referred to myers (1990, see the bibliography). we choose here to indicate a few of them and show the appearance of the graphs that serve as a diagnostic tool. consider table 11.6. several functions are given describing relationships between y and x that can produce a linear regression through the transformation indicated."
2521,1,"['plot', 'functions', 'linear', 'simple linear regression', 'independent', 'table', 'variables', 'dependent', 'regression', 'observation', 'linear regression', 'dependent and independent variables', 'transformation']", Data Plots and Transformations,seg_297,"in addition, for the sake of completeness the reader is given the dependent and independent variables to use in the resulting simple linear regression. figure 11.19 depicts functions listed in table 11.6. these serve as a guide for the analyst in choosing a transformation from the observation of the plot of y against x."
2522,1,['transformations'], Data Plots and Transformations,seg_297,table 11.6: some useful transformations to linearize
2523,1,"['linear', 'regression', 'linear regression', 'transformation']", Data Plots and Transformations,seg_297,functional form proper form of simple relating y to x transformation linear regression
2524,0,[], Data Plots and Transformations,seg_297,1 regress y against x*
2525,1,"['functions', 'table']", Data Plots and Transformations,seg_297,figure 11.19: diagrams depicting functions listed in table 11.6.
2526,1,"['model', 'transformed', 'data', 'transformation']", Data Plots and Transformations,seg_297,"the foregoing is intended as an aid for the analyst when it is apparent that a transformation will provide an improvement. however, before we provide an example, two important points should be made. the first one revolves around the formal writing of the model when the data are transformed. quite often the analyst does not think about this. he or she merely performs the transformation without any"
2527,1,"['model', 'transformed', 'variables', 'transformation', 'error']", Data Plots and Transformations,seg_297,concern about the model form before and after the transformation. the exponential model serves as a good illustration. the model in the natural (untransformed) variables that produces an additive error model in the transformed variables is given by
2528,1,"['model', 'error']", Data Plots and Transformations,seg_297,"which is a multiplicative error model. clearly, taking logs produces"
2529,1,"['model', 'transformed', 'variables', 'transformation', 'error']", Data Plots and Transformations,seg_297,"as a result, it is on ln i that the basic assumptions are made. the purpose of this presentation is merely to remind the reader that one should not view a transformation as merely an algebraic manipulation with an error added. often a model in the transformed variables that has a proper additive error structure is a result of a model in the natural variables with a different type of error structure."
2530,1,"['model', 'transformed', 'residual', 'utility', 'residuals', 'mean square', 'response', 'mean', 'transformation', 'measuring']", Data Plots and Transformations,seg_297,"the second important point deals with the notion of measures of improvement. obvious measures of comparison are, of course, r2 and the residual mean square, s2. (other measures of performance used to compare competing models are given in chapter 12.) now, if the response y is not transformed, then clearly s2 and r2 can be used in measuring the utility of the transformation. the residuals will be in the same units for both the transformed and the untransformed models. but when y is transformed, performance criteria for the transformed model should be based on values of the residuals in the metric of the untransformed response so that comparisons that are made are proper. the example that follows provides an illustration."
2531,1,"['data', 'table']", Data Plots and Transformations,seg_297,"example 11.9: the pressure p of a gas corresponding to various volumes v is recorded, and the data are given in table 11.7."
2532,1,['data'], Data Plots and Transformations,seg_297,table 11.7: data for example 11.9
2533,1,['estimate'], Data Plots and Transformations,seg_297,"the ideal gas law is given by the functional form pv γ = c, where γ and c are constants. estimate the constants c and γ."
2534,1,['model'], Data Plots and Transformations,seg_297,solution : let us take natural logs of both sides of the model
2535,1,"['linear', 'model', 'linear model']", Data Plots and Transformations,seg_297,"as a result, a linear model can be written"
2536,1,"['simple linear regression', 'linear', 'results', 'regression', 'linear regression']", Data Plots and Transformations,seg_297,where ∗i = ln i. the following represents results of the simple linear regression:
2537,1,['slope'], Data Plots and Transformations,seg_297,"intercept: l̂nc = 14.7589, ĉ = 2, 568, 862.88, slope: γ̂ = 2.65347221."
2538,1,"['regression analysis', 'information', 'regression']", Data Plots and Transformations,seg_297,the following represents information taken from the regression analysis.
2539,1,"['plot', 'curve', 'data', 'regression']", Data Plots and Transformations,seg_297,it is instructive to plot the data and the regression equation. figure 11.20 shows a plot of the data in the untransformed pressure and volume and the curve representing the regression equation.
2540,1,"['data', 'regression']", Data Plots and Transformations,seg_297,figure 11.20: pressure and volume data and fitted regression.
2541,1,"['residuals', 'random', 'associated', 'significance', 'data', 'error', 'model', 'regression', 'significance testing', 'regression line', 'plotting', 'plot', 'random variables', 'independent', 'variables', 'independent variable', 'errors', 'variable']", Data Plots and Transformations,seg_297,"plots of the raw data can be extremely helpful in determining the nature of the model that should be fit to the data when there is a single independent variable. we have attempted to illustrate this in the foregoing. detection of proper model form is, however, not the only benefit gained from diagnostic plotting. as in much of the material associated with significance testing in chapter 10, plotting methods can illustrate and detect violation of assumptions. the reader should recall that much of what is illustrated in this chapter requires assumptions made on the model errors, the i. in fact, we assume that the i are independent n(0, σ) random variables. now, of course, the i are not observed. however, the ei = yi − ŷi, the residuals, are the error in the fit of the regression line and thus serve to mimic the i. thus, the general complexion of these residuals can often highlight difficulties. ideally, of course, the plot of the residuals is as depicted in figure 11.21. that is, they should truly show random fluctuations around a value of zero."
2542,1,"['plot', 'condition', 'residual', 'plots', 'residuals', 'regression analysis', 'data', 'residual plot', 'regression', 'variable', 'residual plots', 'information', 'variance', 'error']", Data Plots and Transformations,seg_297,"homogeneous variance is an important assumption made in regression analysis. violations can often be detected through the appearance of the residual plot. increasing error variance with an increase in the regressor variable is a common condition in scientific data. large error variance produces large residuals, and hence a residual plot like the one in figure 11.22 is a signal of nonhomogeneous variance. more discussion regarding these residual plots and information regard-"
2543,1,"['plot', 'residual', 'residual plot', 'variance', 'error']", Data Plots and Transformations,seg_297,figure 11.21: ideal residual plot. figure 11.22: residual plot depicting heterogeneous error variance.
2544,1,"['linear', 'residuals', 'regression', 'linear regression', 'multiple linear regression']", Data Plots and Transformations,seg_297,"ing different types of residuals appears in chapter 12, where we deal with multiple linear regression."
2545,1,"['probability plots', 'interval', 'plots', 'residuals', 'case', 'probability', 'numerical', 'data', 'hypothesis testing', 'confidence', 'confidence interval', 'model', 'estimation', 'plotting', 'normal probability plots', 'hypothesis', 'errors', 'normal', 'interval estimation']", Data Plots and Transformations,seg_297,"the assumption that the model errors are normal is made when the data analyst deals in either hypothesis testing or confidence interval estimation. again, the numerical counterpart to the i, namely the residuals, are subjects of diagnostic plotting to detect any extreme violations. in chapter 8, we introduced normal quantile-quantile plots and briefly discussed normal probability plots. these plots on residuals are illustrated in the case study introduced in the next section."
2546,1,"['densities', 'table', 'data', 'estimate']", Simple Linear Regression Case Study,seg_299,"in the manufacture of commercial wood products, it is important to estimate the relationship between the density of a wood product and its stiffness. a relatively new type of particleboard is being considered that can be formed with considerably more ease than the accepted commercial product. it is necessary to know at what density the stiffness is comparable to that of the well-known, well-documented commercial product. a study was done by terrance e. conners, investigation of certain mechanical properties of a wood-foam composite (m.s. thesis, department of forestry and wildlife management, university of massachusetts). thirty particleboards were produced at densities ranging from roughly 8 to 26 pounds per cubic foot, and the stiffness was measured in pounds per square inch. table 11.8 shows the data."
2547,1,"['simple linear regression', 'interval', 'prediction', 'prediction interval', 'linear', 'data', 'hypothesis testing', 'confidence', 'estimation', 'regression', 'linear regression', 'hypothesis', 'plot', 'scatter plot', 'interval estimation', 'slope']", Simple Linear Regression Case Study,seg_299,"it is necessary for the data analyst to focus on an appropriate fit to the data and use inferential methods discussed in this chapter. hypothesis testing on the slope of the regression, as well as confidence or prediction interval estimation, may well be appropriate. we begin by demonstrating a simple scatter plot of the raw data with a simple linear regression superimposed. figure 11.23 shows this plot."
2548,1,"['simple linear regression', 'linear', 'model', 'data', 'regression', 'linear regression']", Simple Linear Regression Case Study,seg_299,the simple linear regression fit to the data produced the fitted model
2549,0,[], Simple Linear Regression Case Study,seg_299,table 11.8: density and stiffness for 30 particleboards
2550,0,[], Simple Linear Regression Case Study,seg_299,"density, x stiffness, y density, x stiffness, y"
2551,1,"['plot', 'residual', 'scatter plot', 'residual plot', 'data']", Simple Linear Regression Case Study,seg_299,figure 11.23: scatter plot of the wood density data. figure 11.24: residual plot for the wood density
2552,1,"['residuals plotted', 'residuals', 'data', 'set', 'measurements', 'random']", Simple Linear Regression Case Study,seg_299,"and the residuals were computed. figure 11.24 shows the residuals plotted against the measurements of density. this is hardly an ideal or healthy set of residuals. they do not show a random scatter around a value of zero. in fact, clusters of positive and negative values suggest that a curvilinear trend in the data should be investigated."
2553,1,"['plot', 'probability plot', 'residuals', 'normal', 'probability', 'normal probability plot', 'error']", Simple Linear Regression Case Study,seg_299,"to gain some type of idea regarding the normal error assumption, a normal probability plot of the residuals was generated. this is the type of plot discussed in"
2554,1,"['plot', 'model', 'regression model', 'probability plot', 'distribution function', 'residuals', 'regression', 'distribution', 'normal', 'probability', 'normal probability plot', 'function', 'normal distribution']", Simple Linear Regression Case Study,seg_299,"section 8.8 in which the horizontal axis represents the empirical normal distribution function on a scale that produces a straight-line plot when plotted against the residuals. figure 11.25 shows the normal probability plot of the residuals. the normal probability plot does not reflect the straight-line appearance that one would like to see. this is another symptom of a faulty, perhaps overly simplistic choice of a regression model."
2555,1,"['plot', 'probability plot', 'residuals', 'data', 'normal', 'probability', 'normal probability plot']", Simple Linear Regression Case Study,seg_299,figure 11.25: normal probability plot of residuals for wood density data.
2556,1,"['plot', 'model', 'residual', 'scatter plot', 'plots', 'regression', 'transformation', 'residual plots']", Simple Linear Regression Case Study,seg_299,"both types of residual plots and, indeed, the scatter plot itself suggest here that a somewhat complicated model would be appropriate. one possible approach is to use a natural log transformation. in other words, one might choose to regress ln y against x. this produces the regression"
2557,1,"['model', 'transformed', 'plots', 'residuals']", Simple Linear Regression Case Study,seg_299,"to gain some insight into whether the transformed model is more appropriate, consider figures 11.26 and 11.27, which reveal plots of the residuals in stiffness [i.e.,"
2558,1,"['model', 'transformed', 'random']", Simple Linear Regression Case Study,seg_299,"yi-antilog (l̂n y)] against density. figure 11.26 appears to be closer to a random pattern around zero, while figure 11.27 is certainly closer to a straight line. this in addition to the higher r2-value would suggest that the transformed model is more appropriate."
2559,1,"['random variables', 'independent', 'variables', 'observations', 'regression', 'sampling', 'random variable', 'variable', 'measurements', 'population', 'random', 'process', 'error']", Correlation,seg_301,"up to this point we have assumed that the independent regressor variable x is a physical or scientific variable but not a random variable. in fact, in this context, x is often called a mathematical variable, which, in the sampling process, is measured with negligible error. in many applications of regression techniques, it is more realistic to assume that both x and y are random variables and the measurements {(xi, yi); i = 1, 2, . . . , n} are observations from a population having"
2560,1,"['plot', 'probability plot', 'residual', 'residuals', 'residual plot', 'data', 'normal', 'probability', 'normal probability plot', 'transformation']", Correlation,seg_301,figure 11.26: residual plot using the log transforfigure 11.27: normal probability plot of residuals mation for the wood density data. using the log transformation for the wood density data.
2561,1,"['density function', 'variables', 'joint', 'associated', 'function', 'measuring']", Correlation,seg_301,"the joint density function f(x, y). we shall consider the problem of measuring the relationship between the two variables x and y. for example, if x and y represent the length and circumference of a particular kind of bone in the adult body, we might conduct an anthropological study to determine whether large values of x are associated with large values of y, and vice versa."
2562,1,"['variables', 'correlation', 'correlation coefficient', 'coefficient']", Correlation,seg_301,"on the other hand, if x represents the age of a used automobile and y represents the retail book value of the automobile, we would expect large values of x to correspond to small values of y and small values of x to correspond to large values of y. correlation analysis attempts to measure the strength of such relationships between two variables by means of a single number called a correlation coefficient."
2563,1,"['conditional', 'distribution', 'normal', 'mean', 'variance', 'conditional distribution']", Correlation,seg_301,"in theory, it is often assumed that the conditional distribution f(y|x) of y, for fixed values of x, is normal with mean μy |x = β0 + β1x and variance σy"
2564,1,"['mean', 'variance', 'normally distributed']", Correlation,seg_301,and that x is likewise normally distributed with mean μ and variance σx
2565,0,[], Correlation,seg_301,joint density of x and y is then
2566,1,"['random variable', 'variable', 'random']", Correlation,seg_301,let us write the random variable y in the form
2567,1,"['independent', 'random variable', 'variable', 'mean', 'random', 'random error', 'error']", Correlation,seg_301,"where x is now a random variable independent of the random error . since the mean of the random error is zero, it follows that"
2568,1,"['bivariate normal distribution', 'distribution', 'normal', 'bivariate', 'normal distribution']", Correlation,seg_301,"substituting for α and σ2 into the preceding expression for f(x, y), we obtain the bivariate normal distribution"
2569,1,"['linear', 'estimates', 'coefficient', 'results', 'data', 'correlation', 'regression', 'linear regression', 'population', 'bivariate data', 'bivariate', 'correlation coefficient', 'regression line']", Correlation,seg_301,"the constant ρ (rho) is called the population correlation coefficient and plays a major role in many bivariate data analysis problems. it is important for the reader to understand the physical interpretation of this correlation coefficient and the distinction between correlation and regression. the term regression still has meaning here. in fact, the straight line given by μy |x = β0 + β1x is still called the regression line as before, and the estimates of β0 and β1 are identical to those given in section 11.3. the value of ρ is 0 when β1 = 0, which results when there essentially is no linear regression; that is, the regression line is horizontal and any knowledge of x is useless in predicting y. since σy"
2570,1,"['sample', 'linear', 'association', 'variables', 'estimates', 'results', 'case', 'correlation', 'slope']", Correlation,seg_301,"2 ≥ σ2, we must have ρ2 ≤ 1 and hence −1 ≤ ρ ≤ 1. values of ρ = ±1 only occur when σ2 = 0, in which case we have a perfect linear relationship between the two variables. thus, a value of ρ equal to +1 implies a perfect linear relationship with a positive slope, while a value of ρ equal to −1 results from a perfect linear relationship with a negative slope. it might be said, then, that sample estimates of ρ close to unity in magnitude imply good correlation, or linear association, between x and y, whereas values near zero indicate little or no correlation."
2571,1,"['sample', 'estimate', 'sum of squares', 'error sum of squares', 'error']", Correlation,seg_301,"to obtain a sample estimate of ρ, recall from section 11.4 that the error sum of squares is"
2572,0,[], Correlation,seg_301,"dividing both sides of this equation by syy and replacing sxy by b1sxx, we obtain the relation"
2573,1,"['sample', 'linear']", Correlation,seg_301,"the value of b12sxx/syy is zero when b1 = 0, which will occur when the sample points show no linear relationship. since syy ≥ sse, we conclude that b12sxx/sxy"
2574,1,"['sample', 'linear', 'range', 'case', 'data', 'slopes']", Correlation,seg_301,"must be between 0 and l. consequently, b1√sxx/syy must range from −1 to +1, negative values corresponding to lines with negative slopes and positive values to lines with positive slopes. a value of −1 or +1 will occur when sse = 0, but this is the case where all sample points lie in a straight line. hence, a perfect linear relationship appears in the sample data when b1√sxx/syy = ±1. clearly, the"
2575,1,"['sample', 'estimate', 'correlation', 'pearson', 'correlation coefficient', 'population', 'coefficient', 'sample correlation coefficient']", Correlation,seg_301,"quantity b1√sxx/syy, which we shall henceforth designate as r, can be used as an estimate of the population correlation coefficient ρ. it is customary to refer to the estimate r as the pearson product-moment correlation coefficient or simply the sample correlation coefficient."
2576,1,"['sample', 'linear', 'estimated', 'association', 'variables', 'correlation', 'correlation coefficient', 'coefficient', 'sample correlation coefficient']", Correlation,seg_301,"correlation the measure ρ of linear association between two variables x and y is estimated coefficient by the sample correlation coefficient r, where"
2577,1,"['correlations', 'linear', 'mean']", Correlation,seg_301,"for values of r between −1 and +1 we must be careful in our interpretation. for example, values of r equal to 0.3 and 0.6 only mean that we have two positive correlations, one somewhat stronger than the other. it is wrong to conclude that r = 0.6 indicates a linear relationship twice as good as that indicated by the value r = 0.3. on the other hand, if we write"
2578,1,"['sample', 'linear', 'correlation', 'variation', 'random variable', 'variable', 'regression', 'random', 'coefficient', 'coefficient of determination']", Correlation,seg_301,"sx2y r2 = = , sxxsyy syy then r2, which is usually referred to as the sample coefficient of determination, represents the proportion of the variation of syy explained by the regression of y on x, namely ssr. that is, r2 expresses the proportion of the total variation in the values of the variable y that can be accounted for or explained by a linear relationship with the values of the random variable x. thus, a correlation of 0.6 means that 0.36, or 36%, of the total variation of the values of y in our sample is accounted for by a linear relationship with values of x."
2579,1,"['sample', 'quantitative', 'table', 'data', 'correlation', 'correlation coefficient', 'coefficient', 'sample correlation coefficient']", Correlation,seg_301,"example 11.10: it is important that scientific researchers in the area of forest products be able to study correlation among the anatomy and mechanical properties of trees. for the study quantitative anatomical characteristics of plantation grown loblolly pine (pinus taeda l.) and cottonwood (populus deltoides bart. ex marsh.) and their relationships to mechanical properties, conducted by the department of forestry and forest products at virginia tech, 29 loblolly pines were randomly selected for investigation. table 11.9 shows the resulting data on the specific gravity in grams/cm3 and the modulus of rupture in kilopascals (kpa). compute and interpret the sample correlation coefficient."
2580,1,['data'], Correlation,seg_301,table 11.9: data on 29 loblolly pines for example 11.10
2581,1,['data'], Correlation,seg_301,solution : from the data we find that
2582,1,"['linear', 'correlation', 'correlation coefficient', 'variation', 'coefficient']", Correlation,seg_301,"a correlation coefficient of 0.9435 indicates a good linear relationship between x and y. since r2 = 0.8902, we can say that approximately 89% of the variation in the values of y is accounted for by a linear relationship with x."
2583,1,"['simple linear regression', 'degrees of freedom', 'correlation', 'sample correlation coefficient', 'sample', 'linear', 'coefficient', 'linear regression model', 'model', 'regression model', 'regression', 'linear regression', 'correlation coefficient', 'test', 'hypothesis']", Correlation,seg_301,"a test of the special hypothesis ρ = 0 versus an appropriate alternative is equivalent to testing β1 = 0 for the simple linear regression model, and therefore the procedures of section 11.8 using either the t-distribution with n− 2 degrees of freedom or the f -distribution with 1 and n− 2 degrees of freedom are applicable. however, if one wishes to avoid the analysis-of-variance procedure and compute only the sample correlation coefficient, it can be verified (see review exercise 11.66 on page 438) that the t-value"
2584,0,[], Correlation,seg_301,can also be written as
2585,1,"['degrees of freedom', 'statistic']", Correlation,seg_301,"which, as before, is a value of the statistic t having a t-distribution with n − 2 degrees of freedom."
2586,1,"['linear', 'association', 'variables', 'data', 'test', 'hypothesis']", Correlation,seg_301,"example 11.11: for the data of example 11.10, test the hypothesis that there is no linear association among the variables."
2587,1,['critical region'], Correlation,seg_301,4. critical region: t < −2.052 or t > 2.052.
2588,1,"['linear', 'association', 'hypothesis']", Correlation,seg_301,√1−0.94352 6. decision: reject the hypothesis of no linear association.
2589,1,"['sample', 'bivariate normal distribution', 'information', 'distribution', 'normal', 'bivariate', 'test', 'normal distribution', 'hypothesis']", Correlation,seg_301,"a test of the more general hypothesis ρ = ρ0 against a suitable alternative is easily conducted from the sample information. if x and y follow the bivariate normal distribution, the quantity"
2590,1,"['distribution', 'random variable', 'variable', 'normal', 'mean', 'random', 'normal distribution']", Correlation,seg_301,is the value of a random variable that follows approximately the normal distribution with mean 1
2591,1,"['test', 'variance']", Correlation,seg_301,"+ρ ρ and variance 1/(n−3). thus, the test procedure is to compute"
2592,1,"['standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Correlation,seg_301,and compare it with the critical points of the standard normal distribution.
2593,1,"['data', 'level', 'significance', 'test', 'level of significance', 'null hypothesis', 'hypothesis']", Correlation,seg_301,"example 11.12: for the data of example 11.10, test the null hypothesis that ρ = 0.9 against the alternative that ρ > 0.9. use a 0.05 level of significance."
2594,1,['critical region'], Correlation,seg_301,4. critical region: z > 1.645.
2595,1,"['scatter diagram', 'correlation']", Correlation,seg_301,figure 11.28: scatter diagram showing zero correlation.
2596,0,[], Correlation,seg_301,5. computations:
2597,1,"['coefficient', 'correlation coefficient', 'correlation']", Correlation,seg_301,6. decision: there is certainly some evidence that the correlation coefficient does
2598,1,"['correlation', 'random', 'sample correlation coefficient', 'sample', 'linear', 'association', 'results', 'data', 'mean', 'coefficient', 'model', 'experimental', 'nonlinear', 'regression', 'linear regression', 'correlation coefficient', 'plotting', 'variables', 'normal', 'bivariate']", Correlation,seg_301,"not exceed 0.9. it should be pointed out that in correlation studies, as in linear regression problems, the results obtained are only as good as the model that is assumed. in the correlation techniques studied here, a bivariate normal density is assumed for the variables x and y, with the mean value of y at each x-value being linearly related to x. to observe the suitability of the linearity assumption, a preliminary plotting of the experimental data is often helpful. a value of the sample correlation coefficient close to zero will result from data that display a strictly random effect as in figure 11.28(a), thus implying little or no causal relationship. it is important to remember that the correlation coefficient between two variables is a measure of their linear relationship and that a value of r = 0 implies a lack of linearity and not a lack of association. hence, if a strong quadratic relationship exists between x and y, as indicated in figure 11.28(b), we can still obtain a zero correlation indicating a nonlinear relationship."
2599,1,"['plot', 'simple linear regression', 'linear', 'probability plot', 'residual', 'plots', 'residuals', 'data', 'standardized', 'regression', 'normal probability plot', 'normal', 'linear regression', 'probability']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_307,"anytime one is considering the use of simple linear regression, a plot of the data is not only recommended but essential. a plot of the ordinary residuals and a normal probability plot of these residuals are always edifying. in addition, we introduce and illustrate an additional type of residual in chapter 12 that is in a standardized form. all of these plots are designed to detect violation of assumptions."
2600,1,"['residual', 'plots', 'regression coefficients', 'homogeneous', 'regression', 'normality', 'residual plots', 'variance', 'tests', 'coefficients']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_307,"the use of t-statistics for tests on regression coefficients is reasonably robust to the normality assumption. the homogeneous variance assumption is crucial, and residual plots are designed to detect a violation."
2601,1,"['model', 'method', 'variables', 'residual', 'plots', 'regression', 'least squares', 'variable', 'information', 'multiple regression', 'method of least squares', 'residual plots']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_307,"the material in this chapter is used heavily in chapters 12 and 15. all of the information involving the method of least squares in the development of regression models carries over into chapter 12. the difference is that chapter 12 deals with the scientific conditions in which there is more than a single x variable, i.e., more than one regression variable. however, material in the current chapter that deals with regression diagnostics, types of residual plots, measures of model quality, and so on, applies and will carry over. the student will realize that more complications occur in chapter 12 because the problems in multiple regression models often involve the backdrop of questions regarding how the various regression variables enter the model and even issues of which variables should remain in the model. certainly chapter 15 heavily involves the use of regression modeling, but we will preview the connection in the summary at the end of chapter 12."
2602,1,"['case', 'multiple linear regression', 'coefficients', 'linear', 'multiple regression', 'mean', 'linear regression model', 'model', 'regression model', 'regression', 'linear regression', 'response', 'independent', 'variables', 'regression analysis', 'independent variable', 'variable']", Introduction,seg_311,"in most research problems where regression analysis is applied, more than one independent variable is needed in the regression model. the complexity of most scientific mechanisms is such that in order to be able to predict an important response, a multiple regression model is needed. when this model is linear in the coefficients, it is called a multiple linear regression model. for the case of k independent variables x1, x2, . . . , xk, the mean of y |x1, x2, . . . , xk is given by the multiple linear regression model"
2603,1,"['sample', 'estimated', 'regression', 'response']", Introduction,seg_311,and the estimated response is obtained from the sample regression equation
2604,1,"['case', 'multiple linear regression', 'method of least squares', 'sample', 'linear', 'data', 'regression coefficient', 'coefficient', 'linear regression model', 'model', 'regression model', 'regression', 'linear regression', 'estimated', 'independent', 'method', 'variables', 'independent variable', 'least squares', 'variable']", Introduction,seg_311,"where each regression coefficient βi is estimated by bi from the sample data using the method of least squares. as in the case of a single independent variable, the multiple linear regression model can often be an adequate representation of a more complicated structure within certain ranges of the independent variables."
2605,1,"['polynomial regression model', 'model', 'linear', 'independent', 'linear model', 'regression model', 'variables', 'least squares', 'regression', 'polynomial regression']", Introduction,seg_311,"similar least squares techniques can also be applied for estimating the coefficients when the linear model involves, say, powers and products of the independent variables. for example, when k = 1, the experimenter may believe that the means μy |x do not fall on a straight line but are more appropriately described by the polynomial regression model"
2606,1,"['estimated', 'regression', 'polynomial regression', 'response']", Introduction,seg_311,and the estimated response is obtained from the polynomial regression equation
2607,1,"['model', 'linear', 'independent', 'linear model', 'parameters', 'variables', 'nonlinear', 'statisticians', 'exponential']", Introduction,seg_311,"confusion arises occasionally when we speak of a polynomial model as a linear model. however, statisticians normally refer to a linear model as one in which the parameters occur linearly, regardless of how the independent variables enter the model. an example of a nonlinear model is the exponential relationship"
2608,1,"['regression', 'response', 'estimated']", Introduction,seg_311,"μy |x = αβx, whose response is estimated by the regression equation"
2609,1,"['poisson', 'model', 'estimation', 'nonlinear', 'least squares', 'normally distributed', 'distribution', 'binomial', 'poisson distribution', 'response', 'error']", Introduction,seg_311,"there are many phenomena in science and engineering that are inherently nonlinear in nature, and when the true structure is known, an attempt should certainly be made to fit the actual model. the literature on estimation by least squares of nonlinear models is voluminous. the nonlinear models discussed in this chapter deal with nonideal conditions in which the analyst is certain that the response and hence the response model error are not normally distributed but, rather, have a binomial or poisson distribution. these situations do occur extensively in practice."
2610,1,"['nonlinear regression', 'nonlinear', 'regression']", Introduction,seg_311,a student who wants a more general account of nonlinear regression should consult classical and modern regression with applications by myers (1990; see the bibliography).
2611,1,"['linear', 'model', 'regression model', 'parameters', 'least squares', 'estimators', 'regression', 'least squares estimators', 'linear regression', 'multiple linear regression', 'linear regression model']", Estimating the Coefficients,seg_313,"in this section, we obtain the least squares estimators of the parameters β0, β1, . . . , βk by fitting the multiple linear regression model"
2612,1,['data'], Estimating the Coefficients,seg_313,to the data points
2613,1,"['independent', 'variables', 'observation', 'response']", Estimating the Coefficients,seg_313,"where yi is the observed response to the values x1i, x2i, . . . , xki of the k independent variables x1, x2, . . . , xk. each observation (x1i, x2i, . . . , xki, yi) is assumed to satisfy the following equation."
2614,1,['linear'], Estimating the Coefficients,seg_313,multiple linear yi = β0 + β1x1i + β2x2i + · · ·+ βkxki + i
2615,1,['model'], Estimating the Coefficients,seg_313,regression model or
2616,1,"['random', 'residual', 'associated', 'random error', 'response', 'error']", Estimating the Coefficients,seg_313,"where i and ei are the random error and residual, respectively, associated with the response yi and fitted value ŷi."
2617,1,"['simple linear regression', 'linear', 'independent', 'case', 'regression', 'linear regression', 'mean', 'variance']", Estimating the Coefficients,seg_313,"as in the case of simple linear regression, it is assumed that the i are independent and identically distributed with mean 0 and common variance σ2."
2618,1,"['estimates', 'least squares']", Estimating the Coefficients,seg_313,"in using the concept of least squares to arrive at estimates b0, b1, . . . , bk, we minimize the expression"
2619,1,"['normal equations', 'linear', 'regression', 'normal equations for multiple linear regression', 'set', 'normal', 'linear regression', 'multiple linear regression']", Estimating the Coefficients,seg_313,"n n sse =∑ e2i =∑(yi − b0 − b1x1i − b2x2i − · · · − bkxki)2. i=1 i=1 differentiating sse in turn with respect to b0, b1, . . . , bk and equating to zero, we generate the set of k + 1 normal equations for multiple linear regression."
2620,1,"['estimation', 'linear', 'normal']", Estimating the Coefficients,seg_313,n n n n normal estimation nb0 + b1∑x1i + b2∑x2i + · · · + bk∑xki =∑ yi equations for i=1 i=1 i=1 i=1 multiple linear n n n n n
2621,1,"['linear', 'method', 'statistical', 'numerical']", Estimating the Coefficients,seg_313,"these equations can be solved for b0, b1, b2, . . . , bk by any appropriate method for solving systems of linear equations. most statistical software can be used to obtain numerical solutions of the above equations."
2622,1,"['model', 'experimental', 'table', 'data', 'measurements', 'varying']", Estimating the Coefficients,seg_313,"example 12.1: a study was done on a diesel-powered light-duty pickup truck to see if humidity, air temperature, and barometric pressure influence emission of nitrous oxide (in ppm). emission measurements were taken at different times, with varying experimental conditions. the data are given in table 12.2. the model is"
2623,0,[], Estimating the Coefficients,seg_313,"or, equivalently,"
2624,1,"['linear', 'model', 'regression model', 'estimate', 'data', 'regression', 'linear regression', 'multiple linear regression', 'linear regression model']", Estimating the Coefficients,seg_313,"fit this multiple linear regression model to the given data and then estimate the amount of nitrous oxide emitted for the conditions where humidity is 50%, temperature is 76◦f, and barometric pressure is 29.30."
2625,1,['data'], Estimating the Coefficients,seg_313,table 12.1: data for example 12.1
2626,1,['factors'], Estimating the Coefficients,seg_313,"nitrous humidity, temp., pressure, nitrous humidity, temp., pressure, oxide, y x1 x2 x3 oxide, y x1 x2 x3 0.90 72.4 76.3 29.18 1.07 23.2 76.8 29.38 0.91 41.6 70.3 29.35 0.94 47.4 86.6 29.35 0.96 34.3 77.1 29.24 1.10 31.5 76.9 29.63 0.89 35.1 68.0 29.27 1.10 10.6 86.3 29.56 1.00 10.7 79.0 29.78 1.10 11.2 86.0 29.48 1.10 12.9 67.4 29.39 0.91 73.3 76.3 29.40 1.15 8.3 66.8 29.69 0.87 75.4 77.9 29.28 1.03 20.1 76.9 29.48 0.78 96.6 78.7 29.29 0.77 72.2 77.7 29.09 0.82 107.4 86.8 29.03 1.07 24.0 67.7 29.60 0.95 54.9 70.9 29.37 source: charles t. hare, “light-duty diesel emission correction factors for ambient conditions,” epa-600/2-77- 116. u.s. environmental protection agency."
2627,1,"['estimates', 'set']", Estimating the Coefficients,seg_313,solution : the solution of the set of estimating equations yields the unique estimates
2628,1,['regression'], Estimating the Coefficients,seg_313,"therefore, the regression equation is"
2629,1,['estimated'], Estimating the Coefficients,seg_313,"for 50% humidity, a temperature of 76◦f, and a barometric pressure of 29.30, the estimated amount of nitrous oxide emitted is"
2630,0,[], Estimating the Coefficients,seg_313,now suppose that we wish to fit the polynomial equation
2631,1,"['observations', 'observation']", Estimating the Coefficients,seg_313,"to the n pairs of observations {(xi, yi); i = 1, 2, . . . , n}. each observation, yi, satisfies the equation"
2632,1,"['estimated', 'parameters', 'residual', 'associated', 'random', 'random error', 'response', 'error']", Estimating the Coefficients,seg_313,"where r is the degree of the polynomial and i and ei are again the random error and residual associated with the response yi and fitted value ŷi, respectively. here, the number of pairs, n, must be at least as large as r+1, the number of parameters to be estimated."
2633,1,"['normal equations', 'linear', 'model', 'regression model', 'case', 'regression', 'set', 'normal', 'linear regression', 'multiple linear regression', 'linear regression model']", Estimating the Coefficients,seg_313,"notice that the polynomial model can be considered a special case of the more general multiple linear regression model, where we set x1 = x, x2 = x2, . . . , xr = xr. the normal equations assume the same form as those given on page 445. they are then solved for b0, b1, b2, . . . , br."
2634,1,['data'], Estimating the Coefficients,seg_313,example 12.2: given the data x 0 1 2 3 4 5 6 7 8 9 y 9.1 7.3 3.2 4.6 4.8 2.9 5.7 7.1 8.8 10.2
2635,1,"['curve', 'regression', 'estimate']", Estimating the Coefficients,seg_313,fit a regression curve of the form μy |x = β0 + β1x+ β2x2 and then estimate μy |2.
2636,1,['data'], Estimating the Coefficients,seg_313,"solution : from the data given, we find that"
2637,1,"['normal equations', 'normal']", Estimating the Coefficients,seg_313,"solving these normal equations, we obtain"
2638,1,['estimate'], Estimating the Coefficients,seg_313,"when x = 2, our estimate of μy |2 is"
2639,1,"['model', 'coefficients', 'estimate', 'table', 'regression coefficients', 'data', 'regression', 'associated', 'percent']", Estimating the Coefficients,seg_313,example 12.3: the data in table 12.2 represent the percent of impurities that resulted for various temperatures and sterilizing times during a reaction associated with the manufacturing of a certain beverage. estimate the regression coefficients in the polynomial model
2640,1,['data'], Estimating the Coefficients,seg_313,table 12.2: data for example 12.3
2641,0,[], Estimating the Coefficients,seg_313,"sterilizing temperature, x1 (◦c) time, x2 (min) 75 100 125"
2642,1,"['normal equations', 'normal']", Estimating the Coefficients,seg_313,"solution : using the normal equations, we obtain"
2643,1,"['regression', 'estimated']", Estimating the Coefficients,seg_313,and our estimated regression equation is
2644,1,"['design', 'associated', 'process', 'regression functions', 'cases', 'response surface methodology', 'functions', 'model', 'experimental', 'estimation', 'regression', 'interaction terms', 'response', 'variables', 'interaction', 'experiments', 'response surface']", Estimating the Coefficients,seg_313,"many of the principles and procedures associated with the estimation of polynomial regression functions fall into the category of response surface methodology, a collection of techniques that have been used quite successfully by scientists and engineers in many fields. the xi2 are called pure quadratic terms, and the xixj (i = j) are called interaction terms. such problems as selecting a proper experimental design, particularly in cases where a large number of variables are in the model, and choosing optimum operating conditions for x1, x2, . . . , xk are often approached through the use of these methods. for an extensive exposure, the reader is referred to response surface methodology: process and product optimization using designed experiments by myers, montgomery, and anderson-cook (2009; see the bibliography)."
2645,1,"['linear', 'model', 'independent', 'regression model', 'regression', 'linear regression', 'multiple linear regression', 'linear regression model']", Linear Regression Model Using Matrices,seg_315,"in fitting a multiple linear regression model, particularly when the number of variables exceeds two, a knowledge of matrix theory can facilitate the mathematical manipulations considerably. suppose that the experimenter has k independent"
2646,1,['observations'], Linear Regression Model Using Matrices,seg_315,"variables x1, x2, . . . , xk and n observations y1, y2, . . . , yn, each of which can be expressed by the equation"
2647,1,"['model', 'process', 'response']", Linear Regression Model Using Matrices,seg_315,"this model essentially represents n equations describing how the response values are generated in the scientific process. using matrix notation, we can write the following equation:"
2648,1,"['linear', 'model']", Linear Regression Model Using Matrices,seg_315,"general linear y = xβ + , model where"
2649,1,"['least squares method', 'estimation', 'least squares', 'method']", Linear Regression Model Using Matrices,seg_315,"then the least squares method for estimation of β, illustrated in section 12.2, involves finding b for which"
2650,1,['process'], Linear Regression Model Using Matrices,seg_315,is minimized. this minimization process involves solving for b in the equation
2651,0,[], Linear Regression Model Using Matrices,seg_315,we will not present the details regarding solution of the equations above. the result reduces to the solution of b in
2652,1,['response'], Linear Regression Model Using Matrices,seg_315,"(x′x)b = x′y. notice the nature of the x matrix. apart from the initial element, the ith row represents the x-values that give rise to the response yi. writing"
2653,1,"['normal equations', 'normal']", Linear Regression Model Using Matrices,seg_315,allows the normal equations to be put in the matrix form
2654,1,"['regression coefficients', 'regression', 'coefficients']", Linear Regression Model Using Matrices,seg_315,"if the matrix a is nonsingular, we can write the solution for the regression coefficients as"
2655,1,"['estimates', 'prediction', 'regression coefficients', 'regression', 'information', 'multiple regression', 'set', 'coefficients']", Linear Regression Model Using Matrices,seg_315,"thus, we can obtain the prediction equation or regression equation by solving a set of k+1 equations in a like number of unknowns. this involves the inversion of the k+1 by k+1 matrix x′x. techniques for inverting this matrix are explained in most textbooks on elementary determinants and matrices. of course, there are many high-speed computer packages available for multiple regression problems, packages that not only print out estimates of the regression coefficients but also provide other information relevant to making inferences concerning the regression equation."
2656,1,"['linear', 'model', 'rate', 'regression model', 'estimate', 'table', 'combinations', 'data', 'regression', 'linear regression', 'multiple linear regression', 'percent', 'linear regression model']", Linear Regression Model Using Matrices,seg_315,"example 12.4: the percent survival rate of sperm in a certain type of animal semen, after storage, was measured at various combinations of concentrations of three materials used to increase chance of survival. the data are given in table 12.3. estimate the multiple linear regression model for the given data."
2657,1,['data'], Linear Regression Model Using Matrices,seg_315,table 12.3: data for example 12.4
2658,0,[], Linear Regression Model Using Matrices,seg_315,y (% survival) x1 (weight %) x2 (weight %) x3 (weight %)
2659,1,['least squares'], Linear Regression Model Using Matrices,seg_315,"solution : the least squares estimating equations, (x′x)b = x′y, are"
2660,1,['inverse matrix'], Linear Regression Model Using Matrices,seg_315,from a computer readout we obtain the elements of the inverse matrix
2661,1,"['estimated', 'regression coefficients', 'regression', 'coefficients']", Linear Regression Model Using Matrices,seg_315,"and then, using the relation b = (x′x)−1x′y, the estimated regression coefficients are obtained as"
2662,1,"['linear', 'nonlinear regression', 'nonlinear', 'regression', 'linear regression', 'multiple linear regression']", Linear Regression Model Using Matrices,seg_315,450 chapter 12 multiple linear regression and certain nonlinear regression models
2663,1,"['regression', 'estimated']", Linear Regression Model Using Matrices,seg_315,"hence, our estimated regression equation is"
2664,1,"['simple linear regression', 'case', 'estimators', 'multiple linear regression', 'random', 'coefficients', 'linear', 'mean', 'inverse matrix', 'unbiased estimators', 'regression', 'linear regression', 'variance', 'unbiased', 'estimated', 'independent', 'random errors', 'regression coefficients', 'errors', 'variances']", Properties of the Least Squares Estimators,seg_319,"the means and variances of the estimators b0, b1, . . . , bk are readily obtained under certain assumptions on the random errors 1, 2, . . . , k that are identical to those made in the case of simple linear regression. when we assume these errors to be independent, each with mean 0 and variance σ2, it can be shown that b0, b1, . . . , bk are, respectively, unbiased estimators of the regression coefficients β0, β1, . . . , βk. in addition, the variances of the b’s are obtained through the elements of the inverse of the a matrix. note that the off-diagonal elements of a = x′x represent sums of products of elements in the columns of x, while the diagonal elements of a represent sums of squares of elements in the columns of x. the inverse matrix, a−1, apart from the multiplier σ2, represents the variance-covariance matrix of the estimated regression coefficients. that is, the elements of the matrix a−1σ2 display the variances of b0, b1, . . . , bk on the main diagonal and covariances on the off-diagonal. for example, in a k = 2 multiple linear regression problem, we might write"
2665,1,['symmetry'], Properties of the Least Squares Estimators,seg_319,with the elements below the main diagonal determined through the symmetry of the matrix. then we can write
2666,1,"['standard errors', 'experimental', 'estimate', 'estimates', 'data', 'estimators', 'errors', 'standard', 'unbiased', 'variances']", Properties of the Least Squares Estimators,seg_319,"of course, the estimates of the variances and hence the standard errors of these estimators are obtained by replacing σ2 with the appropriate estimate obtained through experimental data. an unbiased estimate of σ2 is once again defined in"
2667,1,"['sum of squares', 'error sum of squares', 'error']", Properties of the Least Squares Estimators,seg_319,"terms of the error sum of squares, which is computed using the formula established in theorem 12.1. in the theorem, we are making the assumptions on the i described above."
2668,1,"['linear', 'linear regression', 'regression']", Properties of the Least Squares Estimators,seg_319,for the linear regression equation
2669,1,"['estimate', 'residual', 'mean square', 'mean', 'error', 'unbiased']", Properties of the Least Squares Estimators,seg_319,an unbiased estimate of σ2 is given by the error or residual mean square
2670,1,"['simple linear regression', 'linear', 'estimate', 'prediction', 'residuals', 'case', 'errors', 'regression', 'linear regression', 'variation']", Properties of the Least Squares Estimators,seg_319,"we can see that theorem 12.1 represents a generalization of theorem 11.1 for the simple linear regression case. the proof is left for the reader. as in the simpler linear regression case, the estimate s2 is a measure of the variation in the prediction errors or residuals. other important inferences regarding the fitted regression equation, based on the values of the individual residuals ei = yi − ŷi, i = 1, 2, . . . , n, are discussed in sections 12.10 and 12.11."
2671,1,"['simple linear regression', 'linear', 'case', 'regression', 'linear regression', 'error']", Properties of the Least Squares Estimators,seg_319,"the error and regression sums of squares take on the same form and play the same role as in the simple linear regression case. in fact, the sum-of-squares identity"
2672,0,[], Properties of the Least Squares Estimators,seg_319,"continues to hold, and we retain our previous notation, namely"
2673,1,"['degrees of freedom', 'condition', 'estimate', 'regression', 'sum of squares', 'multiple regression', 'error sum of squares', 'associated', 'error']", Properties of the Least Squares Estimators,seg_319,"there are k degrees of freedom associated with ssr, and, as always, sst has n− 1 degrees of freedom. therefore, after subtraction, sse has n− k− 1 degrees of freedom. thus, our estimate of σ2 is again given by the error sum of squares divided by its degrees of freedom. all three of these sums of squares will appear on the printouts of most multiple regression computer packages. note that the condition n > k in section 12.2 guarantees that the degrees of freedom of sse cannot be negative."
2674,1,"['model', 'regression', 'sum of squares', 'analysis of variance', 'variation', 'variance', 'error', 'hypothesis']", Properties of the Least Squares Estimators,seg_319,"the partition of the total sum of squares into its components, the regression and error sums of squares, plays an important role. an analysis of variance can be conducted to shed light on the quality of the regression equation. a useful hypothesis that determines if a significant amount of variation is explained by the model is"
2675,1,"['variance', 'analysis of variance', 'table']", Properties of the Least Squares Estimators,seg_319,the analysis of variance involves an f -test via a table given as follows:
2676,1,"['mse', 'degrees of freedom', 'regression', 'sum of squares', 'mean squares', 'mean', 'error']", Properties of the Least Squares Estimators,seg_319,source sum of squares degrees of freedom mean squares f ssr msr regression ssr k msr = f = k mse sse error sse n− (k + 1) mse = n−(k+1)
2677,1,"['regression', 'variable', 'analysis of variance', 'variance', 'test']", Properties of the Least Squares Estimators,seg_319,"this test is an upper-tailed test. rejection of h0 implies that the regression equation differs from a constant. that is, at least one regressor variable is important. more discussion of the use of analysis of variance appears in subsequent sections."
2678,1,"['interval', 'statistic', 'set', 'hypothesis testing', 'mean', 'confidence', 'error', 'confidence interval', 'estimation', 'hypothesis', 'residual', 'utility', 'mean square', 'interval estimation', 'mean square error']", Properties of the Least Squares Estimators,seg_319,"further utility of the mean square error (or residual mean square) lies in its use in hypothesis testing and confidence interval estimation, which is discussed in section 12.5. in addition, the mean square error plays an important role in situations where the scientist is searching for the best from a set of competing models. many model-building criteria involve the statistic s2. criteria for comparing competing models are discussed in section 12.11."
2679,1,"['confidence intervals', 'estimators', 'normally distributed', 'statistic', 'coefficients', 'intervals', 'mean', 'coefficient', 'confidence', 'distributions', 'test', 'variance', 'hypotheses']", Inferences in Multiple Linear Regression,seg_321,"a knowledge of the distributions of the individual coefficient estimators enables the experimenter to construct confidence intervals for the coefficients and to test hypotheses about them. recall from section 12.4 that the bj (j = 0, 1, 2, . . . , k) are normally distributed with mean βj and variance cjjσ2. thus, we can use the statistic"
2680,1,"['degrees of freedom', 'confidence intervals', 'intervals', 'hypotheses', 'confidence', 'test']", Inferences in Multiple Linear Regression,seg_321,"with n − k − 1 degrees of freedom to test hypotheses and construct confidence intervals on βj . for example, if we wish to test"
2681,1,['degrees of freedom'], Inferences in Multiple Linear Regression,seg_321,"we compute the above t-statistic and do not reject h0 if −tα/2 < t < tα/2, where tα/2 has n− k − 1 degrees of freedom."
2682,1,"['model', 'level', 'significance', 'test', 'level of significance', 'hypothesis']", Inferences in Multiple Linear Regression,seg_321,"example 12.5: for the model of example 12.4, test the hypothesis that β2 = −2.5 at the 0.05 level of significance against the alternative that β2 > −2.5."
2683,0,[], Inferences in Multiple Linear Regression,seg_321,decision: reject h0 and conclude that β2 > −2.5.
2684,1,"['tests', 'model', 'coefficients', 'regression', 'multiple regression', 'variable', 'variation', 'coefficient', 'hypothesis']", Inferences in Multiple Linear Regression,seg_321,"the t-test most often used in multiple regression is the one that tests the importance of individual coefficients (i.e., h0: βj = 0 against the alternative h1: βj = 0). these tests often contribute to what is termed variable screening, where the analyst attempts to arrive at the most useful model (i.e., the choice of which regressors to use). it should be emphasized here that if a coefficient is found insignificant (i.e., the hypothesis h0: βj = 0 is not rejected), the conclusion drawn is that the variable is insignificant (i.e., explains an insignificant amount of variation in y), in the presence of the other regressors in the model. this point will be reaffirmed in a future discussion."
2685,1,"['interval', 'predicted', 'set', 'mean', 'confidence', 'response', 'confidence interval']", Inferences in Multiple Linear Regression,seg_321,"one of the most useful inferences that can be made regarding the quality of the predicted response y0 corresponding to the values x10, x20, . . . , xk0 is the confidence interval on the mean response μy |x10,x20,...,xk0 . we are interested in constructing a confidence interval on the mean response for the set of conditions given by"
2686,1,"['normality', 'covariance', 'mean', 'variance']", Inferences in Multiple Linear Regression,seg_321,"we augment the conditions on the x’s by the number 1 in order to facilitate the matrix notation. normality in the i produces normality in the bj and the mean and variance are still the same as indicated in section 12.4. so is the covariance between bi and bj , for i = j. hence,"
2687,1,"['confidence interval', 'unbiased estimator', 'condition', 'interval', 'normally distributed', 'response', 'mean', 'function', 'confidence', 'variance', 'estimator', 'unbiased']", Inferences in Multiple Linear Regression,seg_321,"is likewise normally distributed and is, in fact, an unbiased estimator for the mean response on which we are attempting to attach a confidence interval. the variance of ŷ0, written in matrix notation simply as a function of σ2, (x′x)−1, and the condition vector x′0, is"
2688,1,"['interval', 'case', 'covariance', 'statistic', 'confidence', 'variance', 'confidence interval']", Inferences in Multiple Linear Regression,seg_321,"if this expression is expanded for a given case, say k = 2, it is readily seen that it appropriately accounts for the variance of the bj and the covariance of bi and bj , for i = j. after σ2 is replaced by s2 as given by theorem 12.1, the 100(1 − α)% confidence interval on μy |x10,x20,...,xk0 can be constructed from the statistic"
2689,1,['degrees of freedom'], Inferences in Multiple Linear Regression,seg_321,which has a t-distribution with n− k − 1 degrees of freedom.
2690,1,"['interval', 'mean', 'confidence', 'response', 'confidence interval']", Inferences in Multiple Linear Regression,seg_321,"confidence interval a 100(1− α)% confidence interval for the mean response μy |x10,x20,...,xk0 is for μy |x10,x20,...,xk0"
2691,1,['degrees of freedom'], Inferences in Multiple Linear Regression,seg_321,where tα/2 is a value of the t-distribution with n− k − 1 degrees of freedom.
2692,1,"['standard error', 'regression', 'error', 'standard']", Inferences in Multiple Linear Regression,seg_321,the quantity s√x′0(x′x)−1x0 is often called the standard error of prediction and appears on the printout of many regression computer packages.
2693,1,"['interval', 'data', 'mean', 'confidence', 'response', 'confidence interval']", Inferences in Multiple Linear Regression,seg_321,"example 12.6: using the data of example 12.4, construct a 95% confidence interval for the mean response when x1 = 3%, x2 = 8%, and x3 = 9%."
2694,1,"['percent', 'estimated', 'regression']", Inferences in Multiple Linear Regression,seg_321,"solution : from the regression equation of example 12.4, the estimated percent survival when x1 = 3%, x2 = 8%, and x3 = 9% is"
2695,0,[], Inferences in Multiple Linear Regression,seg_321,"next, we find that"
2696,1,"['degrees of freedom', 'error', 'interval', 'table', 'mean square', 'mean', 'confidence', 'mean square error', 'confidence interval', 'percent']", Inferences in Multiple Linear Regression,seg_321,"using the mean square error, s2 = 4.298 or s = 2.073, and table a.4, we see that t0.025 = 2.262 for 9 degrees of freedom. therefore, a 95% confidence interval for the mean percent survival for x1 = 3%, x2 = 8%, and x3 = 9% is given by"
2697,1,"['simple linear regression', 'linear', 'interval', 'prediction', 'case', 'regression', 'linear regression', 'mean', 'confidence', 'response', 'prediction interval', 'confidence interval']", Inferences in Multiple Linear Regression,seg_321,"as in the case of simple linear regression, we need to make a clear distinction between the confidence interval on a mean response and the prediction interval on an observed response. the latter provides a bound within which we can say with a preselected degree of certainty that a new observed response will fall."
2698,1,"['sampling distribution', 'interval', 'prediction', 'predicted', 'sampling', 'distribution', 'normal', 'mean', 'response', 'prediction interval']", Inferences in Multiple Linear Regression,seg_321,a prediction interval for a single predicted response y0 is once again established by considering the difference ŷ0 − y0. the sampling distribution can be shown to be normal with mean
2699,1,['variance'], Inferences in Multiple Linear Regression,seg_321,and variance
2700,1,"['prediction', 'statistic', 'prediction interval', 'interval']", Inferences in Multiple Linear Regression,seg_321,"thus, a 100(1 − α)% prediction interval for a single prediction value y0 can be constructed from the statistic"
2701,1,['degrees of freedom'], Inferences in Multiple Linear Regression,seg_321,which has a t-distribution with n− k − 1 degrees of freedom.
2702,1,"['prediction', 'response', 'prediction interval', 'interval']", Inferences in Multiple Linear Regression,seg_321,prediction interval a 100(1− α)% prediction interval for a single response y0 is given by for y0
2703,1,['degrees of freedom'], Inferences in Multiple Linear Regression,seg_321,where tα/2 is a value of the t-distribution with n− k − 1 degrees of freedom.
2704,1,"['interval', 'prediction', 'data', 'response', 'prediction interval', 'percent']", Inferences in Multiple Linear Regression,seg_321,"example 12.7: using the data of example 12.4, construct a 95% prediction interval for an individual percent survival response when x1 = 3%, x2 = 8%, and x3 = 9%."
2705,1,"['interval', 'prediction', 'results', 'response', 'prediction interval']", Inferences in Multiple Linear Regression,seg_321,"solution : referring to the results of example 12.6, we find that the 95% prediction interval for the response y0, when x1 = 3%, x2 = 8%, and x3 = 9%, is"
2706,1,"['interval', 'prediction', 'mean', 'confidence', 'prediction interval', 'confidence interval', 'percent']", Inferences in Multiple Linear Regression,seg_321,"which reduces to 19.2459 < y0 < 29.2005. notice, as expected, that the prediction interval is considerably wider than the confidence interval for mean percent survival found in example 12.6."
2707,1,"['linear', 'data', 'regression', 'linear regression', 'multiple linear regression']", Inferences in Multiple Linear Regression,seg_321,figure 12.1 shows an annotated computer printout for a multiple linear regression fit to the data of example 12.4. the package used is sas.
2708,1,"['confidence intervals', 'estimates', 'prediction', 'predicted', 'confident', 'observation', 'probability', 'intervals', 'mean', 'parameter', 'standard', 'standard errors', 'confidence', 'model', 'variance', 'response', 'variability', 'errors', 'variable']", Inferences in Multiple Linear Regression,seg_321,"note the model parameter estimates, the standard errors, and the t-statistics shown in the output. the standard errors are computed from square roots of diagonal elements of (x′x)−1s2. in this illustration, the variable x3 is insignificant in the presence of x1 and x2 based on the t-test and the corresponding p-value of 0.5916. the terms clm and cli are confidence intervals on mean response and prediction limits on an individual observation, respectively. the f-test in the analysis of variance indicates that a significant amount of variability is explained. as an example of the interpretation of clm and cli, consider observation 10. with an observation of 25.2000 and a predicted value of 26.0676, we are 95% confident that the mean response is between 24.5024 and 27.6329, and a new observation will fall between 21.1238 and 31.0114 with probability 0.95. the r2 value of 0.9117 implies that the model explains 91.17% of the variability in the response. more discussion about r2 appears in section 12.6."
2709,1,['data'], Inferences in Multiple Linear Regression,seg_321,figure 12.1: sas printout for data in example 12.4.
2710,1,"['model', 'regression model', 'regression', 'sum of squares', 'analysis of variance', 'test', 'variance', 'error']", Inferences in Multiple Linear Regression,seg_321,"in section 12.4, we discussed briefly the partition of the total sum of squares n ∑ (yi−ȳ)2 into its two components, the regression model and error sums of squares i=1 (illustrated in figure 12.1). the analysis of variance leads to a test of"
2711,1,"['treatment', 'anova', 'null hypothesis', 'hypothesis']", Inferences in Multiple Linear Regression,seg_321,"rejection of the null hypothesis has an important interpretation for the scientist or engineer. (for those who are interested in more extensive treatment of the subject using matrices, it is useful to discuss the development of these sums of squares used in anova.)"
2712,1,"['least squares estimators', 'least squares', 'estimators']", Inferences in Multiple Linear Regression,seg_321,"first, recall in section 12.3, b, the vector of least squares estimators, is given by"
2713,1,['sum of squares'], Inferences in Multiple Linear Regression,seg_321,a partition of the uncorrected sum of squares
2714,0,[], Inferences in Multiple Linear Regression,seg_321,into two components is given by
2715,1,['error'], Inferences in Multiple Linear Regression,seg_321,the second term (in brackets) on the right-hand side is simply the error sum of
2716,0,['n'], Inferences in Multiple Linear Regression,seg_321,n squares ∑ (yi − ŷi)2. the reader should see that an alternative expression for the
2717,1,"['sum of squares', 'error sum of squares', 'error']", Inferences in Multiple Linear Regression,seg_321,i=1 error sum of squares is
2718,1,"['sum of squares', 'regression']", Inferences in Multiple Linear Regression,seg_321,"the term y′x(x′x)−1x′y is called the regression sum of squares. however,"
2719,0,[], Inferences in Multiple Linear Regression,seg_321,n it is not the expression ∑ (ŷi − ȳ)2 used for testing the “importance” of the terms
2720,1,"['sum of squares', 'mean', 'regression']", Inferences in Multiple Linear Regression,seg_321,"which is a regression sum of squares uncorrected for the mean. as such, it would only be used in testing if the regression equation differs significantly from zero, that is,"
2721,0,[], Inferences in Multiple Linear Regression,seg_321,"in general, this is not as important as testing"
2722,1,"['states', 'response', 'mean']", Inferences in Multiple Linear Regression,seg_321,"since the latter states that the mean response is a constant, not necessarily zero."
2723,1,['degrees of freedom'], Inferences in Multiple Linear Regression,seg_321,"thus, the partition of sums of squares and degrees of freedom reduces to"
2724,1,"['estimated', 'intercept', 'regression', 'hypotheses', 'mean', 'estimated regression line', 'regression line', 'anova']", Inferences in Multiple Linear Regression,seg_321,"now, of course, the hypotheses of interest for an anova must eliminate the role of the intercept described previously. strictly speaking, if h0 : β1 = β2 = · · · = βk = 0, then the estimated regression line is merely ŷi = ȳ. as a result, we are actually seeking evidence that the regression equation “varies from a constant.” thus, the total and regression sums of squares must be corrected for the mean. as a result, we have"
2725,0,[], Inferences in Multiple Linear Regression,seg_321,in matrix notation this is simply
2726,0,['n'], Inferences in Multiple Linear Regression,seg_321,"in this expression, 1 is a vector of n ones. as a result, we are merely subtracting"
2727,1,"['mean', 'regression']", Inferences in Multiple Linear Regression,seg_321,"from y′y and from y′x(x′x)−1x′y (i.e., correcting the total and regression sums of squares for the mean)."
2728,1,['degrees of freedom'], Inferences in Multiple Linear Regression,seg_321,"finally, the appropriate partitioning of sums of squares with degrees of freedom is as follows:"
2729,1,"['table', 'anova table', 'regression', 'sum of squares', 'degree of freedom', 'mean', 'associated', 'anova']", Inferences in Multiple Linear Regression,seg_321,"this is the anova table that appears in the computer printout of figure 12.1. the expression y′[1(1′1)−11′]y is often called the regression sum of squares associated with the mean, and 1 degree of freedom is allocated to it."
2730,1,"['confidence intervals', 'range', 'dependent', 'function', 'significance', 'coefficients', 'tests of hypotheses', 'intervals', 'mean', 'tests', 'confidence', 'model', 'parameters', 'regression', 'response', 'concentration', 'variables', 'hypotheses']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"in many regression situations, individual coefficients are of importance to the experimenter. for example, in an economics application, β1, β2, . . . might have some particular significance, and thus confidence intervals and tests of hypotheses on these parameters would be of interest to the economist. however, consider an industrial chemical situation in which the postulated model assumes that reaction yield is linearly dependent on reaction temperature and concentration of a certain catalyst. it is probably known that this is not the true model but an adequate approximation, so interest is likely to be not in the individual parameters but rather in the ability of the entire function to predict the true response in the range of the variables considered. therefore, in this situation, one would put more emphasis on σŷ 2 , confidence intervals on the mean response, and so forth, and likely deemphasize inferences on individual parameters."
2731,1,"['variables', 'regression analysis', 'regression']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"the experimenter using regression analysis is also interested in deletion of variables when the situation dictates that, in addition to arriving at a workable prediction equation, he or she must find the “best regression” involving only variables that are useful predictors. there are a number of computer programs that sequentially arrive at the so-called best regression equation depending on certain criteria. we discuss this further in section 12.9."
2732,1,"['coefficient', 'model', 'coefficient of determination']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"one criterion that is commonly used to illustrate the adequacy of a fitted regression model is the coefficient of determination, or r2."
2733,1,"['model', 'independent', 'variables', 'variability', 'case', 'correlation', 'set', 'correlation coefficient', 'variation', 'coefficient', 'response', 'percent']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"note that this parallels the description of r2 in chapter 11. at this point the explanation might be clearer since we now focus on ssr as the variability explained. the quantity r2 merely indicates what proportion of the total variation in the response y is explained by the fitted model. often an experimenter will report r2 × 100% and interpret the result as percent variation explained by the postulated model. the square root of r2 is called the multiple correlation coefficient between y and the set x1, x2, . . . , xk. the value of r2 for the case in example 12.4, indicating the proportion of variation explained by the three independent variables x1, x2, and x3, is"
2734,1,"['linear', 'model', 'regression model', 'regression', 'linear regression', 'variation', 'percent', 'linear regression model']", Choice of a Fitted Model through Hypothesis Testing,seg_325,which means that 91.17% of the variation in percent survival has been explained by the linear regression model.
2735,1,"['model', 'regression', 'sum of squares', 'test', 'hypothesis']", Choice of a Fitted Model through Hypothesis Testing,seg_325,the regression sum of squares can be used to give some indication concerning whether or not the model is an adequate explanation of the true situation. we can test the hypothesis h0 that the regression is not significant by merely forming the ratio
2736,1,"['significance', 'data']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"and rejecting h0 at the α-level of significance when f > fα(k, n− k − 1). for the data of example 12.4, we obtain"
2737,1,"['model', 'regression']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"from the printout in figure 12.1, the p -value is less than 0.0001. this should not be misinterpreted. although it does indicate that the regression explained by the model is significant, this does not rule out the following possibilities:"
2738,1,"['linear', 'model', 'regression model', 'regression', 'set', 'linear regression', 'linear regression model']", Choice of a Fitted Model through Hypothesis Testing,seg_325,1. the linear regression model for this set of x’s is not the only model that
2739,1,"['data', 'transformations']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"can be used to explain the data; indeed, there may be other models with transformations on the x’s that give a larger value of the f-statistic."
2740,1,"['variables', 'model']", Choice of a Fitted Model through Hypothesis Testing,seg_325,2. the model might have been more effective with the inclusion of other variables
2741,1,"['variables', 'model']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"in addition to x1, x2, and x3 or perhaps with the deletion of one or more of the variables in the model, say x3, which has a p = 0.5916."
2742,1,"['linear', 'regression', 'multiple regression', 'linear regression', 'multiple linear regression']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"the reader should recall the discussion in section 11.5 regarding the pitfalls in the use of r2 as a criterion for comparing competing models. these pitfalls are certainly relevant in multiple linear regression. in fact, in its employment in multiple regression, the dangers are even more pronounced since the temptation"
2743,1,"['degrees of freedom', 'model', 'error']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"to overfit is so great. one should always keep in mind that r2 ≈ 1.0 can always be achieved at the expense of error degrees of freedom when an excess of model terms is employed. however, r2 = 1, describing a model with a near perfect fit, does not always result in a model that predicts well."
2744,1,"['degrees of freedom', 'model', 'adjusted', 'statistic', 'adjusted r2', 'variation', 'coefficient', 'coefficient of determination', 'error']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"in chapter 11, several figures displaying computer printout from both sas and minitab featured a statistic called adjusted r2 or adjusted coefficient of determination. adjusted r2 is a variation on r2 that provides an adjustment for degrees of freedom. the coefficient of determination as defined on page 407 cannot decrease as terms are added to the model. in other words, r2 does not decrease as the error degrees of freedom n − k − 1 are reduced, the latter result being produced by an increase in k, the number of model terms. adjusted r2 is computed by dividing sse and sst by their respective degrees of freedom as follows."
2745,0,[], Choice of a Fitted Model through Hypothesis Testing,seg_325,"to illustrate the use of ra2dj, example 12.4 will be revisited."
2746,1,['model'], Choice of a Fitted Model through Hypothesis Testing,seg_325,"the t-test (or corresponding f -test) for x3 suggests that a simpler model involving only x1 and x2 may well be an improvement. in other words, the complete model with all the regressors may be an overfitted model. it is certainly of interest to investigate r2 and ra2dj for both the full (x1, x2, x3) and the reduced (x1, x2)"
2747,1,['model'], Choice of a Fitted Model through Hypothesis Testing,seg_325,"models. we already know that rf2ull = 0.9117 from figure 12.1. the sse for the reduced model is 40.01, and thus rr2educed = 1 − 4"
2748,1,"['model', 'variability', 'statistic', 'full model']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"1 3 = 0.9087. thus, more variability is explained with x3 in the model. however, as we have indicated, this will occur even if the model is an overfitted model. now, of course, ra2dj is designed to provide a statistic that punishes an overfitted model, so we might expect it to favor the reduced model. indeed, for the full model"
2749,1,['model'], Choice of a Fitted Model through Hypothesis Testing,seg_325,whereas for the reduced model (deletion of x3)
2750,1,"['model', 'statistics']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"thus, ra2dj does indeed favor the reduced model and confirms the evidence produced by the tand f-tests, suggesting that the reduced model is preferable to the model containing all three regressors. the reader may expect that other statistics would suggest rejection of the overfitted model. see exercise 12.40 on page 471."
2751,1,"['model', 'estimated', 'variables', 'prediction', 'regression', 'sum of squares', 'variable', 'error sum of squares', 'response', 'test', 'variance', 'error']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"the addition of any single variable to a regression system will increase the regression sum of squares and thus reduce the error sum of squares. consequently, we must decide whether the increase in regression is sufficient to warrant using the variable in the model. as we might expect, the use of unimportant variables can reduce the effectiveness of the prediction equation by increasing the variance of the estimated response. we shall pursue this point further by considering the importance of x3 in example 12.4. initially, we can test"
2752,1,['degrees of freedom'], Choice of a Fitted Model through Hypothesis Testing,seg_325,by using the t-distribution with 9 degrees of freedom. we have
2753,1,"['normal equations', 'model', 'least squares', 'regression', 'set', 'normal']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"which indicates that β3 does not differ significantly from zero, and hence we may very well feel justified in removing x3 from the model. suppose that we consider the regression of y on the set (x1, x2), the least squares normal equations now reducing to"
2754,1,"['model', 'estimated', 'regression coefficients', 'regression', 'coefficients']", Choice of a Fitted Model through Hypothesis Testing,seg_325,the estimated regression coefficients for this reduced model are
2755,1,"['sum of squares', 'degrees of freedom', 'regression']", Choice of a Fitted Model through Hypothesis Testing,seg_325,and the resulting regression sum of squares with 2 degrees of freedom is
2756,1,"['degrees of freedom', 'model', 'regression', 'sum of squares', 'error sum of squares', 'error']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"here we use the notation r(β1, β2) to indicate the regression sum of squares of the restricted model; it should not be confused with ssr, the regression sum of squares of the original model with 3 degrees of freedom. the new error sum of squares is then"
2757,1,"['degrees of freedom', 'mean square error', 'mean square', 'mean', 'error']", Choice of a Fitted Model through Hypothesis Testing,seg_325,and the resulting mean square error with 10 degrees of freedom becomes
2758,1,"['variables', 'variation', 'percent']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"from example 12.4, the amount of variation in the percent survival that is attributed to x3, in the presence of the variables x1 and x2, is"
2759,1,"['regression', 'variation', 'test']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"which represents a small proportion of the entire regression variation. this amount of added regression is statistically insignificant, as indicated by our previous test on β3. an equivalent test involves the formation of the ratio"
2760,1,['degrees of freedom'], Choice of a Fitted Model through Hypothesis Testing,seg_325,which is a value of the f-distribution with 1 and 9 degrees of freedom. recall that the basic relationship between the t-distribution with v degrees of freedom and the f-distribution with 1 and v degrees of freedom is
2761,1,"['linear', 'model', 'independent', 'regression model', 'independent variable', 'regression', 'variable', 'linear regression', 'multiple linear regression', 'linear regression model']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"to generalize the concepts above, we can assess the work of an independent variable xi in the general multiple linear regression model"
2762,1,"['variables', 'adjusted', 'regression']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"by observing the amount of regression attributed to xi over and above that attributed to the other variables, that is, the regression on xi adjusted for the other variables. for example, we say that x1 is assessed by calculating"
2763,1,"['model', 'regression', 'sum of squares', 'test', 'hypothesis']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"where r(β2, β3, . . . , βk) is the regression sum of squares with β1x1 removed from the model. to test the hypothesis"
2764,1,"['model', 'variables', 'set', 'significance', 'test', 'hypothesis']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"in a similar manner, we can test for the significance of a set of the variables. for example, to investigate simultaneously the importance of including x1 and x2 in the model, we test the hypothesis"
2765,1,"['degrees of freedom', 'variables', 'case', 'set', 'associated']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"and comparing it with fα(2, n−k−1). the number of degrees of freedom associated with the numerator, in this case 2, equals the number of variables in the set being investigated."
2766,1,"['test', 'hypothesis']", Choice of a Fitted Model through Hypothesis Testing,seg_325,suppose we wish to test the hypothesis
2767,1,"['model', 'regression model', 'regression']", Choice of a Fitted Model through Hypothesis Testing,seg_325,for example 12.4. if we develop the regression model
2768,1,"['model', 'full model', 'hypothesis']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"we can obtain r(β1) = ssrreduced = 187.31179. from figure 12.1 on page 459, we have s2 = 4.29738 for the full model. hence, the f -value for testing the hypothesis is"
2769,1,"['model', 'variables', 'case', 'associated', 'coefficient', 'statistical', 'null hypothesis', 'hypothesis']", Choice of a Fitted Model through Hypothesis Testing,seg_325,"this implies that β2 and β3 are not simultaneously zero. using statistical software such as sas one can directly obtain the above result with a p -value of 0.0002. readers should note that in statistical software package output there are p -values associated with each individual model coefficient. the null hypothesis for each is that the coefficient is zero. however, it should be noted that the insignificance of any coefficient does not necessarily imply that it does not belong in the final model. it merely suggests that it is insignificant in the presence of all other variables in the problem. the case study at the end of this chapter illustrates this further."
2770,1,"['linear', 'experimental', 'independent', 'experiment', 'variables', 'prediction', 'regression', 'linear regression', 'error']", Special Case of Orthogonality Optional,seg_327,"prior to our original development of the general linear regression problem, the assumption was made that the independent variables are measured without error and are often controlled by the experimenter. quite often they occur as a result of an elaborately designed experiment. in fact, we can increase the effectiveness of the resulting prediction equation with the use of a suitable experimental plan."
2771,0,[], Special Case of Orthogonality Optional,seg_327,suppose that we once again consider the x matrix as defined in section 12.3. we can rewrite it as
2772,1,['levels'], Special Case of Orthogonality Optional,seg_327,where 1 represents a column of ones and xj is a column vector representing the levels of xj . if
2773,1,['variables'], Special Case of Orthogonality Optional,seg_327,the variables xp and xq are said to be orthogonal to each other. there are certain obvious advantages to having a completely orthogonal situation where x′pxq = 0
2774,1,"['normal equations', 'normal']", Special Case of Orthogonality Optional,seg_327,"the resulting x′x is a diagonal matrix, and the normal equations in section 12.3 reduce to"
2775,1,"['variable', 'variation']", Special Case of Orthogonality Optional,seg_327,"an important advantage is that one is easily able to partition ssr into singledegree-of-freedom components, each of which corresponds to the amount of variation in y accounted for by a given controlled variable. in the orthogonal situation, we can write"
2776,1,"['model', 'independent', 'independent variable', 'regression', 'sum of squares', 'variable', 'associated']", Special Case of Orthogonality Optional,seg_327,the quantity r(βi) is the amount of the regression sum of squares associated with a model involving a single independent variable xi.
2777,1,"['variables', 'regression', 'sum of squares', 'set', 'significance', 'test']", Special Case of Orthogonality Optional,seg_327,"to test simultaneously for the significance of a set of m variables in an orthogonal situation, the regression sum of squares becomes"
2778,0,[], Special Case of Orthogonality Optional,seg_327,and thus we have the further simplification
2779,1,"['degrees of freedom', 'model', 'independent', 'table', 'variables', 'evaluating', 'independent variable', 'variable', 'set', 'hypotheses', 'response', 'variation', 'test', 'error']", Special Case of Orthogonality Optional,seg_327,"when evaluating a single independent variable. therefore, the contribution of a given variable or set of variables is essentially found by ignoring the other variables in the model. independent evaluations of the worth of the individual variables are accomplished using analysis-of-variance techniques, as given in table 12.4. the total variation in the response is partitioned into single-degree-of-freedom components plus the error term with n−k−1 degrees of freedom. each computed f-value is used to test one of the hypotheses"
2780,0,[], Special Case of Orthogonality Optional,seg_327,"by comparing with the critical point fα(1, n − k − 1) or merely interpreting the p-value computed from the f-distribution."
2781,1,"['variables', 'orthogonal variables', 'analysis of variance', 'variance']", Special Case of Orthogonality Optional,seg_327,table 12.4: analysis of variance for orthogonal variables
2782,1,"['mean', 'variation']", Special Case of Orthogonality Optional,seg_327,source of sum of degrees of mean computed variation squares freedom square f n β1 r(β1) = b21 ∑ x21i 1 r(β1) r( s
2783,1,"['model', 'linear', 'rate', 'regression model', 'table', 'experimental', 'data', 'regression', 'variable', 'linear regression', 'function', 'linear regression model']", Special Case of Orthogonality Optional,seg_327,"example 12.8: suppose that a scientist takes experimental data on the radius of a propellant grain y as a function of powder temperature x1, extrusion rate x2, and die temperature x3. fit a linear regression model for predicting grain radius, and determine the effectiveness of each variable in the model. the data are given in table 12.5."
2784,1,['data'], Special Case of Orthogonality Optional,seg_327,table 12.5: data for example 12.8
2785,1,['rate'], Special Case of Orthogonality Optional,seg_327,powder extrusion die grain radius temperature rate temperature
2786,1,"['levels', 'independent', 'experiment', 'variables', 'data', 'variable', 'combinations']", Special Case of Orthogonality Optional,seg_327,"solution : note that each variable is controlled at two levels, and the experiment is composed of the eight possible combinations. the data on the independent variables are coded for convenience by means of the following formulas:"
2787,0,[], Special Case of Orthogonality Optional,seg_327,"powder temperature− 170 x1 = , 20 extrusion rate− 18 x2 = , 6 die temperature− 235 x3 = . 15"
2788,1,"['experimental', 'levels', 'table', 'design', 'data']", Special Case of Orthogonality Optional,seg_327,"the resulting levels of x1, x2, and x3 take on the values −1 and +1 as indicated in the table of data. this particular experimental design affords the orthogonal-"
2789,1,"['linear', 'nonlinear regression', 'nonlinear', 'regression', 'linear regression', 'multiple linear regression']", Special Case of Orthogonality Optional,seg_327,470 chapter 12 multiple linear regression and certain nonlinear regression models
2790,1,"['treatment', 'experimental']", Special Case of Orthogonality Optional,seg_327,ity that we want to illustrate here. (a more thorough treatment of this type of experimental layout appears in chapter 15.) the x matrix is
2791,0,[], Special Case of Orthogonality Optional,seg_327,and the orthogonality conditions are readily verified.
2792,1,['coefficients'], Special Case of Orthogonality Optional,seg_327,we can now compute coefficients
2793,1,"['variables', 'prediction']", Special Case of Orthogonality Optional,seg_327,"so in terms of the coded variables, the prediction equation is"
2794,1,"['observations', 'case', 'combinations', 'estimate', 'results', 'error', 'model', 'experimental', 'analysis of variance', 'level', 'variance', 'experimental error', 'independent', 'table', 'variables', 'independent variable', 'variable', 'variation', 'lack of fit']", Special Case of Orthogonality Optional,seg_327,"the analysis of variance in table 12.6 shows independent contributions to ssr for each variable. the results, when compared to the f0.05(1, 4) critical point of 7.71, indicate that x1 does not contribute significantly at the 0.05 level, whereas variables x2 and x3 are significant. in this example, the estimate for σ2 is 23.1250. as for the single independent variable case, it should be pointed out that this estimate does not solely contain experimental error variation unless the postulated model is correct. otherwise, the estimate is “contaminated” by lack of fit in addition to pure error, and the lack of fit can be separated out only if we obtain multiple experimental observations for the various (x1, x2, x3) combinations."
2795,1,"['variance', 'analysis of variance', 'data']", Special Case of Orthogonality Optional,seg_327,table 12.6: analysis of variance for grain radius data
2796,1,"['variation', 'mean']", Special Case of Orthogonality Optional,seg_327,source of sum of degrees of mean computed variation squares freedom squares f p-value
2797,1,['error'], Special Case of Orthogonality Optional,seg_327,β1 (2.5)2(8) = 50.00 1 50.00 2.16 0.2156 β2 (14.75)2(8) = 1740.50 1 1740.50 75.26 0.0010 β3 (21.75)2(8) = 3784.50 1 3784.50 163.65 0.0002 error 92.50 4 23.13 total 5667.50 7
2798,1,"['model', 'variables', 'factor']", Special Case of Orthogonality Optional,seg_327,"since x1 is not significant, it can simply be eliminated from the model without altering the effects of the other variables. note that x2 and x3 both impact the grain radius in a positive fashion, with x3 being the more important factor based on the smallness of its p-value."
2799,1,"['measurements', 'multiple linear regression', 'vary', 'process', 'dummy variables', 'linear', 'data', 'categorical variable', 'indicator', 'model', 'categorical', 'regression', 'linear regression', 'continuous', 'categories', 'indicator variables', 'variables', 'treatment', 'variable', 'indicator variable']", Categorical or Indicator Variables,seg_331,"an extremely important special-case application of multiple linear regression occurs when one or more of the regressor variables are categorical, indicator, or dummy variables. in a chemical process, the engineer may wish to model the process yield against regressors such as process temperature and reaction time. however, there is interest in using two different catalysts and somehow including “the catalyst” in the model. the catalyst effect cannot be measured on a continuum and is hence a categorical variable. an analyst may wish to model the price of homes against regressors that include square feet of living space x1, the land acreage x2, and age of the house x3. these regressors are clearly continuous in nature. however, it is clear that cost of homes may vary substantially from one area of the country to another. if data are collected on homes in the east, midwest, south, and west, we have an indicator variable with four categories. in the chemical process example, if two catalysts are used, we have an indicator variable with two categories. in a biomedical example in which a drug is to be compared to a placebo, all subjects are evaluated on several continuous measurements such as age, blood pressure, and so on, as well as gender, which of course is categorical with two categories. so, included along with the continuous variables are two indicator variables: treatment with two categories (active drug and placebo) and gender with two categories (male and female)."
2800,1,"['model', 'variables', 'variable', 'indicator variables', 'indicator', 'indicator variable']", Categorical or Indicator Variables,seg_331,"let us use the chemical processing example to illustrate how indicator variables are involved in the model. suppose y = yield and x1 = temperature and x2 = reaction time. now let us denote the indicator variable by z. let z = 0 for catalyst 1 and z = 1 for catalyst 2. the assignment of the (0, 1) indicator to the catalyst is arbitrary. as a result, the model becomes"
2801,1,"['model', 'levels', 'method', 'method of least squares', 'estimation', 'case', 'least squares', 'variable', 'categories', 'indicator', 'indicator variable', 'coefficients']", Categorical or Indicator Variables,seg_331,"the estimation of coefficients by the method of least squares continues to apply. in the case of three levels or categories of a single indicator variable, the model will"
2802,0,[], Categorical or Indicator Variables,seg_331,"include two regressors, say z1 and z2, where the (0, 1) assignment is as follows:"
2803,1,"['categories', 'model']", Categorical or Indicator Variables,seg_331,"where 0 and 1 are vectors of 0’s and 1’s, respectively. in other words, if there are categories, the model includes − 1 actual model terms."
2804,1,"['model', 'graphical', 'variable', 'continuous', 'categories']", Categorical or Indicator Variables,seg_331,"it may be instructive to look at a graphical representation of the model with three categories. for the sake of simplicity, let us assume a single continuous variable x. as a result, the model is given by"
2805,1,"['categories', 'model']", Categorical or Indicator Variables,seg_331,"thus, figure 12.2 reflects the nature of the model. the following are model expressions for the three categories."
2806,1,"['model', 'categorical', 'variables', 'intercept', 'continuous', 'categories', 'categorical variables', 'coefficients']", Categorical or Indicator Variables,seg_331,"as a result, the model involving categorical variables essentially involves a change in the intercept as we change from one category to another. here of course we are assuming that the coefficients of continuous variables remain the same across the categories."
2807,1,"['categories', 'case']", Categorical or Indicator Variables,seg_331,figure 12.2: case of three categories.
2808,1,"['model', 'categorical', 'table', 'data', 'variable', 'response', 'categories']", Categorical or Indicator Variables,seg_331,"example 12.9: consider the data in table 12.7. the response y is the amount of suspended solids in a coal cleansing system. the variable x is the ph of the system. three different polymers are used in the system. thus, “polymer” is categorical with three categories and hence produces two model terms. the model is given by"
2809,1,"['model', 'estimate', 'slope', 'regression analysis', 'regression', 'statistically significant', 'coefficient', 'coefficients']", Categorical or Indicator Variables,seg_331,"from the analysis in figure 12.3, the following conclusions are drawn. the coefficient b1 for ph is the estimate of the common slope that is assumed in the regression analysis. all model terms are statistically significant. thus, ph and the nature of the polymer have an impact on the amount of cleansing. the signs and magnitudes of the coefficients of z1 and z2 indicate that polymer 1 is most effective (producing higher suspended solids) for cleansing, followed by polymer 2. polymer 3 is least effective."
2810,1,['data'], Categorical or Indicator Variables,seg_331,table 12.7: data for example 12.9
2811,1,"['levels', 'case', 'varying', 'indicator', 'model', 'condition', 'slopes', 'interaction terms', 'continuous', 'categories', 'test', 'variables', 'interaction', 'variable', 'indicator variable']", Categorical or Indicator Variables,seg_331,"in the discussion given here, we have assumed that the indicator variable model terms enter the model in an additive fashion. this suggests that the slopes, as in figure 12.2, are constant across categories. obviously, this is not always going to be the case. we can account for the possibility of varying slopes and indeed test for this condition of parallelism by including product or interaction terms between indicator terms and continuous variables. for example, suppose a model with one continuous regressor and an indicator variable with two levels is chosen. the model is given by"
2812,1,"['model', 'error', 'mean', 'mean square']", Categorical or Indicator Variables,seg_331,sum of source df squares mean square f value pr > f model 3 80181.73127 26727.24376 73.68 <.0001 error 14 5078.71318 362.76523
2813,1,"['mse', 'estimate', 'intercept', 'mean', 'parameter', 'standard', 'error']", Categorical or Indicator Variables,seg_331,r-square coeff var root mse y mean 0.940433 6.316049 19.04640 301.5556 standard parameter estimate error t value pr > |t| intercept -161.8973333 37.43315576 -4.32 0.0007 x 54.2940260 4.75541126 11.42 <.0001 z1 89.9980606 11.05228237 8.14 <.0001 z2 27.1656970 11.01042883 2.47 0.0271
2814,1,['model'], Categorical or Indicator Variables,seg_331,"this model suggests that for category l (z = 1),"
2815,1,"['regression', 'slopes', 'intercepts', 'categories', 'varying']", Categorical or Indicator Variables,seg_331,"thus, we allow for varying intercepts and slopes for the two categories. figure 12.4 displays the regression lines with varying slopes for the two categories."
2816,1,"['categorical', 'variables', 'categorical variables']", Categorical or Indicator Variables,seg_331,figure 12.4: nonparallelism in categorical variables.
2817,1,"['model', 'interaction', 'case', 'coefficient', 'slope']", Categorical or Indicator Variables,seg_331,"in this case, β0, β1, and β2 are positive while β3 is negative with |β3| < β1. obviously, if the interaction coefficient β3 is insignificant, we are back to the common slope model."
2818,1,"['correlation', 'multicollinearity', 'significance', 'coefficients', 'correlation coefficients', 'sample', 'linear', 'experiment', 'tests', 'model', 'regression model', 'regression', 'deviation', 'independent', 'variables']", Sequential Methods for Model Selection,seg_335,"at times, the significance tests outlined in section 12.6 are quite adequate for determining which variables should be used in the final regression model. these tests are certainly effective if the experiment can be planned and the variables are orthogonal to each other. even if the variables are not orthogonal, the individual t-tests can be of some use in many problems where the number of variables under investigation is small. however, there are many problems where it is necessary to use more elaborate techniques for screening variables, particularly when the experiment exhibits a substantial deviation from orthogonality. useful measures of multicollinearity (linear dependency) among the independent variables are provided by the sample correlation coefficients rxixj . since we are concerned only"
2819,1,"['linear', 'independent', 'variables']", Sequential Methods for Model Selection,seg_335,"with linear dependency among independent variables, no confusion will result if we drop the x’s from our notation and simply write rxixj = rij , where"
2820,1,"['random variables', 'random', 'variables', 'estimates', 'correlation', 'population', 'standard']", Sequential Methods for Model Selection,seg_335,"note that the rij do not give true estimates of population correlation coefficients in the strict sense, since the x’s are actually not random variables in the context discussed here. thus, the term correlation, although standard, is perhaps a misnomer."
2821,1,"['correlation coefficients', 'sample', 'model', 'variables', 'prediction', 'correlation', 'multicollinearity', 'regression', 'predictor', 'coefficients']", Sequential Methods for Model Selection,seg_335,"when one or more of these sample correlation coefficients deviate substantially from zero, it can be quite difficult to find the most effective subset of variables for inclusion in our prediction equation. in fact, for some problems the multicollinearity will be so extreme that a suitable predictor cannot be found unless all possible subsets of the variables are investigated. informative discussions of model selection in regression by hocking (1976) are cited in the bibliography. procedures for detection of multicollinearity are discussed in the textbook by myers (1990), also cited."
2822,1,"['linear', 'linear regression', 'regression', 'multiple linear regression']", Sequential Methods for Model Selection,seg_335,the user of multiple linear regression attempts to accomplish one of three objectives:
2823,1,"['model', 'estimates', 'coefficients']", Sequential Methods for Model Selection,seg_335,1. obtain estimates of individual coefficients in a complete model.
2824,1,"['variables', 'response']", Sequential Methods for Model Selection,seg_335,2. screen variables to determine which have a significant effect on the response.
2825,1,['prediction'], Sequential Methods for Model Selection,seg_335,3. arrive at the most effective prediction equation.
2826,1,"['model', 'estimated', 'experiment', 'variables', 'prediction', 'regression coefficients', 'regression', 'multicollinearity', 'success', 'response', 'coefficients']", Sequential Methods for Model Selection,seg_335,"in (1) it is known a priori that all variables are to be included in the model. in (2) prediction is secondary, while in (3) individual regression coefficients are not as important as the quality of the estimated response ŷ. for each of the situations above, multicollinearity in the experiment can have a profound effect on the success of the regression."
2827,1,"['variables', 'regression', 'sum of squares', 'variable', 'standard', 'coefficient']", Sequential Methods for Model Selection,seg_335,"in this section, some standard sequential procedures for selecting variables are discussed. they are based on the notion that a single variable or a collection of variables should not appear in the estimating equation unless the variables result in a significant increase in the regression sum of squares or, equivalently, a significant increase in r2, the coefficient of multiple determination."
2828,1,"['correlation', 'measurements', 'coefficients', 'correlation coefficients', 'sample', 'linear', 'experiment', 'symmetric', 'data', 'independent', 'table', 'variables']", Sequential Methods for Model Selection,seg_335,"example 12.10: consider the data of table 12.8, where measurements were taken for nine infants. the purpose of the experiment was to arrive at a suitable estimating equation relating the length of an infant to all or a subset of the independent variables. the sample correlation coefficients, indicating the linear dependency among the independent variables, are displayed in the symmetric matrix"
2829,1,['data'], Sequential Methods for Model Selection,seg_335,table 12.8: data relating to infant length∗
2830,1,"['model', 'estimated', 'least squares', 'regression', 'multicollinearity']", Sequential Methods for Model Selection,seg_335,"note that there appears to be an appreciable amount of multicollinearity. using the least squares technique outlined in section 12.2, the estimated regression equation was fitted using the complete model and is"
2831,1,"['degrees of freedom', 'model', 'table', 'coefficient of determination', 'regression', 'variable', 'measuring', 'variation', 'coefficient']", Sequential Methods for Model Selection,seg_335,"the value of s2 with 4 degrees of freedom is 0.7414, and the value for the coefficient of determination for this model is found to be 0.9908. regression sums of squares, measuring the variation attributed to each individual variable in the presence of the others, and the corresponding t-values are given in table 12.9."
2832,1,"['table', 'data', 'regression']", Sequential Methods for Model Selection,seg_335,table 12.9: t-values for the regression data of table 12.8
2833,1,"['degrees of freedom', 'model', 'level', 'regression function', 'combination', 'variables', 'data', 'regression', 'variable', 'adjusted', 'function', 'critical region']", Sequential Methods for Model Selection,seg_335,"a two-tailed critical region with 4 degrees of freedom at the 0.05 level of significance is given by |t| > 2.776. of the four computed t-values, only variable x3 appears to be significant. however, recall that although the t-statistic described in section 12.6 measures the worth of a variable adjusted for all other variables, it does not detect the potential importance of a variable in combination with a subset of the variables. for example, consider the model with only the variables x2 and x3 in the equation. the data analysis gives the regression function"
2834,1,"['model', 'combination', 'test']", Sequential Methods for Model Selection,seg_335,"with r2 = 0.9905, certainly not a substantial reduction from r2 = 0.9907 for the complete model. however, unless the performance characteristics of this particular combination had been observed, one would not be aware of its predictive potential. this, of course, lends support for a methodology that observes all possible regressions or a systematic sequential procedure designed to test subsets."
2835,1,"['model', 'variables', 'stepwise regression', 'regression', 'stepwise', 'standard', 'forward selection', 'backward elimination']", Sequential Methods for Model Selection,seg_335,"one standard procedure for searching for the “optimum subset” of variables in the absence of orthogonality is a technique called stepwise regression. it is based on the procedure of sequentially introducing the variables into the model one at a time. given a predetermined size α, the description of the stepwise routine will be better understood if the methods of forward selection and backward elimination are described first."
2836,1,"['variables', 'regression']", Sequential Methods for Model Selection,seg_335,forward selection is based on the notion that variables should be inserted one at a time until a satisfactory regression equation is found. the procedure is as follows:
2837,1,"['simple linear regression', 'linear', 'regression', 'sum of squares', 'variable', 'linear regression']", Sequential Methods for Model Selection,seg_335,"step 1. choose the variable that gives the largest regression sum of squares when performing a simple linear regression with y or, equivalently, that which gives the largest value of r2. we shall call this initial variable x1. if x1 is insignificant, the procedure is terminated."
2838,1,"['variable', 'model']", Sequential Methods for Model Selection,seg_335,"step 2. choose the variable that, when inserted in the model, gives the largest increase in r2, in the presence of x1, over the r2 found in step 1. this, of course, is the variable xj for which"
2839,1,"['model', 'regression model', 'regression', 'variable']", Sequential Methods for Model Selection,seg_335,"is largest. let us call this variable x2. the regression model with x1 and x2 is then fitted and r2 observed. if x2 is insignificant, the procedure is terminated."
2840,1,['variable'], Sequential Methods for Model Selection,seg_335,step 3. choose the variable xj that gives the largest value of
2841,1,"['model', 'regression model', 'regression', 'variable']", Sequential Methods for Model Selection,seg_335,"again resulting in the largest increase of r2 over that given in step 2. calling this variable x3, we now have a regression model involving x1, x2, and x3. if x3 is insignificant, the procedure is terminated."
2842,1,"['partial f', 'regression', 'variable', 'process']", Sequential Methods for Model Selection,seg_335,"this process is continued until the most recent variable inserted fails to induce a significant increase in the explained regression. such an increase can be determined at each step by using the appropriate partial f -test or t-test. for example, in step 2 the value"
2843,1,"['model', 'error', 'variables', 'mean square', 'mean', 'test', 'mean square error']", Sequential Methods for Model Selection,seg_335,"can be determined to test the appropriateness of x2 in the model. here the value of s2 is the mean square error for the model containing the variables x1 and x2. similarly, in step 3 the ratio"
2844,1,"['model', 'error', 'variables', 'mean square', 'mean', 'level', 'significance', 'significance level', 'mean square error']", Sequential Methods for Model Selection,seg_335,"tests the appropriateness of x3 in the model. now, however, the value for s2 is the mean square error for the model that contains the three variables x1, x2, and x3. if f < fα(1, n − 3) at step 2, for a prechosen significance level, x2 is not included"
2845,1,"['linear', 'variables', 'regression', 'linear equation', 'process']", Sequential Methods for Model Selection,seg_335,"and the process is terminated, resulting in a simple linear equation relating y and x1. however, if f > fα(1, n− 3), we proceed to step 3. again, if f < fα(1, n− 4) at step 3, x3 is not included and the process is terminated with the appropriate regression equation containing the variables x1 and x2."
2846,1,"['variables', 'model', 'forward selection']", Sequential Methods for Model Selection,seg_335,"backward elimination involves the same concepts as forward selection except that one begins with all the variables in the model. suppose, for example, that there are five variables under consideration. the steps are as follows:"
2847,1,"['model', 'variables', 'regression', 'sum of squares', 'variable', 'adjusted']", Sequential Methods for Model Selection,seg_335,step 1. fit a regression equation with all five variables included in the model. choose the variable that gives the smallest value of the regression sum of squares adjusted for the others. suppose that this variable is x2. remove x2 from the model if
2848,1,"['variables', 'variable', 'regression']", Sequential Methods for Model Selection,seg_335,"step 2. fit a regression equation using the remaining variables x1, x3, x4, and x5, and repeat step 1. suppose that variable x5 is chosen this time. once again, if"
2849,1,"['model', 'error', 'regression model', 'regression', 'variable', 'mean square', 'mean', 'mean square error']", Sequential Methods for Model Selection,seg_335,"is insignificant, the variable x5 is removed from the model. at each step, the s2 used in the f-test is the mean square error for the regression model at that stage."
2850,1,"['results', 'regression', 'sum of squares', 'variable', 'significance level', 'level', 'significance', 'process']", Sequential Methods for Model Selection,seg_335,this process is repeated until at some step the variable with the smallest adjusted regression sum of squares results in a significant f-value for some predetermined significance level.
2851,1,"['model', 'variables', 'regression', 'variable', 'stepwise', 'forward selection']", Sequential Methods for Model Selection,seg_335,"stepwise regression is accomplished with a slight but important modification of the forward selection procedure. the modification involves further testing at each stage to ensure the continued effectiveness of variables that had been inserted into the model at an earlier stage. this represents an improvement over forward selection, since it is quite possible that a variable entering the regression equation at an early stage might have been rendered unimportant or redundant because of relationships that exist between it and other variables entering at later stages. therefore, at a stage in which a new variable has been entered into the regression equation through a significant increase in r2 as determined by the f-test, all the variables already in the model are subjected to f-tests (or, equivalently, to t-tests) in light of this new variable and are deleted if they do not display a significant f-value. the procedure is continued until a stage is reached where no additional variables can be inserted or deleted. we illustrate the stepwise procedure in the following example."
2852,1,"['linear', 'model', 'regression model', 'table', 'stepwise regression', 'data', 'regression', 'stepwise', 'linear regression', 'linear regression model']", Sequential Methods for Model Selection,seg_335,"example 12.11: using the techniques of stepwise regression, find an appropriate linear regression model for predicting the length of infants for the data of table 12.8."
2853,1,"['simple linear regression', 'linear', 'regression', 'variable', 'linear regression']", Sequential Methods for Model Selection,seg_335,"solution : step 1. considering each variable separately, four individual simple linear regression equations are fitted. the following pertinent regression sums of"
2854,0,[], Sequential Methods for Model Selection,seg_335,squares are computed:
2855,1,"['error', 'regression', 'sum of squares', 'mean square', 'mean', 'mean square error']", Sequential Methods for Model Selection,seg_335,"variable x1 clearly gives the largest regression sum of squares. the mean square error for the equation involving only x1 is s2 = 4.7276, and since"
2856,1,"['variable', 'model']", Sequential Methods for Model Selection,seg_335,"which exceeds f0.05(1, 7) = 5.59, the variable x1 is significant and is entered into the model."
2857,1,"['results', 'combinations', 'regression']", Sequential Methods for Model Selection,seg_335,"step 2. three regression equations are fitted at this stage, all containing x1. the important results for the combinations (x1, x2), (x1, x3), and (x1, x4) are"
2858,1,"['sum of squares', 'regression']", Sequential Methods for Model Selection,seg_335,"variable x3 displays the largest regression sum of squares in the presence of x1. the regression involving x1 and x3 gives a new value of s2 = 0.6307, and since"
2859,1,"['model', 'variable', 'significance', 'test']", Sequential Methods for Model Selection,seg_335,"which exceeds f0.05(1, 6) = 5.99, the variable x3 is significant and is included along with x1 in the model. now we must subject x1 in the presence of x3 to a significance test. we find that r(β1 | β3) = 131.349, and hence"
2860,0,[], Sequential Methods for Model Selection,seg_335,"which is highly significant. therefore, x1 is retained along with x3."
2861,1,"['model', 'regression model', 'combination', 'variables', 'regression analysis', 'regression', 'level']", Sequential Methods for Model Selection,seg_335,"step 3. with x1 and x3 already in the model, we now require r(β2 | β1, β3) and r(β4 | β1, β3) in order to determine which, if any, of the remaining two variables is entered at this stage. from the regression analysis using x2 along with x1 and x3, we find r(β2 | β1, β3) = 0.7948, and when x4 is used along with x1 and x3, we obtain r(β4 | β1, β3) = 0.1855. the value of s2 is 0.5979 for the (x1, x2, x3) combination and 0.7198 for the (x1, x2, x4) combination. since neither f-value is significant at the α = 0.05 level, the final regression model includes only the variables x1 and x3. the estimating equation is found to be"
2862,1,"['coefficient', 'model', 'coefficient of determination']", Sequential Methods for Model Selection,seg_335,and the coefficient of determination for this model is r2 = 0.9882.
2863,1,"['combination', 'variables', 'stepwise regression', 'regression', 'stepwise']", Sequential Methods for Model Selection,seg_335,"although (x1, x3) is the combination chosen by stepwise regression, it is not necessarily the combination of two variables that gives the largest value of r2. in fact, we have already observed that the combination (x2, x3) gives r2 = 0.9905. of course, the stepwise procedure never observed this combination. a rational argument could be made that there is actually a negligible difference in performance"
2864,1,"['combination', 'variation', 'backward elimination', 'percent']", Sequential Methods for Model Selection,seg_335,"between these two estimating equations, at least in terms of percent variation explained. it is interesting to observe, however, that the backward elimination procedure gives the combination (x2, x3) in the final equation (see exercise 12.49 on page 494)."
2865,1,"['variables', 'estimation', 'combinations', 'least squares', 'multicollinearity', 'function']", Sequential Methods for Model Selection,seg_335,"the main function of each of the procedures explained in this section is to expose the variables to a systematic methodology designed to ensure the eventual inclusion of the best combinations of the variables. obviously, there is no assurance that this will happen in all problems, and, of course, it is possible that the multicollinearity is so extensive that one has no alternative but to resort to estimation procedures other than least squares. these estimation procedures are discussed in myers (1990), listed in the bibliography."
2866,1,"['efficient', 'quantitative', 'variables', 'results', 'data', 'regression', 'information', 'sets', 'data sets']", Sequential Methods for Model Selection,seg_335,"the sequential procedures discussed here represent three of many such methods that have been put forth in the literature and appear in various regression computer packages that are available. these methods are designed to be computationally efficient but, of course, do not give results for all possible subsets of the variables. as a result, the procedures are most effective for data sets that involve a large number of variables. for regression problems involving a relatively small number of variables, modern regression computer packages allow for the computation and summarization of quantitative information on all models for every possible subset of the variables. illustrations are provided in section 12.11."
2867,1,"['model', 'variables', 'successful', 'test']", Sequential Methods for Model Selection,seg_335,"as one might expect, the choice of the final model with these procedures may depend dramatically on what p -value is chosen. in addition, a procedure is most successful when it is forced to test a large number of candidate variables. for this reason, any forward procedure will be most useful when a relatively large p -value is used. thus, some software packages use a default p -value of 0.50."
2868,1,"['model', 'variables', 'residuals', 'data', 'regression', 'information', 'errors', 'numerical']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"it was suggested earlier in this chapter that the residuals, or errors in the regression fit, often carry information that can be very informative to the data analyst. the ei = yi−ŷi, i = 1, 2, . . . , n, which are the numerical counterpart to the i, the model errors, often shed light on the possible violation of assumptions or the presence of “suspect” data points. suppose that we let the vector xi denote the values of the regressor variables corresponding to the ith data point, supplemented by a 1 in the initial position. that is,"
2869,1,"['confidence intervals', 'intervals', 'mean', 'confidence', 'variance', 'response']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"the reader should recognize that hii was used in the computation of the confidence intervals on the mean response in section 12.5. apart from σ2, hii represents the variance of the fitted value ŷi. the hii values are the diagonal elements of the hat matrix"
2870,1,"['residuals', 'regression analysis', 'regression', 'fitted values']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"which plays an important role in any study of residuals and in other modern aspects of regression analysis (see myers, 1990, listed in the bibliography). the term hat matrix is derived from the fact that h generates the “y-hats,” or the fitted values when multiplied by the vector y of observed responses. that is, ŷ = xb, and thus"
2871,0,[], Study of Residuals and Violation of Assumptions Model Checking,seg_337,where ŷ is the vector whose ith element is ŷi.
2872,1,"['independent', 'residuals', 'normally distributed', 'mean', 'variance', 'statistical']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"if we make the usual assumptions that the i are independent and normally distributed with mean 0 and variance σ2, the statistical properties of the residuals are readily characterized. then"
2873,1,['inequality'], Study of Residuals and Violation of Assumptions Model Checking,seg_337,"for i = 1, 2, . . . , n. (see myers, 1990, for details.) it can be shown that the hat diagonal values are bounded according to the inequality"
2874,1,"['parameters', 'regression']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"n in addition, ∑ hii = k+ 1, the number of regression parameters. as a result, any"
2875,1,"['deviation', 'residual', 'residuals', 'data', 'regression', 'set', 'data set', 'variance', 'average']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"i=1 data point whose hat diagonal element is large, that is, well above the average value of (k + 1)/n, is in a position in the data set where the variance of ŷi is relatively large and the variance of a residual is relatively small. as a result, the data analyst can gain some insight into how large a residual may become before its deviation from zero can be attributed to something other than mere chance. many of the commercial regression computer packages produce the set of studentized residuals."
2876,1,['residual'], Study of Residuals and Violation of Assumptions Model Checking,seg_337,"studentized ei ri = , i = 1, 2, . . . , n residual s√1− hii"
2877,1,"['estimate', 'residual', 'residuals', 'information', 'statistic', 'set', 'standard']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"here each residual has been divided by an estimate of its standard deviation, creating a t-like statistic that is designed to give the analyst a scale-free quantity providing information regarding the size of the residual. in addition, standard computer packages often provide values of another set of studentizedtype residuals called the r-student values."
2878,1,['residual'], Study of Residuals and Violation of Assumptions Model Checking,seg_337,"r-student residual ei ti = , i = 1, 2, . . . , n, s−i√1− hii"
2879,1,"['deviation', 'estimate', 'data', 'standard', 'standard deviation', 'error']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"where s−i is an estimate of the error standard deviation, calculated with the ith data point deleted."
2880,1,"['plot', 'residual', 'residuals', 'plots', 'residual plots']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"there are three types of violations of assumptions that are readily detected through use of residuals or residual plots. while plots of the raw residuals, the ei, can be helpful, it is often more informative to plot the studentized residuals. the three violations are as follows:"
2881,1,['outliers'], Study of Residuals and Violation of Assumptions Model Checking,seg_337,1. presence of outliers
2882,1,"['variance', 'error']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,2. heterogeneous error variance
2883,1,['model'], Study of Residuals and Violation of Assumptions Model Checking,seg_337,3. model misspecification
2884,1,"['outliers', 'deviation', 'model', 'case', 'data', 'outlier']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"in case 1, we choose to define an outlier as a data point where there is a deviation from the usual assumption e( i) = 0 for a specific value of i. if there is a reason to believe that a specific data point is an outlier exerting a large influence on the fitted model, ri or ti may be informative. the r-student values can be expected to be more sensitive to outliers than the ri values."
2885,1,"['degrees of freedom', 'condition', 'information', 'outlier', 'random variable', 'variable', 'random']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"in fact, under the condition that e( i) = 0, ti is a value of a random variable following a t-distribution with n−1− (k+1) = n−k−2 degrees of freedom. thus, a two-sided t-test can be used to provide information for detecting whether or not the ith point is an outlier."
2886,1,"['outliers', 'observations', 'residuals', 'statistics', 'locations', 'location', 'outlier', 'statistic', 'data', 'information', 'sets', 'data sets', 'error', 'regression']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"although the r-student statistic ti produces an exact t-test for detection of an outlier at a specific data location, the t-distribution would not apply for simultaneously testing for outliers at all locations. as a result, the studentized residuals or r-student values should be used strictly as diagnostic tools without formal hypothesis testing as the mechanism. the implication is that these statistics highlight data points where the error of fit is larger than what is expected by chance. r-student values large in magnitude suggest a need for “checking” the data with whatever resources are possible. the practice of eliminating observations from regression data sets should not be done indiscriminately. (for further information regarding the use of outlier diagnostics, see myers, 1990, in the bibliography.)"
2887,1,"['experimental', 'method', 'experiment', 'table', 'data', 'variable', 'set', 'average']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"case study 12.1: method for capturing grasshoppers: in a biological experiment conducted at virginia tech by the department of entomology, n experimental runs were made with two different methods for capturing grasshoppers. the methods were drop net catch and sweep net catch. the average number of grasshoppers caught within a set of field quadrants on a given date was recorded for each of the two methods. an additional regressor variable, the average plant height in the quadrants, was also recorded. the experimental data are given in table 12.10."
2888,1,"['model', 'method', 'estimate', 'data']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"the goal is to be able to estimate grasshopper catch by using only the sweep net method, which is less costly. there was some concern about the validity of the fourth data point. the observed catch that was reported using the net drop method seemed unusually high given the other conditions and, indeed, it was felt that the figure might be erroneous. fit a model of the type"
2889,1,"['outlier', 'residuals', 'data']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,to the 17 data points and study the residuals to determine if data point 4 is an outlier.
2890,1,"['case', 'data', 'set', 'data set']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,table 12.10: data set for case study 12.1
2891,1,['observation'], Study of Residuals and Violation of Assumptions Model Checking,seg_337,"drop net sweep net plant observation catch, y catch, x1 height, x2 (cm)"
2892,1,"['model', 'regression model', 'regression']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,solution : a computer package generated the fitted regression model
2893,1,"['table', 'residuals', 'statistics', 'information']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,along with the statistics r2 = 0.9244 and s2 = 5.580. the residuals and other diagnostic information were also generated and recorded in table 12.11.
2894,1,"['degrees of freedom', 'random', 'measurement', 'residual', 'standard error', 'results', 'residuals', 'observation', 'location', 'variable', 'random variable', 'standard', 'measurement error', 'error']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"as expected, the residual at the fourth location appears to be unusually high, namely 7.769. the vital issue here is whether or not this residual is larger than one would expect by chance. the residual standard error for point 4 is 2.209. the rstudent value t4 is found to be 9.9315. viewing this as a value of a random variable having a t-distribution with 13 degrees of freedom, one would certainly conclude that the residual of the fourth observation is estimating something greater than 0 and that the suspected measurement error is supported by the study of residuals. notice that no other residual results in an r-student value that produces any cause for alarm."
2895,1,"['plot', 'model', 'plots', 'residuals', 'regression analysis', 'regression', 'multiple regression', 'normal', 'probability', 'probability plotting', 'plotting']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"in chapter 11, we discussed, in some detail, the usefulness of plotting residuals in regression analysis. violation of model assumptions can often be detected through these plots. in multiple regression, normal probability plotting of residuals or plotting of residuals against ŷ may be useful. however, it is often preferable to plot studentized residuals."
2896,1,"['residuals', 'plotting', 'variance']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,keep in mind that the preference for the studentized residuals over ordinary residuals for plotting purposes stems from the fact that since the variance of the
2897,1,"['residual', 'case', 'data', 'information', 'set', 'data set']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,table 12.11: residual information for the data set of case study 12.1
2898,1,"['plot', 'model', 'residual', 'residuals', 'dispersion', 'homogeneous', 'standardization', 'variance', 'variances']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"ith residual depends on the ith hat diagonal, variances of residuals will differ if there is a dispersion in the hat diagonals. thus, the appearance of a plot of residuals may seem to suggest heterogeneity because the residuals themselves do not behave, in general, in an ideal way. the purpose of using studentized residuals is to provide a type of standardization. clearly, if σ were known, then under ideal conditions (i.e., a correct model and homogeneous variance), we would have"
2899,1,"['plot', 'residuals', 'statistics', 'case', 'data', 'observation', 'set', 'standard']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,so the studentized residuals produce a set of statistics that behave in a standard way under ideal conditions. figure 12.5 shows a plot of the r-student values for the grasshopper data of case study 12.1. note how the value for observation 4 stands out from the rest. the r-student plot was generated by sas software. the plot shows the residuals against the ŷ-values.
2900,1,"['probability plots', 'plots', 'residuals', 'case', 'probability', 'multiple linear regression', 'linear', 'standard', 'regression', 'linear regression', 'plotting', 'normal probability plots', 'normality', 'normal', 'probability plotting']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,"the reader should recall the importance of normality checking through the use of normal probability plotting, as discussed in chapter 11. the same recommendation holds for the case of multiple linear regression. normal probability plots can be generated using standard regression software. again, however, they can be more effective when one does not use ordinary residuals but, rather, studentized residuals or r-student values."
2901,1,"['predicted', 'case', 'data']", Study of Residuals and Violation of Assumptions Model Checking,seg_337,figure 12.5: r-student values plotted against predicted values for grasshopper data of case study 12.1.
2902,1,"['confidence intervals', 'estimates', 'prediction', 'set', 'cross validation', 'data', 'intervals', 'mean', 'confidence', 'model', 'regression', 'data set', 'response', 'errors']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"for many regression problems, the experimenter must choose among various alternative models or model forms that are developed from the same data set. quite often, the model that best predicts or estimates mean response is required. the experimenter should take into account the relative sizes of the s2-values for the candidate models and certainly the general nature of the confidence intervals on the mean response. one must also consider how well the model predicts response values that were not used in building the candidate models. the models should be subjected to cross validation. what are required, then, are cross-validation errors rather than fitting errors. such errors in prediction are the press residuals"
2903,1,"['model', 'prediction', 'residuals', 'data', 'coefficients']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"where ŷi,−i is the prediction of the ith data point by a model that did not make use of the ith point in the calculation of the coefficients. these press residuals are calculated from the formula"
2904,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,"(the derivation can be found in myers, 1990.)"
2905,1,"['utility', 'residuals', 'data']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,the motivation for press and the utility of press residuals are very simple to understand. the purpose of extracting or setting aside data points one at a time is
2906,1,"['model', 'independent', 'residual', 'prediction', 'predicted', 'observation', 'error']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"to allow the use of separate methodologies for fitting and assessment of a specific model. for assessment of a model, the “−i” indicates that the press residual gives a prediction error where the observation being predicted is independent of the model fit."
2907,1,['residuals'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,criteria that make use of the press residuals are given by
2908,1,"['sum of squares', 'prediction', 'prediction sum of squares']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,(the term press is an acronym for prediction sum of squares.) we suggest that both of these criteria be used. it is possible for press to be dominated by
2909,1,['residuals'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,"n one or only a few large press residuals. clearly, the criterion on |δi| is less"
2910,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,i=1 sensitive to a small number of large values.
2911,1,"['prediction', 'statistic']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"in addition to the press statistic itself, the analyst can simply compute an r2-like statistic reflecting prediction performance. the statistic is often called rp"
2912,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,2red and is given as follows:
2913,1,"['prediction', 'model']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"r2 of prediction given a fitted model with a specific value for press, rp"
2914,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,2red is given by
2915,1,['statistic'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,2red is merely the ordinary r2 statistic with sse replaced by the press statistic.
2916,1,"['model', 'residuals', 'case', 'data', 'regression', 'set', 'statistical']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"in the following case study, an illustration is provided in which many candidate models are fit to a set of data and the best model is chosen. the sequential procedures described in section 12.9 are not used. rather, the role of the press residuals and other statistical values in selecting the best regression equation is illustrated."
2917,1,"['model', 'factors', 'experiment', 'variables', 'table', 'successful', 'average', 'response']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"case study 12.2: football punting: leg strength is a necessary characteristic of a successful punter in american football. one measure of the quality of a good punt is the “hang time.” this is the time that the ball hangs in the air before being caught by the punt returner. to determine what leg strength factors influence hang time and to develop an empirical model for predicting this response, a study on the relationship between selected physical performance variables and football punting ability was conducted by the department of health, physical education, and recreation at virginia tech. thirteen punters were chosen for the experiment, and each punted a football 10 times. the average hang times, along with the strength measures used in the analysis, were recorded in table 12.12."
2918,1,['variable'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,each regressor variable is defined as follows:
2919,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,"1. rls, right leg strength (pounds)"
2920,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,"2. lls, left leg strength (pounds)"
2921,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,"3. rhf, right hamstring muscle flexibility (degrees)"
2922,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,"4. lhf, left hamstring muscle flexibility (degrees)"
2923,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,"5. power, overall leg strength (foot-pounds)"
2924,1,['model'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,determine the most appropriate model for predicting hang time.
2925,1,"['case', 'data']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,table 12.12: data for case study 12.2
2926,1,"['model', 'table', 'information', 'regression', 'statistic']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"solution : in the search for the best of the candidate models for predicting hang time, the information in table 12.13 was obtained from a regression computer package. the models are ranked in ascending order of the values of the press statistic. this display provides enough information on all possible models to enable the user to eliminate from consideration all but a few models. the model containing x2 and x5 (lls and power), denoted by x2x5, appears to be superior for predicting punter"
2927,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,"n hang time. also note that all models with low press, low s2, low ∑ |δi|, and"
2928,1,['variables'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,i=1 high r2-values contain these two variables.
2929,1,"['residuals', 'regression']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,in order to gain some insight from the residuals of the fitted regression
2930,1,"['prediction', 'model', 'residuals']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,the residuals and press residuals were generated. the actual prediction model (see exercise 12.47 on page 494) is given by
2931,1,['table'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,"residuals, hat diagonal values, and press values are listed in table 12.14."
2932,1,"['model', 'independent', 'regression model', 'prediction', 'residuals', 'data', 'regression', 'average', 'error', 'predictions']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"note the relatively good fit of the two-variable regression model to the data. the press residuals reflect the capability of the regression equation to predict hang time if independent predictions were to be made. for example, for punter number 4, the hang time of 4.180 would encounter a prediction error of 0.039 if the model constructed by using the remaining 12 punters were used. for this model, the average prediction error or cross-validation error is"
2933,1,['regression'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,table 12.13: comparing different regression models
2934,1,['average'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,which is small compared to the average hang time for the 13 punters.
2935,1,"['model', 'cp statistic', 'statistics', 'statistic']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"we indicated in section 12.9 that the use of all possible subset regressions is often advisable when searching for the best model. most commercial statistics software packages contain an all possible regressions routine. these algorithms compute various criteria for all subsets of model terms. obviously, criteria such as r2, s2, and press are reasonable for choosing among candidate subsets. another very popular and useful statistic, particularly for areas in the physical sciences and engineering, is the cp statistic, described below."
2936,1,['residuals'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,table 12.14: press residuals
2937,1,"['cp statistic', 'model', 'statistic']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"quite often, the choice of the most appropriate model involves many considerations. obviously, the number of model terms is important; the matter of parsimony is a consideration that cannot be ignored. on the other hand, the analyst cannot be pleased with a model that is too simple, to the point where there is serious underspecification. a single statistic that represents a nice compromise in this regard is the cp statistic. (see mallows, 1973, in the bibliography.)"
2938,1,"['model', 'bias', 'error', 'parameters', 'cp statistic', 'prediction', 'mean square', 'statistic', 'mean', 'function', 'variance', 'mean square error']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,the cp statistic appeals nicely to common sense and is developed from considerations of the proper compromise between excessive bias incurred when one underfits (chooses too few model terms) and excessive prediction variance produced when one overfits (has redundancies in the model). the cp statistic is a simple function of the total number of parameters in the candidate model and the mean square error s2.
2939,1,"['cp statistic', 'model', 'statistic', 'estimate']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"we will not present the entire development of the cp statistic. (for details, the reader is referred to myers, 1990, in the bibliography.) the cp for a particular subset model is an estimate of the following:"
2940,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,n n 1 1 γ(p) = σ2 ∑var(ŷi) + σ2 ∑(bias ŷi)2. i=1 i=1
2941,1,"['model', 'variables', 'least squares', 'standard']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"it turns out that under the standard least squares assumptions indicated earlier in this chapter, and assuming that the “true” model is the model containing all candidate variables,"
2942,1,"['model', 'parameters']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,n 1 σ2 ∑var(ŷi) = p (number of parameters in the candidate model) i=1
2943,1,"['unbiased', 'estimate']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,(see review exercise 12.63) and an unbiased estimate of
2944,1,['bias'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,n n 1 1 (s2 − σ2)(n− p) (bias ŷ )2 is given by (b̂ias ŷ )2 = . σ2 ∑ i σ2 ∑ i σ2 i=1 i=1
2945,1,"['model', 'error', 'estimate', 'mean square', 'mean', 'population', 'variance', 'mean square error']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"in the above, s2 is the mean square error for the candidate model and σ2 is the population error variance. thus, if we assume that some estimate σ̂2 is available for σ2, cp is given by the following equation:"
2946,1,['statistic'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,"cp statistic (s2 − σ̂2)(n− p) cp = p+ , σ̂2"
2947,1,"['model', 'error', 'parameters', 'estimate', 'mean square', 'mean', 'mean square error']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"where p is the number of model parameters, s2 is the mean square error for the candidate model, and σ̂2 is an estimate of σ2."
2948,1,"['model', 'statistic', 'biased']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"obviously, the scientist should adopt models with small values of cp. the reader should note that, unlike the press statistic, cp is scale-free. in addition, one can gain some insight concerning the adequacy of a candidate model by observing its value of cp. for example, cp > p indicates a model that is biased due to being an underfitted model, whereas cp ≈ p indicates a reasonable model."
2949,1,"['model', 'bias', 'error', 'experimental', 'estimate', 'design', 'cp statistic', 'mean square', 'statistic', 'mean', 'population', 'mean square error']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"there is often confusion concerning where σ̂2 comes from in the formula for cp. obviously, the scientist or engineer does not have access to the population quantity σ2. in applications where replicated runs are available, say in an experimental design situation, a model-independent estimate of σ2 is available (see chapters 11 and 15). however, most software packages use σ̂2 as the mean square error from the most complete model. obviously, if this is not a good estimate, the bias portion of the cp statistic can be negative. thus, cp can be less than p."
2950,1,"['factors', 'table', 'data', 'set', 'data set']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"example 12.12: consider the data set in table 12.15, in which a maker of asphalt shingles is interested in the relationship between sales for a particular year and factors that influence sales. (the data were taken from kutner et al., 2004, in the bibliography.)"
2951,1,"['statistics', 'information']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"of the possible subset models, three are of particular interest. these three are x2x3, x1x2x3, and x1x2x3x4. the following represents pertinent information for comparing the three models. we include the press statistics for the three models to supplement the decision making."
2952,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,model r2 rp2red s2 press cp
2953,1,"['bias', 'model', 'error', 'table', 'information', 'mean square', 'mean', 'mean square error', 'full model']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"it seems clear from the information in the table that the model x1, x2, x3 is preferable to the other two. notice that, for the full model, cp = 5.0. this occurs since the bias portion is zero, and σ̂2 = 26.2073 is the mean square error from the full model."
2954,1,['information'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,"figure 12.6 is a sas proc reg printout showing information for all possible regressions. here we are able to show comparisons of other models with (x1, x2, x3). note that (x1, x2, x3) appears to be quite good when compared to all models."
2955,1,"['plot', 'model', 'residuals', 'normal']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,"as a final check on the model (x1, x2, x3), figure 12.7 shows a normal probability plot of the residuals for this model."
2956,1,['data'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,table 12.15: data for example 12.12
2957,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,"promotional active competing potential, sales, y"
2958,0,[], Cross Validation Cp and Other Criteria for Model Selection,seg_339,"district accounts, x1 accounts, x2 brands, x3 x4 (thousands)"
2959,1,"['mse', 'model', 'variables', 'variable', 'adjusted']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,dependent variable: sales number in adjusted model c(p) r-square r-square mse variables in model
2960,1,['data'], Cross Validation Cp and Other Criteria for Model Selection,seg_339,figure 12.6: sas printout of all possible subsets on sales data for example 12.12.
2961,1,"['linear', 'nonlinear regression', 'nonlinear', 'regression', 'linear regression', 'multiple linear regression']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,494 chapter 12 multiple linear regression and certain nonlinear regression models
2962,1,"['plot', 'model', 'probability plot', 'residuals', 'normal', 'probability', 'normal probability plot']", Cross Validation Cp and Other Criteria for Model Selection,seg_339,figure 12.7: normal probability plot of residuals using the model x1x2x3 for example 12.12.
2963,1,"['model', 'errors', 'normal', 'mean', 'variance']", Special Nonlinear Models for Nonideal Conditions,seg_343,"in much of the preceding material in this chapter and in chapter 11, we have benefited substantially from the assumption that the model errors, the i, are normal with mean 0 and constant variance σ2. however, there are many real-life"
2964,1,"['model', 'level', 'risk', 'factors', 'trial', 'bernoulli', 'function', 'response']", Special Nonlinear Models for Nonideal Conditions,seg_343,"situations in which the response is clearly nonnormal. for example, a wealth of applications exist where the response is binary (0 or 1) and hence bernoulli in nature. in the social sciences, the problem may be to develop a model to predict whether or not an individual is a good credit risk (0 or 1) as a function of certain socioeconomic regressors such as income, age, gender, and level of education. in a biomedical drug trial, the response is often whether or not the patient responds positively to a drug while regressors may include drug dosage as well as biological factors such as age, weight, and blood pressure. again the response is binary in nature. applications are also abundant in manufacturing areas where certain controllable factors influence whether a manufactured item is defective or not."
2965,1,"['poisson', 'variables', 'data', 'process', 'response']", Special Nonlinear Models for Nonideal Conditions,seg_343,"a second type of nonnormal application on which we will touch briefly has to do with count data. here the assumption of a poisson response is often convenient. in biomedical applications, the number of cancer cell colonies may be the response which is modeled against drug dosages. in the textile industry, the number of imperfections per yard of cloth may be a reasonable response which is modeled against certain process variables."
2966,1,"['poisson', 'model', 'independent', 'case', 'bernoulli', 'normal', 'mean', 'binomial', 'variance', 'response']", Special Nonlinear Models for Nonideal Conditions,seg_343,"the reader should note the comparison of the ideal (i.e., the normal response) situation with that of the bernoulli (or binomial) or the poisson response. we have become accustomed to the fact that the normal case is very special in that the variance is independent of the mean. clearly this is not the case for either bernoulli or poisson responses. for example, if the response is 0 or l, suggesting a bernoulli response, then the model is of the form"
2967,1,"['poisson', 'model', 'linear', 'case', 'least squares', 'regression', 'bernoulli', 'success', 'normal', 'linear regression', 'response', 'probability', 'standard', 'function', 'parameter', 'variance', 'probability of a success']", Special Nonlinear Models for Nonideal Conditions,seg_343,"where p is the probability of a success (say response = 1). the parameter p plays the role of μy |x in the normal case. however, the bernoulli variance is p(1 − p), which, of course, is also a function of the regressor x. as a result, the variance is not constant. this rules out the use of standard least squares, which we have utilized in our linear regression work up to this point. the same is true for the poisson case since the model is of the form"
2968,1,"['logistic', 'experiment', 'logistic regression', 'observational studies', 'case', 'regression', 'distribution', 'bernoulli', 'trial', 'binomial', 'level', 'response']", Special Nonlinear Models for Nonideal Conditions,seg_343,"the most popular approach to modeling binary responses is a technique entitled logistic regression. it is used extensively in the biological sciences, biomedical research, and engineering. indeed, even in the social sciences binary responses are found to be plentiful. the basic distribution for the response is either bernoulli or binomial. the former is found in observational studies where there are no repeated runs at each regressor level, while the latter will be the case when an experiment is designed. for example, in a clinical trial in which a new drug is being evaluated, the goal might be to determine the dose of the drug that provides efficacy. so"
2969,1,"['case', 'experiment']", Special Nonlinear Models for Nonideal Conditions,seg_343,"certain doses will be employed in the experiment, and more than one subject will be used for each dose. this case is called the grouped case."
2970,1,"['model', 'logistic', 'estimate', 'case', 'trial', 'mean', 'probability', 'function', 'response']", Special Nonlinear Models for Nonideal Conditions,seg_343,"in the case of binary responses ,the mean response is a probability. in the preceding clinical trial illustration, we might say that we wish to estimate the probability that the patient responds properly to the drug, p(success). thus, the model is written in terms of a probability. given regressors x, the logistic function is given by"
2971,1,"['linear', 'model', 'linear predictor', 'case', 'bernoulli', 'binomial', 'mean', 'predictor']", Special Nonlinear Models for Nonideal Conditions,seg_343,"the portion x′β is called the linear predictor, and in the case of a single regressor x it might be written x′β = β0 + β1x. of course, we do not rule out involving multiple regressors and polynomial terms in the so-called linear predictor. in the grouped case, the model involves modeling the mean of a binomial rather than a bernoulli, and thus we have the mean given by"
2972,1,"['plot', 'logistic', 'estimated', 'nonlinear', 'case', 'probability', 'function']", Special Nonlinear Models for Nonideal Conditions,seg_343,"a plot of the logistic function reveals a great deal about its characteristics and why it is utilized for this type of problem. first, the function is nonlinear. in addition, the plot in figure 12.8 reveals the s-shape with the function approaching p = 1.0 as an asymptote. in this case, β1 > 0. thus, we would never experience an estimated probability exceeding 1.0."
2973,1,"['function', 'logistic']", Special Nonlinear Models for Nonideal Conditions,seg_343,figure 12.8: the logistic function.
2974,1,"['linear', 'estimated', 'method of maximum likelihood', 'method', 'regression coefficients', 'linear predictor', 'likelihood', 'regression', 'maximum likelihood', 'predictor', 'coefficients']", Special Nonlinear Models for Nonideal Conditions,seg_343,"the regression coefficients in the linear predictor can be estimated by the method of maximum likelihood, as described in chapter 9. the solution to the"
2975,0,[], Special Nonlinear Models for Nonideal Conditions,seg_343,"likelihood equations involves an iterative methodology that will not be described here. however, we will present an example and discuss the computer printout and conclusions."
2976,1,"['logistic', 'experiment', 'table', 'logistic regression', 'results', 'data', 'regression', 'set', 'data set']", Special Nonlinear Models for Nonideal Conditions,seg_343,example 12.13: the data set in table 12.16 will be used to illustrate the use of logistic regression to analyze a single-agent quantal bioassay of a toxicity experiment. the results show the effect of different doses of nicotine on the common fruit fly.
2977,1,"['set', 'data set', 'data']", Special Nonlinear Models for Nonideal Conditions,seg_343,table 12.16: data set for example 12.13
2978,1,"['concentration', 'percent']", Special Nonlinear Models for Nonideal Conditions,seg_343,x ni y concentration number of number percent (grams/100 cc) insects killed killed
2979,1,"['model', 'experiment', 'results', 'probability', 'concentration']", Special Nonlinear Models for Nonideal Conditions,seg_343,"the purpose of the experiment was to arrive at an appropriate model relating probability of “kill” to concentration. in addition, the analyst sought the so-called effective dose (ed), that is, the concentration of nicotine that results in a certain probability. of particular interest was the ed50, the concentration that produces a 0.5 probability of “insect kill.”"
2980,1,['model'], Special Nonlinear Models for Nonideal Conditions,seg_343,"this example is grouped, and thus the model is given by"
2981,1,"['standard errors', 'method of maximum likelihood', 'method', 'likelihood', 'errors', 'maximum likelihood', 'variance', 'standard', 'tests', 'coefficients']", Special Nonlinear Models for Nonideal Conditions,seg_343,estimates of β0 and β1 and their standard errors are found by the method of maximum likelihood. tests on individual coefficients are found using χ2-statistics rather than t-statistics since there is no common variance σ2. the χ2-statistic is
2982,1,['standard'], Special Nonlinear Models for Nonideal Conditions,seg_343,coeff derived from ( standard error)2.
2983,0,[], Special Nonlinear Models for Nonideal Conditions,seg_343,"thus, we have the following from a sas proc logist printout."
2984,1,"['estimate', 'estimates', 'standard error', 'parameter', 'standard', 'error']", Special Nonlinear Models for Nonideal Conditions,seg_343,analysis of parameter estimates df estimate standard error chi-squared p-value
2985,1,"['model', 'probability', 'coefficients']", Special Nonlinear Models for Nonideal Conditions,seg_343,"both coefficients are significantly different from zero. thus, the fitted model used to predict the probability of “kill” is given by"
2986,1,"['linear', 'nonlinear regression', 'nonlinear', 'regression', 'linear regression', 'multiple linear regression']", Special Nonlinear Models for Nonideal Conditions,seg_343,500 chapter 12 multiple linear regression and certain nonlinear regression models
2987,1,"['logistic', 'estimate', 'estimates', 'function']", Special Nonlinear Models for Nonideal Conditions,seg_343,"the estimate of ed50 for example 12.13 is found very simply from the estimates b0 for β0 and b1 for β1. from the logistic function, we see that"
2988,1,['estimate'], Special Nonlinear Models for Nonideal Conditions,seg_343,"as a result, for p = 0.5, an estimate of x is found from"
2989,0,[], Special Nonlinear Models for Nonideal Conditions,seg_343,"thus, ed50 is given by"
2990,1,"['logistic', 'odds ratio']", Special Nonlinear Models for Nonideal Conditions,seg_343,another form of inference that is conveniently accomplished using logistic regression is derived from the use of the odds ratio. the odds ratio is designed to
2991,1,"['success', 'case']", Special Nonlinear Models for Nonideal Conditions,seg_343,"p determine how the odds of success, 1−p , increases as certain changes in regressor values occur. for example, in the case of example 12.13 we may wish to know how the odds would increase if one were to increase dosage by, say, 0.2 gram/100 cc."
2992,1,"['logistic', 'condition', 'odds ratio', 'logistic regression', 'regression', 'success']", Special Nonlinear Models for Nonideal Conditions,seg_343,"definition 12.1: in logistic regression, an odds ratio is the ratio of odds of success at condition 2 to that of condition 1 in the regressors, that is,"
2993,1,['utility'], Special Nonlinear Models for Nonideal Conditions,seg_343,this allows the analyst to ascertain a sense of the utility of changing the regressor
2994,0,[], Special Nonlinear Models for Nonideal Conditions,seg_343,"by a certain number of units. now, since ( 1−"
2995,1,['success'], Special Nonlinear Models for Nonideal Conditions,seg_343,the ratio reflecting the increase in odds of success when the dosage of nicotine is increased by 0.2 gram/100 cc is given by
2996,1,"['factor', 'success', 'odds ratio']", Special Nonlinear Models for Nonideal Conditions,seg_343,the implication of an odds ratio of 3.522 is that the odds of success is enhanced by a factor of 3.522 when the nicotine dose is increased by 0.2 gram/100 cc.
2997,1,"['linear', 'nonlinear regression', 'nonlinear', 'regression', 'linear regression', 'multiple linear regression']",Review Exercises,seg_347,502 chapter 12 multiple linear regression and certain nonlinear regression models
2998,0,[],Review Exercises,seg_347,figure 12.9: sas output for review exercise 12.72; part i.
2999,1,"['predicted', 'variable', 'mean', 'error']",Review Exercises,seg_347,dependent predicted std error obs variable value mean predict 95% cl mean 95% cl predict 1 566.5200 775.0251 241.2323 244.0765 1306 -734.6494 2285 2 696.8200 740.6702 331.1402 11.8355 1470 -849.4275 2331 3 1033 1104 278.5116 490.9234 1717 -436.5244 2644 4 1604 1240 268.1298 650.3459 1831 -291.0028 2772 5 1611 1564 211.2372 1099 2029 76.6816 3052 6 1613 2151 279.9293 1535 2767 609.5796 3693 7 1854 1690 218.9976 1208 2172 196.5345 3183 8 2161 1736 468.9903 703.9948 2768 -13.8306 3486 9 2306 2737 290.4749 2098 3376 1186 4288 10 3504 3682 585.2517 2394 4970 1770 5594 11 3572 3239 189.0989 2823 3655 1766 4713 12 3741 4353 328.8507 3630 5077 2766 5941 13 4027 4257 314.0481 3566 4948 2684 5830 14 10344 8768 252.2617 8213 9323 7249 10286 15 11732 12237 573.9168 10974 13500 10342 14133 16 15415 15038 585.7046 13749 16328 13126 16951 17 18854 19321 599.9780 18000 20641 17387 21255
3000,1,"['residual', 'error']",Review Exercises,seg_347,std error student obs residual residual residual -2-1 0 1 2 1 -208.5051 595.0 -0.350 | | | 2 -43.8502 550.1 -0.0797 | | | 3 -70.7734 578.5 -0.122 | | | 4 363.1244 583.4 0.622 | |* | 5 46.9483 606.3 0.0774 | | | 6 -538.0017 577.9 -0.931 | *| | 7 164.4696 603.6 0.272 | | | 8 424.3145 438.5 0.968 | |* | 9 -431.4090 572.6 -0.753 | *| | 10 -177.9234 264.1 -0.674 | *| | 11 332.6011 613.6 0.542 | |* | 12 -611.9330 551.5 -1.110 | **| | 13 -230.5684 560.0 -0.412 | | | 14 1576 590.5 2.669 | |***** | 15 -504.8574 287.9 -1.753 | ***| | 16 376.5491 263.1 1.431 | |** | 17 -466.2470 228.7 -2.039 | ****| |
3001,0,[],Review Exercises,seg_347,figure 12.10: sas output for review exercise 12.72; part ii.
3002,1,"['model', 'linear', 'linear model', 'variables', 'nonlinear', 'statistical']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_349,"there are several procedures discussed in this chapter for use in the “attempt” to find the best model. however, one of the most important misconceptions under which näıve scientists or engineers labor is that there is a true linear model and that it can be found. in most scientific phenomena, relationships between scientific variables are nonlinear in nature and the true model is unknown. linear statistical models are empirical approximations."
3003,1,"['model', 'prediction', 'regression', 'collinearity']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_349,"at times, the choice of the model to be adopted may depend on what information needs to be derived from the model. is it to be used for prediction? is it to be used for the purpose of explaining the role of each regressor? this “choice” can be made difficult in the presence of collinearity. it is true that for many regression problems there are multiple models that are very similar in performance. see the myers reference (1990) for details."
3004,1,"['model', 'data', 'set', 'data set']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_349,"one of the most damaging misuses of the material in this chapter is to assign too much importance to r2 in the choice of the so-called best model. it is important to remember that for any data set, one can obtain an r2 as large as one desires, within the constraint 0 ≤ r2 ≤ 1. too much attention to r2 often leads to overfitting."
3005,1,"['outliers', 'model', 'statistics', 'data', 'outlier', 'model selection', 'set', 'data set', 'error']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_349,"much attention was given in this chapter to outlier detection. a classical serious misuse of statistics centers around the decision made concerning the detection of outliers. we hope it is clear that the analyst should absolutely not carry out the exercise of detecting outliers, eliminating them from the data set, fitting a new model, reporting outlier detection, and so on. this is a tempting and disastrous procedure for arriving at a model that fits the data well, with the result being an example of how to lie with statistics. if an outlier is detected, the history of the data should be checked for possible clerical or procedural error before it is eliminated from the data set. one must remember that an outlier by definition is a data point that the model did not fit well. the problem may not be in the data but rather in the model selection. a changed model may result in the point not being detected as an outlier."
3006,1,"['poisson', 'homogeneous', 'least squares', 'errors', 'distribution', 'observation', 'normal', 'response', 'mean', 'binomial', 'standard', 'binomial distribution', 'variance', 'vary']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_349,"there are many types of responses that occur naturally in practice but can’t be used in an analysis of standard least squares because classic least squares assumptions do not hold. the assumptions that often fail are those of normal errors and homogeneous variance. for example, if the response is a proportion, say proportion defective, the response distribution is related to the binomial distribution. a second response that occurs often in practice is that of poisson counts. clearly the distribution is not normal, and the response variance, which is equal to the poisson mean, will vary from observation to observation. for more details on these nonideal conditions, see myers et al. (2008) in the bibliography."
3007,1,"['independent', 'parameters', 'estimate', 'estimation', 'case', 'hypothesis testing', 'samples', 'normal', 'population', 'populations', 'variance', 'hypothesis']", AnalysisofVariance Technique,seg_353,"in the estimation and hypothesis testing material covered in chapters 9 and 10, we were restricted in each case to considering no more than two population parameters. such was the case, for example, in testing for the equality of two population means using independent samples from normal populations with common but unknown variance, where it was necessary to obtain a pooled estimate of σ2."
3008,1,"['sample', 'levels', 'factor', 'treatment', 'case', 'sampling', 'samples', 'process']", AnalysisofVariance Technique,seg_353,"this material dealing in two-sample inference represents a special case of what we call the one-factor problem. for example, in exercise 10.35 on page 357, the survival time was measured for two samples of mice, where one sample received a new serum for leukemia treatment and the other sample received no treatment. in this case, we say that there is one factor, namely treatment, and the factor is at two levels. if several competing treatments were being used in the sampling process, more samples of mice would be necessary. in this case, the problem would involve one factor with more than two levels and thus more than two samples."
3009,1,"['sample', 'samples', 'analysis of variance', 'variance', 'populations', 'population', 'anova']", AnalysisofVariance Technique,seg_353,"in the k > 2 sample problem, it will be assumed that there are k samples from k populations. one very common procedure used to deal with testing population means is called the analysis of variance, or anova."
3010,1,"['regression', 'sum of squares', 'analysis of variance', 'variance', 'error']", AnalysisofVariance Technique,seg_353,the analysis of variance is certainly not a new technique to the reader who has followed the material on regression theory. we used the analysis-of-variance approach to partition the total sum of squares into a portion due to regression and a portion due to error.
3011,1,"['experiment', 'table', 'data', 'samples', 'mean']", AnalysisofVariance Technique,seg_353,"suppose in an industrial experiment that an engineer is interested in how the mean absorption of moisture in concrete varies among 5 different concrete aggregates. the samples are exposed to moisture for 48 hours. it is decided that 6 samples are to be tested for each aggregate, requiring a total of 30 samples to be tested. the data are recorded in table 13.1."
3012,1,"['model', 'observations', 'set', 'populations', 'test']", AnalysisofVariance Technique,seg_353,"the model for this situation may be set up as follows. there are 6 observations taken from each of 5 populations with means μ1, μ2, . . . , μ5, respectively. we may wish to test"
3013,0,[], AnalysisofVariance Technique,seg_353,h1: at least two of the means are not equal.
3014,0,[], AnalysisofVariance Technique,seg_353,table 13.1: absorption of moisture in concrete aggregates
3015,1,['mean'], AnalysisofVariance Technique,seg_353,"total 3320 3416 3663 2791 3664 16,854 mean 553.33 569.33 610.50 465.17 610.67 561.80"
3016,1,['population'], AnalysisofVariance Technique,seg_353,"in addition, we may be interested in making individual comparisons among these 5 population means."
3017,1,"['observations', 'random', 'sample', 'rate', 'experiment', 'sample means', 'random variation', 'analysis of variance', 'variance', 'variation']", AnalysisofVariance Technique,seg_353,"in the analysis-of-variance procedure, it is assumed that whatever variation exists among the aggregate averages is attributed to (1) variation in absorption among observations within aggregate types and (2) variation among aggregate types, that is, due to differences in the chemical composition of the aggregates. the withinaggregate variation is, of course, brought about by various causes. perhaps humidity and temperature conditions were not kept entirely constant throughout the experiment. it is possible that there was a certain amount of heterogeneity in the batches of raw materials that were used. at any rate, we shall consider the within-sample variation to be chance or random variation. part of the goal of the analysis of variance is to determine if the differences among the 5 sample means are what we would expect due to random variation alone or, rather, due to variation beyond merely random effects, i.e., differences in the chemical composition of the aggregates."
3018,1,"['random variation', 'variation', 'samples', 'statistical', 'random', 'control']", AnalysisofVariance Technique,seg_353,"many pointed questions appear at this stage concerning the preceding problem. for example, how many samples must be tested for each aggregate? this is a question that continually haunts the practitioner. in addition, what if the withinsample variation is so large that it is difficult for a statistical procedure to detect the systematic differences? can we systematically control extraneous sources of variation and thus remove them from the portion we call random variation? we shall attempt to answer these and other questions in the following sections."
3019,1,"['experimental', 'levels', 'experiment', 'design of experiments', 'design', 'estimation', 'case', 'factor', 'experimental units', 'experiments']", The Strategy of Experimental Design,seg_355,"in chapters 9 and 10, the notions of estimation and testing for the two-sample case were covered under the important backdrop of the way the experiment is conducted. this falls into the broad category of design of experiments. for example, for the pooled t-test discussed in chapter 10, it is assumed that the factor levels (treatments in the mice example) are assigned randomly to the experimental units (mice). the notion of experimental units was discussed in chapters 9 and 10 and"
3020,1,"['levels', 'factor', 'random', 'bias', 'error', 'model', 'experimental', 'analysis of variance', 'risks', 'experimental units', 'variance', 'experimental error', 'homogeneous', 'experiments']", The Strategy of Experimental Design,seg_355,"illustrated through examples. simply put, experimental units are the units (mice, patients, concrete specimens, time) that provide the heterogeneity that leads to experimental error in a scientific investigation. the random assignment eliminates bias that could result with systematic assignment. the goal is to distribute uniformly among the factor levels the risks brought about by the heterogeneity of the experimental units. random assignment best simulates the conditions that are assumed by the model. in section 13.7, we discuss blocking in experiments. the notion of blocking was presented in chapters 9 and 10, when comparisons between means were accomplished with pairing, that is, the division of the experimental units into homogeneous pairs called blocks. the factor levels or treatments are then assigned randomly within blocks. the purpose of blocking is to reduce the effective experimental error. in this chapter, we naturally extend the pairing to larger block sizes, with analysis of variance being the primary analytical tool."
3021,1,"['treatment', 'populations', 'samples']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"random samples of size n are selected from each of k populations. the k different populations are classified on the basis of a single criterion such as different treatments or groups. today the term treatment is used generally to refer to the various classifications, whether they be different aggregates, different analysts, different fertilizers, or different regions of the country."
3022,1,"['independent', 'populations', 'randomization', 'normally distributed', 'variance', 'hypothesis']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"it is assumed that the k populations are independent and normally distributed with means μ1, μ2, . . . , μk and common variance σ2. as indicated in section 13.2, these assumptions are made more palatable by randomization. we wish to derive appropriate methods for testing the hypothesis"
3023,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,h1: at least two of the means are not equal.
3024,1,"['sample', 'table', 'treatment', 'observations', 'data', 'observation', 'mean']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"let yij denote the jth observation from the ith treatment and arrange the data as in table 13.2. here, yi. is the total of all observations in the sample from the ith treatment, ȳi. is the mean of all observations in the sample from the ith treatment, y.. is the total of all nk observations, and ȳ.. is the mean of all nk observations."
3025,1,['observation'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,each observation may be written in the form
3026,1,"['deviation', 'sample', 'treatment', 'observation', 'regression', 'mean', 'random', 'random error', 'error']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,where ij measures the deviation of the jth observation of the ith sample from the corresponding treatment mean. the ij-term represents random error and plays the same role as the error terms in the regression models. an alternative and
3027,1,"['random', 'random samples', 'samples']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,table 13.2: k random samples
3028,1,['mean'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,total y1. y2. · · · yi. · · · yk. y.. mean ȳ1. ȳ2. · · · ȳi. · · · ȳk. ȳ..
3029,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"preferred form of this equation is obtained by substituting μi = μ+ αi, subject to"
3030,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"k the constraint ∑ αi = 0. hence, we may write"
3031,1,"['grand mean', 'mean']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"where μ is just the grand mean of all the μi, that is,"
3032,1,['treatment'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,and αi is called the effect of the ith treatment.
3033,1,"['population', 'null hypothesis', 'hypothesis']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,the null hypothesis that the k population means are equal against the alternative that at least two of the means are unequal may now be replaced by the equivalent hypothesis
3034,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,h1: at least one of the αi is not equal to zero.
3035,1,"['population variance', 'independent', 'variability', 'estimates', 'data', 'summation', 'variance', 'population', 'test']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"our test will be based on a comparison of two independent estimates of the common population variance σ2. these estimates will be obtained by partitioning the total variability of our data, designated by the double summation"
3036,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,into two components.
3037,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,sum-of-squares identity
3038,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,it will be convenient in what follows to identify the terms of the sum-of-squares identity by the following notation:
3039,0,['n'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,three important k n
3040,1,"['sum of squares', 'treatment', 'variability']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"measures of sst = ∑∑(yij − ȳ..)2 = total sum of squares, variability i=1 j=1 k ssa = n∑(ȳi. − ȳ..)2 = treatment sum of squares, i=1 k n"
3041,1,"['sum of squares', 'error sum of squares', 'error']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,sse = ∑∑(yij − ȳi.)2 = error sum of squares. i=1 j=1
3042,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,the sum-of-squares identity can then be represented symbolically by the equation
3043,1,"['estimates', 'variation', 'sum of squares', 'expected value', 'population', 'test', 'variance']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"the identity above expresses how between-treatment and within-treatment variation add to the total sum of squares. however, much insight can be gained by investigating the expected value of both ssa and sse. eventually, we shall develop variance estimates that formulate the ratio to be used to test the equality of population means."
3044,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,the proof of the theorem is left as an exercise (see review exercise 13.53 on page 556).
3045,1,"['degrees of freedom', 'estimate']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"if h0 is true, an estimate of σ2, based on k− 1 degrees of freedom, is provided by this expression:"
3046,1,['mean'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,treatment mean ssa square s12 = k − 1
3047,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"if h0 is true and thus each αi in theorem 13.2 is equal to zero, we see that"
3048,1,"['unbiased', 'estimate']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"and s21 is an unbiased estimate of σ2. however, if h1 is true, we have"
3049,1,"['estimates', 'variation']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"and s12 estimates σ2 plus an additional term, which measures variation due to the systematic effects."
3050,1,"['degrees of freedom', 'independent', 'estimate']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"a second and independent estimate of σ2, based on k(n−1) degrees of freedom, is this familiar formula:"
3051,1,['mean'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,error mean sse s2 = square k(n− 1)
3052,1,"['condition', 'treatment', 'expected values', 'mean square', 'mean squares', 'mean', 'test']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"it is instructive to point out the importance of the expected values of the mean squares indicated above. in the next section, we discuss the use of an f-ratio with the treatment mean square residing in the numerator. it turns out that when h1 is true, the presence of the condition e(s12) > e(s2) suggests that the f-ratio be used in the context of a one-sided upper-tailed test. that is, when h1 is true, we would expect the numerator s21 to exceed the denominator."
3053,1,"['degrees of freedom', 'variability', 'estimate', 'data', 'null hypothesis', 'unbiased', 'hypothesis']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"the estimate s2 is unbiased regardless of the truth or falsity of the null hypothesis (see review exercise 13.52 on page 556). it is important to note that the sum-of- squares identity has partitioned not only the total variability of the data, but also the total number of degrees of freedom. that is,"
3054,1,"['degrees of freedom', 'distribution', 'random variable', 'variable', 'random', 'tail', 'test', 'critical region']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"when h0 is true, the ratio f = s12/s2 is a value of the random variable f having the f-distribution with k−1 and k(n−1) degrees of freedom (see theorem 8.8). since s21 overestimates σ2 when h0 is false, we have a one-tailed test with the critical region entirely in the right tail of the distribution."
3055,1,"['significance', 'null hypothesis', 'hypothesis']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,the null hypothesis h0 is rejected at the α-level of significance when
3056,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"another approach, the p-value approach, suggests that the evidence in favor of or against h0 is"
3057,1,['table'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"the computations for an analysis-of-variance problem are usually summarized in tabular form, as shown in table 13.3."
3058,1,"['anova', 'variance', 'analysis of variance']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,table 13.3: analysis of variance for the one-way anova
3059,1,"['mean', 'variation']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,source of sum of degrees of mean computed variation squares freedom square f
3060,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,ssa s21 treatments ssa k − 1 s21 = k − 1 s2
3061,1,['error'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,sse error sse k(n− 1) s2 = k(n− 1)
3062,1,"['table', 'data', 'level', 'significance', 'test', 'level of significance', 'hypothesis']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,example 13.1: test the hypothesis μ1 = μ2 = · · · = μ5 at the 0.05 level of significance for the data of table 13.1 on absorption of moisture by various types of cement aggregates.
3063,1,['hypotheses'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,solution : the hypotheses are
3064,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,h1: at least two of the means are not equal.
3065,1,['degrees of freedom'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,critical region: f > 2.76 with v1 = 4 and v2 = 25 degrees of freedom. the sum-of-squares computations give
3066,1,"['results', 'anova']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,these results and the remaining computations are exhibited in figure 13.1 in the sas anova procedure.
3067,1,"['glm', 'dependent variable', 'dependent', 'variable']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,the glm procedure dependent variable: moisture
3068,1,"['model', 'error', 'mean', 'mean square']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,sum of source df squares mean square f value pr > f model 4 85356.4667 21339.1167 4.30 0.0088 error 25 124020.3333 4960.8133 corrected total 29 209376.8000
3069,1,"['mse', 'mean']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,r-square coeff var root mse moisture mean 0.407669 12.53703 70.43304 561.8000
3070,1,"['mean square', 'mean']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,source df type i ss mean square f value pr > f aggregate 4 85356.46667 21339.11667 4.30 0.0088
3071,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,figure 13.1: sas output for the analysis-of-variance procedure.
3072,1,['mean'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"decision: reject h0 and conclude that the aggregates do not have the same mean absorption. the p-value for f = 4.30 is 0.0088, which is smaller than 0.05."
3073,1,"['plot', 'plots', 'box plot', 'anova']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"in addition to the anova, a box plot was constructed for each aggregate. the plots are shown in figure 13.2. from these plots it is evident that the absorption is not the same for all aggregates. in fact, it appears as if aggregate 4 stands out from the rest. a more formal analysis showing this result will appear in exercise 13.21 on page 531."
3074,1,"['sample size', 'sample', 'experimental', 'random samples', 'observations', 'sum of squares', 'samples', 'random']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"during experimental work, one often loses some of the desired observations. experimental animals may die, experimental material may be damaged, or human subjects may drop out of a study. the previous analysis for equal sample size will still be valid if we slightly modify the sum of squares formulas. we now assume the k random samples to be of sizes n1, n2, . . . , nk, respectively."
3075,1,['sample'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"unequal sample sst =∑∑(yij − ȳ..)2, ssa =∑ni(ȳi. − ȳ..)2, sse = sst − ssa sizes i=1 j=1 i=1"
3076,1,"['box plots', 'plots']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,figure 13.2: box plots for the absorption of moisture in concrete aggregates.
3077,1,['degrees of freedom'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"the degrees of freedom are then partitioned as before: n − 1 for sst, k − 1 for"
3078,1,['levels'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,example 13.2: part of a study conducted at virginia tech was designed to measure serum alkaline phosphatase activity levels (in bessey-lowry units) in children with seizure disorders who were receiving anticonvulsant therapy under the care of a private physician. forty-five subjects were found for the study and categorized into four drug groups:
3079,1,['control'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,g-1: control (not receiving anticonvulsants and having no history of seizure
3080,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,g-2: phenobarbital
3081,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,g-3: carbamazepine
3082,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,g-4: other anticonvulsants
3083,1,"['table', 'samples', 'level', 'significance', 'test', 'level of significance', 'average']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"from blood samples collected from each subject, the serum alkaline phosphatase activity level was determined and recorded as shown in table 13.4. test the hypothesis at the 0.05 level of significance that the average serum alkaline phosphatase activity level is the same for the four drug groups."
3084,1,['level'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,table 13.4: serum alkaline phosphatase activity level
3085,1,"['level', 'significance', 'hypotheses', 'level of significance']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"solution : with the level of significance at 0.05, the hypotheses are"
3086,0,[], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,h1: at least two of the means are not equal.
3087,1,"['variance', 'analysis of variance', 'table']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"critical region: f > 2.836, from interpolating in table a.6. computations: y1. = 1460.25, y2. = 440.36, y3. = 842.45, y4. = 707.41, and y.. = 3450.47. the analysis of variance is shown in the minitab output of figure 13.3."
3088,1,['anova'], OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"one-way anova: g-1, g-2, g-3, g-4"
3089,1,"['factor', 'error']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,source df ss ms f p factor 3 13939 4646 3.57 0.022 error 41 53376 1302 total 44 67315
3090,1,"['level', 'mean']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,individual 95% cis for mean based on pooled stdev level n mean stdev --+---------+---------+---------+------- g-1 20 73.01 25.75 (----*-----) g-2 9 48.93 47.11 (-------*-------) g-3 9 93.61 46.57 (-------*-------) g-4 7 101.06 30.76 (--------*--------) --+---------+---------+---------+------- 30 60 90 120
3091,1,"['data', 'table']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,figure 13.3: minitab analysis of data in table 13.4.
3092,1,"['average', 'levels']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,decision: reject h0 and conclude that the average serum alkaline phosphatase activity levels for the four drug groups are not all the same. the calculated pvalue is 0.022.
3093,1,"['sample', 'type ii error', 'samples', 'analysis of variance', 'probability', 'populations', 'type ii', 'variance', 'error', 'variances']", OneWay Analysis of Variance Completely Randomized Design OneWay ANOVA,seg_357,"in concluding our discussion on the analysis of variance for the one-way classification, we state the advantages of choosing equal sample sizes over the choice of unequal sample sizes. the first advantage is that the f-ratio is insensitive to slight departures from the assumption of equal variances for the k populations when the samples are of equal size. second, the choice of equal sample sizes minimizes the probability of committing a type ii error."
3094,1,"['sample', 'homogeneity', 'case', 'samples', 'normal', 'population', 'hypothesis', 'populations', 'test', 'null hypothesis', 'variances']", Tests for the Equality of Several Variances,seg_359,"although the f-ratio obtained from the analysis-of-variance procedure is insensitive to departures from the assumption of equal variances for the k normal populations when the samples are of equal size, we may still prefer to exercise caution and run a preliminary test for homogeneity of variances. such a test would certainly be advisable in the case of unequal sample sizes if there was a reasonable doubt concerning the homogeneity of the population variances. suppose, therefore, that we wish to test the null hypothesis"
3095,1,['variances'], Tests for the Equality of Several Variances,seg_359,h1: the variances are not all equal.
3096,1,"['sample', 'critical values', 'sampling', 'distribution', 'statistic', 'test', 'sampling distribution']", Tests for the Equality of Several Variances,seg_359,"the test that we shall use, called bartlett’s test, is based on a statistic whose sampling distribution provides exact critical values when the sample sizes are equal. these critical values for equal sample sizes can also be used to yield highly accurate approximations to the critical values for unequal sample sizes."
3097,1,"['sample', 'sample variances', 'samples', 'variances']", Tests for the Equality of Several Variances,seg_359,"first, we compute the k sample variances s21, s22, . . . , s2k from samples of size k n1, n2, . . . , nk, with"
3098,1,"['sample', 'sample variances', 'variances']", Tests for the Equality of Several Variances,seg_359,"ni = n . second, we combine the sample variances to give"
3099,1,['estimate'], Tests for the Equality of Several Variances,seg_359,the pooled estimate
3100,1,"['case', 'distribution', 'random variable', 'variable', 'random', 'significance']", Tests for the Equality of Several Variances,seg_359,"is a value of a random variable b having the bartlett distribution. for the special case where n1 = n2 = · · · = nk = n, we reject h0 at the α-level of significance if"
3101,1,"['critical values', 'table', 'distribution', 'tail', 'critical value']", Tests for the Equality of Several Variances,seg_359,"where bk(α;n) is the critical value leaving an area of size α in the left tail of the bartlett distribution. table a.10 gives the critical values, bk(α;n), for α = 0.01 and 0.05; k = 2, 3, . . . , 10; and selected values of n from 3 to 100."
3102,1,"['sample', 'significance', 'null hypothesis', 'hypothesis']", Tests for the Equality of Several Variances,seg_359,"when the sample sizes are unequal, the null hypothesis is rejected at the α-level of significance if"
3103,1,"['sample', 'table']", Tests for the Equality of Several Variances,seg_359,"as before, all the bk(α;ni) for sample sizes n1, n2, . . . , nk are obtained from table a.10."
3104,1,"['variances', 'population', 'level', 'significance', 'test', 'level of significance', 'hypothesis']", Tests for the Equality of Several Variances,seg_359,example 13.3: use bartlett’s test to test the hypothesis at the 0.01 level of significance that the population variances of the four drug groups of example 13.2 are equal.
3105,1,['hypotheses'], Tests for the Equality of Several Variances,seg_359,solution : we have the hypotheses
3106,1,['variances'], Tests for the Equality of Several Variances,seg_359,"h1: the variances are not equal,"
3107,0,[], Tests for the Equality of Several Variances,seg_359,computations: first compute
3108,1,"['variances', 'population', 'hypothesis']", Tests for the Equality of Several Variances,seg_359,"decision: do not reject the hypothesis, and conclude that the population variances of the four drug groups are not significantly different."
3109,1,"['sample', 'test', 'method', 'homogeneity']", Tests for the Equality of Several Variances,seg_359,"although bartlett’s test is most often used for testing of homogeneity of variances, other methods are available. a method due to cochran provides a computationally simple procedure, but it is restricted to situations in which the sample"
3110,1,['experiments'], Tests for the Equality of Several Variances,seg_359,518 chapter 13 one-factor experiments: general
3111,1,"['test', 'variance', 'statistic']", Tests for the Equality of Several Variances,seg_359,sizes are equal. cochran’s test is particularly useful for detecting if one variance is much larger than the others. the statistic that is used is
3112,1,"['table', 'hypothesis', 'variances']", Tests for the Equality of Several Variances,seg_359,"and the hypothesis of equality of variances is rejected if g > gα, where the value of gα is obtained from table a.11."
3113,1,"['table', 'data', 'analysis of variance', 'variance', 'test', 'variances']", Tests for the Equality of Several Variances,seg_359,"to illustrate cochran’s test, let us refer again to the data of table 13.1 on moisture absorption in concrete aggregates. were we justified in assuming equal variances when we performed the analysis of variance in example 13.1? we find that"
3114,1,"['table', 'variances']", Tests for the Equality of Several Variances,seg_359,"which does not exceed the table value g0.05 = 0.5065. hence, we conclude that the assumption of equal variances is reasonable."
3115,1,"['experiment', 'treatment', 'analysis of variance', 'test', 'variance', 'null hypothesis', 'hypothesis']", SingleDegreeofFreedom Comparisons,seg_363,"the analysis of variance in a one-way classification, or a one-factor experiment, as it is often called, merely indicates whether or not the hypothesis of equal treatment means can be rejected. usually, an experimenter would prefer his or her analysis to probe deeper. for instance, in example 13.1, by rejecting the null hypothesis we concluded that the means are not all equal, but we still do not know where the differences exist among the aggregates. the engineer might have the feeling a priori that aggregates 1 and 2 should have similar absorption properties and that the same is true for aggregates 3 and 5. however, it is of interest to study the difference between the two groups. it would seem, then, appropriate to test the hypothesis"
3116,1,"['linear', 'coefficients', 'population', 'function', 'hypothesis']", SingleDegreeofFreedom Comparisons,seg_363,we notice that the hypothesis is a linear function of the population means where the coefficients sum to zero.
3117,1,"['function', 'linear']", SingleDegreeofFreedom Comparisons,seg_363,definition 13.1: any linear function of the form
3118,1,"['contrast', 'treatment']", SingleDegreeofFreedom Comparisons,seg_363,"k where ∑ ci = 0, is called a comparison or contrast in the treatment means."
3119,1,"['treatment', 'contrasts', 'multiple comparisons', 'significance', 'hypothesis']", SingleDegreeofFreedom Comparisons,seg_363,"the experimenter can often make multiple comparisons by testing the significance of contrasts in the treatment means, that is, by testing a hypothesis of the following type:"
3120,0,[], SingleDegreeofFreedom Comparisons,seg_363,hypothesis for a k
3121,1,"['sample', 'contrast', 'sample means', 'test']", SingleDegreeofFreedom Comparisons,seg_363,"the test is conducted by first computing a similar contrast in the sample means,"
3122,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random variable', 'variable', 'normal', 'random', 'normal random variable', 'variances']", SingleDegreeofFreedom Comparisons,seg_363,"since ȳ1., ȳ2., . . . , ȳk. are independent random variables having normal distributions with means μ1, μ2, . . . , μk and variances σ12/n1, σ22/n2, . . . , σk2/nk, respectively, theorem 7.11 assures us that w is a value of the normal random variable w with"
3123,1,"['variance', 'mean']", SingleDegreeofFreedom Comparisons,seg_363,k k mean μw = ciμi and variance σ2 = σ2 ci2 . ∑ w ∑ ni i=1 i=1
3124,1,['statistic'], SingleDegreeofFreedom Comparisons,seg_363,"therefore, when h0 is true, μw = 0 and, by example 7.5, the statistic"
3125,1,"['random variable', 'variable', 'degree of freedom', 'random']", SingleDegreeofFreedom Comparisons,seg_363,is distributed as a chi-squared random variable with 1 degree of freedom.
3126,1,"['statistic', 'test', 'significance', 'hypothesis']", SingleDegreeofFreedom Comparisons,seg_363,test statistic for our hypothesis is tested at the α-level of significance by computing testing a
3127,1,"['degrees of freedom', 'random variable', 'variable', 'random']", SingleDegreeofFreedom Comparisons,seg_363,here f is a value of the random variable f having the f -distribution with 1 and n − k degrees of freedom.
3128,1,['sample'], SingleDegreeofFreedom Comparisons,seg_363,"when the sample sizes are all equal to n,"
3129,1,"['sum of squares', 'contrast']", SingleDegreeofFreedom Comparisons,seg_363,"the quantity ssw, called the contrast sum of squares, indicates the portion of ssa that is explained by the contrast in question."
3130,1,"['sum of squares', 'test', 'hypothesis']", SingleDegreeofFreedom Comparisons,seg_363,this sum of squares will be used to test the hypothesis that
3131,1,"['independent', 'contrasts', 'test']", SingleDegreeofFreedom Comparisons,seg_363,"it is often of interest to test multiple contrasts, particularly contrasts that are linearly independent or orthogonal. as a result, we need the following definition:"
3132,1,['contrasts'], SingleDegreeofFreedom Comparisons,seg_363,definition 13.2: the two contrasts
3133,1,"['degrees of freedom', 'contrast', 'independent', 'treatment', 'sum of squares', 'degree of freedom']", SingleDegreeofFreedom Comparisons,seg_363,"if ω1 and ω2 are orthogonal, then the quantities ssw1 and ssw2 are components of ssa, each with a single degree of freedom. the treatment sum of squares with k − 1 degrees of freedom can be partitioned into at most k − 1 independent single-degree-of-freedom contrast sums of squares satisfying the identity"
3134,1,['contrasts'], SingleDegreeofFreedom Comparisons,seg_363,if the contrasts are orthogonal to each other.
3135,1,"['contrast', 'orthogonal contrasts', 'contrasts', 'sum of squares']", SingleDegreeofFreedom Comparisons,seg_363,"example 13.4: referring to example 13.1, find the contrast sum of squares corresponding to the orthogonal contrasts"
3136,1,"['tests', 'contrast', 'independent', 'case', 'set', 'significance']", SingleDegreeofFreedom Comparisons,seg_363,"and carry out appropriate tests of significance. in this case, it is of interest a priori to compare the two groups (1, 2) and (3, 5). an important and independent contrast is the comparison between the set of aggregates (1, 2, 3, 5) and aggregate 4."
3137,1,['contrasts'], SingleDegreeofFreedom Comparisons,seg_363,"solution : it is obvious that the two contrasts are orthogonal, since"
3138,1,"['contrast', 'contrasts']", SingleDegreeofFreedom Comparisons,seg_363,"the second contrast indicates a comparison between aggregates (1, 2, 3, and 5) and aggregate 4. we can write two additional contrasts orthogonal to the first two, namely"
3139,1,"['data', 'table']", SingleDegreeofFreedom Comparisons,seg_363,"from the data of table 13.1, we have"
3140,1,"['contrast', 'table', 'sum of squares', 'hypothesis']", SingleDegreeofFreedom Comparisons,seg_363,"a more extensive analysis-of-variance table is shown in table 13.5. we note that the two contrast sums of squares account for nearly all the aggregate sum of squares. there is a significant difference between aggregates in their absorption properties, and the contrast ω1 is marginally significant. however, the f-value of 14.12 for ω2 is highly significant, and the hypothesis"
3141,1,"['orthogonal contrasts', 'contrasts', 'analysis of variance', 'variance']", SingleDegreeofFreedom Comparisons,seg_363,table 13.5: analysis of variance using orthogonal contrasts
3142,1,"['mean', 'variation']", SingleDegreeofFreedom Comparisons,seg_363,source of sum of degrees of mean computed variation squares freedom square f
3143,1,['error'], SingleDegreeofFreedom Comparisons,seg_363,"3 2 error 124,021 25 4961 total 209,377 29"
3144,1,"['independent', 'treatment', 'contrasts', 'case', 'variation', 'significance', 'test']", SingleDegreeofFreedom Comparisons,seg_363,"orthogonal contrasts allow the practitioner to partition the treatment variation into independent components. normally, the experimenter would have certain contrasts that were of interest to him or her. such was the case in our example, where a priori considerations suggested that aggregates (1, 2) and (3, 5) constituted distinct groups with different absorption properties, a postulation that was not strongly supported by the significance test. however, the second comparison supported the conclusion that aggregate 4 seemed to “stand out” from the rest. in this case, the complete partitioning of ssa was not necessary, since two of the four possible independent comparisons accounted for a majority of the variation in treatments."
3145,1,"['glm', 'contrasts', 'sum of squares', 'set']", SingleDegreeofFreedom Comparisons,seg_363,"figure 13.4 shows a sas glm procedure that displays a complete set of orthogonal contrasts. note that the sums of squares for the four contrasts add to the aggregate sum of squares. also, note that the latter two contrasts (1 versus 2, 3 versus 5) reveal insignificant comparisons."
3146,1,"['homogeneity', 'set', 'analysis of variance', 'variance', 'population', 'null hypothesis', 'hypothesis']", Multiple Comparisons,seg_365,"the analysis of variance is a powerful procedure for testing the homogeneity of a set of means. however, if we reject the null hypothesis and accept the stated alternative—that the means are not all equal—we still do not know which of the population means are equal and which are different."
3147,1,"['glm', 'model', 'dependent variable', 'dependent', 'variable', 'mean square', 'mean', 'error']", Multiple Comparisons,seg_365,the glm procedure dependent variable: moisture sum of source df squares mean square f value pr > f model 4 85356.4667 21339.1167 4.30 0.0088 error 25 124020.3333 4960.8133 corrected total 29 209376.8000
3148,1,"['mse', 'mean']", Multiple Comparisons,seg_365,r-square coeff var root mse moisture mean 0.407669 12.53703 70.43304 561.8000
3149,1,"['mean square', 'mean']", Multiple Comparisons,seg_365,source df type i ss mean square f value pr > f aggregate 4 85356.46667 21339.11667 4.30 0.0088
3150,1,"['mean square', 'mean']", Multiple Comparisons,seg_365,source df type iii ss mean square f value pr > f aggregate 4 85356.46667 21339.11667 4.30 0.0088
3151,1,"['contrast', 'mean square', 'mean']", Multiple Comparisons,seg_365,"contrast df contrast ss mean square f value pr > f (1,2,3,5) vs. 4 1 70035.00833 70035.00833 14.12 0.0009 (1,2) vs. (3,5) 1 14553.37500 14553.37500 2.93 0.0991 1 vs. 2 1 768.00000 768.00000 0.15 0.6973 3 vs. 5 1 0.08333 0.08333 0.00 0.9968"
3152,1,"['orthogonal contrasts', 'contrasts', 'set']", Multiple Comparisons,seg_365,figure 13.4: a set of orthogonal contrasts
3153,1,"['contrast', 'test', 'paired']", Multiple Comparisons,seg_365,"often it is of interest to make several (perhaps all possible) paired comparisons among the treatments. actually, a paired comparison may be viewed as a simple contrast, namely, a test of"
3154,1,"['aggregate data', 'table', 'contrasts', 'data', 'test', 'paired']", Multiple Comparisons,seg_365,"for all i = j. making all possible paired comparisons among the means can be very beneficial when particular complex contrasts are not known a priori. for example, in the aggregate data of table 13.1, suppose that we wish to test"
3155,1,"['interval', 'confidence', 'test', 'confidence interval']", Multiple Comparisons,seg_365,"the test is developed through use of an f, t, or confidence interval approach. using t, we have"
3156,1,"['sample size', 'sample', 'error', 'treatment', 'case', 'mean square', 'mean', 'mean square error']", Multiple Comparisons,seg_365,"where s is the square root of the mean square error and n = 6 is the sample size per treatment. in this case,"
3157,1,['degrees of freedom'], Multiple Comparisons,seg_365,"the p-value for the t-test with 25 degrees of freedom is 0.17. thus, there is not sufficient evidence to reject h0."
3158,1,"['degrees of freedom', 'contrast', 'estimate', 'samples', 'mean', 'mean squared error', 'test', 'error']", Multiple Comparisons,seg_365,"in the foregoing, we displayed the use of a pooled t-test along the lines of that discussed in chapter 10. the pooled estimate was taken from the mean squared error in order to enjoy the degrees of freedom that are pooled across all five samples. in addition, we have tested a contrast. the reader should note that if the t-value is squared, the result is exactly of the same form as the value of f for a test on a contrast, discussed in the preceding section. in fact,"
3159,1,"['confidence', 'interval', 'confidence interval', 'paired']", Multiple Comparisons,seg_365,"it is straightforward to solve the same problem of a paired comparison (or a contrast) using a confidence interval approach. clearly, if we compute a 100(1− α)% confidence interval on μ1 − μ5, we have"
3160,1,"['degrees of freedom', 'contrast', 'confidence intervals', 'interval', 'intervals', 'hypothesis testing', 'confidence', 'test', 'confidence interval', 'hypothesis']", Multiple Comparisons,seg_365,"where tα/2 is the upper 100(1 − α/2)% point of a t-distribution with 25 degrees of freedom (degrees of freedom coming from s2). this straightforward connection between hypothesis testing and confidence intervals should be obvious from discussions in chapters 9 and 10. the test of the simple contrast μ1 − μ5 involves no more than observing whether or not the confidence interval above covers zero. substituting the numbers, we have as the 95% confidence interval"
3161,1,"['contrast', 'interval']", Multiple Comparisons,seg_365,"thus, since the interval covers zero, the contrast is not significant. in other words, we do not find a significant difference between the means of aggregates 1 and 5."
3162,1,"['rate', 'independent', 'case', 'hypotheses', 'probability', 'type i error', 'error', 'paired']", Multiple Comparisons,seg_365,"serious difficulties occur when the analyst attempts to make many or all possible paired comparisons. for the case of k means, there will be, of course, r = k(k − 1)/2 possible paired comparisons. assuming independent comparisons, the experiment-wise error rate or family error rate (i.e., the probability of false rejection of at least one of the hypotheses) is given by 1 − (1 − α)r, where α is the selected probability of a type i error for a specific comparison. clearly, this measure of experiment-wise type i error can be quite large. for example, even"
3163,1,"['rate', 'case']", Multiple Comparisons,seg_365,"if there are only 6 comparisons, say, in the case of 4 means, and α = 0.05, the experiment-wise rate is"
3164,1,"['contrast', 'confidence intervals', 'interval', 'case', 'intervals', 'confidence', 'confidence interval', 'paired']", Multiple Comparisons,seg_365,"when many paired comparisons are being tested, there is usually a need to make the effective contrast on a single comparison more conservative. that is, with the confidence interval approach, the confidence intervals would be much wider than the ±tα/2s√2/n used for the case where only a single comparison is being made."
3165,1,"['degrees of freedom', 'confidence intervals', 'range', 'function', 'paired', 'rate', 'studentized range distribution', 'intervals', 'standard', 'confidence', 'type i error', 'error', 'percentile', 'distribution', 'percentage', 'method', 'table']", Multiple Comparisons,seg_365,"there are several standard methods for making paired comparisons that sustain the credibility of the type i error rate. we shall discuss and illustrate two of them here. the first one, called tukey’s procedure, allows formation of simultaneous 100(1− α)% confidence intervals for all paired comparisons. the method is based on the studentized range distribution. the appropriate percentile point is a function of α, k, and v = degrees of freedom for s2. a list of upper percentage points for α = 0.05 is shown in table a.12. the method of paired comparisons by tukey involves finding a significant difference between means i and j (i = j) if |ȳi. − ȳj.|"
3166,1,"['sample', 'degrees of freedom', 'error', 'table', 'design', 'treatment', 'sample means', 'mean square', 'completely randomized design', 'mean', 'mean square error']", Multiple Comparisons,seg_365,"tukey’s procedure is easily illustrated. consider a hypothetical example where we have 6 treatments in a one-factor completely randomized design, with 5 observations taken per treatment. suppose that the mean square error taken from the analysis-of-variance table is s2 = 2.45 (24 degrees of freedom). the sample means are in ascending order:"
3167,0,[], Multiple Comparisons,seg_365,"as a result, the following represent means found to be significantly different using tukey’s procedure:"
3168,1,"['confidence intervals', 'multiple comparisons', 'intervals', 'confidence']", Multiple Comparisons,seg_365,we briefly alluded to the concept of simultaneous confidence intervals being employed for tukey’s procedure. the reader will gain a useful insight into the notion of multiple comparisons if he or she gains an understanding of what is meant by simultaneous confidence intervals.
3169,1,"['interval', 'mean', 'probability', 'confidence', 'confidence interval']", Multiple Comparisons,seg_365,"in chapter 9, we saw that if we compute a 95% confidence interval on, say, a mean μ, then the probability that the interval covers the true mean μ is 0.95."
3170,1,"['confidence intervals', 'case', 'multiple comparisons', 'probability', 'null hypothesis', 'rate', 'intervals', 'confidence', 'error', 'level', 'hypothesis', 'independent', 'confidence level']", Multiple Comparisons,seg_365,"however, as we have discussed, for the case of multiple comparisons, the effective probability of interest is tied to the experiment-wise error rate, and it should be emphasized that the confidence intervals of the type ȳi. − ȳj. ± q(α, k, v)s√1/n are not independent since they all involve s and many involve the use of the same averages, the ȳi.. despite the difficulties, if we use q(0.05, k, v), the simultaneous confidence level is controlled at 95%. the same holds for q(0.01, k, v); namely, the confidence level is controlled at 99%. in the case of α = 0.05, there is a probability of 0.05 that at least one pair of measures will be falsely found to be different (false rejection of at least one null hypothesis). in the α = 0.01 case, the corresponding probability will be 0.01."
3171,1,"['sample', 'range', 'sample means', 'test']", Multiple Comparisons,seg_365,"the second procedure we shall discuss is called duncan’s procedure or duncan’s multiple-range test. this procedure is also based on the general notion of studentized range. the range of any subset of p sample means must exceed a certain value before any of the p means are found to be different. this value is called the least significant range for the p means and is denoted by rp, where"
3172,1,"['degrees of freedom', 'level', 'error', 'table', 'range', 'mean square', 'mean', 'significance', 'level of significance', 'mean square error']", Multiple Comparisons,seg_365,"the values of the quantity rp, called the least significant studentized range, depend on the desired level of significance and the number of degrees of freedom of the mean square error. these values may be obtained from table a.13 for p = 2, 3, . . . , 10 means."
3173,1,"['treatment', 'results', 'observations', 'test']", Multiple Comparisons,seg_365,"to illustrate the multiple-range test procedure, let us consider the hypothetical example where 6 treatments are compared, with 5 observations per treatment. this is the same example used to illustrate tukey’s test. we obtain rp by multiplying each rp by 0.70. the results of these computations are summarized as follows:"
3174,0,[], Multiple Comparisons,seg_365,"comparing these least significant ranges with the differences in ordered means, we arrive at the following conclusions:"
3175,0,[], Multiple Comparisons,seg_365,difference is significant.
3176,1,['homogeneous'], Multiple Comparisons,seg_365,"differences significant except for μ4−μ3. therefore, μ3, μ4, and μ6 constitute a subset of homogeneous means."
3177,0,[], Multiple Comparisons,seg_365,μ3 and μ1 are not significantly different.
3178,0,[], Multiple Comparisons,seg_365,"it is customary to summarize the conclusions above by drawing a line under any subsets of adjacent means that are not significantly different. thus, we have"
3179,1,"['results', 'case']", Multiple Comparisons,seg_365,"it is clear that in this case the results from tukey’s and duncan’s procedures are very similar. tukey’s procedure did not detect a difference between 2 and 5, whereas duncan’s did."
3180,1,"['experimental', 'experiment', 'table', 'treatment', 'data', 'joint', 'control', 'mean', 'level', 'significance', 'test', 'significance level']", Multiple Comparisons,seg_365,"in many scientific and engineering problems, one is not interested in drawing inferences regarding all possible comparisons among the treatment means of the type μi−μj . rather, the experiment often dictates the need to simultaneously compare each treatment with a control. a test procedure developed by c. w. dunnett determines significant differences between each treatment mean and the control, at a single joint significance level α. to illustrate dunnett’s procedure, let us consider the experimental data of table 13.6 for a one-way classification where the effect of three catalysts on the yield of a reaction is being studied. a fourth treatment, no catalyst, is used as a control."
3181,0,[], Multiple Comparisons,seg_365,table 13.6: yield of reaction
3182,0,[], Multiple Comparisons,seg_365,control catalyst 1 catalyst 2 catalyst 3
3183,1,"['test', 'hypotheses']", Multiple Comparisons,seg_365,"in general, we wish to test the k hypotheses"
3184,1,"['experimental', 'treatment', 'observations', 'hypotheses', 'measurements', 'mean', 'population', 'control', 'test']", Multiple Comparisons,seg_365,"where μ0 represents the mean yield for the population of measurements in which the control is used. the usual analysis-of-variance assumptions, as outlined in section 13.3, are expected to remain valid. to test the null hypotheses specified by h0 against two-sided alternatives for an experimental situation in which there are k treatments, excluding the control, and n observations per treatment, we first calculate the values"
3185,1,"['sample', 'error', 'mean square error', 'mean square', 'analysis of variance', 'mean', 'sample variance', 'variance', 'critical region']", Multiple Comparisons,seg_365,"the sample variance s2 is obtained, as before, from the mean square error in the analysis of variance. now, the critical region for rejecting h0, at the α-level of"
3186,1,['inequality'], Multiple Comparisons,seg_365,"significance, is established by the inequality"
3187,1,"['degrees of freedom', 'mean square error', 'table', 'mean square', 'mean', 'test', 'error']", Multiple Comparisons,seg_365,"where v is the number of degrees of freedom for the mean square error. the values of the quantity dα/2(k, v) for a two-tailed test are given in table a.14 for α = 0.05 and α = 0.01 for various values of k and v."
3188,1,"['table', 'data', 'joint', 'hypotheses', 'level', 'significance', 'test', 'significance level']", Multiple Comparisons,seg_365,"example 13.5: for the data of table 13.6, test hypotheses comparing each catalyst with the control, using two-sided alternatives. choose α = 0.05 as the joint significance level."
3189,1,"['degrees of freedom', 'mean square error', 'table', 'mean square', 'mean', 'error']", Multiple Comparisons,seg_365,"solution : the mean square error with 16 degrees of freedom is obtained from the analysisof-variance table, using all k + 1 treatments. the mean square error is given by"
3190,1,"['control', 'critical value', 'mean', 'table']", Multiple Comparisons,seg_365,"from table a.14 the critical value for α = 0.05 is found to be d0.025(3, 16) = 2.59. since |d1| < 2.59 and |d3| < 2.59, we conclude that only the mean yield for catalyst 2 is significantly different from the mean yield of the reaction using the control."
3191,1,"['critical values', 'table', 'level', 'control', 'test']", Multiple Comparisons,seg_365,"many practical applications dictate the need for a one-tailed test for comparing treatments with a control. certainly, when a pharmacologist is concerned with the effect of various dosages of a drug on cholesterol level and his control is zero dosage, it is of interest to determine if each dosage produces a significantly larger reduction than the control. table a.15 shows the critical values of dα(k, v) for one-sided alternatives."
3192,1,"['experimental error', 'experimental', 'homogeneous', 'sets', 'error']", Comparing a Set of Treatments in Blocks,seg_369,"in section 13.2, we discussed the idea of blocking, that is, isolating sets of experimental units that are reasonably homogeneous and randomly assigning treatments to these units. this is an extension of the “pairing” concept discussed in chapters 9 and 10, and it is done to reduce experimental error, since the units in a block have more common characteristics than units in different blocks."
3193,1,"['factor', 'design', 'case', 'experiment', 'samples', 'sets', 'mean', 'error', 'experimental', 'experimental units', 'concentration', 'randomization', 'completely randomized design']", Comparing a Set of Treatments in Blocks,seg_369,"the reader should not view blocks as a second factor, although this is a tempting way of visualizing the design. in fact, the main factor (treatments) still carries the major thrust of the experiment. experimental units are still the source of error, just as in the completely randomized design. we merely treat sets of these units more systematically when blocking is accomplished. in this way, we say there are restrictions in randomization. before we turn to a discussion of blocking, let us look at two examples of a completely randomized design. the first example is a chemical experiment designed to determine if there is a difference in mean reaction yield among four catalysts. samples of materials to be tested are drawn from the same batches of raw materials, while other conditions, such as temperature and concentration of reactants, are held constant. in this case, the time of day for the experimental runs might represent the experimental units, and if the experimenter believed that there could possibly be a slight time effect, he or she would randomize the assignment of the catalysts to the runs to counteract the possible trend. as a second example of such a design, consider an experiment to compare four methods"
3194,1,"['sample', 'experimental', 'method', 'experiment', 'variation', 'sampling', 'samples', 'measurements', 'random', 'experimental units', 'process', 'measuring', 'error']", Comparing a Set of Treatments in Blocks,seg_369,"of measuring a particular physical property of a fluid substance. suppose the sampling process is destructive; that is, once a sample of the substance has been measured by one method, it cannot be measured again by any of the other methods. if it is decided that five measurements are to be taken for each method, then 20 samples of the material are selected from a large batch at random and are used in the experiment to compare the four measuring methods. the experimental units are the randomly selected samples. any variation from sample to sample will appear in the error variation, as measured by s2 in the analysis."
3195,1,"['experimental error', 'experimental', 'sensitivity', 'treatment', 'homogeneous', 'variation', 'random', 'experimental units', 'test', 'error']", Comparing a Set of Treatments in Blocks,seg_369,"if the variation due to heterogeneity in experimental units is so large that the sensitivity with which treatment differences are detected is reduced due to an inflated value of s2, a better plan might be to “block off” variation due to these units and thus reduce the extraneous variation to that accounted for by smaller or more homogeneous blocks. for example, suppose that in the previous catalyst illustration it is known a priori that there definitely is a significant day-to-day effect on the yield and that we can measure the yield for four catalysts on a given day. rather than assign the four catalysts to the 20 test runs completely at random, we choose, say, five days and run each of the four catalysts on each day, randomly assigning the catalysts to the runs within days. in this way, the dayto-day variation is removed from the analysis, and consequently the experimental error, which still includes any time trend within days, more accurately represents chance variation. each day is referred to as a block."
3196,1,"['experimental', 'design', 'treatment', 'replication']", Comparing a Set of Treatments in Blocks,seg_369,"the most straightforward of the randomized block designs is one in which we randomly assign each treatment once to every block. such an experimental layout is called a randomized complete block (rcb) design, each block constituting a single replication of the treatments."
3197,1,"['design', 'measurements', 'randomized complete block design']", Randomized Complete Block Designs,seg_371,a typical layout for the randomized complete block design using 3 measurements in 4 blocks is as follows:
3198,0,[], Randomized Complete Block Designs,seg_371,block 1 block 2 block 3 block 4
3199,1,"['experiment', 'data', 'random']", Randomized Complete Block Designs,seg_371,"the t’s denote the assignment to blocks of each of the 3 treatments. of course, the true allocation of treatments to units within blocks is done at random. once the experiment has been completed, the data can be recorded in the following 3 × 4 array:"
3200,0,[], Randomized Complete Block Designs,seg_371,treatment block: 1 2 3 4
3201,1,"['treatment', 'response']", Randomized Complete Block Designs,seg_371,"where y11 represents the response obtained by using treatment 1 in block l, y12 represents the response obtained by using treatment 1 in block 2, . . . , and y34 represents the response obtained by using treatment 3 in block 4."
3202,1,"['normal distributions', 'random variables', 'independent', 'table', 'variables', 'case', 'data', 'normal', 'independent random variables', 'mean', 'random', 'variance', 'distributions']", Randomized Complete Block Designs,seg_371,"let us now generalize and consider the case of k treatments assigned to b blocks. the data may be summarized as shown in the k × b rectangular array of table 13.7. it will be assumed that the yij , i = 1, 2, . . . , k and j = 1, 2, . . . , b, are values of independent random variables having normal distributions with mean μij and common variance σ2."
3203,1,['design'], Randomized Complete Block Designs,seg_371,table 13.7: k × b array for the rcb design
3204,1,['mean'], Randomized Complete Block Designs,seg_371,treatment 1 2 · · · j · · · b total mean
3205,1,['mean'], Randomized Complete Block Designs,seg_371,total t.1 t.2 · · · t.j · · · t.b t.. mean ȳ.1 ȳ.2 · · · ȳ.j · · · ȳ.b ȳ..
3206,1,"['treatment', 'population', 'average']", Randomized Complete Block Designs,seg_371,"let μi. represent the average (rather than the total) of the b population means for the ith treatment. that is,"
3207,1,"['average', 'population']", Randomized Complete Block Designs,seg_371,"similarly, the average of the population means for the jth block, μ.j , is defined by"
3208,1,"['average', 'population']", Randomized Complete Block Designs,seg_371,"and the average of the bk population means, μ, is defined by"
3209,1,"['observations', 'variation', 'test']", Randomized Complete Block Designs,seg_371,"to determine if part of the variation in our observations is due to differences among the treatments, we consider the following test:"
3210,1,['treatment'], Randomized Complete Block Designs,seg_371,"hypothesis of h0: μ1. = μ2. = · · ·μk. = μ, equal treatment means h1: the μi. are not all equal."
3211,1,['observation'], Randomized Complete Block Designs,seg_371,each observation may be written in the form
3212,1,"['deviation', 'population mean', 'mean', 'population']", Randomized Complete Block Designs,seg_371,where ij measures the deviation of the observed value yij from the population mean μij . the preferred form of this equation is obtained by substituting
3213,1,['treatment'], Randomized Complete Block Designs,seg_371,"where αi is, as before, the effect of the ith treatment and βj is the effect of the jth block. it is assumed that the treatment and block effects are additive. hence, we may write"
3214,1,"['model', 'variation']", Randomized Complete Block Designs,seg_371,"notice that the model resembles that of the one-way classification, the essential difference being the introduction of the block effect βj . the basic concept is much like that of the one-way classification except that we must account in the analysis for the additional effect due to blocks, since we are now systematically controlling variation in two directions. if we now impose the restrictions that"
3215,1,"['treatment', 'null hypothesis', 'hypothesis']", Randomized Complete Block Designs,seg_371,"the null hypothesis that the k treatment means μi· are equal, and therefore equal to μ, is now equivalent to testing the hypothesis"
3216,0,[], Randomized Complete Block Designs,seg_371,h1: at least one of the αi is not equal to zero.
3217,1,"['population variance', 'independent', 'estimates', 'variance', 'population', 'tests']", Randomized Complete Block Designs,seg_371,each of the tests on treatments will be based on a comparison of independent estimates of the common population variance σ2. these estimates will be obtained
3218,1,"['sum of squares', 'data']", Randomized Complete Block Designs,seg_371,by splitting the total sum of squares of our data into three components by means of the following identity.
3219,0,[], Randomized Complete Block Designs,seg_371,sum-of-squares identity
3220,0,[], Randomized Complete Block Designs,seg_371,the proof is left to the reader.
3221,0,[], Randomized Complete Block Designs,seg_371,the sum-of-squares identity may be presented symbolically by the equation
3222,1,"['functions', 'random variables', 'independent', 'variables', 'treatment', 'independent random variables', 'random', 'expected values', 'error']", Randomized Complete Block Designs,seg_371,"following the procedure outlined in theorem 13.2, where we interpreted the sums of squares as functions of the independent random variables y11, y12, . . . , ykb, we can show that the expected values of the treatment, block, and error sums of squares are given by"
3223,1,"['treatment', 'mean square', 'mean', 'case']", Randomized Complete Block Designs,seg_371,"as in the case of the one-factor problem, we have the treatment mean square"
3224,1,"['treatment', 'unbiased', 'estimate']", Randomized Complete Block Designs,seg_371,"if the treatment effects α1 = α2 = · · · = αk = 0, s12 is an unbiased estimate of σ2. however, if the treatment effects are not all zero, we have the following:"
3225,1,['mean'], Randomized Complete Block Designs,seg_371,treatment mean e = σ2 + ∑αi2 i=1
3226,1,"['degrees of freedom', 'case', 'estimate']", Randomized Complete Block Designs,seg_371,"in this case, s12 overestimates σ2. a second estimate of σ2, based on b− 1 degrees of freedom, is"
3227,1,"['unbiased', 'estimate']", Randomized Complete Block Designs,seg_371,"the estimate s22 is an unbiased estimate of σ2 if the block effects β1 = β2 = · · · = βb = 0. if the block effects are not all zero, then"
3228,1,"['degrees of freedom', 'independent', 'estimate']", Randomized Complete Block Designs,seg_371,"and s22 will overestimate σ2. a third estimate of σ2, based on (k−1)(b−1) degrees of freedom and independent of s21 and s22, is"
3229,1,"['null hypothesis', 'unbiased', 'hypothesis']", Randomized Complete Block Designs,seg_371,which is unbiased regardless of the truth or falsity of either null hypothesis.
3230,1,"['degrees of freedom', 'treatment', 'random variable', 'variable', 'random', 'significance', 'test', 'null hypothesis', 'hypothesis']", Randomized Complete Block Designs,seg_371,"to test the null hypothesis that the treatment effects are all equal to zero, we compute the ratio f1 = s12/s2, which is a value of the random variable f1 having an f-distribution with k − 1 and (k − 1)(b − 1) degrees of freedom when the null hypothesis is true. the null hypothesis is rejected at the α-level of significance when"
3231,1,"['degrees of freedom', 'associated']", Randomized Complete Block Designs,seg_371,"in practice, we first compute sst , ssa, and ssb and then, using the sumof-squares identity, obtain sse by subtraction. the degrees of freedom associated with sse are also usually obtained by subtraction; that is,"
3232,1,"['table', 'design', 'randomized complete block design']", Randomized Complete Block Designs,seg_371,the computations in an analysis-of-variance problem for a randomized complete block design may be summarized as shown in table 13.8.
3233,1,"['table', 'experiment', 'random']", Randomized Complete Block Designs,seg_371,"example 13.6: four different machines, m1, m2, m3, and m4, are being considered for the assembling of a particular product. it was decided that six different operators would be used in a randomized block experiment to compare the machines. the machines were assigned in a random order to each operator. the operation of the machines requires physical dexterity, and it was anticipated that there would be a difference among the operators in the speed with which they operated the machines. the amounts of time (in seconds) required to assemble the product are shown in table 13.9."
3234,1,"['rate', 'mean', 'level', 'significance', 'level of significance', 'hypothesis']", Randomized Complete Block Designs,seg_371,"test the hypothesis h0, at the 0.05 level of significance, that the machines perform at the same mean rate of speed."
3235,1,"['design', 'analysis of variance', 'variance', 'randomized complete block design']", Randomized Complete Block Designs,seg_371,table 13.8: analysis of variance for the randomized complete block design
3236,1,"['mean', 'variation']", Randomized Complete Block Designs,seg_371,source of sum of degrees of mean computed variation squares freedom square f
3237,0,[], Randomized Complete Block Designs,seg_371,"table 13.9: time, in seconds, to assemble product"
3238,1,['hypotheses'], Randomized Complete Block Designs,seg_371,solution : the hypotheses are
3239,0,[], Randomized Complete Block Designs,seg_371,h1: at least one of the αi is not equal to zero.
3240,1,"['degrees of freedom', 'rate', 'table', 'analysis of variance', 'mean', 'variance']", Randomized Complete Block Designs,seg_371,"the sum-of-squares formulas shown on page 536 and the degrees of freedom are used to produce the analysis of variance in table 13.10. the value f = 3.34 is significant at p = 0.048. if we use α = 0.05 as at least an approximate yardstick, we conclude that the machines do not perform at the same mean rate of speed."
3241,1,"['variance', 'analysis of variance', 'data', 'table']", Randomized Complete Block Designs,seg_371,table 13.10: analysis of variance for the data of table 13.9
3242,1,"['mean', 'variation']", Randomized Complete Block Designs,seg_371,source of sum of degrees of mean computed variation squares freedom square f
3243,1,['error'], Randomized Complete Block Designs,seg_371,machines 15.93 3 5.31 3.34 operators 42.09 5 8.42 error 23.84 15 1.59
3244,1,['paired'], Randomized Complete Block Designs,seg_371,"in chapter 10, we presented a procedure for comparing means when the observations were paired. the procedure involved “subtracting out” the effect due to the"
3245,1,"['design', 'case', 'homogeneous', 'randomized complete block design']", Randomized Complete Block Designs,seg_371,homogeneous pair and thus working with differences. this is a special case of a randomized complete block design with k = 2 treatments. the n homogeneous units to which the treatments were assigned take on the role of blocks.
3246,1,"['degrees of freedom', 'error', 'experimental', 'experimental units', 'variance', 'test', 'experimental error', 'power of the test', 'estimated', 'treatment', 'homogeneous', 'sensitivity']", Randomized Complete Block Designs,seg_371,"if there is heterogeneity in the experimental units, the experimenter should not be misled into believing that it is always advantageous to reduce the experimental error through the use of small homogeneous blocks. indeed, there may be instances where it would not be desirable to block. the purpose in reducing the error variance is to increase the sensitivity of the test for detecting differences in the treatment means. this is reflected in the power of the test procedure. (the power of the analysis-of-variance test procedure is discussed more extensively in section 13.11.) the power to detect certain differences among the treatment means increases with a decrease in the error variance. however, the power is also affected by the degrees of freedom with which this variance is estimated, and blocking reduces the degrees of freedom that are available from k(b − 1) for the one-way classification to (k − 1)(b−1). so one could lose power by blocking if there is not a significant reduction in the error variance."
3247,1,"['design', 'model', 'treatment']", Randomized Complete Block Designs,seg_371,another important assumption that is implicit in writing the model for a randomized complete block design is that the treatment and block effects are additive. this is equivalent to stating that
3248,1,"['treatment', 'set', 'mean', 'population', 'experiments', 'average']", Randomized Complete Block Designs,seg_371,"for every value of i, i′, j, and j′. that is, the difference between the population means for blocks j and j′ is the same for every treatment and the difference between the population means for treatments i and i′ is the same for every block. the parallel lines of figure 13.6(a) illustrate a set of mean responses for which the treatment and block effects are additive, whereas the intersecting lines of figure 13.6(b) show a situation in which treatment and block effects are said to interact. referring to example 13.6, if operator 3 is 0.5 second faster on the average than operator 2 when machine 1 is used, then operator 3 will still be 0.5 second faster on the average than operator 2 when machine 2, 3, or 4 is used. in many experiments, the assumption of additivity does not hold and the analysis described in this section leads to erroneous conclusions. suppose, for instance, that operator 3 is 0.5 second faster on the average than operator 2 when machine 1 is used but is 0.2 second slower on the average than operator 2 when machine 2 is used. the operators and machines are now interacting."
3249,1,"['sum of squares', 'probability', 'type ii', 'type ii error', 'data', 'mean', 'error', 'model', 'experimental', 'error sum of squares', 'experimental error', 'variability', 'table', 'treatment', 'interaction', 'mean square', 'variation', 'mean square error']", Randomized Complete Block Designs,seg_371,"an inspection of table 13.9 suggests the possible presence of interaction. this apparent interaction may be real or it may be due to experimental error. the analysis of example 13.6 was based on the assumption that the apparent interaction was due entirely to experimental error. if the total variability of our data was in part due to an interaction effect, this source of variation remained a part of the error sum of squares, causing the mean square error to overestimate σ2 and thereby increasing the probability of committing a type ii error. we have, in fact, assumed an incorrect model. if we let (αβ)ij denote the interaction effect of the ith treatment and the jth block, we can write a more appropriate model in the"
3250,0,[], Randomized Complete Block Designs,seg_371,n n a a e e block 1 block 1 m m n n o o i i t block 2 t a a l l up pu block 2 o o p p t1 t2 t3 t1 t2 t3 treatments treatments
3251,1,"['results', 'population']", Randomized Complete Block Designs,seg_371,figure 13.6: population means for (a) additive results and (b) interacting effects.
3252,0,[], Randomized Complete Block Designs,seg_371,on which we impose the additional restrictions
3253,0,[], Randomized Complete Block Designs,seg_371,we can now readily verify that
3254,1,"['design', 'estimate', 'biased', 'cases', 'mean', 'error', 'experimental', 'test', 'unbiased', 'independent', 'interaction', 'mean square', 'mean square error']", Randomized Complete Block Designs,seg_371,"thus, the mean square error is seen to be a biased estimate of σ2 when existing interaction has been ignored. it would seem necessary at this point to arrive at a procedure for the detection of interaction for cases where there is suspicion that it exists. such a procedure requires the availability of an unbiased and independent estimate of σ2. unfortunately, the randomized block design does not lend itself to such a test unless the experimental setup is altered. this subject is discussed extensively in chapter 14."
3255,1,"['sample', 'graphical', 'residual', 'results', 'plots', 'data', 'samples', 'standard', 'residual plots']", Graphical Methods and Model Checking,seg_373,"in several chapters, we make reference to graphical procedures displaying data and analytical results. in early chapters, we used stem-and-leaf and box-and-whisker plots as visuals to aid in summarizing samples. we used similar diagnostics to better understand the data in two sample problems in chapter 10. in chapter 11 we introduced the notion of residual plots to detect violations of standard assumptions. in recent years, much attention in data analysis has centered on graphical"
3256,1,"['observations', 'sample', 'graphical', 'aggregate data', 'sample means', 'data', 'samples', 'mean', 'graphics', 'regression', 'analysis of variance', 'plotting', 'variance', 'plot', 'variability', 'table', 'treatment', 'homogeneous']", Graphical Methods and Model Checking,seg_373,"methods. like regression, analysis of variance lends itself to graphics that aid in summarizing data as well as detecting violations. for example, a simple plotting of the raw observations around each treatment mean can give the analyst a feel for variability between sample means and within samples. figure 13.7 depicts such a plot for the aggregate data of table 13.1. from the appearance of the plot one may even gain a graphical insight into which aggregates (if any) stand out from the others. it is clear that aggregate 4 stands out from the others. aggregates 3 and 5 certainly form a homogeneous group, as do aggregates 1 and 2."
3257,1,"['plot', 'aggregate data', 'table', 'residuals', 'data', 'mean']", Graphical Methods and Model Checking,seg_373,"figure 13.7: plot of data around the mean for the figure 13.8: plot of residuals for five aggregates, aggregate data of table 13.1. using data in table 13.1."
3258,1,"['model', 'residuals', 'case', 'regression', 'analysis of variance', 'variance']", Graphical Methods and Model Checking,seg_373,"as in the case of regression, residuals can be helpful in analysis of variance in providing a diagnostic that may detect violations of assumptions. to form the residuals, we merely need to consider the model of the one-factor problem, namely"
3259,1,"['plot', 'model', 'residuals plotted', 'estimate', 'residual', 'residuals', 'homogeneous', 'variance']", Graphical Methods and Model Checking,seg_373,"it is straightforward to determine that the estimate of μi is ȳi.. hence, the ijth residual is yij − ȳi.. this is easily extendable to the randomized complete block model. it may be instructive to have the residuals plotted for each aggregate in order to gain some insight regarding the homogeneous variance assumption. this plot is shown in figure 13.8."
3260,1,"['graphical', 'plots', 'residuals', 'case', 'homogeneous', 'variance', 'variances']", Graphical Methods and Model Checking,seg_373,"trends in plots such as these may reveal difficulties in some situations, particularly when the violation of a particular assumption is graphic. in the case of figure 13.8, the residuals seem to indicate that the within-treatment variances are reasonably homogeneous apart from aggregate 1. there is some graphical evidence that the variance for aggregate 1 is larger than the rest."
3261,1,"['graphical', 'experimental', 'design', 'randomized complete block design']", Graphical Methods and Model Checking,seg_373,the randomized complete block design is another experimental situation in which graphical displays can make the analyst feel comfortable with an “ideal picture” or
3262,1,"['design', 'model', 'randomized complete block design']", Graphical Methods and Model Checking,seg_373,perhaps highlight difficulties. recall that the model for the randomized complete block design is
3263,0,[], Graphical Methods and Model Checking,seg_373,with the imposed constraints
3264,1,['residual'], Graphical Methods and Model Checking,seg_373,"to determine what indeed constitutes a residual, consider that"
3265,1,"['predicted', 'estimated']", Graphical Methods and Model Checking,seg_373,"and that μ is estimated by ȳ.., μi. is estimated by ȳi., and μ.j is estimated by ȳ.j . as a result, the predicted or fitted value ŷij is given by"
3266,1,"['residual', 'observation']", Graphical Methods and Model Checking,seg_373,"and thus the residual at the (i, j) observation is given by"
3267,1,"['estimate', 'variability', 'sum of squares', 'error sum of squares', 'mean', 'error']", Graphical Methods and Model Checking,seg_373,"note that ŷij , the fitted value, is an estimate of the mean μij . this is consistent with the partitioning of variability given in theorem 13.3, where the error sum of squares is"
3268,1,"['model', 'variability', 'design', 'treatment', 'residuals', 'interaction', 'case', 'homogeneous', 'plotting', 'random', 'variance', 'randomized complete block design']", Graphical Methods and Model Checking,seg_373,"the visual displays in the randomized complete block design involve plotting the residuals separately for each treatment and for each block. the analyst should expect roughly equal variability if the homogeneous variance assumption holds. the reader should recall that in chapter 12 we discussed plotting residuals for the purpose of detecting model misspecification. in the case of the randomized complete block design, the serious model misspecification may be related to our assumption of additivity (i.e., no interaction). if no interaction is present, a random pattern should appear."
3269,1,"['plot', 'residual', 'plots', 'residuals', 'data', 'fitted values', 'variance', 'random', 'residual plots', 'error']", Graphical Methods and Model Checking,seg_373,"consider the data of example 13.6, in which treatments are four machines and blocks are six operators. figures 13.9 and 13.10 give the residual plots for separate treatments and separate blocks. figure 13.11 shows a plot of the residuals against the fitted values. figure 13.9 reveals that the error variance may not be the same for all machines. the same may be true for error variance for each of the six operators. however, two unusually large residuals appear to produce the apparent difficulty. figure 13.11 is a plot of residuals that shows reasonable evidence of random behavior. however, the two large residuals displayed earlier still stand out."
3270,1,"['plot', 'residual', 'residual plot', 'data']", Graphical Methods and Model Checking,seg_373,figure 13.9: residual plot for the four machines for figure 13.10: residual plot for the six operators the data of example 13.6. for the data of example 13.6.
3271,1,"['residuals plotted', 'residuals', 'data', 'fitted values']", Graphical Methods and Model Checking,seg_373,figure 13.11: residuals plotted against fitted values for the data of example 13.6.
3272,1,"['linear', 'model', 'regression model', 'data', 'transformations', 'regression', 'set', 'linear regression', 'exponential', 'multiple linear regression', 'transformation', 'response', 'linear regression model']", Data Transformations in Analysis of Variance,seg_375,"in chapter 11, considerable attention was given to transformation of the response y in situations where a linear regression model was being fit to a set of data. obviously, the same concept applies to multiple linear regression, though it was not discussed in chapter 12. in the regression modeling discussion, emphasis was placed on the transformations of y that would produce a model that fit the data better than the model in which y enters linearly. for example, if the “time” structure is exponential in nature, then a log transformation on y linearizes the"
3273,1,"['success', 'transformed', 'response']", Data Transformations in Analysis of Variance,seg_375,structure and thus more success is anticipated when one uses the transformed response.
3274,1,"['model', 'residual', 'treatment', 'results', 'plots', 'homogeneous', 'data', 'residual plots', 'transform', 'analysis of variance', 'standard', 'transformation', 'variance', 'response', 'anova']", Data Transformations in Analysis of Variance,seg_375,"while the primary purpose for data transformation discussed thus far has been to improve the fit of the model, there are certainly other reasons to transform or reexpress the response y, and many of them are related to assumptions that are being made (i.e., assumptions on which the validity of the analysis depends). one very important assumption in analysis of variance is the homogeneous variance assumption discussed early in section 13.4. we assume a common variance σ2. if the variance differs a great deal from treatment to treatment and we perform the standard anova discussed in this chapter (and future chapters), the results can be substantially flawed. in other words, the analysis of variance is not robust to the assumption of homogeneous variance. as we have discussed thus far, this is the centerpiece of motivation for the residual plots discussed in the previous section and illustrated in figures 13.9, 13.10, and 13.11. these plots allow us to detect nonhomogeneous variance problems. however, what do we do about them? how can we accommodate them?"
3275,1,"['poisson', 'lognormal', 'tests', 'gamma', 'anova', 'failure', 'data', 'distribution', 'normality', 'exponential', 'variance', 'response', 'distributions']", Data Transformations in Analysis of Variance,seg_375,"often, but not always, nonhomogeneous variance in anova is present because of the distribution of the responses. now, of course we assume normality in the response. but there certainly are situations in which tests on means are needed even though the distribution of the response is one of the nonnormal distributions discussed in chapters 5 and 6, such as poisson, lognormal, exponential, or gamma. anova-type problems certainly exist with count data, time to failure data, and so on."
3276,1,"['poisson', 'lognormal', 'exponential distribution', 'case', 'distribution', 'normal', 'exponential', 'mean', 'function', 'transformation', 'variance', 'normal distribution']", Data Transformations in Analysis of Variance,seg_375,"we demonstrated in chapters 5 and 6 that, apart from the normal case, the variance of a distribution will often be a function of the mean, say σi2 = g(μi). for example, in the poisson case var(yi) = μi = σi2 (i.e., the variance is equal to the mean). in the case of the exponential distribution, var(yi) = σi2 = μi2 (i.e., the variance is equal to the square of the mean). for the case of the lognormal, a log transformation produces a normal distribution with constant variance σ2."
3277,1,"['taylor series', 'nonlinear', 'function', 'transformation', 'variance']", Data Transformations in Analysis of Variance,seg_375,the same concepts that we used in chapter 4 to determine the variance of a nonlinear function can be used as an aid to determine the nature of the variance stabilizing transformation g(yi). recall that the first order taylor series expansion
3278,1,['transformation'], Data Transformations in Analysis of Variance,seg_375,. the transformation func-
3279,1,"['transformation', 'variance', 'independent']", Data Transformations in Analysis of Variance,seg_375,"tion g(y) must be independent of μ in order to suffice as the variance stabilizing transformation. from the above,"
3280,1,"['poisson', 'poisson distributed', 'response']", Data Transformations in Analysis of Variance,seg_375,"σ response is poisson distributed, σi = μ1"
3281,1,['transformation'], Data Transformations in Analysis of Variance,seg_375,stabilizing transformation is g(yi) = yi
3282,0,[], Data Transformations in Analysis of Variance,seg_375,1/2. from this illustration and similar ma-
3283,1,"['exponential and gamma distributions', 'gamma', 'gamma distributions', 'exponential', 'distributions']", Data Transformations in Analysis of Variance,seg_375,"nipulation for the exponential and gamma distributions, we have the following."
3284,1,"['variance', 'transformations']", Data Transformations in Analysis of Variance,seg_375,distribution variance stabilizing transformations
3285,1,"['exponential', 'gamma']", Data Transformations in Analysis of Variance,seg_375,poisson g(y) = y1/2 exponential g(y) = ln y gamma g(y) = ln y
3286,1,"['model', 'fixed effects experiments', 'levels', 'experiment', 'treatment', 'experiments', 'fixed effects model', 'response']", Random Effects Models,seg_379,"throughout this chapter, we deal with analysis-of-variance procedures in which the primary goal is to study the effect on some response of certain fixed or predetermined treatments. experiments in which the treatments or treatment levels are preselected by the experimenter as opposed to being chosen randomly are called fixed effects experiments. for the fixed effects model, inferences are made only on those particular treatments used in the experiment."
3287,1,"['experiment', 'treatment', 'population', 'variance']", Random Effects Models,seg_379,"it is often important that the experimenter be able to draw inferences about a population of treatments by means of an experiment in which the treatments used are chosen randomly from the population. for example, a biologist may be interested in whether or not there is significant variance in some physiological characteristic due to animal type. the animal types actually used in the experiment are then chosen randomly and represent the treatment effects. a chemist may be interested in studying the effect of analytical laboratories on the chemical analysis of a substance. she is not concerned with particular laboratories but rather with a large population of laboratories. she might then select a group of laboratories"
3288,1,"['statistical inference', 'results', 'samples', 'statistical', 'variance', 'random']", Random Effects Models,seg_379,at random and allocate samples to each for analysis. the statistical inference would then involve (1) testing whether or not the laboratories contribute a nonzero variance to the analytical results and (2) estimating the variance due to laboratories and the variance within laboratories.
3289,1,"['model', 'random effects model', 'random variable', 'variable', 'random', 'fixed effects model', 'response']", Random Effects Models,seg_379,the one-way random effects model is written like the fixed effects model but with the terms taking on different meanings. the response yij = μ + αi + ij is now a value of the random variable
3290,1,"['fixed effects model', 'model', 'independent', 'normally distributed', 'mean', 'variance']", Random Effects Models,seg_379,"where the ai are independently and normally distributed with mean 0 and variance σα 2 and are independent of the ij . as for the fixed effects model, the ij are also independently and normally distributed with mean 0 and variance σ2. note that"
3291,1,"['experiment', 'random']", Random Effects Models,seg_379,"k for a random effects experiment, the constraint that ∑ αi = 0 no longer applies."
3292,1,"['model', 'random']", Random Effects Models,seg_379,"for the one-way random effects analysis-of-variance model,"
3293,1,"['experiment', 'table', 'mean squares', 'fixed effects experiment', 'mean', 'random', 'expected mean squares']", Random Effects Models,seg_379,"table 13.11 shows the expected mean squares for both a fixed effects and a random effects experiment. the computations for a random effects experiment are carried out in exactly the same way as for a fixed effects experiment. that is, the sum-of-squares, degrees-of-freedom, and mean-square columns in an analysisof-variance table are the same for both models."
3294,1,"['experiment', 'mean squares', 'mean', 'expected mean squares']", Random Effects Models,seg_379,table 13.11: expected mean squares for the one-factor experiment
3295,1,"['expected mean squares', 'mean', 'mean squares']", Random Effects Models,seg_379,source of degrees of mean expected mean squares
3296,1,['random'], Random Effects Models,seg_379,variation freedom squares fixed effects random effects
3297,0,[], Random Effects Models,seg_379,n treatments k − 1 s21 σ2 + ∑αi2 σ2 + nσα
3298,1,['error'], Random Effects Models,seg_379,2 k − 1 i error k(n− 1) s2 σ2 σ2
3299,1,"['model', 'random effects model', 'treatment', 'random', 'hypothesis']", Random Effects Models,seg_379,"for the random effects model, the hypothesis that the treatment effects are all zero is written as follows:"
3300,1,['random'], Random Effects Models,seg_379,hypothesis for a random effects h0: σα
3301,1,"['table', 'variability', 'response', 'hypothesis']", Random Effects Models,seg_379,this hypothesis says that the different treatments contribute nothing to the variability of the response. it is obvious from table 13.11 that s12 and s2 are both
3302,0,[], Random Effects Models,seg_379,estimates of σ2 when h0 is true and that the ratio
3303,1,"['degrees of freedom', 'random variable', 'variable', 'random', 'significance', 'null hypothesis', 'hypothesis']", Random Effects Models,seg_379,is a value of the random variable f having the f-distribution with k−1 and k(n−1) degrees of freedom. the null hypothesis is rejected at the α-level of significance when
3304,1,"['factors', 'variability', 'estimation', 'random variation', 'variation', 'variance', 'random']", Random Effects Models,seg_379,"in many scientific and engineering studies, interest is not centered on the ftest. the scientist knows that the random effect does, indeed, have a significant effect. what is more important is estimation of the various variance components. this produces a ranking in terms of what factors produce the most variability and by how much. in the present context, it may be of interest to quantify how much larger the single-factor variance component is than that produced by chance (random variation)."
3305,1,"['variance', 'estimate']", Random Effects Models,seg_379,table 13.11 can also be used to estimate the variance components σ2 and σα
3306,1,['estimates'], Random Effects Models,seg_379,since s12 estimates σ2 + nσα
3307,1,['estimates'], Random Effects Models,seg_379,"2 and s2 estimates σ2,"
3308,1,"['process', 'estimate', 'table', 'observations', 'data', 'variance']", Random Effects Models,seg_379,"example 13.7: the data in table 13.12 are coded observations on the yield of a chemical process, using five batches of raw material selected randomly. show that the batch variance component is significantly greater than zero and obtain its estimate."
3309,1,['data'], Random Effects Models,seg_379,table 13.12: data for example 13.7
3310,1,['error'], Random Effects Models,seg_379,"solution : the total, batch, and error sums of squares are, respectively,"
3311,1,"['results', 'table']", Random Effects Models,seg_379,"these results, with the remaining computations, are shown in table 13.13."
3312,1,"['variance', 'analysis of variance']", Random Effects Models,seg_379,table 13.13: analysis of variance for example 13.7
3313,1,"['mean', 'variation']", Random Effects Models,seg_379,source of sum of degrees of mean computed variation squares freedom square f
3314,1,['error'], Random Effects Models,seg_379,batches 72.60 4 18.15 4.46 error 122.04 30 4.07
3315,1,"['level', 'variance', 'estimate', 'hypothesis']", Random Effects Models,seg_379,"the f-ratio is significant at the α = 0.05 level, indicating that the hypothesis of a zero batch component is rejected. an estimate of the batch variance component is"
3316,1,"['mse', 'variance', 'estimate']", Random Effects Models,seg_379,"note that while the batch variance component is significantly different from zero, when gauged against the estimate of σ2, namely σ̂2 = mse = 4.07, it appears as if the batch variance component is not appreciably large."
3317,1,"['biased', 'biased estimator', 'set', 'estimator']", Random Effects Models,seg_379,2 is then set to zero. this is a biased estimator. in order to have a better estimator of σα
3318,1,"['method', 'residual', 'estimation', 'likelihood', 'statistical', 'maximum likelihood', 'estimator']", Random Effects Models,seg_379,"2 , a method called restricted (or residual) maximum likelihood (reml) is commonly used (see harville, 1977, in the bibliography). such an estimator can be found in many statistical software packages. the details for this estimation procedure are beyond the scope of this text."
3319,1,"['model', 'random effects model', 'experiment', 'results', 'random']", Random Effects Models,seg_379,"in a randomized complete block experiment where the blocks represent days, it is conceivable that the experimenter would like the results to apply not only to the actual days used in the analysis but to every day in the year. he or she would then select at random the days on which to run the experiment as well as the treatments and use the random effects model"
3320,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random', 'variances']", Random Effects Models,seg_379,"with the ai, bj , and ij being independent random variables with means 0 and variances σα"
3321,1,"['experiment', 'table', 'design', 'mean squares', 'fixed effects experiment', 'mean', 'random', 'expected mean squares', 'randomized complete block design']", Random Effects Models,seg_379,"2 , σβ 2 , and σ2, respectively. the expected mean squares for a random effects randomized complete block design are obtained, using the same procedure as for the one-factor problem, and are presented along with those for a fixed effects experiment in table 13.14."
3322,1,"['model', 'fixed effects model', 'hypothesis']", Random Effects Models,seg_379,again the computations for the individual sums of squares and degrees of freedom are identical to those of the fixed effects model. the hypothesis
3323,0,[], Random Effects Models,seg_379,is carried out by computing
3324,1,"['design', 'mean squares', 'mean', 'expected mean squares', 'randomized complete block design']", Random Effects Models,seg_379,table 13.14: expected mean squares for the randomized complete block design
3325,1,"['expected mean squares', 'mean', 'mean squares']", Random Effects Models,seg_379,source of degrees of mean expected mean squares
3326,1,['random'], Random Effects Models,seg_379,variation freedom squares fixed effects random effects
3327,0,[], Random Effects Models,seg_379,b treatments k − 1 s21 σ2 + ∑αi2 σ2 + bσα
3328,1,['error'], Random Effects Models,seg_379,2 b− 1 j error (k − 1)(b− 1) s2 σ2 σ2
3329,1,"['estimates', 'variance', 'unbiased']", Random Effects Models,seg_379,the unbiased estimates of the variance components are
3330,1,"['table', 'hypotheses', 'mean squares', 'mean', 'variance']", Random Effects Models,seg_379,"tests of hypotheses concerning the various variance components are made by computing the ratios of appropriate mean squares, as indicated in table 13.14, and comparing them with corresponding f-values from table a.6."
3331,1,"['experiment', 'range', 'data', 'samples', 'set', 'data set']", Case Study,seg_381,"case study 13.1: chemical analysis: personnel in the chemistry department of virginia tech were called upon to analyze a data set that was produced to compare 4 different methods of analysis of aluminum in a certain solid igniter mixture. to get a broad range of analytical laboratories involved, 5 laboratories were used in the experiment. these laboratories were selected because they are generally adept in doing these types of analyses. twenty samples of igniter material containing 2.70% aluminum were assigned randomly, 4 to each laboratory, and directions were given on how to carry out the chemical analysis using all 4 methods. the data retrieved are as follows:"
3332,1,['mean'], Case Study,seg_381,method 1 2 3 4 5 mean
3333,1,"['model', 'design', 'plots', 'data', 'population', 'random', 'randomized complete block design', 'additive model']", Case Study,seg_381,the laboratories are not considered as random effects since they were not selected randomly from a larger population of laboratories. the data were analyzed as a randomized complete block design. plots of the data were sought to determine if an additive model of the type
3334,1,"['plot', 'model', 'interaction', 'observation']", Case Study,seg_381,"is appropriate: in other words, a model with additive effects. the randomized block is not appropriate when interaction between laboratories and methods exists. consider the plot shown in figure 13.12. although this plot is a bit difficult to interpret because each point is a single observation, there appears to be no appreciable interaction between methods and laboratories."
3335,1,"['plot', 'interaction', 'case', 'data']", Case Study,seg_381,figure 13.12: interaction plot for data of case study 13.1.
3336,1,"['plot', 'variability', 'probability plot', 'residual', 'plots', 'residuals', 'homogeneous', 'normal', 'probability', 'normal probability plot']", Case Study,seg_381,"residual plots were used as diagnostic indicators regarding the homogeneous variance assumption. figure 13.13 shows a plot of residuals against analytical methods. the variability depicted in the residuals seems to be remarkably homogeneous. for completeness, a normal probability plot of the residuals is shown in figure 13.14."
3337,1,"['plot', 'method', 'probability plot', 'residuals', 'case', 'data', 'normal', 'probability', 'normal probability plot']", Case Study,seg_381,figure 13.14: normal probability plot of residuals figure 13.13: plot of residuals against method for the data of case study 13.1. for the data of case study 13.1.
3338,1,"['glm', 'residual', 'plots', 'homogeneous', 'errors', 'normal', 'variance', 'residual plots']", Case Study,seg_381,the residual plots show no difficulty with either the assumption of normal errors or the assumption of homogeneous variance. sas proc glm was used
3339,1,"['variance', 'analysis of variance']", Case Study,seg_381,to conduct the analysis of variance. figure 13.15 shows the annotated computer printout.
3340,0,[], Case Study,seg_381,the computed fand p-values do indicate a significant difference between analytical methods. this analysis can be followed by a multiple comparison analysis to determine where the differences are among the methods.
3341,1,"['homogeneous', 'normality', 'analysis of variance', 'variance', 'test']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_387,"as in other procedures covered in previous chapters, the analysis of variance is reasonably robust to the normality assumption but less robust to the homogeneous variance assumption. also we note here that bartlett’s test for equal variance is extremely nonrobust to normality."
3342,1,"['design of experiments', 'factor', 'design', 'process', 'graphical', 'experiment', 'results', 'factors', 'analysis of variance', 'variance', 'interaction', 'randomization', 'combinations', 'experiments']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_387,"this chapter is an extremely pivotal chapter in that it is essentially an “entry level” point for important topics such as design of experiments and analysis of variance. chapter 14 will concern itself with the same topics, but the expansion will be to more than one factor, with the total analysis further complicated by the interpretation of interaction among factors. there are times when the role of interaction in a scientific experiment is more important than the role of the main factors (main effects). the presence of interaction results in even more emphasis placed on graphical displays. in chapters 14 and 15, it will be necessary to give more details regarding the randomization process since the number of factor combinations can be large."
3343,0,[], Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_387,this page intentionally left blank
3344,1,"['levels', 'factors', 'experiment', 'factor', 'trial', 'levels of a factor', 'samples', 'vary', 'response', 'percent']", Introduction,seg_391,"consider a situation where it is of interest to study the effects of two factors, a and b, on some response. for example, in a chemical experiment, we would like to vary simultaneously the reaction pressure and reaction time and study the effect of each on the yield. in a biological experiment, it is of interest to study the effects of drying time and temperature on the amount of solids (percent by weight) left in samples of yeast. as in chapter 13, the term factor is used in a general sense to denote any feature of the experiment such as temperature, time, or pressure that may be varied from trial to trial. we define the levels of a factor to be the actual values used in the experiment."
3345,1,"['factor', 'design', 'case', 'experiment', 'cases', 'samples', 'experimental', 'factors', 'experimental units', 'response', 'randomized complete block design', 'treatment', 'interaction', 'completely randomized design', 'combinations']", Introduction,seg_391,"for each of these cases, it is important to determine not only if each of the two factors has an influence on the response, but also if there is a significant interaction between the two factors. as far as terminology is concerned, the experiment described here is a two-factor experiment and the experimental design may be either a completely randomized design, in which the various treatment combinations are assigned randomly to all the experimental units, or a randomized complete block design, in which factor combinations are assigned randomly within blocks. in the case of the yeast example, the various treatment combinations of temperature and drying time would be assigned randomly to the samples of yeast if we were using a completely randomized design."
3346,1,"['levels', 'factor', 'design', 'factorial', 'experiment', 'trials', 'samples', 'trial', 'factorial experiment', 'experimental', 'factors', 'percent', 'completely randomized design', 'combinations']", Introduction,seg_391,"many of the concepts studied in chapter 13 are extended in this chapter to two and three factors. the main thrust of this material is the use of the completely randomized design with a factorial experiment. a factorial experiment in two factors involves experimental trials (or a single trial) with all factor combinations. for example, in the temperature-drying-time example with, say, 3 levels of each and n = 2 runs at each of the 9 combinations, we have a two-factor factorial experiment in a completely randomized design. neither factor is a blocking factor; we are interested in how each influences percent solids in the samples and whether or not they interact. the biologist would have available 18 physical samples of"
3347,1,"['experimental', 'combinations', 'treatment', 'experimental units']", Introduction,seg_391,"material which are experimental units. these would then be assigned randomly to the 18 combinations (9 treatment combinations, each duplicated)."
3348,1,"['degrees of freedom', 'levels', 'experiment', 'factor', 'treatment', 'combinations']", Introduction,seg_391,"before we launch into analytical details, sums of squares, and so on, it may be of interest for the reader to observe the obvious connection between what we have described and the situation with the one-factor problem. consider the yeast experiment. explanation of degrees of freedom aids the reader or the analyst in visualizing the extension. we should initially view the 9 treatment combinations as if they represented one factor with 9 levels (8 degrees of freedom). thus, an initial look at degrees of freedom gives"
3349,1,"['error', 'combinations']", Introduction,seg_391,treatment combinations 8 error 9 total 17
3350,1,"['degrees of freedom', 'combinations', 'factors', 'experiment', 'factor', 'table', 'interaction', 'information', 'associated']", Introduction,seg_391,"the experiment could be analyzed as described in the above table. however, the f-test for combinations would probably not give the analyst the information he or she desires, namely, that which considers the role of temperature and drying time. three drying times have 2 associated degrees of freedom; three temperatures have 2 degrees of freedom. the main factors, temperature and drying time, are called main effects. the main effects represent 4 of the 8 degrees of freedom for factor combinations. the additional 4 degrees of freedom are associated with interaction between the two factors. as a result, the analysis involves"
3351,1,"['interaction', 'error']", Introduction,seg_391,combinations 8 temperature 2 drying time 2 interaction 4 error 9 total 17
3352,1,"['levels', 'factors', 'interaction', 'cases', 'mean squares', 'analysis of variance', 'mean', 'random', 'expected mean squares', 'variance']", Introduction,seg_391,"recall from chapter 13 that factors in an analysis of variance may be viewed as fixed or random, depending on the type of inference desired and how the levels were chosen. here we must consider fixed effects, random effects, and even cases where effects are mixed. most attention will be directed toward expected mean squares when we advance to these topics. in the following section, we focus on the concept of interaction."
3353,1,"['experimental error', 'model', 'error', 'experimental', 'treatment', 'interaction', 'observation', 'expected value', 'mean square', 'mean', 'variance', 'mean square error']", Interaction in the TwoFactor Experiment,seg_393,"in the randomized block model discussed previously, it was assumed that one observation on each treatment is taken in each block. if the model assumption is correct, that is, if blocks and treatments are the only real effects and interaction does not exist, the expected value of the mean square error is the experimental error variance σ2. suppose, however, that there is interaction occurring between treatments and blocks as indicated by the model"
3354,1,"['mean square error', 'expected value', 'mean square', 'mean', 'error']", Interaction in the TwoFactor Experiment,seg_393,of section 13.8. the expected value of the mean square error is then given as
3355,1,"['experimental error', 'model', 'error', 'experimental', 'treatment', 'interaction', 'mean square', 'mean', 'variation', 'mean square error']", Interaction in the TwoFactor Experiment,seg_393,"the treatment and block effects do not appear in the expected mean square error, but the interaction effects do. thus, if there is interaction in the model, the mean square error reflects variation due to experimental error plus an interaction contribution, and for this experimental plan, there is no way of separating them."
3356,1,"['experimental', 'levels', 'factor', 'interaction', 'samples', 'level of factor', 'variation', 'level', 'significance', 'test', 'response', 'error']", Interaction in the TwoFactor Experiment,seg_393,"from an experimenter’s point of view it should seem necessary to arrive at a significance test on the existence of interaction by separating true error variation from that due to interaction. the main effects, a and b, take on a different meaning in the presence of interaction. in the previous biological example, the effect that drying time has on the amount of solids left in the yeast might very well depend on the temperature to which the samples are exposed. in general, there could be experimental situations in which factor a has a positive effect on the response at one level of factor b, while at a different level of factor b the effect of a is negative. we use the term positive effect here to indicate that the yield or response increases as the levels of a given factor increase according to some defined order. in the same sense, a negative effect corresponds to a decrease in response for increasing levels of the factor."
3357,1,"['levels', 'factor', 'data', 'response', 'percent']", Interaction in the TwoFactor Experiment,seg_393,"consider, for example, the following data on temperature (factor a at levels t1, t2, and t3 in increasing order) and drying time d1, d2, and d3 (also in increasing order). the response is percent solids. these data are completely hypothetical and given to illustrate a point."
3358,1,"['interaction', 'sum of squares', 'average', 'percent']", Interaction in the TwoFactor Experiment,seg_393,"clearly the effect of temperature on percent solids is positive at the low drying time d1 but negative for high drying time d3. this clear interaction between temperature and drying time is obviously of interest to the biologist, but, based on the totals of the responses for temperatures t1, t2, and t3, the temperature sum of squares, ssa, will yield a value of zero. we say then that the presence of interaction is masking the effect of temperature. thus, if we consider the average effect of temperature, averaged over drying time, there is no effect. this then defines the main effect. but, of course, this is likely not what is pertinent to the biologist."
3359,1,"['significance', 'interaction', 'tests', 'test']", Interaction in the TwoFactor Experiment,seg_393,"before drawing any final conclusions resulting from tests of significance on the main effects and interaction effects, the experimenter should first observe whether or not the test for interaction is significant. if interaction is"
3360,1,"['levels', 'factor', 'results', 'interaction', 'tests']", Interaction in the TwoFactor Experiment,seg_393,"not significant, then the results of the tests on the main effects are meaningful. however, if interaction should be significant, then only those tests on the main effects that turn out to be significant are meaningful. nonsignificant main effects in the presence of interaction might well be a result of masking and dictate the need to observe the influence of each factor at fixed levels of the other."
3361,1,"['factor', 'plots', 'interaction', 'data', 'level']", Interaction in the TwoFactor Experiment,seg_393,the presence of interaction as well as its scientific impact can be interpreted nicely through the use of interaction plots. the plots clearly give a pictorial view of the tendency in the data to show the effect of changing one factor as one moves from one level to another of a second factor. figure 14.1 illustrates the strong temperature by drying time interaction. the interaction is revealed in nonparallel lines.
3362,1,"['plot', 'interaction', 'data']", Interaction in the TwoFactor Experiment,seg_393,figure 14.1: interaction plot for temperature–drying time data.
3363,1,"['plots', 'interaction', 'slope', 'set', 'percent']", Interaction in the TwoFactor Experiment,seg_393,"the relatively strong temperature effect on percent solids at the lower drying time is reflected in the steep slope at d1. at the middle drying time d2 the temperature has very little effect, while at the high drying time d3 the negative slope illustrates a negative effect of temperature. interaction plots such as this set give the scientist a quick and meaningful interpretation of the interaction that is present. it should be apparent that parallelism in the plots signals an absence of interaction."
3364,1,"['experimental error', 'experimental', 'combination', 'experiment', 'treatment', 'observations', 'measurements', 'combinations', 'efficiency', 'replications', 'error']", Interaction in the TwoFactor Experiment,seg_393,"interaction and experimental error are separated in the two-factor experiment only if multiple observations are taken at the various treatment combinations. for maximum efficiency, there should be the same number n of observations at each combination. these should be true replications, not just repeated measurements. for"
3365,1,"['sample', 'experimental', 'measurement', 'variability', 'observations', 'samples', 'measurements', 'variation', 'measurement error', 'experimental units', 'error']", Interaction in the TwoFactor Experiment,seg_393,"example, in the yeast illustration, if we take n = 2 observations at each combination of temperature and drying time, there should be two separate samples and not merely repeated measurements on the same sample. this allows variability due to experimental units to appear in “error,” so the variation is not merely measurement error."
3366,1,"['levels', 'factor', 'design', 'observations', 'case', 'observation', 'experiment', 'combination', 'analysis of variance', 'level', 'replications', 'variance', 'table', 'treatment', 'completely randomized design', 'level of factor', 'combinations']", TwoFactor Analysis of Variance,seg_395,"to present general formulas for the analysis of variance of a two-factor experiment using repeated observations in a completely randomized design, we shall consider the case of n replications of the treatment combinations determined by a levels of factor a and b levels of factor b. the observations may be classified by means of a rectangular array where the rows represent the levels of factor a and the columns represent the levels of factor b. each treatment combination defines a cell in our array. thus, we have ab cells, each cell containing n observations. denoting the kth observation taken at the ith level of factor a and the jth level of factor b by yijk, table 14.1 shows the abn observations."
3367,1,"['replications', 'experiment']", TwoFactor Analysis of Variance,seg_395,table 14.1: two-factor experiment with n replications
3368,1,['mean'], TwoFactor Analysis of Variance,seg_395,a 1 2 · · · b total mean
3369,1,['mean'], TwoFactor Analysis of Variance,seg_395,total y.1. y.2. · · · y.b. y... mean ȳ.1. ȳ.2. · · · ȳ.b. ȳ...
3370,1,"['sample', 'observations', 'random sample', 'populations', 'normally distributed', 'mean', 'population', 'random', 'variance']", TwoFactor Analysis of Variance,seg_395,the observations in the (ij)th cell constitute a random sample of size n from a population that is assumed to be normally distributed with mean μij and variance σ2. all ab populations are assumed to have the same variance σ2. let us define
3371,1,['table'], TwoFactor Analysis of Variance,seg_395,"the following useful symbols, some of which are used in table 14.1:"
3372,1,['observations'], TwoFactor Analysis of Variance,seg_395,"yij. = sum of the observations in the (ij)th cell,"
3373,1,"['factor', 'observations', 'level of factor', 'level']", TwoFactor Analysis of Variance,seg_395,"yi.. = sum of the observations for the ith level of factor a,"
3374,1,"['factor', 'observations', 'level of factor', 'level']", TwoFactor Analysis of Variance,seg_395,"y.j. = sum of the observations for the jth level of factor b,"
3375,1,['observations'], TwoFactor Analysis of Variance,seg_395,"y = sum of all abn observations, ..."
3376,1,"['observations', 'mean']", TwoFactor Analysis of Variance,seg_395,"ȳij. = mean of the observations in the (ij)th cell,"
3377,1,"['factor', 'observations', 'level of factor', 'mean', 'level']", TwoFactor Analysis of Variance,seg_395,"ȳi.. = mean of the observations for the ith level of factor a,"
3378,1,"['factor', 'observations', 'level of factor', 'mean', 'level']", TwoFactor Analysis of Variance,seg_395,"ȳ.j. = mean of the observations for the jth level of factor b,"
3379,1,"['observations', 'mean']", TwoFactor Analysis of Variance,seg_395,ȳ... = mean of all abn observations.
3380,1,"['sample', 'independent', 'combinations', 'factors', 'combination', 'factor', 'observations', 'populations', 'cases']", TwoFactor Analysis of Variance,seg_395,"unlike in the one-factor situation covered at length in chapter 13, here we are assuming that the populations, where n independent identically distributed observations are taken, are combinations of factors. also we will assume throughout that an equal number (n) of observations are taken at each factor combination. in cases in which the sample sizes per combination are unequal, the computations are more complicated but the concepts are transferable."
3381,1,"['observation', 'table']", TwoFactor Analysis of Variance,seg_395,each observation in table 14.1 may be written in the form
3382,1,"['factor', 'population mean', 'interaction', 'deviations', 'level of factor', 'mean', 'population', 'level']", TwoFactor Analysis of Variance,seg_395,"where ijk measures the deviations of the observed yijk values in the (ij)th cell from the population mean μij . if we let (αβ)ij denote the interaction effect of the ith level of factor a and the jth level of factor b, αi the effect of the ith level of factor a, βj the effect of the jth level of factor b, and μ the overall mean, we can write"
3383,0,[], TwoFactor Analysis of Variance,seg_395,on which we impose the restrictions
3384,1,['hypotheses'], TwoFactor Analysis of Variance,seg_395,the three hypotheses to be tested are as follows:
3385,1,"['tests of hypotheses', 'model', 'results', 'interaction', 'hypotheses', 'tests', 'test']", TwoFactor Analysis of Variance,seg_395,"we warned the reader about the problem of masking of main effects when interaction is a heavy contributor in the model. it is recommended that the interaction test result be considered first. the interpretation of the main effect test follows, and the nature of the scientific conclusion depends on whether interaction is found. if interaction is ruled out, then hypotheses 1 and 2 above can be tested and the interpretation is quite simple. however, if interaction is found to be present the interpretation can be more complicated, as we have seen from the discussion of the drying time and temperature in the previous section. in what follows, the structure of the tests of hypotheses 1, 2, and 3 will be discussed. interpretation of results will be incorporated in the discussion of the analysis in example 14.1."
3386,1,"['independent', 'estimates', 'data', 'sum of squares', 'hypotheses', 'tests']", TwoFactor Analysis of Variance,seg_395,the tests of the hypotheses above will be based on a comparison of independent estimates of σ2 provided by splitting the total sum of squares of our data into four components by means of the following identity.
3387,0,[], TwoFactor Analysis of Variance,seg_395,sum-of-squares identity
3388,0,[], TwoFactor Analysis of Variance,seg_395,"symbolically, we write the sum-of-squares identity as"
3389,1,"['degrees of freedom', 'interaction', 'sum of squares', 'error sum of squares', 'error']", TwoFactor Analysis of Variance,seg_395,"where ssa and ssb are called the sums of squares for the main effects a and b, respectively, ss(ab) is called the interaction sum of squares for a and b, and sse is the error sum of squares. the degrees of freedom are partitioned according to the identity"
3390,1,"['degrees of freedom', 'statistics']", TwoFactor Analysis of Variance,seg_395,"if we divide each of the sums of squares on the right side of the sum-of-squares identity by its corresponding number of degrees of freedom, we obtain the four statistics"
3391,1,"['independent', 'condition', 'estimates', 'variance']", TwoFactor Analysis of Variance,seg_395,"all of these variance estimates are independent estimates of σ2 under the condition that there are no effects αi, βj , and, of course, (αβ)ij . if we interpret the sums of"
3392,1,"['functions', 'random variables', 'independent', 'variables', 'independent random variables', 'random']", TwoFactor Analysis of Variance,seg_395,"squares as functions of the independent random variables y111, y112, . . . , yabn, it is not difficult to verify that"
3393,1,"['estimates', 'unbiased']", TwoFactor Analysis of Variance,seg_395,from which we immediately observe that all four estimates of σ2 are unbiased when
3394,1,"['test', 'factors', 'hypothesis']", TwoFactor Analysis of Variance,seg_395,"to test the hypothesis h0 ′ , that the effects of factors a are all equal to zero, we compute the following ratio:"
3395,1,"['random variable', 'variable', 'random']", TwoFactor Analysis of Variance,seg_395,which is a value of the random variable f1 having the f-distribution with a− 1
3396,1,"['degrees of freedom', 'significance', 'null hypothesis', 'hypothesis']", TwoFactor Analysis of Variance,seg_395,"′ and ab(n−1) degrees of freedom when h0 is true. the null hypothesis is rejected at the α-level of significance when f1 > fα[a− 1, ab(n− 1)]."
3397,1,"['factor', 'test', 'hypothesis']", TwoFactor Analysis of Variance,seg_395,"′′ similarly, to test the hypothesis h0 that the effects of factor b are all equal to zero, we compute the following ratio:"
3398,1,['factor'], TwoFactor Analysis of Variance,seg_395,"f-test for s22 factor b f2 = s2 ,"
3399,1,"['random variable', 'variable', 'random']", TwoFactor Analysis of Variance,seg_395,which is a value of the random variable f2 having the f-distribution with b− 1
3400,1,"['degrees of freedom', 'significance', 'hypothesis']", TwoFactor Analysis of Variance,seg_395,"′′ and ab(n− 1) degrees of freedom when h0 is true. this hypothesis is rejected at the α-level of significance when f2 > fα[b− 1, ab(n− 1)]."
3401,1,"['test', 'interaction', 'hypothesis']", TwoFactor Analysis of Variance,seg_395,"′′′ finally, to test the hypothesis h0 , that the interaction effects are all equal to zero, we compute the following ratio:"
3402,1,['interaction'], TwoFactor Analysis of Variance,seg_395,"f-test for s23 interaction f3 = s2 ,"
3403,1,"['random variable', 'variable', 'random']", TwoFactor Analysis of Variance,seg_395,which is a value of the random variable f3 having the f-distribution with
3404,1,"['degrees of freedom', 'interaction', 'significance']", TwoFactor Analysis of Variance,seg_395,"′′′ (a − 1)(b − 1) and ab(n − 1) degrees of freedom when h0 is true. we conclude that, at the α-level of significance, interaction is present when f3 > fα[(a− 1)(b− 1), ab(n− 1)]."
3405,1,"['levels', 'interaction', 'tests', 'test', 'response', 'hypothesis']", TwoFactor Analysis of Variance,seg_395,"as indicated in section 14.2, it is advisable to interpret the test for interaction before attempting to draw inferences on the main effects. if interaction is not significant, there is certainly evidence that the tests on main effects are interpretable. rejection of hypothesis 1 on page 566 implies that the response means at the levels"
3406,1,"['levels', 'condition', 'factor', 'interaction', 'data', 'hypothesis']", TwoFactor Analysis of Variance,seg_395,"of factor a are significantly different, while rejection of hypothesis 2 implies a similar condition for the means at levels of factor b. however, a significant interaction could very well imply that the data should be analyzed in a somewhat different manner—perhaps observing the effect of factor a at fixed levels of factor b, and so forth."
3407,1,"['replications', 'table']", TwoFactor Analysis of Variance,seg_395,"the computations in an analysis-of-variance problem, for a two-factor experiment with n replications, are usually summarized as in table 14.2."
3408,1,"['experiment', 'analysis of variance', 'replications', 'variance']", TwoFactor Analysis of Variance,seg_395,table 14.2: analysis of variance for the two-factor experiment with n replications
3409,1,"['mean', 'variation']", TwoFactor Analysis of Variance,seg_395,source of sum of degrees of mean computed variation squares freedom square f
3410,1,"['rate', 'combination', 'experiment', 'observations', 'rates']", TwoFactor Analysis of Variance,seg_395,"example 14.1: in an experiment conducted to determine which of 3 different missile systems is preferable, the propellant burning rate for 24 static firings was measured. four different propellant types were used. the experiment yielded duplicate observations of burning rates at each combination of the treatments."
3411,1,"['table', 'data', 'hypotheses', 'mean', 'test', 'rates']", TwoFactor Analysis of Variance,seg_395,"the data, after coding, are given in table 14.3. test the following hypotheses: ′ (a) h0: there is no difference in the mean propellant burning rates when different"
3412,1,['mean'], TwoFactor Analysis of Variance,seg_395,"′′ missile systems are used, (b) h0 : there is no difference in the mean propellant"
3413,1,"['interaction', 'rates']", TwoFactor Analysis of Variance,seg_395,"′′′ burning rates of the 4 propellant types, (c) h0 : there is no interaction between the different missile systems and the different propellant types."
3414,1,['rates'], TwoFactor Analysis of Variance,seg_395,table 14.3: propellant burning rates
3415,0,[], TwoFactor Analysis of Variance,seg_395,system b1 b2 b3 b4
3416,1,"['variance', 'analysis of variance', 'table']", TwoFactor Analysis of Variance,seg_395,the sum-of-squares formula is used as described in theorem 14.1. the analysis of variance is shown in table 14.4.
3417,1,"['variance', 'analysis of variance', 'data', 'table']", TwoFactor Analysis of Variance,seg_395,table 14.4: analysis of variance for the data of table 14.3
3418,1,"['mean', 'variation']", TwoFactor Analysis of Variance,seg_395,source of sum of degrees of mean computed variation squares freedom square f
3419,1,"['interaction', 'error']", TwoFactor Analysis of Variance,seg_395,missile system 14.52 2 7.26 5.84 propellant type 40.08 3 13.36 10.75 interaction 22.16 6 3.69 2.97 error 14.91 12 1.24
3420,1,"['glm', 'degrees of freedom', 'linear', 'model', 'rate', 'interaction', 'data']", TwoFactor Analysis of Variance,seg_395,"the reader is directed to a sas glm procedure (general linear models) for analysis of the burning rate data in figure 14.2. note how the “model” (11 degrees of freedom) is initially tested and the system, type, and system by type interaction are tested separately. the f-test on the model (p = 0.0030) is testing the accumulation of the two main effects and the interaction."
3421,1,['mean'], TwoFactor Analysis of Variance,seg_395,′ (a) reject h0 and conclude that different missile systems result in different mean
3422,1,"['mean', 'rates']", TwoFactor Analysis of Variance,seg_395,propellant burning rates. the p -value is approximately 0.0169. ′′ (b) reject h0 and conclude that the mean propellant burning rates are not the
3423,0,[], TwoFactor Analysis of Variance,seg_395,same for the four propellant types. the p -value is approximately 0.0010.
3424,1,"['level', 'interaction']", TwoFactor Analysis of Variance,seg_395,"(c) interaction is barely insignificant at the 0.05 level, but the p -value of approx-"
3425,1,['interaction'], TwoFactor Analysis of Variance,seg_395,imately 0.0513 would indicate that interaction must be taken seriously.
3426,1,"['table', 'marginal', 'interaction', 'statistical', 'statistical significance', 'significance']", TwoFactor Analysis of Variance,seg_395,"at this point we should draw some type of interpretation of the interaction. it should be emphasized that statistical significance of a main effect merely implies that marginal means are significantly different. however, consider the two-way table of averages in table 14.5."
3427,1,['interaction'], TwoFactor Analysis of Variance,seg_395,table 14.5: interpretation of interaction
3428,1,['average'], TwoFactor Analysis of Variance,seg_395,b1 b2 b3 b4 average
3429,1,['average'], TwoFactor Analysis of Variance,seg_395,a1 33.35 31.45 28.25 28.95 30.50 a2 32.60 30.00 28.40 27.70 29.68 a3 28.85 28.10 28.50 28.95 28.60 average 31.60 29.85 28.38 28.53
3430,1,"['table', 'marginal', 'information']", TwoFactor Analysis of Variance,seg_395,it is apparent that more important information exists in the body of the table— trends that are inconsistent with the trend depicted by marginal averages. table 14.5 certainly suggests that the effect of propellant type depends on the system
3431,1,"['glm', 'model', 'rate', 'dependent variable', 'dependent', 'variable', 'mean square', 'mean', 'error']", TwoFactor Analysis of Variance,seg_395,the glm procedure dependent variable: rate sum of source df squares mean square f value pr > f model 11 76.76833333 6.97893939 5.62 0.0030 error 12 14.91000000 1.24250000 corrected total 23 91.67833333
3432,1,"['mse', 'rate', 'mean']", TwoFactor Analysis of Variance,seg_395,r-square coeff var root mse rate mean 0.837366 3.766854 1.114675 29.59167
3433,1,"['mean square', 'mean']", TwoFactor Analysis of Variance,seg_395,source df type iii ss mean square f value pr > f system 2 14.52333333 7.26166667 5.84 0.0169 type 3 40.08166667 13.36055556 10.75 0.0010 system*type 6 22.16333333 3.69388889 2.97 0.0512
3434,1,"['rate', 'data', 'table']", TwoFactor Analysis of Variance,seg_395,figure 14.2: sas printout of the analysis of the propellant rate data of table 14.3.
3435,1,"['interaction', 'factors']", TwoFactor Analysis of Variance,seg_395,"being used. for example, for system 3 the propellant-type effect does not appear to be important, although it does have a large effect if either system 1 or system 2 is used. this explains the “significant” interaction between these two factors. more will be revealed subsequently concerning this interaction."
3436,1,"['sum of squares', 'orthogonal contrasts', 'contrasts']", TwoFactor Analysis of Variance,seg_395,"example 14.2: referring to example 14.1, choose two orthogonal contrasts to partition the sum of squares for the missile systems into single-degree-of-freedom components to be used in comparing systems 1 and 2 versus 3, and system 1 versus system 2."
3437,1,['contrast'], TwoFactor Analysis of Variance,seg_395,solution : the contrast for comparing systems 1 and 2 with 3 is
3438,1,['contrast'], TwoFactor Analysis of Variance,seg_395,"a second contrast, orthogonal to w1, for comparing system 1 with system 2, is given by w2 = μ1. − μ2.. the single-degree-of-freedom sums of squares are"
3439,0,[], TwoFactor Analysis of Variance,seg_395,"notice that ssw1 + ssw2 = ssa, as expected. the computed f-values corresponding to w1 and w2 are, respectively,"
3440,1,"['contrast', 'critical value']", TwoFactor Analysis of Variance,seg_395,"compared to the critical value f0.05(1, 12) = 4.75, we find f1 to be significant. in fact, the p-value is less than 0.01. thus, the first contrast indicates that the"
3441,1,"['mean', 'rates']", TwoFactor Analysis of Variance,seg_395,"is rejected. since f2 < 4.75, the mean burning rates of the first and second systems are not significantly different."
3442,1,"['degrees of freedom', 'interaction', 'level', 'hypothesis']", TwoFactor Analysis of Variance,seg_395,"if the hypothesis of no interaction in example 14.1 is true, we could make the general comparisons of example 14.2 regarding our missile systems rather than separate comparisons for each propellant. similarly, we might make general comparisons among the propellants rather than separate comparisons for each missile system. for example, we could compare propellants 1 and 2 with 3 and 4 and also propellant 1 versus propellant 2. the resulting f-ratios, each with 1 and 12 degrees of freedom, turn out to be 24.81 and 7.39, respectively, and both are quite significant at the 0.05 level."
3443,1,"['rate', 'interaction', 'mean', 'average', 'critical value', 'rates']", TwoFactor Analysis of Variance,seg_395,"from propellant averages there appears to be evidence that propellant 1 gives the highest mean burning rate. a prudent experimenter might be somewhat cautious in drawing overall conclusions in a problem such as this one, where the f-ratio for interaction is barely below the 0.05 critical value. for example, the overall evidence, 31.60 versus 29.85 on the average for the two propellants, certainly indicates that propellant 1 is superior, in terms of a higher burning rate, to propellant 2. however, if we restrict ourselves to system 3, where we have an average of 28.85 for propellant 1 as opposed to 28.10 for propellant 2, there appears to be little or no difference between these two propellants. in fact, there appears to be a stabilization of burning rates for the different propellants if we operate with system 3. there is certainly overall evidence which indicates that system 1 gives a higher burning rate than system 3, but if we restrict ourselves to propellant 4, this conclusion does not appear to hold."
3444,1,"['degrees of freedom', 'estimate', 'interaction', 'average', 'rates']", TwoFactor Analysis of Variance,seg_395,"the analyst can conduct a simple t-test using average burning rates for system 3 in order to display conclusive evidence that interaction is producing considerable difficulty in allowing broad conclusions on main effects. consider a comparison of propellant 1 against propellant 2 only using system 3. borrowing an estimate of σ2 from the overall analysis, that is, using s2 = 1.24 with 12 degrees of freedom, we have"
3445,1,['interaction'], TwoFactor Analysis of Variance,seg_395,which is not even close to being significant. this illustration suggests that one must be cautious about strict interpretation of main effects in the presence of interaction.
3446,1,"['graphical', 'combination', 'treatment', 'plots', 'case']", TwoFactor Analysis of Variance,seg_395,many of the same types of graphical displays that were suggested in the one-factor problems certainly apply in the two-factor case. two-dimensional plots of cell means or treatment combination means can provide insight into the presence of
3447,1,"['plot', 'factors', 'residuals', 'homogeneous', 'response', 'fitted values', 'variance', 'error']", TwoFactor Analysis of Variance,seg_395,"interactions between the two factors. in addition, a plot of residuals against fitted values may well provide an indication of whether or not the homogeneous variance assumption holds. often, of course, a violation of the homogeneous variance assumption involves an increase in the error variance as the response numbers get larger. as a result, this plot may point out the violation."
3448,1,"['plot', 'factors', 'residuals', 'interaction', 'case', 'data', 'homogeneous', 'fitted values', 'variance']", TwoFactor Analysis of Variance,seg_395,figure 14.3 shows the plot of cell means in the case of the missile system propellant illustration in example 14.1. notice how graphically (in this case) the lack of parallelism shows through. note the flatness of the part of the figure showing the propellant effect for system 3. this illustrates interaction among the factors. figure 14.4 shows the plot of residuals against fitted values for the same data. there is no apparent sign of difficulty with the homogeneous variance assumption.
3449,1,"['plot', 'data']", TwoFactor Analysis of Variance,seg_395,figure 14.3: plot of cell means for data of example 14.1. numbers represent missile systems.
3450,1,"['plot', 'residual', 'residual plot', 'data']", TwoFactor Analysis of Variance,seg_395,figure 14.4: residual plot of data of example 14.1.
3451,1,"['rate', 'experimental', 'levels', 'factors', 'factor', 'design', 'table', 'combinations', 'data', 'completely randomized design', 'process', 'response']", TwoFactor Analysis of Variance,seg_395,"example 14.3: an electrical engineer is investigating a plasma etching process used in semiconductor manufacturing. it is of interest to study the effects of two factors, the c2f6 gas flow rate (a) and the power applied to the cathode (b). the response is the etch rate. each factor is run at 3 levels, and 2 experimental runs on etch rate are made for each of the 9 combinations. the setup is that of a completely randomized design. the data are given in table 14.6. the etch rate is in a◦/min."
3452,1,['data'], TwoFactor Analysis of Variance,seg_395,table 14.6: data for example 14.3
3453,1,['rate'], TwoFactor Analysis of Variance,seg_395,power supplied c2f6 flow rate 1 2 3
3454,1,"['level', 'levels', 'factors']", TwoFactor Analysis of Variance,seg_395,"the levels of the factors are in ascending order, with level 1 being low level and level 3 being the highest."
3455,1,"['variance', 'analysis of variance', 'table']", TwoFactor Analysis of Variance,seg_395,"(a) show an analysis of variance table and draw conclusions, beginning with the"
3456,1,['interaction'], TwoFactor Analysis of Variance,seg_395,test on interaction.
3457,1,['tests'], TwoFactor Analysis of Variance,seg_395,(b) do tests on main effects and draw conclusions.
3458,0,[], TwoFactor Analysis of Variance,seg_395,solution : a sas output is given in figure 14.5. from the output we learn the following.
3459,1,"['glm', 'model', 'dependent variable', 'dependent', 'variable', 'mean square', 'mean', 'error']", TwoFactor Analysis of Variance,seg_395,the glm procedure dependent variable: etchrate sum of source df squares mean square f value pr > f model 8 379508.7778 47438.5972 61.00 <.0001 error 9 6999.5000 777.7222 corrected total 17 386508.2778
3460,1,"['mse', 'mean']", TwoFactor Analysis of Variance,seg_395,r-square coeff var root mse etchrate mean 0.981890 5.057714 27.88767 551.3889
3461,1,"['mean square', 'mean']", TwoFactor Analysis of Variance,seg_395,source df type iii ss mean square f value pr > f c2f6 2 46343.1111 23171.5556 29.79 0.0001 power 2 330003.4444 165001.7222 212.16 <.0001 c2f6*power 4 3162.2222 790.5556 1.02 0.4485
3462,1,"['test', 'interaction']", TwoFactor Analysis of Variance,seg_395,(a) the p-value for the test of interaction is 0.4485. we can conclude that there
3463,1,['interaction'], TwoFactor Analysis of Variance,seg_395,is no significant interaction.
3464,1,"['rate', 'levels', 'mean']", TwoFactor Analysis of Variance,seg_395,(b) there is a significant difference in mean etch rate for the 3 levels of c2f6 flow
3465,1,"['rate', 'mean', 'level', 'test']", TwoFactor Analysis of Variance,seg_395,rate. duncan’s test shows that the mean etch rate for level 3 is significantly
3466,1,"['level', 'rate']", TwoFactor Analysis of Variance,seg_395,higher than that for level 2 and the rate for level 2 is significantly higher than that for level 1. see figure 14.6(a).
3467,1,"['rate', 'mean', 'level', 'test']", TwoFactor Analysis of Variance,seg_395,there is a significant difference in mean etch rate based on the level of power to the cathode. duncan’s test revealed that the etch rate for level 3 is significantly higher than that for level 2 and the rate for level 2 is significantly higher than that for level 1. see figure 14.6(b).
3468,1,['mean'], TwoFactor Analysis of Variance,seg_395,duncan grouping mean n c2f6 duncan grouping mean n power a 619.83 6 3 a 728.00 6 3 b 535.83 6 2 b 527.17 6 2 c 498.50 6 1 c 399.00 6 1 (a) (b)
3469,1,"['rate', 'test']", TwoFactor Analysis of Variance,seg_395,"figure 14.6: sas output, for example 14.3. (a) duncan’s test on gas flow rate; (b) duncan’s test on power."
3470,1,"['levels', 'design', 'observations', 'significance', 'experiment', 'tests', 'experimental', 'factors', 'interactions', 'treatment', 'combinations']", ThreeFactor Experiments,seg_399,"in this section, we consider an experiment with three factors, a, b, and c, at a, b, and c levels, respectively, in a completely randomized experimental design. assume again that we have n observations for each of the abc treatment combinations. we shall proceed to outline significance tests for the three main effects and interactions involved. it is hoped that the reader can then use the description given here to generalize the analysis to k > 3 factors."
3471,1,"['model', 'experiment']", ThreeFactor Experiments,seg_399,"model for the the model for the three-factor experiment is three-factor experiment yijkl = μ+ αi + βj + γk + (αβ)ij + (αγ)ik + (βγ)jk + (αβγ)ijk + ijkl,"
3472,1,"['experiment', 'interaction']", ThreeFactor Experiments,seg_399,"i = 1, 2, . . . , a; j = 1, 2, . . . , b; k = 1, 2, . . . , c; and l = 1, 2, . . . , n, where αi, βj , and γk are the main effects and (αβ)ij , (αγ)ik, and (βγ)jk are the twofactor interaction effects that have the same interpretation as in the two-factor experiment."
3473,1,"['experimental', 'levels', 'factor', 'interaction', 'random variation', 'variation', 'mean squares', 'interactions', 'mean', 'random']", ThreeFactor Experiments,seg_399,"the term (αβγ)ijk is called the three-factor interaction effect, a term that represents a nonadditivity of the (αβ)ij over the different levels of the factor c. as before, the sum of all main effects is zero and the sum over any subscript of the twoand three-factor interaction effects is zero. in many experimental situations, these higher-order interactions are insignificant and their mean squares reflect only random variation, but we shall outline the analysis in its most general form."
3474,1,"['tests', 'random variables', 'independent', 'variables', 'significance', 'errors', 'normally distributed', 'mean', 'random', 'variance']", ThreeFactor Experiments,seg_399,"again, in order that valid significance tests can be made, we must assume that the errors are values of independent and normally distributed random variables, each with mean 0 and common variance σ2."
3475,1,"['independent', 'estimate', 'estimates', 'factor', 'interaction', 'sum of squares', 'mean square', 'mean', 'variation', 'experiments', 'variance', 'error']", ThreeFactor Experiments,seg_399,"the general philosophy concerning the analysis is the same as that discussed for the oneand two-factor experiments. the sum of squares is partitioned into eight terms, each representing a source of variation from which we obtain independent estimates of σ2 when all the main effects and interaction effects are zero. if the effects of any given factor or interaction are not all zero, then the mean square will estimate the error variance plus a component due to the systematic effect in question."
3476,1,['experiment'], ThreeFactor Experiments,seg_399,sum of squares a for a ssa = bcn∑(ȳi... − ȳ....)2 ss(ab) = cn∑∑(ȳij.. − ȳi... − ȳ.j.. + ȳ....)2 three-factor i=1 i j experiment b
3477,1,['interactions'], ThreeFactor Experiments,seg_399,"although we emphasize interpretation of annotated computer printout in this section rather than being concerned with laborious computation of sums of squares, we do offer the following as the sums of squares for the three main effects and interactions. notice the obvious extension from the twoto three-factor problem."
3478,1,"['observations', 'average']", ThreeFactor Experiments,seg_399,"the averages in the formulas are defined as follows: ȳ.... = average of all abcn observations,"
3479,1,"['factor', 'observations', 'level of factor', 'level', 'average']", ThreeFactor Experiments,seg_399,"ȳi... = average of the observations for the ith level of factor a,"
3480,1,"['factor', 'observations', 'level of factor', 'level', 'average']", ThreeFactor Experiments,seg_399,"ȳ.j.. = average of the observations for the jth level of factor b,"
3481,1,"['factor', 'observations', 'level of factor', 'level', 'average']", ThreeFactor Experiments,seg_399,"ȳ..k. = average of the observations for the kth level of factor c,"
3482,1,"['level', 'observations', 'average']", ThreeFactor Experiments,seg_399,"ȳij.. = average of the observations for the ith level of a and the jth level of b,"
3483,1,"['level', 'observations', 'average']", ThreeFactor Experiments,seg_399,"ȳi.k. = average of the observations for the ith level of a and the kth level of c,"
3484,1,"['level', 'observations', 'average']", ThreeFactor Experiments,seg_399,"ȳ.jk. = average of the observations for the jth level of b and the kth level of c,"
3485,1,"['combination', 'treatment', 'observations', 'average']", ThreeFactor Experiments,seg_399,ȳijk. = average of the observations for the (ijk)th treatment combination.
3486,1,"['combination', 'factor', 'table']", ThreeFactor Experiments,seg_399,the computations in an analysis-of-variance table for a three-factor problem with n replicated runs at each factor combination are summarized in table 14.7.
3487,1,"['experiment', 'replications', 'anova']", ThreeFactor Experiments,seg_399,table 14.7: anova for the three-factor experiment with n replications
3488,1,"['mean', 'variation']", ThreeFactor Experiments,seg_399,source of sum of degrees of mean computed variation squares freedom square f
3489,1,['interaction'], ThreeFactor Experiments,seg_399,two-factor interaction:
3490,1,['interaction'], ThreeFactor Experiments,seg_399,three-factor interaction:
3491,1,"['experimental', 'combination', 'experiment', 'table', 'interaction', 'case', 'sum of squares']", ThreeFactor Experiments,seg_399,"for the three-factor experiment with a single experimental run per combination, we may use the analysis of table 14.7 by setting n = 1 and using the abc interaction sum of squares for sse. in this case, we are assuming that the (αβγ)ijk interaction effects are all equal to zero so that"
3492,1,"['experimental error', 'experimental', 'estimate', 'sum of squares', 'mean square', 'error sum of squares', 'interactions', 'mean', 'variation', 'variance', 'error', 'unbiased']", ThreeFactor Experiments,seg_399,"that is, ss(abc) represents variation due only to experimental error. its mean square thereby provides an unbiased estimate of the error variance. with n = 1 and sse = ss(abc), the error sum of squares is found by subtracting the sums of squares of the main effects and two-factor interactions from the total sum of squares."
3493,1,"['process', 'factors', 'experiment', 'variables', 'table', 'combination', 'interactions', 'analysis of variance', 'test', 'variance']", ThreeFactor Experiments,seg_399,"example 14.4: in the production of a particular material, three variables are of interest: a, the operator effect (three operators): b, the catalyst used in the experiment (three catalysts); and c, the washing time of the product following the cooling process (15 minutes and 20 minutes). three runs were made at each combination of factors. it was felt that all interactions among the factors should be studied. the coded yields are in table 14.8. perform an analysis of variance to test for significant effects."
3494,1,['data'], ThreeFactor Experiments,seg_399,table 14.8: data for example 14.4
3495,1,"['table', 'data', 'interactions', 'analysis of variance', 'level', 'variance']", ThreeFactor Experiments,seg_399,"solution : table 14.9 shows an analysis of variance of the data given above. none of the interactions show a significant effect at the α = 0.05 level. however, the p-value for bc is 0.0610; thus, it should not be ignored. the operator and catalyst effects are significant, while the effect of washing time is not significant."
3496,1,"['table', 'factor', 'interaction', 'level', 'test']", ThreeFactor Experiments,seg_399,"more should be discussed regarding example 14.4, particularly about dealing with the effect that the interaction between catalyst and washing time is having on the test on the washing time main effect (factor c). recall our discussion in section 14.2. illustrations were given of how the presence of interaction could change the interpretation that we make regarding main effects. in example 14.4, the bc interaction is significant at approximately the 0.06 level. suppose, however, that we observe a two-way table of means as in table 14.10."
3497,0,[], ThreeFactor Experiments,seg_399,"it is clear why washing time was found not to be significant. a non-thorough analyst may get the impression that washing time can be eliminated from any future study in which yield is being measured. however, it is obvious how the"
3498,1,"['experiment', 'design', 'completely randomized design', 'anova']", ThreeFactor Experiments,seg_399,table 14.9: anova for a three-factor experiment in a completely randomized design
3499,1,"['sum of squares', 'mean square', 'mean']", ThreeFactor Experiments,seg_399,source df sum of squares mean square f-value p-value
3500,1,['error'], ThreeFactor Experiments,seg_399,a 2 13.98 6.99 11.64 0.0001 b 2 10.18 5.09 8.48 0.0010 ab 4 4.77 1.19 1.99 0.1172 c 1 1.19 1.19 1.97 0.1686 ac 2 2.91 1.46 2.43 0.1027 bc 2 3.63 1.82 3.03 0.0610 abc 4 4.91 1.23 2.04 0.1089 error 36 21.61 0.60 total 53 63.19
3501,1,['table'], ThreeFactor Experiments,seg_399,table 14.10: two-way table of means for example 14.4
3502,0,[], ThreeFactor Experiments,seg_399,"washing time, c"
3503,0,[], ThreeFactor Experiments,seg_399,"catalyst, b 15 min 20 min"
3504,1,['data'], ThreeFactor Experiments,seg_399,"effect of washing time changes from a negative effect for the first catalyst to what appears to be a positive effect for the third catalyst. if we merely focus on the data for catalyst 1, a simple comparison between the means at the two washing times will produce a simple t-statistic:"
3505,1,['level'], ThreeFactor Experiments,seg_399,"which is significant at a level less than 0.02. thus, an important negative effect of washing time for catalyst 1 might very well be ignored if the analyst makes the incorrect broad interpretation of the insignificant f-ratio for washing time."
3506,1,"['degrees of freedom', 'model', 'factors', 'experiment', 'variables', 'factor', 'sum of squares', 'error sum of squares', 'interactions', 'estimator', 'error', 'concentration']", ThreeFactor Experiments,seg_399,"we have described the three-factor model and its analysis in the most general form by including all possible interactions in the model. of course, there are many situations where it is known a priori that the model should not contain certain interactions. we can then take advantage of this knowledge by combining or pooling the sums of squares corresponding to negligible interactions with the error sum of squares to form a new estimator for σ2 with a larger number of degrees of freedom. for example, in a metallurgy experiment designed to study the effect on film thickness of three important processing variables, suppose it is known that factor a, acid concentration, does not interact with factors b and c. the"
3507,1,"['factor', 'anova']", ThreeFactor Experiments,seg_399,table 14.11: anova with factor a noninteracting
3508,1,"['mean', 'variation']", ThreeFactor Experiments,seg_399,source of sum of degrees of mean computed variation squares freedom square f
3509,1,['interaction'], ThreeFactor Experiments,seg_399,two-factor interaction: s2 bc ss(bc) (b− 1)(c− 1) s24 f4 = s42
3510,0,[], ThreeFactor Experiments,seg_399,error sse subtraction s2
3511,1,"['tests', 'degrees of freedom', 'error', 'estimate', 'significance', 'mean square', 'mean squares', 'mean', 'variance', 'mean square error']", ThreeFactor Experiments,seg_399,"sums of squares ssa, ssb, ssc, and ss(bc) are computed using the methods described earlier in this section. the mean squares for the remaining effects will now all independently estimate the error variance σ2. therefore, we form our new mean square error by pooling ss(ab), ss(ac), ss(abc), and sse, along with the corresponding degrees of freedom. the resulting denominator for the significance tests is then the mean square error given by"
3512,1,"['degrees of freedom', 'table', 'sum of squares']", ThreeFactor Experiments,seg_399,"computationally, of course, one obtains the pooled sum of squares and the pooled degrees of freedom by subtraction once sst and the sums of squares for the existing effects are computed. the analysis-of-variance table would then take the form of table 14.11."
3513,1,"['levels', 'factor', 'design', 'experiment', 'mean', 'error', 'experimental', 'factors', 'analysis of variance', 'variance', 'table', 'interaction', 'mean square', 'completely randomized design', 'mean square error']", ThreeFactor Experiments,seg_399,"in this chapter, we have assumed that the experimental design used is a completely randomized design. by interpreting the levels of factor a in table 14.11 as different blocks, we then have the analysis-of-variance procedure for a two-factor experiment in a randomized block design. for example, if we interpret the operators in example 14.4 as blocks and assume no interaction between blocks and the other two factors, the analysis of variance takes the form of table 14.12 rather than that of table 14.9. the reader can verify that the mean square error is also"
3514,1,['factor'], ThreeFactor Experiments,seg_399,"which demonstrates the pooling of the sums of squares for the nonexisting interaction effects. note that factor b, catalyst, has a significant effect on yield."
3515,1,"['experiment', 'design', 'anova']", ThreeFactor Experiments,seg_399,table 14.12: anova for a two-factor experiment in a randomized block design
3516,1,"['variation', 'mean']", ThreeFactor Experiments,seg_399,source of sum of degrees of mean computed variation squares freedom square f p-value
3517,1,"['interaction', 'error']", ThreeFactor Experiments,seg_399,blocks 13.98 2 6.99 main effect: b 10.18 2 5.09 6.88 0.0024 c 1.18 1 1.18 1.59 0.2130 two-factor interaction: bc 3.64 2 1.82 2.46 0.0966 error 34.21 46 0.74
3518,1,"['rate', 'experimental', 'levels', 'experiment', 'factor', 'table', 'data', 'interactions', 'random']", ThreeFactor Experiments,seg_399,"example 14.5: an experiment was conducted to determine the effects of temperature, pressure, and stirring rate on product filtration rate. this was done in a pilot plant. the experiment was run at two levels of each factor. in addition, it was decided that two batches of raw materials should be used, where batches were treated as blocks. eight experimental runs were made in random order for each batch of raw materials. it is thought that all two-factor interactions may be of interest. no interactions with batches are assumed to exist. the data appear in table 14.13. “l” and “h” imply low and high levels, respectively. the filtration rate is in gallons per hour."
3519,1,"['anova table', 'table', 'anova']", ThreeFactor Experiments,seg_399,(a) show the complete anova table. pool all “interactions” with blocks into
3520,1,['interactions'], ThreeFactor Experiments,seg_399,(b) what interactions appear to be significant?
3521,1,"['plots', 'interactions']", ThreeFactor Experiments,seg_399,(c) create plots to reveal and interpret the significant interactions. explain what
3522,1,['plot'], ThreeFactor Experiments,seg_399,the plot means to the engineer.
3523,1,['data'], ThreeFactor Experiments,seg_399,table 14.13: data for example 14.5
3524,1,['rate'], ThreeFactor Experiments,seg_399,low stirring rate high stirring rate
3525,0,[], ThreeFactor Experiments,seg_399,temp. pressure l pressure h temp. pressure l pressure h l 43 49 l 44 47 h 64 68 h 97 102
3526,1,['rate'], ThreeFactor Experiments,seg_399,low stirring rate high stirring rate
3527,0,[], ThreeFactor Experiments,seg_399,temp. pressure l pressure h temp. pressure l pressure h l 49 57 l 51 55 h 70 76 h 103 106
3528,1,"['rate', 'interaction']", ThreeFactor Experiments,seg_399,"(b) as seen in figure 14.7, the temperature by stirring rate (strate) interaction"
3529,1,"['rate', 'interaction', 'interactions', 'error']", ThreeFactor Experiments,seg_399,"appears to be highly significant. the pressure by stirring rate interaction also appears to be significant. incidentally, if one were to do further pooling by combining the insignificant interactions with error, the conclusions would remain the same and the p-value for the pressure by stirring rate interaction would become stronger, namely 0.0517."
3530,1,['rate'], ThreeFactor Experiments,seg_399,"(c) the main effects for both stirring rate and temperature are highly significant,"
3531,1,"['plot', 'rate', 'interaction', 'dependent', 'mean', 'level']", ThreeFactor Experiments,seg_399,"as shown in figure 14.7. a look at the interaction plot of figure 14.8(a) shows that the effect of stirring rate is dependent upon the level of temperature. at the low level of temperature the stirring rate effect is negligible, whereas at the high level of temperature stirring rate has a strong positive effect on mean filtration rate. in figure 14.8(b), the interaction between pressure and stirring rate, though not as pronounced as that of figure 14.8(a), still shows a slight inconsistency of the stirring rate effect across pressure."
3532,1,"['mean square', 'error', 'mean']", ThreeFactor Experiments,seg_399,source df type iii ss mean square f value pr > f batch 1 175.562500 175.562500 177.14 <.0001 pressure 1 95.062500 95.062500 95.92 <.0001 temp 1 5292.562500 5292.562500 5340.24 <.0001 pressure*temp 1 0.562500 0.562500 0.57 0.4758 strate 1 1040.062500 1040.062500 1049.43 <.0001 pressure*strate 1 5.062500 5.062500 5.11 0.0583 temp*strate 1 1072.562500 1072.562500 1082.23 <.0001 pressure*temp*strate 1 1.562500 1.562500 1.58 0.2495 error 7 6.937500 0.991071 corrected total 15 7689.937500
3533,1,"['interaction', 'error', 'anova']", ThreeFactor Experiments,seg_399,"figure 14.7: anova for example 14.5, batch interaction pooled with error."
3534,1,"['plots', 'interaction']", ThreeFactor Experiments,seg_399,figure 14.8: interaction plots for example 14.5.
3535,1,"['factors', 'factorial', 'factorial experiments', 'experiments']", ThreeFactor Experiments,seg_399,586 chapter 14 factorial experiments (two or more factors)
3536,1,"['model', 'experiment', 'random']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"in a two-factor experiment with random effects, we have the model"
3537,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random', 'variances']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"for i = 1, 2, . . . , a; j = 1, 2, . . . , b; and k = 1, 2, . . . , n, where the ai, bj , (ab)ij , and ijk are independent random variables with means 0 and variances σα"
3538,1,"['fixed effects experiments', 'random', 'experiments']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"σα 2β , and σ2, respectively. the sums of squares for random effects experiments are computed in exactly the same way as for fixed effects experiments. we are now"
3539,1,['hypotheses'], Factorial Experiments for Random Effects and Mixed Models,seg_403,interested in testing hypotheses of the form
3540,1,"['error', 'table', 'mean square', 'mean squares', 'mean', 'expected values', 'mean square error']", Factorial Experiments for Random Effects and Mixed Models,seg_403,where the denominator in the f-ratio is not necessarily the mean square error. the appropriate denominator can be determined by examining the expected values of the various mean squares. these are shown in table 14.14.
3541,1,"['experiment', 'mean squares', 'mean', 'random', 'expected mean squares']", Factorial Experiments for Random Effects and Mixed Models,seg_403,table 14.14: expected mean squares for a two-factor random effects experiment
3542,1,"['mean square', 'mean', 'variation']", Factorial Experiments for Random Effects and Mixed Models,seg_403,source of degrees of mean expected variation freedom square mean square
3543,1,['table'], Factorial Experiments for Random Effects and Mixed Models,seg_403,from table 14.14 we see that h0
3544,1,"['estimates', 'variance', 'unbiased']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"′ and h0 ′′ are tested by using s23 in the de′′′ nominator of the f-ratio, whereas h0 is tested using s2 in the denominator. the unbiased estimates of the variance components are"
3545,1,"['experiment', 'mean squares', 'mean', 'random', 'expected mean squares']", Factorial Experiments for Random Effects and Mixed Models,seg_403,table 14.15: expected mean squares for a three-factor random effects experiment
3546,1,"['mean square', 'mean', 'variation']", Factorial Experiments for Random Effects and Mixed Models,seg_403,source of degrees of mean expected variation freedom square mean square
3547,1,"['experiment', 'table', 'design', 'mean squares', 'completely randomized design', 'mean', 'random', 'expected mean squares']", Factorial Experiments for Random Effects and Mixed Models,seg_403,the expected mean squares for the three-factor experiment with random effects in a completely randomized design are shown in table 14.15. it is evident from the expected mean squares of table 14.15 that one can form appropriate f-ratios for
3548,1,"['interaction', 'variance', 'test', 'hypothesis']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"testing all two-factor and three-factor interaction variance components. however, to test a hypothesis of the form"
3549,1,"['interaction', 'mean square', 'mean', 'variance']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"there appears to be no appropriate f-ratio unless we have found one or more of the two-factor interaction variance components not significant. suppose, for example, that we have compared s52 (mean square ac) with s72 (mean square abc) and found σα"
3550,1,"['table', 'mean squares', 'mean', 'variance', 'expected mean squares', 'significance', 'test']", Factorial Experiments for Random Effects and Mixed Models,seg_403,dropped from all the expected mean squares of table 14.15; then the ratio s21/s24 provides a test for the significance of the variance component σα
3551,1,"['interaction', 'mean square', 'hypotheses', 'mean', 'variance', 'significance', 'test']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"are to test hypotheses concerning the variance components of the main effects, it is necessary first to investigate the significance of the two-factor interaction components. an approximate test derived by satterthwaite (1946; see the bibliography) may be used when certain two-factor interaction variance components are found to be significant and hence must remain a part of the expected mean square."
3552,1,"['estimates', 'measurements', 'significance', 'process', 'data', 'statistical', 'statistical test', 'level', 'variance', 'test', 'level of significance', 'response', 'percent', 'table', 'interaction', 'variation']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"example 14.6: in a study to determine which are the important sources of variation in an industrial process, 3 measurements are taken on yield for 3 operators chosen randomly and 4 batches of raw materials chosen randomly. it is decided that a statistical test should be made at the 0.05 level of significance to determine if the variance components due to batches, operators, and interaction are significant. in addition, estimates of variance components are to be computed. the data are given in table 14.16, with the response being percent by weight."
3553,1,['data'], Factorial Experiments for Random Effects and Mixed Models,seg_403,table 14.16: data for example 14.6
3554,1,['results'], Factorial Experiments for Random Effects and Mixed Models,seg_403,"solution : the sums of squares are found in the usual way, with the following results:"
3555,1,['error'], Factorial Experiments for Random Effects and Mixed Models,seg_403,"sst (total) = 84.5564, sse (error) = 10.6733,"
3556,1,['interaction'], Factorial Experiments for Random Effects and Mixed Models,seg_403,ss(ab) (interaction) = 5.5161.
3557,1,['table'], Factorial Experiments for Random Effects and Mixed Models,seg_403,all other computations are carried out and exhibited in table 14.17. since
3558,1,"['estimates', 'interaction', 'level', 'variance']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"we find the operator and batch variance components to be significant. although the interaction variance is not significant at the α = 0.05 level, the p-value is 0.095. estimates of the main effect variance components are"
3559,1,"['variance', 'analysis of variance']", Factorial Experiments for Random Effects and Mixed Models,seg_403,table 14.17: analysis of variance for example 14.6
3560,1,"['mean', 'variation']", Factorial Experiments for Random Effects and Mixed Models,seg_403,source of sum of degrees of mean computed variation squares freedom square f
3561,1,"['interaction', 'error']", Factorial Experiments for Random Effects and Mixed Models,seg_403,operators 18.2106 2 9.1053 9.90 batches 50.1564 3 16.7188 18.18 interaction 5.5161 6 0.9194 2.07 error 10.6733 24 0.4447
3562,1,"['model', 'factors', 'experiment', 'case', 'random']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"there are situations where the experiment dictates the assumption of a mixed model (i.e., a mixture of random and fixed effects). for example, for the case of two factors, we may have"
3563,1,"['model', 'random variables', 'independent', 'variables', 'interaction', 'independent random variables', 'hypotheses', 'interaction terms', 'random']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"for i = 1, 2, . . . , a; j = 1, 2, . . . , b; k = 1, 2, . . . , n. the ai may be independent random variables, independent of ijk, and the bj may be fixed effects. the mixed nature of the model requires that the interaction terms be random variables. as a result, the relevant hypotheses are of the form"
3564,1,"['model', 'table', 'mean squares', 'mean', 'random', 'expected mean squares']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"again, the computations of sums of squares are identical to those of fixed and random effects situations, and the f-test is dictated by the expected mean squares. table 14.18 provides the expected mean squares for the two-factor mixed model problem."
3565,1,"['model', 'experiment', 'mean squares', 'mean', 'expected mean squares']", Factorial Experiments for Random Effects and Mixed Models,seg_403,table 14.18: expected mean squares for two-factor mixed model experiment
3566,1,"['mean square', 'mean']", Factorial Experiments for Random Effects and Mixed Models,seg_403,factor expected mean square
3567,1,['random'], Factorial Experiments for Random Effects and Mixed Models,seg_403,a (random) σ2 + bnσα
3568,1,['random'], Factorial Experiments for Random Effects and Mixed Models,seg_403,n 1 ∑bj2 j ab (random) σ2 + nσα
3569,1,"['factors', 'factorial', 'factorial experiments', 'experiments']", Factorial Experiments for Random Effects and Mixed Models,seg_403,592 chapter 14 factorial experiments (two or more factors)
3570,1,"['error', 'factors', 'table', 'factor', 'interaction', 'mean square', 'mean squares', 'mean', 'random', 'expected mean squares', 'test', 'mean square error']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"from the nature of the expected mean squares it becomes clear that the test on the random effect employs the mean square error s2 as the denominator, whereas the test on the fixed effect uses the interaction mean square. suppose we now consider three factors. here, of course, we must take into account the situation where one factor is fixed and the situation in which two factors are fixed. table 14.19 covers both situations."
3571,1,"['model', 'factors', 'factorial', 'mean squares', 'factorial experiments', 'mean', 'expected mean squares', 'experiments']", Factorial Experiments for Random Effects and Mixed Models,seg_403,table 14.19: expected mean squares for mixed model factorial experiments in three factors
3572,1,['random'], Factorial Experiments for Random Effects and Mixed Models,seg_403,"a random a random, b random"
3573,1,"['experiment', 'case', 'random']", Factorial Experiments for Random Effects and Mixed Models,seg_403,"note that in the case of a random, all effects have proper f-tests. but in the case of a and b random, the main effect c must be tested using a satterthwaite-type procedure similar to that used in the random effects experiment."
3574,1,"['levels', 'plots', 'interaction', 'factorial', 'factorial experiments', 'interactions', 'response', 'experiments']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_409,"one of the most confusing issues in the analysis of factorial experiments resides in the interpretation of main effects in the presence of interaction. the presence of a relatively large p-value for a main effect when interactions are clearly present may tempt the analyst to conclude “no significant main effect.” however, one must understand that if a main effect is involved in a significant interaction, then the main effect is influencing the response. the nature of the effect is inconsistent across levels of other effects. the nature of the role of the main effect can be deduced from interaction plots."
3575,1,"['factors', 'interaction', 'statistics', 'multiple comparison test', 'test']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_409,"in light of what is communicated in the preceding paragraph, there is danger of a substantial misuse of statistics when one employs a multiple comparison test on main effects in the clear presence of interaction among the factors."
3576,1,"['plot', 'experimental', 'levels', 'factors', 'experiment', 'factor', 'design', 'randomization', 'factorial', 'factorial experiment']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_409,"one must be cautious in the analysis of a factorial experiment when the assumption of a complete randomized design is made when in fact complete randomization is not carried out. for example, it is common to encounter factors that are very difficult to change. as a result, factor levels may need to be held without change for long periods of time throughout the experiment. for instance, a temperature factor is a common example. moving temperature up and down in a randomization scheme is a costly plan, and most experimenters will refuse to do it. experimental designs with restrictions in randomization are quite common and are called split plot designs. they are beyond the scope of the book, but presentations are found in montgomery (2008a)."
3577,1,"['quantitative', 'prediction', 'factorial', 'process', 'experiment', 'results', 'data', 'factorial experiment', 'factors', 'regression', 'factorial experiments', 'interaction', 'regression analysis', 'randomization', 'experiments']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_409,"many of the concepts discussed in this chapter carry over into chapter 15 (e.g., the importance of randomization and the role of interaction in the interpretation of results). however, there are two areas covered in chapter 15 that represent an expansion of principles dealt with both in chapter 13 and in this chapter. in chapter 15, problem solving through the use of factorial experiments is done with regression analysis since most of the factors are assumed to be quantitative and measured on a continuum (e.g., temperature and time). prediction equations are developed from the data of the designed experiment, and they are used for process improvement or even process optimization. in addition, development is given on the topic of fractional factorials, in which only a portion or fraction of the entire factorial experiment is implemented due to the prohibitive cost of doing the entire experiment."
3578,1,"['experimental', 'design', 'homogeneous', 'randomization', 'normal', 'mean', 'population', 'experimental units', 'variance']", Introduction,seg_413,"we have already been exposed to certain experimental design concepts. the sampling plan for the simple t-test on the mean of a normal population and the analysis of variance involve randomly allocating pre-chosen treatments to experimental units. the randomized block design, where treatments are assigned to units within relatively homogeneous blocks, involves restricted randomization."
3579,1,"['levels', 'factor', 'design', 'case', 'factorial', 'experimental', 'factors', 'factorial experiments', 'level', 'response', 'variables', 'treatment', 'combinations', 'experiments']", Introduction,seg_413,"in this chapter, we give special attention to experimental designs in which the experimental plan calls for the study of the effect on a response of k factors, each at two levels. these are commonly known as 2k factorial experiments. we often denote the levels as “high” and “low” even though this notation may be arbitrary in the case of qualitative variables. the complete factorial design requires that each level of every factor occur with each level of every other factor, giving a total of 2k treatment combinations."
3580,1,"['model', 'rate', 'experimental', 'experiment', 'design', 'factors', 'response', 'level', 'response surface methodology', 'response surface']", Introduction,seg_413,"often, when experimentation is conducted either on a research or on a development level, a well-planned experimental design is a stage of what is truly a sequential plan of experimentation. more often than not, the scientists and engineers at the outset of a study may not be aware of which factors are important or what are appropriate ranges for the potential factors on which experimentation should be conducted. for example, in the text response surface methodology by myers, montgomery, and anderson-cook (2009), one example is given of an investigation of a pilot plant experiment in which four factors—temperature, pressure, concentration of formaldehyde, and steering rate—are varied in order to establish their influence on the response, filtration rate of a certain chemical product. even at the pilot plant level, the scientists are not certain if all four factors should be involved in the model. in addition, the eventual goal is to determine the proper settings of contributing factors that maximize the filtration rate. thus, there is a need"
3581,1,"['experimental', 'method']", Introduction,seg_413,"to determine the proper region of experimentation. these questions can be answered only if the total experimental plan is done sequentially. many experimental endeavors are plans that feature iterative learning, the type of learning that is consistent with the scientific method, with the word iterative implying stage-wise experimentation."
3582,1,"['experimental', 'factors', 'factor', 'design', 'results', 'information', 'variable', 'experiments', 'process']", Introduction,seg_413,"generally, the initial stage of the ideal sequential plan is variable or factor screening, a procedure that involves an inexpensive experimental design using the candidate factors. this is particularly important when the plan involves a complex system like a manufacturing process. the information received from the results of a screening design is used to design one or more subsequent experiments in which adjustments in the important factors are made, the adjustments that provide improvements in the system or process."
3583,1,"['graphical', 'factorial', 'factorial experiments', 'experiments']", Introduction,seg_413,"the 2k factorial experiments and fractions of the 2k are powerful tools that are ideal screening designs. they are simple, practical, and intuitively appealing. many of the general concepts discussed in chapter 14 continue to apply. however, there are graphical methods that provide useful intuition in the analysis of the two-level designs."
3584,1,"['graphical', 'factor', 'utility', 'regression analysis', 'factorial', 'regression', 'analysis of variance', 'variance']", Introduction,seg_413,"when k is small, say k = 2 or even k = 3, the utility of the 2k factorial for factor screening is clear. analysis of variance and/or regression analysis as discussed and illustrated in chapters 12, 13, and 14 remain useful as tools. in addition, graphical approaches are helpful."
3585,1,"['experimental', 'factors', 'factor', 'design', 'factorial', 'information', 'combinations']", Introduction,seg_413,"if k is large, say as large as 6, 7, or 8, the number of factor combinations and thus experimental runs required for the 2k factorial often becomes prohibitive. for example, suppose one is interested in carrying out a screening design involving k = 8 factors. there may be interest in gaining information on all k = 8 main"
3586,1,['interactions'], Introduction,seg_413,"k(k−1) effects as well as the = 28 two-factor interactions. however, including"
3587,1,"['efficient', '2k factorial experiment', 'experiment', 'design', 'factorial', 'information', 'interactions', 'factorial experiment']", Introduction,seg_413,"2 28 = 256 runs would appear to make the study much too large and be wasteful for studying 28 + 8 = 36 effects. but, as we will illustrate in future sections, when k is large we can gain considerable information in an efficient manner by using only a fraction of the complete 2k factorial experiment. this class of designs is the class of fractional factorial designs. the goal is to retain high-quality information on main effects and interesting interactions even though the size of the design is reduced considerably."
3588,1,"['experimental', 'factors', 'factor', 'design', 'combination', 'case', 'factorial', 'design point', 'level']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"consider initially a 22 factorial with factors a and b and n experimental observations per factor combination. it is useful to use the symbols (1), a, b, and ab to signify the design points, where the presence of a lowercase letter implies that the factor (a or b) is at the high level. thus, absence of the lower case implies that the factor is at the low level. so ab is the design point (+,+), a is (+,−), b is (−,+) and (1) is (−,−). there are situations in which the notation also stands"
3589,1,"['factors', 'table', 'design', 'data', 'design point', 'analysis of variance', 'variance', 'response']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"for the response data at the design point in question. as an introduction to the calculation of important effects that aid in the determination of the influence of the factors and sums of squares that are incorporated into analysis of variance computations, we have table 15.1."
3590,1,"['factorial experiment', 'factorial', 'experiment']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,table 15.1: a 22 factorial experiment
3591,1,['mean'], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,a mean b+ab b ab 2n b a (1)+a { (1) 2n (1)+b a+ab mean 2n 2n
3592,1,"['experimental error', 'experimental', 'design', 'treatment', 'interaction', 'table', 'contrasts', 'factorial', 'information', 'degree of freedom', 'response', 'error']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"in this table, (1), a, b, and ab signify totals of the n response values at the individual design points. the simplicity of the 22 factorial lies in the fact that apart from experimental error, important information comes to the analyst in single-degree-of-freedom components, one each for the two main effects a and b and one degree of freedom for interaction ab. the information retrieved on all these takes the form of three contrasts. let us define the following contrasts among the treatment totals:"
3593,1,"['contrast', 'contrast ']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"a contrast = ab+ a− b− (1),"
3594,1,"['contrast', 'contrast ']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"b contrast = ab− a+ b− (1),"
3595,1,"['contrast', 'contrast ']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,ab contrast = ab− a− b+ (1).
3596,1,"['contrasts', 'experiment']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,the three effects from the experiment involve these contrasts and appeal to common sense and intuition. the two computed main effects are of the form
3597,1,"['level', 'average', 'response']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"where ȳh and ȳl are average response at the high, or “+” level and average response at the low, or “−” level, respectively. as a result,"
3598,1,['contrast'], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,calculation of ab+ a− b− (1) a contrast a = = main effects 2n 2n
3599,1,['contrast'], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,ab− a+ b− (1) b contrast b = = . 2n 2n
3600,1,"['levels', 'table', 'factor', 'interaction', 'data', 'mean']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"the quantity a is seen to be the difference between the mean responses at the low and high levels of factor a. in fact, we call a the main effect of factor a. similarly, b is the main effect of factor b. apparent interaction in the data is observed by inspecting the difference between ab− b and a− (1) or between ab− a and b− (1) in table 15.1. if, for example,"
3601,1,"['contrast', 'evaluating', 'factor', 'treatment', 'interaction', 'contrasts', 'level of factor', 'level', 'test']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"a line connecting the responses for each level of factor a at the high level of factor b will be approximately parallel to a line connecting the responses for each level of factor a at the low level of factor b. the nonparallel lines of figure 15.1 suggest the presence of interaction. to test whether this apparent interaction is significant, a third contrast in the treatment totals orthogonal to the main effect contrasts, called the interaction effect, is constructed by evaluating"
3602,1,['contrast'], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,interaction effect ab− a− b+ (1) ab contrast ab = = . 2n 2n
3603,1,"['interaction', 'response']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,figure 15.1: response suggesting apparent interaction.
3604,1,"['experiment', 'data', 'factorial', 'tables', 'factorial experiment']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,example 15.1: consider the data in tables 15.2 and 15.3 with n = 1 for a 22 factorial experiment.
3605,1,"['table', 'interaction', 'factorial']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,table 15.2: 22 factorial with no interaction table 15.3: 22 factorial with interaction
3606,1,"['levels', 'table', 'factor', 'interaction', 'contrasts', 'information', 'tables']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"the numbers in the cells in tables 15.2 and 15.3 clearly illustrate how contrasts and the resulting calculation of the two main effects and resulting conclusions can be highly influenced by the presence of interaction. in table 15.2, the effect of a is −30 at both the low and high levels of factor b and the effect of b is 20 at both the low and high levels of factor a. this “consistency of effect” (no interaction) can be very important information to the analyst. the main effects are"
3607,1,['interaction'], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,while the interaction effect is
3608,1,"['levels', 'table', 'interaction', 'cases', 'level']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"on the other hand, in table 15.3 the effect of a is once again −30 at the low level of b but +30 at the high level of b. this “inconsistency of effect” (interaction) also is present for b across levels of a. in these cases, the main effects can be meaningless and, in fact, highly misleading. for example, the effect of a is"
3609,1,"['levels', 'interaction']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,since there is a complete “masking” of the effect as one averages over levels of b. the strong interaction is illustrated by the calculated effect
3610,1,"['plot', 'plots', 'interaction', 'tables']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,here it is convenient to illustrate the scenarios of tables 15.2 and 15.3 with interaction plots. note the parallelism in the plot of figure 15.2 and the interaction that is apparent in figure 15.3.
3611,1,"['plot', 'table', 'interaction', 'data']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,figure 15.2: interaction plot for data of figure 15.3: interaction plot for data of table 15.2. table 15.3.
3612,1,"['factorial', 'random', 'combinations', 'experiment', 'random variation', 'tests', 'factorial experiment', 'model', 'experimental', 'systematic variation', 'contrasts', 'contrast', '2k factorial experiment', 'treatment', 'interaction', 'normality', 'degree of freedom', 'variation']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"we take advantage of the fact that in the 22 factorial, or for that matter in the general 2k factorial experiment, each main effect and interaction effect has an associated single degree of freedom. therefore, we can write 2k − 1 orthogonal single-degree-of-freedom contrasts in the treatment combinations, each accounting for variation due to some main or interaction effect. thus, under the usual independence and normality assumptions in the experimental model, we can make tests to determine if the contrast reflects systematic variation or merely chance or random variation. the sums of squares for each contrast are found by following the procedures given in section 13.5. writing"
3613,1,['observations'], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"where y1.. and y2.. are the total of 2n observations, we have"
3614,0,[], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,[ab+ a− b− (1)]2 (a contrast)2 (
3615,1,['degree of freedom'], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"with 1 degree of freedom. similarly, we find that"
3616,0,[], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,[ab+ b− a− (1)]2 (b contrast)2 ssb = = 2
3617,0,[], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,[ab+ (1)− a− b]2 (ab contrast)2 ss(ab) = = . 2
3618,1,"['degrees of freedom', 'contrast', 'sum of squares', 'error sum of squares', 'degree of freedom', 'error']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"each contrast has l degree of freedom, whereas the error sum of squares, with 22(n− 1) degrees of freedom, is obtained by subtraction from the formula"
3619,1,"['contrast', 'levels', 'factors', 'table', 'factor', 'combination', 'treatment', 'interaction', 'contrasts', 'combinations', 'level']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"in computing the sums of squares for the main effects a and b and the interaction effect ab, it is convenient to present the total responses of the treatment combinations along with the appropriate algebraic signs for each contrast, as in table 15.4. the main effects are obtained as simple comparisons between the low and high levels. therefore, we assign a positive sign to the treatment combination that is at the high level of a given factor and a negative sign to the treatment combination at the low level. the positive and negative signs for the interaction effect are obtained by multiplying the corresponding signs of the contrasts of the interacting factors."
3620,1,"['experiment', 'contrasts', 'factorial', 'factorial experiment']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,table 15.4: signs for contrasts in a 22 factorial experiment
3621,1,['factorial'], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,treatment factorial effect
3622,0,[], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,combination a b ab
3623,1,"['contrast', 'levels', 'factors', 'experiment', 'table', 'treatment', 'interaction', 'factorial', 'combinations', 'factorial experiment']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"let us now consider an experiment using three factors, a, b, and c, each with levels −1 and +1. this is a 23 factorial experiment giving the eight treatment combinations (1), a, b, c, ab, ac, bc, and abc. the treatment combinations and the appropriate algebraic signs for each contrast used in computing the sums of squares for the main effects and interaction effects are presented in table 15.5."
3624,1,"['experiment', 'contrasts', 'factorial', 'factorial experiment']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,table 15.5: signs for contrasts in a 23 factorial experiment
3625,1,"['treatment', 'combination']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,factorial effect (symbolic) treatment combination a b c ab ac bc abc
3626,1,['geometric'], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,figure 15.4: geometric view of 23.
3627,1,"['design', 'factorial']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"it is helpful to discuss and illustrate the geometry of the 23 factorial much as we illustrated that of the 22 factorial in figure 15.1. for the 23, the eight design points represent the vertices of a cube, as shown in figure 15.4."
3628,1,"['table', 'design', 'contrasts', 'case']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"the columns of table 15.5 represent the signs that are used for the contrasts and thus computation of seven effects and corresponding sums of squares. these columns are analogous to those given in table 15.4 for the case of the 22. seven effects are available since there are eight design points. for example,"
3629,0,[], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,and so on. the sums of squares are merely given by
3630,0,[], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,(contrast)2 ss(effect) = . 2
3631,1,"['table', 'experiment', 'contrasts']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,an inspection of table 15.5 reveals that for the 23 experiment all contrasts
3632,0,[], The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"among the seven are mutually orthogonal, and therefore the seven effects are assessed independently."
3633,1,"['2k factorial experiment', 'experiment', 'treatment', 'interaction', 'factorial', 'combinations', 'replications', 'factorial experiment']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"for a 2k factorial experiment the single-degree-of-freedom sums of squares for the main effects and interaction effects are obtained by squaring the appropriate contrasts in the treatment totals and dividing by 2kn, where n is the number of replications of the treatment combinations."
3634,1,"['table', 'information', 'interactions', 'level', 'average', 'response']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"as before, an effect is always calculated by subtracting the average response at the “low” level from the average response at the “high” level. the high and low for main effects are quite clear. the symbolic high and low for interactions are evident from information as in table 15.5."
3635,1,"['factor', 'factorial', 'sum of squares', 'independence', 'associated', 'experiment', 'factorial experiment', 'contrasts', 'level', 'contrast', 'estimated', 'independent', 'table', 'treatment', 'combinations']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,"the orthogonality property has the same importance here as it does for the material on comparisons discussed in chapter 13. orthogonality of contrasts implies that the estimated effects and thus the sums of squares are independent. this independence is readily illustrated in the 23 factorial experiment if the responses, with factor a at its high level, are increased by an amount x in table 15.5. only the a contrast leads to a larger sum of squares, since the x effect cancels out in the formation of the six remaining contrasts as a result of the two positive and two negative signs associated with treatment combinations in which a is at the high level."
3636,1,"['2k factorial experiment', 'experiment', 'factorial', 'regression', 'factorial experiment']", The k Factorial Calculation of Effects and Analysis of Variance,seg_415,there are additional advantages produced by orthogonality. these are pointed out when we discuss the 2k factorial experiment in regression situations.
3637,1,"['degrees of freedom', 'model', 'combination', 'experiment', 'factor', 'data', 'factorial', 'replication', 'interactions', 'error']", Nonreplicated k Factorial Experiment,seg_417,"the full 2k factorial may often involve considerable experimentation, particularly when k is large. as a result, replication of each factor combination is often not feasible. if all effects, including all interactions, are included in the model of the experiment, no degrees of freedom are allowed for error. often, when k is large, the data analyst will pool sums of squares and corresponding degrees of freedom for high-order interactions that are known or assumed to be negligible. this will produce f-tests for main effects and lower-order interactions."
3638,1,"['estimates', 'factorial', 'replication', 'probability', 'random', 'experiment', 'data', 'mean', 'factorial experiment', 'distribution', 'interactions', 'variance', 'plotting', 'plot', 'independent', 'normal', 'probability plotting', 'normal distribution']", Nonreplicated k Factorial Experiment,seg_417,"normal probability plotting can be a very useful methodology for determining the relative importance of effects in a reasonably large two-level factored experiment when there is no replication. this type of diagnostic plot can be particularly useful when the data analyst is hesitant to pool high-order interactions for fear that some of the effects pooled in the “error” may truly be real effects and not merely random. the reader should bear in mind that all effects that are not real (i.e., they are independent estimates of zero) follow a normal distribution with mean near zero and constant variance. for example, in a 24 factorial experiment, we are reminded that all effects (keep in mind that n = 1) are of the form"
3639,1,"['probability', 'random', 'experimental', 'level', 'variance', 'plotting', 'contrast', 'random variables', 'independent', 'variables', 'normal', 'probability plotting', 'average']", Nonreplicated k Factorial Experiment,seg_417,"where ȳh is the average of eight independent experimental runs at the high, or “+,” level and ȳl is the average of eight independent runs at the low, or “−,” level. thus, the variance of each contrast is var(ȳh − ȳl) = σ2/4. for any real effects, e(ȳh − ȳl) = 0. thus, normal probability plotting should reveal “significant” effects as those that fall off the straight line that depicts realizations of independent, identically distributed normal random variables."
3640,1,"['plot', 'probability plots', 'probability paper', 'normal probability plots', 'plots', 'normal', 'probability', 'probability plotting', 'plotting']", Nonreplicated k Factorial Experiment,seg_417,"the probability plotting can take one of many forms. the reader is referred to chapter 8, where these plots were first presented. the empirical normal quantilequantile plot may be used. the plotting procedure that makes use of normal probability paper may also be used. in addition, there are several other types of diagnostic normal probability plots. in summary, the procedure for diagnostic effect plots is as follows."
3641,0,[], Nonreplicated k Factorial Experiment,seg_417,probability effect 1. calculate effects as
3642,1,['contrast'], Nonreplicated k Factorial Experiment,seg_417,plots for contrast nonreplicated 24 effect = . 2
3643,1,['factorial'], Nonreplicated k Factorial Experiment,seg_417,k−1 factorial
3644,1,"['plot', 'probability plot', 'normal', 'probability', 'normal probability plot']", Nonreplicated k Factorial Experiment,seg_417,experiments 2. construct a normal probability plot of all effects.
3645,0,[], Nonreplicated k Factorial Experiment,seg_417,3. effects that fall off the straight line should be considered real effects.
3646,1,"['experiment', 'results', 'plots', 'data', 'normal', 'interactions', 'probability', 'probability plotting', 'experiments', 'plotting']", Nonreplicated k Factorial Experiment,seg_417,"further comments regarding normal probability plotting of effects are in order. first, the data analyst may feel frustrated if he or she uses these plots with a small experiment. on the other hand, the plotting is likely to give satisfying results when there is effect sparsity—many effects that are truly not real. this sparsity will be evident in large experiments where high-order interactions are not likely to be real."
3647,1,"['experimental', 'levels', 'factors', 'experiment', 'states', 'nominal', 'deviations', 'process', 'response']", Nonreplicated k Factorial Experiment,seg_417,"case study 15.1: injection molding: many manufacturing companies in the united states and abroad use molded parts as components. shrinkage is often a major problem. often, a molded die for a part is built larger than nominal to allow for part shrinkage. in the following experimental situation, a new die is being produced, and ultimately it is important to find the proper process settings to minimize shrinkage. in the following experiment, the response values are deviations from nominal (i.e., shrinkage). the factors and levels are as follows:"
3648,1,['levels'], Nonreplicated k Factorial Experiment,seg_417,coded levels
3649,1,"['factors', 'experiment', 'table', 'interaction', 'data', 'factorial', 'factorial experiment']", Nonreplicated k Factorial Experiment,seg_417,"the purpose of the experiment was to determine what effects (main effects and interaction effects) influence shrinkage. the experiment was considered a preliminary screening experiment from which the factors for a more complete analysis might be determined. also, it was hoped that some insight might be gained into how the important factors impact shrinkage. the data from a nonreplicated 24 factorial experiment are given in table 15.6."
3650,1,"['case', 'data']", Nonreplicated k Factorial Experiment,seg_417,table 15.6: data for case study 15.1
3651,1,"['factor', 'response', 'combination']", Nonreplicated k Factorial Experiment,seg_417,factor response factor response combination (cm × 104) combination (cm × 104)
3652,1,"['plot', 'probability plot', 'normal', 'probability', 'normal probability plot']", Nonreplicated k Factorial Experiment,seg_417,"initially, effects were calculated and placed on a normal probability plot. the calculated effects are as follows:"
3653,1,"['plot', 'normal']", Nonreplicated k Factorial Experiment,seg_417,"the normal quantile-quantile plot is shown in figure 15.5. the plot seems to imply that effects a, b, and ab stand out as being important. the signs of the important effects indicate the preliminary conclusions."
3654,1,['quantiles'], Nonreplicated k Factorial Experiment,seg_417,effects quantiles
3655,1,"['plot', 'case', 'normal']", Nonreplicated k Factorial Experiment,seg_417,figure 15.5: normal quantile-quantile plot of effects for case study 15.1.
3656,0,[], Nonreplicated k Factorial Experiment,seg_417,1. an increase in injection velocity from 1.0 to 2.0 increases shrinkage.
3657,0,[], Nonreplicated k Factorial Experiment,seg_417,2. an increase in mold temperature from 100◦c to 150◦c increases shrinkage.
3658,1,['interaction'], Nonreplicated k Factorial Experiment,seg_417,3. there is an interaction between injection velocity and mold temperature; al-
3659,1,['interaction'], Nonreplicated k Factorial Experiment,seg_417,"though both main effects are important, it is crucial that we understand the impact of the two-factor interaction."
3660,1,"['interaction', 'table']", Nonreplicated k Factorial Experiment,seg_417,"as one would expect, a two-way table of means provides ease in interpretation of the ab interaction. consider the two-factor situation in table 15.7."
3661,1,['interaction'], Nonreplicated k Factorial Experiment,seg_417,table 15.7: illustration of two-factor interaction
3662,0,[], Nonreplicated k Factorial Experiment,seg_417,b (temperature)
3663,1,"['sample', 'sample mean', 'results', 'interaction', 'mean', 'level', 'control']", Nonreplicated k Factorial Experiment,seg_417,"notice that the large sample mean at high velocity and high temperature created the significant interaction. the shrinkage increases in a nonadditive manner. mold temperature appears to have a positive effect despite the velocity level. but the effect is greatest at high velocity. the velocity effect is very slight at low temperature but clearly is positive at high mold temperature. to control shrinkage at a low level, one should avoid using high injection velocity and high mold temperature simultaneously. all of these results are illustrated graphically in figure 15.6."
3664,0,[], Nonreplicated k Factorial Experiment,seg_417,90 eg velocity ak 85
3665,1,"['plot', 'interaction', 'case']", Nonreplicated k Factorial Experiment,seg_417,figure 15.6: interaction plot for case study 15.1.
3666,1,"['glm', 'plot', 'error', 'probability plot', 'data', 'mean square', 'normal', 'analysis of variance', 'mean', 'probability', 'normal probability plot', 'interactions', 'variance', 'mean square error']", Nonreplicated k Factorial Experiment,seg_417,it may be of interest to observe an analysis of variance of the injection molding data with high-order interactions pooled to form a mean square error. interactions of order three and four are pooled. figure 15.7 shows a sas proc glm printout. the analysis of variance reveals essentially the same conclusion as that of the normal probability plot.
3667,1,"['tests', 'levels', 'factors', 'interactions', 'level', 'significance']", Nonreplicated k Factorial Experiment,seg_417,the tests and p-values shown in figure 15.7 require interpretation. a significant p-value suggests that the effect differs significantly from zero. the tests on main effects (which in the presence of interactions may be regarded as the effects averaged over the levels of the other factors) indicate significance for effects a and b. the signs of the effects are also important. an increase in the level from low
3668,1,"['glm', 'mse', 'model', 'dependent variable', 'estimate', 'dependent', 'intercept', 'variable', 'mean square', 'mean', 'parameter', 'standard', 'error']", Nonreplicated k Factorial Experiment,seg_417,the glm procedure dependent variable: y sum of source df squares mean square f value pr > f model 10 1689.237462 168.923746 9.37 0.0117 error 5 90.180831 18.036166 corrected total 15 1779.418294 r-square coeff var root mse y mean 0.949320 5.308667 4.246901 79.99938 source df type iii ss mean square f value pr > f a 1 446.1600062 446.1600062 24.74 0.0042 b 1 619.6365563 619.6365563 34.36 0.0020 c 1 23.3047563 23.3047563 1.29 0.3072 d 1 18.3826563 18.3826563 1.02 0.3590 a*b 1 520.1820562 520.1820562 28.84 0.0030 a*c 1 6.3630063 6.3630063 0.35 0.5784 a*d 1 13.3042562 13.3042562 0.74 0.4297 b*c 1 13.1950562 13.1950562 0.73 0.4314 b*d 1 20.7708062 20.7708062 1.15 0.3322 c*d 1 7.9383063 7.9383063 0.44 0.5364 standard parameter estimate error t value pr > |t| intercept 79.99937500 1.06172520 75.35 <.0001 a 5.28062500 1.06172520 4.97 0.0042 b 6.22312500 1.06172520 5.86 0.0020 c 1.20687500 1.06172520 1.14 0.3072 d 1.07187500 1.06172520 1.01 0.3590 a*b 5.70187500 1.06172520 5.37 0.0030 a*c 0.63062500 1.06172520 0.59 0.5784 a*d -0.91187500 1.06172520 -0.86 0.4297 b*c 0.90812500 1.06172520 0.86 0.4314 b*d -1.13937500 1.06172520 -1.07 0.3322 c*d 0.70437500 1.06172520 0.66 0.5364
3669,1,"['case', 'data']", Nonreplicated k Factorial Experiment,seg_417,figure 15.7: sas printout for data of case study 15.1.
3670,1,"['levels', 'factors', 'table', 'results', 'interaction']", Nonreplicated k Factorial Experiment,seg_417,"to high of a, injection velocity, results in increased shrinkage. the same is true for b. however, because of the significant interaction ab, main effect interpretations may be viewed as trends across the levels of the other factors. the impact of the significant ab interaction is better understood by using a two-way table of means."
3671,1,"['levels', 'factorial', 'function', 'experiment', 'data', 'factorial experiment', 'model', 'factors', 'analysis of variance', 'variance', 'processes', 'concentration', 'method', 'variables']", Factorial Experiments in a Regression Setting,seg_421,"thus far in this chapter, we have mostly confined our discussion of analysis of the data for a 2k factorial to the method of analysis of variance. the only reference to an alternative analysis resides in exercise 15.9. indeed, this exercise introduces much of what motivates the present section. there are situations in which model fitting is important and the factors under study can be controlled. for example, a biologist may wish to study the growth of a certain type of algae in the water, and so a model that looks at units of algae as a function of the amount of a pollutant and, say, time would be very helpful. thus, the study involves a factorial experiment in a laboratory setting in which concentration of the pollutant and time are the factors. as we shall discuss later in this section, a more precise model can be fitted if the factors are controlled in a factorial array, with the 2k factorial often being a useful choice. in many biological and chemical processes, the levels of the regressor variables can and should be controlled."
3672,1,"['model', 'regression model', 'regression']", Factorial Experiments in a Regression Setting,seg_421,recall that the regression model employed in chapter 12 can be written in matrix notation as
3673,1,"['model', 'experiment', 'variables', 'factorial', 'factorial experiment']", Factorial Experiments in a Regression Setting,seg_421,"the x matrix is referred to as the model matrix. suppose, for example, that a 23 factorial experiment is employed with the variables"
3674,0,[], Factorial Experiments in a Regression Setting,seg_421,temperature: 150◦c 200◦c humidity: 15% 20% pressure (psi): 1000 1500
3675,1,"['design', 'levels']", Factorial Experiments in a Regression Setting,seg_421,"the familiar +1, −1 levels can be generated through the following centering and scaling to design units:"
3676,0,[], Factorial Experiments in a Regression Setting,seg_421,"temperature− 175 humidity− 17.5 pressure− 1250 x1 = , x2 = , x3 = ."
3677,0,[], Factorial Experiments in a Regression Setting,seg_421,"as a result, the x matrix becomes"
3678,1,['design'], Factorial Experiments in a Regression Setting,seg_421,x1 x2 x3 design identification
3679,1,"['regression coefficients', 'contrasts', 'regression', 'coefficients']", Factorial Experiments in a Regression Setting,seg_421,"it is now seen that the contrasts illustrated and discussed in section 15.2 are directly related to regression coefficients. notice that all the columns of the x matrix in our 23 example are orthogonal. as a result, the computation of regression coefficients as described in section 12.3 becomes"
3680,1,['response'], Factorial Experiments in a Regression Setting,seg_421,"where a, ab, and so on, are response measures."
3681,1,"['model', 'experimental', 'regression model', 'quantitative', 'factors', 'design', 'regression coefficients', 'regression', 'design point', 'coefficients']", Factorial Experiments in a Regression Setting,seg_421,"one can now see that the notion of calculated main effects, which has been emphasized throughout this chapter with 2k factorials, is related to coefficients in a fitted regression model when factors are quantitative. in fact, for a 2k with, say, n experimental runs per design point, the relationships between effects and regression coefficients are as follows:"
3682,1,"['coefficient', 'regression coefficient', 'regression']", Factorial Experiments in a Regression Setting,seg_421,contrast effect regression coefficient = = . 2k(n) 2
3683,1,"['rate', 'design', 'regression', 'variable', 'regression coefficient', 'response', 'coefficient', 'average']", Factorial Experiments in a Regression Setting,seg_421,"this relationship should make sense to the reader, since a regression coefficient bj is an average rate of change in response per unit change in xj . of course, as one goes from −1 to +1 in xj (low to high), the design variable changes by 2 units."
3684,1,"['linear', 'model', 'regression model', 'factors', 'experiment', 'estimate', 'table', 'data', 'regression', 'linear regression', 'multiple linear regression', 'linear regression model']", Factorial Experiments in a Regression Setting,seg_421,example 15.2: consider an experiment where an engineer desires to fit a linear regression of yield y against holding time x1 and flexing time x2 in a certain chemical system. all other factors are held fixed. the data in the natural units are given in table 15.8. estimate the multiple linear regression model.
3685,1,"['model', 'regression model', 'regression']", Factorial Experiments in a Regression Setting,seg_421,solution : the fitted regression model is
3686,1,['data'], Factorial Experiments in a Regression Setting,seg_421,table 15.8: data for example 15.2
3687,0,[], Factorial Experiments in a Regression Setting,seg_421,holding time (hr) flexing time (hr) yield (%)
3688,1,['design'], Factorial Experiments in a Regression Setting,seg_421,the design units are
3689,0,[], Factorial Experiments in a Regression Setting,seg_421,and the x matrix is x1 x2
3690,1,"['regression coefficients', 'regression', 'coefficients']", Factorial Experiments in a Regression Setting,seg_421,with the regression coefficients
3691,1,"['least squares regression', 'least squares', 'regression']", Factorial Experiments in a Regression Setting,seg_421,"thus, the least squares regression equation is"
3692,1,"['rate', 'experimental', 'estimated', 'coefficients', 'experiment', 'design', 'regression coefficients', 'factorial', 'regression', 'response', 'percent']", Factorial Experiments in a Regression Setting,seg_421,"this example provides an illustration of the use of the two-level factorial experiment in a regression setting. the four experimental runs in the 22 design were used to calculate a regression equation, with the obvious interpretation of the regression coefficients. the value b1 = 6.25 represents the estimated increase in response (percent yield) per design unit change (0.15 hour) in holding time. the value b2 = 2.75 represents a similar rate of change for flexing time."
3693,1,"['model', 'interaction', 'contrasts', 'regression', 'interactions']", Factorial Experiments in a Regression Setting,seg_421,"the interaction contrasts discussed in section 15.2 have definite interpretations in the regression context. in fact, interactions are accounted for in regression models by product terms. for example, in example 15.2, the model with interaction is"
3694,1,"['linear', 'interaction', 'regression']", Factorial Experiments in a Regression Setting,seg_421,"thus, the regression equation expressing two linear main effects and interaction is"
3695,1,"['case', 'factorial', 'independence', 'coefficients', 'experiment', 'data', 'population', 'factorial experiment', 'regression', 'analysis of variance', 'variance', 'anova', '2k factorial experiment', 'variables', 'regression coefficients', 'hypotheses']", Factorial Experiments in a Regression Setting,seg_421,"the regression context provides a framework in which the reader should better understand the advantage of orthogonality that is enjoyed by the 2k factorial. in section 15.2, the merits of orthogonality were discussed from the point of view of analysis of variance of the data in a 2k factorial experiment. it was pointed out that orthogonality among effects leads to independence among the sums of squares. of course, the presence of regression variables certainly does not rule out the use of analysis of variance. in fact, f-tests are conducted just as they were described in section 15.2. of course, a distinction must be made. in the case of anova, the hypotheses evolve from population means, while in the regression case, the hypotheses involve regression coefficients."
3696,1,"['levels', 'experimental', 'factor', 'design', 'continuous']", Factorial Experiments in a Regression Setting,seg_421,"for instance, consider the experimental design in exercise 15.2 on page 609. each factor is continuous. suppose that the levels are"
3697,1,"['design', 'levels']", Factorial Experiments in a Regression Setting,seg_421,"and we have, for design levels,"
3698,0,[], Factorial Experiments in a Regression Setting,seg_421,"% solids− 30 flow rate− 7.5 ph− 5.25 x1 = , x2 = , x3 = . 10 2.5 0.25"
3699,1,"['linear', 'model', 'levels', 'regression model', 'factor', 'case', 'regression', 'multiple regression', 'interactions', 'response', 'coefficients']", Factorial Experiments in a Regression Setting,seg_421,"suppose that it is of interest to fit a multiple regression model in which all linear coefficients and available interactions are to be considered. in addition, the engineer wants to obtain some insight into what levels of the factor will maximize cleansing (i.e., maximize the response). this problem will be the subject of case study 15.2."
3700,1,"['model', 'regression analysis', 'regression']", Factorial Experiments in a Regression Setting,seg_421,case study 15.2: coal cleansing experiment1: figure 15.9 represents annotated computer printout for the regression analysis for the fitted model
3701,0,[], Factorial Experiments in a Regression Setting,seg_421,"ŷ = b0 + b1x1 + b2x2 + b3x3 + b12x1x2 + b13x1x3 + b23x2x3 + b123x1x2x3,"
3702,1,"['rate', 'percent']", Factorial Experiments in a Regression Setting,seg_421,"where x1, x2, and x3 are percent solids, flow rate, and ph of the system, respectively. the computer system used is sas proc reg."
3703,1,"['confidence intervals', 'prediction intervals', 'estimates', 'prediction', 'residuals', 'coefficients', 'intervals', 'parameter', 'standard', 'confidence', 'error', 'model', 'regression', 'standard error', 'interaction']", Factorial Experiments in a Regression Setting,seg_421,"note the parameter estimates, standard error, and p-values in the printout. the parameter estimates represent coefficients in the model. all model coefficients are significant except the x2x3 term (bc interaction). note also that residuals, confidence intervals, and prediction intervals appear as discussed in the regression material in chapters 11 and 12."
3704,1,"['model', 'rate', 'coefficients', 'factors', 'combination', 'factor', 'predicted', 'results', 'efficiency', 'coefficient', 'percent']", Factorial Experiments in a Regression Setting,seg_421,"the reader can use the values of the model coefficients and predicted values from the printout to ascertain what combination of the factors results in maximum cleansing efficiency. factor a (percent solids circulated) has a large positive coefficient, suggesting a high value for percent solids. in addition, a low value for factor c (ph of the tank) is suggested. though the b main effect (flow rate of the polymer) coefficient is positive, the rather large positive coefficient of"
3705,1,"['case', 'data']", Factorial Experiments in a Regression Setting,seg_421,figure 15.9: sas printout for data of case study 15.2.
3706,1,"['model', 'rate', 'regression model', 'factors', 'combination', 'results', 'regression', 'level']", Factorial Experiments in a Regression Setting,seg_421,"x1x2x3 (abc) suggests that flow rate should be at the low level to enhance efficiency. indeed, the regression model generated in the sas printout suggests that the combination of factors that may produce optimum results, or perhaps suggest direction for further experimentation, is given by"
3707,1,['level'], Factorial Experiments in a Regression Setting,seg_421,a: high level b: low level c: low level
3708,1,"['linear', 'experimental', 'variables', 'design', 'factorial', 'interactions', 'mean']", The Orthogonal Design,seg_423,"in experimental situations where it is appropriate to fit models that are linear in the design variables and possibly should involve interactions or product terms, there are advantages gained from the two-level orthogonal design, or orthogonal array. by an orthogonal design we mean one in which there is orthogonality among the columns of the x matrix. for example, consider the x matrix for the 22 factorial of example 15.2. notice that all three columns are mutually orthogonal. the x matrix for the 23 factorial also contains orthogonal columns. the 23 factorial with interactions would yield an x matrix of the type"
3709,1,['degrees of freedom'], The Orthogonal Design,seg_423,the outline of degrees of freedom is
3710,1,"['lack of fit', 'error', 'regression']", The Orthogonal Design,seg_423,"source d.f. regression 3 lack of fit 4 (x1x2, x1x3, x2x3, x1x2x3) error (pure) 8"
3711,1,"['degrees of freedom', 'model', 'design', 'case', 'design point', 'error']", The Orthogonal Design,seg_423,"the 8 degrees of freedom for pure error are obtained from the duplicate runs at each design point. lack-of-fit degrees of freedom may be viewed as the difference between the number of distinct design points and the number of total model terms; in this case, there are 8 points and 4 model terms."
3712,1,"['standard errors', 'coefficients', 'experiment', 'design', 'estimates', 'minimum variance', 'errors', 'regression', 'standard', 'coefficient', 'variance', 'variances']", The Orthogonal Design,seg_423,"in previous sections, we showed how the designer of an experiment may exploit the notion of orthogonality to design a regression experiment with coefficients that attain minimum variance on a per cost basis. we should be able to make use of our exposure to regression in section 12.4 to compute estimates of variances of coefficients and hence their standard errors. it is also of interest to note the relationship between the t-statistic on a coefficient and the f-statistic described and illustrated in previous chapters."
3713,1,"['variances', 'coefficients']", The Orthogonal Design,seg_423,"recall from section 12.4 that the variances and covariances of coefficients appear in a−1, or, in terms of present notation, the variance-covariance matrix of coefficients is"
3714,1,"['2k factorial experiment', 'experiment', 'case', 'factorial', 'factorial experiment']", The Orthogonal Design,seg_423,"in the case of the 2k factorial experiment, the columns of x are mutually orthog-"
3715,0,[], The Orthogonal Design,seg_423,"onal, imposing a very special structure. in general, for the 2k we can write"
3716,1,"['design', 'design point', 'replicate']", The Orthogonal Design,seg_423,"where each column contains 2k or 2kn entries, where n is the number of replicate runs at each design point. thus, formation of x′x yields"
3717,1,"['model', 'parameters']", The Orthogonal Design,seg_423,"where i is the identity matrix of dimension p, the number of model parameters."
3718,1,"['design', 'model', 'factorial']", The Orthogonal Design,seg_423,example 15.3: consider a 23 factorial design with duplicated runs fitted to the model
3719,1,"['estimates', 'least squares estimates', 'least squares', 'errors', 'standard', 'standard errors']", The Orthogonal Design,seg_423,"give expressions for the standard errors of the least squares estimates of b0, b1, b2, b3, b12, b13, and b23."
3720,0,[], The Orthogonal Design,seg_423,solution : x1 x2 x3 x1x2 x1x3 x2x3
3721,1,['observation'], The Orthogonal Design,seg_423,"with each unit viewed as being repeated (i.e., each observation is duplicated). as a result,"
3722,1,"['variances', 'design', 'factorial', 'design point', 'coefficients']", The Orthogonal Design,seg_423,from the foregoing it should be clear that the variances of all coefficients for a 2k factorial with n runs at each design point are
3723,1,"['errors', 'standard', 'standard errors', 'coefficients']", The Orthogonal Design,seg_423,"and, of course, all covariances are zero. as a result, standard errors of coefficients are calculated as"
3724,1,"['error', 'case', 'replication', 'mean square', 'mean', 'mean square error']", The Orthogonal Design,seg_423,"where s is found from the square root of the mean square error (hopefully obtained from adequate replication). thus, in our case with the 23,"
3725,1,"['model', 'experiment']", The Orthogonal Design,seg_423,example 15.4: consider the metallurgy experiment in exercise 15.3 on page 609. suppose that the fitted model is
3726,1,"['least squares regression', 'regression coefficients', 'least squares', 'regression', 'errors', 'standard', 'standard errors', 'coefficients']", The Orthogonal Design,seg_423,what are the standard errors of the least squares regression coefficients?
3727,1,"['factorial', 'errors', 'standard', 'standard errors', 'coefficients']", The Orthogonal Design,seg_423,solution : standard errors of all coefficients for the 2k factorial are equal and are
3728,0,[], The Orthogonal Design,seg_423,which in this illustration is
3729,1,"['degrees of freedom', 'error', 'case', 'mean square', 'mean', 'mean square error']", The Orthogonal Design,seg_423,"in this case, the pure mean square error is given by s2 = 2.46 (16 degrees of freedom). thus,"
3730,1,"['factorial', 'errors', 'analysis of variance', 'variance', 'standard', 'coefficient', 'standard errors', 'coefficients']", The Orthogonal Design,seg_423,"the standard errors of coefficients can be used to construct t-statistics on all coefficients. these t-values are related to the f-statistics in the analysis of variance. we have already demonstrated that an f-statistic on a coefficient, using the 2k factorial, is"
3731,0,[], The Orthogonal Design,seg_423,(contrast)2 f = . (2kn)s2
3732,1,['experiment'], The Orthogonal Design,seg_423,this is the form of the f-statistics on page 610 for the metallurgy experiment (exercise 15.3). it is easy to verify that if we write
3733,1,['contrast'], The Orthogonal Design,seg_423,"bj contrast t = , where bj = , sbj 2"
3734,0,[], The Orthogonal Design,seg_423,(contrast)2 t2 = = f. s22
3735,1,"['coefficient', 'significance', 'coefficients']", The Orthogonal Design,seg_423,"as a result, the usual relationship holds between t-statistics on coefficients and the f-values. as we might expect, the only difference between the use of t and f in assessing significance lies in the fact that the t-statistic indicates the sign, or direction, of the effect of the coefficient."
3736,1,"['linear', 'estimates', 'design', 'factorial', 'regression', 'variance', 'coefficients']", The Orthogonal Design,seg_423,"it would appear that the 2k factorial plan would handle many practical situations in which regression models are fitted. it can accommodate linear and interaction terms, providing optimal estimates of all coefficients (from a variance point of view). however, when k is large, the number of design points required is very large. often, portions of the total design can be used and still allow orthogonality with all its advantages. these designs are discussed in section 15.6."
3737,1,"['factor', 'case', 'factorial', 'significance', 'coefficients', 'information', 'regression coefficient', 'efficiency', 'coefficient', 'tests', 'model', 'regression model', 'contrasts', 'regression', 'interactions', 'analysis of variance', 'variance', 'response', 'anova', 'independent', 'regression coefficients', 'regression analysis', 'variable', 'degree of freedom', 'combinations']", The Orthogonal Design,seg_423,"we have learned that for the case of the 2k factorial all the information that is delivered to the analyst about the main effects and interactions is in the form of contrasts. these “2k − 1 pieces of information” carry a single degree of freedom apiece and they are independent of each other. in an analysis of variance, they manifest themselves as effects, whereas if a regression model is being constructed, the effects turn out to be regression coefficients, apart from a factor of 2. with either form of analysis, significance tests can be carried out and the t-test for a given effect is numerically the same as that for the corresponding regression coefficient. in the case of anova, variable screening and scientific interpretation of interactions are important, whereas in the case of a regression analysis, a model may be used to predict response and/or determine which factor/level combinations are optimum (e.g. maximize yield or maximize cleaning efficiency, as in the case of case study 15.2)."
3738,1,"['model', 'linear', 'precision', 'coefficients', 'regression model', 'design', 'estimation', 'results', 'regression coefficients', 'anova', 'regression', 'variance', 'variances']", The Orthogonal Design,seg_423,"it turns out that the orthogonality property is important whether the analysis is to be anova or regression. the orthogonality among the columns of x, the model matrix in, say, example 15.3, provides special conditions that have an important impact on the variance of effects or regression coefficients. in fact, it has already become apparent that the orthogonal design results in equality of variance for all effects or coefficients. thus, in this way, the precision, for purposes of estimation or testing, is the same for all coefficients, main effects, or interactions. in addition, if the regression model contains only linear terms and thus only main effects are of interest, the following conditions result in the minimization of variances of all effects (or, correspondingly, first-order regression coefficients)."
3739,1,"['model', 'coefficients', 'levels', 'regression model', 'variables', 'design', 'regression', 'variances']", The Orthogonal Design,seg_423,"conditions for if the regression model contains terms no higher than first order, and if the minimum ranges on the variables are given by xj ∈ [−1,+1] for j = 1, 2, . . . , k, then variances of var(bj)/σ2, for j = 1, 2, . . . , k, is minimized if the design is orthogonal and all coefficients xi levels in the design are at ±1 for i = 1, 2, . . . , k."
3740,1,"['model', 'coefficients']", The Orthogonal Design,seg_423,"thus, in terms of coefficients of model terms or main effects, orthogonality in the 2k is a very desirable property."
3741,1,"['independent', 'interaction', 'mutually independent', 'contrasts', 'geometric']", The Orthogonal Design,seg_423,"another approach to a better understanding of the “balance” provided by the 23 is to look at the situation graphically. all of the contrasts that are orthogonal and thus mutually independent are shown graphically in figure 15.10. in the graphs, the planes of the squares whose vertices contain the responses labeled “+” are compared to those containing the responses labeled “−.” those given in (a) show contrasts for main effects and should be obvious to the reader. those in (b) show the planes representing “+” vertices and “−” vertices for the three two-factor interaction contrasts. in (c), we see the geometric representation of the contrasts for the three-factor (abc) interaction."
3742,1,"['model', 'linear', 'regression model', 'variables', 'design', 'regression', 'linear regression', 'continuous', 'linear regression model']", The Orthogonal Design,seg_423,"in the situation in which the 2k design is implemented with continuous design variables and one is seeking to fit a linear regression model, the use of replicated runs in the design center can be extremely useful. in fact, quite apart from the advantages that will be discussed in what follows, a majority of scientists and"
3743,1,['interactions'], The Orthogonal Design,seg_423,+ − + ab ac bc (b) two−factor interactions
3744,1,['interaction'], The Orthogonal Design,seg_423,abc (c) three−factor interaction
3745,1,"['design', 'contrasts', 'factorial', 'geometric']", The Orthogonal Design,seg_423,figure 15.10: geometric presentation of contrasts for the 23 factorial design.
3746,1,"['factors', 'design', 'case', 'data', 'cases', 'process', 'response']", The Orthogonal Design,seg_423,"engineers would consider center runs (i.e., the runs at xi = 0 for i = 1, 2, . . . , k) as not only a reasonable practice but something that was intuitively appealing. in many areas of application of the 2k design, the scientist desires to determine if he or she might benefit from moving to a different region of interest in the factors. in many cases, the center (i.e., the point (0, 0, . . . , 0) in the coded factors) is often either the current operating conditions of the process or at least those conditions that are considered “currently optimum.” so it is often the case that the scientist will require data on the response at the center."
3747,1,"['model', 'case', 'data']", The Orthogonal Design,seg_423,"in addition to the intuitive appeal of the augmentation of the 2k with center runs, a second advantage is enjoyed that relates to the kind of model that is fitted to the data. consider, for example, the case with k = 2, illustrated in figure 15.11."
3748,1,['design'], The Orthogonal Design,seg_423,figure 15.11: a 22 design with center runs.
3749,1,"['degrees of freedom', 'factor', 'design', 'replication', 'locations', 'significance', 'linear', 'estimate', 'information', 'error', 'model', 'combination', 'response', 'linear combination', 'intercept', 'degree of freedom', 'lack of fit']", The Orthogonal Design,seg_423,"it is clear that without the center runs the model terms are the intercept, x1, x2, x1x2. these account for the four model degrees of freedom delivered by the four design points, apart from any replication. since each factor has response information available only at two locations {−1,+1}, no “pure” second-order curvature terms can be accommodated in the model (i.e, x12 or x22). but the information at (0, 0) produces an additional model degree of freedom. while this important degree of freedom does not allow both x21 and x22 to be used in the model, it does allow for testing the significance of a linear combination of x21 and x22. for nc center runs, there are then nc − 1 degrees of freedom available for replication or “pure” error. this allows an estimate of σ2 for testing the model terms and significance of the 1 d.f. for quadratic lack of fit. the concept here is very much like that discussed in the lack-of-fit material in chapter 11."
3750,1,"['model', 'complement', 'test']", The Orthogonal Design,seg_423,"in order to gain a complete understanding of how the lack-of-fit test works, assume that for k = 2 the true model contains the full second-order complement of terms, including x21 and x22. in other words,"
3751,1,['contrast'], The Orthogonal Design,seg_423,"now, consider the contrast"
3752,1,"['factorial', 'locations', 'average', 'response']", The Orthogonal Design,seg_423,where ȳf is the average response at the factorial locations and ȳ0 is the average response at the center point. it can be shown easily (see review exercise 15.46) that
3753,1,"['case', 'factors']", The Orthogonal Design,seg_423,"and, in fact, for the general case with k factors,"
3754,1,['test'], The Orthogonal Design,seg_423,"as a result, the lack-of-fit test is a simple t-test (or f = t2) with"
3755,1,"['mse', 'sample', 'factorial', 'sample variance', 'variance', 'response']", The Orthogonal Design,seg_423,"where nf is the number of factorial points and mse is simply the sample variance of the response values at (0, 0, . . . , 0)."
3756,1,"['model', 'experimental', 'process', 'experiment', 'variables', 'design', 'factorial', 'test', 'percent']", The Orthogonal Design,seg_423,"example 15.5: this example is taken from myers, montgomery, and anderson-cook (2009). a chemical engineer is attempting to model the percent conversion in a process. there are two variables of interest, reaction time and reaction temperature. in an attempt to arrive at the appropriate model, a preliminary experiment was conducted in a 22 factorial using the current region of interest in reaction time and temperature. single runs were made at each of the four factorial points and five runs were made at the design center in order that a lack-of-fit test for curvature could be conducted. figure 15.12 shows the design region and the experimental runs on yield."
3757,1,"['experiment', 'estimates', 'interaction', 'contrasts', 'case', 'errors', 'intercept', 'standard', 'coefficient', 'standard errors']", The Orthogonal Design,seg_423,"the time and temperature readings at the center are, of course, 35 minutes and 145◦c. the estimates of the main effects and single interaction coefficient are computed through contrasts, just as before. the center runs play no role in the computation of b1, b2, and b12. this should be intuitively reasonable to the reader. the intercept is merely ȳ for the entire experiment. this value is ȳ = 40.4444. the standard errors are found through the use of diagonal elements of (x′x)−1, as discussed earlier. for this case,"
3758,1,['factorial'], The Orthogonal Design,seg_423,figure 15.12: 22 factorial with 5 center runs.
3759,0,[], The Orthogonal Design,seg_423,"after the computations, we have"
3760,1,"['contrast', 'tests']", The Orthogonal Design,seg_423,"the contrast ȳf − ȳ0 = 40.425− 40.46 = −0.035, and the t-statistic that tests for curvature is given by"
3761,1,"['model', 'intercept']", The Orthogonal Design,seg_423,"as a result, it appears as if the appropriate model should contain only first-order terms (apart from the intercept)."
3762,1,"['deviation', 'model', 'variables', 'design', 'case', 'variable', 'response', 'average']", The Orthogonal Design,seg_423,"if one considers the simple case of a single design variable with runs at −1 and +1, it should seem clear that the average response at −1 and +1 should be close to the response at 0, the center, if the model is first order in nature. any deviation would certainly suggest curvature. this is simple to extend to two variables. consider figure 15.13."
3763,1,"['model', 'response', 'case']", The Orthogonal Design,seg_423,"the figure shows the plane on y that passes through the y values of the factorial points. this is the plane that would represent the perfect fit for the model containing x1, x2, and x1x2. if the model contains no quadratic curvature (i.e., β11 = β22 = 0), we would expect the response at (0, 0) to be at or near the plane. if the response is far away from the plane, as in the case of figure 15.13, then it can be seen graphically that quadratic curvature is present."
3764,1,['factorial'], The Orthogonal Design,seg_423,"figure 15.13: 22 factorial with runs at (0, 0)."
3765,1,"['experimental', '2k factorial experiment', 'experiment', 'treatment', 'interaction', 'observations', 'factorial', 'interactions', 'degree of freedom', 'combinations', 'experimental units', 'factorial experiment']", Fractional Factorial Experiments,seg_427,"the 2k factorial experiment can become quite demanding, in terms of the number of experimental units required, when k is large. one of the real advantages of this experimental plan is that it allows a degree of freedom for each interaction. however, in many experimental situations, it is known that certain interactions are negligible, and thus it would be a waste of experimental effort to use the complete factorial experiment. in fact, the experimenter may have an economic constraint that disallows taking observations at all of the 2k treatment combinations. when k is large, we can often make use of a fractional factorial experiment where per-"
3766,1,['factorial'], Fractional Factorial Experiments,seg_427,"haps one-half, one-fourth, or even one-eighth of the total factorial plan is actually carried out."
3767,1,"['contrast', 'experimental', '2k factorial experiment', 'experiment', 'design', 'factorial', 'factorial experiment']", Fractional Factorial Experiments,seg_427,the construction of the half-replicate design is identical to the allocation of the 2k factorial experiment into two blocks. we begin by selecting a defining contrast that is to be completely sacrificed. we then construct the two blocks accordingly and choose either of them as the experimental plan.
3768,1,"['design', 'factorial']", Fractional Factorial Experiments,seg_427,"a 1 fraction of a 2k factorial is often referred to as a 2k−1 design, the latter 2 indicating the number of design points. our first illustration of a 2k−1 will be a 2"
3769,1,"['table', 'design', 'contrasts', 'complement']", Fractional Factorial Experiments,seg_427,"of 23, or a 23−1, design. in other words, the scientist or engineer cannot use the full complement (i.e., the full 23 with 8 design points) and hence must settle for a design with only four design points. the question is, of the design points (1), a, b, ab, ac, c, bc, and abc, which four design points would result in the most useful design? the answer, along with the important concepts involved, appears in the table of + and − signs displaying contrasts for the full 23. consider table 15.9."
3770,1,"['experiment', 'contrasts', 'factorial', 'factorial experiment']", Fractional Factorial Experiments,seg_427,table 15.9: contrasts for the seven available effects for a 23 factorial experiment
3771,1,"['treatment', 'combination']", Fractional Factorial Experiments,seg_427,effects treatment combination i a b c ab ac bc abc
3772,1,"['contrast', 'table', 'design', 'interaction', 'contrasts']", Fractional Factorial Experiments,seg_427,"1 note that the two fractions are {a, b, c, abc} and {ab, ac, bc, (1)}. note also 2 from table 15.9 that in both designs abc has no contrast but all other effects do have contrasts. in one of the fractions we have abc containing all + signs, and in the other fraction the abc effect contains all − signs. as a result, we say that the top design in the table is described by abc = i and the bottom design by abc = −i. the interaction abc is called the design generator, and abc = i (or abc = −i for the second design) is called the defining relation."
3773,1,"['experimental error', 'degrees of freedom', 'experimental', 'design', 'contrasts', 'error']", Fractional Factorial Experiments,seg_427,"if we focus on the abc = i design (the upper 23−1), it becomes apparent that six effects contain contrasts. this produces the initial appearance that all effects can be studied apart from abc. however, the reader can certainly recall that with only four design points, even if points are replicated, the degrees of freedom available (apart from experimental error) are"
3774,1,"['model', 'intercept']", Fractional Factorial Experiments,seg_427,regression model terms 3 intercept 1
3775,1,"['contrast', 'contrasts']", Fractional Factorial Experiments,seg_427,"a closer look suggests that the seven effects are not orthogonal, and each contrast is represented in another effect. in fact, using ≡ to signify identical contrasts, we have"
3776,1,['estimated'], Fractional Factorial Experiments,seg_427,"as a result, within a pair an effect cannot be estimated independently of its alias “partner.” the effects"
3777,1,"['estimated', 'estimates', 'information', 'degree of freedom', 'numerical']", Fractional Factorial Experiments,seg_427,"will produce the same numerical result and thus contain the same information. in fact, it is often said that they share a degree of freedom. in truth, the estimated effect actually estimates the sum, namely a+ bc. we say that a and bc are aliases, b and ac are aliases, and c and ab are aliases."
3778,0,[], Fractional Factorial Experiments,seg_427,"for the abc = −i fraction we can observe that the aliases are the same as those for the abc = i fraction, apart from sign. thus, we have"
3779,0,[], Fractional Factorial Experiments,seg_427,the two fractions appear on corners of the cubes in figures 15.15(a) and 15.15(b).
3780,1,['factorial'], Fractional Factorial Experiments,seg_427,figure 15.15: the 1 2 fractions of the 23 factorial.
3781,0,[], Fractional Factorial Experiments,seg_427,"in general, for a 2k−1, each effect, apart from that defined by the generator, will have a single alias partner. the effect defined by the generator will not be aliased"
3782,1,"['estimator', 'mean', 'least squares']", Fractional Factorial Experiments,seg_427,"by another effect but rather will be aliased with the mean since the least squares estimator will be the mean. to determine the alias for each effect, one merely begins with the defining relation, say abc = i for the 23−1. then to find, say, the alias for effect a, multiply a by both sides of the equation abc = i and reduce any exponent by modulo 2. for example,"
3783,0,[], Fractional Factorial Experiments,seg_427,"in a similar fashion,"
3784,0,[], Fractional Factorial Experiments,seg_427,"now for the second fraction (i.e., defined by the relation abc = −i),"
3785,1,"['estimates', 'numerical']", Fractional Factorial Experiments,seg_427,"as a result, the numerical value of effect a is actually estimating a−bc. similarly, the value of b estimates b −ac, and the value of c estimates c −ab."
3786,1,"['factors', 'table', 'factor', 'design', 'factorial']", Fractional Factorial Experiments,seg_427,"a clear understanding of the concept of aliasing makes it very simple to understand the construction of the 2k−1. we begin with investigation of the 23−1. there are three factors and four design points required. the procedure begins with a full factorial in k − 1 = 2 factors a and b. then a third factor is added according to the desired alias structures. for example, with abc as the generator, clearly c = ±ab. thus, c = ab or c = −ab is found to supplement the full factorial in a and b. table 15.10 illustrates what is a very simple procedure."
3787,0,[], Fractional Factorial Experiments,seg_427,table 15.10: construction of the two 23−1 designs
3788,1,"['contrast', 'table', 'design', 'contrasts', 'tables']", Fractional Factorial Experiments,seg_427,"note that we saw earlier that abc = i gives the design points a, b, c, and abc while abc = −i gives (1), ac, bc, and ab. earlier we were able to construct the same designs using the table of contrasts in table 15.9. however, as the design becomes more complicated with higher fractions, these contrast tables become more difficult to deal with."
3789,1,"['factors', 'design', 'interaction', 'case', 'factorial']", Fractional Factorial Experiments,seg_427,"1 of a 24 factorial design) involving factors a, b, c, and d. as in the case of the 23−1, the highest-order interaction, in this case"
3790,1,"['table', 'information']", Fractional Factorial Experiments,seg_427,"abcd, is used as the generator. we must keep in mind that abcd = i; the defining relation suggests that the information on abcd is sacrificed. here we begin with the full 23 in a, b, and c and form d = ±abc to generate the two 24−1 designs. table 15.11 illustrates the construction of both designs."
3791,0,[], Fractional Factorial Experiments,seg_427,table 15.11: construction of the two 24−1 designs
3792,0,[], Fractional Factorial Experiments,seg_427,"here, using the notation a, b, c, and so on, we have the following designs:"
3793,1,"['design', 'case']", Fractional Factorial Experiments,seg_427,"the aliases in the case of the 24−1 are found as illustrated earlier for the 23−1. each effect has a single alias partner and is found by multiplication via the use of the defining relation. for example, the alias for a for the abcd = i design is given by"
3794,0,[], Fractional Factorial Experiments,seg_427,the alias for ab is given by
3795,1,['interactions'], Fractional Factorial Experiments,seg_427,"as we can observe easily, main effects are aliased with three-factor interactions and two-factor interactions are aliased with other two-factor interactions. a complete listing is given by"
3796,1,['case'], Fractional Factorial Experiments,seg_427,in the case of the 1
3797,1,"['results', 'interactions', 'interaction']", Fractional Factorial Experiments,seg_427,"4 fraction, two interactions are selected to be sacrificed rather than one, and the third results from finding the generalized interaction of the"
3798,0,[], Fractional Factorial Experiments,seg_427,selected two. note that this is very much like the construction of four blocks. the fraction used is simply one of the blocks. a simple example aids a great deal in seeing the connection to the construction of the 2
3799,0,[], Fractional Factorial Experiments,seg_427,1 fraction. consider the
3800,1,"['factors', 'factor', 'design', 'interaction', 'factorial', 'confounding', 'interactions']", Fractional Factorial Experiments,seg_427,"1 of a 25 factorial (i.e., a 25−2), with factors a, b, c, d, and e. one procedure that avoids the confounding of two main effects is the choice of abd and ace as the interactions that correspond to the two generators, giving abd = i and ace = i as the defining relations. the third interaction sacrificed would then be (abd)(ace) = a2bcde = bcde. for the construction of the design, we begin with a 25−2 = 23 factorial in a, b, and c. we use the interactions abd and ace to supply the generators, so the 23 factorial in a, b, and c is supplemented by factor d = ±ab and e = ±ac. thus, one of the fractions is given by"
3801,1,"['design', 'factors']", Fractional Factorial Experiments,seg_427,"the other three fractions are found by using the generators {d = −ab,e = ac}, {d = ab,e = −ac}, and {d = −ab,e = −ac}. consider an analysis of the above 25−2 design. it contains eight design points to study five factors. the aliases for main effects are given by"
3802,1,"['degrees of freedom', 'replication']", Fractional Factorial Experiments,seg_427,aliases for other effects can be found in the same fashion. the breakdown of degrees of freedom is given by (apart from replication)
3803,1,['lack of fit'], Fractional Factorial Experiments,seg_427,"main effects 5 lack of fit 2 (cd = be, bc = de)"
3804,1,"['lack of fit', 'interactions']", Fractional Factorial Experiments,seg_427,we list interactions only through degree 2 in the lack of fit.
3805,1,"['factors', 'table', 'design', 'case', 'factorial']", Fractional Factorial Experiments,seg_427,"consider now the case of a 26−2, which allows 16 design points to study six factors. once again two design generators are chosen. a pragmatic choice to supplement a 26−2 = 24 full factorial in a, b, c, and d is to use e = ±abc and f = ±bcd. the construction is given in table 15.12."
3806,1,['design'], Fractional Factorial Experiments,seg_427,"obviously, with eight more design points than in the 25−2, the aliases for main effects will not present as difficult a problem. in fact, note that with defining relations abce = ±i, bcdf = ±i, and (abce)(bcdf ) = adef = ±i,"
3807,1,['design'], Fractional Factorial Experiments,seg_427,table 15.12: a 26−2 design
3808,1,['combination'], Fractional Factorial Experiments,seg_427,treatment a b c d e = abc f = bcd combination
3809,1,['interactions'], Fractional Factorial Experiments,seg_427,main effects will be aliased with interactions that are no less complex than those of third order. the alias structure for main effects is written
3810,1,"['degree of freedom', 'interactions']", Fractional Factorial Experiments,seg_427,"each with a single degree of freedom. for the two-factor interactions,"
3811,1,"['degrees of freedom', 'interactions']", Fractional Factorial Experiments,seg_427,"here, of course, there is some aliasing among the two-factor interactions. the remaining 2 degrees of freedom are accounted for by the following groups:"
3812,1,"['experimental', 'experiment', 'contrasts']", Fractional Factorial Experiments,seg_427,"it becomes evident that we should always be aware of what the alias structure is for a fractional experiment before we finally recommend the experimental plan. proper choice in defining contrasts is important, since it dictates the alias structure."
3813,1,"['tests', 'significance', 'data', 'factorial', 'factorial experiments', 'experiments', 'error']", Analysis of Fractional Factorial Experiments,seg_429,the difficulty of making formal significance tests using data from fractional factorial experiments lies in the determination of the proper error term. unless there are
3814,1,"['contrasts', 'experiments', 'error']", Analysis of Fractional Factorial Experiments,seg_429,"data available from prior experiments, the error must come from a pooling of contrasts representing effects that are presumed to be negligible."
3815,1,"['contrast', 'experiment', 'table', 'treatment', 'factorial', 'set', 'combinations', 'factorial experiment']", Analysis of Fractional Factorial Experiments,seg_429,"sums of squares for individual effects are found by using essentially the same procedures given for the complete factorial. we can form a contrast in the treatment combinations by constructing the table of positive and negative signs. for example, for a half-replicate of a 23 factorial experiment with abc the defining contrast, one possible set of treatment combinations, along with the appropriate algebraic sign for each contrast used in computing effects and the sums of squares for the various effects, is presented in table 15.13."
3816,1,"['experiment', 'contrasts', 'factorial', 'factorial experiment']", Analysis of Fractional Factorial Experiments,seg_429,table 15.13: signs for contrasts in a half-replicate of a 23 factorial experiment
3817,1,"['combination', 'factorial']", Analysis of Fractional Factorial Experiments,seg_429,treatment factorial effect combination a b c ab ac bc abc
3818,1,"['degrees of freedom', 'estimate', 'orthogonal contrasts', 'table', 'treatment', 'interaction', 'contrasts', 'observations', 'variance', 'combinations', 'significance', 'test', 'error']", Analysis of Fractional Factorial Experiments,seg_429,"note that in table 15.13 the a and bc contrasts are identical, illustrating the aliasing. also, b ≡ ac and c ≡ ab. in this situation, we have three orthogonal contrasts representing the 3 degrees of freedom available. if two observations were obtained for each of the four treatment combinations, we would then have an estimate of the error variance with 4 degrees of freedom. assuming the interaction effects to be negligible, we could test all the main effects for significance."
3819,1,['sum of squares'], Analysis of Fractional Factorial Experiments,seg_429,an example effect and corresponding sum of squares is
3820,1,"['2k factorial experiment', 'experiment', 'treatment', 'contrasts', 'factorial', 'sum of squares', 'combinations', 'replications', 'factorial experiment']", Analysis of Fractional Factorial Experiments,seg_429,"in general, the single-degree-of-freedom sum of squares for any effect in a 2−p fraction of a 2k factorial experiment (p < k) is obtained by squaring contrasts in the treatment totals selected and dividing by 2k−pn, where n is the number of replications of these treatment combinations."
3821,1,"['degrees of freedom', 'levels', 'factor', 'significance', 'data', 'error', 'factors', 'contrasts', 'interactions', 'analysis of variance', 'level', 'variance', 'response', 'contrast', 'table']", Analysis of Fractional Factorial Experiments,seg_429,"example 15.6: suppose that we wish to use a half-replicate to study the effects of five factors, each at two levels, on some response, and it is known that whatever the effect of each factor, it will be constant for each level of the other factors. in other words, there are no interactions. let the defining contrast be abcde, causing main effects to be aliased with four-factor interactions. the pooling of contrasts involving interactions provides 15 − 5 = 10 degrees of freedom for error. perform an analysis of variance on the data in table 15.14, testing all main effects for significance at the 0.05 level."
3822,0,[], Analysis of Fractional Factorial Experiments,seg_429,solution : the sums of squares and effects for the main effects are
3823,1,"['factorial', 'factorial experiments', 'experiments']", Analysis of Fractional Factorial Experiments,seg_429,634 chapter 15 2k factorial experiments and fractions
3824,1,['data'], Analysis of Fractional Factorial Experiments,seg_429,table 15.14: data for example 15.6
3825,1,"['treatment', 'response']", Analysis of Fractional Factorial Experiments,seg_429,treatment response treatment response a 11.3 bcd 14.1 b 15.6 abe 14.2 c 12.7 ace 11.7 d 10.4 ade 9.4 e 9.2 bce 16.2 abc 11.0 bde 13.9 abd 8.9 cde 14.7 acd 9.6 abcde 13.2
3826,1,"['factors', 'table', 'factor', 'significance', 'level', 'tests', 'response']", Analysis of Fractional Factorial Experiments,seg_429,"8 all other calculations and tests of significance are summarized in table 15.15. the tests indicate that factor a has a significant negative effect on the response, whereas factor b has a significant positive effect. factors c, d, and e are not significant at the 0.05 level."
3827,1,"['linear', 'model', 'experimental', 'regression model', 'factors', 'regression', 'interactions', 'analysis of variance', 'response', 'variance', 'estimated', 'variables', 'variable']", Higher Fractions and Screening Designs,seg_433,"some industrial situations require the analyst to determine which of a large number of controllable factors have an impact on some important response. the factors may be qualitative or class variables, regression variables, or a mixture of both. the analytical procedure may involve analysis of variance, regression, or both. often the regression model used involves only linear main effects, although a few interactions may be estimated. the situation calls for variable screening and the resulting experimental designs are known as screening designs. clearly, two-level orthogonal designs that are saturated or nearly saturated are viable candidates."
3828,0,[], Higher Fractions and Screening Designs,seg_433,"two-level orthogonal designs are often classified according to their resolution, the latter determined through the following definition."
3829,1,"['design', 'interaction', 'contrasts', 'set']", Higher Fractions and Screening Designs,seg_433,definition 15.1: the resolution of a two-level orthogonal design is the length of the smallest (least complex) interaction among the set of defining contrasts.
3830,1,"['linear', 'design', 'case', 'factorial', 'regression', 'interactions']", Higher Fractions and Screening Designs,seg_433,"if the design is constructed as a full or fractional factorial (i.e., either a 2k or a 2k−p design, p = 1, 2, . . . , k − 1), the notion of design resolution is an aid in categorizing the impact of the aliasing. for example, a resolution ii design would have little use, since there would be at least one instance of aliasing of one main effect with another. a resolution iii design will have all main effects (linear effects) orthogonal to each other. however, there will be some aliasing among linear effects and two-factor interactions. clearly, then, if the analyst is interested in studying main effects (linear effects in the case of regression) and there are no two-factor interactions, a design of resolution at least iii is required."
3831,1,"['variables', 'design', 'factorial', 'interactions']", Construction of Resolution III and IV Designs with   and  Design Points,seg_435,useful designs of resolution iii and iv can be constructed for 2 to 7 variables with 8 design points. we begin with a 23 factorial that has been symbolically saturated with interactions.
3832,1,"['variables', 'design', 'interaction']", Construction of Resolution III and IV Designs with   and  Design Points,seg_435,"it is clear that a resolution iii design can be constructed merely by replacing interaction columns by new main effects through 7 variables. for example, we may define"
3833,1,['contrast'], Construction of Resolution III and IV Designs with   and  Design Points,seg_435,x4 = x1x2 (defining contrast abd)
3834,1,['contrast'], Construction of Resolution III and IV Designs with   and  Design Points,seg_435,x5 = x1x3 (defining contrast ace)
3835,1,['contrast'], Construction of Resolution III and IV Designs with   and  Design Points,seg_435,x6 = x2x3 (defining contrast bcf )
3836,1,['contrast'], Construction of Resolution III and IV Designs with   and  Design Points,seg_435,x7 = x1x2x3 (defining contrast abcg)
3837,1,"['design', 'contrasts', 'factorial']", Construction of Resolution III and IV Designs with   and  Design Points,seg_435,"and obtain a 2−4 fraction of a 27 factorial. the preceding expressions identify the chosen defining contrasts. eleven additional defining contrasts result, and all defining contrasts contain at least three letters. thus, the design is a resolution iii design. clearly, if we begin with a subset of the augmented columns and conclude"
3838,0,[], Construction of Resolution III and IV Designs with   and  Design Points,seg_435,"table 15.16: some resolution iii, iv, v, vi and vii 2k−p designs"
3839,1,"['design', 'factors']", Construction of Resolution III and IV Designs with   and  Design Points,seg_435,number of number of factors design points generators
3840,1,"['variables', 'design']", Construction of Resolution III and IV Designs with   and  Design Points,seg_435,"with a design involving fewer than 7 design variables, the result is a resolution iii design in fewer than 7 variables."
3841,1,"['variables', 'design', 'set', 'interactions']", Construction of Resolution III and IV Designs with   and  Design Points,seg_435,"a similar set of possible designs can be constructed for 16 design points by beginning with a 24 saturated with interactions. definitions of variables that correspond to these interactions produce resolution iii designs through 15 variables. in a similar fashion, designs containing 32 runs can be constructed by beginning with a 25."
3842,1,"['factors', 'table', 'factorial']", Construction of Resolution III and IV Designs with   and  Design Points,seg_435,"table 15.16 provides guidelines for constructing 8, 16, 32, and 64 point designs that are resolution iii, iv and even v. the table gives the number of factors, the number of runs, and the generators that are used to produce the 2k−p designs. the generator given is used to augment the full factorial containing k − p factors."
3843,1,"['sample size', 'sample', 'design matrix', 'variables', 'design', 'permutation']", Other TwoLevel Resolution III Designs The PlackettBurman Designs,seg_437,"a family of designs developed by plackett and burman (1946; see the bibliography) fills sample size voids that exist with the fractional factorials. the latter are useful with sample sizes 2r (i.e., they involve sample sizes 4, 8, 16, 32, 64, . . . ). the plackett-burman designs involve 4r design points, and thus designs of sizes 12, 20, 24, 28, and so on, are available. these two-level plackett-burman designs are resolution iii designs and are very simple to construct. “basic lines” are given for each sample size. these lines of + and − signs are n− 1 in number. to construct the columns of the design matrix, we begin with the basic line and do a cyclic permutation on the columns until k (the desired number of variables) columns are formed. then we fill in the last row with negative signs. the result will be"
3844,1,"['variables', 'design']", Other TwoLevel Resolution III Designs The PlackettBurman Designs,seg_437,"a resolution iii design with k variables (k = 1, 2, . . . , n). the basic lines are as follows:"
3845,1,"['variables', 'design']", Other TwoLevel Resolution III Designs The PlackettBurman Designs,seg_437,"example 15.7: construct a two-level screening design with 6 variables containing 12 design points. solution : begin with the basic line in the initial column. the second column is formed by bringing the bottom entry of the first column to the top of the second column and repeating the first column. the third column is formed in the same fashion, using entries in the second column. when there is a sufficient number of columns, simply fill in the last row with negative signs. the resulting design is as follows:"
3846,1,"['sample size', 'sample', 'linear', 'variables', 'design']", Other TwoLevel Resolution III Designs The PlackettBurman Designs,seg_437,"the plackett-burman designs are popular in industry for screening situations. because they are resolution iii designs, all linear effects are orthogonal. for any sample size, the user has available a design for k = 2, 3, . . . , n − 1 variables."
3847,1,"['degrees of freedom', 'design', 'case', 'regression', 'interactions', 'control']", Other TwoLevel Resolution III Designs The PlackettBurman Designs,seg_437,"the alias structure for the plackett-burman design is very complicated, and thus the user cannot construct the design with complete control over the alias structure, as in the case of 2k or 2k−p designs. however, in the case of regression models, the plackett-burman design can accommodate interactions (although they will not be orthogonal) when sufficient degrees of freedom are available."
3848,1,"['interaction term', 'model', 'linear', 'regression model', 'variables', 'design', 'interaction', 'case', 'data', 'regression', 'set', 'interaction terms', 'efficiency']", Introduction to Response Surface Methodology,seg_439,"in case study 15.2, a regression model was fitted to a set of data with the specific goal of finding conditions on those design variables that optimize (maximize) the cleansing efficiency of coal. the model contained three linear main effects, three two-factor interaction terms, and one three-factor interaction term. the model response was the cleansing efficiency, and the optimum conditions on x1, x2, and x3"
3849,1,"['model', 'estimated', 'design', 'response', 'response surface', 'process', 'response surface methodology', 'coefficients']", Introduction to Response Surface Methodology,seg_439,"were found by using the signs and the magnitude of the model coefficients. in this example, a two-level design was employed for process improvement or process optimization. in many areas of science and engineering, the application is expanded to involve more complicated models and designs, and this collection of techniques is called response surface methodology (rsm). it encompasses both graphical and analytical approaches. the term response surface is derived from the appearance of the multidimensional surface of constant estimated response from a second-order model, i.e., a model with firstand second-order terms. an example will follow."
3850,1,"['model', 'variables', 'design', 'case', 'process', 'response']", Introduction to Response Surface Methodology,seg_439,"in many industrial examples of process optimization, a second-order response surface model is used. for the case of, say, k = 2 process variables, or design variables, and a single response y, the model is given by"
3851,1,"['interaction term', 'model', 'variables', 'design', 'interaction', 'error']", Introduction to Response Surface Methodology,seg_439,"here we have k = 2 first-order terms, two pure second-order, or quadratic, terms, and one interaction term given by β12x1x2. the terms x1 and x2 are taken to be in the familiar ±1 coded form. the term designates the usual model error. in general, for k design variables the model will contain 1 + k+ k+ (k"
3852,1,"['model', 'experimental', 'levels', 'variables', 'design']", Introduction to Response Surface Methodology,seg_439,"2) model terms, and hence the experimental design must contain at least a like number of design points. in addition, the quadratic terms require that the design variables be fixed in the design with at least three levels. the resulting design is referred to as a second-order design. illustrations will follow."
3853,1,"['central composite designs', 'levels', 'factors', 'table', 'factor', 'design', 'central composite design', 'observations', 'factorial', 'replications', 'process', 'response', 'concentration', 'percent']", Introduction to Response Surface Methodology,seg_439,"the following central composite design (ccd) and example is taken from myers, montgomery, and anderson-cook (2009). perhaps the most popular class of second-order designs is the class of central composite designs. the example given in table 15.17 involves a chemical process in which reaction temperature, ξ1, and reactant concentration, ξ2, are shown at their natural levels. they also appear in coded form. there are five levels of each of the two factors. in addition, we have the order in which the observations on x1 and x2 were run. the column on the right gives values of the response y, the percent conversion of the process. the first four design points represent the familiar factorial points at levels ±1. the next four points are called axial points. they are followed by the center runs that were discussed and illustrated earlier in this chapter. thus, the five levels of each of the two factors are −1, +1, −1.414, +1.414, and 0. a clear picture of the geometry of the central composite design for this k = 2 example appears in figure 15.16. this figure illustrates the source of the term axial points.these four points are on the factor axes at an axial distance of α = √2 = 1.414 from the design center. in fact, for this particular ccd, the perimeter points, axial and factorial, are all at the distance √2 from the design center, and as a result we have eight equally spaced points on a circle plus four replications at the design center."
3854,1,"['data', 'function', 'response', 'response surface']", Introduction to Response Surface Methodology,seg_439,example 15.8: response surface analysis: an analysis of the data in the two-variable example may involve the fitting of a second-order response function. the resulting response surface can be used analytically or graphically to determine the impact that x1
3855,1,"['design', 'central composite design']", Introduction to Response Surface Methodology,seg_439,table 15.17: central composite design for example 15.8
3856,1,"['concentration', 'observation']", Introduction to Response Surface Methodology,seg_439,temperature (◦c) concentration (%) observation run ξ1 ξ2 x1 x2 y 1 4 200 15 −1 −1 43 2 12 250 15 1 −1 78 3 11 200 25 −1 1 69 4 5 250 25 1 1 73 5 6 189.65 20 −1.414 0 48 6 7 260.35 20 1.414 0 78 7 1 225 12.93 0 −1.414 65 8 3 225 27.07 0 1.414 74 9 8 225 20 0 0 76 10 10 225 20 0 0 79 11 9 225 20 0 0 83 12 2 225 20 0 0 81
3857,1,"['design', 'central composite design']", Introduction to Response Surface Methodology,seg_439,figure 15.16: central composite design for example 15.8.
3858,1,"['model', 'coefficients', 'method', 'method of least squares', 'variables', 'least squares', 'function', 'process', 'response', 'percent']", Introduction to Response Surface Methodology,seg_439,and x2 have on percent conversion of the process. the coefficients in the response function are determined by the method of least squares developed in chapter 12 and illustrated throughout this chapter. the resulting second-order response model is given in the coded variables as
3859,1,['variables'], Introduction to Response Surface Methodology,seg_439,whereas in the natural variables it is given by
3860,1,"['variables', 'design']", Introduction to Response Surface Methodology,seg_439,"since the current example contains only two design variables, the most illumi-"
3861,1,"['levels', 'design', 'predicted', 'plots', 'location', 'function', 'cases', 'graphics', 'plotting', 'response', 'concentration', 'percent', 'estimated', 'variables', 'response surface']", Introduction to Response Surface Methodology,seg_439,"nating approach to determining the nature of the response surface in the design region is through twoor three-dimensional graphics. it is of interest to determine what levels of temperature x1 and concentration x2 produce a desirable estimated percent conversion, ŷ. the estimated response function above was plotted in three dimensions, and the resulting response surface is shown in figure 15.17. the height of the surface is ŷ in percent. it is readily seen from this figure why the term response surface is employed. in cases where only two design variables are used, two-dimensional contour plotting can be useful. thus, make note of figure 15.18. contours of constant estimated conversion are seen as slices from the response surface. note that the viewer of either figure can readily observe which coordinates of temperature and concentration produce the largest estimated percent conversion. in the plots, the coordinates are given in both coded units and natural units. notice that the largest estimated conversion is at approximately 240◦c and 20% concentration. the maximum estimated (or predicted) response at that location is 82.47%."
3862,1,"['plot', 'prediction', 'response', 'response surface']", Introduction to Response Surface Methodology,seg_439,figure 15.17: plot for the response surface prediction conversion for example 15.8.
3863,1,"['graphical', 'design', 'results', 'information', 'response', 'response surface']", Introduction to Response Surface Methodology,seg_439,"the book by myers, montgomery, and anderson-cook (2009) provides a great deal of information concerning both design and analysis of rsm. the graphical illustration we have used here can be augmented by analytical results that provide information about the nature of the response surface inside the design region."
3864,1,"['plot', 'predicted']", Introduction to Response Surface Methodology,seg_439,figure 15.18: contour plot of predicted conversion for example 15.8.
3865,1,"['experimental', 'design', 'location', 'process']", Introduction to Response Surface Methodology,seg_439,"other computations can be used to determine whether the location of the optimum conditions is, in fact, inside or remote from the experimental design region. there are many important considerations when one is required to determine appropriate conditions for future operation of a process."
3866,1,"['design', 'experimental', 'case']", Introduction to Response Surface Methodology,seg_439,"other material in myers, montgomery, and anderson-cook (2009) deals with further experimental design issues. for example, the ccd, while the most generally useful design, is not the only class of design used in rsm. many others are discussed in the aforementioned text. also, the ccd discussed here is a special case in which k = 2. the more general k > 2 case is discussed in myers, montgomery, and anderson-cook (2009)."
3867,1,"['design of experiments', 'design', 'case', 'statistical', 'experiments', 'process', 'processes']", Robust Parameter Design,seg_441,"in this chapter, we have emphasized the notion of using design of experiments (doe) to learn about engineering and scientific processes. in the case where the process involves a product, doe can be used to provide product improvement or quality improvement. as we pointed out in chapter 1, much importance has been attached to the use of statistical methods in product improvement. an important aspect of this quality improvement effort that surfaced in the 1980s and continued through the 1990s is to design quality into processes and products at the research stage or the process design stage. one often requires doe in the development of processes that have the following properties:"
3868,0,[], Robust Parameter Design,seg_441,1. insensitive (robust) to environmental conditions
3869,1,"['control', 'factors']", Robust Parameter Design,seg_441,2. insensitive (robust) to factors difficult to control
3870,1,['variation'], Robust Parameter Design,seg_441,3. provide minimum variation in performance
3871,1,"['parameters', 'factors', 'variables', 'design', 'parameter', 'process']", Robust Parameter Design,seg_441,"the methods used to attain the desirable characteristics in 1, 2, and 3 are a part of what is referred to as robust parameter design, or rpd (see taguchi, 1991; taguchi and wu, 1985; and kackar, 1985, in the bibliography). the term design in this context refers to the design of the process or system; parameter refers to the parameters in the system. these are what we have been calling factors or variables."
3872,1,['process'], Robust Parameter Design,seg_441,"it is very clear that goals 1, 2, and 3 above are quite noble. for example, a petroleum engineer may have a fine gasoline blend that performs quite well as long as conditions are ideal and stable. however, the performance may deteriorate because of changes in environmental conditions, such as type of driver, weather conditions, type of engine, and so forth. a scientist at a food company may have a cake mix that is quite good unless the user does not exactly follow directions on the box, directions that deal with oven temperature, baking time, and so forth. a product or process whose performance is consistent when exposed to these changing environmental conditions is called a robust product or robust process. (see myers, montgomery, and anderson-cook, 2009, in the bibliography.)"
3873,1,"['factors', 'variables', 'design', 'control']", Robust Parameter Design,seg_441,taguchi (1991) emphasized the notion of using two classes of design variables in a study involving rpd: control factors and noise factors.
3874,1,"['factors', 'experiment', 'variables', 'control', 'process']", Robust Parameter Design,seg_441,definition 15.2: control factors are variables that can be controlled both in the experiment and in the process. noise factors are variables that may or may not be controlled in the experiment but cannot be controlled in the process (or not controlled well in the process).
3875,1,"['experiment', 'variables', 'control']", Robust Parameter Design,seg_441,an important approach is to use control variables and noise variables in the same experiment as fixed effects. orthogonal designs or orthogonal arrays are popular designs to use in this effort.
3876,1,"['levels', 'variables', 'design', 'parameter', 'control', 'process']", Robust Parameter Design,seg_441,"goal of robust the goal of robust parameter design is to choose the levels of the control variparameter design ables (i.e., the design of the process) that are most robust (insensitive) to changes in the noise variables."
3877,1,"['variables', 'process']", Robust Parameter Design,seg_441,"it should be noted that changes in the noise variables actually imply changes during the process, changes in the field, changes in the environment, changes in handling or usage by the consumer, and so forth."
3878,1,"['experimental', 'experiment', 'variables', 'design', 'design of experiments', 'experiments', 'control']", Robust Parameter Design,seg_441,"one approach to the design of experiments involving both control and noise variables is to use an experimental plan that calls for an orthogonal design for both the control and the noise variables separately. the complete experiment, then, is merely the product or crossing of these two orthogonal designs. the following is a simple example of a product array with two control and two noise variables."
3879,1,"['levels', 'factor', 'design', 'factorial', 'process', 'experiment', 'parameter', 'control', 'response', 'method', 'variability', 'variables', 'table', 'variable', 'combinations']", Robust Parameter Design,seg_441,"example 15.9: in the article “the taguchi approach to parameter design” in quality progress, december 1987, d. m. byrne and s. taguchi discuss an interesting example in which a method is sought for attaching an electrometric connector to a nylon tube so as to deliver the pull-off performance required for an automotive engine application. the objective is to find controllable conditions that maximize pull-off force. among the controllable variables are a, connector wall thickness, and b, insertion depth. during routine operation there are several variables that cannot be controlled, although they will be controlled during the experiment. among them are c, conditioning time, and d, conditioning temperature. three levels are taken for each control variable and two for each noise variable. as a result, the crossed array is as follows. the control array is a 3 × 3 array, and the noise array is a familiar 22 factorial with (1), c, d, and cd representing the four factor combinations. the purpose of the noise factor is to create the kind of variability in the response, pull-off force, that might be expected in day-to-day operation with the process. the design is shown in table 15.18."
3880,1,['design'], Robust Parameter Design,seg_441,table 15.18: design for example 15.9
3881,0,[], Robust Parameter Design,seg_441,b (depth) shallow medium deep
3882,1,"['levels', 'factors', 'experiment', 'table', 'control', 'experiments', 'process']", Robust Parameter Design,seg_441,"case study 15.3: solder process optimization: in an experiment described in understanding industrial designed experiments by schmidt and launsby (1991; see the bibliography), solder process optimization is accomplished by a printed circuit-board assembly plant. parts are inserted either manually or automatically into a bare board with a circuit printed on it. after the parts are inserted, the board is put through a wave solder machine, which is used to connect all the parts into the circuit. boards are placed on a conveyor and taken through a series of steps. they are bathed in a flux mixture to remove oxide. to minimize warpage, they are preheated before the solder is applied. soldering takes place as the boards move across the wave of solder. the object of the experiment is to minimize the number of solder defects per million joints. the control factors and levels are as given in table 15.19."
3883,1,"['control', 'case', 'factors']", Robust Parameter Design,seg_441,table 15.19: control factors for case study 15.3
3884,1,"['experimental', 'process', 'factors', 'level', 'control']", Robust Parameter Design,seg_441,these factors are easy to control at the experimental level but are more formidable at the plant or process level.
3885,1,"['levels', 'factors', 'variability', 'factor', 'table', 'nominal', 'control', 'process', 'response', 'processes']", Robust Parameter Design,seg_441,"often in processes such as this one, the natural noise factors are tolerances on the control factors. for example, in the actual on-line process, solder pot temperature and conveyor-belt speed are difficult to control. it is known that the control of temperature is within ±5◦f and the control of conveyor-belt speed is within ±0.2 ft/min. it is certainly conceivable that variability in the product response (soldering performance) is increased because of an inability to control these two factors at some nominal levels. the third noise factor is the type of assembly involved. in practice, one of two types of assemblies will be used. thus, we have the noise factors given in table 15.20."
3886,1,"['case', 'factors']", Robust Parameter Design,seg_441,table 15.20: noise factors for case study 15.3
3887,1,"['deviation', 'nominal']", Robust Parameter Design,seg_441,"a*, solder pot temperature tolerance (◦f) −5 +5 (deviation from nominal) b*, conveyor speed tolerance (ft/min) −0.2 +0.2 (deviation from ideal) c*, assembly type 1 2"
3888,1,['control'], Robust Parameter Design,seg_441,"both the control array (inner array) and the noise array (outer array) were chosen to be fractional factorials, the former a 4"
3889,1,"['combination', 'table', 'plots', 'interactions', 'mean', 'standard', 'control', 'response']", Robust Parameter Design,seg_441,"the crossed array and the response values are shown in table 15.21. the first three columns of the inner array represent a 23. the fourth and fifth columns are formed by d = −ac and e = −bc. thus, the defining interactions for the inner array are acd, bce, and abde. the outer array is a standard resolution iii fraction of a 23. notice that each inner array point contains runs from the outer array. thus, four response values are observed at each combination of the control array. figure 15.19 displays plots which reveal the effect of temperature and density on the mean response."
3890,1,"['response', 'case']", Robust Parameter Design,seg_441,table 15.21: crossed arrays and response values for case study 15.3
3891,0,[], Robust Parameter Design,seg_441,solder pot temperature flux density
3892,0,[], Robust Parameter Design,seg_441,120 120 low high low high
3893,1,"['plot', 'factors', 'response', 'mean']", Robust Parameter Design,seg_441,figure 15.19: plot showing the influence of factors on the mean response.
3894,1,"['process', 'variables', 'information', 'mean', 'varying', 'control', 'variance', 'response']", Robust Parameter Design,seg_441,"in most examples using rpd, the analyst is interested in finding conditions on the control variables that give suitable values for the mean response ȳ. however, varying the noise variables produces information on the process variance σy2 that might be anticipated in the process. obviously a robust product is one for which the process is consistent and thus has a small process variance. rpd may involve the simultaneous analysis of ȳ and sy."
3895,1,"['factors', 'case']", Robust Parameter Design,seg_441,"it turns out that temperature and flux density are the most important factors in case study 15.3. they seem to influence both sy and ȳ. fortunately, high temperature and low flux density are preferable for both. from figure 15.19, the “optimum” conditions are"
3896,0,[], Robust Parameter Design,seg_441,"solder temperature = 510◦f, flux density = 0.9◦."
3897,1,"['sample', 'model', 'process', 'sample mean', 'experiment', 'mean', 'sample variance', 'variance']", Robust Parameter Design,seg_441,"one approach suggested by many is to model the sample mean and sample variance separately. separate modeling often helps the experimenter to obtain a better understanding of the process involved. in the following example, we illustrate this approach with the solder process experiment."
3898,1,"['case', 'set', 'sample', 'linear', 'data', 'mean', 'standard', 'standard deviation', 'factors', 'regression', 'data set', 'linear regression', 'control', 'response', 'deviation', 'sample standard deviation', 'errors']", Robust Parameter Design,seg_441,"case study 15.4: consider the data set of case study 15.3. an alternative approach is to fit separate models for the mean ȳ and the sample standard deviation. suppose that we use the usual +1 and −1 coding for the control factors. based on the apparent importance of solder pot temperature x1 and flux density x2, linear regression on the response (number of errors per million joints) produces"
3899,1,"['levels', 'variability', 'mean', 'transformation', 'response']", Robust Parameter Design,seg_441,"to find the most robust levels of temperature and flux density, it is important to procure a compromise between the mean response and variability, which requires a modeling of the variability. an important tool in this regard is the log transformation (see bartlett and kendall, 1946, or carroll and ruppert, 1988):"
3900,1,['process'], Robust Parameter Design,seg_441,this modeling process produces the following result:
3901,1,"['sample', 'model', 'linear', 'linear model', 'method', 'method of least squares', 'results', 'homogeneous', 'least squares', 'normality', 'transformation', 'sample variance', 'variance', 'response']", Robust Parameter Design,seg_441,"the log linear model finds extensive use for modeling sample variance, since the log transformation on the sample variance lends itself to use of the method of least squares. this results from the fact that normality and homogeneous variance assumptions are often quite good when one uses ln s2 rather than s2 as the model response."
3902,1,"['estimates', 'plots', 'location', 'process', 'graphical', 'mean', 'standard', 'standard deviation', 'response', 'deviation', 'variability', 'errors', 'response surface']", Robust Parameter Design,seg_441,"the analysis that is important to the scientist or engineer makes use of the two models simultaneously. a graphical approach can be very useful. figure 15.20 shows simple plots of the mean and standard deviation models simultaneously. as one would expect, the location of temperature and flux density that minimizes the mean number of errors is the same as that which minimizes variability, namely high temperature and low flux density. the graphical multiple response surface approach allows the user to see tradeoffs between process mean and process variability. for this example, the engineer may be dissatisfied with the extreme conditions in solder temperature and flux density. the figure offers estimates of how much is lost as one moves away from the optimum mean and variability conditions to any intermediate conditions."
3903,1,"['process', 'variables', 'case', 'distribution', 'mean', 'control', 'variance', 'response', 'response surface']", Robust Parameter Design,seg_441,"in case study 15.4, values for control variables were chosen that gave desirable conditions for both the mean and the variance of the process. the mean and variance were taken across the distribution of noise variables in the process and were modeled separately, and appropriate conditions were found through a dual response surface approach. since case study 15.4 involved two models (mean and variance), this can be viewed as a dual response surface analysis. fortunately, in this example the same conditions on the two relevant control variables, solder"
3904,1,"['deviation', 'case', 'mean', 'standard', 'standard deviation']", Robust Parameter Design,seg_441,figure 15.20: mean and standard deviation for case study 15.4.
3905,1,"['variance', 'process', 'mean']", Robust Parameter Design,seg_441,"temperature and flux density, were optimal for both the process mean and the variance. much of the time in practice some type of compromise between the mean and variance would need to be invoked."
3906,1,"['levels', 'design', 'case', 'process', 'data', 'mean', 'model', 'experimental', 'control', 'variance', 'response', 'variables', 'variation', 'response surface']", Robust Parameter Design,seg_441,"the approach illustrated in case study 15.4 involves finding optimal process conditions when the data used are from a product array (or crossed array) type of experimental design. often, using the product array, a cross between two designs, can be very costly. however, the development of dual response surface models, i.e., a model for the mean and a model for the variance, can be accomplished without a product array. a design that involves both control and noise variables is often called a combined array. this type of design and the resulting analysis can be used to determine what conditions on the control variables are most robust (insensitive) to variation in the noise variables. this can be viewed as tantamount to finding control levels that minimize the process variance produced by movement in the noise variables."
3907,1,"['process', 'variables', 'table', 'interaction', 'data', 'variable', 'function', 'control', 'variance']", Robust Parameter Design,seg_441,"the structure of the process variance is greatly determined by the nature of the control-by-noise interaction. the nature of the nonhomogeneity of process variance is a function of which control variables interact with which noise variables. specifically, as we will illustrate, those control variables that interact with one or more noise variables can be the object of the analysis. for example, let us consider an illustration used in myers, montgomery, and anderson-cook (2009) involving two control variables and a single noise variable with the data given in table 15.22. a and b are control variables and c is a noise variable."
3908,1,"['plots', 'interactions']", Robust Parameter Design,seg_441,"one can illustrate the interactions ac and bc with plots, as given in figure"
3909,1,"['experimental', 'data']", Robust Parameter Design,seg_441,table 15.22: experimental data in a crossed array
3910,1,"['response', 'mean']", Robust Parameter Design,seg_441,inner array outer array a b c = −1 c = +1 response mean −1 −1 11 15 13.0 −1 1 7 8 7.5 1 −1 10 26 18.0 1 1 10 14 12.0
3911,1,"['process', 'levels', 'information', 'distribution', 'variable', 'probability distribution', 'probability', 'variance']", Robust Parameter Design,seg_441,"15.21. one must understand that while a and b are held constant in the process c follows a probability distribution during the process. given this information, it becomes clear that a = −1 and b = +1 are levels that produce smaller values for the process variance, while a = +1 and b = −1 give larger values. thus, we say that a = −1 and b = +1 are robust values, i.e., insensitive to inevitable changes in the noise variable c during the process."
3912,1,"['table', 'plots', 'interaction', 'data']", Robust Parameter Design,seg_441,figure 15.21: interaction plots for the data in table 15.22.
3913,1,"['process', 'factors', 'location', 'mean', 'variance', 'dispersion']", Robust Parameter Design,seg_441,"in the above example, we say that both a and b are dispersion effects (i.e. both factors impact the process variance). in addition, both factors are location effects since the mean of y changes as both factors move from −1 to +1."
3914,1,"['model', 'experiment', 'variables', 'results', 'interactions', 'control', 'process', 'response']", Robust Parameter Design,seg_441,"while it has been emphasized that noise variables are not constant during the working of the process, analysis that results in desirable or even optimal conditions on the control variables is best accomplished through an experiment in which both control and noise variables are fixed effects. thus, both main effects in the control and noise variables and all the important control-by-noise interactions can be evaluated. this model in x and z, often called a response model, can both"
3915,1,"['case', 'factorial', 'process', 'data', 'information', 'mean', 'model', 'experimental', 'control', 'response', 'variance', 'table', 'variables', 'variable', 'response surface']", Robust Parameter Design,seg_441,"directly and indirectly provide useful information regarding the process. the response model is actually a response surface model in vector x and vector z, where x contains control variables and z the noise variables. certain operations allow models to be generated for the process mean and variance much as in case study 15.4. details are supplied in myers, montgomery, and anderson-cook (2009); we will illustrate with a very simple example. consider the data of table 15.22 on page 650 with control variables a and b and noise variable c. there are eight experimental runs in a 22 × 2, or 23, factorial. thus, the response model can be written"
3916,1,"['model', 'regression model', 'table', 'interaction', 'regression', 'independence', 'variance', 'error']", Robust Parameter Design,seg_441,"we will not include the three-factor interaction in the regression model. a, b, and c in table 15.22 are represented by x1, x2, and z, respectively, in the model. we assume that the error term has the usual independence and constant variance properties."
3917,1,"['process', 'variable', 'expectation', 'response', 'mean', 'continuous', 'variance']", Robust Parameter Design,seg_441,"the process mean and variance response surfaces are best understood by considering the expectation and variance of z across the process. we assume that the noise variable c [denoted by z in y(x, z)] is continuous with mean 0 and variance σz2. the process mean and variance models may be viewed as"
3918,1,['slope'], Robust Parameter Design,seg_441,"where lx is the slope ∂y(x,z) in the direction of z. as we indicated earlier, note how"
3919,1,"['process', 'factors', 'variable', 'interactions', 'variance']", Robust Parameter Design,seg_441,∂z the interactions of factors a and b with the noise variable c are key components of the process variance.
3920,1,"['process', 'condition', 'estimate', 'results', 'plots', 'interactions', 'coefficient', 'variance']", Robust Parameter Design,seg_441,"though we have already analyzed the current example through plots in figure 15.21, which displayed the role of ab and ac interactions, it is instructive to look at the analysis in light of ez[y(x, z)] and varz[y(x, z)] above. in this example, the reader can easily verify the estimate b1z for β1z is 15/8 while the estimate b2z for β2z is −15/8. the coefficient b3 = 25/8. thus, the condition x1 = +1 and x2 = −1 results in a process variance estimate of"
3921,1,"['process', 'condition', 'estimated', 'variable', 'variance']", Robust Parameter Design,seg_441,"thus, for the most desirable (robust) condition of x1 = −1 and x2 = 1, the estimated process variance due to the noise variable c (or z) is (25/64)σz2. the"
3922,1,"['factorial', 'factorial experiments', 'experiments']", Robust Parameter Design,seg_441,652 chapter 15 2k factorial experiments and fractions
3923,1,"['process', 'estimated', 'condition', 'mean', 'variance', 'response']", Robust Parameter Design,seg_441,"most undesirable condition, the condition of maximum process variance (i.e., x1 = +1 and x2 = −1), produces an estimated process variance of (3025/64)σz2. as far as the mean response is concerned, figure 15.21 indicates that if maximum response is desired x1 = +1 and x2 = −1 produce the best result."
3924,1,"['plots', 'interaction', 'data']", Robust Parameter Design,seg_441,figure 15.22: interaction plots for the data in exercise 15.31.
3925,1,"['design', 'factorial', 'factorial experiments', 'interactions', 'experiments']", Potential Misconceptions and Hazards Relationship to Material in Other Chapters,seg_447,"in the use of fractional factorial experiments, one of the most important considerations that the analyst must be aware of is the design resolution. a design of low resolution is smaller (and hence cheaper) than one of higher resolution. however, a price is paid for the cheaper design. the design of lower resolution has heavier aliasing than one of higher resolution. for example, if the researcher has expectations that two-factor interactions may be important, then resolution iii should not be used. a resolution iii design is strictly a main effects plan."
3926,1,"['random samples', 'random', 'sample', 'samples', 'populations', 'tests', 'distributions', 'sample size', 'continuous', 'test', 'normality', 'normal']", Nonparametric Tests,seg_451,"most of the hypothesis-testing procedures discussed in previous chapters are based on the assumption that the random samples are selected from normal populations. fortunately, most of these tests are still reliable when we experience slight departures from normality, particularly when the sample size is large. traditionally, these testing procedures have been referred to as parametric methods. in this chapter, we consider a number of alternative test procedures, called nonparametric or distribution-free methods, that often assume no knowledge whatsoever about the distributions of the underlying populations, except perhaps that they are continuous."
3927,1,"['nonparametric methods', 'ordinal', 'data', 'ordinal scale']", Nonparametric Tests,seg_451,"nonparametric, or distribution-free procedures, are used with increasing frequency by data analysts. there are many applications in science and engineering where the data are reported as values not on a continuum but rather on an ordinal scale such that it is quite natural to assign ranks to the data. in fact, the reader may notice quite early in this chapter that the distribution-free methods described here involve an analysis of ranks. most analysts find the computations involved in nonparametric methods to be very appealing and intuitive."
3928,1,['test'], Nonparametric Tests,seg_451,"for an example where a nonparametric test is applicable, consider the situation in which two judges rank five brands of premium beer by assigning a rank of 1 to the brand believed to have the best overall quality, a rank of 2 to the second best, and so forth. a nonparametric test could then be used to determine whether there is any agreement between the two judges."
3929,1,"['sample size', 'sample', 'tests', 'efficient', 'information', 'nonparametric tests', 'test']", Nonparametric Tests,seg_451,"we should also point out that there are a number of disadvantages associated with nonparametric tests. primarily, they do not utilize all the information provided by the sample, and thus a nonparametric test will be less efficient than the corresponding parametric procedure when both methods are applicable. consequently, to achieve the same power, a nonparametric test will require a larger sample size than will the corresponding parametric test."
3930,1,"['tests', 'case', 'normality', 'standard']", Nonparametric Tests,seg_451,"as we indicated earlier, slight departures from normality result in minor deviations from the ideal for the standard parametric tests. this is particularly true for the t-test and the f-test. in the case of the t-test and the f-test, the p-value"
3931,1,"['normality', 'error']", Nonparametric Tests,seg_451,quoted may be slightly in error if there is a moderate violation of the normality assumption.
3932,1,"['efficient', 'quantitative', 'set', 'measurements', 'data', 'standard', 'experimental', 'standard normal', 'test', 'method', 'nonparametric method', 'statisticians', 'normality', 'normal']", Nonparametric Tests,seg_451,"in summary, if a parametric and a nonparametric test are both applicable to the same set of data, we should carry out the more efficient parametric technique. however, we should recognize that the assumptions of normality often cannot be justified and that we do not always have quantitative measurements. it is fortunate that statisticians have provided us with a number of useful nonparametric procedures. armed with nonparametric techniques, the data analyst has more ammunition to accommodate a wider variety of experimental situations. it should be pointed out that even under the standard normal theory assumptions, the efficiencies of the nonparametric techniques are remarkably close to those of the corresponding parametric procedure. on the other hand, serious departures from normality will render the nonparametric method much more efficient than the parametric procedure."
3933,1,"['sample', 'normal', 'population', 'test', 'null hypothesis', 'hypothesis']", Nonparametric Tests,seg_451,"the reader should recall that the procedures discussed in section 10.4 for testing the null hypothesis that μ = μ0 are valid only if the population is approximately normal or if the sample is large. if n < 30 and the population is decidedly nonnormal, we must resort to a nonparametric test."
3934,1,"['sample median', 'sample', 'median', 'case', 'location', 'sign test', 'random variable', 'variable', 'hypotheses', 'mean', 'population', 'random', 'continuous', 'parameter', 'test', 'location parameter']", Nonparametric Tests,seg_451,"the sign test is used to test hypotheses on a population median. in the case of many of the nonparametric procedures, the mean is replaced by the median as the pertinent location parameter under test. recall that the sample median was defined in section 1.3. the population counterpart, denoted by μ̃, has an analogous definition. given a random variable x, μ̃ is defined such that p (x > μ̃) ≤ 0.5 and p (x < μ̃) ≤ 0.5. in the continuous case,"
3935,1,"['sample', 'median', 'symmetric', 'population mean', 'random sample', 'distribution', 'mean', 'population', 'random', 'null hypothesis', 'hypothesis']", Nonparametric Tests,seg_451,"of course, if the distribution is symmetric, the population mean and median are equal. in testing the null hypothesis h0 that μ̃ = μ̃0 against an appropriate alternative, on the basis of a random sample of size n, we replace each sample value exceeding μ̃0 with a plus sign and each sample value less than μ̃0 with a minus sign. if the null hypothesis is true and the population is symmetric, the sum of the plus signs should be approximately equal to the sum of the minus signs. when one sign appears more frequently than it should based on chance alone, we reject the hypothesis that the population median μ̃ is equal to μ̃0."
3936,1,"['sample size', 'sample', 'precision', 'observations', 'data', 'observation', 'sign test', 'probability', 'population', 'continuous', 'test']", Nonparametric Tests,seg_451,"in theory, the sign test is applicable only in situations where μ̃0 cannot equal the value of any of the observations. although there is a zero probability of obtaining a sample observation exactly equal to μ̃0 when the population is continuous, nevertheless, in practice a sample value equal to μ̃0 will often occur from a lack of precision in recording the data. when sample values equal to μ̃0 are observed, they are excluded from the analysis and the sample size is correspondingly reduced."
3937,1,"['sample', 'test statistic', 'results', 'random sample', 'sign test', 'variable', 'random variable', 'statistic', 'binomial random variable', 'binomial', 'probability', 'random', 'test', 'null hypothesis', 'hypothesis']", Nonparametric Tests,seg_451,"the appropriate test statistic for the sign test is the binomial random variable x, representing the number of plus signs in our random sample. if the null hypothesis that μ̃ = μ̃0 is true, the probability that a sample value results in either a plus or a minus sign is equal to 1/2. therefore, to test the null hypothesis that"
3938,1,"['distribution', 'random variable', 'variable', 'binomial', 'parameter', 'random', 'binomial distribution', 'test', 'null hypothesis', 'hypothesis']", Nonparametric Tests,seg_451,"μ̃ = μ̃0, we actually test the null hypothesis that the number of plus signs is a value of a random variable having the binomial distribution with the parameter p = 1/2. p-values for both one-sided and two-sided alternatives can then be calculated using this binomial distribution. for example, in testing"
3939,1,"['random variable', 'variable', 'random']", Nonparametric Tests,seg_451,"we shall reject h0 in favor of h1 only if the proportion of plus signs is sufficiently less than 1/2, that is, when the value x of our random variable is small. hence, if the computed p-value"
3940,1,"['level', 'significance', 'significance level', 'table']", Nonparametric Tests,seg_451,"is less than or equal to some preselected significance level α, we reject h0 in favor of h1. for example, when n = 15 and x = 3, we find from table a.1 that"
3941,1,"['level', 'null hypothesis', 'hypothesis']", Nonparametric Tests,seg_451,so the null hypothesis μ̃ = μ̃0 can certainly be rejected at the 0.05 level of significance but not at the 0.01 level.
3942,1,"['test', 'hypothesis']", Nonparametric Tests,seg_451,to test the hypothesis
3943,0,[], Nonparametric Tests,seg_451,"we reject h0 in favor of h1 only if the proportion of plus signs is sufficiently greater than 1/2, that is, when x is large. hence, if the computed p-value"
3944,1,"['test', 'hypothesis']", Nonparametric Tests,seg_451,"is less than α, we reject h0 in favor of h1. finally, to test the hypothesis"
3945,0,[], Nonparametric Tests,seg_451,"we reject h0 in favor of h1 when the proportion of plus signs is significantly less than or greater than 1/2. this, of course, is equivalent to x being sufficiently small or sufficiently large. therefore, if x < n/2 and the computed p-value"
3946,0,[], Nonparametric Tests,seg_451,"is less than or equal to α, or if x > n/2 and the computed p-value"
3947,0,[], Nonparametric Tests,seg_451,"is less than or equal to α, we reject h0 in favor of h1."
3948,1,"['curve', 'probabilities', 'normal', 'binomial', 'test', 'hypothesis']", Nonparametric Tests,seg_451,"whenever n > 10, binomial probabilities with p = 1/2 can be approximated from the normal curve, since np = nq > 5. suppose, for example, that we wish to test the hypothesis"
3949,1,"['sample', 'curve', 'random', 'approximation', 'normal', 'random sample', 'level', 'significance', 'level of significance']", Nonparametric Tests,seg_451,"at the α = 0.05 level of significance, for a random sample of size n = 20 that yields x = 6 plus signs. using the normal curve approximation with"
3950,1,"['null hypothesis', 'hypothesis']", Nonparametric Tests,seg_451,which leads to the nonrejection of the null hypothesis.
3951,1,['data'], Nonparametric Tests,seg_451,example 16.1: the following data represent the number of hours that a rechargeable hedge trimmer operates before a recharge is required:
3952,1,"['median', 'sign test', 'level', 'significance', 'test', 'level of significance', 'hypothesis']", Nonparametric Tests,seg_451,"use the sign test to test the hypothesis, at the 0.05 level of significance, that this particular trimmer operates a median of 1.8 hours before requiring a recharge."
3953,1,"['test statistic', 'variable', 'statistic', 'binomial', 'test']", Nonparametric Tests,seg_451,1 4. test statistic: binomial variable x with p = 2 .
3954,0,[], Nonparametric Tests,seg_451,5. computations: replacing each value by the symbol “+” if it exceeds 1.8 and
3955,1,['measurement'], Nonparametric Tests,seg_451,"by the symbol “−” if it is less than 1.8 and discarding the one measurement that equals 1.8, we obtain the sequence"
3956,1,['table'], Nonparametric Tests,seg_451,"for which n = 10, x = 3, and n/2 = 5. therefore, from table a.1 the computed p-value is"
3957,1,"['median', 'null hypothesis', 'hypothesis']", Nonparametric Tests,seg_451,6. decision: do not reject the null hypothesis and conclude that the median
3958,0,[], Nonparametric Tests,seg_451,operating time is not significantly different from 1.8 hours.
3959,1,"['null hypothesis', 'symmetric', 'observations', 'skewed', 'paired observations', 'sign test', 'adjusted', 'hypotheses', 'population', 'populations', 'medians', 'test', 'paired', 'hypothesis']", Nonparametric Tests,seg_451,"we can also use the sign test to test the null hypothesis μ̃1 − μ̃2 = d0 for paired observations. here we replace each difference, di, with a plus or minus sign depending on whether the adjusted difference, di − d0, is positive or negative. throughout this section, we have assumed that the populations are symmetric. however, even if populations are skewed, we can carry out the same test procedure, but the hypotheses refer to the population medians rather than the means."
3960,1,"['table', 'level', 'significance', 'test', 'level of significance']", Nonparametric Tests,seg_451,"example 16.2: a taxi company is trying to decide whether the use of radial tires instead of regular belted tires improves fuel economy. sixteen cars are equipped with radial tires and driven over a prescribed test course. without changing drivers, the same cars are then equipped with the regular belted tires and driven once again over the test course. the gasoline consumption, in kilometers per liter, is given in table 16.1. can we conclude at the 0.05 level of significance that cars equipped with radial tires obtain better fuel economy than those equipped with regular belted tires?"
3961,1,['data'], Nonparametric Tests,seg_451,table 16.1: data for example 16.2
3962,1,['median'], Nonparametric Tests,seg_451,"solution : let μ̃1 and μ̃2 represent the median kilometers per liter for cars equipped with radial and belted tires, respectively."
3963,1,"['test statistic', 'variable', 'statistic', 'binomial', 'test']", Nonparametric Tests,seg_451,4. test statistic: binomial variable x with p = 1/2.
3964,0,[], Nonparametric Tests,seg_451,5. computations: after replacing each positive difference by a “+” symbol and
3965,0,[], Nonparametric Tests,seg_451,"each negative difference by a “−” symbol and then discarding the two zero differences, we obtain the sequence"
3966,1,"['curve', 'approximation', 'normal']", Nonparametric Tests,seg_451,"for which n = 14 and x = 11. using the normal curve approximation, we find"
3967,1,['average'], Nonparametric Tests,seg_451,"6. decision: reject h0 and conclude that, on the average, radial tires do improve"
3968,1,"['data', 'sign test', 'experiments', 'test', 'response', 'numerical']", Nonparametric Tests,seg_451,"not only is the sign test one of the simplest nonparametric procedures to apply; it has the additional advantage of being applicable to dichotomous data that cannot be recorded on a numerical scale but can be represented by positive and negative responses. for example, the sign test is applicable in experiments where a qualitative response such as “hit” or “miss” is recorded, and in sensory-type experiments where a plus or minus sign is recorded depending on whether the taste tester correctly or incorrectly identifies the desired ingredient."
3969,1,"['tests', 'symmetric', 'case', 'sampling', 'distribution', 'sign test', 'normal', 'test', 'normal distribution']", Nonparametric Tests,seg_451,"we shall attempt to make comparisons between many of the nonparametric procedures and the corresponding parametric tests. in the case of the sign test the competition is, of course, the t-test. if we are sampling from a normal distribution, the use of the t-test will result in a larger power for the test. if the distribution is merely symmetric, though not normal, the t-test is preferred in terms of power unless the distribution has extremely “heavy tails” compared to the normal distribution."
3970,1,"['observations', 'case', 'sign test', 'test']", SignedRank Test,seg_453,"the reader should note that the sign test utilizes only the plus and minus signs of the differences between the observations and μ̃0 in the one-sample case, or the plus and minus signs of the differences between the pairs of observations in the paired-sample case; it does not take into consideration the magnitudes of these differences. a test utilizing both direction and magnitude, proposed in 1945 by frank wilcoxon, is now commonly referred to as thewilcoxon signed-rank test."
3971,1,"['absolute value', 'sample', 'continuous distribution', 'condition', 'symmetric', 'case', 'data', 'information', 'distribution', 'continuous', 'test', 'average', 'null hypothesis', 'hypothesis']", SignedRank Test,seg_453,"the analyst can extract more information from the data in a nonparametric fashion if it is reasonable to invoke an additional restriction on the distribution from which the data were taken. the wilcoxon signed-rank test applies in the case of a symmetric continuous distribution. under this condition, we can test the null hypothesis μ̃ = μ̃0. we first subtract μ̃0 from each sample value, discarding all differences equal to zero. the remaining differences are then ranked without regard to sign. a rank of 1 is assigned to the smallest absolute difference (i.e., without sign), a rank of 2 to the next smallest, and so on. when the absolute value of two or more differences is the same, assign to each the average of the ranks that would have been assigned if the differences were distinguishable. for example, if the fifth and sixth smallest differences are equal in absolute value, each is assigned a rank of 5.5. if the hypothesis μ̃ = μ̃0 is true, the total of the ranks corresponding to the positive differences should nearly equal the total of the ranks corresponding to the negative differences. let us represent these totals by w+ and w−, respectively. we designate the smaller of w+ and w− by w."
3972,1,"['alternative hypothesis', 'random variables', 'variables', 'samples', 'random', 'vary', 'null hypothesis', 'hypothesis']", SignedRank Test,seg_453,"in selecting repeated samples, we would expect w+ and w−, and therefore w, to vary. thus, we may think of w+, w−, and w as values of the corresponding random variables w+, w−, and w . the null hypothesis μ̃ = μ̃0 can be rejected in favor of the alternative μ̃ < μ̃0 only if w+ is small and w− is large. likewise, the alternative μ̃ > μ̃0 can be accepted only if w+ is large and w− is small. for a two-sided alternative, we may reject h0 in favor of h1 if either w+ or w−, and hence w, is sufficiently small. therefore, no matter what the alternative hypothesis"
3973,1,"['statistic', 'null hypothesis', 'hypothesis']", SignedRank Test,seg_453,"may be, we reject the null hypothesis when the value of the appropriate statistic w+, w−, or w is sufficiently small."
3974,1,"['table', 'symmetric', 'observations', 'case', 'paired observations', 'sampling', 'cases', 'continuous', 'test', 'paired', 'null hypothesis', 'hypothesis']", SignedRank Test,seg_453,"to test the null hypothesis that we are sampling two continuous symmetric populations with μ̃1 = μ̃2 for the paired-sample case, we rank the differences of the paired observations without regard to sign and proceed as in the single-sample case. the various test procedures for both the singleand paired-sample cases are summarized in table 16.2."
3975,1,['test'], SignedRank Test,seg_453,table 16.2: signed-rank test
3976,1,"['critical values', 'levels', 'table', 'null hypothesis', 'level', 'significance', 'test', 'level of significance', 'hypothesis']", SignedRank Test,seg_453,"it is not difficult to show that whenever n < 5 and the level of significance does not exceed 0.05 for a one-tailed test or 0.10 for a two-tailed test, all possible values of w+, w−, or w will lead to the acceptance of the null hypothesis. however, when 5 ≤ n ≤ 30, table a.16 shows approximate critical values of w+ and w− for levels of significance equal to 0.01, 0.025, and 0.05 for a one-tailed test and critical values of w for levels of significance equal to 0.02, 0.05, and 0.10 for a two-tailed test. the null hypothesis is rejected if the computed value w+, w−, or w is less than or equal to the appropriate tabled value. for example, when n = 12, table a.16 shows that a value of w+ ≤ 17 is required for the one-sided alternative μ̃ < μ̃0 to be significant at the 0.05 level."
3977,1,['test'], SignedRank Test,seg_453,example 16.3: rework example 16.1 by using the signed-rank test.
3978,1,"['measurement', 'critical region']", SignedRank Test,seg_453,4. critical region: since n = 10 after discarding the one measurement that equals
3979,1,"['critical region', 'table']", SignedRank Test,seg_453,"1.8, table a.16 shows the critical region to be w ≤ 8."
3980,1,['measurement'], SignedRank Test,seg_453,5. computations: subtracting 1.8 from each measurement and then ranking the
3981,0,[], SignedRank Test,seg_453,"differences without regard to sign, we have"
3982,1,['median'], SignedRank Test,seg_453,"6. decision: as before, do not reject h0 and conclude that the median operating"
3983,0,[], SignedRank Test,seg_453,time is not significantly different from 1.8 hours.
3984,1,"['symmetric', 'populations', 'case', 'sign test', 'adjusted', 'test', 'null hypothesis', 'hypothesis']", SignedRank Test,seg_453,"the signed-rank test can also be used to test the null hypothesis that μ̃1− μ̃2 = d0. in this case, the populations need not be symmetric. as with the sign test, we subtract d0 from each difference, rank the adjusted differences without regard to sign, and apply the same procedure as above."
3985,1,"['sample', 'table', 'scores', 'random', 'test']", SignedRank Test,seg_453,"example 16.4: it is claimed that a college senior can increase his or her score in the major field area of the graduate record examination by at least 50 points if he or she is provided with sample problems in advance. to test this claim, 20 college seniors are divided into 10 pairs such that the students in each matched pair have almost the same overall grade-point averages for their first 3 years in college. sample problems and answers are provided at random to one member of each pair 1 week prior to the examination. the examination scores are given in table 16.3."
3986,1,['data'], SignedRank Test,seg_453,table 16.3: data for example 16.4
3987,1,['sample'], SignedRank Test,seg_453,with sample problems 531 621 663 579 451 660 591 719 543 575 without sample problems 509 540 688 502 424 683 568 748 530 524
3988,1,"['alternative hypothesis', 'sample', 'scores', 'level', 'significance', 'level of significance', 'null hypothesis', 'hypothesis']", SignedRank Test,seg_453,"test the null hypothesis, at the 0.05 level of significance, that sample problems increase scores by 50 points against the alternative hypothesis that the increase is less than 50 points."
3989,1,"['sample', 'median', 'scores', 'test']", SignedRank Test,seg_453,"solution : let μ̃1 and μ̃2 represent the median scores of all students taking the test in question with and without sample problems, respectively."
3990,1,"['critical region', 'table']", SignedRank Test,seg_453,"4. critical region: since n = 10, table a.16 shows the critical region to be"
3991,0,[], SignedRank Test,seg_453,5. computations:
3992,1,"['sample', 'average']", SignedRank Test,seg_453,"6. decision: reject h0 and conclude that sample problems do not, on average,"
3993,0,[], SignedRank Test,seg_453,increase one’s graduate record score by as much as 50 points.
3994,1,"['sampling', 'distribution', 'normal', 'mean', 'variance', 'sampling distribution', 'normal distribution']", SignedRank Test,seg_453,"when n ≥ 15, the sampling distribution of w+ (or w−) approaches the normal distribution with mean and variance given by"
3995,1,"['statistic', 'table']", SignedRank Test,seg_453,"therefore, when n exceeds the largest value in table a.16, the statistic"
3996,1,"['test', 'critical region']", SignedRank Test,seg_453,can be used to determine the critical region for the test.
3997,1,"['independent', 'observations', 'continuous distributions', 'normality', 'samples', 'normal', 'continuous', 'test', 'distributions']", Wilcoxon RankSum Test,seg_457,"as we indicated earlier, the nonparametric procedure is generally an appropriate alternative to the normal theory test when the normality assumption does not hold. when we are interested in testing equality of means of two continuous distributions that are obviously nonnormal, and samples are independent (i.e., there is no pairing of observations), the wilcoxon rank-sum test or wilcoxon two-sample test is an appropriate alternative to the two-sample t-test described in chapter 10."
3998,1,"['sample', 'observations', 'random sample', 'populations', 'case', 'observation', 'samples', 'mean', 'random', 'number of observations', 'test', 'null hypothesis', 'hypothesis']", Wilcoxon RankSum Test,seg_457,"we shall test the null hypothesis h0 that μ̃1 = μ̃2 against some suitable alternative. first we select a random sample from each of the populations. let n1 be the number of observations in the smaller sample, and n2 the number of observations in the larger sample. when the samples are of equal size, n1 and n2 may be randomly assigned. arrange the n1 + n2 observations of the combined samples in ascending order and substitute a rank of 1, 2, . . . , n1 + n2 for each observation. in the case of ties (identical observations), we replace the observations by the mean of the ranks that the observations would have if they were distinguishable. for example, if the seventh and eighth observations were identical, we would assign a rank of 7.5 to each of the two observations."
3999,1,"['sample', 'experiment', 'observations', 'results', 'samples', 'number of observations', 'numerical']", Wilcoxon RankSum Test,seg_457,"the sum of the ranks corresponding to the n1 observations in the smaller sample is denoted by w1. similarly, the value w2 represents the sum of the n2 ranks corresponding to the larger sample. the total w1+w2 depends only on the number of observations in the two samples and is in no way affected by the results of the experiment. hence, if n1 = 3 and n2 = 4, then w1 + w2 = 1 + 2 + · · · + 7 = 28, regardless of the numerical values of the observations. in general,"
4000,0,[], Wilcoxon RankSum Test,seg_457,"the arithmetic sum of the integers 1, 2, . . . , n1 + n2. once we have determined w1, it may be easier to find w2 by the formula"
4001,1,"['random variables', 'variables', 'samples', 'random', 'test', 'vary', 'null hypothesis', 'hypothesis']", Wilcoxon RankSum Test,seg_457,"in choosing repeated samples of sizes n1 and n2, we would expect w1, and therefore w2, to vary. thus, we may think of w1 and w2 as values of the random variables w1 and w2, respectively. the null hypothesis μ̃1 = μ̃2 will be rejected in favor of the alternative μ̃1 < μ̃2 only if w1 is small and w2 is large. likewise, the alternative μ̃1 > μ̃2 can be accepted only if w1 is large and w2 is small. for a two-tailed test, we may reject h0 in favor of h1 if w1 is small and w2 is large or if w1 is large and w2 is small. in other words, the alternative μ̃1 < μ̃2 is accepted if w1 is sufficiently small; the alternative μ̃1 > μ̃2 is accepted if w2 is sufficiently small; and the alternative μ̃1 = μ̃2 is accepted if the minimum of w1 and w2 is sufficiently small. in actual practice, we usually base our decision on the value"
4002,1,"['critical values', 'statistics', 'tables', 'statistic']", Wilcoxon RankSum Test,seg_457,"of the related statistic u1 or u2 or on the value u of the statistic u , the minimum of u1 and u2. these statistics simplify the construction of tables of critical values,"
4003,1,"['interval', 'symmetric', 'sampling', 'sampling distributions', 'distributions']", Wilcoxon RankSum Test,seg_457,since both u1 and u2 have symmetric sampling distributions and assume values in the interval from 0 to n1n2 such that u1 + u2 = n1n2.
4004,1,"['critical value', 'table', 'statistic', 'test', 'null hypothesis', 'hypothesis']", Wilcoxon RankSum Test,seg_457,"from the formulas for u1 and u2 we see that u1 will be small when w1 is small and u2 will be small when w2 is small. consequently, the null hypothesis will be rejected whenever the appropriate statistic u1, u2, or u assumes a value less than or equal to the desired critical value given in table a.17. the various test procedures are summarized in table 16.4."
4005,1,['test'], Wilcoxon RankSum Test,seg_457,table 16.4: rank-sum test
4006,1,"['critical values', 'levels', 'random samples', 'table', 'null hypothesis', 'samples', 'random', 'level', 'significance', 'test', 'level of significance', 'critical value', 'hypothesis']", Wilcoxon RankSum Test,seg_457,"table a.17 gives critical values of u1 and u2 for levels of significance equal to 0.001, 0.01, 0.025, and 0.05 for a one-tailed test, and critical values of u for levels of significance equal to 0.002, 0.02, 0.05, and 0.10 for a two-tailed test. if the observed value of u1, u2, or u is less than or equal to the tabled critical value, the null hypothesis is rejected at the level of significance indicated by the table. suppose, for example, that we wish to test the null hypothesis that μ̃1 = μ̃2 against the one-sided alternative that μ̃1 < μ̃2 at the 0.05 level of significance for random samples of sizes n1 = 3 and n2 = 5 that yield the value w1 = 8. it follows that"
4007,1,"['table', 'rejection region', 'statistic', 'test', 'null hypothesis', 'hypothesis']", Wilcoxon RankSum Test,seg_457,"our one-tailed test is based on the statistic u1. using table a.17, we reject the null hypothesis of equal means when u1 ≤ 1. since u1 = 2 does not fall in the rejection region, the null hypothesis cannot be rejected."
4008,0,[], Wilcoxon RankSum Test,seg_457,"example 16.5: the nicotine content of two brands of cigarettes, measured in milligrams, was found to be as follows:"
4009,1,"['median', 'level', 'significance', 'level of significance', 'hypothesis']", Wilcoxon RankSum Test,seg_457,"test the hypothesis, at the 0.05 level of significance, that the median nicotine contents of the two brands are equal against the alternative that they are unequal."
4010,1,"['critical region', 'table']", Wilcoxon RankSum Test,seg_457,4. critical region: u ≤ 17 (from table a.17).
4011,1,['observations'], Wilcoxon RankSum Test,seg_457,5. computations: the observations are arranged in ascending order and ranks
4012,0,[], Wilcoxon RankSum Test,seg_457,from 1 to 18 assigned.
4013,1,['data'], Wilcoxon RankSum Test,seg_457,original data ranks original data ranks 0.6 1 4.0 10.5* 1.6 2 4.0 10.5 1.9 3 4.1 12 2.1 4* 4.8 13* 2.2 5 5.4 14.5* 2.5 6 5.4 14.5 3.1 7 6.1 16* 3.3 8* 6.2 17 3.7 9* 6.3 18*
4014,1,"['null hypothesis', 'hypothesis']", Wilcoxon RankSum Test,seg_457,6. decision: do not reject the null hypothesis h0 and conclude that there is
4015,1,['median'], Wilcoxon RankSum Test,seg_457,no significant difference in the median nicotine contents of the two brands of cigarettes.
4016,1,"['sampling', 'distribution', 'normal', 'mean', 'variance', 'sampling distribution', 'normal distribution']", Wilcoxon RankSum Test,seg_457,"when both n1 and n2 exceed 8, the sampling distribution of u1 (or u2) approaches the normal distribution with mean and variance given by"
4017,1,"['statistic', 'table']", Wilcoxon RankSum Test,seg_457,"consequently, when n2 is greater than 20, the maximum value in table a.17, and n1 is at least 9, we can use the statistic"
4018,1,"['standard normal', 'tails', 'distribution', 'normal', 'standard', 'standard normal distribution', 'test', 'critical region', 'normal distribution']", Wilcoxon RankSum Test,seg_457,"for our test, with the critical region falling in either or both tails of the standard normal distribution, depending on the form of h1."
4019,1,"['populations', 'normal', 'test']", Wilcoxon RankSum Test,seg_457,"the use of the wilcoxon rank-sum test is not restricted to nonnormal populations. it can be used in place of the two-sample t-test when the populations are normal, although the power will be smaller. the wilcoxon rank-sum test is always superior to the t-test for decidedly nonnormal populations."
4020,1,"['normality', 'analysis of variance', 'variance', 'population']", KruskalWallis Test,seg_459,"in chapters 13, 14, and 15, the technique of analysis of variance was prominent as an analytical technique for testing equality of k ≥ 2 population means. again, however, the reader should recall that normality must be assumed in order for the f-test to be theoretically correct. in this section, we investigate a nonparametric alternative to analysis of variance."
4021,1,"['independent', 'case', 'samples', 'normal', 'analysis of variance', 'populations', 'test', 'variance', 'null hypothesis', 'hypothesis']", KruskalWallis Test,seg_459,"the kruskal-wallis test, also called the kruskal-wallis h test, is a generalization of the rank-sum test to the case of k > 2 samples. it is used to test the null hypothesis h0 that k independent samples are from identical populations. introduced in 1952 by w. h. kruskal and w. a. wallis, the test is a nonparametric procedure for testing the equality of means in the one-factor analysis of variance when the experimenter wishes to avoid the assumption that the samples were selected from normal populations."
4022,1,"['sample', 'observations', 'case', 'observation', 'variable', 'random variable', 'samples', 'statistic', 'mean', 'random', 'number of observations']", KruskalWallis Test,seg_459,"let ni (i = 1, 2, . . . , k) be the number of observations in the ith sample. first, we combine all k samples and arrange the n = n1 + n2 + · · · + nk observations in ascending order, substituting the appropriate rank from 1, 2, . . . , n for each observation. in the case of ties (identical observations), we follow the usual procedure of replacing the observations by the mean of the ranks that the observations would have if they were distinguishable. the sum of the ranks corresponding to the ni observations in the ith sample is denoted by the random variable ri. now let us consider the statistic"
4023,1,"['sample', 'degrees of freedom', 'independent', 'observations', 'distribution', 'samples', 'populations']", KruskalWallis Test,seg_459,"which is approximated very well by a chi-squared distribution with k−1 degrees of freedom when h0 is true, provided each sample consists of at least 5 observations. the fact that h, the assumed value of h, is large when the independent samples come from populations that are not identical allows us to establish the following decision criterion for testing h0:"
4024,1,"['independent', 'samples', 'populations', 'test', 'null hypothesis', 'hypothesis']", KruskalWallis Test,seg_459,"kruskal-wallis to test the null hypothesis h0 that k independent samples are from identical test populations, compute"
4025,1,"['degrees of freedom', 'significance', 'critical region']", KruskalWallis Test,seg_459,"where ri is the assumed value of ri, for i = 1, 2, . . . , k. if h falls in the critical region h > χα2 with v = k − 1 degrees of freedom, reject h0 at the α-level of significance; otherwise, fail to reject h0."
4026,1,"['rate', 'experiment', 'table', 'rates', 'data', 'level', 'significance', 'test', 'significance level', 'hypothesis']", KruskalWallis Test,seg_459,"example 16.6: in an experiment to determine which of three different missile systems is preferable, the propellant burning rate is measured. the data, after coding, are given in table 16.5. use the kruskal-wallis test and a significance level of α = 0.05 to test the hypothesis that the propellant burning rates are the same for the three missile systems."
4027,1,['rates'], KruskalWallis Test,seg_459,table 16.5: propellant burning rates
4028,0,[], KruskalWallis Test,seg_459,2. h1: the three means are not all equal.
4029,1,"['degrees of freedom', 'critical region']", KruskalWallis Test,seg_459,"4. critical region: h > χ20.05 = 5.991, for v = 2 degrees of freedom."
4030,1,"['observations', 'table']", KruskalWallis Test,seg_459,"5. computations: in table 16.6, we convert the 19 observations to ranks and"
4031,0,[], KruskalWallis Test,seg_459,sum the ranks for each missile system.
4032,1,['rates'], KruskalWallis Test,seg_459,table 16.6: ranks for propellant burning rates
4033,1,"['test statistic', 'statistic', 'test']", KruskalWallis Test,seg_459,"now, substituting n1 = 5, n2 = 6, n3 = 8 and r1 = 61.0, r2 = 63.5, r3 = 65.5, our test statistic h assumes the value"
4034,1,['critical region'], KruskalWallis Test,seg_459,"6. decision: since h = 1.66 does not fall in the critical region h > 5.991, we"
4035,1,"['rates', 'hypothesis']", KruskalWallis Test,seg_459,have insufficient evidence to reject the hypothesis that the propellant burning rates are the same for the three missile systems.
4036,1,"['sample', 'observations', 'randomization', 'data', 'statistical', 'runs test', 'random', 'test', 'null hypothesis', 'hypothesis']", Runs Test,seg_463,"in applying the many statistical concepts discussed throughout this book, it was always assumed that the sample data had been collected by some randomization procedure. the runs test, based on the order in which the sample observations are obtained, is a useful technique for testing the null hypothesis h0 that the observations have indeed been drawn at random."
4037,1,"['sample', 'experiment', 'runs test', 'test', 'outcomes']", Runs Test,seg_463,"to illustrate the runs test, let us suppose that 12 people are polled to find out if they use a certain product. we would seriously question the assumed randomness of the sample if all 12 people were of the same sex. we shall designate a male and a female by the symbols m and f , respectively, and record the outcomes according to their sex in the order in which they occur. a typical sequence for the experiment might be"
4038,0,[], Runs Test,seg_463,where we have grouped subsequences of identical symbols. such groupings are called runs.
4039,1,['data'], Runs Test,seg_463,definition 16.1: a run is a subsequence of one or more identical symbols representing a common property of the data.
4040,1,"['sample size', 'sample', 'mutually exclusive', 'data', 'tails', 'runs test', 'measurements', 'associated', 'categories', 'test']", Runs Test,seg_463,"regardless of whether the sample measurements represent qualitative or quantitative data, the runs test divides the data into two mutually exclusive categories: male or female; defective or nondefective; heads or tails; above or below the median; and so forth. consequently, a sequence will always be limited to two distinct symbols. let n1 be the number of symbols associated with the category that occurs the least and n2 be the number of symbols that belong to the other category. then the sample size n = n1 + n2."
4041,1,"['sample', 'hypothesis', 'random']", Runs Test,seg_463,"for the n = 12 symbols in our poll, we have five runs, with the first containing two m’s, the second containing three f’s, and so on. if the number of runs is larger or smaller than what we would expect by chance, the hypothesis that the sample was drawn at random should be rejected. certainly, a sample resulting in only two runs,"
4042,1,"['sample', 'process', 'random']", Runs Test,seg_463,"or the reverse, is most unlikely to occur from a random selection process. such a result indicates that the first 7 people interviewed were all males, followed by 5 females. likewise, if the sample resulted in the maximum number of 12 runs, as in the alternating sequence"
4043,0,[], Runs Test,seg_463,we would again be suspicious of the order in which the individuals were selected for the poll.
4044,1,"['experiment', 'table', 'random variable', 'variable', 'runs test', 'random', 'test']", Runs Test,seg_463,"the runs test for randomness is based on the random variable v , the total number of runs that occur in the complete sequence of the experiment. in table a.18, values of p (v ≤ v∗ when h0 is true) are given for v∗ = 2, 3, . . . , 20 runs and"
4045,1,['tests'], Runs Test,seg_463,values of n1 and n2 less than or equal to 10. the p-values for both one-tailed and two-tailed tests can be obtained using these tabled values.
4046,1,"['table', 'test']", Runs Test,seg_463,"for the poll taken previously, we exhibit a total of 5 f’s and 7 m’s. hence, with n1 = 5, n2 = 7, and v = 5, we note from table a.18 that the p-value for a two-tailed test is"
4047,1,"['sample', 'level', 'significance', 'level of significance', 'hypothesis']", Runs Test,seg_463,"that is, the value v = 5 is reasonable at the 0.05 level of significance when h0 is true, and therefore we have insufficient evidence to reject the hypothesis of randomness in our sample."
4048,1,['test'], Runs Test,seg_463,"when the number of runs is large (for example, if v = 11 while n1 = 5 and n2 = 7), the p-value for a two-tailed test is"
4049,1,"['sample', 'hypothesis', 'random']", Runs Test,seg_463,which leads us to reject the hypothesis that the sample values occurred at random.
4050,1,"['measurement', 'quantitative', 'median', 'runs test', 'measurements', 'test']", Runs Test,seg_463,"the runs test can also be used to detect departures from randomness of a sequence of quantitative measurements over time, caused by trends or periodicities. replacing each measurement, in the order in which it was collected, by a plus symbol if it falls above the median or by a minus symbol if it falls below the median and omitting all measurements that are exactly equal to the median, we generate a sequence of plus and minus symbols that is tested for randomness as illustrated in the following example."
4051,1,"['level', 'significance', 'level of significance']", Runs Test,seg_463,"example 16.7: a machine dispenses acrylic paint thinner into containers. would you say that the amount of paint thinner being dispensed by this machine varies randomly if the contents of the next 15 containers are measured and found to be 3.6, 3.9, 4.1, 3.6, 3.8, 3.7, 3.4, 4.0, 3.8, 4.1, 3.9, 4.0, 3.8, 4.2, and 4.1 liters? use a 0.1 level of significance."
4052,1,['random'], Runs Test,seg_463,solution : 1. h0: sequence is random.
4053,1,['random'], Runs Test,seg_463,2. h1: sequence is not random.
4054,1,"['test statistic', 'statistic', 'test']", Runs Test,seg_463,"4. test statistic: v , the total number of runs."
4055,1,['sample'], Runs Test,seg_463,"5. computations: for the given sample, we find x̃ = 3.9. replacing each mea-"
4056,1,['measurements'], Runs Test,seg_463,"surement by the symbol “+” if it falls above 3.9 or by the symbol “−” if it falls below 3.9 and omitting the two measurements that equal 3.9, we obtain the sequence"
4057,1,['table'], Runs Test,seg_463,"for which n1 = 6, n2 = 7, and v = 8. therefore, from table a.18, the computed p-value is"
4058,1,"['measurements', 'hypothesis']", Runs Test,seg_463,6. decision: do not reject the hypothesis that the sequence of measurements
4059,0,[], Runs Test,seg_463,varies randomly.
4060,1,"['alternative hypothesis', 'random samples', 'symmetric', 'observations', 'populations', 'observation', 'samples', 'runs test', 'hypothesis', 'random', 'population', 'test', 'distributions']", Runs Test,seg_463,"the runs test, although less powerful, can also be used as an alternative to the wilcoxon two-sample test to test the claim that two random samples come from populations having the same distributions and therefore equal means. if the populations are symmetric, rejection of the claim of equal distributions is equivalent to accepting the alternative hypothesis that the means are not equal. in performing the test, we first combine the observations from both samples and arrange them in ascending order. now assign the letter a to each observation taken from one of the populations and the letter b to each observation from the other population, thereby generating a sequence consisting of the symbols a and b. if observations from one population are tied with observations from the other population, the sequence of a and b symbols generated will not be unique and consequently the number of runs is unlikely to be unique. procedures for breaking ties usually result in additional tedious computations, and for this reason we might prefer to apply the wilcoxon rank-sum test whenever these situations occur."
4061,0,[], Runs Test,seg_463,"to illustrate the use of runs in testing for equal means, consider the survival times of the leukemia patients of exercise 16.16 on page 670, for which we have"
4062,1,"['sample', 'symmetric', 'observations', 'populations', 'case', 'samples', 'population']", Runs Test,seg_463,"resulting in v = 6 runs. if the two symmetric populations have equal means, the observations from the two samples will be intermingled, resulting in many runs. however, if the population means are significantly different, we would expect most of the observations for one of the two samples to be smaller than those for the other sample. in the extreme case where the populations do not overlap, we would obtain a sequence of the form"
4063,1,"['case', 'population', 'significance', 'hypothesis']", Runs Test,seg_463,"and in either case there would be only two runs. consequently, the hypothesis of equal population means will be rejected at the α-level of significance only when v is small enough so that"
4064,1,['test'], Runs Test,seg_463,implying a one-tailed test.
4065,1,"['data', 'table']", Runs Test,seg_463,"returning to the data of exercise 16.16 on page 670, for which n1 = 4, n2 = 5, and v = 6, we find from table a.18 that"
4066,1,"['null hypothesis', 'hypothesis']", Runs Test,seg_463,"and therefore fail to reject the null hypothesis of equal means. hence, we conclude that the new serum does not prolong life by arresting leukemia."
4067,1,"['sampling', 'distribution', 'normal', 'mean', 'variance', 'sampling distribution', 'normal distribution']", Runs Test,seg_463,"when n1 and n2 increase in size, the sampling distribution of v approaches the normal distribution with mean and variance given by"
4068,1,['statistic'], Runs Test,seg_463,"consequently, when n1 and n2 are both greater than 10, we can use the statistic"
4069,1,"['test', 'runs test', 'critical region']", Runs Test,seg_463,to establish the critical region for the runs test.
4070,1,"['observations', 'measurements', 'sample', 'intervals', 'confidence', 'sample size', 'distribution', 'tolerance limits', 'tolerance intervals', 'independent', 'method', 'normality', 'normal', 'normal distribution']", Tolerance Limits,seg_465,"tolerance limits for a normal distribution of measurements were discussed in chapter 9. in this section, we consider a method for constructing tolerance intervals that is independent of the shape of the underlying distribution. as we might suspect, for a reasonable degree of confidence they will be substantially longer than those constructed assuming normality, and the sample size required is generally very large. nonparametric tolerance limits are stated in terms of the smallest and largest observations in our sample."
4071,1,"['sample', 'observations', 'distribution', 'tolerance limits', 'measurements', 'confidence']", Tolerance Limits,seg_465,"two-sided for any distribution of measurements, two-sided tolerance limits are indicated by tolerance limits the smallest and largest observations in a sample of size n, where n is determined so that one can assert with 100(1−γ)% confidence that at least the proportion 1− α of the distribution is included between the sample extremes."
4072,1,"['sample', 'random', 'confident', 'distribution', 'measurements', 'random sample']", Tolerance Limits,seg_465,"table a.19 gives required sample sizes for selected values of γ and 1 − α. for example, when γ = 0.01 and 1 − α = 0.95, we must choose a random sample of size n = 130 in order to be 99% confident that at least 95% of the distribution of measurements is included between the sample extremes."
4073,1,"['sample size', 'sample', 'observation', 'tolerance limits', 'measurements', 'population', 'processes']", Tolerance Limits,seg_465,"instead of determining the sample size n such that a specified proportion of measurements is contained between the sample extremes, it is desirable in many industrial processes to determine the sample size such that a fixed proportion of the population falls below the largest (or above the smallest) observation in the sample. such limits are called one-sided tolerance limits."
4074,1,"['sample', 'tolerance limit', 'observation', 'distribution', 'tolerance limits', 'measurements', 'confidence', 'limit']", Tolerance Limits,seg_465,"one-sided for any distribution of measurements, a one-sided tolerance limit is determined tolerance limits by the smallest (largest) observation in a sample of size n, where n is determined so that one can assert with 100(1−γ)% confidence that at least the proportion 1 − α of the distribution will exceed the smallest (be less than the largest) observation in the sample."
4075,1,"['sample', 'confident', 'observation', 'distribution', 'measurements']", Tolerance Limits,seg_465,"table a.20 shows required sample sizes corresponding to selected values of γ and 1−α. hence, when γ = 0.05 and 1−α = 0.70, we must choose a sample of size n = 9 in order to be 95% confident that 70% of our distribution of measurements will exceed the smallest observation in the sample."
4076,1,"['observations', 'correlation', 'set', 'measurements', 'sample correlation coefficient', 'numerical', 'sample', 'linear', 'coefficient', 'correlation coefficient', 'continuous', 'rank correlation coefficient', 'variables', 'rank correlation']", Rank Correlation Coefficient,seg_467,"in chapter 11, we used the sample correlation coefficient r to measure the population correlation coefficient ρ, the linear relationship between two continuous variables x and y . if ranks 1, 2, . . . , n are assigned to the x observations in order of magnitude and similarly to the y observations, and if these ranks are then substituted for the actual numerical values in the formula for the correlation coefficient in chapter 11, we obtain the nonparametric counterpart of the conventional correlation coefficient. a correlation coefficient calculated in this manner is known as the spearman rank correlation coefficient and is denoted by rs. when there are no ties among either set of measurements, the formula for rs reduces to a much simpler expression involving the differences di between the ranks assigned to the n pairs of x’s and y’s, which we now state."
4077,1,"['association', 'variables', 'correlation', 'rank correlation', 'correlation coefficient', 'coefficient', 'rank correlation coefficient']", Rank Correlation Coefficient,seg_467,rank correlation a nonparametric measure of association between two variables x and y is given coefficient by the rank correlation coefficient
4078,1,['data'], Rank Correlation Coefficient,seg_467,where di is the difference between the ranks assigned to xi and yi and n is the number of pairs of data.
4079,1,"['observations', 'test']", Rank Correlation Coefficient,seg_467,"in practice, the preceding formula is also used when there are ties among either the x or y observations. the ranks for tied observations are assigned as in the signed-rank test by averaging the ranks that would have been assigned if the observations were distinguishable."
4080,1,"['association', 'variables', 'range', 'measurements', 'numerical']", Rank Correlation Coefficient,seg_467,"the value of rs will usually be close to the value obtained by finding r based on numerical measurements and is interpreted in much the same way. as before, the value of rs will range from −1 to +1. a value of +1 or −1 indicates perfect association between x and y , the plus sign occurring for identical rankings and the minus sign occurring for reverse rankings. when rs is close to zero, we conclude that the variables are uncorrelated."
4081,1,"['table', 'correlation', 'rank correlation', 'correlation coefficient', 'coefficient', 'rank correlation coefficient']", Rank Correlation Coefficient,seg_467,"example 16.8: the figures listed in table 16.7, released by the federal trade commission, show the milligrams of tar and nicotine found in 10 brands of cigarettes. calculate the rank correlation coefficient to measure the degree of relationship between tar and nicotine content in cigarettes."
4082,0,[], Rank Correlation Coefficient,seg_467,table 16.7: tar and nicotine contents
4083,0,[], Rank Correlation Coefficient,seg_467,cigarette brand tar content nicotine content
4084,1,"['table', 'observations', 'set', 'measurements']", Rank Correlation Coefficient,seg_467,"solution : let x and y represent the tar and nicotine contents, respectively. first we assign ranks to each set of measurements, with the rank of 1 assigned to the lowest number in each set, the rank of 2 to the second lowest number in each set, and so forth, until the rank of 10 is assigned to the largest number. table 16.8 shows the individual rankings of the measurements and the differences in ranks for the 10 pairs of observations."
4085,0,[], Rank Correlation Coefficient,seg_467,table 16.8: rankings for tar and nicotine content
4086,0,[], Rank Correlation Coefficient,seg_467,cigarette brand x i y i d i
4087,0,[], Rank Correlation Coefficient,seg_467,"substituting into the formula for rs, we find that"
4088,1,['correlation'], Rank Correlation Coefficient,seg_467,indicating a high positive correlation between the amounts of tar and nicotine found in cigarettes.
4089,1,"['case', 'correlation', 'measurements', 'numerical', 'linear', 'data', 'coefficient', 'distributions', 'correlation coefficient', 'rank correlation coefficient', 'consistency', 'normality', 'rank correlation']", Rank Correlation Coefficient,seg_467,"some advantages to using rs rather than r do exist. for instance, we no longer assume the underlying relationship between x and y to be linear and therefore, when the data possess a distinct curvilinear relationship, the rank correlation coefficient will likely be more reliable than the conventional measure. a second advantage to using the rank correlation coefficient is the fact that no assumptions of normality are made concerning the distributions of x and y . perhaps the greatest advantage occurs when we are unable to make meaningful numerical measurements but nevertheless can establish rankings. such is the case, for example, when different judges rank a group of individuals according to some attribute. the rank correlation coefficient can be used in this situation as a measure of the consistency of the two judges."
4090,1,"['degrees of freedom', 'observations', 'correlation', 'critical values', 'symmetric', 'coefficient', 'alternative hypothesis', 'distribution', 'correlation coefficient', 'rank correlation coefficient', 'test', 'sampling distribution', 'hypothesis', 'table', 'sampling', 'rank correlation', 'tail', 'tails', 'critical region']", Rank Correlation Coefficient,seg_467,"to test the hypothesis that ρ = 0 by using a rank correlation coefficient, one needs to consider the sampling distribution of the rs-values under the assumption of no correlation. critical values for α = 0.05, 0.025, 0.01, and 0.005 have been calculated and appear in table a.21. the setup of this table is similar to that of the table of critical values for the t-distribution except for the left column, which now gives the number of pairs of observations rather than the degrees of freedom. since the distribution of the rs-values is symmetric about zero when ρ = 0, the rs-value that leaves an area of α to the left is equal to the negative of the rs-value that leaves an area of α to the right. for a two-sided alternative hypothesis, the critical region of size α falls equally in the two tails of the distribution. for a test in which the alternative hypothesis is negative, the critical region is entirely in the left tail of the distribution, and when the alternative is positive, the critical region is placed entirely in the right tail."
4091,1,"['correlation', 'level', 'significance', 'test', 'level of significance', 'hypothesis']", Rank Correlation Coefficient,seg_467,example 16.9: refer to example 16.8 and test the hypothesis that the correlation between the amounts of tar and nicotine found in cigarettes is zero against the alternative that it is greater than zero. use a 0.01 level of significance.
4092,1,"['critical region', 'table']", Rank Correlation Coefficient,seg_467,4. critical region: rs > 0.745 from table a.21.
4093,1,['correlation'], Rank Correlation Coefficient,seg_467,6. decision: reject h0 and conclude that there is a significant correlation be-
4094,0,[], Rank Correlation Coefficient,seg_467,tween the amounts of tar and nicotine found in cigarettes.
4095,1,"['deviation', 'table', 'correlation', 'distribution', 'normal', 'mean', 'standard', 'standard deviation', 'test', 'normal distribution']", Rank Correlation Coefficient,seg_467,"under the assumption of no correlation, it can be shown that the distribution of the rs-values approaches a normal distribution with a mean of 0 and a standard deviation of 1/√n− 1 as n increases. consequently, when n exceeds the values given in table a.21, one can test for a significant correlation by computing"
4096,1,"['critical values', 'table', 'standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Rank Correlation Coefficient,seg_467,and comparing with critical values of the standard normal distribution shown in table a.3.
4097,1,"['chart', 'variability', 'states', 'successful', 'processes', 'sampling', 'control chart', 'statistical', 'control', 'quality control', 'charts']", Introduction,seg_475,"the notion of using sampling and statistical analysis techniques in a production setting had its beginning in the 1920s. the objective of this highly successful concept is the systematic reduction of variability and the accompanying isolation of sources of difficulties during production. in 1924, walter a. shewhart of the bell telephone laboratories developed the concept of a control chart. however, it was not until world war ii that the use of control charts became widespread. this was due to the importance of maintaining quality in production processes during that period. in the 1950s and 1960s, the development of quality control and the general area of quality assurance grew rapidly, particularly with the emergence of the space program in the united states. there has been widespread and successful use of quality control in japan thanks to the efforts of w. edwards deming, who served as a consultant in japan following world war ii. quality control has been, and is, an important ingredient in the development of japan’s industry and economy."
4098,1,"['sampling', 'standard', 'efficiency', 'control', 'statistical', 'quality control']", Introduction,seg_475,"quality control is receiving increasing attention as a management tool in which important characteristics of a product are observed, assessed, and compared with some type of standard. the various procedures in quality control involve considerable use of sampling procedures and statistical principles that have been presented in previous chapters. the primary users of quality control are, of course, industrial corporations. it has become clear that an effective quality control program enhances the quality of the product being produced and increases profits. this is particularly true today since products are produced in such high volume. before the movement toward quality control methods, quality often suffered because of lack of efficiency, which, of course, increases cost."
4099,1,"['chart', 'variability', 'control chart', 'variation', 'level', 'control', 'process']", Introduction,seg_475,"the purpose of a control chart is to determine if the performance of a process is maintaining an acceptable level of quality. it is expected, of course, that any process will experience natural variability, that is, variability due to essentially unimportant and uncontrollable sources of variation. on the other hand, a process may experience more serious types of variability in key performance measures."
4100,1,"['process', 'variability', 'successful', 'errors', 'adjusted', 'variation', 'control', 'statistical']", Introduction,seg_475,"these sources of variability may arise from one of several types of nonrandom “assignable causes,” such as operator errors or improperly adjusted dials on a machine. a process operating in this state is called out of control. a process experiencing only chance variation is said to be in statistical control. of course, a successful production process may operate in an in-control state for a long period. it is presumed that during this period, the process is producing an acceptable product. however, there may be either a gradual or a sudden “shift” that requires detection."
4101,1,"['control chart', 'chart', 'process', 'control']", Introduction,seg_475,"a control chart is intended as a device to detect the nonrandom or out-of- control state of a process. typically, the control chart takes the form indicated in figure 17.1. it is important that the shift be detected quickly so that the problem can be corrected. obviously, if detection is slow, many defective or nonconforming items are produced, resulting in considerable waste and increased cost."
4102,1,"['control chart', 'chart', 'control']", Introduction,seg_475,figure 17.1: typical control chart.
4103,1,"['sample', 'process', 'results', 'samples', 'random', 'control', 'average', 'limit']", Introduction,seg_475,"some type of quality characteristic must be under consideration, and units of the process must be sampled over time. say, for example, the characteristic is the circumference of an engine bearing. the centerline represents the average value of the characteristic when the process is in control. the points depicted in the figure represent results of, say, sample averages of this characteristic, with the samples taken over time. the upper control limit and the lower control limit are chosen in such a way that one would expect all sample points to be covered by these boundaries if the process is in control. as a result, the general complexion of the plotted points over time determines whether or not the process is concluded to be in control. the “in control” evidence is produced by a random pattern of points, with all plotted values being inside the control limits. when a point falls outside the control limits, this is taken to be evidence of a process that is out of control, and a search for the assignable cause is suggested. in addition, a nonrandom pattern of points may be considered suspicious and certainly an indication that an investigation for the appropriate corrective action is needed."
4104,1,"['type ii error', 'error', 'process', 'null hypothesis', 'hypothesis testing', 'probability', 'type ii', 'control', 'type i error', 'critical region', 'charts', 'hypothesis']", Nature of the Control Limits,seg_477,"the fundamental ideas on which control charts are based are similar in structure to those of hypothesis testing. control limits are established to control the probability of making the error of concluding that the process is out of control when in fact it is not. this corresponds to the probability of making a type i error if we were testing the null hypothesis that the process is in control. on the other hand, we must be attentive to an error of the second kind, namely, not finding the process out of control when in fact it is (type ii error). thus, the choice of control limits is similar to the choice of a critical region."
4105,1,"['case', 'set', 'process', 'sample', 'results', 'data', 'hypothesis testing', 'samples', 'quality control', 'sample size', 'control', 'hypothesis', 'variability', 'sensitivity']", Nature of the Control Limits,seg_477,"as in the case of hypothesis testing, the sample size at each point is important. the choice of sample size depends to a large extent on the sensitivity or power of detection of the out-of-control state. in this application, the notion of power is very similar to that of the hypothesis-testing situation. clearly, the larger the sample at each time period, the quicker the detection of an out-of-control process. in a sense, the control limits actually define what the user considers as being in control. in other words, the latitude given by the control limits must depend in some sense on the process variability. as a result, the computation of the control limits will naturally depend on data taken from the process results. thus, any quality control application must have its beginning with computation from a preliminary sample or set of samples which will establish both the centerline and the quality control limits."
4106,1,"['deviation', 'chart', 'random', 'estimation', 'random variation', 'data', 'variation', 'sampling', 'control chart', 'mean', 'standard', 'standard deviation', 'control', 'process']", Purposes of the Control Chart,seg_479,"one obvious purpose of the control chart is mere surveillance of the process, that is, to determine if changes need to be made. in addition, the constant systematic gathering of data often allows management to assess process capability. clearly, if a single performance characteristic is important, continual sampling and estimation of the mean and standard deviation of that performance characteristic provide an update on what the process can do in terms of mean performance and random variation. this is valuable even if the process stays in control for long periods. the systematic and formal structure of the control chart can often prevent overreaction to changes that represent only random fluctuations. obviously, in many situations, changes brought about by overreaction can create serious problems that are difficult to solve."
4107,1,"['chart', 'measurement', 'variables', 'case', 'categories', 'control', 'charts']", Purposes of the Control Chart,seg_479,"quality characteristics of control charts fall generally into two categories, variables and attributes. as a result, types of control charts often take the same classifications. in the case of the variables type of chart, the characteristic is usually a measurement on a continuum, such as diameter or weight. for the attribute chart, the characteristic reflects whether the individual product conforms (defective or not). applications for these two distinct situations are obvious."
4108,1,"['chart', 'precision', 'process', 'variability', 'variables', 'results', 'case', 'control', 'average', 'quality control']", Purposes of the Control Chart,seg_479,"in the case of the variables chart, control must be exerted on both central tendency and variability. a quality control analyst must be concerned about whether there has been a shift in values of the performance characteristic on average. in addition, there will always be a concern about whether some change in process conditions results in a decrease in precision (i.e., an increase in variability). separate"
4109,1,"['range', 'case', 'charts', 'sample', 'control chart', 'samples', 'mean', 'standard', 'standard deviation', 'chart', 'control', 'deviation', 'variability', 'variables', 'sample standard deviation', 'sampling']", Purposes of the Control Chart,seg_479,"control charts are essential for dealing with these two concepts. central tendency is controlled by the x̄-chart, where means of relatively small samples are plotted on a control chart. variability around the mean is controlled by the range in the sample, or the sample standard deviation. in the case of attribute sampling, the proportion defective from a sample is often the quantity plotted on the chart. in the following section, we discuss the development of control charts for the variables type of performance characteristic."
4110,1,"['observations', 'random', 'process', 'charts', 'sample', 'sample mean', 'random variable', 'mean', 'standard', 'standard deviation', 'quality control', 'chart', 'control', 'deviation', 'independent', 'variables', 'variable', 'average']", Control Charts for Variables,seg_481,"providing an example is a relatively easy way to explain the rudiments of the x̄- chart for variables. suppose that quality control charts are to be used on a process for manufacturing a certain engine part. suppose the process mean is μ = 50 mm and the standard deviation is σ = 0.01 mm. suppose that groups of 5 are sampled every hour and the values of the sample mean x̄ are recorded and plotted on a chart like the one in figure 17.2. the limits for the x̄-charts are based on the standard deviation of the random variable x̄. we know from material in chapter 8 that for the average of independent observations in a sample of size n,"
4111,1,"['deviation', 'condition', 'observation', 'central limit theorem', 'probability', 'standard', 'standard deviation', 'control', 'process', 'limit']", Control Charts for Variables,seg_481,"where σ is the standard deviation of an individual observation. the control limits are designed to result in a small probability that a given value of x̄ is outside the limits given that, indeed, the process is in control (i.e., μ = 50). if we invoke the central limit theorem, we have that under the condition that the process is in control,"
4112,1,"['control', 'process']", Control Charts for Variables,seg_481,"as a result, 100(1− α)% of the x̄-values fall inside the limits when the process is in control if we use the limits"
4113,1,"['control', 'limit']", Control Charts for Variables,seg_481,"here lcl and ucl stand for lower control limit and upper control limit, respectively. often the x̄-charts are based on limits that are referred to as “three-sigma” limits, referring, of course, to zα/2 = 3 and limits that become"
4114,0,[], Control Charts for Variables,seg_481,"in our illustration, the upper and lower limits become"
4115,1,"['sample', 'process', 'hypothesis testing', 'probability', 'control', 'hypothesis']", Control Charts for Variables,seg_481,"thus, if we view the structure of the 3σ limits from the point of view of hypothesis testing, for a given sample point, the probability is 0.0026 that the x̄-value falls outside control limits, given that the process is in control. this is the probability"
4116,1,['control'], Control Charts for Variables,seg_481,figure 17.2: the 3σ control limits for the engine part example.
4117,1,"['control', 'process', 'table']", Control Charts for Variables,seg_481,of the analyst erroneously determining that the process is out of control (see table a.3).
4118,1,"['estimates', 'case', 'statistic', 'probability', 'process', 'charts', 'approximation', 'parameter', 'standard', 'standard deviation', 'limit', 'central limit theorem', 'control', 'deviation', 'variables', 'sampling', 'normality']", Control Charts for Variables,seg_481,"the example above not only illustrates the x̄-chart for variables, but also should provide the reader with insight into the nature of control charts in general. the centerline generally reflects the ideal value of an important parameter. control limits are established from knowledge of the sampling properties of the statistic that estimates the parameter in question. they very often involve a multiple of the standard deviation of the statistic. it has become general practice to use 3σ limits. in the case of the x̄-chart provided here, the central limit theorem provides the user with a good approximation of the probability of falsely ruling that the process is out of control. in general, though, the user may not be able to rely on the normality of the statistic on the centerline. as a result, the exact probability of “type i error” may not be known. despite this, it has become fairly standard to use the kσ limits. while use of the 3σ limits is widespread, at times the user may wish to deviate from this approach. a smaller multiple of σ may be appropriate when it is important to quickly detect an out-of-control situation. because of economic considerations, it may prove costly to allow a process to continue to run out of control for even short periods, while the cost of the search and correction of assignable causes may be relatively small. clearly, in this case, control limits that are tighter than 3σ limits are appropriate."
4119,1,"['sample', 'information', 'sampling', 'success', 'control', 'quality control']", Control Charts for Variables,seg_481,"the sample values to be used in a quality control effort are divided into subgroups, with a sample representing a subgroup. as we indicated earlier, time order of production is certainly a natural basis for selection of the subgroups. we may view the quality control effort very simply as (1) sampling, (2) detection of an out-of-control state, and (3) a search for assignable causes that may be occurring over time. the selection of the basis for these sample groups would appear to be straightforward, but the choice of these subgroups of sampling information can have an important effect on the success of the quality control program. these subgroups are often called rational subgroups. generally, if the analyst is interested in detecting a"
4120,1,"['sample', 'variability', 'observations', 'random sample', 'case', 'location', 'samples', 'statistic', 'random', 'control', 'process', 'charts']", Control Charts for Variables,seg_481,"shift in location, the subgroups should be chosen so that within-subgroup variability is small and assignable causes, if they are present, have the greatest chance of being detected. thus, we want to choose the subgroups in such a way as to maximize the between-subgroup variability. choosing units in a subgroup that are produced close together in time, for example, is a reasonable approach. on the other hand, control charts are often used to control variability, in which case the performance statistic is variability within the sample. thus, it is more important to choose the rational subgroups to maximize the within-sample variability. in this case, the observations in the subgroups should behave more like a random sample and the variability within samples needs to be a depiction of the variability of the process."
4121,1,"['chart', 'variability', 'estimate', 'data', 'location', 'control chart', 'control', 'charts']", Control Charts for Variables,seg_481,"it is important to note that control charts on variability should be established before the development of charts on center of location (say, x̄-charts). any control chart on center of location will certainly depend on variability. for example, we have seen an illustration of the central tendency chart and it depends on σ. in the sections that follow, an estimate of σ from the data will be discussed."
4122,1,"['deviation', 'central limit theorem', 'mean', 'standard', 'standard deviation', 'control', 'process', 'limit']", Control Charts for Variables,seg_481,"in the foregoing, we have illustrated notions of the x̄-chart that make use of the central limit theorem and employ known values of the process mean and standard deviation. as we indicated earlier, the control limits"
4123,1,"['control', 'process', 'mean']", Control Charts for Variables,seg_481,"are used, and an x̄-value falling outside these limits is viewed as evidence that the mean μ has changed and thus the process may be out of control."
4124,1,"['sample', 'chart', 'estimates', 'range', 'sample means', 'data', 'information', 'control chart', 'samples', 'mean', 'control', 'process']", Control Charts for Variables,seg_481,"in many practical situations, it is unreasonable to assume that we know μ and σ. as a result, estimates must be supplied from data taken when the process is in control. typically, the estimates are determined during a period in which background information or start-up information is gathered. a basis for rational subgroups is chosen, and data are gathered with samples of size n in each subgroup. the sample sizes are usually small, say 4, 5, or 6, and k samples are taken, with k being at least 20. during this period in which it is assumed that the process is in control, the user establishes estimates of μ and σ on which the control chart is based. the important information gathered during this period includes the sample means in the subgroup, the overall mean, and the sample range in each subgroup. in the following paragraphs, we outline how this information is used to develop the control chart."
4125,1,"['sample', 'information', 'variable', 'random variable', 'samples', 'random', 'average']", Control Charts for Variables,seg_481,"a portion of the sample information from these k samples takes the form x̄1, x̄2, . . . , x̄k, where the random variable x̄i is the average of the values in the ith sample. obviously, the overall average is the random variable"
4126,1,"['chart', 'estimator', 'control chart', 'mean', 'control', 'process', 'quality control']", Control Charts for Variables,seg_481,"this is the appropriate estimator of the process mean and, as a result, is the centerline in the x̄ control chart. in quality control applications, it is often convenient"
4127,1,"['sample', 'sample standard deviations', 'standard deviations', 'estimate', 'information', 'deviations', 'samples', 'standard']", Control Charts for Variables,seg_481,to estimate σ from the information related to the ranges in the samples rather than sample standard deviations. let us define
4128,1,"['sample', 'estimate', 'range', 'observations', 'data', 'function', 'average']", Control Charts for Variables,seg_481,"as the range for the data in the ith sample. here xmax,i and xmin,i are the largest and smallest observations, respectively, in the sample. the appropriate estimate of σ is a function of the average range"
4129,1,['estimate'], Control Charts for Variables,seg_481,"an estimate of σ, say σ̂, is obtained by"
4130,1,"['sample size', 'sample', 'table']", Control Charts for Variables,seg_481,where d2 is a constant depending on the sample size. values of d2 are shown in table a.22.
4131,1,"['chart', 'efficient', 'variability', 'estimate', 'range', 'estimates', 'observations', 'normality', 'random variable', 'variable', 'central limit theorem', 'random', 'limit']", Control Charts for Variables,seg_481,"use of the range in producing an estimate of σ has roots in quality-control-type applications, particularly since the range was so easy to compute, compared to other variability estimates, in the era when efficient computation was still an issue. the assumption of normality of the individual observations is implicit in the x̄- chart. of course, the existence of the central limit theorem is certainly helpful in this regard. under the assumption of normality, we make use of a random variable called the relative range, given by"
4132,1,"['sample size', 'sample', 'functions', 'expected value', 'moments']", Control Charts for Variables,seg_481,"it turns out that the moments of w are simple functions of the sample size n (see the reference to montgomery, 2000b, in the bibliography). the expected value of w is often referred to as d2. thus, by taking the expected value of w above, we have"
4133,1,"['efficient', 'range', 'estimator', 'charts', 'sample', 'estimate', 'results', 'samples', 'efficient estimator', 'quality control', 'parameters', 'estimation', 'control', 'method']", Control Charts for Variables,seg_481,"as a result, the rationale for the estimate σ̂ = r̄/d2 is readily understood. it is well known that the range method produces an efficient estimator of σ in relatively small samples. this makes the estimator particularly attractive in quality control applications, since the sample sizes in the subgroups are generally small. using the range method for estimation of σ results in control charts with the following parameters:"
4134,1,"['sample', 'table']", Control Charts for Variables,seg_481,"to simplify the structure, the user of x̄-charts often finds values of a2 tabulated. values of a2 are given for various sample sizes in table a.22."
4135,1,"['range', 'observations', 'plots', 'case', 'location', 'random', 'process', 'sample', 'estimate', 'data', 'random variable', 'mean', 'standard', 'quality control', 'distribution', 'control', 'plot', 'estimated', 'variability', 'normality', 'variable']", Control Charts for Variables,seg_481,"up to this point, all illustrations and details have dealt with the quality control analysts’ attempts at detection of out-of-control conditions produced by a shift in the mean. the control limits are based on the distribution of the random variable x̄ and depend on the assumption of normality of the individual observations. it is important for control to be applied to variability as well as center of location. in fact, many experts believe that control of variability of the performance characteristic is more important and should be established before center of location is considered. process variability can be controlled through the use of plots of the sample range. a plot over time of the sample ranges is called an r-chart. the same general structure can be used as in the case of the x̄-chart, with r̄ being the centerline and the control limits depending on an estimate of the standard deviation of the random variable r. thus, as in the case of the x̄-chart, 3σ limits are established where “3σ” implies 3σr. the quantity σr must be estimated from the data just as σx̄ is estimated."
4136,1,"['deviation', 'estimate', 'range', 'distribution', 'standard', 'standard deviation']", Control Charts for Variables,seg_481,"the estimate of σr, the standard deviation, is also based on the distribution of the relative range"
4137,1,"['sample size', 'sample', 'deviation', 'standard', 'function', 'standard deviation']", Control Charts for Variables,seg_481,"the standard deviation ofw is a known function of the sample size and is generally denoted by d3. as a result,"
4138,1,['estimator'], Control Charts for Variables,seg_481,"we can now replace σ by σ̂ = r̄/d2, and thus the estimator of σr is"
4139,0,[], Control Charts for Variables,seg_481,"thus, the quantities that define the r-chart are"
4140,0,['n'], Control Charts for Variables,seg_481,where the constants d4 and d3 (depending only on n) are
4141,1,['table'], Control Charts for Variables,seg_481,the constants d4 and d3 are tabulated in table a.22.
4142,1,"['process', 'table', 'data', 'samples']", Control Charts for Variables,seg_481,"a process manufacturing missile component parts is being controlled, with the performance characteristic being the tensile strength in pounds per square inch. samples of size 5 each are taken every hour and 25 samples are reported. the data are shown in table 17.1."
4143,1,"['sample', 'data', 'information']", Control Charts for Variables,seg_481,table 17.1: sample information on tensile strength data
4144,1,['observations'], Control Charts for Variables,seg_481,sample number observations x̄i ri
4145,1,['variability'], Control Charts for Variables,seg_481,"as we indicated earlier, it is important initially to establish “in control” conditions on variability. the calculated centerline for the r-chart is"
4146,1,"['control', 'table']", Control Charts for Variables,seg_481,"we find from table a.22 that for n = 5, d3 = 0 and d4 = 2.114. as a result, the control limits for the r-chart are"
4147,1,['control'], Control Charts for Variables,seg_481,"the r-chart is shown in figure 17.3. none of the plotted ranges fall outside the control limits. as a result, there is no indication of an out-of-control situation."
4148,0,[], Control Charts for Variables,seg_481,figure 17.3: r-chart for the tensile strength example.
4149,0,[], Control Charts for Variables,seg_481,the x̄-chart can now be constructed for the tensile strength readings. the centerline is
4150,1,"['table', 'samples', 'control']", Control Charts for Variables,seg_481,"for samples of size 5, we find a2 = 0.577 from table a.22. thus, the control limits are"
4151,1,"['control', 'quality control']", Control Charts for Variables,seg_481,"the x̄-chart is shown in figure 17.4. as the reader can observe, three values fall outside the control limits. as a result, the control limits for x̄ should not be used for line quality control."
4152,1,"['method', 'variability', 'mean', 'control', 'process', 'quality control', 'charts']", Control Charts for Variables,seg_481,"a process may appear to be in control and, in fact, may stay in control for a long period. does this necessarily mean that the process is operating successfully? a process that is operating in control is merely one in which the process mean and variability are stable. apparently, no serious changes have occurred. “in control” implies that the process remains consistent with natural variability. quality control charts may be viewed as a method in which the inherent natural variability governs the width of the control limits. there is no implication, however, to what extent an in-control process satisfies predetermined specifications required of the process. specifications are limits that are established by the consumer. if the current natural"
4153,0,[], Control Charts for Variables,seg_481,figure 17.4: x̄-chart for the tensile strength example.
4154,1,"['frequency', 'process', 'control']", Control Charts for Variables,seg_481,"variability of the process is larger than that dictated by the specifications, the process will not produce items that meet specifications with high frequency, even though the process is stable and in control."
4155,1,"['range', 'observations', 'case', 'statistic', 'probability', 'robustness', 'approximation', 'results', 'symmetric', 'control chart', 'samples', 'cases', 'standard', 'standard deviation', 'type i error', 'quality control', 'error', 'chart', 'distribution', 'control', 'deviation', 'method', 'variables', 'normality', 'normal']", Control Charts for Variables,seg_481,"we have alluded to the normality assumption on the individual observations in a variables control chart. for the x̄-chart, if the individual observations are normal, the statistic x̄ is normal. as a result, the quality control analyst has control over the probability of type i error in this case. if the individual x’s are not normal, x̄ is approximately normal and thus there is approximate control over the probability of type i error for the case in which σ is known. however, the use of the range method for estimating the standard deviation also depends on the normality assumption. studies regarding the robustness of the x̄-chart to departures from normality indicate that for samples of size k ≥ 4 the x̄ chart results in an α-risk close to that advertised (see the work by montgomery, 2000b, and schilling and nelson, 1976, in the bibliography). we indicated earlier that the ±kσr approach to the r-chart is a matter of convenience and tradition. even if the distribution of individual observations is normal, the distribution of r is not normal. in fact, the distribution of r is not even symmetric. the symmetric control limits of ±kσr only give an approximation to the α-risk, and in some cases the approximation is not particularly good."
4156,1,"['sample size', 'sample', 'chart', 'factors', 'design', 'sampling', 'control chart', 'frequency', 'associated', 'control', 'process', 'quality control']", Control Charts for Variables,seg_481,"scientists and engineers dealing in quality control often refer to factors that affect the design of the control chart. components that determine the design of the chart include the sample size taken in each subgroup, the width of the control limits, and the frequency of sampling. all of these factors depend to a large extent on economic and practical considerations. frequency of sampling obviously depends on the cost of sampling and the cost incurred if the process continues out of control for a long period. these same factors affect the width of the “in-control” region. the cost that is associated with investigation and search for assignable causes has an impact"
4157,1,"['design', 'sampling', 'frequency', 'control', 'charts']", Control Charts for Variables,seg_481,"on the width of the region and on frequency of sampling. a considerable amount of attention has been devoted to optimal design of control charts, and extensive details will not be given here. the reader should refer to the work by montgomery (2000b) cited in the bibliography for an excellent historical account of much of this research."
4158,1,"['sample size', 'sample', 'sampling', 'cases', 'frequency']", Control Charts for Variables,seg_481,"choice of sample size and frequency of sampling involves balancing available resources allocated to these two efforts. in many cases, the analyst may need to make changes in the strategy until the proper balance is achieved. the analyst should always be aware that if the cost of producing nonconforming items is great, a high sampling frequency with relatively small sample size is a proper strategy."
4159,1,"['sample size', 'sample', 'statistical inference', 'factors', 'results', 'statistical', 'set', 'control', 'process', 'quality control']", Control Charts for Variables,seg_481,"many factors must be taken into consideration in the choice of a sample size. in the illustrations and discussion, we have emphasized the use of n = 4, 5, or 6. these values are considered relatively small for general problems in statistical inference but perhaps proper sample sizes for quality control. one justification, of course, is that quality control is a continuing process and the results produced by one sample or set of units will be followed by results from many more. thus, the “effective” sample size of the entire quality control effort is many times larger than that used in a subgroup. it is generally considered to be more effective to sample frequently with a small sample size."
4160,1,"['probability', 'charts', 'sample', 'tests of hypotheses', 'results', 'information', 'power of a test', 'mean', 'standard', 'population', 'standard deviation', 'tests', 'quality control', 'sample size', 'condition', 'analysis of variance', 'test', 'variance', 'control', 'hypothesis', 'deviation', 'population mean', 'sampling', 'hypotheses']", Control Charts for Variables,seg_481,"the analyst can make use of the notion of the power of a test to gain some insight into the effectiveness of the sample size chosen. this is particularly important since small sample sizes are usually used in each subgroup. refer to chapters 10 and 13 for a discussion of the power of formal tests on means and the analysis of variance. although formal tests of hypotheses are not actually being conducted in quality control, one can treat the sampling information as if the strategy at each subgroup were to test a hypothesis, either on the population mean μ or on the standard deviation σ. of interest is the probability of detection of an out-of-control condition for a given sample and, perhaps more important, the expected number of runs required for detection. the probability of detection of a specified out-of- control condition corresponds to the power of a test. it is not our intention to show development of the power for all of the types of control charts presented here, but rather to show the development for the x̄-chart and present power results for the r-chart."
4161,1,"['sample size', 'sample', 'mean', 'probability', 'control']", Control Charts for Variables,seg_481,"consider the x̄-chart for σ known. suppose that the in-control state has μ = μ0. a study of the role of the subgroup sample size is tantamount to investigating the β-risk, that is, the probability that an x̄-value remains inside the control limits given that, indeed, a shift in the mean has occurred. suppose that the form the shift takes is"
4162,1,['normality'], Control Charts for Variables,seg_481,"again, making use of the normality of x̄, we have"
4163,1,['case'], Control Charts for Variables,seg_481,"for the case of kσ limits,"
4164,1,"['standard', 'standard normal', 'random variable', 'variable', 'normal', 'standard normal random variable', 'random', 'normal random variable']", Control Charts for Variables,seg_481,"as a result, if we denote by z the standard normal random variable,"
4165,1,"['sample size', 'sample', 'probability']", Control Charts for Variables,seg_481,"notice the role of n, r, and k in the expression for the β-risk. the probability of not detecting a specific shift clearly increases with an increase in k, as expected. β decreases with an increase in r, the magnitude of the shift, and decreases with an increase in the sample size n."
4166,1,"['type ii error', 'sample', 'error', 'results', 'case', 'mean', 'probability', 'type ii']", Control Charts for Variables,seg_481,"it should be emphasized that the expression above results in the β-risk (probability of type ii error) for the case of a single sample. for example, suppose that in the case of a sample of size 4, a shift of σ occurs in the mean. the probability of detecting the shift (power) in the first sample following the shift is (assuming 3σ limits)"
4167,1,['probability'], Control Charts for Variables,seg_481,"on the other hand, the probability of detecting a shift of 2σ is"
4168,1,"['sample', 'functions', 'plot', 'results', 'mean', 'probability', 'plotting', 'control']", Control Charts for Variables,seg_481,"the results above illustrate a fairly modest probability of detecting a shift of magnitude σ and a fairly high probability of detecting a shift of magnitude 2σ. the complete picture of how, say, 3σ control limits perform for the x̄-chart described here is depicted in figure 17.5. rather than plotting the power functions, a plot is given of β against r, where the shift in the mean is of magnitude rσ. of course, the sample sizes of n = 4, 5, 6 result in a small probability of detecting a shift of 1.0σ or even 1.5σ on the first sample after the shift."
4169,1,"['sample', 'independent', 'sampling', 'samples', 'probability', 'average']", Control Charts for Variables,seg_481,"but if sampling is done frequently, the probability may not be as important as the average or expected number of runs required before detection of the shift. quick detection is important and is certainly possible even though the probability of detection on the first sample is not high. it turns out that x̄-charts with these small samples will result in relatively rapid detection. if β is the probability of not detecting a shift on the first sample following the shift, then the probability of detecting the shift on the sth sample after the shift is (assuming independent samples)"
4170,1,"['geometric distribution', 'distribution', 'samples', 'expected value', 'average', 'geometric']", Control Charts for Variables,seg_481,the reader should recognize this as an application of the geometric distribution. the average or expected value of the number of samples required for detection is
4171,1,"['sample', 'samples', 'mean', 'probability']", Control Charts for Variables,seg_481,"thus, the expected number of samples required to detect the shift in the mean is the reciprocal of the power (i.e., the probability of detection on the first sample following the shift)."
4172,1,"['sample', 'error', 'mean', 'probability', 'type ii']", Control Charts for Variables,seg_481,figure 17.5: operating characteristic curves for the x̄-chart with 3σ limits. here β is the type ii probability error on the first sample after a shift in the mean of rσ.
4173,1,"['sample size', 'sample', 'chart', 'control chart', 'samples', 'mean', 'control', 'quality control']", Control Charts for Variables,seg_481,"example 17.1: in a certain quality control effort, it is important for the quality control analyst to quickly detect shifts in the mean of ±σ while using a 3σ control chart with a sample size n = 4. the expected number of samples that are required following the shift for the detection of the out-of-control state can be an aid in the assessment of the quality control procedure."
4174,1,"['samples', 'mean']", Control Charts for Variables,seg_481,"from figure 17.5, for n = 4 and r = 1, it can be seen that β ≈ 0.84. if we allow s to denote the number of samples required to detect the shift, the mean of s is"
4175,1,['average'], Control Charts for Variables,seg_481,"thus, on the average, seven subgroups are required before detection of a shift of ±σ."
4176,1,"['curve', 'deviation', 'oc curve', 'standard', 'function', 'standard deviation', 'control', 'process']", Control Charts for Variables,seg_481,"the oc curve for the r-chart is shown in figure 17.6. since the r-chart is used for control of the process standard deviation, the β-risk is plotted as a function of the in-control standard deviation, σ0, and the standard deviation after the process goes out of control. the latter standard deviation will be denoted σ1. let"
4177,1,['sample'], Control Charts for Variables,seg_481,"for various sample sizes, β is plotted against λ."
4178,1,"['curve', 'operating characteristic curve']", Control Charts for Variables,seg_481,figure 17.6: operating characteristic curve for the r-charts with 3σ limits.
4179,1,"['sample', 'chart', 'sample size', 'efficient', 'variability', 'range', 'statistics', 'statistic', 'efficiency', 'sample variance', 'control', 'variance', 'estimator']", Control Charts for Variables,seg_481,"it is natural for the student of statistics to anticipate use of the sample variance in the x̄-chart and in a chart to control variability. the range is efficient as an estimator for σ, but this efficiency decreases as the sample size gets larger. for n as large as 10, the familiar statistic"
4180,1,"['chart', 'unbiased estimator', 'bias', 'variability', 'control chart', 'mean', 'control', 'estimator', 'unbiased']", Control Charts for Variables,seg_481,"should be used in the control chart for both the mean and the variability. the reader should recall from chapter 9 that s2 is an unbiased estimator for σ2 but that s is not unbiased for σ. it has become customary to correct s for bias in control chart applications. we know, in general, that"
4181,1,"['independent', 'case', 'normally distributed', 'mean', 'variance']", Control Charts for Variables,seg_481,"in the case in which the xi are independent and normally distributed with mean μ and variance σ2,"
4182,1,"['gamma', 'function', 'variance', 'estimator', 'gamma function']", Control Charts for Variables,seg_481,"and γ(·) refers to the gamma function (see chapter 6). for example, for n = 5, c4 = (3/8)√2π. in addition, the variance of the estimator s is"
4183,1,"['samples', 'set', 'control']", Control Charts for Variables,seg_481,"we have established the properties of s that will allow us to write control limits for both x̄ and s. to build a proper structure, we begin by assuming that σ is known. later we discuss estimating σ from a preliminary set of samples."
4184,1,"['chart', 'parameters', 'control chart', 'statistic', 'control']", Control Charts for Variables,seg_481,"if the statistic s is plotted, the obvious control chart parameters are"
4185,1,['control'], Control Charts for Variables,seg_481,"as usual, the control limits are defined more succinctly through use of tabulated constants. let"
4186,0,[], Control Charts for Variables,seg_481,and thus we have
4187,1,"['sample', 'table']", Control Charts for Variables,seg_481,the values of b5 and b6 for various sample sizes are tabulated in table a.22.
4188,1,"['sample standard deviations', 'set', 'estimator', 'sample', 'unbiased estimator', 'estimate', 'samples', 'standard', 'quality control', 'parameters', 'control', 'unbiased', 'standard deviations', 'deviations']", Control Charts for Variables,seg_481,"now, of course, the control limits above serve as a basis for the development of the quality control parameters for the situation that is most often seen in practice, namely, that in which σ is unknown. we must once again assume that a set of base samples or preliminary samples is taken to produce an estimate of σ during what is assumed to be an “in-control” period. sample standard deviations s1, s2, . . . , sm are obtained from samples that are each of size n. an unbiased estimator of the type"
4189,1,"['case', 'estimators', 'sample', 'control chart', 'standard', 'standard deviation', 'chart', 'unbiased estimators', 'control', 'unbiased', 'deviation', 'variability', 'sample standard deviation', 'average']", Control Charts for Variables,seg_481,"is often used for σ. here, of course, s̄, the average value of the sample standard deviation in the preliminary sample, is the logical centerline in the control chart to control variability. the upper and lower control limits are unbiased estimators of the control limits that are appropriate for the case where σ is known. since"
4190,1,"['unbiased estimator', 'statistic', 'estimator', 'unbiased']", Control Charts for Variables,seg_481,the statistic s̄ is an appropriate centerline (as an unbiased estimator of c4σ) and the quantities
4191,1,"['control', 'variability']", Control Charts for Variables,seg_481,"are the appropriate lower and upper 3σ control limits, respectively. as a result, the centerline and limits for the s-chart to control variability are"
4192,1,['table'], Control Charts for Variables,seg_481,the constants b3 and b4 appear in table a.22.
4193,1,"['deviation', 'sample', 'unbiased estimator', 'parameters', 'sample standard deviation', 'standard', 'standard deviation', 'estimator', 'unbiased']", Control Charts for Variables,seg_481,"we can now write the parameters of the corresponding x̄-chart involving the use of the sample standard deviation. let us assume that s and x̄ are available from the base preliminary sample. the centerline remains x̄ and the 3σ limits are merely of the form x̄ ± 3σ̂/√n, where σ̂ is an unbiased estimator. we simply supply s̄/c4 as an estimator for σ, and thus we have"
4194,1,"['sample', 'table']", Control Charts for Variables,seg_481,the constant a3 appears for various sample sizes in table a.22.
4195,1,"['process', 'parameters', 'table', 'information', 'samples', 'control', 'quality control']", Control Charts for Variables,seg_481,example 17.2: containers are produced by a process where the volume of the containers is subject to quality control. twenty-five samples of size 5 each were used to establish the quality control parameters. information from these samples is documented in table 17.2.
4196,1,"['control', 'table']", Control Charts for Variables,seg_481,"from table a.22, b3 = 0, b4 = 2.089, and a3 = 1.427. as a result, the control limits for x̄ are given by"
4197,1,['control'], Control Charts for Variables,seg_481,and the control limits for the s-chart are
4198,1,"['sample', 'data', 'information', 'samples', 'set', 'data set', 'control', 'charts']", Control Charts for Variables,seg_481,"figures 17.7 and 17.8 show the x̄ and s control charts, respectively, for this example. sample information for all 25 samples in the preliminary data set is plotted on the charts. control seems to have been established after the first few samples."
4199,1,"['chart', 'measurement', 'sampling', 'distribution', 'probability distribution', 'control chart', 'cases', 'binomial', 'probability', 'standard', 'continuous', 'binomial distribution', 'control', 'quality control']", Control Charts for Attributes,seg_483,"as we indicated earlier in this chapter, many industrial applications of quality control require that the quality characteristic indicate no more than that the item “conforms.” in other words, there is no continuous measurement that is crucial to the performance of the item. an obvious illustration of this type of sampling, called sampling for attributes, is the performance of a light bulb, which either performs satisfactorily or does not. the item is either defective or not defective. manufactured metal pieces may contain deformities. containers from a production line may leak. in both of these cases, a defective item negates usage by the customer. the standard control chart for this situation is the p-chart, or chart for fraction defective. as we might expect, the probability distribution involved is the binomial distribution. the reader is referred to chapter 5 for background on the binomial distribution."
4200,1,"['sample', 'samples']", Control Charts for Attributes,seg_483,table 17.2: volume of containers for 25 samples in a preliminary sample (in cubic centimeters)
4201,1,['observations'], Control Charts for Attributes,seg_483,sample observations x̄i si
4202,1,"['sample', 'random', 'probability', 'random sample']", Control Charts for Attributes,seg_483,"any manufactured item may have several characteristics that are important and should be examined by an inspector. however, the entire development here focuses on a single characteristic. suppose that for all items the probability of a defective item is p, and that all items are being produced independently. then, in a random sample of n items produced, allowing x to be the number of defective items, we have"
4203,1,"['chart', 'control chart', 'random variable', 'variable', 'binomial random variable', 'mean', 'binomial', 'random', 'control', 'variance']", Control Charts for Attributes,seg_483,"as one might suspect, the mean and variance of the binomial random variable will play an important role in the development of the control chart. the reader should recall that"
4204,1,"['data', 'control']", Control Charts for Attributes,seg_483,figure 17.7: the x̄-chart with control limits esfigure 17.8: the s-chart with control limits estabtablished by the data of example 17.2. lished by the data of example 17.2.
4205,1,"['unbiased estimator', 'estimator', 'unbiased']", Control Charts for Attributes,seg_483,"an unbiased estimator of p is the fraction defective or the proportion defective, p̂, where"
4206,1,['sample'], Control Charts for Attributes,seg_483,number of defectives in the sample of size n p̂ = . n
4207,1,"['chart', 'variables', 'case', 'control chart', 'control', 'charts']", Control Charts for Attributes,seg_483,"as in the case of the variables control charts, the distributional properties of p are important in the development of the control chart. we know that"
4208,1,"['variables', 'control', 'charts']", Control Charts for Attributes,seg_483,"here we apply the same 3σ principles that we use for the variables charts. let us assume initially that p is known. the structure, then, of the control charts involves the use of 3σ limits with"
4209,0,[], Control Charts for Attributes,seg_483,"thus, the limits are"
4210,1,"['sample', 'process', 'control']", Control Charts for Attributes,seg_483,with the process considered in control when the p̂-values from the sample lie inside the control limits.
4211,1,"['observations', 'case', 'set', 'estimator', 'charts', 'sample', 'unbiased estimator', 'control chart', 'samples', 'chart', 'control', 'unbiased', 'estimated', 'variables']", Control Charts for Attributes,seg_483,"generally, of course, the value of p is not known and must be estimated from a base set of samples very much like the case of μ and σ in the variables charts. assume that there are m preliminary samples of size n. for a given sample, each of the n observations is reported as either “defective” or “not defective.” the obvious unbiased estimator for p to use in the control chart is"
4212,1,"['sample', 'control']", Control Charts for Attributes,seg_483,"where p̂i is the proportion defective in the ith sample. as a result, the control limits are"
4213,1,"['chart', 'table', 'data', 'control chart', 'samples', 'control', 'charts']", Control Charts for Attributes,seg_483,example 17.3: consider the data shown in table 17.3 on the number of defective electronic components in samples of size 50. twenty samples were taken in order to establish preliminary control chart values. the control charts determined by this preliminary period will have centerline p̄ = 0.088 and control limits
4214,1,"['data', 'samples', 'control']", Control Charts for Attributes,seg_483,"table 17.3: data for example 17.3 to establish control limits for p-charts, samples of size 50"
4215,1,['sample'], Control Charts for Attributes,seg_483,number of fraction defective sample defective components p̂i
4216,1,"['set', 'control', 'process']", Control Charts for Attributes,seg_483,"obviously, with a computed value that is negative, the lcl will be set to zero. it is apparent from the values of the control limits that the process is in control during this preliminary period."
4217,1,"['sample size', 'sample', 'chart', 'variables']", Control Charts for Attributes,seg_483,the choice of sample size for the p-chart for attributes involves the same general types of considerations as that of the chart for variables. a sample size is required
4218,1,"['sample size', 'sample', 'normal approximation', 'method', 'condition', 'approximation', 'distribution', 'normal', 'binomial', 'probability', 'binomial distribution']", Control Charts for Attributes,seg_483,"that is sufficiently large to have a high probability of detection of an out-of-control condition when, in fact, a specified change in p has occurred. there is no best method for choice of sample size. however, one reasonable approach, suggested by duncan (1986; see the bibliography), is to choose n so that there is probability 0.5 that we detect a shift in p of a particular amount. the resulting solution for n is quite simple. suppose that the normal approximation to the binomial distribution applies. we wish, under the condition that p has shifted to, say, p1 > p0, that"
4219,1,['set'], Control Charts for Attributes,seg_483,"since p (z > 0) = 0.5, we set"
4220,1,['sample'], Control Charts for Attributes,seg_483,"we can now solve for n, the size of each sample:"
4221,1,"['probability', 'control', 'charts']", Control Charts for Attributes,seg_483,"where, of course, δ is the “shift” in the value of p, and p is the probability of a defective on which the control limits are based. however, if the control charts are based on kσ limits, then"
4222,1,"['sample size', 'sample', 'chart', 'control chart', 'control', 'probability', 'process', 'quality control']", Control Charts for Attributes,seg_483,example 17.4: suppose that an attribute quality control chart is being designed with a value of p = 0.01 for the in-control probability of a defective. what is the sample size per subgroup producing a probability of 0.5 that a process shift to p = p1 = 0.05 will be detected? the resulting p-chart will involve 3σ limits.
4223,1,"['sample size', 'sample']", Control Charts for Attributes,seg_483,solution : here we have δ = 0.04. the appropriate sample size is
4224,1,"['case', 'function', 'control', 'quality control']", Control Charts for Attributes,seg_483,"in the preceding development, we assumed that the item under consideration is one that is either defective (i.e., nonfunctional) or not defective. in the latter case, it is functional and thus acceptable to the consumer. in many situations, this “defective or not” approach is too simplistic. units may contain defects or nonconformities but still function quite well for the consumer. indeed, in this case, it may be important to exert control on the number of defects or number of nonconformities. this type of quality control effort finds application when the units are either not simplistic or large. for example, the number of defects may be quite useful as the object of control when the single item or unit is, say, a personal computer. another example is a unit defined by 50 feet of manufactured pipeline, where the number of defective welds is the object of quality control; the number of defects in 50 feet of manufactured carpeting; or the number of “bubbles” in a large manufactured sheet of glass."
4225,1,"['poisson', 'sample', 'chart', 'distribution', 'control chart', 'binomial', 'poisson distribution', 'binomial distribution', 'control', 'average']", Control Charts for Attributes,seg_483,it is clear from what we describe here that the binomial distribution is not appropriate. the total number of nonconformities in a unit or the average number per unit can be used as the measure for the control chart. often it is assumed that the number of nonconformities in a sample of items follows the poisson distribution. this type of chart is often called a c-chart.
4226,1,"['poisson', 'model', 'distribution', 'parameter', 'poisson distribution']", Control Charts for Attributes,seg_483,"suppose that the number of defects x in one unit of product follows the poisson distribution with parameter λ. (here t = 1 for the poisson model.) recall that for the poisson distribution,"
4227,1,"['poisson', 'chart', 'poisson random variable', 'control chart', 'random variable', 'variable', 'mean', 'random', 'control', 'variance', 'quality control']", Control Charts for Attributes,seg_483,"here, the random variable x is the number of nonconformities. in chapter 5, we learned that the mean and variance of the poisson random variable are both λ. thus, if the quality control chart were to be structured according to the usual 3σ limits, we could have, for λ known,"
4228,1,"['sample', 'chart', 'estimate', 'data', 'control chart', 'control', 'average', 'estimator', 'unbiased']", Control Charts for Attributes,seg_483,"as usual, λ often must come from an estimator from the data. an unbiased estimate of λ is the average number of nonconformities per sample. denote this estimate by λ̂. thus, the control chart has the limits"
4229,1,"['poisson', 'chart', 'estimate', 'table', 'data', 'control chart', 'samples', 'parameter', 'control']", Control Charts for Attributes,seg_483,"example 17.5: table 17.4 represents the number of defects in 20 successive samples of sheet metal rolls each 100 feet long. a control chart is to be developed from these preliminary data for the purpose of controlling the number of defects in such samples. the estimate of the poisson parameter λ is given by λ̂ = 5.95. as a result, the control limits suggested by these preliminary data are"
4230,1,['set'], Control Charts for Attributes,seg_483,with lcl being set to zero.
4231,1,"['control', 'data']", Control Charts for Attributes,seg_483,table 17.4: data for example 17.5; control involves number of defects in sheet metal rolls
4232,1,['sample'], Control Charts for Attributes,seg_483,sample number number of defects sample number number of defects 1 8 11 3 2 7 12 7 3 5 13 5 4 4 14 9 5 4 15 7 6 7 16 7 7 6 17 8 8 4 18 6 9 5 19 7 10 6 20 4 ave. 5.95
4233,1,"['plot', 'data', 'control']", Control Charts for Attributes,seg_483,figure 17.9 shows a plot of the preliminary data with the control limits revealed.
4234,1,"['plot', 'sample', 'chart', 'data', 'information', 'samples', 'control', 'process']", Control Charts for Attributes,seg_483,"table 17.5 shows additional data taken from the production process. for each sample, the unit on which the chart was based, namely 100 feet of the metal, was inspected. the information on 20 samples is included. figure 17.10 shows a plot of the additional production data. it is clear that the process is in control, at least through the period for which the data were taken."
4235,1,"['process', 'data']", Control Charts for Attributes,seg_483,table 17.5: additional data from the production process of example 17.5
4236,1,['sample'], Control Charts for Attributes,seg_483,sample number number of defects sample number number of defects 1 3 11 7 2 5 12 5 3 8 13 9 4 5 14 4 5 8 15 6 6 4 16 5 7 3 17 3 8 6 18 2 9 5 19 1 10 2 20 6
4237,1,"['sample size', 'sample', 'chart', 'factors', 'sampling', 'control chart', 'samples', 'set', 'cases', 'control']", Control Charts for Attributes,seg_483,"in example 17.5, we have made very clear what the sampling or inspection unit is, namely, 100 feet of metal. in many cases where the item is a specific one (e.g., a personal computer or a specific type of electronic device), the inspection unit may be a set of items. for example, the analyst may decide to use 10 computers in each subgroup and observe a count of the total number of defects found. thus, the preliminary sample for construction of the control chart would involve several samples, each containing 10 computers. the choice of the sample size may depend on many factors. often, we may want a sample size that will ensure an lcl that is positive."
4238,1,"['chart', 'sampling unit', 'case', 'sampling', 'control chart', 'control', 'average']", Control Charts for Attributes,seg_483,"the analyst may wish to use the average number of defects per sampling unit as the basic measure in the control chart. for example, for the case of the personal"
4239,1,['sample'], Control Charts for Attributes,seg_483,lcl lcl 0 0 0 5 10 15 20 0 5 10 15 20 sample sample
4240,1,"['chart', 'data']", Control Charts for Attributes,seg_483,figure 17.9: preliminary data plotted on the configure 17.10: additional production data for extrol chart for example 17.5. ample 17.5.
4241,1,"['random variable', 'variable', 'random']", Control Charts for Attributes,seg_483,"computer, let the random variable total number of defects"
4242,0,['n'], Control Charts for Attributes,seg_483,u = total number of defects n
4243,1,"['random', 'sample', 'poisson random variable', 'control chart', 'random variable', 'parameter', 'functions', 'chart', 'control', 'poisson', 'method', 'sampling unit', 'sampling', 'variable']", Control Charts for Attributes,seg_483,"be measured for each sample of, say, n = 10. we can use the method of momentgenerating functions to show that u is a poisson random variable (see review exercise 17.1) if we assume that the number of defects per sampling unit is poisson with parameter λ. thus, the control chart for this situation is characterized by the following:"
4244,1,"['data', 'set', 'data set', 'average']", Control Charts for Attributes,seg_483,"here, of course, ū is the average of the u -values in the preliminary or base data set. the term ū/n is derived from the result that"
4245,1,"['chart', 'estimate', 'control chart', 'control', 'unbiased']", Control Charts for Attributes,seg_483,and thus ū is an unbiased estimate of e(u) = λ and ū/n is an unbiased estimate of var(u) = λ/n. this type of control chart is often called a u-chart.
4246,1,"['poisson', 'model', 'normal approximation', 'condition', 'combination', 'approximation', 'results', 'case', 'distribution', 'normality', 'asymmetric', 'normal', 'probability', 'control', 'charts']", Control Charts for Attributes,seg_483,"in this section, we based our entire development of control charts on the poisson probability model. this model has been used in combination with the 3σ concept. as we implied earlier in this chapter, the notion of 3σ limits has its roots in the normal approximation, although many users feel that the concept works well as a pragmatic tool even if normality is not even approximately correct. the difficulty, of course, is that in the absence of normality, we cannot control the probability of incorrect specification of an out-of-control state. in the case of the poisson model, when λ is small the distribution is quite asymmetric, a condition that may produce undesirable results if we hold to the 3σ approach."
4247,1,"['chart', 'method', 'observations', 'statistics', 'control chart', 'mean', 'reference level', 'level', 'control', 'quality control', 'charts']", Cusum Control Charts,seg_485,"the disadvantage of the shewhart-type control charts, developed and illustrated in the preceding sections, lies in their inability to detect small changes in the mean. a quality control mechanism that has received considerable attention in the statistics literature and usage in industry is the cumulative sum (cusum) chart. the method for the cusum chart is simple and its appeal is intuitive. it should become obvious to the reader why it is more responsive to small changes in the mean. consider a control chart for the mean with a reference level established at value w . consider particular observations x1, x2, . . . , xr. the first r cusums are"
4248,1,"['level', 'reference level']", Cusum Control Charts,seg_485,"it becomes clear that the cusum is merely the accumulation of differences from the reference level. that is,"
4249,1,"['plot', 'chart']", Cusum Control Charts,seg_485,"the cusum chart is, then, a plot of sk against time."
4250,1,"['chart', 'observation', 'reference level', 'mean', 'level', 'slope']", Cusum Control Charts,seg_485,"suppose that we consider the reference level w to be an acceptable value of the mean μ. clearly, if there is no shift in μ, the cusum chart should be approximately horizontal, with some minor fluctuations balanced around zero. now, if there is only a moderate change in the mean, a relatively large change in the slope of the cusum chart should result, since each new observation has a chance of contributing a shift and the measure being plotted is accumulating these shifts. of course, the signal that the mean has shifted lies in the nature of the slope of the cusum chart. the purpose of the chart is to detect changes that are moving away from the reference level. a nonzero slope (in either direction) represents a change away from the reference level. a positive slope indicates an increase in the mean above the reference level, while a negative slope signals a decrease."
4251,1,"['process', 'acceptable quality level', 'hypothesis testing', 'set', 'rejectable quality level', 'mean', 'reference level', 'level', 'charts', 'hypothesis']", Cusum Control Charts,seg_485,cusum charts are often devised with a defined acceptable quality level (aql) and rejectable quality level (rql) preestablished by the user. both represent values of the mean. these may be viewed as playing roles somewhat similar to those of the null and alternative mean of hypothesis testing. consider a situation where the analyst hopes to detect an increase in the value of the process mean. we shall use the notation μ0 for aql and μ1 for rql and let μ1 > μ0. the reference level is now set at
4252,1,"['process', 'mean', 'slope']", Cusum Control Charts,seg_485,"the values of sr (r = 1, 2, . . . .) will have a negative slope if the process mean is at μ0 and a positive slope if the process mean is at μ1."
4253,1,"['statistical', 'quality control', 'control']", Cusum Control Charts,seg_485,706 chapter 17 statistical quality control
4254,1,"['chart', 'sampling', 'control', 'quality control', 'slope']", Cusum Control Charts,seg_485,"as indicated earlier, the slope of the cusum chart provides the signal for action by the quality control analyst. the decision rule calls for action if, at the rth sampling period,"
4255,1,['interval'], Cusum Control Charts,seg_485,where h is a prespecified value called the length of the decision interval and
4256,1,['data'], Cusum Control Charts,seg_485,"dr = sr − min si. 1≤i≤r−1 in other words, action is taken if the data reveal that the current cusum value exceeds by a specified amount the previous smallest cusum value."
4257,1,"['interval', 'method', 'plots', 'mean', 'plotting']", Cusum Control Charts,seg_485,"a modification in the mechanics described above makes employing the method easier. we have described a procedure that plots the cusums and computes differences. a simple modification involves plotting the differences directly and allows for checking against the decision interval. the general expression for dr is quite simple. for the cusum procedure where we are detecting increases in the mean,"
4258,0,[], Cusum Control Charts,seg_485,"the choice of the value of h is, of course, very important. we do not choose in this book to provide the many details in the literature dealing with this choice. the reader is referred to ewan and kemp, 1960, and montgomery, 2000b (see the bibliography) for a thorough discussion. one important consideration is the expected run length. ideally, the expected run length is quite large under μ = μ0 and quite small when μ = μ1."
4259,1,"['sample', 'probabilities', 'interval', 'estimation', 'frequencies', 'random sample', 'relative frequencies', 'information', 'random', 'confidence', 'confidence interval']", Bayesian Concepts,seg_491,"the classical methods of estimation that we have studied in this text are based solely on information provided by the random sample. these methods essentially interpret probabilities as relative frequencies. for example, in arriving at a 95% confidence interval for μ, we interpret the statement"
4260,1,"['experiments', 'mean']", Bayesian Concepts,seg_491,to mean that 95% of the time in repeated experiments z will fall between −1.96 and 1.96. since
4261,1,"['bayesian', 'probability', 'random', 'sample', 'intervals', 'mean', 'statistical', 'parameters', 'estimation', 'variance', 'random variables', 'method', 'variables', 'normal']", Bayesian Concepts,seg_491,"for a normal sample with known variance, the probability statement here means that 95% of the random intervals (x̄− 1.96σ/√n, x̄+1.96σ/√n) contain the true mean μ. another approach to statistical methods of estimation is called bayesian methodology. the main idea of the method comes from bayes’ rule, described in section 2.7. the key difference between the bayesian approach and the classical or frequentist approach is that in bayesian concepts, the parameters are viewed as random variables."
4262,1,"['bias', 'bayesian', 'percentage', 'relative frequency', 'frequency', 'probability', 'experiments']", Bayesian Concepts,seg_491,"subjective probability is the foundation of bayesian concepts. in chapter 2, we discussed two possible approaches to probability, namely the relative frequency and the indifference approaches. the first one determines a probability as a consequence of repeated experiments. for instance, to decide the free-throw percentage of a basketball player, we can record the number of shots made and the total number of attempts this player has made. the probability of hitting a free-throw for this player can be calculated as the ratio of these two numbers. on the other hand, if we have no knowledge of any bias in a die, the probability that a 3 will appear in the next throw will be 1/6. such an approach to probability interpretation is based on the indifference rule."
4263,1,"['probabilities', 'likelihood', 'probability', 'subjective', 'subjective probability']", Bayesian Concepts,seg_491,"however, in many situations, the preceding probability interpretations cannot be applied. for instance, consider the questions “what is the probability that it will rain tomorrow?” “how likely is it that this stock will go up by the end of the month?” and “what is the likelihood that two companies will be merged together?” they can hardly be interpreted by the aforementioned approaches, and the answers to these questions may be different for different people. yet these questions are constantly asked in daily life, and the approach used to explain these probabilities is called subjective probability, which reflects one’s subjective opinion."
4264,1,"['bayesian', 'estimates', 'bayesian statistics', 'statistics', 'random', 'sample', 'data', 'statistical', 'parameters', 'maximum likelihood estimates', 'variables', 'likelihood', 'maximum likelihood']", Bayesian Concepts,seg_491,"recall that in chapters 9 through 17, all statistical inferences were based on the fact that the parameters are unknown but fixed quantities, apart from those in section 9.14, in which the parameters were treated as variables and the maximum likelihood estimates (mles) were calculated conditioning on the observed sample data. in bayesian statistics, not only are the parameters treated as variables as in mle calculation, but also they are treated as random."
4265,1,"['bayesian', 'prior belief', 'conditional', 'probability', 'random', 'subjective', 'experiment', 'results', 'data', 'information', 'conditional perspective', 'parameter', 'statistical', 'subjective probability', 'experimental', 'parameters', 'statistical inference', 'distribution', 'probability distribution', 'prior distribution']", Bayesian Concepts,seg_491,"because the observed data are the only experimental results for the practitioner, statistical inference is based on the actual observed data from a given experiment. such a view is called a conditional perspective. furthermore, in bayesian concepts, since the parameters are treated as random, a probability distribution can be specified, generally by using the subjective probability for the parameter. such a distribution is called a prior distribution and it usually reflects the experimenter’s prior belief about the parameter. in the bayesian perspective, once an experiment is conducted and data are observed, all knowledge about the parameter is contained in the actual observed data and in the prior information."
4266,1,"['bayesian', 'distribution', 'binomial', 'binomial distribution', 'bayes']", Bayesian Concepts,seg_491,"although bayes’ rule is credited to thomas bayes, bayesian applications were first introduced by french scientist pierre simon laplace, who published a paper on using bayesian inference on the unknown binomial proportions (for binomial distribution, see section 5.2)."
4267,1,"['bayesian', 'bayesian statistics', 'statistics', 'data', 'markov chain monte carlo', 'statistical']", Bayesian Concepts,seg_491,"since the introduction of the markov chain monte carlo (mcmc) computational tools for bayesian analysis in the early 1990s, bayesian statistics has become more and more popular in statistical modeling and data analysis. meanwhile, methodology developments using bayesian concepts have progressed dramatically, and they are applied in fields such as bioinformatics, biology, business, engineering, environmental and ecology science, life science and health, medicine, and many others."
4268,1,"['sample', 'random', 'estimate', 'distribution', 'prior distribution', 'point estimate', 'random sample', 'parameter']", Bayesian Inferences,seg_493,"consider the problem of finding a point estimate of the parameter θ for the population with distribution f(x| θ), given θ. denote by π(θ) the prior distribution of θ. suppose that a random sample of size n, denoted by x = (x1, x2, . . . , xn), is observed."
4269,1,"['distribution', 'posterior distribution', 'posterior']", Bayesian Inferences,seg_493,"definition 18.1: the distribution of θ, given x, which is called the posterior distribution, is given by"
4270,1,"['distribution', 'marginal', 'marginal distribution']", Bayesian Inferences,seg_493,where g(x) is the marginal distribution of x.
4271,1,"['distribution', 'marginal', 'marginal distribution']", Bayesian Inferences,seg_493,the marginal distribution of x in the above definition can be calculated using the following formula:
4272,1,"['continuous', 'discrete']", Bayesian Inferences,seg_493,"⎧ ∑ f(x|θ)π(θ), θ is discrete, g(x) = ⎨ θ∞ ⎩∫−∞ f(x|θ)π(θ) dθ, θ is continuous."
4273,1,"['distribution', 'prior distribution']", Bayesian Inferences,seg_493,example 18.1: assume that the prior distribution for the proportion of defectives produced by a
4274,1,"['sample', 'posterior probability distribution', 'random sample', 'distribution', 'probability distribution', 'probability', 'random', 'posterior probability', 'posterior']", Bayesian Inferences,seg_493,"π(p) 0.6 0.4 denote by x the number of defectives among a random sample of size 2. find the posterior probability distribution of p, given that x is observed."
4275,1,"['distribution', 'random variable', 'variable', 'binomial', 'random', 'binomial distribution']", Bayesian Inferences,seg_493,solution : the random variable x follows a binomial distribution
4276,1,"['distribution', 'marginal', 'marginal distribution']", Bayesian Inferences,seg_493,the marginal distribution of x can be calculated as
4277,1,"['marginal', 'probabilities', 'marginal probabilities']", Bayesian Inferences,seg_493,"hence, for x = 0, 1, 2, we obtain the marginal probabilities as"
4278,1,"['posterior probability', 'probability', 'posterior']", Bayesian Inferences,seg_493,"x 0 1 2 g(x) 0.742 0.236 0.022 the posterior probability of p = 0.1, given x, is"
4279,0,[], Bayesian Inferences,seg_493,suppose that x = 0 is observed.
4280,1,"['range', 'discrete', 'distribution', 'prior distribution']", Bayesian Inferences,seg_493,"the prior distribution for example 18.1 is discrete, although the natural range of p is from 0 to 1. consider the following example, where we have a prior distribution covering the whole space for p."
4281,1,"['distribution', 'random variable', 'variable', 'prior distribution', 'random', 'posterior distribution', 'posterior']", Bayesian Inferences,seg_493,"example 18.2: suppose that the prior distribution of p is uniform (i.e., π(p) = 1, for 0 < p < 1). use the same random variable x as in example 18.1 to find the posterior distribution of p."
4282,0,[], Bayesian Inferences,seg_493,"solution : as in example 18.1, we have"
4283,1,"['distribution', 'marginal', 'marginal distribution']", Bayesian Inferences,seg_493,the marginal distribution of x can be calculated as
4284,1,"['distribution', 'posterior distribution', 'posterior']", Bayesian Inferences,seg_493,"the integral above can be evaluated at each x directly as g(0) = 1/3, g(1) = 1/3, and g(2) = 1/3. therefore, the posterior distribution of p, given x, is"
4285,1,"['parameters', 'distribution', 'mean', 'posterior distribution', 'beta distribution', 'posterior']", Bayesian Inferences,seg_493,"the posterior distribution above is actually a beta distribution (see section 6.8) with parameters α = x + 1 and β = 3 − x. so, if x = 0 is observed, the posterior distribution of p is a beta distribution with parameters (1, 3). the posterior mean"
4286,1,"['estimate', 'distribution', 'variable', 'posterior distribution', 'posterior', 'posterior distributions', 'distributions']", Bayesian Inferences,seg_493,"3 0 . using the posterior distribution, we can estimate the parameter(s) in a population in a straightforward fashion. in computing posterior distributions, it is very helpful if one is familiar with the distributions in chapters 5 and 6. note that in definition 18.1, the variable in the posterior distribution is θ, while x is given. thus, we can treat g(x) as a constant as we calculate the posterior distribution of θ. then the posterior distribution can be expressed as"
4287,1,"['factors', 'marginal', 'distribution', 'marginal density', 'posterior distribution', 'posterior']", Bayesian Inferences,seg_493,"where the symbol “∝” stands for is proportional to. in the calculation of the posterior distribution above, we can leave the factors that do not depend on θ out of the normalization constant, i.e., the marginal density g(x)."
4288,1,"['poisson', 'random variables', 'independent', 'variables', 'distribution', 'exponential', 'mean', 'prior distribution', 'random', 'poisson distribution', 'posterior distribution', 'posterior']", Bayesian Inferences,seg_493,"example 18.3: suppose that random variables x1, . . . , xn are independent and from a poisson distribution with mean λ. assume that the prior distribution of λ is exponential with mean 1. find the posterior distribution of λ when x̄ = 3 with n = 10."
4289,1,"['function', 'density function']", Bayesian Inferences,seg_493,"solution : the density function of x = (x1, . . . , xn) is"
4290,1,"['distribution', 'prior distribution']", Bayesian Inferences,seg_493,and the prior distribution is
4291,1,"['distribution', 'posterior distribution', 'posterior']", Bayesian Inferences,seg_493,"hence, using definition 18.1 we obtain the posterior distribution of λ as"
4292,1,"['distribution', 'gamma distribution', 'gamma', 'posterior']", Bayesian Inferences,seg_493,"referring to the gamma distribution in section 6.6, we conclude that the posterior"
4293,1,"['parameters', 'gamma', 'distribution', 'gamma distribution']", Bayesian Inferences,seg_493,n distribution of λ follows a gamma distribution with parameters 1+∑ xi and n+
4294,1,"['parameters', 'gamma', 'distribution', 'posterior distribution', 'gamma distribution', 'posterior']", Bayesian Inferences,seg_493,"+1 . ∑10 so, when x̄ = 3 with n = 10, we have i=1 xi = 30. hence, the posterior distribution of λ is a gamma distribution with parameters 31 and 1/11."
4295,1,"['distribution', 'posterior distribution', 'posterior']", Bayesian Inferences,seg_493,"from example 18.3 we observe that sometimes it is quite convenient to use the “proportional to” technique in calculating the posterior distribution, especially when the result can be formed to a commonly used distribution as described in chapters 5 and 6."
4296,1,"['parameters', 'estimate', 'median', 'distribution', 'mean', 'parameter', 'population', 'posterior distribution', 'posterior']", Bayesian Inferences,seg_493,"once the posterior distribution is derived, we can easily use the summary of the posterior distribution to make inferences on the population parameters. for instance, the posterior mean, median, and mode can all be used to estimate the parameter."
4297,1,"['mean', 'posterior']", Bayesian Inferences,seg_493,example 18.4: suppose that x = 1 is observed for example 18.2. find the posterior mean and the posterior mode.
4298,1,"['distribution', 'posterior distribution', 'posterior']", Bayesian Inferences,seg_493,"solution : when x = 1, the posterior distribution of p can be expressed as"
4299,1,"['distribution', 'mean']", Bayesian Inferences,seg_493,"to calculate the mean of this distribution, we need to find"
4300,1,"['distribution', 'posterior distribution', 'posterior']", Bayesian Inferences,seg_493,"to find the posterior mode, we need to obtain the value of p such that the posterior distribution is maximized. taking derivative of π(p) with respect to p, we obtain 6− 12p. solving for p in 6− 12p = 0, we obtain p = 1/2. the second derivative is −12, which implies that the posterior mode is achieved at p = 1/2."
4301,1,"['estimation', 'normal', 'mean', 'population']", Bayesian Inferences,seg_493,bayesian methods of estimation concerning the mean μ of a normal population are based on the following example.
4302,1,"['sample', 'population mean', 'random sample', 'distribution', 'normal', 'mean', 'population', 'random', 'prior distribution', 'posterior distribution', 'variance', 'normal distribution', 'posterior']", Bayesian Inferences,seg_493,"example 18.5: if x̄ is the mean of a random sample of size n from a normal population with known variance σ2, and the prior distribution of the population mean is a normal distribution with known mean μ0 and known variance σ02, then show that the posterior distribution of the population mean is also a normal distribution with"
4303,1,"['deviation', 'standard deviation', 'standard']", Bayesian Inferences,seg_493,"mean μ∗ and standard deviation σ∗, where"
4304,1,"['sample', 'density function', 'function']", Bayesian Inferences,seg_493,solution : the density function of our sample is
4305,1,"['distribution', 'posterior distribution', 'posterior']", Bayesian Inferences,seg_493,then the posterior distribution of μ is
4306,1,"['distribution', 'posterior distribution', 'posterior']", Bayesian Inferences,seg_493,from section 8.5. completing the squares for μ yields the posterior distribution
4307,1,"['deviation', 'distribution', 'normal', 'mean', 'standard', 'standard deviation', 'normal distribution']", Bayesian Inferences,seg_493,this is a normal distribution with mean μ∗ and standard deviation σ∗.
4308,1,"['experimental', 'random samples', 'symmetric', 'populations', 'distribution', 'samples', 'cases', 'normal', 'central limit theorem', 'mean', 'random', 'prior distribution', 'limit']", Bayesian Inferences,seg_493,"the central limit theorem allows us to use example 18.5 also when we select sufficiently large random samples (n ≥ 30 for many engineering experimental cases) from nonnormal populations (the distribution is not very far from symmetric), and when the prior distribution of the mean is approximately normal."
4309,1,"['mean', 'posterior']", Bayesian Inferences,seg_493,several comments need to be made about example 18.5. the posterior mean μ∗ can also be written as
4310,1,"['sample', 'weighted average', 'sample mean', 'mean', 'posterior', 'average', 'coefficients']", Bayesian Inferences,seg_493,"which is a weighted average of the sample mean x̄ and the prior mean μ0. since both coefficients are between 0 and 1 and they sum to 1, the posterior mean μ∗ is always"
4311,1,"['case', 'function', 'subjective', 'sample', 'sample mean', 'data', 'information', 'mean', 'parameter', 'population', 'density function', 'estimation', 'distribution', 'posterior distribution', 'variance', 'population mean', 'normal', 'normal distribution', 'posterior']", Bayesian Inferences,seg_493,"between x̄ and μ0. this means that the posterior estimation of μ is influenced by both x̄ and μ0. furthermore, the weight of x̄ depends on the prior variance as well as the variance of the sample mean. for a large sample problem (n → ∞), the posterior mean μ∗ → x̄. this means that the prior mean does not play any role in estimating the population mean μ using the posterior distribution. this is very reasonable since it indicates that when the amount of data is substantial, information from the data will dominate the information on μ provided by the prior. on the other hand, when the prior variance is large (σ02 → ∞), the posterior mean μ∗ also goes to x̄. note that for a normal distribution, the larger the variance, the flatter the density function. the flatness of the normal distribution in this case means that there is almost no subjective prior information available on the parameter μ before the data are collected. thus, it is reasonable that the posterior estimation μ∗ only depends on the data value x̄."
4312,1,"['deviation', 'standard', 'standard deviation', 'posterior']", Bayesian Inferences,seg_493,now consider the posterior standard deviation σ∗. this value can also be written as
4313,1,"['deviation', 'sample', 'bayesian', 'sample variance', 'estimation', 'results', 'case', 'data', 'information', 'standard', 'standard deviation', 'variance', 'posterior']", Bayesian Inferences,seg_493,"it is obvious that the value σ∗ is smaller than both σ0 and σ/√n, the prior standard deviation and the standard deviation of x̄, respectively. this suggests that the posterior estimation is more accurate than both the prior and the sample data. hence, incorporating both the data and prior information results in better posterior information than using any of the data or prior alone. this is a common phenomenon in bayesian inference. furthermore, to compute μ∗ and σ∗ by the formulas in example 18.5, we have assumed that σ2 is known. since this is generally not the case, we shall replace σ2 by the sample variance s2 whenever n ≥ 30."
4314,1,"['bayesian', 'bayesian interval', 'interval', 'distribution', 'confidence', 'posterior distribution', 'confidence interval', 'posterior']", Bayesian Inferences,seg_493,"similar to the classical confidence interval, in bayesian analysis we can calculate a 100(1− α)% bayesian interval using the posterior distribution."
4315,1,"['bayesian', 'bayesian interval', 'interval']", Bayesian Inferences,seg_493,definition 18.2: the interval a < θ < b will be called a 100(1− α)% bayesian interval for θ if
4316,1,"['bayesian', 'bayesian interval', 'true parameter', 'coverage probability', 'interval', 'experiment', 'data', 'intervals', 'probability', 'parameter', 'confidence', 'confidence interval']", Bayesian Inferences,seg_493,"recall that under the frequentist approach, the probability of a confidence interval, say 95%, is interpreted as a coverage probability, which means that if an experiment is repeated again and again (with considerable unobserved data), the probability that the intervals calculated according to the rule will cover the true parameter is 95%. however, in bayesian interval interpretation, say for a 95% interval, we can state that the probability of the unknown parameter falling into the calculated interval (which only depends on the observed data) is 95%."
4317,1,"['bayesian', 'bayesian interval', 'interval', 'distribution', 'prior distribution']", Bayesian Inferences,seg_493,"example 18.6: supposing that x ∼ b(x;n, p), with known n = 2, and the prior distribution of p is uniform π(p) = 1, for 0 < p < 1, find a 95% bayesian interval for p."
4318,1,"['parameters', 'distribution', 'posterior distribution', 'beta distribution', 'posterior']", Bayesian Inferences,seg_493,"solution : as in example 18.2, when x = 0, the posterior distribution is a beta distribution with parameters 1 and 3, i.e., π(p|0) = 3(1− p)2, for 0 < p < 1. thus, we need to solve for a and b using definition 18.2, which yields the following:"
4319,1,['probability'], Bayesian Inferences,seg_493,"the solutions to the above equations result in a = 0.0084 and b = 0.7076. therefore, the probability that p falls into (0.0084, 0.7076) is 95%."
4320,1,"['interval', 'estimate', 'population mean', 'bayes estimate', 'case', 'normal', 'mean', 'population', 'bayes', 'posterior']", Bayesian Inferences,seg_493,"for the normal population and normal prior case described in example 18.5, the posterior mean μ∗ is the bayes estimate of the population mean μ, and a 100(1−α)%bayesian interval for μ can be constructed by computing the interval"
4321,1,"['posterior probability', 'mean', 'probability', 'posterior']", Bayesian Inferences,seg_493,which is centered at the posterior mean and contains 100(1−α)% of the posterior probability.
4322,1,"['bayesian', 'interval', 'normally distributed', 'random', 'random sample', 'sample', 'random variable', 'mean', 'standard', 'normal random variable', 'standard deviation', 'deviation', 'bayesian interval', 'variable', 'normal', 'average']", Bayesian Inferences,seg_493,"example 18.7: an electrical firm manufactures light bulbs that have a length of life that is approximately normally distributed with a standard deviation of 100 hours. prior experience leads us to believe that μ is a value of a normal random variable with a mean μ0 = 800 hours and a standard deviation σ0 = 10 hours. if a random sample of 25 bulbs has an average life of 780 hours, find a 95% bayesian interval for μ."
4323,1,"['distribution', 'normal', 'mean', 'posterior distribution', 'normal distribution', 'posterior']", Bayesian Inferences,seg_493,"solution : according to example 18.5, the posterior distribution of the mean is also a normal distribution with mean"
4324,1,"['deviation', 'standard deviation', 'standard']", Bayesian Inferences,seg_493,and standard deviation
4325,1,"['bayesian', 'bayesian interval', 'interval']", Bayesian Inferences,seg_493,the 95% bayesian interval for μ is then given by
4326,1,"['confidence', 'interval', 'confidence interval', 'information']", Bayesian Inferences,seg_493,"on the other hand, ignoring the prior information about μ, we could proceed as in section 9.4 and construct the classical 95% confidence interval"
4327,1,"['bayesian', 'bayesian interval', 'interval']", Bayesian Inferences,seg_493,"or 740.8 < μ < 819.2, which is seen to be wider than the corresponding bayesian interval."
4328,1,"['bayesian', 'estimates', 'loss function', 'bayes estimates', 'loss', 'associated', 'function', 'bayes', 'parameter', 'event', 'functions', 'distribution', 'posterior distribution', 'posterior']", Bayes Estimates Using Decision Theory Framework,seg_495,"using bayesian methodology, the posterior distribution of a parameter can be obtained. bayes estimates can also be derived using the posterior distribution and a loss function when a loss is incurred. a loss function is a function that describes the cost of a decision associated with an event of interest. here we only list a few commonly used loss functions and their associated bayes estimates."
4329,1,"['function', 'loss', 'loss function']", Bayes Estimates Using Decision Theory Framework,seg_495,definition 18.3: the squared-error loss function is
4330,1,"['parameter', 'estimate']", Bayes Estimates Using Decision Theory Framework,seg_495,where θ is the parameter (or state of nature) and a an action (or estimate).
4331,1,"['sample', 'estimate', 'bayes estimate', 'data', 'loss', 'bayes', 'posterior']", Bayes Estimates Using Decision Theory Framework,seg_495,"a bayes estimate minimizes the posterior expected loss, given on the observed sample data."
4332,1,"['loss function', 'distribution', 'loss', 'mean', 'function', 'posterior distribution', 'bayes', 'posterior']", Bayes Estimates Using Decision Theory Framework,seg_495,"theorem 18.1: the mean of the posterior distribution π(θ|x), denoted by θ∗, is the bayes estimate of θ under the squared-error loss function."
4333,1,"['estimates', 'loss function', 'bayes estimates', 'loss', 'function', 'bayes']", Bayes Estimates Using Decision Theory Framework,seg_495,"example 18.8: find the bayes estimates of p, for all the values of x, for example 18.1 when the squared-error loss function is used."
4334,1,"['estimate', 'estimates', 'bayes estimates', 'bayes']", Bayes Estimates Using Decision Theory Framework,seg_495,"note that the classical estimate of p is p̂ = x/n = 0, 1/2, and 1, respectively, for the x values at 0, 1, and 2. these classical estimates are very different from the corresponding bayes estimates."
4335,1,"['estimate', 'bayes estimate', 'distribution', 'posterior distribution', 'bayes', 'posterior']", Bayes Estimates Using Decision Theory Framework,seg_495,"example 18.9: repeat example 18.8 in the situation of example 18.2. solution : since the posterior distribution of p is a b(x + 1, 3 − x) distribution (see section 6.8 on page 201), the bayes estimate of p is"
4336,1,"['bayes estimate', 'bayes', 'estimate']", Bayes Estimates Using Decision Theory Framework,seg_495,"which yields p∗ = 1/4 for x = 0, p∗ = 1/2 for x = 1, and p∗ = 3/4 for x = 2, respectively. notice that when x = 1 is observed, the bayes estimate and the classical estimate p̂ are equivalent."
4337,1,"['estimate', 'bayes estimate', 'loss', 'normal', 'mean', 'bayes', 'posterior']", Bayes Estimates Using Decision Theory Framework,seg_495,"for the normal situation as described in example 18.5, the bayes estimate of μ under the squared-error loss will be the posterior mean μ∗."
4338,1,"['poisson', 'sampling distribution', 'gamma', 'sampling', 'random variable', 'variable', 'distribution', 'prior distribution', 'parameter', 'random', 'gamma distribution']", Bayes Estimates Using Decision Theory Framework,seg_495,"example 18.10: suppose that the sampling distribution of a random variable, x, is poisson with parameter λ. assume that the prior distribution of λ follows a gamma distribution"
4339,1,"['bayesian', 'bayesian statistics', 'statistics']", Bayes Estimates Using Decision Theory Framework,seg_495,718 chapter 18 bayesian statistics
4340,1,"['parameters', 'estimate', 'bayes estimate', 'loss function', 'loss', 'function', 'bayes']", Bayes Estimates Using Decision Theory Framework,seg_495,"with parameters (α, β). find the bayes estimate of λ under the squared-error loss function."
4341,1,"['parameters', 'gamma', 'distribution', 'mean', 'posterior distribution', 'gamma distribution', 'posterior']", Bayes Estimates Using Decision Theory Framework,seg_495,"solution : using example 18.3, we conclude that the posterior distribution of λ follows a gamma distribution with parameters (x+α, (1 + 1/β)−1). using theorem 6.4, we obtain the posterior mean"
4342,1,"['estimate', 'bayes estimate', 'loss', 'mean', 'bayes', 'posterior']", Bayes Estimates Using Decision Theory Framework,seg_495,"since the posterior mean is the bayes estimate under the squared-error loss, λ̂ is our bayes estimate."
4343,1,"['loss function', 'regression', 'loss', 'function']", Bayes Estimates Using Decision Theory Framework,seg_495,"the squared-error loss described above is similar to the least-squares concept we discussed in connection with regression in chapters 11 and 12. in this section, we introduce another loss function as follows."
4344,1,"['function', 'loss', 'loss function']", Bayes Estimates Using Decision Theory Framework,seg_495,definition 18.4: the absolute-error loss function is defined as
4345,1,['parameter'], Bayes Estimates Using Decision Theory Framework,seg_495,where θ is the parameter and a an action.
4346,1,"['median', 'estimate', 'bayes estimate', 'loss function', 'distribution', 'loss', 'function', 'posterior distribution', 'bayes', 'posterior']", Bayes Estimates Using Decision Theory Framework,seg_495,"theorem 18.2: the median of the posterior distribution π(θ|x), denoted by θ∗, is the bayes estimate of θ under the absolute-error loss function."
4347,1,"['loss', 'estimator', 'bayes', 'bayes estimator']", Bayes Estimates Using Decision Theory Framework,seg_495,"example 18.11: under the absolute-error loss, find the bayes estimator for example 18.9 when x = 1 is observed."
4348,1,"['median', 'distribution', 'posterior distribution', 'beta distribution', 'posterior']", Bayes Estimates Using Decision Theory Framework,seg_495,"solution : again, the posterior distribution of p is a b(x+1, 3−x). when x = 1, it is a beta distribution with density π(p | x = 1) = 6x(1− x) for 0 < x < 1 and 0 otherwise. the median of this distribution is the value of p∗ such that"
4349,0,[], Bayes Estimates Using Decision Theory Framework,seg_495,which yields the answer p∗ = 2
4350,1,"['bayes', 'bayes estimate', 'case', 'estimate']", Bayes Estimates Using Decision Theory Framework,seg_495,"1 . hence, the bayes estimate in this case is 0.5."
4351,1,"['analysis of variance', 'variance', 'statistical']",Bibliography,seg_499,"[1] bartlett, m. s., and kendall, d. g. (1946). “the statistical analysis of variance heterogeneity and logarithmic transformation,” journal of the royal statistical society, ser. b. 8, 128–138."
4352,1,['statistics'],Bibliography,seg_499,"[2] bowker, a. h., and lieberman, g. j. (1972). engineering statistics, 2nd ed. upper saddle river, n.j.: prentice hall."
4353,1,['transformations'],Bibliography,seg_499,"[3] box, g. e. p. (1988). “signal to noise ratios, performance criteria and transformations (with discussion),” technometrics, 30, 1–17."
4354,1,"['parameter', 'variation']",Bibliography,seg_499,"[4] box, g. e. p., and fung, c. a. (1986). “studies in quality improvement: minimizing transmitted variation by parameter design,” report 8. university of wisconsin-madison, center for quality and productivity improvement."
4355,1,['statistics'],Bibliography,seg_499,"[5] box, g. e. p., hunter, w. g., and hunter, j. s. (1978). statistics for experimenters. new york: john wiley & sons."
4356,1,['statistical'],Bibliography,seg_499,"[6] brownlee, k. a. (1984). statistical theory and methodology: in science and engineering, 2nd ed. new york: john wiley & sons."
4357,1,"['transformation', 'regression']",Bibliography,seg_499,"[7] carroll, r. j., and ruppert, d. (1988). transformation and weighting in regression. new york: chapman and hall."
4358,1,"['regression analysis', 'regression']",Bibliography,seg_499,"[8] chatterjee, s., hadi, a. s., and price, b. (1999). regression analysis by example, 3rd ed. new york: john wiley & sons."
4359,1,"['residuals', 'regression']",Bibliography,seg_499,"[9] cook, r. d., and weisberg, s. (1982). residuals and influence in regression. new york: chapman and hall."
4360,1,['data'],Bibliography,seg_499,"[10] daniel, c. and wood, f. s. (1999). fitting equations to data: computer analysis of multifactor data, 2nd ed. new york: john wiley & sons."
4361,1,"['nonparametric statistics', 'statistics']",Bibliography,seg_499,"[11] daniel, w. w. (1989). applied nonparametric statistics, 2nd ed. belmont, calif.: wadsworth publishing company."
4362,1,"['statistics', 'control', 'quality control']",Bibliography,seg_499,"[15] duncan, a. (1986). quality control and industrial statistics, 5th ed. homewood, ill.: irwin."
4363,1,"['critical values', 'association', 'statistical']",Bibliography,seg_499,"[16] dyer, d. d., and keating, j. p. (1980). “on the determination of critical values for bartlett’s test,” journal of the american statistical association, 75, 313–319."
4364,1,"['continuous', 'processes']",Bibliography,seg_499,"[17] ewan, w. d., and kemp, k. w. (1960). “sampling inspection of continuous processes with no autocorrelation between successive results,” biometrika, 47, 363–380."
4365,0,[],Bibliography,seg_499,"[18] geary, r. c. (1947). “testing for normality,” biometrika, 34, 209–242."
4366,1,"['regression analysis', 'regression']",Bibliography,seg_499,"[19] gunst, r. f., and mason, r. l. (1980). regression analysis and its application: a data-oriented approach. new york: marcel dekker."
4367,1,['statistics'],Bibliography,seg_499,"[20] guttman, i., wilks, s. s., and hunter, j. s. (1971). introductory engineering statistics. new york: john wiley & sons."
4368,1,"['association', 'estimation', 'likelihood', 'statistical', 'variance']",Bibliography,seg_499,"[21] harville, d. a. (1977). “maximum likelihood approaches to variance component estimation and to related problems,” journal of the american statistical association, 72, 320--338."
4369,1,"['design of experiments', 'design', 'experiments']",Bibliography,seg_499,"[22] hicks, c. r., and turner, k. v. (1999). fundamental concepts in the design of experiments, 5th ed. oxford: oxford university press."
4370,1,"['variance', 'analysis of variance']",Bibliography,seg_499,"[23] hoaglin, d. c., mosteller, f., and tukey, j. w. (1991). fundamentals of exploratory analysis of variance. new york: john wiley & sons."
4371,1,"['linear', 'variables']",Bibliography,seg_499,"[24] hocking, r. r. (1976). “the analysis and selection of variables in linear regression,” biometrics, 32, 1–49."
4372,1,"['statistics', 'probability']",Bibliography,seg_499,"[25] hodges, j. l., and lehmann, e. l. (2005). basic concepts of probability and statistics, 2nd ed. philadelphia: society for industrial and applied mathematics."
4373,1,['regression'],Bibliography,seg_499,"[26] hoerl, a. e., and wennard, r. w. (1970). “ridge regression: applications to nonorthogonal problems,” technometrics, 12, 55–67."
4374,1,['statistics'],Bibliography,seg_499,"[27] hogg, r. v., and ledolter, j. (1992). applied statistics for engineers and physical scientists, 2nd ed. upper saddle river, n.j.: prentice hall."
4375,1,['statistics'],Bibliography,seg_499,"[28] hogg, r. v., mckean, j. w., and craig, a. (2005). introduction to mathematical statistics, 6th ed. upper saddle river, n.j.: prentice hall."
4376,1,['statistical'],Bibliography,seg_499,"[29] hollander, m., and wolfe, d. (1999). nonparametric statistical methods. new york: john wiley & sons."
4377,1,"['experimental', 'design', 'statistics']",Bibliography,seg_499,"[30] johnson, n. l., and leone, f. c. (1977). statistics and experimental design: in engineering and the physical sciences, 2nd ed. vols. i and ii, new york: john wiley & sons."
4378,1,"['design', 'parameter', 'control', 'quality control']",Bibliography,seg_499,"[31] kackar, r. (1985). “off-line quality control, parameter design, and the taguchi methods,” journal of quality technology, 17, 176–188."
4379,1,['statistics'],Bibliography,seg_499,"[32] koopmans, l. h. (1987). an introduction to contemporary statistics, 2nd ed. boston: duxbury press."
4380,1,"['linear', 'regression', 'linear regression']",Bibliography,seg_499,"[33] kutner, m. h., nachtsheim, c. j., neter, j., and li, w. (2004). applied linear regression models, 5th ed. new york: mcgraw-hill/irwin."
4381,1,['statistics'],Bibliography,seg_499,"[34] larsen, r. j., and morris, m. l. (2000). an introduction to mathematical statistics and its applications, 3rd ed. upper saddle river, n.j.: prentice hall."
4382,1,['statistical'],Bibliography,seg_499,"[35] lehmann, e. l., and d’abrera, h. j. m. (1998). nonparametrics: statistical methods based on ranks, rev. ed. upper saddle river, n.j.: prentice hall."
4383,1,"['design', 'experiments']",Bibliography,seg_499,"[36] lentner, m., and bishop, t. (1986). design and analysis of experiments, 2nd ed. blacksburg, va.: valley book co."
4384,1,['statistics'],Bibliography,seg_499,"[38] mcclave, j. t., dietrich, f. h., and sincich, t. (1997). statistics, 7th ed. upper saddle river, n.j.: prentice hall."
4385,1,"['design', 'experiments']",Bibliography,seg_499,"[39] montgomery, d. c. (2008a). design and analysis of experiments, 7th ed. new york: john wiley & sons."
4386,1,"['statistical', 'quality control', 'control']",Bibliography,seg_499,"[40] montgomery, d. c. (2008b). introduction to statistical quality control, 6th ed. new york: john wiley & sons."
4387,1,"['data', 'regression']",Bibliography,seg_499,"[41] mosteller, f., and tukey, j. (1977). data analysis and regression. reading, mass.: addison-wesley publishing co."
4388,1,['regression'],Bibliography,seg_499,"[42] myers, r. h. (1990). classical and modern regression with applications, 2nd ed. boston: duxbury press."
4389,1,"['design', 'statistician', 'parameter']",Bibliography,seg_499,"[43] myers, r. h., khuri, a. i., and vining, g. g. (1992). “response surface alternatives to the taguchi robust parameter design approach,” the american statistician, 46, 131–139."
4390,1,"['experiments', 'process', 'response', 'response surface']",Bibliography,seg_499,"[44] myers, r. h., montgomery, d. c., and anderson-cook, c. m. (2009). response surface methodology: process and product optimization using designed experiments, 3rd ed. new york: john wiley & sons."
4391,1,"['linear', 'generalized linear models']",Bibliography,seg_499,"[45] myers, r. h., montgomery, d. c., vining, g. g., and robinson, t. j. (2008). generalized linear models with applications in engineering and the sciences, 2nd ed., new york: john wiley & sons."
4392,1,['statistics'],Bibliography,seg_499,"[46] noether, g. e. (1976). introduction to statistics: a nonparametric approach, 2nd ed. boston: houghton mifflin company."
4393,1,['probability'],Bibliography,seg_499,"[47] olkin, i., gleser, l. j., and derman, c. (1994). probability models and applications, 2nd ed. new york: prentice hall."
4394,1,"['data', 'statistical']",Bibliography,seg_499,"[48] ott, r. l., and longnecker, m. t. (2000). an introduction to statistical methods and data analysis, 5th ed. boston: duxbury press."
4395,1,['dispersion'],Bibliography,seg_499,"[49] pacansky, j., england, c. d., and wattman, r. (1986). “infrared spectroscopic studies of poly (perfluoropropyleneoxide) on gold substrate: a classical dispersion analysis for the refractive index.” applied spectroscopy, 40, 8–16."
4396,1,['design'],Bibliography,seg_499,"[50] plackett, r. l., and burman, j. p. (1946). “the design of multifactor experiments,” biometrika, 33, 305–325."
4397,1,['probability'],Bibliography,seg_499,"[51] ross, s. m. (2002). introduction to probability models, 9th ed. new york: academic press, inc."
4398,1,"['distribution', 'variance', 'estimates']",Bibliography,seg_499,"[52] satterthwaite, f. e. (1946). “an approximate distribution of estimates of variance components,” biometrics, 2, 110–114."
4399,1,['control'],Bibliography,seg_499,"[53] schilling, e. g., and nelson, p. r. (1976). “the effect of nonnormality on the control limits of x̄ charts,” journal of quality technology, 8, 347–373."
4400,1,['experiments'],Bibliography,seg_499,"[54] schmidt, s. r., and launsby, r. g. (1991). understanding industrial designed experiments. colorado springs, col. air academy press."
4401,1,['parameter'],Bibliography,seg_499,"[55] shoemaker, a. c., tsui, k.-l., and wu, c. f. j. (1991). “economical experimentation methods for robust parameter design,” technometrics, 33, 415–428."
4402,1,['statistical'],Bibliography,seg_499,"[56] snedecor, g. w., and cochran, w. g. (1989). statistical methods, 8th ed. allies, iowa: the iowa state university press."
4403,1,['statistics'],Bibliography,seg_499,"[57] steel, r. g. d., torrie, j. h., and dickey, d. a. (1996). principles and procedures of statistics: a biometrical approach, 3rd ed. new york: mcgraw-hill."
4404,0,[],Bibliography,seg_499,"[58] taguchi, g. (1991). introduction to quality engineering. white plains, n.y.: unipub/kraus international."
4405,1,"['control', 'quality control', 'association']",Bibliography,seg_499,"[59] taguchi, g., and wu, y. (1985). introduction to off-line quality control. nagoya, japan: central japan quality control association."
4406,1,"['variables', 'regression', 'predictor variables', 'predictor']",Bibliography,seg_499,"[60] thompson, w. o., and cady, f. b. (1973). proceedings of the university of kentucky conference on regression with a large number of predictor variables. lexington, ken.: university of kentucky press."
4407,1,['data'],Bibliography,seg_499,"[61] tukey, j. w. (1977). exploratory data analysis. reading, mass.: addison-wesley publishing co."
4408,1,"['response', 'response surface']",Bibliography,seg_499,"[62] vining, g. g., and myers, r. h. (1990). “combining taguchi and response surface philosophies: a dual response approach,” journal of quality technology, 22, 38–45."
4409,1,"['parameter', 'experiments', 'control', 'quality control']",Bibliography,seg_499,"[63] welch, w. j., yu, t. k., kang, s. m., and sacks, j. (1990). “computer experiments for quality control by parameter design,” journal of quality technology, 22, 15–22."
4410,1,"['experimental', 'design', 'statistical']",Bibliography,seg_499,"[64] winer, b. j. (1991). statistical principles in experimental design, 3rd ed. new york: mcgraw-hill."
