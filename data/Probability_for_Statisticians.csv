,Relevance,Tags,Heading,Seg,Sentence
0,1,"['function', 'limit', 'continuous']", Basic Properties of Measures,seg_3,"motivation 1.1 (the lebesgue integral) the riemann integral of a continuous function f (we will restrict attention to f(x) ≥ 0 on a ≤ x ≤ b for convenience) is formed by subdividing the domain of f, forming approximating sums, and passing to the limit. thus the mth riemann"
1,1,"['functions', 'interval', 'continuous']", Basic Properties of Measures,seg_3,"where a ≡ xm0 < xm1 < · · · < xmm ≡ b (with xm,i−1 ≤ x∗mi ≤ xmi for all i) satisfy meshm ≡ max[xmi − xm,i−1] → 0. note that xmi − xm,i−1 is the measure (or length) of the interval [xm,i−1, xmi], while f(x∗mi) approximates the values of f(x) for all xm,i−1 ≤ x ≤ xmi (at least it does if f is continuous on [a, b]). within the class c+ of all nonnegative continuous functions, this definition works reasonably well. but it has one major shortcoming. the conclusion"
2,0,[], Basic Properties of Measures,seg_3,it is well defined).
3,1,['function'], Basic Properties of Measures,seg_3,a different approach is needed. (note figure 1.1.) the lebesgue integral of a bounded and nonnegative function is formed by subdividing
4,1,['range'], Basic Properties of Measures,seg_3,the range. thus the mth lebesgue sum for ∫a
5,1,"['functions', 'sets', 'limit']", Basic Properties of Measures,seg_3,b f(x) dx is defined to be the limit of the lsm sums as m → ∞. for what class m of functions f can this approach succeed? the members f of the class m will need to be such that the measure (or length) of all sets of the form
6,1,['sets'], Basic Properties of Measures,seg_3,"can be specified. this approach leads to the concept of a σ-field a of subsets of [a, b] that are measurable (that is, we must be able to assign to these sets a number called their “length”),"
7,1,"['functions', 'limit']", Basic Properties of Measures,seg_3,"and this leads to the concept of the class m of measurable functions. this class m of measurable functions will be seen to be closed under passage to the limit and all the other operations that we are accustomed to performing on functions. moreover, the desirable property"
8,1,['functions'], Basic Properties of Measures,seg_3,b f(x) dx for functions fn “converging” to f will be broadly true.
9,0,[], Basic Properties of Measures,seg_3,figure 1.1 riemann sums and lebesgue sums.
10,1,"['sample', 'experiment', 'set', 'set theory', 'sample space']", Basic Properties of Measures,seg_3,"definition 1.1 (set theory) consider a nonvoid class a of subsets a of a nonvoid set ω. (for us, ω will be the sample space of an experiment.)"
11,1,"['disjoint', 'symmetric', 'intersection', 'sets', 'complement', 'set', 'union']", Basic Properties of Measures,seg_3,"(a) let ac denote the complement of a, let a ∪ b denote the union of a and b, let a ∩ b and ab both denote the intersection, let a \b ≡ abc denote the set difference, let a b ≡ (acb∪abc) denote the symmetric difference, and let ∅ denote the empty set. the class of all subsets of ω will be denoted by 2ω. sets a and b are called disjoint if ab = ∅, and sequences of sets an or classes of sets at are called disjoint if all pairs are disjoint. writing a + b or"
12,1,"['sets', 'union']", Basic Properties of Measures,seg_3,"1 an will also denote a union, but will imply the disjointness of the sets in the union. as usual, a ⊂ b denotes that a is a subset of b. we call a sequence an increasing (and we will nearly always denote this fact by writing an ↗) when an ⊂ an+1 for all n ≥ 1. we call the sequence decreasing (denoted by an ↘) when an ⊃ an+1 for all n ≥ 1. we call the sequence monotone if it is either increasing or decreasing. let ω denote a generic element of ω. we"
13,1,"['indicator function', 'function', 'indicator']", Basic Properties of Measures,seg_3,"will use 1a(·) to denote the indicator function of a, which equals 1 or 0 at ω according as ω ∈ a or ω ∈/ a. (b) a will be called a field if it is closed under complements and unions. (that is, a and b in a requires that ac and a ∪ b be in a.) [note that both ω and ∅ are necessarily in a, as a was assumed to be nonvoid, with ω = a ∪ ac and ∅ = ωc.] (c) a will be called a σ-field if it is closed under complements and countable unions. (that is, a,a1, a2, . . . in a requires that ac and ∪1∞ an be in a.) (d) a will be called a monotone class provided it contains ∪∞1 an for all increasing sequences an in a and contains ∩∞1 an for all decreasing sequences an in a. (e) (ω,a) will be called a measurable space provided a is a σ-field of subsets of ω. (f) a will be called a π-system provided ab is in a for all a and b in a; and a will be called a π̄-system when ω in a is also guaranteed."
14,1,[], Basic Properties of Measures,seg_3,"if a is a field (or a σ-field), then it is closed under intersections (under countable intersections); since ab = (ac ∪ bc)c (since ∩1∞an = (∪1∞anc )c). likewise, we could have used “intersection” instead of “union” in our definitions by making use of a ∪ b = (ac ∩ bc)c and ∪∞1 an = (∩∞1 acn)c. (this used de morgan’s laws.)"
15,1,[], Basic Properties of Measures,seg_3,"proposition 1.1 (closure under intersections) (a) arbitrary intersections of fields, σ-fields, or monotone classes are fields, σ-fields, or monotone classes, respectively. [for example, f ≡ ∩{fα : fα is a field under consideration} is a field.] (b) there is a minimal field, σ-field, or monotone class generated by (or, containing) any specified class c of subsets of ω. call c the generators. for example,"
16,0,[], Basic Properties of Measures,seg_3,"is the minimal σ-field generated by c (that is, containing c). (c) a collection a of subsets of ω is a σ-field if and only if it is both a field and a monotone class."
17,1,['set'], Basic Properties of Measures,seg_3,"exercise 1.1 (generators) let c1 and c2 denote two collections of subsets of the set ω. if c2 ⊂ σ[c1] and c1 ⊂ σ[c2], then σ[c1] = σ[c2]. prove this important fact."
18,1,"['events', 'set', 'function']", Basic Properties of Measures,seg_3,"definition 1.2 (measures and events) consider a measurable space (ω,a) and a set function μ : a → [0, ∞] (that is, μ(a) ≥ 0 for each a ∈ a) having μ(∅) = 0. (a) now a is a σ-field and if μ is countably additive (abbreviated c.a.) in that"
19,1,['disjoint'], Basic Properties of Measures,seg_3,"μ(an) for all disjoint sequences an in a,"
20,1,"['sets', 'events']", Basic Properties of Measures,seg_3,1 ωn with ωn ∈ a and μ(ωn) < ∞ for all n. the sets a in the σ-field a are called events.
21,0,[], Basic Properties of Measures,seg_3,"1 an is in a. we will not, however, use the term “measure"
22,1,['sets'], Basic Properties of Measures,seg_3,"space” to describe such a triple. we will consider below measures on fields, on certain π̄- systems, and on some other collections of sets. a useful property of a collection of sets is that along with any sets a1, . . . , ak it also includes all sets of the type bk ≡ akack−1 · · · ac2ac1;"
23,0,[], Basic Properties of Measures,seg_3,"1bk is easier to work with.] (b) of less interest, call μ a finitely additive measure (abbreviated f.a.) on (ω,a) if"
24,1,['disjoint'], Basic Properties of Measures,seg_3,for all disjoint sequences ak in a for which ∑n
25,1,"['function', 'set']", Basic Properties of Measures,seg_3,"definition 1.3 (outer measures) consider a set function μ∗ : 2ω → [0, ∞]. (a) suppose that μ∗ also satisfies the following three properties. null: μ∗(∅) = 0. monotone: μ∗(a) ≤ μ∗(b) for all a ⊂ b. countable subadditivity: μ∗(⋃∞"
26,1,"['sets', 'test']", Basic Properties of Measures,seg_3,"sets t used in this capacity are called test sets. (c) we let a∗ denote the class of all μ∗-measurable sets, that is,"
27,1,['inequality'], Basic Properties of Measures,seg_3,"[note that a ∈ a∗ if and only if μ∗(t ) ≥ μ∗(ta) + μ∗(tac) for all t ⊂ ω, since the other inequality is trivial by the subadditivity of μ∗.]"
28,1,"['disjoint', 'intersection', 'intervals', 'cases', 'sets', 'set', 'union', 'limit']", Basic Properties of Measures,seg_3,"motivation 1.2 (measure) in this paragraph we will consider only one possible measure μ, namely the lebesgue-measure generalization of length. let ci denote the set of all intervals of the types (a, b], (−∞, b], and (a, +∞) on the real line r, and for each of these intervals i we assign a measure value μ(i) equal to its length, thus b − a,∞,∞ in the three special cases. all is well until we manipulate the sets in ci , as even the union of two elements in ci need not be in ci . thus, ci is not a very rich collection of sets. a natural extension is to let cf denote the collection of all finite disjoint unions of sets in ci , where the measure μ(a) we assign to each such set a is just the sum of the measures (lengths) of all its disjoint pieces. now cf is a field, and is thus closed under the elementary operations of union, intersection, and complementation. much can be done using only cf and letting “measure” be the “exact length.” but cf is not closed under passage to the limit, and it is thus insufficient for many of our needs. for this reason the concept of the smallest σ-field containing cf , labeled b ≡ σ[cf ], is introduced. we call b the borel sets. but let us work backwards. let us assign an outer measure value μ∗(a) to every subset a in the class 2r of all subsets of the real line r. in particular, to any subset a we assign the value μ∗(a) that is the infimum of all possible"
29,1,['set'], Basic Properties of Measures,seg_3,"natural upper bound to the measure (or generalized length) of the set a, and we will specify the infimum of such upper bounds to be the outer measure of a. thus to each subset a of the real line we assign a value μ∗(a) of generalized length. this value seems “reasonable,” but does it “perform correctly”? let us say that a particular set a is μ∗-measurable (that is, it “performs correctly”) if μ∗(t ) = μ∗(ta) + μ∗(tac) for all subsets t of the real line r—that is, if the a versus ac division of the line divides every subset t of the line into two"
30,1,"['combination', 'condition', 'sets']", Basic Properties of Measures,seg_3,"pieces in a fashion that is μ∗-additive. this is undoubtedly a combination of reasonableness and fine technicality that took some time to evolve in the mind of its creator, carathéodory, while he searched for a condition that “worked.” in what sense does it “work”? the collection a∗ of all μ∗-measurable sets turns out to be a σ-field. thus the collection a∗ is closed under all operations that we are likely to perform; and it is big enough, in that it is a σ-field that contains cf . thus we will work with the restriction μ∗|a∗ of μ∗ to the sets of a∗ (here, the vertical line means “restricted to”). this is enough to meet our needs (and it turns out to be exactly the maximal possible stopping point)."
31,1,"['interval', 'case', 'sets', 'associated', 'function']", Basic Properties of Measures,seg_3,"there are many measures other than length. for an ↗ and right-continuous function f on the real line (called a generalized df) we define the stieltjes measure of an arbitrary interval (a, b] (with −∞ ≤ a < b ≤ ∞) in ci by μf ((a, b]) = f (b) − f (a), and we extend it to sets in cf by adding up the measure of the pieces. reapplying the previous paragraph, we can extend μf to the μ∗f -measurable sets. it is the important carathéodory extension theorem that will establish that all stieltjes measures (including the case of ordinary length, where f (x) = x, as considered in the first paragraph) can be extended from cf to the borel sets b. that is, all borel sets are μ∗-measurable for every stieltjes measure. one further extension is possible, in that every measure can be completed” (see the end of section 1.2). we note here only that when the stieltjes measure μf associated with the generalized df f is “completed,” its domain of definition is extended from the borel sets b (which all stieltjes measures have in common) to a larger collection b̂μ that depends on the particular f. it is left to section"
32,1,['cases'], Basic Properties of Measures,seg_3,"f 1.2 to simply state that this is as far as we can go. that is, except in rather trivial special cases (especially, mass at only countably many points), we find that b̂μ is a proper subset"
33,0,[], Basic Properties of Measures,seg_3,"f of 2r. (otherwise, it is typically impossible to try to define the measure of all subsets of ω in a suitable fashion.)"
34,0,[], Basic Properties of Measures,seg_3,"example 1.1 (some examples of measures, informally)"
35,0,[], Basic Properties of Measures,seg_3,(a) lebesgue measure:
36,0,[], Basic Properties of Measures,seg_3,(b) counting measure:
37,1,['cardinality'], Basic Properties of Measures,seg_3,let #(a) denote the number of “points” in a (or the cardinality of a).
38,0,[], Basic Properties of Measures,seg_3,(∗) rigorous proof that these are measures will follow from theorem 1.3.1 below.
39,1,['sets'], Basic Properties of Measures,seg_3,example 1.2 (borel sets)
40,1,"['disjoint', 'intervals', 'sets']", Basic Properties of Measures,seg_3,"(a) let ω = r and let c consist of all finite disjoint unions of intervals of the types (a, b], (−∞, b], and (a, +∞). clearly, c is a field. then b ≡ σ[c] will be called the borel sets (or the borel subsets of r). let μ(a) be defined to be the sum of the lengths of the intervals composing a, for each a ∈ c. then μ is a (c.a.) measure on the field c, as will be seen in the proof of theorem 1.3.1 below."
41,1,['sets'], Basic Properties of Measures,seg_3,"(b) if (ω, d) is a metric space and u ≡ {all d-open subsets of ω}, then b ≡ σ[u ] will be called the borel sets or the borel σ-field."
42,1,['absolute value'], Basic Properties of Measures,seg_3,"(c) if (ω, d) is (r, | · |) for absolute value | · |, then σ[c] = σ[u ] even though c = u . (this claim is true, since c ⊂ σ[u ] and u ⊂ σ[c] are clear. then, use exercise 1.1.)"
43,1,['interval'], Basic Properties of Measures,seg_3,"(e) for any interval i ⊂ [−∞, ∞], let b̄i ≡ {b ∩ i : b ∈ b̄} = b̄ ∩ i."
44,0,['n'], Basic Properties of Measures,seg_3,"(letting ω denote the real line r, letting an = [n, ∞), and letting μ denote either lebesgue measure or counting measure, we see the need for some requirement.) (c) (countable subadditivity) whenever a1, a2, . . . and ∪∞1 an are all in a, then"
45,0,[], Basic Properties of Measures,seg_3,(d) all this also holds true for a measure on a field (via the same proofs).
46,1,"['loss', 'sets']", Basic Properties of Measures,seg_3,"(b) without loss of generality, redefine a1 = a2 = · · · = an0 . let bn ≡ a1\an so that the sets bn are ↗, with b1 = ∅. draw a picture of concentric circles of decreasing radaii to represent the general decreasing sets an. then locate the sets bn in this picture, and note that they are indeed increasing. it is pictorially clear that a1 ∩ (∩∞n=1an)c = ∪∞n=1bn. thus,"
47,0,[], Basic Properties of Measures,seg_3,"on the other hand,"
48,1,"['sets', 'disjoint']", Basic Properties of Measures,seg_3,"(c) let b1 ≡ a1, b2 ≡ a2ac1, . . . , bk ≡ akack−1 · · · ac1. then these newly defined sets bk are disjoint, and ∪nk=1ak = ∑k"
49,0,['n'], Basic Properties of Measures,seg_3,n =1bk. hence [a technique worth remembering]
50,1,[], Basic Properties of Measures,seg_3,1 μ(ak) by monotonicity.
51,1,['sets'], Basic Properties of Measures,seg_3,definition 1.4 (liminf and limsup of sets) let
52,1,['cases'], Basic Properties of Measures,seg_3,where we use a.b.f. to abbreviate in all but finitely many cases. let
53,0,[], Basic Properties of Measures,seg_3,where we use i.o. to abbreviate infinitely often.
54,0,[], Basic Properties of Measures,seg_3,(it is important to learn to read these two mathematical equations in a way that makes it clear that the verbal description is correct.) it is verbally trivial that we always have lim an ⊂ lim an. define
55,0,[], Basic Properties of Measures,seg_3,"we also let lim inf an ≡ lim an and lim supan ≡ lim an, giving us alternative notations."
56,0,[], Basic Properties of Measures,seg_3,"proposition 1.3 clearly, lim an equals ∪∞1 an when an is an ↗ sequence, and lim an equals ∩∞1 an when an is a ↘ sequence."
57,1,['condition'], Basic Properties of Measures,seg_3,"exercise 1.2 (a) now μ(lim infan) ≤ lim infμ(an) is always true. (b) also, lim supμ(an) ≤ μ(lim supan) holds if μ(ω) < ∞. (why the condition?)"
58,0,[], Basic Properties of Measures,seg_3,definition 1.5 (lim inf and lim sup of numbers) recall that for real number sequences an one defines lim an ≡ lim inf an and lim an ≡ lim sup an by
59,1,['limit'], Basic Properties of Measures,seg_3,"and these yield the smallest limit point and the largest limit point, respectively, of the sequence an."
60,1,"['functions', 'continuous', 'case', 'set', 'function']", Basic Properties of Measures,seg_3,"definition 1.6 (continuity of set functions) a set function μ defined on some class of subsets a of a non-void set ω is continuous from below (or, from above) if μ(lim an) = limμ(an) for all sequences an in ω that are ↗ to a set in a (or, for all sequences an in ω that are ↘ to a set in a, with at least one μ(an) finite). call μ continuous in case it is continuous both from below and from above. if limμ(an) = μ(a) whenever an ↘ a, then μ is said to be continuous from above at this particular a in a, etc."
61,0,[], Basic Properties of Measures,seg_3,the next result is often used in conjunction with the carathéodory extension theorem of the next section. view it as a converse to the proposition 1.2.
62,1,['continuous'], Basic Properties of Measures,seg_3,"proposition 1.4 (continuity of measures) if a finitely additive measure μ on either a field or σ-field is either continuous from below or has μ(ω) < ∞ and is continuous from above at ∅, then it is a countably additive measure."
63,1,['continuous'], Basic Properties of Measures,seg_3,proof. suppose first that μ is continuous from below. then
64,0,[], Basic Properties of Measures,seg_3,1ak) by continuity from below
65,0,[], Basic Properties of Measures,seg_3,giving the required countable additivity. thus μ is c.a. on a.
66,1,['continuous'], Basic Properties of Measures,seg_3,suppose next that μ is finite and is also continuous from above at ∅. then f.a. (even if a is only a field) gives
67,1,['inequality'], Basic Properties of Measures,seg_3,"this last notation allows us to string inequalities together linearly, instead of having to start a new inequality on a new line. (i use it often.)"
68,1,['intersection'], Basic Properties of Measures,seg_3,"exercise 1.3 (π-systems and λ-systems) consider a measurable space (ω, a). a class d of subsets is called a λ-system if it contains the space ω and all proper differences (a\b, when b ⊂ a with both a, b ∈ d) and if it is closed under monotone increasing limits. (recall that a class is called a π-system if it is closed under finite intersections, while π̄-systems are also required to contain ω.) (a) the minimal λ-system generated by a class c is denoted by λ[c]. show that λ[c] is equal to the intersection of all λ-systems containing c. (b) a collection a of subsets of ω is a σ-field if and only if it is both a π-system and a λ-system. (c) let c be a π-system and let d be a λ-system. then c ⊂ d implies that σ[c] ⊂ d. note (or, show) that this follows from (19) below."
69,0,[], Basic Properties of Measures,seg_3,"proof. (the author never includes this proof or the next one in lectures.) we first show that on any measurable space (ω, a) we have"
70,0,[], Basic Properties of Measures,seg_3,"to go the rest of the way, we define"
71,1,[], Basic Properties of Measures,seg_3,"that is, d is closed under intersections; and thus d is a π-system. thus (19) holds."
72,1,"['conditional probability', 'conditional', 'distribution', 'sets', 'probability distribution', 'independence', 'probability', 'conditional probability distribution']", Basic Properties of Measures,seg_3,the previous result is very useful in extending the verification of independence from small classes of sets to larger ones. the next proposition is used for both fubini’s theorem and the existence of a regular conditional probability distribution. it could also have been used below to give an alternate proof of uniqueness in the carathéodory extension theorem.
73,0,[], Basic Properties of Measures,seg_3,proposition 1.6 ∗(minimal monotone class; halmos)∗ the minimal monotone class m ≡ m[c] containing the field c and the minimal σ-field σ[c] generated by the same field c satisfy
74,0,[], Basic Properties of Measures,seg_3,"to show that m is a field, it suffices to show that"
75,1,[], Basic Properties of Measures,seg_3,"suppose that (a) has been established. we will now show that (a) implies that m is a field. complements: let a ∈ m, and note that ω ∈ m, since c ⊂ m. then a,ω ∈ m implies that ac = acω ∈ m by (a). unions: let a,b ∈ m. then a ∪ b = (ac ∩ bc)c ∈ m. thus m is indeed a field, provided that (a) is true. it thus suffices to prove (a)."
76,0,[], Basic Properties of Measures,seg_3,we first show that
77,1,"['set', 'limit']", Basic Properties of Measures,seg_3,"let bn be monotone in ma, with limit set b. since bn is monotone in ma, it is also monotone in m, and thus b ≡ limn bn ∈ m. since bn ∈ ma, we have abn ∈ m, and since abn is monotone in m, we have ab = limn abn ∈ m. in like fashion, acb and abc are in m. therefore, b ∈ ma, by definition of ma. that is, (c) holds."
78,0,[], Basic Properties of Measures,seg_3,we next show that
79,1,['symmetry'], Basic Properties of Measures,seg_3,"let a ∈ c and let c ∈ c. then a ∈ mc , since c is a field. but a ∈ mc if and only if c ∈ ma, by the symmetry of the definition of ma. thus c ⊂ ma. that is, c ⊂ ma ⊂ m, and ma is a monotone class by (c). but m is the minimal monotone class containing c, by the definition of m. thus (d) holds. but in fact, we shall now strengthen (d) to"
80,1,['symmetric'], Basic Properties of Measures,seg_3,"the conditions for membership in m imposed on pairs a,b are symmetric. thus for a ∈ c, the statement established above in (d) that b ∈ m(= ma) is true if and only if a ∈ mb . thus c ⊂ mb , where mb is a monotone class. thus mb = m, since (as was earlier noted) m is the smallest such monotone class. thus (e) (and hence (a)) is established."
81,0,[], Construction and Extension of Measures,seg_5,definition 2.1 (outer extension) let ω be arbitrary. let μ be a c.a. measure on a field c of subsets ω. for each a ∈ 2ω define
82,0,[], Construction and Extension of Measures,seg_5,"(this μ∗ will turn out to be an outer measure on the class 2ω.) now, μ∗ is called the outer extension of μ. the sequences a1, a2, . . . are called carathéodory coverings. (there is always at least one covering of every subset a, since ω ∈ c.)"
83,1,['sets'], Construction and Extension of Measures,seg_5,"theorem 2.1 (carathéodory extension theorem) let μ be a c.a. measure on a field c. then the μ∗ of (1) is a measure on the class a∗ of μ∗-measurable sets (as defined in (1.1.7)), and a∗ is necessarily a σ-field. moreover, σ[c] ⊂ a∗ the c.a. measure μ is extended from c to be a c.a. measure on a∗ simply by defining"
84,0,[], Construction and Extension of Measures,seg_5,"on c, corollary 3 and proposition 2.1 below will then imply that a∗ = âμ and that this is the largest possible σ-field to which μ can be uniquely extended."
85,1,[], Construction and Extension of Measures,seg_5,proof. the proof proceeds by a series of claims.
86,1,[], Construction and Extension of Measures,seg_5,[the choice of a convergent series (like /2n) that adds to is an important technique for the reader to learn.] now ∪nan ⊂ ∪n(∪kank). thus (since μ∗ is monotone)
87,1,['set'], Construction and Extension of Measures,seg_5,since the ank’s form a covering of the set ⋃n ⋃k ank
88,1,"['set', 'test']", Construction and Extension of Measures,seg_5,"and thus μ(c) ≤ μ∗(c). thus μ(c) = μ∗(c). we next show that any c ∈ c is also in a∗. let c ∈ c. let > 0, and let a test set t be given. there exists a covering {an}∞1 ⊂ c of t such that"
89,1,"['symmetric', 'set', 'test', 'inequality']", Construction and Extension of Measures,seg_5,"claim 3: the class a∗ of μ∗-measurable subsets of ω is a field that contains c. now, a ∈ a∗ implies that ac ∈ a∗: the definition of μ∗-measurable is symmetric in a and ac. and a,b ∈ a∗ implies that ab ∈ a∗: for any test set t ⊂ ω we have the required inequality"
90,1,"['set', 'test']", Construction and Extension of Measures,seg_5,since b ∈ a∗ with test set ta and with test set tac
91,1,['inequality'], Construction and Extension of Measures,seg_5,"since μ∗ is countably subadditive. as the reverse inequality is trivial,"
92,1,['disjoint'], Construction and Extension of Measures,seg_5,"claim 4: μ∗ is a f.a. measure on a∗. let a,b ∈ a∗ be disjoint. finite additivity follows from"
93,1,"['set', 'test']", Construction and Extension of Measures,seg_5,since a ∈ a∗ with test set a + b
94,1,['sets'], Construction and Extension of Measures,seg_5,"trivially, μ∗(a) ≥ 0 for all sets a. and μ∗(∅) = 0 was shown in the first claim."
95,1,['inequality'], Construction and Extension of Measures,seg_5,"1 an in (f) to get μ∗(a) ≥ ∑∞ 1 μ∗(an), and then countable subadditivity gives the reverse inequality."
96,0,[], Construction and Extension of Measures,seg_5,"claim 7: when μ is a finite measure, its extension μ∗ to a∗ is unique. let ν denote any other extension of μ to a∗. let a in a∗. for any carathéodory covering a1, a2, . . . of a (with the an’s in c), countable subadditivity gives"
97,0,[], Construction and Extension of Measures,seg_5,"(where all four of these terms are finite), we can infer from (i) that"
98,1,['sets'], Construction and Extension of Measures,seg_5,"claim 8: uniqueness of μ∗ on a∗ also holds when μ is a σ-finite measure on c. label the sets of the measurable partition as dn, and let ωn ≡ ∑n"
99,0,[], Construction and Extension of Measures,seg_5,"completing the proof. in fact, the following corollary was established."
100,1,['sets'], Construction and Extension of Measures,seg_5,"corollary 1 the μ∗-measurable sets a∗ of (1.1.7) were shown to contain σ[c]. thus we can now view the c.a. measure μ on the field c has having been extended uniquely to the (useful and intelligible) σ-field σ[c], or as having been extended to the (less intelligible, but at least as large) σ-field a∗. (we seek clarity.)"
101,1,[], Construction and Extension of Measures,seg_5,"questions when we extended our measure μ from the field c to the σ-field a∗, did we actually go beyond σ[c]? can we go further? corollary 2 and corollary 3 will show that we can always “complete” μ on σ[c], and in so doing extend it to (an obviously useful) σ-field named âμ, that is still contained in a∗. proposition 2.1 will show that there are definite limitations to extension (the most famous example being the “lebesgue sets” of proposition 2.3). in doing so, proposition 2.1 will imply that a∗ = âμ, which now gives us a very useful interpretation of a∗."
102,1,"['sets', 'set', 'null set']", Construction and Extension of Measures,seg_5,"definition 2.2 (complete measures) let (ω, a, μ) denote a measure space. if μ(a) = 0, then a is called a null set. we call (ω, a, μ) complete if whenever we have a ⊂ (some b) ∈ a with μ(b) = 0, we necessarily also have a ∈ a. (that is, all subsets of sets of measure 0 are required to be measurable.)"
103,1,['sets'], Construction and Extension of Measures,seg_5,"definition 2.3 (lebesgue sets) the completion of lebesgue measure on (r, b, λ) is still called lebesgue measure, and it is still denoted by λ. the resulting completed σ-field b̂λ of the borel sets b is called the lebesgue sets."
104,0,[], Construction and Extension of Measures,seg_5,"corollary 2 when we complete a measure μ on a σ-field a, this completed measure μ̂ is the unique extension of μ to âμ. (it is typical to denote such extensions by μ also (rather than by μ̂), and to always make such extensions automatically.)"
105,1,['sets'], Construction and Extension of Measures,seg_5,"corollary 3 (thus when we begin with a σ-finite measure μ on a field c, both the extension to a ≡ σ[c] and the further extension to âμ ≡ σ̂[c]μ are unique.) here, we note that all sets"
106,1,['sets'], Construction and Extension of Measures,seg_5,in âμ = σ̂[c]μ are in the class a∗ of μ∗-measurable sets. (proposition 2.1 below will imply
107,0,[], Construction and Extension of Measures,seg_5,proof. consider corollary 2 first. let ν denote any extension to âμ. we will demonstrate that
108,1,"['sets', 'null sets']", Construction and Extension of Measures,seg_5,"(a) ν(a ∪ n) = μ(a) for all a ∈ a, and all null sets n"
109,1,['sets'], Construction and Extension of Measures,seg_5,"(that is, ν = μ̂). assume not. then there exist sets a ∈ a and n ⊂ (some b) in a with μ(b) = 0 such that ν(a ∪ n) > μ(a) (necessarily, ν(a ∪ n) ≥ ν(a) = μ(a)). for this a and n we have"
110,0,[], Construction and Extension of Measures,seg_5,since ν is a measure on the completion
111,0,[], Construction and Extension of Measures,seg_5,"hence μ(b) > 0, which is a contradiction. thus the extension is unique."
112,0,['n'], Construction and Extension of Measures,seg_5,"we now turn to corollary 3. only the final claim needs demonstrating. suppose a is in σ̂[c]μ. then a = a′ ∪ n for some a′ ∈ a and some n satisfying n ⊂ b with μ(b) = 0. since a∗ is a σ-field, it suffices to show that any such n is in a∗. since μ∗ is subadditive and monotone, we have"
113,0,['n'], Construction and Extension of Measures,seg_5,"because μ∗(tn) = 0 follows from using b, ∅ ∅, . . . to cover tn . thus equality holds in this last equation, showing that n is μ∗-measurable."
114,1,"['sets', 'null sets']", Construction and Extension of Measures,seg_5,(b) prove or disprove each half: âμ = âν iff μ and ν have identical null sets.
115,1,"['disjoint', 'approximation', 'sets', 'disjoint sets']", Construction and Extension of Measures,seg_5,"exercise 2.3 (a) replace the sets an in the field c used in definition (1) by the disjoint sets bn ≡ anacn−1 · · · ac2ac1 (which are also in c), to show that this definition could have insisted on using disjoint sets an. (b) (approximation lemma; halmos) let the σ-finite measure μ on the field c be extended to a = σ[c]; also refer to this extension as μ. show that for each a ∈ a (or âμ) having μ(a) < ∞, and for each > 0,"
116,1,['set'], Construction and Extension of Measures,seg_5,(8) μ(a c) < for some set c ∈ c.
117,1,['set'], Construction and Extension of Measures,seg_5,"(hint. truncate the sum in (1.2.1) to define c.) (c) if μ(a) = ∞, then (8) can fail, even in a simple case—with a bad choice for c. let μ denote counting measure on the integers. now c ≡ {c : c or cc is finite} is a field. determine σ[c]. show that (8) fails for the set a of even integers."
118,1,"['contrast', 'sets', 'set']", Construction and Extension of Measures,seg_5,"definition 2.4 (regular measures on metric spaces) let d denote a metric on ω, let a denote the borel sets, and let μ be a measure on (ω, a). suppose that for each set a in âμ, and for every > 0, one can find an open set o and a closed set c for which both c ⊂ a ⊂ o and μ(o \c ) < . suppose also that if μ(a) < ∞, one then requires that the set c be compact. then μ is called a regular measure. (note exercise 1.3.1 below. contrast its content with (8).)"
119,1,['sets'], Construction and Extension of Measures,seg_5,"exercise 2.4 (nonmeasurable sets) let ω consist of the sixteen values 1, . . . , 16. (think of them arranged in four rows of four values.) let"
120,1,['sets'], Construction and Extension of Measures,seg_5,"let c denote the field generated by {c1, c2, c3, c4}, and let a = σ[c]. (a) show that a ≡ σ[c] = 2ω. (note that 2ω contains 216 = 65,536 sets.)"
121,1,['sets'], Construction and Extension of Measures,seg_5,"2 , 1 ≤ i ≤ 4, with μ(c1c3) = 1 4 . show âμ = a, with 24 = 16 sets."
122,1,['sets'], Construction and Extension of Measures,seg_5,"1 , i = 2, 3, 4, with μ(c2c4) = 0. show that âμ has 210 = 1024 sets. (d) illustrate proposition 2.1 below in the context of this exercise."
123,1,['sets'], Construction and Extension of Measures,seg_5,"proposition 2.1 (not all sets need be measurable) let μ be a measure on a ≡ σ[c], with c a field. if b ∈/ âμ, then there are infinitely many measures on σ[âμ ∪ {b}] that agree with"
124,1,"['process', 'observation']", Construction and Extension of Measures,seg_5,"μ on c. (thus the σ-field âμ is as far as we can go with the unique extension process. we merely state this observation for reference, without proof. note exercise 2.4d.)"
125,1,['sets'], Construction and Extension of Measures,seg_5,proposition 2.2 (not all subsets are lebesgue sets) there is a subset d of r that is not in the lebesgue sets b̂λ. (this will require the axiom of choice.)
126,1,"['disjoint', 'sets', 'set', 'union', 'equivalence relation', 'disjoint sets']", Construction and Extension of Measures,seg_5,"proof. define the equivalence relation ∼ on elements of [0, 1) by x ∼ y if x−y is a rational number. use the axiom of choice to specify a set d that contains exactly one element from each equivalence class. now define dz ≡ {z + x (modulo 1) : x ∈ d} for each rational z in [0, 1), so that [0, 1) = ∑zdz represents [0, 1) as a countable union of disjoint sets. moreover, all dz must have the same outer measure; call it a. assume d = d0 is measurable. but then 1 = λ([0, 1)) = ∑z λ(dz) = ∑z a gives only ∑z a = 0 (when a = 0) and ∑z a = ∞ (when"
127,0,[], Construction and Extension of Measures,seg_5,a > 0) as possibilities. this is a contradiction. thus d ∈/ b̂λ.
128,1,"['sets', 'disjoint', 'disjoint sets']", Construction and Extension of Measures,seg_5,"exercise 2.5 just understand the sketch above, noting that d = ∑z dz (with disjoint sets dz for the rationals z)."
129,1,"['sets', 'set']", Construction and Extension of Measures,seg_5,proposition 2.3 (not all lebesgue sets are borel sets) there necessarily exists a set a ∈ b̂λ\b that is a lebesgue set but not a borel set.
130,0,[], Construction and Extension of Measures,seg_5,proof. this proof follows exercise 6.3.4 below; it requires the axiom of choice.
131,1,['set'], Construction and Extension of Measures,seg_5,exercise 2.6 every subset a of ω having μ∗(a) = 0 is a μ∗-measurable set.
132,0,[], Construction and Extension of Measures,seg_5,exercise 2.7 ∗ show that the carathéodory theorem can fail if μ is not σ-finite.
133,0,[], Construction and Extension of Measures,seg_5,earlier in this section we encountered carathéodory coverings.
134,1,"['disjoint', 'interval', 'intervals', 'set']", Construction and Extension of Measures,seg_5,"exercise 2.8 ∗ (vitali covering) (a) we say that a family v of intervals i is a vitali covering of a set d if for each x ∈ d and each > 0 there exists an interval i ∈ v for which x ∈ i and λ(i) < . (b) (vitali covering theorem) let d ⊂ r have outer lebesgue measure λ∗(d) < ∞. let v be a collection of closed intervals that forms a vitali covering of d. then there exists a finite number of pairwise disjoint intervals (i1, . . . , im) in v whose lebesgue outer measure λ∗ satisfies"
135,1,"['functions', 'set']", Construction and Extension of Measures,seg_5,"(compare this “nice approximation” of a set to the nice approximations given in exercise 2.3 and in definition 2.4.) [lebesgue measure λ will be formally shown to exist in the next section, and λ∗ will be discussed more fully.] [result (9) will be useful in establishing the lebesgue result that increasing functions on r necessarily have a derivative, except perhaps on a set having lebesgue measure zero.]"
136,1,['sets'], Construction and Extension of Measures,seg_5,"exercise 2.9 ∗ (heine–borel) if {ut : t ∈ t} is an arbitrary collection of open sets that covers a compact subset d of r, then there exists a finite number of them u1, . . . , um that also covers d."
137,1,['contrast'], Construction and Extension of Measures,seg_5,the familiar heine–borel result will be frequently used. it is stated here only to contrast it with the important new ideas of carathéodory and vitali coverings.
138,1,"['probability theory', 'probability', 'moment']", LebesgueStieltjes Measures,seg_7,at the moment we know only a few measures informally. we now construct the large class of measures that lies at the heart of probability theory.
139,1,"['set', 'intervals']", LebesgueStieltjes Measures,seg_7,"definition 3.1 (lebesgue–stieltjes measure) a measure μ on the real line r assigning finite values to finite intervals is called a lebesgue–stieltjes measure. (the measure μ on (r, 2r) whose value μ(a) for any set a equals the number of rationals in a is not a lebesgue–stieltjes measure.)"
140,1,"['mass function', 'function']", LebesgueStieltjes Measures,seg_7,definition 3.2 (gdf) a finite ↗ function f on r that is right-continuous is called a generalized df (to be abbreviated gdf). then f (·) ≡ limy↗ .f (y) denotes the left-continuous version of f. the mass function of f is defined by
141,1,"['function', 'representative']", LebesgueStieltjes Measures,seg_7,"is called the increment function of f. identify gdfs having the same increment function. only one member f of each equivalence class so obtained satisfies f−(0) = 0, and this f can (and occasionally will) be used as the representative member of the class (also to be called the representative gdf)."
142,1,['function'], LebesgueStieltjes Measures,seg_7,"example 3.1 we earlier defined three measures on (r,b) informally. (a) for lebesgue measure λ, a gdf is the identity function f (x) = x. (b) for counting measure, a gdf is the greatest integer function f (x) = [x]. (c) for unit point mass at x0, a gdf is f (x) = 1[x0,∞)(x)."
143,0,[], LebesgueStieltjes Measures,seg_7,theorem 3.1 (correspondence theorem; loève) the relationship
144,1,"['representative', 'set']", LebesgueStieltjes Measures,seg_7,"establishes a 1-to-1 correspondence between the lebesgue–stieltjes measures μ on b and the set of representative members of the equivalence classes of generalized dfs. (each such μ extends uniquely to b̂μ−but it is still labeled as μ, not μ̂.)"
145,1,['sets'], LebesgueStieltjes Measures,seg_7,notation 3.1 we formally establish some notation that will be used throughout. important classes of sets include:
146,1,['intervals'], LebesgueStieltjes Measures,seg_7,"(2) ci ≡ {all intervals (a, b], (−∞, b], or (a, +∞) : −∞ < a < b < +∞}."
147,1,"['disjoint', 'intervals']", LebesgueStieltjes Measures,seg_7,(3) cf ≡ {all finite disjoint unions of intervals in ci} = (a field).
148,1,['sets'], LebesgueStieltjes Measures,seg_7,(4) b ≡ σ[cf ] ≡ (the σ-field of borel sets).
149,1,"['function', 'representative']", LebesgueStieltjes Measures,seg_7,"proof. given a ls-measure μ, define the increment function f (a, b] via (1). we clearly have 0 ≤ f (a, b] < ∞ for all finite a, b, and f (a, b] → 0 as b ↘ a, by proposition 1.1.2. now specify f−(0) ≡ 0, f (0) ≡ μ({0}), f (b) ≡ f (0)+f (0, b] for b > 0, and f (a) = f (0)−f (a, 0] for a < 0. this f (·) is the representative gdf."
150,1,"['representative', 'intervals']", LebesgueStieltjes Measures,seg_7,"given a representative gdf, we define μ on the collection i of all finite intervals (a, b] via (1). we will now show that μ is a well-defined and c.a. measure on this collection i of finite intervals."
151,0,[], LebesgueStieltjes Measures,seg_7,"first, we will show that ∑1"
152,1,['intervals'], LebesgueStieltjes Measures,seg_7,"necessary, so that i1, . . . , in is a left-to-right ordering of these intervals)"
153,1,['case'], LebesgueStieltjes Measures,seg_7,"∞μ(ik). suppose b−a > > 0 (the case b−a = 0 is trivial, as μ(∅) = 0). fix θ > 0. for each k ≥ 1, use the right continuity of f to choose an k > 0 so small that"
154,1,"['interval', 'intervals']", LebesgueStieltjes Measures,seg_7,"these jk form an open cover of the compact interval [a + , b], so that some finite number of them are known to cover [a + , b], by the heine–borel theorem. sorting through these intervals one at a time, choose (a1, c1) to contain b, choose (a2, c2) to contain a1, choose (a3, c3) to contain a2, . . . ; finally (for some k), choose (ak , ck) to contain a + . then (relabeling the subscripts, if necessary)"
155,0,['n'], LebesgueStieltjes Measures,seg_7,"′ , then we must show (where the subscripts m and n could take on either a finite or a countably infinite number of values) that"
156,1,['disjoint'], LebesgueStieltjes Measures,seg_7,"the c.a. of μ on cf is then trivial; if disjoint an = ∑minm for each n, it then follows that"
157,0,[], LebesgueStieltjes Measures,seg_7,"finally, a measure μ on cf determines a unique measure on b, as is guaranteed by the carathéodory extension of theorem 1.2.1."
158,1,['associated'], LebesgueStieltjes Measures,seg_7,"exercise 3.1 (calculating lebesgue–stieltjes measure) (i) consider a gdf f on r and its associated lebesgue–stieltjes measure μf on (r, cf ), for the field cf of (3). show that we can replace the definition of the outer extension μ∗f to the borel σ-field b in the carethéodory fashion of (1.3.1) by"
159,1,['disjoint'], LebesgueStieltjes Measures,seg_7,"for countable unions of disjoint subintervals (cn, dn] of r̄. show also that"
160,0,[], LebesgueStieltjes Measures,seg_7,(this gives a very concrete visualization of how the μf measure of an arbitrary subset of r is formed–and this approach is the same for every lebesgue–stieltjes measure μf on r.) (ii) let i denote any subinterval of the real line r. let f denote any fixed gdf on i. (now
161,1,"['disjoint', 'intervals']", LebesgueStieltjes Measures,seg_7,"−x f (x) = x is a gdf on r itself, f (x) = 1 − e is a gdf on [0, ∞), and f (x) = 1/(1 − x) − 1 is a gdf on [0,1).) define μf ((a, b]) = f (b) − f (a) for all a ≤ b with a and b in ī. let bi ≡ {b ∩ i : b ∈ b}. it is still true that the right hand side of (7) gives the value of μf (a) for every a in the completion of the σ-field bi . that is, taking the infimum over disjoint intervals of the type (a, b] is suffficient."
162,1,"['continuous', 'interval']", LebesgueStieltjes Measures,seg_7,definition 3.3 (absolutely continuous dfs) say that a gdf f on an interval i is absolutely continuous if for all > 0 there exists a δ > 0 for which
163,1,"['continuous', 'disjoint']", LebesgueStieltjes Measures,seg_7,"whenever n ≥ 1 and all of the intevals (ck, dk] are mutually disjoint subintervals of i. (note that the first and third dfs in part (ii) of the previous exercise are absolutely continuous, while the third one is continuous but not even uniformly continuous.)"
164,1,['intervals'], LebesgueStieltjes Measures,seg_7,"exercise 3.2 (all ls-measures on (r,b) are regular) show that all lebesgue–stieltjes measures on (r,b) are regular measures (see definition 1.2.4). (use the open intervals jn in the theorem 3.1 proof.)"
165,1,['probability'], LebesgueStieltjes Measures,seg_7,"probability measures, probability spaces, and dfs"
166,1,"['distribution function', 'set', 'probability', 'function', 'sample', 'experiment', 'probability distributions', 'probability theory', 'events', 'event', 'sample space', 'distributions', 'distribution', 'probability measure', 'outcomes', 'probability measures', 'representative']", LebesgueStieltjes Measures,seg_7,"definition 3.4 (probability distributions p (·) and dfs f (·)) (a) in probability theory we think of ω as the set of all possible outcomes of some experiment, and we refer to it as the sample space. the individual points ω in ω are referred to as the elementary outcomes. the measurable subsets a in the collection a are referred to as events. a measure of interest is now denoted by p ; it is called a probability measure, and must satisfy p (ω) = 1. we refer to p (a) as the probability of a, for each event a in âp . the triple (ω,a, p ) (or (ω, âp , p̂ ), if this is different) is referred to as a probability space. (b) an ↗ right-continuous function f on r having f (−∞) ≡ limx→−∞ f (x) = 0 and f (+∞) ≡ limx→+∞ f (x) = 1 is called a distribution function (which we will abbreviate as df). (for probability measures, setting f (−∞) = 0 is used to specify the representative df.)"
167,1,"['probability distributions', 'intervals', 'probability', 'distributions']", LebesgueStieltjes Measures,seg_7,"corollary 1 (the correspondence theorem for dfs) defining p (·) on all intervals (a, b] via p ((a, b]) ≡ f (b) − f (a) for all −∞ ≤ a < b ≤ +∞ establishes a 1-to-1 correspondence between probability distributions p (·) on (r,b) and dfs f (·) on r."
168,0,[], LebesgueStieltjes Measures,seg_7,exercise 3.3 prove this trivial corollary.
169,1,"['functions', 'density function', 'continuous', 'probability distributions', 'density functions', 'set', 'probability', 'function', 'distributions']", LebesgueStieltjes Measures,seg_7,"remark 3.1 (density functions) those probability distributions that have an absolutely continuous df will turn out be be exactly those probablity distributions that have a “density function” f. moreover, f ′ will always exist for any df f (except perhaps on a set of lebesgue measure 0), but it will serve as a density function only for the absolutely continuous dfs. moreover, in chapter 4 we will also learn to think of a “probability mass function” as a density with respect to counting measure."
170,1,"['function', 'set']", Mappings and σFields,seg_11,"notation 1.1 (inverse images) suppose x denotes a function mapping some set ω into the extended real line r̄ ≡ r∪{±∞}; we denote this by x : ω → r̄. let x+ and x− denote the positive part and the negative part of x, respectively:"
171,0,[], Mappings and σFields,seg_11,we also use the following notation:
172,1,['sets'], Mappings and σFields,seg_11,"(5) [x ∈ b] ≡ x−1(b) ≡ {ω : x(ω) ∈ b} for all borel sets b,"
173,0,[], Mappings and σFields,seg_11,"we call these the inverse images of r,b, and b, respectively. we let"
174,1,['sets'], Mappings and σFields,seg_11,inverse images are also well-defined when x : ω → ω′ for arbitrary sets ω and ω′.
175,0,[], Mappings and σFields,seg_11,and we will also reintroduce this sup norm in other contexts below.
176,1,['set'], Mappings and σFields,seg_11,"proposition 1.1 (basics of inverse images) let x : ω → ω′ and y : ω′ → ω′′. let t denote an arbitrary index set. then for all a,b,at ⊂ ω′ we have"
177,1,['sets'], Mappings and σFields,seg_11,"for all sets a ⊂ ω′′, the composition y ◦ x satisfies"
178,0,[], Mappings and σFields,seg_11,proof. trivial.
179,0,[], Mappings and σFields,seg_11,this gives (14). consider (13). using (12) gives
180,1,['sets'], Mappings and σFields,seg_11,"roughly, using (12) we will restrict x so that f(x) ≡ x−1(b̄) ⊂ a for our original (ω,a, μ), so that we can then “induce” a measure on (r̄, b̄). or, (14) tells us that the collection a′ is such that we can always induce a measure on (ω′,a′). we do this in the next section. first, we generalize our definition of borel sets to n dimensions."
181,0,[], Mappings and σFields,seg_11,example 1.1 (euclidean space) let
182,1,[], Mappings and σFields,seg_11,"let un denote all open subsets of rn, in the usual euclidean metric. then"
183,1,['sets'], Mappings and σFields,seg_11,(15) bn ≡ σ[un] is called the class of borel sets of rn.
184,1,['sets'], Mappings and σFields,seg_11,"note that the three σ-fields of (15), (16), and (17) are equal. just observe that each of these three classes generates the generators of the other two classes, and apply exercise 1.1.1. (surely, we can define a generalization of area λ2 on (r2,b2) by beginning with λ2(b1×b2) = λ(b1) × λ(b2) for all b1 and b2 in b, and then extending to all sets in b2. we will do this in theorem 5.1.1, and we will call it lebesgue measure on two-dimensional euclidean space. this clearly extends to λn on (rn,bn).)"
185,1,"['functions', 'limit', 'standard']", Measurable Functions,seg_13,"we seek a large usable class of functions that is closed under passage to the limit. this is the fundamental property of the class of measurable functions. propositions 2.2 and 2.3 below will show that the class of measurable functions is also closed under all of the standard mathematical operations. thus, this class is sufficient for our needs."
186,1,"['functions', 'indicator function', 'set', 'function', 'indicator']", Measurable Functions,seg_13,"definition 2.1 (simple functions, etc.) let the measure space (ω,a, μ) be given and fixed throughout our discussion. consider the following classes of functions. the indicator function 1a(·) of the set a ⊂ ω is defined by"
187,1,['function'], Measurable Functions,seg_13,a simple function is of the form
188,1,['function'], Measurable Functions,seg_13,an elementary function is of the form
189,1,['case'], Measurable Functions,seg_13,"(or even x : (ω,a, μ) → (ω′,a′, μ′) for the measure μ′ “induced” on (ω′,a′) by the mapping x, as will soon be defined). in the special case x : (ω,a) → (r̄, b̄), we simply call x measurable; and in this special case we let f(x) ≡ x−1(b̄) denote the sub σ-field of a generated by x."
190,0,[], Measurable Functions,seg_13,proposition 2.1 (measurability criteria) let x : ω → r̄. suppose σ[c] = b̄. then measurability can be characterized by either of the following:
191,0,[], Measurable Functions,seg_13,"the other direction is trivial. thus (5) holds. to demonstrate (6), we need to show that b satisfies"
192,0,[], Measurable Functions,seg_13,the equality (c) is obvious. the rest is trivial.
193,1,"['function', 'functions', 'continuous']", Measurable Functions,seg_13,"proposition 2.2 (measurability of common functions) let x,y , and xn’s be measurable functions. consider cx with c > 0,−x, inf xn, supxn, lim inf xn, lim supxn, lim xn if it exists, x2,x ± y if it is well-defined, xy where 0 · ∞ ≡ 0,x/y if it is welldefined, x+,x−, |x|, and the composite g(x) for a continuous g and for any measurable function g. all of these are measurable functions."
194,1,['functions'], Measurable Functions,seg_13,proposition 2.3 (measurability via simple functions)
195,1,['functions'], Measurable Functions,seg_13,(7) simple and elementary functions are measurable.
196,1,"['functions', 'limit']", Measurable Functions,seg_13,(8) x is the limit of a sequence of simple functions.
197,1,"['functions', 'limit']", Measurable Functions,seg_13,(9) the limit of a sequence of simple functions that are ≥ 0 and ↗ .
198,1,['functions'], Measurable Functions,seg_13,"proof. the functions in proposition 2.2 are measurable, as is now shown."
199,1,['sets'], Measurable Functions,seg_13,"each of the sets where x or y equals 0,∞, or −∞ is measurable; use this below."
200,1,['set'], Measurable Functions,seg_13,"(f) [x > y ] = ⋃r{ x > r > y : r is rational}, so [x > y ] is a measurable set."
201,1,['case'], Measurable Functions,seg_13,"since [1/y < x] = [y > 1/x] for x > 0 in case y > 0, and for general y one can write"
202,1,"['functions', 'indicator']", Measurable Functions,seg_13,1 y 1[y <0] with the two indicator functions measurable.
203,1,['continuous'], Measurable Functions,seg_13,"for g measurable, (g ◦ x)−1(b̄) = x−1(g−1(b̄)) ⊂ x−1(b̄) ⊂ a. then continuous g are measurable, since"
204,1,['sets'], Measurable Functions,seg_13,"(j) g−1(b) = g−1(σ[open sets]) = σ[g−1(open sets)] ⊂ σ[open sets] ⊂ b̄, and both g−1({+∞}) and g−1({−∞}) are a (possibly void) subset of {−∞,+∞}. now apply the result for measurable g."
205,1,['functions'], Measurable Functions,seg_13,we now prove proposition 2.3. claim (7) is trivial. consider (8). define simple functions xn by
206,0,[], Measurable Functions,seg_13,"also, the nested subdivisions k/2n cause xn to satisfy"
207,0,[], Measurable Functions,seg_13,we extend proposition 2.3 slightly by further observing that
208,1,['functions'], Measurable Functions,seg_13,"also, the elementary functions"
209,0,[], Measurable Functions,seg_13,are always such that
210,1,"['set', 'function']", Measurable Functions,seg_13,"proposition 2.4 (the discontinuity set is measurable; billingsley) if (m,d) and (m ′, d′) are metric spaces and ψ : m → m ′ is any function (not necessarily a measurable function), then the discontinuity set of ψ defined by"
211,1,['continuous'], Measurable Functions,seg_13,(14) dψ ≡ {x ∈ m : ψ is not continuous at x}
212,0,[], Measurable Functions,seg_13,"is necessarily in the borel σ-field bd (that is, the σ-field generated by the d-open subsets of m)."
213,1,['set'], Measurable Functions,seg_13,"note that a ,δ is an open set, since {u ∈ m : d(x, u) < δ0} ⊂ a ,δ will necessarily occur if δ0 ≡ {δ − [d(x, y) ∨ d(x, z)]}/2; that is, the y and z that work for x also work for all u in m that are sufficiently close to x. (note: the y that worked for x may have been x itself.) then"
214,1,['set'], Measurable Functions,seg_13,"where 1, 2, . . . and δ1, δ2, . . . both denote the positive rationals, since each a ,δ is an open set."
215,1,"['transformation', 'probability measure', 'probability']", Measurable Functions,seg_13,"thus if μ is a probability measure, then so is μx ≡ μ′. note also that we could regard x as an a′-f(x)-measurable transformation from the measure space (ω,f(x), μ) to (ω′,a′, μx)."
216,1,['associated'], Measurable Functions,seg_13,"suppose further that f is a generalized df on the real line r, and that μf (·) is the associated measure on (r,b) satisfying μf ((a, b]) = f (b) − f (a) for all a and b (as was guaranteed by the correspondence theorem (theorem 1.3.1)). thus (r,b, μf ) is a measure space. define"
217,1,"['function', 'transformation']", Measurable Functions,seg_13,"then x is a measurable transformation from (r,b, μf ) to (r,b) whose induced measure μx is equal to μf . thus for any given df f we can always construct a measurable function x whose df is f."
218,1,"['transformations', 'sets', 'geometric']", Measurable Functions,seg_13,"exercise 2.1 suppose (ω,a) = (r2,b2), where b2 denotes the σ-field generated by all open subsets of the plane. recall that this σ-field contains all sets b × r and r × b for all b ∈ b; here b1 × b2 ≡ {(r1, r2) : r1 ∈ b1, r2 ∈ b2}. now define measurable transformations x1((r1, r2)) = r1 and x2(r1, r2)) = r2. then define z1 ≡ (x12 + x22)1/2 and z2 ≡ sign(x1 − x2), where sign(r) equals 1, 0,−1 according as r is > 0,= 0, < 0. the exercise is to give geometric descriptions of the σ-fields f(z1),f(z2), and f(z1, z2)."
219,1,['function'], Measurable Functions,seg_13,"proposition 2.5 (the form of an f(z)-measurable function) suppose that z is a measurable function on (ω,a) and that y is f(z)-measurable. then there must exist a measurable function g on (r̄, b̄) such that y = g(z)."
220,1,"['functions', 'indicator function', 'set', 'function', 'indicator']", Measurable Functions,seg_13,"proof. (the approach of this proof is to consider indicator functions, simple functions, nonnegative functions, general functions. this approach will be used again and again. learn it!) suppose that y = 1d for some set d ∈ f(z), so that y is an indicator function that is f(z)-measurable. then we can rewrite y as y = 1d = 1z−1(b) = 1b(z) ≡ g(z), for some b ∈ b̄ that depends on d, where g(r) ≡ 1b(r). thus the proposition holds for indicator functions. it holds for simple functions, since when all bi ∈ b̄,"
221,1,['functions'], Measurable Functions,seg_13,"let y ≥ 0 be f(z)-measurable. then there do exist ↗simple f(z)-measurable functions yn such that y ≡ limn yn = limn gn(z) for the ↗simple b̄-measurable functions gn. now let g = lim gn, which is b̄-measurable, and note that y = g(z). for general y = y + − y −, use g = g+ − g−."
222,1,['functions'], Measurable Functions,seg_13,"exercise 2.2 (measurability criterion) let c denote a π̄-system of subsets of ω. let v denote a vector space of functions; that is, x + y ∈ v and αx ∈ v for all x,y ∈ v and all α ∈ r—and, all the usual elementary facts hold."
223,1,['function'], Measurable Functions,seg_13,(b) it then follows trivially that every simple function
224,1,['functions'], Measurable Functions,seg_13,(c) now suppose further that xn ↗ x for xn’s as in (19) implies that x ∈ v. show that v contains all σ[c]-measurable functions.
225,1,"['functions', 'cauchy']", Convergence,seg_15,"definition 3.1 (→a.e.) let x1,x2, . . . denote measurable functions on (ω,a, μ) to (r̄, b̄). say that the sequence xn converges almost everywhere to x (denoted by xn →a.e. x as n → ∞) if for some n ∈ a for which μ(n) = 0 we have xn(ω) → x(ω) as n → ∞ for all ω ∈/ n . if for all ω ∈/ n the sequence xn(ω) is a cauchy sequence, then we say that the sequence xn mutually converges a.e. and denote this by writing xn − xm →a.e. 0 as m ∧ n → ∞. (here, m ∧ n ≡ min(m,n).)"
226,1,['functions'], Convergence,seg_15,"exercise 3.1 let x1,x2, . . . be measurable functions from (ω,a, μ) to (r̄, b̄)."
227,1,"['functions', 'null sets', 'sets', 'function']", Convergence,seg_15,proposition 3.1 a sequence of measurable functions xn that are a.e. finite converges a.e. to a measurable function x that is a.e. finite if and only if these functions xn converges mutually a.e. (thus we can redefine such functions on null sets and make them everywhere finite and everywhere convergent and/or follow the convention of corollary 2 to the carethéodory theorem 1.2.1 and automatically complete every measure.)
228,1,"['null sets', 'sets', 'set', 'null set', 'union', 'convergence']", Convergence,seg_15,"proof. the union of the countable number of null sets on which finiteness or convergence fails is again a null set n . on n c, the claim is just a property of the real numbers."
229,1,"['functions', 'null sets', 'sets', 'set', 'convergence']", Convergence,seg_15,"proposition 3.2 (the convergence and divergence sets are measurable) consider the finite measurable functions x,x1,x2, . . . (perhaps redefined on null sets to achieve this); thus, they are b-a-measurable. then the convergence and mutual convergence sets are measurable. in fact, the convergence set is given by"
230,1,"['convergence', 'set']", Convergence,seg_15,and the mutual convergence set is given by
231,1,['set'], Convergence,seg_15,taking complements in (1) allows the divergence set to be expressed via
232,0,[], Convergence,seg_15,"proposition 3.3 consider finite measurable xn’s and a finite measurable x on any (ω,a, μ). (i) we have"
233,1,"['cauchy', 'limit']", Convergence,seg_15,"(a finite limit x(ω) exists if and only if the cauchy criterion holds; and we want to be able to check for the existence of a finite limit x(ω) without knowing its value.) (ii)(most useful criterion for →a.e.) on any (ω,a, μ), we have"
234,0,[], Convergence,seg_15,xn → a.e. (some finite measurable x) provided
235,1,"['intersection', 'sets', 'events', 'set', 'convergence']", Convergence,seg_15,"proof. use proposition 1.1.2 on the ↗ sets in the mutual convergence analog of the sets ak in (3) to obtain (5). then the intersection of sets in (5) is a subset of each set in the intersection; thus (6) yields (5). finally, the sets in (7) increase to the set in (6); so use proposition 1.1.2 yet again. (replace xn by x in (5), (6), and (7) and require μ(ω) < ∞. then the converse that (5) implies (6) holds, as the events in (6) are then ↘ .)"
236,1,"['functions', 'sets', 'convergence']", Convergence,seg_15,"remark 3.1 (additional measurability for convergence and divergence) suppose we still assume that x1,x2, . . . are finite measurable functions. then the following sets are seen to be measurable:"
237,1,['events'], Convergence,seg_15,"these comments reflect the following fact: if xn(ω) does not converge to a finite number, then there are several different possibilities; but these interesting events are all measurable."
238,1,"['function', 'functions']", Convergence,seg_15,"definition 3.2 (→μ) a given sequence of measurable and a.e. finite functions x1,x2, . . . is said to converge in measure to the measurable function x taking values in r̄ (to be denoted by xn →μ x as n → ∞) if"
239,1,['convergence'], Convergence,seg_15,"(such convergence implies that x must be finite a.s., as"
240,1,"['set', 'null set']", Convergence,seg_15,"(b) on a complete measure space, x = x̃ on n c, for a null set n."
241,1,['functions'], Convergence,seg_15,"theorem 3.1 (relating →μ to →a.e.) let x and x1,x2, . . . be measurable and finite a.e. functions. the following are true."
242,1,['observation'], Convergence,seg_15,"proof. now, (11) is proposition 3.1, and (12) is exercise 3.3 below. result (13) comes from the elementary observation that"
243,0,[], Convergence,seg_15,"to prove (14), choose nk ↑ such that"
244,1,['convergence'], Convergence,seg_15,with μ(bm) ≤ 1/2m−1. since convergence occurs on each bm
245,0,[], Convergence,seg_15,completing the proof of (14).
246,1,['convergence'], Convergence,seg_15,"thus ak ≡ { |xnk − xnk+1 | ≥ 1/2k} has p (ak) ≤ 1/2k for all k. in analogy with the first paragraph, prove the a.s. convergence of the xn to some x on this subsequence by"
247,0,[], Convergence,seg_15,then show that the whole sequence converges in measure to this x.)
248,0,['o'], Convergence,seg_15,consider the unproven half of (15). suppose that every n′ contains a further n′′ as claimed (with a particular x). assume that xn →μ x fails. then for some o > 0 and some n′
249,0,[], Convergence,seg_15,but we are given that some further subsequence n′′ has xn
250,0,[], Convergence,seg_15,but this is a contradiction of (g).
251,1,"['continuous', 'set']", Convergence,seg_15,"exercise 3.4 (a) suppose that μ(ω) < ∞ and g is continuous a.e. μx (that is, g is continuous except perhaps on a set of μx measure 0). then xn →μ x implies that g(xn) →μ g(x). (b) let g be uniformly continuous on the real line. then xn →μ x implies that g(xn) →μ g(x). (here, μ(ω) = ∞ is allowed.)"
252,1,"['functions', 'continuous', 'transformations', 'limit']", Convergence,seg_15,"exercise 3.5 (a) (dini) consider continuous transformations xn from a compact space ω to r for which xn(ω) ↗ x(ω) for each ω ∈ ω, where x is continuous. then xn converges uniformly to x on ω.(likewise, if xn(ω) ↘ x(ω) for all ω.) (b) in general, a uniform limit of bounded and continuous functions xn is also bounded and continuous."
253,1,"['distribution function', 'distribution', 'random variable', 'variable', 'probability', 'random', 'function']", Probability RVs and Convergence in Law,seg_17,"definition 4.1 (random variable and df) (a) a probability space (ω,a, p ) is just a measure space for which p (ω) = 1. now, x : (ω,a, p ) → (r,b) will be called a random variable (to be abbreviated rv); thus it is a b-a-measurable function. if x : (ω,a, p ) → (r̄, b̄), then we will call x an extended rv. (b) the distribution function (to be abbreviated df) of a rv is defined by"
254,0,[], Probability RVs and Convergence in Law,seg_17,we recall that f ≡ fx satisfies
255,1,['continuous'], Probability RVs and Convergence in Law,seg_17,"(2) f is ↗ and right continuous, with f (−∞) = 0 and f (+∞) = 1."
256,1,"['continuous', 'set']", Probability RVs and Convergence in Law,seg_17,"we let cf denote the continuity set of f that contains all points at which f is continuous. (that f ↗ is trivial, and the other three properties all follow from the monotone property of measure, since (∞, x] = ⋂n"
257,1,['continuous'], Probability RVs and Convergence in Law,seg_17,"∞ =1(−∞, n] = r.) (c) if f is ↗ and right continuous with f (−∞) ≥ 0 and f (+∞) ≤ 1, then f will be called a sub df. (d) the induced measure on (r,b) (or (r̄, b̄)) will be denoted by px . it satisfies"
258,1,['distribution'], Probability RVs and Convergence in Law,seg_17,(for all b ∈ b̄ if x is an extended rv). we call this the induced distribution of x. we use the
259,1,['distribution'], Probability RVs and Convergence in Law,seg_17,∼ notation x = f to denote that the induced distribution px(·) of the rv x has df f. (e) we say that rvs xn (with dfs fn) converge in distribution or converge in law to a rv x0 (with df f0) if
260,1,"['probability measure', 'mean', 'probability']", Probability RVs and Convergence in Law,seg_17,"notation 4.1 suppose now that {xn : n ≥ 0} are rvs on (ω,a, p ). then it is customary to write xn →p x0 (in place of xn →μ x0) and xn →a.s. x0 (as well as xn →a.e. x0). the “p” is an abbreviation for in probability, and the “a.s.” is an abbreviation for almost surely. anticipating the next chapter, we let eg(x) denote ∫ g(x)dμ, or ∫ g(x)dp when μ is a probability measure p . we say that xn converges to x0 in rth mean if e|xn − x0|r → 0. we denote this by writing xn →r x0 or xn →lr x0."
261,1,[], Probability RVs and Convergence in Law,seg_17,"proof. (this result has limited importance. but the technique introduced here is useful; see exercise 4.1 below.) now,"
262,1,[], Probability RVs and Convergence in Law,seg_17,"the following elementary result is extremely useful. often, one knows that xn →d x, but what one is really interested in is a slight variant of xn, rather than xn itself. the next result was designed for just such situations."
263,1,['probability'], Probability RVs and Convergence in Law,seg_17,"theorem 4.1 (slutsky) suppose that xn →d x, while the rvs yn →p a and zn →p b as n → ∞ (here xn, yn, and zn are defined on a common probability space, but x need not be). then"
264,1,['independent'], Probability RVs and Convergence in Law,seg_17,"remark 4.1 suppose x1,x2, . . . are independent rvs with a common df f. then xn →d x0 for any rv x0 having df f . however, there is no rv x for which xn converges to x in the sense of →a.s.,→p, or →r. (of course, we are assuming that x is not a degenerate rv (that is, that μf is not a unit point mass).)"
265,1,"['union', 'intersection']", Discussion of Sub σFields ,seg_19,"consider again a sequence of rvs x1,x2, . . . where each quantity xn is a measurable transformation xn : (ω,a, p ) → (r,b, pxn), and where pxn denotes the induced measure. each rv xn is b-f(xn)-measurable, with f(xn) a sub σ-field of a. even though the intersection of any number of σ-fields is a σ-field, the union of even two σ-fields need not be a σ-field. we thus define the sub σ-field generated by x1, . . . , xn as"
266,0,[], Discussion of Sub σFields ,seg_19,where the equality will be shown in the elementary proposition 5.2.1 below.
267,1,"['probabilities', 'information']", Discussion of Sub σFields ,seg_19,"think of f(x1, . . . , xn) as the amount of information available at time n from x1, . . . , xn; that is, you have available for inspection all of the probabilities"
268,1,"['sets', 'probabilities']", Discussion of Sub σFields ,seg_19,"for all borel sets bn ∈ bn. rephrasing, you have available for inspection all of the probabilities"
269,1,['information'], Discussion of Sub σFields ,seg_19,"at stage n+1 you have available p (a) for all a ∈ f(x1, . . . , xn,xn+1); that is, you have more information available. (think of fn\f(x1, . . . , xn) as the amount of information available to you at time n that goes beyond the information available from x1, . . . , xn; perhaps some of it comes from other rvs not yet mentioned, but it is available nonetheless.)"
270,1,"['probability measures', 'information', 'joint', 'probability']", Discussion of Sub σFields ,seg_19,"suppose we are not given rvs, but rather (speaking informally now, based on your general feel for probability) we are given joint dfs fn(x1, . . . , xn) that we think ought to suffice to construct probability measures on (rn,bn). in (2.2.16) we saw that for n = 1 we could just let (ω,a, μ) = (r,b, μf ) and use x(ω) = ω to define a rv that carried the information in the df f . how do we define probability measures pn on (rn,bn) so that the coordinate rvs"
271,1,"['consistency', 'condition', 'information']", Discussion of Sub σFields ,seg_19,"and thus carry all the information in fn? chapter 5 will deal with this construction. but even now it is clear that for this to be possible, the fn’s will have to satisfy some kind of consistency condition as we go from step n to n + 1. moreover, the consistency problem should disappear if the resulting xn’s are “independent.”"
272,1,['probabilities'], Discussion of Sub σFields ,seg_19,"but we need more. we will let r∞ denote all infinite sequences ω1, ω2, . . . for which each ωi ∈ r. now, the construction of (5) and (6) will determine probabilities on the collection"
273,1,"['probability measure', 'probability']", Discussion of Sub σFields ,seg_19,"with bn ∈ bn. each of these collections is a σ-field (which within this special probability space can be denoted by f(x1, . . . , xn)) in this overall probability space (r∞,b∞, p∞), for some appropriate b∞. but what is an appropriate σ-field b∞ for such a probability measure p∞? at a minimum, b∞ must contain"
274,0,[], Discussion of Sub σFields ,seg_19,"and indeed, this is what we will use for b∞. of course, we also want to construct the measure p∞ on (r∞,b∞) in such a way that"
275,1,"['contrast', 'statistics', 'probability theory', 'measurements', 'probability']", Discussion of Sub σFields ,seg_19,"until chapter 5 we will assume that we are given the rvs x1,x2, . . . on some (ω,a, p ), and we will need to deal only with the known quantities f(x1, . . . , xn) and f(x1,x2, . . .) defined in (1) and (2). this is probability theory: given (ω,a, p ), we study the behavior of rvs x1,x2, . . . that are defined on this space. now contrast this with statistics: given a physical situation producing measurements x1,x2, . . ., we construct models {(r∞,b∞, p∞"
276,1,"['model', 'statistician', 'data', 'probability theory', 'probability']", Discussion of Sub σFields ,seg_19,"based on various plausible models for fnθ(x1, . . . , xn), θ ∈ θ, and we then use the data x1,x2, . . . and the laws of probability theory to decide which model θ0 ∈ θ was most likely to have been correct and what action to take. in particular, the statistician must know that the models to be used are well-defined."
277,1,['interval'], Discussion of Sub σFields ,seg_19,"we also need to extend all this to uncountably many rvs {xt : t ∈ t}, for some interval t such as [a, b], or [a,∞), or [a,∞], or (−∞,∞), . . . . we say that rvs xt : (ω,a, p ) → (r,b) for t ∈ t are adapted to an ↗sequence of σ-fields ft if fs ⊂ ft for all s ≤ t with both s, t ∈ t and if each xt is ft-measurable. in this situation we typically let rt ≡ ∏t∈t rt and then let"
278,1,['sets'], Discussion of Sub σFields ,seg_19,"this is also done in chapter 5 (where more general sets t are, in fact, considered)."
279,1,[], Discussion of Sub σFields ,seg_19,the purpose in presenting this section here is to let the reader start now to become familiar and comfortable with these ideas before we meet them again in chapter 5 in a more substantial and rigorous presentation. (the author assigns this as reading at this point and presents only a very limited amount of chapter 5 in his lectures.)
280,0,[], Discussion of Sub σFields ,seg_19,"(b) recall the dynkin π-λ theorem, and state its implications in this context."
281,1,['functions'], The Lebesgue Integral,seg_23,"let (ω,a, μ) be a fixed measure space and let x,y,xn, . . . denote measurable functions from (ω,a, μ) to (r̄, b̄). if ω = ∑n"
282,1,['function'], The Lebesgue Integral,seg_23,"n =1xi1ai ≥ 0 is a simple function (where all xi ≥ 0 and a1, . . . , an is a partition of ω), then"
283,1,['independent'], The Lebesgue Integral,seg_23,"(we must verify that this is well defined. that is, we must show that the value assigned to ∫ x dμ in (1) is independent of the representation of x that is specified.) if x ≥ 0, then"
284,1,['function'], The Lebesgue Integral,seg_23,∫ y dμ : 0 ≤ y ≤ x and y is such a simple function} .
285,0,[], The Lebesgue Integral,seg_23,"for general measurable x,"
286,1,['set'], The Lebesgue Integral,seg_23,"in each of these definitions we agree to identify x and x ′ whenever x = x ′ a.e. μ. if x (which is not measurable) equals a measurable y on a set a having μ(ac) = 0, then ∫ x dμ ≡ ∫ y dμ. (clearly, ∫ x dμ is not affected by the choice of y or a.) if x is measurable and ∫ x dμ is finite, then x is called integrable. for any a ∈ a,"
287,0,[], The Lebesgue Integral,seg_23,"we also use the notation (especially in proofs, to save space)"
288,1,['expectation'], The Lebesgue Integral,seg_23,(6) ∫ x ≡ ∫ x dμ ≡ (the integral of x) ≡ ex ≡ (the expectation of x).
289,0,[], The Lebesgue Integral,seg_23,it will now be demonstrated that the definition (1) makes sense and that ∫ x dμ satisfies the following elementary properties.
290,1,['functions'], The Lebesgue Integral,seg_23,"proposition 1.1 (elementary properties of the integral) it holds that definition 1.1 of the integral is unambiguous. now suppose that the functions x and y are measurable, that ∫ x dμ and ∫ y dμ are well-defined, and that their sum (the number ∫ x dμ + ∫ y dμ) is a well-defined number in [−∞,+∞]. then"
291,1,"['functions', 'case']", The Lebesgue Integral,seg_23,proof. case 1: consider simple functions x ≥ 0 and y ≥ 0.
292,1,['functions'], The Lebesgue Integral,seg_23,mxiμ(ai) for such simple functions x = ∑1
293,1,['functions'], The Lebesgue Integral,seg_23,∫ x dμ well-defined for these simple functions.
294,0,[], The Lebesgue Integral,seg_23,suppose that we also have x = ∑1
295,0,[], The Lebesgue Integral,seg_23,"and since the two extreme terms that represent the two different definitions of the quantity ∫ x dμ are equal, we see that ∫ x dμ is well-defined."
296,1,['functions'], The Lebesgue Integral,seg_23,claim 2: the integral behaves linearly for such simple functions.
297,1,['functions'], The Lebesgue Integral,seg_23,which establishes the additivity for simple functions.
298,1,[], The Lebesgue Integral,seg_23,"claim 4: so too, the monotonicity in (8) is trivial for any measurable 0 ≤ x ≤ y ."
299,1,"['functions', 'case', 'convergence']", The Lebesgue Integral,seg_23,"the proof of linearity for general x ≥ 0 and y ≥ 0 is included in the proof of the monotone convergence theorem (mct) (that is, the first theorem of the next section). that is, we will prove the mct using only claims 1, 3, and 4. then we will use the mct and claim 2 to obtain the linearity of the integral for any functions x ≥ 0 and y ≥ 0. case 2: the final linearity step is then trivial. just write x = x+ − x− and y = y + − y − and do algebra."
300,1,"['function', 'associated']", The Lebesgue Integral,seg_23,notation 1.1 let f denote a generalized df and let μf denote the associated lebesgue– stieltjes measure. suppose that g is an integrable function on r. we will then freely use the notation
301,1,"['functions', 'convergence']", Fundamental Properties of Integrals,seg_25,"theorem 2.1 (mct, monotone convergence theorem) suppose that xn ↗ x a.e. for measurable functions xn ≥ 0 a.e. then"
302,1,"['sets', 'null sets']", Fundamental Properties of Integrals,seg_25,"proof. by redefining on null sets if necessary, we may assume that xn ↗ x for all ω. thus x is measurable, by proposition 2.2.2. also, ∫ xn is ↗, and so a ≡ lim ∫ xn exists in [0,∞]. moreover, xn ≤ x implies ∫ xn ≤ ∫ x; and so we conclude that a = lim ∫ xn ≤ ∫ x."
303,1,['function'], Fundamental Properties of Integrals,seg_25,mcj1dj be an arbitrary simple function satisfying 0 ≤ y ≤ x. fix 0 < θ < 1. then note that an ≡ [xn ≥ θy ] ↗ ω (since 0 ≤ θy ≤ x on [x = 0] and 0 ≤ θy < x on [x > 0] are both trivial). claims 3 and 4 of the proposition 3.1.1 proof give (for any simple
304,0,[], Fundamental Properties of Integrals,seg_25,mcj1dj as above)
305,1,['functions'], Fundamental Properties of Integrals,seg_25,"proof. we now return to the linearity of the integral for general measurable functions x ≥ 0 and y ≥ 0. let xn ↗ x and yn ↗ y for the measurable simple functions of (2.2.10). then xn + yn ↗ x + y . thus the mct twice, the linearity of the integral for simple functions, and then the mct again give the general linearity of the integral via"
306,1,['function'], Fundamental Properties of Integrals,seg_25,(a) = lim ∫(xn + yn) by simple function linearity
307,0,[], Fundamental Properties of Integrals,seg_25,"in general, combine the integrals of x+,x−, y +, and y − appropriately."
308,1,"['sets', 'null sets']", Fundamental Properties of Integrals,seg_25,proof. redefine on null sets (if necessary) so that all xn ≥ 0. then
309,1,"['function', 'convergence']", Fundamental Properties of Integrals,seg_25,"theorem 2.3 (dct, dominated convergence theorem) let |xn| ≤ y a.e. for all n, for some dominating function y ∈ l1; and suppose either (i) xn →a.e. x or (ii) xn →μ x. then"
310,1,['function'], Fundamental Properties of Integrals,seg_25,"(if (supn≥1 |xn|) is integrable, then it is a suitable dominating function.)"
311,0,[], Fundamental Properties of Integrals,seg_25,corollary 1 note that (4) implies both
312,1,['functions'], Fundamental Properties of Integrals,seg_25,"proof. (i) suppose that xn →a.e. x. then zn ≡ |xn − x| →a.e. 0. (here, 0 ≤ zn ≤ 2y a.s., where both of the functions 0 and 2y are in l1.) now apply fatou’s lemma to the rvs 2y − zn ≥ 0, and conclude that"
313,1,['results'], Fundamental Properties of Integrals,seg_25,"hence, lim ∫ zn ≤ ∫ 0 = 0 (as ∫ 2y is finite). combining the two results gives"
314,1,['case'], Fundamental Properties of Integrals,seg_25,"′′ →a.e. 0, while we still have ∫ zn ′′ → a. but ∫ zn ′′ → 0 by case (i). thus a = 0. thus"
315,0,[], Fundamental Properties of Integrals,seg_25,(iii) consider the corollary. we have
316,0,[], Fundamental Properties of Integrals,seg_25,theorem 2.5 (absolute continuity of the integral) fix x ∈ l1. then
317,1,['function'], Fundamental Properties of Integrals,seg_25,exercise 2.1 (only the zero function) show that
318,1,['function'], Fundamental Properties of Integrals,seg_25,"exercise 2.2 (only the zero function) (a) suppose σ[c] = a, for a field c. show that"
319,1,['intervals'], Fundamental Properties of Integrals,seg_25,"on (r,b, μf ), for a generalized df f , we only need (10) for all intervals a = (a, b]."
320,1,"['functions', 'cases', 'function', 'indicator']", Fundamental Properties of Integrals,seg_25,"exercise 2.3 consider a measure space (ω,a, μ). let μ0 ≡ μ|a0 for a sub σ-field a0 of a. starting with indicator functions, show that ∫ x dμ = ∫ x dμ0 for any a0-measurable function x. hint: consider four cases, as in the next proof."
321,1,['function'], Fundamental Properties of Integrals,seg_25,"definition 2.1 (induced measure) suppose that x : (ω,a, μ) → (ω′,a′) is a measurable function. recall from (2.2.15) that"
322,1,"['functions', 'change of variable', 'statistician', 'variable', 'function']", Fundamental Properties of Integrals,seg_25,"theorem 2.6 (theorem of the unconscious statistician) (i) the induced measure μx(·) of the measurable function x : (ω,a, μ) → (ω′,a′, μx) determines the induced measure μg(x) for all measurable functions g: ω′,a′) → (r̄, b̄). (ii) (change of variable) then"
323,0,[], Fundamental Properties of Integrals,seg_25,"in the sense that if either side exists then so does the other and they are equal. so,"
324,1,['case'], Fundamental Properties of Integrals,seg_25,"(ii) we only prove the first equality in (12) when a′ = ω′ and x−1(ω′) = ω, since we can replace g by g × 1a′ , noting that 1a′(x(ω)) = 1x−1(a′)(ω). case 1. g = 1a′ : then"
325,1,['case'], Fundamental Properties of Integrals,seg_25,= lim ∫ gn dμx by case 2
326,1,['case'], Fundamental Properties of Integrals,seg_25,= ∫ g+ dμx − ∫ g− dμx by case 3
327,0,['e'], Fundamental Properties of Integrals,seg_25,"in the arguments (b), (c), (d), (e) one should start from the end that is assumed to exist, in order to make a logically tight argument. (note the next exercise.)"
328,0,[], Fundamental Properties of Integrals,seg_25,exercise 2.4 let y ≡ g(x) in the context of the theorem 2.6. verify the truth of the second equality in (13).
329,1,['probability'], Fundamental Properties of Integrals,seg_25,"exercise 2.5 let x equal −1, 0, 1 with probability 1/3 for each possibility. let g(x) = x2. then evaluate both sides in (13), and see why such calculations were performed unconsciously for years."
330,1,['function'], Fundamental Properties of Integrals,seg_25,"exercise 2.6 (integrals as measures) let x ≥ 0 for some measurable function x on (ω,a). show that"
331,1,['continuous'], Fundamental Properties of Integrals,seg_25,"exercise 2.7 (absolutely continuous dfs) let z ≥ 0 on r with z ∈ l1(r,b, λ) for the lebsegue measure λ(·) generalization of length. then f (x) ≡ ∫(−∞,x] zdλ defines a generalized"
332,1,['continuous'], Fundamental Properties of Integrals,seg_25,df on r. use the absolute continuity of the integral theorem 2.5 to show that this gdf is absolutely continuous on r (in the sense of definition 1.3.3).
333,0,[], Evaluating and Differentiating Integrals,seg_27,"let (r, b̂μ, μ) denote a lebesgue–stieltjes measure space that has been completed. if g"
334,0,[], Evaluating and Differentiating Integrals,seg_27,"is a generalized df corresponding to μ, then we also use the notation ∫(a,b] g dμf . also,"
335,0,[], Evaluating and Differentiating Integrals,seg_27,"b g df ≡ ∫(a,b] g df will denote the riemann–stieltjes integral."
336,1,['continuous'], Evaluating and Differentiating Integrals,seg_27,"theorem 3.1 (equality of ls and rs integrals) let g be continuous on [a, b]. then the lebesgue–stieltjes integral and the riemann–stieltjes integral are equal. (since the lsintegral and the rs-integral are equal, we can continue to evaluate most ls-integrals using the methods learned in a more elementary calculus.)"
337,1,"['partitions', 'associated']", Evaluating and Differentiating Integrals,seg_27,"proof. we first recall the classical setup associated with the definition of the rs-integral. consider any sequence of partitions a ≡ xn0 < · · · < xnn ≡ b such that the partition xn ≡ {xn0, xn1, . . . , xnn} is a refinement of xn−1 in the sense that xn−1 ⊂ xn. then if meshn ≡ max1≤k≤n(xnk − xn,k−1) → 0, and if x∗nk’s are such that xn,k−1 < x∗nk ≤ xnk, we have"
338,1,['continuous'], Evaluating and Differentiating Integrals,seg_27,"since g is (necessarily) uniformly continuous on [a, b]. thus for all such sequences the lsintegral of section 3.1 satisfies"
339,1,['partitions'], Evaluating and Differentiating Integrals,seg_27,"and this holds for all partitions and x∗n′ks as above, provided only that meshn → 0. thus the"
340,1,['continuous'], Evaluating and Differentiating Integrals,seg_27,b g df and the rs-integral are equal for continuous g.
341,0,[], Evaluating and Differentiating Integrals,seg_27,"exercise 3.1 ∗(rs-integral compared to ls-integral) we state a few additional facts here, just for completeness (that are valid when g is more general):"
342,1,['continuous'], Evaluating and Differentiating Integrals,seg_27,g is rs-integrable with respect to f if and only if (1) g is continuous a.e. μf (·).
343,0,[], Evaluating and Differentiating Integrals,seg_27,"if g is rs-integrable with respect to f,"
344,1,['sets'], Evaluating and Differentiating Integrals,seg_27,let d(f ) and d(g) denote the discontinuity sets of f and g. then
345,1,"['function', 'continuous']", Evaluating and Differentiating Integrals,seg_27,"exercise 3.2 suppose that the improper rs-integral of a continuous function g on r,"
346,1,['function'], Evaluating and Differentiating Integrals,seg_27,"b |g|df ) need not be finite. thus the fact that an improper rs-integral exists does not imply that the function is ls-integrable. construct an example on [0,∞)."
347,1,['function'], Evaluating and Differentiating Integrals,seg_27,"exercise 3.3 (differentiation under the integral sign) (a) suppose that the function x(t, ·) is an integrable function on (ω, μ), for each t ∈ [a, b]. suppose also that for a.e. ω the"
348,1,['interval'], Evaluating and Differentiating Integrals,seg_27,"∂ tx(t, ω) exists for all t in the nondegenerate interval [a, b] (use one-sided derivatives at the end points), and that"
349,0,[], Evaluating and Differentiating Integrals,seg_27,"then the derivative and integral may be interchanged, in that"
350,1,['hypotheses'], Evaluating and Differentiating Integrals,seg_27,"(b) fix t ∈ (a, b). formulate hypotheses that yield (4) at this fixed t."
351,0,[], Evaluating and Differentiating Integrals,seg_27,exercise 3.4 and exercise 3.5 below combine to offer a more elementary problem that is still along the lines of exercise 3.1.
352,1,"['function', 'interval', 'continuous']", Evaluating and Differentiating Integrals,seg_27,"exercise 3.4 (continuity on an interval [a, b] implies uniform continuity) let g denote a continuous real valued function on a closed interval [a, b]. show that g is uniformly continuous on [a, b]. that is, show that for every tiny number > 0 there exists a number δ > 0 for which"
353,1,"['function', 'continuous']", Evaluating and Differentiating Integrals,seg_27,exercise 3.5 (riemann integrability) let g ≥ 0 denote a continuous real valued function on
354,1,['interval'], Evaluating and Differentiating Integrals,seg_27,"b a closed interval [a, b]. let and δ be as in (5). let g(x) dx denote the area under g. show that if meshm < δ in the riemann sum rsm defined in (1.1.1), then rsm is so close"
355,1,"['function', 'interval']", Inequalities,seg_29,convexity we begin by briefly reviewing convexity. a real-valued function f defined on some interval i of real numbers is convex if
356,1,"['continuous', 'interval']", Inequalities,seg_29,"we will use the following facts. if f is convex on an interval, then f is continuous on the interior io of the interval. also, the left and right derivatives exist and satisfy d(x−) ≤ d(x+) at each point in the interior io of the interval. the following is useful. convexity on the interval i holds if and only if"
357,1,['continuous'], Inequalities,seg_29,"f((x + y)/2) ≤ [f(x) + f(y)]/2 for all x, y in i, provided that (2) f is also assumed to be bounded (or continuous, or measurable) on i."
358,1,"['functions', 'interval', 'continuous', 'test', 'inequality']", Inequalities,seg_29,"(there exist functions satisfying the inequality in (2) that are not continuous, but they are unbounded in every finite interval. thus requiring (1) for all 0 ≤ α ≤ 1 is strictly stronger then requiring it to hold only for α = 1/2.) we need a simple test for convexity (when f is ‘nice’), and so note that f is convex if"
359,1,"['function', 'linear', 'inequality']", Inequalities,seg_29,"we call f strictly convex if strict inequality holds in any of the above. if f is convex, then there exists a linear function such that f(x) ≥ (x) with equality at any prespecified x0 in the interior io of the domain i of f ; this function is called the supporting hyperplane. (call f concave in −f is convex.)"
360,1,['moments'], Inequalities,seg_29,"definition 4.1 (moments) (the following definitions make sense on a general measure space (ω,a, μ), recall, as in (3.1.6), that eh(x) = ∫ h(x(ω)) dμ(ω) = ∫ h(x) dμ = ∫ h(x).) let"
361,1,['mean'], Inequalities,seg_29,(4) μ ≡ μx ≡ (the mean ofx) ≡ ex. (note the two different uses of μ)
362,1,['moment'], Inequalities,seg_29,"(5) exk ≡ (kth moment of x), for k ≥ 1 an integer,"
363,1,['moment'], Inequalities,seg_29,"(6) e|x|r ≡ (rth absolute moment of x), for r > 0,"
364,1,"['probability', 'standard']", Inequalities,seg_29,"the following notation is standard on a probability space (ω,a, p ), where μ(ω) = 1 :"
365,1,['probability'], Inequalities,seg_29,"we will write x =∼ (μ, σ2) (on a probability space) if ex = μ and var [x] = σ2 < ∞. we will write x =∼ f (μ, σ2) if x also has df f (·). further (on a probability space)"
366,1,['moment'], Inequalities,seg_29,"(9) μk ≡ e(x − μ)k ≡ (kth central moment of x), for k ≥ 1,"
367,1,['covariance'], Inequalities,seg_29,"(10) cov[x,y ] ≡ e[(x − μx)(y − μy )] = (the covariance of x andy )."
368,1,"['probability theory', 'probability']", Inequalities,seg_29,"note that cov [x,x] = var[x]. (probability theory has p (ω) ≡ μ(ω) = 1.)"
369,1,['functions'], Inequalities,seg_29,throughout this section x and y will denote measurable functions.
370,1,['probability'], Inequalities,seg_29,"proposition 4.2 let μ(ω) < ∞. then σ2 < ∞ holds if and only if ex2 < ∞. from here on, we will only refer to σ2 on a probability space; then σ2 = ex2 − μ2."
371,1,"['expectations', 'case']", Inequalities,seg_29,"proof. there are no restrictions on μ. note that e|x + y |r ≤ e(|x| + |y |)r. case 1. r > 1: then |x|r is convex in x for x ≥ 0, since its derivative is ↑. thus |(x+y)/2|r ≤ [|x|r + |y|r]/2; and now take expectations. case 2. 0 < r ≤ 1: now, |x|r is concave and ↗ forx ≥ 0; just examine derivatives. thus |x + y|r − |x|r ≤ |0 + y|r − 0r since the increase from x to x + y can not exceed the increase from 0 to y, and now take expectations."
372,1,['inequality'], Inequalities,seg_29,"inequality 4.2 (hölder’s inequality) for r > 1, with 1/r + 1/s = 1,"
373,1,['expectations'], Inequalities,seg_29,"proof. the result is trivial if e|x|r = 0 or ∞. likewise for e|y |s. so suppose that both expectations are in (0,∞). since f(x) = ex is convex by fact (3), it satisfies (1) with α ≡ 1/r and 1 − α = 1/s, x ≡ r log |a| and y ≡ s log |b| for some a and b; thus (1) becomes (with equality if and only if r log |a| = x = y = s log |b|)"
374,1,['inequality'], Inequalities,seg_29,"young’s inequality for all a, b we have"
375,1,['expectations'], Inequalities,seg_29,"now let a = |x|/‖x‖r and b = |y |/‖y ‖s, and take expectations. equality holds if and only if (|y |/‖y ‖s)s =a.e. (|x|/‖x‖r)r (that is, all mass is located at equality in (12)) if and only if"
376,1,['inequality'], Inequalities,seg_29,exercise 4.1 (convexity inequality) show that
377,1,['inequality'], Inequalities,seg_29,use this to reprove hölder’s inequality.
378,1,['inequality'], Inequalities,seg_29,"inequality 4.3 (cauchy–schwarz) {e(xy )}2 ≤ (e|xy |)2 ≤ ex2ey 2. if both ex2 and ey 2 take values in (0,∞), then equality holds throughout both of the inequalities if and only if either y = ax a.e. or y = −ax a.e., for some a > 0; in fact, a2 = ey 2/ex2. (only y 2 = cx2 a.e. for some c > 0 is required for equality in the rightmost inequality above.)"
379,1,"['variances', 'correlation', 'probability', 'inequality']", Inequalities,seg_29,"example 4.1 (correlation inequality) for rvs x and y (on a probability space) having positive and finite variances, it holds that"
380,1,['correlation'], Inequalities,seg_29,for the correlation ρ of x and y defined by
381,0,[], Inequalities,seg_29,"exercise 4.2 consider rvs x and y having ex2 and ey 2 in (0,∞). show that"
382,1,"['linear', 'dependence']", Inequalities,seg_29,"2 /σx 2 . thus ρ measures linear dependence, not general dependence."
383,1,['inequality'], Inequalities,seg_29,inequality 4.4 (a) (liapunov’s inequality) it holds that
384,1,['inequality'], Inequalities,seg_29,proof. (c) apply hölder to |x|αa and |x|(1−α)b with r = 1/α and s = 1/(1 − α) and obtain the inequality
385,1,['expectations'], Inequalities,seg_29,(all expectations are finite if x ∈ la ∩ lb; since a ≤ r ≤ b and c > 0 implies that cr ≤ cb or cr ≤ ca as c ≥ 1 or c ≤ 1.) taking logarithms gives the convexity
386,1,['inequality'], Inequalities,seg_29,"exercise 4.3 (littlewood’s inequality) define mr ≡ e|x|r. show that for 0 ≤ r ≤ s ≤ t we have (write ms = e(|x|λs · |x|(1−λ)s), and apply hölder)"
387,1,['inequality'], Inequalities,seg_29,"inequality 4.5 (minkowski’s inequality) e1/r|x +y |r ≤ e1/r|x|r +e1/r|y |r for all r ≥ 1. that is, ‖x + y ‖r ≤ ‖x‖r + ‖y ‖r for r ≥ 1. (recall that ‖x + y ‖r ≤ ‖x‖r + ‖y ‖r for 0 < r ≤ 1, by the cr-inequality and (7).) thus ‖ · ‖r turns lr into a metric space (if we identify x and x ′ when x =a.e. x ′)."
388,1,['inequality'], Inequalities,seg_29,≤ (‖x‖r + ‖y ‖r) ‖|x + y |r−1‖s by hölder’s inequality twice
389,1,['inequality'], Inequalities,seg_29,"inequality 4.6 (basic inequality) let g ≥ 0 be ↗ on [0,∞) and even. then for all measurable x we have"
390,0,[], Inequalities,seg_29,the next two inequalities are immediate corollaries.
391,1,['inequality'], Inequalities,seg_29,inequality 4.7 (markov’s inequality) μ(|x| ≥ λ) ≤ e|x|r/λr for all λ > 0.
392,1,['inequality'], Inequalities,seg_29,"inequality 4.8 (chebyshev’s inequality) if e|x| < ∞, then"
393,1,['inequality'], Inequalities,seg_29,"proof. cauchy–schwarz, and then rearrangement give the inequality"
394,1,"['interval', 'inequality']", Inequalities,seg_29,"inequality 4.10 (jensen’s inequality) let x : (ω,a, p ) → (i,bi , px), where i any interval subset of [−∞,∞]; thus p (x ∈ i) = 1. suppose ex is in the interior io of the interval i. let g be convex on i. then the rv x satisfies"
395,0,[], Inequalities,seg_29,proof. let (·) be a supporting hyperplane to g(.) at ex. then
396,1,['linear'], Inequalities,seg_29,(b) = (ex) since (·) is linear and μ(ω) = 1
397,1,"['events', 'probability']", Inequalities,seg_29,"inequality 4.11 (bonferroni) for any events ak on a probability space (ω,a, p ),"
398,1,"['set', 'variance', 'mean']", Inequalities,seg_29,"exercise 4.4 (w̃insorized variance) (a) let the rv x have finite mean μ. fix c, d with c ≤ μ ≤ d. let x̃ equal c,x, d according as [x ≤ c], [c < x ≤ d], [d < x], and set μ̃ ≡ ex̃. show that e|x̃ − μ̃|2 ≤ e|x̃ − μ|2 ≤ e|x − μ|2. (b)∗ (chow and teicher) given both a rv x with finite mean μ and a number r ≥ 1, show how to choose c, d so that e|x̃ − μ̃|r ≤ e|x − μ|r."
399,1,['inequality'], Inequalities,seg_29,let r > 1. use the hölder inequality to show that
400,1,[], Inequalities,seg_29,"exercise 4.6 (wellner) let t =∼ binomial(n, p), so p (t = k) = pk(1 − p)n−k for"
401,1,"['variance', 'associated', 'mean', 'inequality']", Inequalities,seg_29,"0 ≤ k ≤ n. the measure associated with t has mean np and variance np(1 − p). then use inequality 4.6 with g(x) = exp(rx) and r > 0, to show that"
402,1,"['geometric mean', 'geometric', 'mean']", Inequalities,seg_29,exercise 4.7 (geometric mean) show that (x1 ×· · ·×xn)1/n ≤ (x1 + · · ·+xn)/n whenever all xk ≥ 0.
403,1,['inequality'], Inequalities,seg_29,"exercise 4.9 ∗(clarkson’s inequality) let x,y in lr(ω,a, μ). show that"
404,0,[], Inequalities,seg_29,"exercise 4.11 show that for all a, b we have"
405,1,['inequality'], Inequalities,seg_29,with the reverse inequality for 0 < r < 1.
406,1,"['probability measure', 'probability', 'inequality']", Inequalities,seg_29,"exercise 4.12 let (ω,a, μ) have μ(ω) < ∞. then p (a) ≡ μ(a)/μ(ω), for all a ∈ a, is a probability measure p . (a) restate jensen’s inequality (26) in terms of μ. (b) restate liapunov’s inequality (16) in terms of μ."
407,1,['convergence'], Modes of Convergence,seg_31,"definition 5.1 (modes of convergence) let x and xn’s be measurable and a.e. finite from the measure space (ω,a, μ) to (r̄, b̄). (a) recall that xn converges a.e. to x (denoted by xn →a.e. x) if"
408,0,[], Modes of Convergence,seg_31,"(b) also, recall that xn converges in measure to x (denoted by xn →μ x) if"
409,1,['mean'], Modes of Convergence,seg_31,"(c) now (rigorously for the first time), xn converges in rth mean to x (denoted by xn →r x or xn →lr x) if"
410,1,['inequality'], Modes of Convergence,seg_31,"recall from chapter 2 that xn → a.e. (some a.e. finite x) holds if and only if xn − xm →a.e. 0 as m ∧ n → ∞. likewise, in chapter 2 we had xn →μ (some x) if and only if xn − xm →μ 0 as m ∧ n → ∞. next, we will consider xn →r x.(first, note that xn →r x trivially implies xn →μ x, using the markov inequality.)"
411,1,[], Modes of Convergence,seg_31,"that is, lr is complete with respect to →r. prove (a), using (2.3.14). (show that (lr, ‖ · ‖r) is a complete metric space (when r > 0), provided that we identify x and x ′ whenever x = x ′ a.e.) (note theorem 5.8 below regarding separability.) (ii) let μ(ω) < ∞. then:"
412,1,['inequality'], Modes of Convergence,seg_31,(hint: use fatou’s lemma in (a) and hölder’s inequality in (b).) summary let x and xn’s be measurable and a.e. finite (see definition 5.1). then
413,1,['cauchy'], Modes of Convergence,seg_31,"xn is cauchy a.e., cauchy in measure, or cauchy in lr."
414,1,"['distribution', 'convergence']", Modes of Convergence,seg_31,"consequences of convergence in distribution on (ω,a, p )"
415,1,"['distribution', 'probability measure', 'probability']", Modes of Convergence,seg_31,"notation 5.1 suppose now that μ really denotes a probability measure, and so we will label it p . recall that rvs xn converges in distribution to a rv x (denoted by xn →d x, fn →d f or l(xn) → l(x) with l(·) referring to “law”) when the dfs f and fn of the rvs x and xn satisfy (recall (2.4.4))"
416,1,"['probability measure', 'probability']", Modes of Convergence,seg_31,"[note that fn ≡ 1[1/n,∞) →d f ≡ 1[0,∞), even though fn(0) = 0 →/ 1 = f (0).] the statement →d will carry with it the implication that f corresponds to a probability measure p , which can be viewed as the px = μx of an appropriate rv x."
417,1,['continuous'], Modes of Convergence,seg_31,"theorem 5.1 (a) (helly–bray) consider the rvs x and xn on some (ω,a, p ). suppose fn →d f , and suppose that g is bounded and is continuous a.s. f . then"
418,1,['continuous'], Modes of Convergence,seg_31,"(b) conversely, eg(xn) → eg(x) for all bounded, continuous g implies fn →d f ."
419,1,['continuous'], Modes of Convergence,seg_31,"theorem 5.2 (mann–wald) consider the rvs x and xn on some (ω,a, p ). suppose xn →d x, and let g be continuous a.s. f . then g(xn) →d g(x)."
420,1,['continuous'], Modes of Convergence,seg_31,"proof. we ask for a proof for continuous g in the next exercise, but we give a “look-ahead” proof now. (see theorem 6.3.2 below for the skorokhod proof.) skorokhod if xn →d xo, then the rvs yn ≡ fn−1 on ([0, 1],b[0, 1]p ≡ λ) have"
421,1,['continuous'], Modes of Convergence,seg_31,(a) p (a2) ≡ p ({ω : g is continuous at yo(ω)})
422,1,['continuous'], Modes of Convergence,seg_31,(b) = pyo({y : g is continuous at y}) = pyo(cg) = px(cg) = 1.
423,0,[], Modes of Convergence,seg_31,"since g is bounded, applying the dct to (7) gives the helly–bray claim that"
424,0,['n'], Modes of Convergence,seg_31,"we note additionally that since (7) implies g(yn) →a.s. g(yo), it also implies ∼ g(yn) →d g(yo). since g(xn) = g(yn) for all n, we can also conclude that g(xn) →d g(xo). this argument did not use the boundedness of g, and so proves the mann–wald theorem. theorem 3.2.6 was used twice in (d). (proving helly–bray as indicated in the next exercise would have been possible now, but the proof based on skorokhod’s theorem is more in keeping with the spirit of this book. the helly–bray theorem will be used later in this section, in proving vitali’s theorem.)"
425,1,"['continuous', 'linear']", Modes of Convergence,seg_31,"consider the converse. let g (·) equal 1, be linear, equal 0 on (−∞, x − ], on [x − , x], on [x,∞); and let h (·) equal 1, be linear, equal 0 on (−∞, x], on [x, x + ], on [x + ,∞), with g and h both continuous. (let g ≡ {all such g and h }.) then"
426,1,"['functions', 'condition', 'continuous']", Modes of Convergence,seg_31,definition 5.2 (determining class) let g denote a collection of bounded and continuous functions g on the real line r. if for any rvs x and y the condition
427,1,['functions'], Modes of Convergence,seg_31,then call g a determining class. (the proof of the converse half of helly–bray exhibited one such class of particularly simple functions. see also section 9.1 for further examples which will prove particularly useful.)
428,1,"['continuous', 'case', 'interval']", Modes of Convergence,seg_31,"exercise 5.2 (a) prove the helly–bray result ∫ g dfn → ∫ g df for all bounded and continuous g, without appeal to theorem 6.3.2 of skorokhod. (truncate the real line at large continuity points ±m of f , and then use the uniform continuity of g on the interval [−m,m ] to obtain a simple proof in this case. note exercise 9.1.1.) (b) alter your proof to be valid when g is bounded and merely continuous a.s. μf ."
429,1,"['convergence', 'moment']", Modes of Convergence,seg_31,"general moment convergence on (ω,a, μ)"
430,1,"['convergence', 'moment']", Modes of Convergence,seg_31,"theorem 5.3 (moment convergence under →r) let xn →r x, r > 0. then"
431,1,['inequality'], Modes of Convergence,seg_31,suppose r ≥ 1. then using minkowski’s inequality twice (as in (a)) gives
432,0,[], Modes of Convergence,seg_31,uniform integrability and vitali’s theorem
433,0,[], Modes of Convergence,seg_31,"definition 5.3 (uniformly integrable) a collection of measurable xt’s is called integrable if supt e|xt| < ∞. further, a collection of rvs {xt : t ∈ t} is said to be uniformly integrable (which is abbreviated u.i.) if"
434,1,['functions'], Modes of Convergence,seg_31,the functions xn(t) ≡ 1 1[−n
435,1,"['null sets', 'sets', 'inequality']", Modes of Convergence,seg_31,"remark 5.1 (dominated xt’s are u.i.) suppose these |xt| ≤ y a.s. for some y ∈ l1. then these xt’s are integrable, in that supt e|xt| ≤ ey < ∞. but, more is true. for some null sets nt, we have [|xt| ≥ λ] ⊂ [|y | ≥ λ] ∪ nt. it follows that μ(|xt| ≥ λ) ≤ μ(|y | ≥ λ) → 0 uniformly in t as λ → ∞ (use markov’s inequality). then for each fixed t,"
436,0,[], Modes of Convergence,seg_31,by the absolute continuity of the integral of y in theorem 3.2.5. thus:
437,1,['functions'], Modes of Convergence,seg_31,"∞ p (y ≥ y) dy = ∫0 ∞[1 − f (y)] dy for any rv y ≥ 0 with df f (as will follow from fubini’s theorem below). sketch a proof. (b) in fact, this formula can also be established rigorously now. begin with simple functions y and sum by parts. then apply the mct for the general result. (c) use the result of (a) to show that for y ≥ 0 and λ ≥ 0 we have"
438,0,[], Modes of Convergence,seg_31,"exercise 5.4 (uniform integrability criterion) if supt e|xt|r ≤ m < ∞ for some r > 1, then the xt’s are uniformly integrable. (compare this to theorem 5.6 of de la vallée poussin below, by letting g(x) = xr.)"
439,0,[], Modes of Convergence,seg_31,theorem 5.4 (uniform absolute continuity of integrals) let μ(ω) < ∞. a family of measurable xt’s is uniformly integrable if and only if both
440,0,[], Modes of Convergence,seg_31,|xt| dμ < (uniform absolute continuity).
441,1,['inequality'], Modes of Convergence,seg_31,proof. suppose these conditions hold. then markov’s inequality and (14) give
442,1,['sets'], Modes of Convergence,seg_31,for λ large enough. then (15) applied to the sets [|xt| ≥ λ] yields (12). (note that μ(ω) < ∞ was not used.)
443,1,['condition'], Modes of Convergence,seg_31,"suppose the u.i. condition (12) holds. if μ(a) < δ, then"
444,0,[], Modes of Convergence,seg_31,thus the collection is integrable. thus (15) holds.
445,1,['set'], Modes of Convergence,seg_31,(b) suppose the uniform absolute continuity of (15) holds for the |xn|r. suppose for each > 0 there exists a set a having μ(a ) < ∞ for which supn ∫ac |xn|r dμ ≤ (compare this
446,1,['functions'], Modes of Convergence,seg_31,"corollary 1 (lr-convergence) let μ(ω) < ∞. let r > 0. let all xn ∈ lr. then xn →r x(or,e|xn − x|r → 0) if and only if both xn →μ x and one (hence both) of the two families of functions {|xn|r : n ≥ 1} or {|xn − x|r : n ≥ 1} is u.i."
447,1,['case'], Modes of Convergence,seg_31,e|xn − x|r → 0 if and only if e|xn|r → e|x|r (any μ(ω) value) (22) if and only if (in case μ(ω) < ∞) the rvs {|xn|r : n ≥ 1} are u.i.
448,1,['hypothesis'], Modes of Convergence,seg_31,"the in (k) is from (15), since μ(|xn − x| ≥ ) → 0 by hypothesis. thus (17) holds."
449,1,"['function', 'linear', 'continuous']", Modes of Convergence,seg_31,"suppose (19) holds. define fλ to be a continuous function on [0,∞) that equals |x|r, 0, or is linear, according as |x|r ≤ λ, |x|r ≥ λ + 1, or λ ≤ |x|r ≤ λ + 1. then (graphing fλ(x) and xr on [0, λ + 1]) we have yn ≡ fλ(xn) →μ y ≡ fλ(x) by the uniform continuity of each fλ. (see exercise 2.3.4(b).) let n′ denote any subsequence of n, and let n′′ denote a further subsequence on which xn′′ →a.e. x (see (2.3.14) in theorem 2.3.1 of riesz). on the subsequence n′′ we then have"
450,1,"['function', 'functions', 'convex function']", Modes of Convergence,seg_31,"theorem 5.6 ∗ (de la vallée poussin) let μ(ω) < ∞. a family of l1-integrable functions xt is uniformly integrable if and only if there exists a convex function g on [0,∞) for which g(0) = 0, g(x)/x → ∞ as x → ∞ and"
451,0,[], Modes of Convergence,seg_31,for c sufficiently large. thus (23) implies {xt : t ∈ t} is uniformly integrable.
452,0,[], Modes of Convergence,seg_31,"1 bnan(t) < ∞. by the definition of uniform integrability, we can choose integers cn ↑ ∞ such that"
453,0,[], Modes of Convergence,seg_31,thus for all t we have
454,1,['summation'], Modes of Convergence,seg_31,"thus, interchanging the order of summation,"
455,0,[], Modes of Convergence,seg_31,exercise 5.6 consider only the definition of u.i. do not appeal to vitali.
456,1,"['convergence', 'results']", Modes of Convergence,seg_31,summary of modes of convergence results
457,1,['convergence'], Modes of Convergence,seg_31,theorem 5.7 (convergence implications) let x and xn’s be measurable and a.e. finite. (note figure 5.1.)
458,1,['convergence'], Modes of Convergence,seg_31,figure 5.1 convergence implications.
459,1,['inequality'], Modes of Convergence,seg_31,proof. see theorem 2.3.1 for (i) and (ii). markov’s inequality gives (iii) via
460,1,['inequality'], Modes of Convergence,seg_31,vitali’s theorem includes both halves of (iv). hölder’s inequality gives (v) via
461,1,['inequality'], Modes of Convergence,seg_31,note also exercise 5.1(b) and the proof of inequality 3.4.4(b). proposition 2.4.1 gives (vi). theorem 2.3.1 then gives (vii). the skorokhod construction (to appear more formally as theorem 6.3.2 below) was stated above in (7); (7) gives (viii).
462,1,"['densities', 'convergence']", Modes of Convergence,seg_31,(think of this as the uniform convergence of measures with densities fn.) (hint. integrate (fo − fn)+ and (fo − fn)− separately. note that (fo − fn)+ ≤ fo.) (b) show that lim ∫ω fn dμ ≤ ∫ω fo dμ < ∞ and fn →μ or a.e. fo is sufficient for (24).
463,1,"['functions', 'continuous']", Modes of Convergence,seg_31,approximation of functions in lr by continuous functions∗
464,1,"['functions', 'continuous', 'set']", Modes of Convergence,seg_31,"let cc denote the class of continuous functions on r that vanish outside a compact set, and"
465,1,"['functions', 'continuous', 'step function', 'function']", Modes of Convergence,seg_31,"then let cc(∞) denote the subclass that has an infinite number of continuous derivatives. let sc denote the class of all step functions on r, where such a step function is of the form ∑1"
466,1,"['disjoint', 'intervals', 'associated', 'function']", Modes of Convergence,seg_31,"for disjoint finite intervals ij . further, let f denote a generalized df, and let μ(·) ≡ μf (·) denote the associated lebesgue–stieltjes measure. let x denote a measurable function on (ω,a, μ) = (r,b, μf )."
467,1,"['function', 'functions', 'continuous']", Modes of Convergence,seg_31,"theorem 5.8 (the continuous functions are dense in lr(r,b, μf ), r ≥ 1) suppose throughout that x ∈ lr, for some fixed 1 ≤ r < ∞. (a) (continuous functions) then for each > 0 there is a bounded and continuous function y in cc for which ∫ |x −y |r dμf < . thus the class cc is -dense within the class lr under the ‖ · ‖r-norm."
468,1,"['functions', 'approximation', 'sets']", Modes of Convergence,seg_31,"(b) we may insist that y ∈ cc(∞). (the y of exercise 5.17 has sup |y | ≤ sup |x|.) (c) (step functions) such a close approximation may also be found within the step functions sc, making them -dense also. (d) all this extends to rvs on (rn,bn) (or on locally compact hausdorff spaces). (e) all these spaces lr are separable, provided μ is σ-finite and a is countably generated (that is,a = σ[c] with c a countable collection of sets)."
469,1,['function'], Modes of Convergence,seg_31,proof. let r = 1. consider only x+. approximate it by a simple function x = ∑1
470,1,"['disjoint', 'intervals', 'sets', 'union']", Modes of Convergence,seg_31,"approximation lemma of exercise 1.2.3 guarantees sets b1, . . . , bn made up of a finite disjoint union of intervals of the form (a, b] (with a and b finite continuity points of f , as in (b) of the proof of theorem 1.3.1) for which"
471,1,['disjoint'], Modes of Convergence,seg_31,∫ |x − x ′| dμf < /3. (note that these bi need not be disjoint).
472,1,"['function', 'step function']", Modes of Convergence,seg_31,this x ′ is the step function called for in part (c). rewrite this x ′ = ∑1
473,1,['disjoint'], Modes of Convergence,seg_31,myj1cj with disjoint
474,1,"['linear', 'continuous', 'sets', 'function']", Modes of Convergence,seg_31,"cj = (aj , bj ]. now approximate 1cj by the continuous function wj that equals 0, is linear, equals 1 according as x ∈ [aj , bj ]c, as x ∈ [aj , aj + δ] ∪ [bj − δ, bj ], as x ∈ [aj + δ, bj − δ]. (we require that δ be specified so small that the combined μf measure of all 2m sets of the type x ∈ [aj , aj + δ] and [bj − δ, bj ] is at most θ ≡ /(6∑1"
475,1,['function'], Modes of Convergence,seg_31,"has ∫ |x ′ − y | dμf < /3. thus ∫ |x − y | dμf < , as called for in part (a). for (b), the function ψ(x/δ) (where"
476,1,['continuous'], Modes of Convergence,seg_31,"with ψ(x) equal to 1 or 0 according as x ≤ 0 or x ≥ 1) is easily seen to have an infinite number of continuous derivatives on r (with all said derivatives equal to 0 when x equals 0 or 1). use ψ(−x/δ) on [aj , aj + δ] and ψ(x/δ) on [bj − δ, bj ] to connect values 0 to 1, instead"
477,1,"['function', 'linear']", Modes of Convergence,seg_31,of linear connections. the result is a function in cc(∞).
478,1,['case'], Modes of Convergence,seg_31,"for r > 1, write x = x+ − x− and use the cr-inequality and |a − b|r ≤ |ar − br| for all a, b ≥ 0. for example, make e|x+ − y +|r ≤ e|(x+)r − (y +)r| < by the case r = 1. (exercise 5.18 asks for a proof of (e).)"
479,1,[], Modes of Convergence,seg_31,miscellaneous results∗
480,1,['convergence'], Modes of Convergence,seg_31,"exercise 5.8 ∗(→a.u.,egorov; convergence “almost” implies uniform convergence)"
481,1,"['convergence', 'mean']", Modes of Convergence,seg_31,(i) we define xn →a.u. x (which is used as an abbreviation for almost uniform convergence) to mean that for all > 0 there exists an a with μ(a ) < such that xn →uniformly x on ac. recall (2.3.5) to show
482,1,"['function', 'set']", Modes of Convergence,seg_31,"exercise 5.9 (a) (an integrable function “almost” equals a bounded function) suppose μ(ω) ∈ [0,∞] and ∫ω |x| dμ < ∞. fix > 0. show the existence of a set a with μ(a ) < ∞ for which both |x| ≤ (some m ) on a and ∫ac |x| dμ < ."
483,1,['set'], Modes of Convergence,seg_31,"(b) let μ(ω) < ∞. let x be measurable and finite a.e. for any > 0, specify a finite number m and a set a having μ(ac) < and |x| ≤ m on a ."
484,1,"['functions', 'cardinality', 'set']", Modes of Convergence,seg_31,"exercise 5.11 ( r-spaces) let ω be an arbitrary set and consider the class of all subsets a. let μ(a) denote the cardinality of a when this is finite, and let it equal ∞ otherwise. this is counting measure on ω. let 0 < r < ∞. let r(ω) denote all functions x : ω →r for which ∑ω∈ω|x(ω)|r < ∞. then"
485,1,['case'], Modes of Convergence,seg_31,"defines a norm on r(ω) (see (3.4.7) for 0 < r < 1). this is just a special case of an lr-space, and it is important enough to deserve its specialized notation. show that"
486,1,['set'], Modes of Convergence,seg_31,this set inclusion is proper if ω has infinitely many points.
487,0,[], Modes of Convergence,seg_31,"the exercises below are presented for “flavor” or as tools, rather than to be worked."
488,1,"['functions', 'results', 'inequality']", Modes of Convergence,seg_31,"exercise 5.13 ∗(weak lr-convergence; and in l∞) let xn,x ∈ lr, with r ≥ 1. let 1/r + 1/s = 1 define s for r > 1. let s = ∞ when r = 1, and l∞ denotes all bounded a-measurable y on ω, and let ‖x‖∞ ≡ inf{c : μ({ω : |x(ω)| > c}) = 0} denote the essential supremum of such functions x. let s = 1 when r = ∞. (the following results can be compared with vitali’s theorem.) (a) (a) fix 1 ≤ r < ∞. let xn →r x on l(ω,a, μ). show (via the hölder inequality) that xn converges weakly in lr (denoted by xn →w−lr x) in that"
489,1,['moments'], Modes of Convergence,seg_31,"(b) (radon–reisz) conversely, suppose that xn →w−lr x and additionally that the moments satisfy e|xn|r → e|x|r, where 1 < r < ∞. show that xn →lr x."
490,1,['functions'], Modes of Convergence,seg_31,"(c) (d) (lehmann) fix m . let fm ≡ {x : ‖x‖∞ ≤ m < ∞}. let x,x1,x2, . . . denote specific functions in fm . then (29) holds for all y ∈ l1 if and only if (29) holds for all y in the subclass {1a : μ(a) < ∞}. (note also exercise 5.21 below.)"
491,1,['sets'], Modes of Convergence,seg_31,"exercise 5.14 ∗ (a) l∞(ω, σ[{al1 open sets}], μ) is a complete metric space under the essential sup norm ‖ · ‖∞ whenever ω is a locally compact hausdorff space."
492,1,"['functions', 'sets', 'set']", Modes of Convergence,seg_31,"(b) the set sc of simple functions that vanish off of compact sets is dense in this complete metric space (l∞, ‖ · ‖∞). (recall theorem 5.8.)"
493,1,"['functions', 'continuous']", Modes of Convergence,seg_31,"(c) no family of continuous functions is dense in (l∞([0, 1],b, λ), ‖ · ‖∞), and the space l∞ is not separable under the norm ‖ · ‖∞."
494,1,"['function', 'continuous']", Modes of Convergence,seg_31,exercise 5.16 ∗ (lusin; any meaurable function is “almost” continuous)
495,1,['function'], Modes of Convergence,seg_31,"let x be an (r,b) measurable function on r."
496,1,"['continuous', 'set', 'function']", Modes of Convergence,seg_31,(a) let > 0. show that there exists a continuous function y on r and a closed set d such that λ(dc) < and x = y on d .
497,1,"['function', 'continuous']", Modes of Convergence,seg_31,(b) show that a function x : r → r is b-measurable if and only if there exists a sequence of continuous function yn : r → r for which yn →a.e. x.
498,1,['functions'], Modes of Convergence,seg_31,"(hint. (a) begin with simple functions like those in (2.2.10). consider each [n, n + 1] separately. apply egorov’s theorem.)"
499,1,"['continuous', 'sets', 'set', 'function']", Modes of Convergence,seg_31,"exercise 5.17 ∗(lusin) let x be measurable on (ω,a, μ), where ω is a locally compact hausdorff space (every point has a neighborhood whose closure is compact, such as the real line r with the usual euclidean metric) and a = σ[{open sets}]. suppose x(ω) = 0 for all ω ∈ ac, where μ(a) < ∞. let > 0. then there exists y , where y (ω) = 0 for all ω ∈ bc, with the set b compact, and where y is continuous, sup |y | ≤ sup |x|, and μ({ω : x(ω) = y (ω)}) < . (again, a measurable function is “almost equal” to a continuous function.) (note exercise b.1.14 below.)"
500,0,[], Modes of Convergence,seg_31,exercise 5.18 ∗ prove the separability of lr in theorem 5.8(e).
501,1,[], Modes of Convergence,seg_31,"(a) show that (a0, ρ) is a metric space."
502,1,[], Modes of Convergence,seg_31,"(b) show that the metric space (a0, ρ) is separable whenever a = σ[c] for some countable collection c (that is, whenever a is countably generated)."
503,0,[], Modes of Convergence,seg_31,"definition 5.4 (dominated families of measures) suppose that m is a family of measures μ on some (ω,a) having μ μ0 for some σ-finite measure (ω,a, μ0). denote this by m μ0,"
504,1,"['distribution', 'probability distribution', 'probability']", Modes of Convergence,seg_31,"and say that m is dominated by μ0. show that there exists a probability distribution p0 on (ω,a) for which μ p0 for all μ ∈ m; that is, for which m p0. (note definition 4.1.3.)"
505,1,"['probability measures', 'probability', 'variation']", Modes of Convergence,seg_31,"exercise 5.20 ∗(berger) let p denote a collection of probability measures p on the measurable space (ω,a). suppose a = σ[c] for some countable collection c; that is, a is countably generated. let dtv denote the total variation metric on p; see exercise 4.2.10 below. show that"
506,1,[], Modes of Convergence,seg_31,"(30) p is dominated if and only if (p, dtv ) is a separable metric space."
507,1,['distributions'], Modes of Convergence,seg_31,"(for example, let p denote all poisson(λ) distributions on 0, 1, 2, . . . having λ > 0. the countable collection of distributions with λ rational is dense in (p, dtv ).) (hint. use the previous exercise.)"
508,1,"['function', 'set']", Modes of Convergence,seg_31,"exercise 5.21 ∗(lehmann) suppose that (ω,a, μ) is σ-finite and a is countably generated. let φ denote the set of all a-measurable φ for which 0 ≤ φ(ω) ≤ 1 for all ω ∈ ω. consider an arbitrary sequence φn ∈ φ. show that a subsequence n′ and a function φ ∈ φ must exist for which"
509,1,"['function', 'interval', 'limit']", Introduction,seg_35,"in a typical calculus class the derivative f ′(x) of a function f at x is defined as the limit of the difference quotients [f (x+h)−f (x)]/h as h → 0. one of the major theorems encountered is then the fundamental theorem of calculus that expresses f as the integral of its derivative (with this result formulated on some interval [a, b] with respect to ordinary lebesgue measure dλ = dx). we can thus write f (x) − f (a) = ∫a"
510,1,['hypothesis'], Introduction,seg_35,x f ′(y) dy under appropriate hypothesis on f .
511,1,['probability'], Introduction,seg_35,in the context of an elementary probability class we let f ≡ f ′ and rewrite the fundamental
512,1,['events'], Introduction,seg_35,"(1) p (a) = ∫a f(y) dy for all events a of the form [a, x]."
513,1,['function'], Introduction,seg_35,"let us now turn this order around and begin by defining one function φ as the “indefinite integral” of another function x, and do it on an arbitrary measure space (ω,a, μ). thus for a fixed x ∈ l1(ω,a, μ), define"
514,0,[], Introduction,seg_35,"as in exercise 3.2.6 (and as example 1.1 will show), if x ≥ 0 then this φ is a measure on (ω,a). in general φ(a) ≡ ∫a x dμ = ∫a x+ dμ−∫a x− dμ is the difference of two measures, and is thus called a “signed measure.” as (2) suggests, we can think of x as a derivative of the signed measure φ with respect to the measure μ. this is the so called “radon–nikodym derivative.” in this context it is possible to formulate important general questions that have clean conclusions via straight forward and/or clever proofs. this is done in section 4.1 and section 4.2, and this gives us most of what we need as we go forward. but before going on, in section 4.3 and section 4.4 we relate this new approach back to the more familar approach represented by (1). of course, the f in (1) must equal the radon–nikodym derivative (viewed in the new context); but much is gained by this new perspective."
515,1,"['disjoint', 'case', 'set', 'function']", Decomposition of Signed Measures,seg_37,"definition 1.1 (signed measure) a signed measure on a σ-field (or a field) a is a set function φ : a → (−∞,+∞] for which φ(∅) = 0 and φ(∑ an) = ∑ φ(an) for all countable disjoint sequences of an’s in a (requiring ∑ an in a in the case of a field). when additivity is required only for finite unions, then φ is called a finitely additive (f.a.) signed measure. (if φ ≥ 0, then φ is a measure or f.a. measure.) if |φ(ω)| < ∞, then φ is called finite."
516,1,[], Decomposition of Signed Measures,seg_37,"implies that φ(b) and φ(a \ b) are both finite numbers. (ii) let a+n equal an or ∅ as φ(an) is ≥ 0 or < 0. and let a−n equal an or ∅ as φ(an) is ≤ 0 or > 0. then ∑ φ(a+n ) = φ(∑ a+n ) < ∞ by (i), since ∑a+n ⊂ ∑ an. likewise, ∑ φ(a−n ) = φ(∑ a−n ). now, convergent series of numbers in [0,∞) may be rearranged at will. thus ∑ |φ(an)| = ∑ φ(a+n ) − ∑ φ(a−n ) is finite."
517,0,[], Decomposition of Signed Measures,seg_37,example 1.1 (the prototypical example) let x be measurable. then
518,0,[], Decomposition of Signed Measures,seg_37,thus φ is a signed measure.
519,1,['sets'], Decomposition of Signed Measures,seg_37,"note that |φ(a)| = |∫a x| ≤ ∫a |x| ≤ ∫ |x| < ∞ for all a, if x ∈ l1. let ω ≡ ∑n ωn be a measurable decomposition for the σ-finite μ. then the sets ωnm ≡ ωn ∩ [m ≤ x < m + 1] and ωn,±∞ ≡ ωn ∩ [x = ±∞], for n ≥ 1 and for all integers m, is a decomposition showing φ to be σ-finite."
520,1,"['continuous', 'case']", Decomposition of Signed Measures,seg_37,"definition 1.2 (continuous signed measure) a signed measure φ is continuous from below (above) if φ(lim an) = lim φ(an) for all an ↗ (for all an ↘, with at least one φ(an) finite). we call φ continuous in case it is continuous both from below and from above."
521,1,['continuous'], Decomposition of Signed Measures,seg_37,"proposition 1.2 (continuity of signed measures) a signed measure on either a field or a σ-field is countably additive and continuous. conversely, if a finitely additive signed measure on either a field or σ-field is either continuous from below or is finite and continuous from above at ∅, then it is a countably additive signed measure."
522,0,[], Decomposition of Signed Measures,seg_37,proof. this result has nearly the same proof as does the corresponding result for measures; see proposition 1.1.4.
523,0,[], Decomposition of Signed Measures,seg_37,"exercise 1.1 (a) actually write out all details of the proof of proposition 1.2. (b) if φ and ψ are signed measures, then so is φ + ψ."
524,1,['events'], Decomposition of Signed Measures,seg_37,"theorem 1.1 (jordan–hahn decomposition) let φ be a signed measure on the measurable space (ω,a), having events a. then ω can be decomposed into events as ω = ω+ +ω−, where"
525,1,"['events', 'set']", Decomposition of Signed Measures,seg_37,"(2) ω+ is a positive set for φ, in that φ(a) ≥ 0 for all events a ⊂ ω+,"
526,1,"['events', 'set']", Decomposition of Signed Measures,seg_37,"(3) ω− is anegative set for φ, in that φ(a) ≤ 0 for all events a ⊂ ω−."
527,0,[], Decomposition of Signed Measures,seg_37,"trivially, we obtain measures on the measurable space (ω,a) via the definitions"
528,1,"['associated', 'variation']", Decomposition of Signed Measures,seg_37,"with φ+ a measure and φ− a finite measure on (ω,a). of course, φ+(ω−) = 0 and φ−(ω+) = 0. we will call φ+, φ−, and |φ|(·) ≡ φ+ +φ− the positive part, the negative part, and the total variation measure associated with φ; thus"
529,1,['variation'], Decomposition of Signed Measures,seg_37,"(5) |φ|(·) ≡ φ+(·) + φ−(·) is the total variation measure on (ω,a),"
530,0,[], Decomposition of Signed Measures,seg_37,"moreover, the following relationships hold:"
531,1,['set'], Decomposition of Signed Measures,seg_37,"consider claims (2) and (3). let b denote some set having φ(b) < 0. [that φ(b) > −∞ is crucial; this proof will not work on the positive side.] (if no such set exists, let ω+ ≡ ω, giving |φ| = φ+ = φ and φ− ≡ 0.) we now show that"
532,1,['set'], Decomposition of Signed Measures,seg_37,(a) b contains a negative set c.
533,1,"['disjoint', 'sets', 'set', 'disjoint sets']", Decomposition of Signed Measures,seg_37,"if b is a negative set, use it for c. if not, then we will keep removing sets ak with φ(ak) > 0 from b until only a negative set c is left. we will remove disjoint sets ak with φ(ak) ≥ 1"
534,1,['sets'], Decomposition of Signed Measures,seg_37,"1 as many times as we can, then sets with φ(ak) ≥ as many times as we can, . . . . to this"
535,1,"['process', 'union']", Decomposition of Signed Measures,seg_37,"1 (if (n1, n2, . . .) = (1, 1, 1, 1, 2, .. then some ak has φ(ak) ≥ 1 for 1 ≤ k ≤ 4, 2 ≤ φ(ak) < 1 for k = 5, ...) let c ≡ b\∑k ak, where the union is infinite (unless the process of choosing nk’s terminates); note that only finitely many ak exist for each 1/i [else proposition 1.1(i) would be violated]. the c.a. of φ then gives"
536,1,['set'], Decomposition of Signed Measures,seg_37,"moreover, c is a negative set, since no subset can have measure exceeding 1/i for any i. now we know that we have at least one negative set. so we let"
537,1,"['set', 'set ']", Decomposition of Signed Measures,seg_37,"(d) d ≡ inf{φ(c) : c is a negative set } < 0, and define ω− ≡ ∪kck,"
538,1,"['sets', 'set']", Decomposition of Signed Measures,seg_37,"where ck denotes a sequence of negative sets for which φ(ck) ↘ d. replace these ck by c̃k ≡ ∪1kcj ; these are ↗ sets with ω− = ∪kc̃k and with φ(c̃k) ↘ d. now, ω− is also a negative set (else one of the ck’s would not be), and thus φ(ω−) ≥ d, because it must exceed the infimum of such values. but φ(ω−) ≤ d also holds, since φ(ω−) = φ(c̃k) + φ(ω−\c̃k) ≤ φ(c̃k) for all k gives φ(ω−) ≤ d. thus φ(ω−) = d; so, d must be finite. then ω+ is a positive set, since if φ(a) < 0 for some a ⊂ ω+, then the set ω− ∪ a would have φ(a ∪ ω−) < d (which is a contradiction)."
539,1,['set'], Decomposition of Signed Measures,seg_37,"exercise 1.3 the set ω+ is essentially unique, in that if ω+1 and ω+2 both satisfy the theorem, then |φ|(ω+1 ω+2 ) = 0."
540,1,"['continuous', 'set']", Decomposition of Signed Measures,seg_37,"definition 1.3 (absolute continuity of measures) let μ and φ denote a measure and a signed measure on a σ-field a. call φ absolutely continuous with respect to μ, denoted by φ μ, if φ(a) = 0 for each a ∈ a having μ(a) = 0. we say φ is singular with respect to μ, denoted by φ ⊥ μ, if there exists a set n ∈ a for which μ(n) = 0 while |φ|(n c) = 0."
541,0,[], Decomposition of Signed Measures,seg_37,"theorem 1.2 (lebesgue decomposition) let μ denote any σ-finite measure on the measurable space (ω,a). let φ be any other σ-finite signed measure on this space (ω,a). then there exists a unique decomposition of φ with respect μ as"
542,1,['function'], Decomposition of Signed Measures,seg_37,"for some finite a-measurable function z0, which is unique a.e. μ."
543,0,[], Decomposition of Signed Measures,seg_37,"proof. by σ-finiteness and the jordan–hahn decomposition, we need only give the proof if μ and φ are finite measures; just separately consider φ+"
544,1,['joint'], Decomposition of Signed Measures,seg_37,"ωn(n = 1, 2, . . .) for a joint σ-finite decomposition ω = ∑∞"
545,0,[], Decomposition of Signed Measures,seg_37,1 ωn of μ and |φ|. (to give the details would be pedantic.) we now establish the existence of the decomposition in the reduced problem when φ and μ are finite measures. let
546,0,[], Decomposition of Signed Measures,seg_37,case 1. φ μ: the first step is to observe that
547,0,[], Decomposition of Signed Measures,seg_37,thus (b) holds. now choose a sequence zn ∈ z such that
548,1,['functions'], Decomposition of Signed Measures,seg_37,we may replace zn by z̃n ≡ z1 ∨ · · · ∨ zn in (d). these z̃n in (d) are an ↗ sequence of functions. then let z0 ≡ lim z̃n. the mct then gives (for any a ∈ a)
549,1,"['set', 'null set']", Decomposition of Signed Measures,seg_37,(redefine z0 on a null set so that it is always finite.)
550,0,[], Decomposition of Signed Measures,seg_37,"then φac is a finite measure, which can be seen by applying example 1.1 with c finite; and φac μ. moreover,"
551,1,['case'], Decomposition of Signed Measures,seg_37,"(since z0 ∈ z), so that φs is a finite measure by exercise 1.1. if φs(ω) = 0, then φ = φac and we are done, with φs ≡ 0. (in the next paragraph we verify that φs ≡ 0 always holds in case 1; that is, we will verify that φs(ω) = 0.)"
552,1,['case'], Decomposition of Signed Measures,seg_37,"implies that φs(ω+) = 0 (since φs = φ − φac μ, as φ μ is assumed for case 1 and as φac μ is obvious from example 4.1.1). but φs(ω+) = 0 contradicts (i) by implying that"
553,1,['inequality'], Decomposition of Signed Measures,seg_37,"thus (j) must also hold, under the assumption made above that inequality (i) holds.) now, φs(aω+) ≥ θμ(aω+) (by the definition of ω+ below (i)). thus (as φs ≥ 0 by (h) gives the inequality φs(aω−) ≥ 0),"
554,1,['set'], Decomposition of Signed Measures,seg_37,≥ ∫a z0 dμ + θμ(aω+) as ω+ is a positive set for φ∗ ≡ φs − θμ
555,1,['case'], Decomposition of Signed Measures,seg_37,"this implies both zθ ≡ z0 + θ1ω+ ∈ z and ∫ω zθ dμ = c + θμ(ω+) > c. but this is a contradiction. thus φs(ω) = 0. thus φ equals φac and satisfies (8), and the theorem holds in case 1. the a.s. μ uniqueness of z0 follows from exercise 3.2.2. (this also establishes the radon–nikodym theorem below.)"
556,1,['case'], Decomposition of Signed Measures,seg_37,"ocase 2. general φ: let ν ≡ φ + μ, and note that both φ ν and μ ν. then by case 1 we can infer that"
557,1,['functions'], Decomposition of Signed Measures,seg_37,"for finite ν-integrable functions x ≥ 0 and y ≥ 0 that are unique a.e. ν. let d ≡ {ω : y (ω) = 0}, and then dc = {ω : y (ω) > 0}. define"
558,1,[], The RadonNikodym Theorem,seg_39,recall that the absolute continuity φ μ means that
559,1,['function'], The RadonNikodym Theorem,seg_39,"theorem 2.1 (radon–nikodym) suppose both the signed measure φ and the measure μ are σ-finite on a measurable space (ω,a). then φ μ if and only if there exists uniquely a.e. μ a finite-valued a-measurable function z0 on ω for which"
560,0,[], The RadonNikodym Theorem,seg_39,"moreover, φ is finite if and only if z0 is integrable."
561,1,['function'], The RadonNikodym Theorem,seg_39,the function z0 of (2) is often denoted by [d
562,0,[], The RadonNikodym Theorem,seg_39,also have the following very suggestive notation:
563,0,[], The RadonNikodym Theorem,seg_39,we call z0 the radon–nikodym derivative (or the density) of φ with respect to μ.
564,1,['sufficiency'], The RadonNikodym Theorem,seg_39,proof. the lebesgue decomposition theorem shows that such a z0 necessarily exists. the sufficiency is just the trivial example 4.1.1. the “moreover” part is also a trivial result.
565,1,"['variable', 'change of variable']", The RadonNikodym Theorem,seg_39,"theorem 2.2 (change of variable theorem) let μ ν where μ and ν are σ-finite measures on (ω,a). if ∫ x dμ has a well-defined value in [−∞,∞], then"
566,1,"['results', 'case']", The RadonNikodym Theorem,seg_39,one useful special case results from
567,0,[], The RadonNikodym Theorem,seg_39,"g dh for a generalized df h,"
568,1,['case'], The RadonNikodym Theorem,seg_39,"proof. case 1. x = 1b , for b ∈ a: then the radon–nikodym theorem gives"
569,1,['case'], The RadonNikodym Theorem,seg_39,"1 ci1bi , for a partition bi: case 1 and linearity of the integral give"
570,1,['functions'], The RadonNikodym Theorem,seg_39,case 3. x ≥ 0: let xn ≥ 0 be simple functions that ↗ to x. then the mct twice gives
571,1,"['change of variable', 'statistician', 'variable']", The RadonNikodym Theorem,seg_39,"note that theorem 3.2.6 (of the unconscious statistician) is another change of variable theorem. that is, if x : (ω,a) → (ω̄, ā) and g : (ω̄, ā) → (r̄, b̄), then"
572,0,[], The RadonNikodym Theorem,seg_39,when one of the these integrals is well-defined. (see also exercise 6.3.3 below.)
573,1,['distribution'], The RadonNikodym Theorem,seg_39,"exercise 2.2 let pμ,σ2 denote the n(μ, σ2) distribution. let p have the density f ≡ [dp/dλ] with respect to lebesgue measure λ for which f > 0."
574,1,"['distribution', 'cauchy', 'cauchy distribution']", The RadonNikodym Theorem,seg_39,"(a) show that λ p with density 1/f. (b) show that pμ,1 p0,1 and compute [dpμ,1/dp0,1]. (c) show that p0,σ2 p0,1 and compute [dp0,σ2/dp0,1]. (d) compute [dp/dp0,1] and [dp0,1/dp ] when p denotes the cauchy distribution."
575,1,"['poisson', 'results', 'distribution', 'outcome', 'tails', 'distributions']", The RadonNikodym Theorem,seg_39,"exercise 2.3 flip a coin. if heads results, let x be a uniform(0, 1) outcome; but if tails results, let x be a poisson (λ) outcome. the resulting distribution on r is labeled φ. (a) let μ denote lebesgue measure on r. find the lebesgue decomposition of φ with respect to this μ; that is, write φ = φac + φs. (b) let μ be counting measure on {0, 1, 2, . . .}. find the lebesgue decomposition of φ with respect to this μ. (if need be, see the definitions of various distributions in chapter 9.)"
576,1,['function'], The RadonNikodym Theorem,seg_39,"exercise 2.4 let μ be a σ-finite measure on (ω,a). define φ(a) ≡ ∫a x dμ for all a ∈ a for some μ-integrable function x. show that"
577,0,[], The RadonNikodym Theorem,seg_39,"exercise 2.5 (alternative definition of absolute continuity) let φ be finite and let μ be σ-finite, for measures on (ω,a). then φ μ if and only if for every > 0 there exists δ > 0 such that μ(a) < δ implies |φ|(a) < . show that if φ is not finite, then the claim could fail (even if μ is a finite measure); give an example."
578,1,"['variables', 'change of variables']", The RadonNikodym Theorem,seg_39,"these can be thought of as theorems about radon–nikodym derivatives, about absolute continuity of measures, or about change of variables."
579,1,"['cardinality', 'set', 'case']", The RadonNikodym Theorem,seg_39,exercise 2.8 let a denote the collection of all subsets a of an uncountable set ω for which either a or ac is countable. let μ(a) denote the cardinality of a. define φ(a) to equal 0 or ∞ according as a is countable or uncountable. show that φ μ. then show that the radon–nikodym theorem fails in this non σ-finite case.
580,1,"['functions', 'case']", The RadonNikodym Theorem,seg_39,"for measurable functions f and g with g ∈ l+1 (μ + ν). (note example 4.1.1.) (c) determine φ+, φ−, and |φ|; and determine |φ|(ω) in case μ is also a finite measure."
581,1,"['probability', 'probability measures', 'variation']", The RadonNikodym Theorem,seg_39,"exercise 2.10 (total variation distance between probability measures) define p and q to be probability measures on (ω,a). (a) show that the total variation distance dtv (p,q) between p and q satisfies"
582,1,"['probability', 'probability measures']", The RadonNikodym Theorem,seg_39,"exercise 2.11 (hellinger distance between probability measures) let p and q denote probability measures on (ω,a). define the hellinger distance h(p,q) by"
583,0,[], The RadonNikodym Theorem,seg_39,"for any measure μ dominating both p and q. show that the choice of dominating measure μ does not affect the value of h(p,q). (note section 14.2 below.)"
584,0,[], The RadonNikodym Theorem,seg_39,exercise 2.12 let φ be a σ-finite signed measure. define
585,1,[], The RadonNikodym Theorem,seg_39,"exercise 2.13 let (ω,a) be a measurable space, and let m denote the collection of all finite signed measures μ on (ω,a). let ‖μ‖ ≡ |μ|(ω). thus ‖μ1 − μ2‖ = |μ1 − μ2|(ω). show that (m, ‖· is a complete metric space."
586,1,['function'], The RadonNikodym Theorem,seg_39,"finally, there exists an integrable function x (that is unique a.e. μ) for which"
587,1,['likelihood'], The RadonNikodym Theorem,seg_39,"exercise 2.15 (likelihood ratios) let p and q denote any two measures on some σ-finite measure space (ω,a, μ). suppose that p << μ and q << μ. show that"
588,1,"['probability measures', 'discrete', 'set', 'probability', 'continuous', 'statistical']", The RadonNikodym Theorem,seg_39,"except on a μ-null set (which is also a (p + q)-null set). this means that the “likelihood ratio” on the right hand side of (14) (that appears in various statistical settings) can always be replaced by the one on the left hand side. (in a statistical situation where p and q are probability measures, the right hand side is always defined—even if, say, p is absolutely continuous and q is discrete.)"
589,1,['function'], Lebesgues Theorem,seg_41,"theorem 3.1 (lebesgue) (a) suppose f is any ↗ function on [a, b]. then f has an integrable derivative f ′ that exists and is finite a.e. λ on [a, b]."
590,0,[], Lebesgues Theorem,seg_41,proof. ∗consider the dini derivates
591,0,[], Lebesgues Theorem,seg_41,"trivially, d+f (x) ≥ d+f (x) and d−f (x) ≥ d−f (x). all four derivates having the same finite value is (of course) the definition of f being differentiable at x, with the common value called the derivative of f at x and being denoted by f ′(x). let"
592,1,"['disjoint', 'intervals', 'set', 'union']", Lebesgues Theorem,seg_41,"where the union is over all rational r and s. to show that λ(a) = 0, it suffices to show that all ars have outer lebesgue measure zero, in that λ∗(ars) = 0. to this end, let u be an open set for which ars ⊂ u with λ(u) < λ∗(ars) + . for each x ∈ ars we can specify infinitely many and arbitrarily small h for which [x − h, x] ⊂ u and [f (x) − f (x − h)]/h < r. this collection of closed intervals covers ars in the sense of vitali (see exercise 1.2.8). thus some finite disjoint collection of them has interiors i1 ≡ (x1 − h1, x1), . . . , im ≡ (xm − hm, xm) for which brs ≡ ars ∩ (∑m"
593,1,['disjoint'], Lebesgues Theorem,seg_41,"for each y ∈ brs we can specify infinitely many and arbitrarily small h for which [y, y + h] ⊂ (some ii) and [f (y +h)−f (y)]/h > s. this collection covers brs in the sense of vitali. thus some finite disjoint collection of them has interiors j1 ≡ (y1, y1 + h1), . . . , jn ≡ (yn, yn + hn) for which crs ≡ brs ∩ (∑j"
594,1,"['disjoint', 'results', 'union']", Lebesgues Theorem,seg_41,"moreover, since the disjoint union of the jj ’s is a subset of the disjoint union of the ii’s, results (b) and (c) yield"
595,1,['function'], Lebesgues Theorem,seg_41,important: read this paragraph. the measurable function difference quotients
596,1,['inequality'], Lebesgues Theorem,seg_41,"thus f ′ is integrable, and hence f ′ is also finite a.e. λ. (we now present this last fact as a corollary, since situations with strict inequality are very revealing.)"
597,1,['function'], Lebesgues Theorem,seg_41,"corollary 1 (a) suppose f is an ↗ function on [a, b], with −∞ ≤ a ≤ b ≤ ∞. then f ′ exists a.e. λ and"
598,1,"['disjoint', 'discrete', 'intervals', 'distribution', 'continuous', 'discrete distribution']", Lebesgues Theorem,seg_41,"the lebesgue singular df in example 6.1.1 below will show that equality need not hold in equation (1); this continuous df is constant valued on a collection of disjoint intervals of total length 1. (an example in hewitt and stromberg (1965, p. 278) shows that f ′(x) = 0 is possible for a.e. x, even with a ↑ f .) equality also fails for any discrete distribution that places any mass in (a, b]. this is the point of the following exercise."
599,1,"['continuous', 'discrete', 'distribution', 'discrete distribution', 'distributions']", Lebesgues Theorem,seg_41,"exercise 3.1 (distributions can be discrete, singular, or absolutely continuous) (a) let fd denote the discrete distribution on [0, 1] that puts mass 1/(n + 1) at each of the n + 1 points i/n for 0 ≤ i ≤ n. graph fd, and calculate fd"
600,0,[], Lebesgues Theorem,seg_41,′ from your graph. then show
601,1,"['set', 'associated']", Lebesgues Theorem,seg_41,(b) let fc denote the lebesgue singular df associated with the cantor set; see example 6.1.1 below. what is the value of fc
602,1,['function'], Lebesgues Theorem,seg_41,"n =1 gk(x) converges at x = a and x = b. then sn(x) → s(x) for all x in [a, b], for some finite-valued measurable function s(x). mainly, s′(·) exists a.s. λ and is given by"
603,1,[], Lebesgues Theorem,seg_41,corollary 1 if the power series s(x) ≡ ∑n
604,0,[], Lebesgues Theorem,seg_41,proof. note that sn(a) is a convergent sum. now write
605,1,"['convergence', 'interval']", Lebesgues Theorem,seg_41,"since ↗ sequences bounded above converge, the convergence at x = a and x = b gives convergence at all x in the interval. we may replace gk(x) − gk(a) by gk(x) and then assume all gk ≥ 0 on [a, b] with gk(a) = 0. since s and all sn are ↗, the derivatives s′ and all sn"
606,0,[], Lebesgues Theorem,seg_41,both essentially follow from
607,1,[], Lebesgues Theorem,seg_41,where the series in (e) has summands
608,0,[], Lebesgues Theorem,seg_41,"thus conclusion (c) also applies to these hi’s (not just the gk’s), and we conclude from (c) that the series"
609,1,[], Lebesgues Theorem,seg_41,"but a series of real numbers can converge only if its nth term goes to 0; that is,"
610,0,[], Lebesgues Theorem,seg_41,"as noted above, this suffices for the theorem."
611,0,[], Lebesgues Theorem,seg_41,exercise 3.2 prove the corollary.
612,0,[], Lebesgues Theorem,seg_41,example 3.1 (taylor’s expansion) suppose g(·) is defined in a neighborhood of a. let x∗ denote a point somewhere between x and a. let
613,1,['representations'], Lebesgues Theorem,seg_41,"thus we find it useful to use the representations (with g(k)(a) abbreviating that g(k)(·) exists at a, and with g(k)(·) abbreviating that g(k)(x) exists for all x in a neighborhood of a)"
614,0,[], Lebesgues Theorem,seg_41,exercise 3.4* prove the vitali covering theorem. (see exercise 1.2.8.)
615,1,"['likelihood ratio', 'likelihood', 'interval']", Lebesgues Theorem,seg_41,"∞ akxk/∑1 ∞ bkxk in some interval. suppose that all ak, bk > 0 and ak/bk ↑. then f ′(x) > 0 for all x in that interval. (this result is useful in conjunction with the monotone likelihood ratio principle.)"
616,1,"['function', 'variation']", The Fundamental Theorem of Calculus,seg_43,"definition 4.1 (bounded variation) let f denote a real-valued function on [a, b]. the total variation of f over [a, b] is defined by"
617,1,['variation'], The Fundamental Theorem of Calculus,seg_43,"we say that f is of bounded variation (bv ) on [a, b] if vabf < ∞. (note the vabf is a measure of the “total amount of wiggle” of f over [a, b].)"
618,0,[], The Fundamental Theorem of Calculus,seg_43,it is clear that
619,1,"['function', 'functions', 'continuous']", The Fundamental Theorem of Calculus,seg_43,definition 4.2 (absolutely continuous functions) a real-valued function f on any subinterval i of the line r is said to be absolutely continuous if for all > 0 there exists a δ > 0 such that
620,1,['disjoint'], The Fundamental Theorem of Calculus,seg_43,"with n ≥ 1 and with disjoint subintervals (ck, dk] contained in i. (this implies that the “wiggle” of f over a combined length must be small if f is not given much combined length in which to wiggle.)"
621,1,"['function', 'condition']", The Fundamental Theorem of Calculus,seg_43,definition 4.3 (lipschitz condition) a real-valued function f on any subinterval i of r is said to be lipschitz if for some finite constant m we have
622,1,"['associated', 'condition', 'variation']", The Fundamental Theorem of Calculus,seg_43,"we first establish some elementary relationships among the lipschitz condition, absolute continuity, bounded variation, and the familiar property of being ↗. these concepts have proven to be important in the study of differentiation. we will soon proceed further in this direction, and we will also consider the relationship between ordinary derivatives and radon–nikodym derivatives. we first recall from theorem 1.3.1 (the correspondence theorem) that every generalized df f can be associated with a lebesgue-stieltjes measure μf via μf ((a, b]) ≡ f (b) − f (a)."
623,1,"['functions', 'continuous']", The Fundamental Theorem of Calculus,seg_43,"(ii) if f is absolutely continuous, then it is of bv. the f1 and f2 in (i) are both absolutely continuous and ↗. (iii) lipschitz functions are absolutely continuous."
624,0,[], The Fundamental Theorem of Calculus,seg_43,"since f1 and f2 are ↗, their derivatives f1"
625,1,['continuous'], The Fundamental Theorem of Calculus,seg_43,"consider (ii). let f (·) be absolutely continuous. we will first show that such an f is of bv. let = 1 with its δ1, and choose n so large that the equally spaced values a ≡ x0 < x1 < · · · < xn ≡ b have mesh ≡ (b − a)/n < δ1. then (2) yields"
626,1,['continuous'], The Fundamental Theorem of Calculus,seg_43,"and thus f is of bv. we must still show that f1 is absolutely continuous if f is. so we suppose that f is absolutely continuous, and specify that ∑n"
627,1,['continuous'], The Fundamental Theorem of Calculus,seg_43,"1 (dk − ck) < δ /2 for some choice of n, ck’s, and dk’s. we now use these same n, ck, dk to verify that f1 is absolutely continuous. well, for each fixed k with 1 ≤ k ≤ n and the tiny number /(2n), the definition of the bv of f gives"
628,0,[], The Fundamental Theorem of Calculus,seg_43,"by absolute continuity of f , since it follows from above that"
629,0,[], The Fundamental Theorem of Calculus,seg_43,consider (iii). being lipschitz implies absolute continuity with δ = /m.
630,1,['variation'], The Fundamental Theorem of Calculus,seg_43,verify that f − f (a) = f+ − f− with f+ and f− both ↗ (an alternative to (5)). (note how the f1 of proposition 4.1i corresponds to the total variation measure.)
631,1,['continuous'], The Fundamental Theorem of Calculus,seg_43,"exercise 4.2 let f be continuous on [a, b], and define f (x) = ∫a"
632,1,['continuous'], The Fundamental Theorem of Calculus,seg_43,"x f(y) dy for each a ≤ x ≤ b. then f is differentiable at each x ∈ (a, b) and f ′ = f on (a, b). (since f is continuous, we need only the riemann integral. can we extend this to the lebesgue integral? can we reverse the order, and first differentiate and then integrate? the next theorem answers these important questions.)"
633,1,['continuous'], The Fundamental Theorem of Calculus,seg_43,"theorem 4.1 (fundamental theorem of calculus) (i) let f be absolutely continuous on [a, b], and let λ denote lebesgue measure. then f ′ exists a.e. λ and"
634,0,[], The Fundamental Theorem of Calculus,seg_43,"x f dλ for some f that is integrable with respect to λ on [a, b], then f"
635,1,['continuous'], The Fundamental Theorem of Calculus,seg_43,"is absolutely continuous on [a, b]. moreover, f = f ′ = [dμf ] a.e. λ."
636,1,"['continuous', 'associated']", The Fundamental Theorem of Calculus,seg_43,"remark 4.1 (a) the fundamental theorem of calculus can be summarized by saying that f is absolutely continuous if and only if it is the integral of its derivative. the ordinary derivative f ′ is, in fact, also a radon–nikodym derivative of the signed measure μf naturally associated with f ; see the proof of theorem 4.2 below. (b) if f is ↗ on [a, b], then the derivative f ′ exists a.e. λ on [a, b] and is integrable with"
637,1,"['discrete', 'condition', 'inequality']", The Fundamental Theorem of Calculus,seg_43,"∫a singular df f of (6.1.9) below yields a strict inequality, as does any discrete dstribution on (a, b]; recall exercise 4.3.2. (c) the lipschitz condition represents “niceness with a vengeance,” as it guarantees that all difference quotients are uniformly bounded."
638,1,"['continuous', 'variation']", The Fundamental Theorem of Calculus,seg_43,"x f(y) dy for a ≤ x ≤ b, then f is absolutely continuous by the absolute continuity of the integral theorem. then f is of bounded variation on [a, b] and f ′ exists a.e. λ in [a, b], by proposition 4.1(ii). moreover, f ′ is integrable, using (4.3.1). but does f ′ = f a.e. λ?"
639,1,['loss'], The Fundamental Theorem of Calculus,seg_43,"case 1: suppose |f | is bounded by some finite m on [a, b]. we could consider f+ and f− separately, but we will simply assume without loss of generality that f ≥ 0. then the"
640,1,['function'], The Fundamental Theorem of Calculus,seg_43,"x+1/n f(y) dy of f also satisfies |dnf | ≤ m on (a, b), and dnf (x) → f ′(x) a.e. applying the dct (with dominating function identically equal to m) once for each fixed x ∈ (a, b) gives"
641,1,['case'], The Fundamental Theorem of Calculus,seg_43,"x fn has derivative fn a.e. on [a, b], by case 1. thus"
642,1,"['continuous', 'loss']", The Fundamental Theorem of Calculus,seg_43,"consider the direct half when f is absolutely continuous on [a, b]. without loss, suppose that f is ↗ (by proposition 4.1(ii)), so that f ′ exists a.e. on [a, b] (see theorem 4.3.1) and that f ′ is integrable (see (4.3.1)). use"
643,0,[], The Fundamental Theorem of Calculus,seg_43,and the correspondence theorem to associate a lebesgue–stieltjes measure μf with f (which is a generalized df). we will show that μf << λ in theorem 4.2 below. then the radon– nikodym will give
644,0,[], The Fundamental Theorem of Calculus,seg_43,"now apply the converse half of the fundamental theorem of calculus to conclude that f ′ = f ≡ [dμf /dλ] a.e. on [a, b]."
645,1,"['continuous', 'densities']", The Fundamental Theorem of Calculus,seg_43,"theorem 4.2 (densities) (i) let f be ↗ and absolutely continuous on some subinterval [a, b] of r. then the lebesgue-stieltjes measure μf (as in (e) above) satisfies both μf λ (name its radon–nikodym derivative [dμf /dλ]), and also"
646,1,"['continuous', 'interval']", The Fundamental Theorem of Calculus,seg_43,"(ii) mainly, let f be absolutely continuous on every finite subinterval of i (with i any fixed interval in r), and fix a anywhere in i. then (7) holds for all x in i."
647,1,['interval'], The Fundamental Theorem of Calculus,seg_43,"proof. (i) let μ ≡ μf and fix the finite interval [a, b]. given > 0, let δ > 0 be as in the definition (3) of absolute continuity. let a ∈ b be a subset of [a, b] having λ(a) < δ /2. recalling our definition (1.2.1) of λ via carathéodory coverings, we can claim that lebesgue measure satisfies"
648,1,"['disjoint', 'intervals', 'sets', 'union', 'disjoint sets']", The Fundamental Theorem of Calculus,seg_43,"we replaced the an’s of (1.2.1) by the disjoint sets bn ≡ anacn−1 · · · ac2ac1. note that each bn is in cf , and thus equals a finite union of intervals of the type (c, d], while bn+1 then adds at most a finite number of additional such intervals. thus"
649,0,[], The Fundamental Theorem of Calculus,seg_43,thus (using absolutely continity of f to obtain the first in (d))
650,1,"['continuous', 'interval', 'intervals']", The Fundamental Theorem of Calculus,seg_43,"1 (dn − cn) < δ . thus μf (a) < when λ(a) < δ /2, so that μf (a) = 0 whenever λ(a) = 0. now apply radon–nikodym to obtain the f exhibited in (7). (ii) now, let f be ↗ and absolutely continuous on all finite subintervals of i, and fix any a in the interval i. applying (7) to finite intervals in ≡ [an, bn] ↗ i, the mct gives μf (a) = 0 whenever λ(a) = 0, for any a ∈ b ∩ i. thus μf λ."
651,1,"['function', 'continuous']", The Fundamental Theorem of Calculus,seg_43,"exercise 4.3 (absolutely continuous dfs) let f be ↗, right continuous and bounded on r, with f (−∞) = 0. define μf via μf ((a, b]) = f (b)−f (a) for all a < b. show that μf λ if and only if f is an absolutely continuous function on r."
652,1,"['function', 'functions', 'continuous']", The Fundamental Theorem of Calculus,seg_43,"exercise 4.4 (a) show that the composition g(h) of two absolutely continuous functions is absolutely continuous when h is monotone. (b) show that g(h) need not be absolutely continuous without restrictions on h. (c) define a continuous function on [0, 1] that is not absolutely continuous. (d) the functions g + h and g · h are absolutely continuous when both f and g are."
653,1,['continuous'], The Fundamental Theorem of Calculus,seg_43,"exercise 4.5* suppose that h : [a, b] → (0,∞) is absolutely continuous on [a, b]. show that log h is also absolutely continuous on [a, b]."
654,1,"['functions', 'null sets', 'continuous', 'sets', 'function']", The Fundamental Theorem of Calculus,seg_43,"exercise 4.6* (another characterization of absolute continuity) (a) f is lipschitz on [a, b] iff f is differentiable a.e. λ on [a, b] with f ′ bounded. (b) absolutely continuous functions on r map b into b and null sets into null sets. (c) a continuous function of bv is absolutely continuous iff it maps b into b."
655,1,"['densities', 'transformed', 'change of variable', 'variable']", The Fundamental Theorem of Calculus,seg_43,"example 4.1 (change of variable; densities of transformed rvs) let x be a rv on (ω,a, p ) with df fx λ ≡ (lebesgue measure) and density fx . let"
656,1,['continuous'], The Fundamental Theorem of Calculus,seg_43,−1 (8) y ≡ g(x) where g is ↑ and absolutely continuous.
657,1,"['functions', 'continuous']", The Fundamental Theorem of Calculus,seg_43,where the composition fy = fx(g−1) of these absolutely continuous functions is absolutely continuous (by exercise 4.4a). so the fundamental theorem of calculus tells us that fy is the integral of its derivative. we can then compute this derivative from the ordinary chain rule. thus
658,1,"['transformation', 'jacobian']", The Fundamental Theorem of Calculus,seg_43,on the real line. call (d/dy)g−1(y) the jacobian of the transformation.
659,1,"['distribution', 'distributed uniformly']", The Fundamental Theorem of Calculus,seg_43,"exercise 4.7 let ≡ log 1/x where the rv x is distributed uniformly on [0, 1], with df fx(x) equal to 0, x, 1 according as x is in (−∞, x], (0, 1], (1,∞). we say that x has the uniform(0, 1) distribution. determine the df and density of y ≡ log 1/x. it is called the exponential(1) distribution."
660,1,"['change of variable', 'results', 'variable', 'probability']", The Fundamental Theorem of Calculus,seg_43,exercise 4.8 use your exposure to a more elementary version of probability to come up with three more examples of elementary change of variable results of the type presented in the previous exercise.
661,1,['functions'], The Fundamental Theorem of Calculus,seg_43,"exercise 4.9 (specific step functions that are dense in l2) let h ∈ l2([0, 1],b, λ). consider the following two approximations to h(·). let"
662,1,"['functions', 'continuous']", The Fundamental Theorem of Calculus,seg_43,"alternatively, use the fact that the continuous functions are dense in l2.)"
663,1,['sets'], FiniteDimensional Product Measures,seg_47,"here a × a′ ≡ {(ω, ω′) : ω ∈ a, ω′ ∈ a′}, which is called a measurable rectangle. the σ-field a × a′ ≡ σ[f ] is called the product σ-field. (ω × ω′,a × a′) is called the product measurable space. the sets a × ω′ and ω × a′ are called cylinder sets."
664,1,"['disjoint', 'sets', 'union', 'disjoint sets']", FiniteDimensional Product Measures,seg_47,"proposition 1.1 f is a field. see figure 1.1, and write the displayed union as a union of sets disjoint in f0. perhaps, start with decomposing ω into the 22 disjoint sets b1 ∩ · · · ∩ b4, where each bi equals ai or aci (and with 22 analogous b1"
665,1,['disjoint'], FiniteDimensional Product Measures,seg_47,′ ∩ · · · ∩ b4 ′ ). then sum disjoint subsets from the 24 possible cross products.
666,1,"['sets', 'disjoint', 'disjoint sets']", FiniteDimensional Product Measures,seg_47,μ(ai) × ν(a′i) for disjoint sets ai × a′i.
667,0,[], FiniteDimensional Product Measures,seg_47,"then φ is a well-defined and σ-finite measure on the field f . moreover, φ extends uniquely to a σ-finite measure, called the product measure and also denoted by φ, on (ω × ω′,a × a′). even when completed, this measure is still unique and is still referred to as the product measure φ."
668,1,['intervals'], FiniteDimensional Product Measures,seg_47,"proof. (see the following exercise; it mimicks the proof of the correspondence theorem. here, f0 and f play the roles of all finite intervals i and the field cf . although the proof asked for in exercise 1.1 below is “obvious,” it still requires much tedious detail.) we will give a better proof herein very soon."
669,0,[], FiniteDimensional Product Measures,seg_47,"exercise 1.1 verify that φ is well-defined on f0, and that φ is countably additive on f0. then verify that φ is well-defined on f , and that φ is countably additive on f . thus φ is a σ-finite measure on f , so that the conclusion of theorem 1.1 follows from the carathéodory extension of theorem 1.2.1 and its corollary."
670,0,[], FiniteDimensional Product Measures,seg_47,exercise 1.2 ∗ use induction to show that theorem 1.1 extends to n-fold products.
671,0,['n'], FiniteDimensional Product Measures,seg_47,"example 1.1 (lebesgue measure in n dimensions, etc.) (a) we define"
672,1,['sets'], FiniteDimensional Product Measures,seg_47,"to be the n-fold products of the real line r with the borel sets b and of the extended real line r̄ with the σ-field b̄ ≡ σ[b, {+∞}, {−∞}], respectively. recall from example 2.1.1 that bn = σ[un], where un denotes all open subsets of rn. we will refer to both bn and b̄n as the borel sets. (b) let λ denote lebesgue measure on (r,b), as usual. we extend λ to (r̄, b̄) by the convention that λ({+∞}) = 0 and λ({−∞}) = 0. then"
673,0,[], FiniteDimensional Product Measures,seg_47,provides us with a definition of n-dimensional lebesgue measure λn as the natural generalization of the concept of volume. it is clear that
674,1,"['disjoint', 'approximation', 'case', 'sets']", FiniteDimensional Product Measures,seg_47,"∧ and that this holds on the extended euclidean spaces as well. (it is usual not to add the symbol in dealing with the completions of these particular measures.) (c) now, λ is just a particular lebesgue–stieltjes measure on (r,b). any lebesgue–stieltjes measure μf on (r,b) or (r̄, b̄) yields an obvious n-fold product on either (rn,bn) or (r̄n, b̄n), which could appropriately be denoted by μf × · · · × μf . further, we will let fn denote the field consisting of all finite disjoint unions of sets of the form i1 × · · · × in where each ik is of the form (a, b], (−∞, b] or (a,+∞) when considering (rn,bn) (or of the form (a, b], [−∞, b], or (a,+∞] when considering (r̄n, b̄n)). (that is, in the case of (rn,bn) there is the alternative field fn that also generates the σ-field bn; and this fn is made up of simpler sets than is the field b × · · · × b used in definition 1.1.) (d) the halmos approximation lemma now shows that if (μf × · · · × μf )(a) < ∞ and if > 0 is given, then (μf × · · · × μf )(a c ) < for some c in (the simpler field) fn. that is, the simpler field gives us a nicer conclusion in this example, because its sets c are simpler. (or, use a in the field f of (1) in place of c .)"
675,1,['function'], FiniteDimensional Product Measures,seg_47,"definition 1.2 (sections) (a) let x denote a function on ω × ω′. for each ω in ω, the function xω(·) on ω′ defined by xω(ω′) ≡ x(ω, ω′) for each ω′ in ω′ is called an ω-section of x(·, ·). an ω′-section xω′(·) of x(·, ·) is defined analogously."
676,1,['set'], FiniteDimensional Product Measures,seg_47,"(b) let c be a subset of ω × ω′. for each ω in ω, the set cω = {ω′ : (ω, ω′) is in c} is called the ω-section of c. an ω′-section of c is defined analogously."
677,1,"['set', 'results']", FiniteDimensional Product Measures,seg_47,"exercise 1.4 (an elementary illustation) let c denote the set in 2-dimensional space r2 enclosed by the two curves y = x and y = x2 on [0, 1]. draw a picture of c on an ordinary (x, y)-axes system that represents r2. (i)(a) first, use ordinary introductory calculus to evaluate the area of c. (ii) answer the following in the context of the product measure theorem. (b) evaluate (and describe—possibly, via a picture) the sections cx and cy—for the appropriate measures μ and ν. then valuate ν(cx) and μ(cy). (c) use the results of (b) to evaluate the area of c in two different ways."
678,1,['sets'], FiniteDimensional Product Measures,seg_47,"proof. we first show (6). this result is trivial for any c in f0, or any c in f (and for cc). now let s denote the class of all sets c in a × a′ for which (6) is true. then s is trivially seen to be a σ-field, using"
679,1,"['sets', 'set']", FiniteDimensional Product Measures,seg_47,"consider (7). note that if the sets cn converge monotonically to some set c, then 1cn converges monotonically to 1c and"
680,1,[], FiniteDimensional Product Measures,seg_47,every section of 1c converges monotonically (b) n
681,0,[], FiniteDimensional Product Measures,seg_47,to the corresponding section of 1c .
682,1,"['case', 'sets', 'set']", FiniteDimensional Product Measures,seg_47,"let m denote the collection of all sets c in a × a′ for which (7) holds. clearly, m contains f0 and f . we now use (b) to show that m is a monotone class; it will then follow by proposition 1.1.6 that m = σ[f ] = a × a′. let cn denote a sequence of sets in the class m that converge monotonically (we will consider only the ↗ case, since we only need to take complements in the ↘ case), and we give the name c to the limiting set. since 1c ↗ 1c ,"
683,1,"['function', 'functions', 'limit']", FiniteDimensional Product Measures,seg_47,"n the function 1c is (a × a′)-measurable, and thus every section of 1c is measurable by (6). now, for fixed ω′ the number h(ω′) ≡ μ(cω′) = ∫ω 1cω′ (ω) dμ(ω) is (by the mct and (b)) the ↗ limit of the sequence of numbers hn(ω′) ≡ μ(cn,ω′) = ∫ω 1cn,ω′ (ω) dμ(ω), for each ω′ in ω′. thus the function h on ω′ is the limit of the functions hn on ω′; and since cn is in m, the functions hn are a′-measurable by (7); thus h is a′-measurable by proposition 2.2.2. moreover, the fnite ↗ numbers φ(cn) are bounded above by μ(ω)ν(ω′), and thus converge to some number; call it φ(c). that is,"
684,1,"['symmetric', 'set', 'function']", FiniteDimensional Product Measures,seg_47,"(since φ(c) is finite, we see that h is ν-integrable. thus h(ω′) is finite for a.e. [ν]ω′.) the argument for each fixed ω is symmetric, and it gives the second equality in (7). thus c is in m, making m the monotone class a × a′; and (b) holds. (thus the result (7) holds for the set function φ. but is φ a measure?)"
685,1,"['sets', 'disjoint', 'disjoint sets']", FiniteDimensional Product Measures,seg_47,"in this paragraph we will show that the product measure φ of theorem 1.1 exists, and is defined by (e). to this end, let d1,d2, . . . be pairwise disjoint sets in a × a′, and let"
686,1,[], FiniteDimensional Product Measures,seg_47,1 dk. then linearity of both single integrals shows (in the second equality) that
687,0,[], FiniteDimensional Product Measures,seg_47,"so that φ is c.a., and a measure on a × a′. we have just verified that the product measure of (3) exists on a × a′, and is given by (7). that is, we have just proven theorem 1.1 and given the representation (7) for φ(c). note that the product measure φ also satisfies"
688,0,[], FiniteDimensional Product Measures,seg_47,exercise 1.5 give the details to verify that ∑n
689,1,['functions'], FiniteDimensional Product Measures,seg_47,(8) all ω′-sections xω′(·) of x are a-measurable functions on ω.
690,1,['function'], FiniteDimensional Product Measures,seg_47,"(9) for a.e. [ν] fixed ω′, the function xω′(·) = x(·, ω′) is μ-integrable."
691,1,['function'], FiniteDimensional Product Measures,seg_47,(10) the function h(ω′) ≡ ∫ω
692,1,['function'], FiniteDimensional Product Measures,seg_47,xω′(ω) dμ(ω) is a ν-integrable function of ω′.
693,1,[], FiniteDimensional Product Measures,seg_47,(setting x equal to 1c in (11) for c ∈ a × a′ shows how the value φ(c) of the product measure φ at c was defined as an iterated integral; recall (7).)
694,0,[], FiniteDimensional Product Measures,seg_47,corollary 1 (tonelli) let x be a × a′-measurable and suppose either
695,0,[], FiniteDimensional Product Measures,seg_47,"then the claims of fubini’s theorem are true, including"
696,1,"['null sets', 'sets', 'set']", FiniteDimensional Product Measures,seg_47,"corollary 2 (μ × ν null sets) a set c in a × a′ is (μ × ν)-null if and only if almost every ω-section of c is a ν-null set. that is, for c ∈ a × a′ we have"
697,0,[], FiniteDimensional Product Measures,seg_47,"proof. by using the σ-finiteness of the two measures to decompose both ω and ω′, we may assume in this proof that both μ and ν are finite measures. we begin by discussing only measurability questions."
698,0,[], FiniteDimensional Product Measures,seg_47,we will first show that
699,1,['function'], FiniteDimensional Product Measures,seg_47,(a) all ω′-sections of an (a × a′)-measurable function x are a-measurable.
700,0,[], FiniteDimensional Product Measures,seg_47,the previous theorem shows that
701,1,['function'], FiniteDimensional Product Measures,seg_47,"now let x denote any (a × a′)-measurable function. then for any b in b̄,"
702,1,"['indicator function', 'set', 'function', 'indicator']", FiniteDimensional Product Measures,seg_47,"is the ω′-section of the indicator function of the set c = x−1(b); so (b) shows that any arbitrary ω′-section of this x is a-measurable, and so establishes (a) and (8)."
703,1,"['functions', 'indicator']", FiniteDimensional Product Measures,seg_47,we now turn to all the other claims of the fubini and tonelli theorems. by theorem 1.2 they hold for all (a × a′)-measurable indicator functions. linearity of the various integrals shows that the theorems also hold for all simple functions. applying the mct to the various integrals shows that the theorems also hold for all (a × a′)-measurable x ≥ 0. then linearity of the integral shows that the theorems also hold for all x for whichever of the three integrals exists finitely (the double integral or either iterated integral).
704,1,['function'], FiniteDimensional Product Measures,seg_47,corollary 2 follows immediately by applying (13) and exercise 3.2.2 (only the zero function) to the integral of the function 1c .
705,0,['n'], FiniteDimensional Product Measures,seg_47,corollary 3 all this extends naturally to n dimensions.
706,0,[], FiniteDimensional Product Measures,seg_47,example 1.2 (summing infinite series) (i) (one-dimensional infinite series) we can think of
707,1,['transform'], FiniteDimensional Product Measures,seg_47,"now let π denote any one-to-one transform that maps ω onto ω. then an arbitrary rearrangement of the sequence an can be written as aπ(n). appealing to the definition in (3.1.2) for the value of ∫ω x+ dμ (which was given there as having the value sup{∫ω y dμ : 0 ≤ y ≤ x+, with y a simple fuction}; and noting that 0 ≤ y ≤ x+ if and only if 0 ≤ yπ ≤ xπ+, where xπ(n) ≡ x(π(n))), we see that the values of"
708,1,['transform'], FiniteDimensional Product Measures,seg_47,π(n) for any 1-1 transform π.
709,0,[], FiniteDimensional Product Measures,seg_47,thus the values of
710,0,[], FiniteDimensional Product Measures,seg_47,"have the same values for every π. thus, for every π,"
711,1,['transformation'], FiniteDimensional Product Measures,seg_47,"that is, for every one-to-one transformation π of {0, 1, 2, . . .} onto {0, 1, 2, . . .},"
712,1,['transform'], FiniteDimensional Product Measures,seg_47,"(ii) (two-dimensional infinite series) likewise, for any 1-1 transform π of ω × ω onto itself, we have"
713,0,[], FiniteDimensional Product Measures,seg_47,(including (17) to have one specific useful alternative formulation exhibited). note that the conclusion (16) is also explicitly supplied by tonelli. (iii) this clearly extends to multi-dimensional arrays.
714,1,['sets'], FiniteDimensional Product Measures,seg_47,"exercise 1.6 (fubini’s (11) can fail if x is not φ-integrable) let ω = (0, 1) and ω′ = (1,∞), both equipped with the borel sets and lebesgue measure. (i) let f(x, y) = e−xy − 2e−2xy for all x ∈ ω = (0, 1) and y ∈ ω′ = (1,∞). show that:"
715,0,[], FiniteDimensional Product Measures,seg_47,"(ii) why does fubini’s theorem fail here? (solve f(x, y) = 0, and use this to divide the domain of f . integrate over each of these two regions separately.) (iii) construct another example of this type."
716,1,"['conditional expectation', 'statistics', 'conditional', 'expectation', 'probability']", FiniteDimensional Product Measures,seg_47,"example 1.3 (application to conditional expectation in probability and statistics) consider the measure space (r2,b2, λ2) = (r × r,b × b, λ × λ) in the product measure theorem context; thus φ = λ2 is two-dimensional lebesgue measure. next, consider fubini’s theorem, replacing x(ω, ω′) in that result by"
717,1,"['conditional density', 'marginal', 'conditional', 'probabilistic', 'marginal density', 'expectation', 'probability', 'random', 'function']", FiniteDimensional Product Measures,seg_47,"as in an elementary probability class. continuing in the probabilistic context, think of fx(·) as the marginal density of x, of fy |x=x(·) as the conditional density of y given that x = x, and of g(x,y ) as a random function whose expectation you would like to evaluate. tonelli gives"
718,1,"['model', 'conditional expectation', 'treatment', 'dependent', 'conditional', 'discrete', 'set', 'expectation', 'expectation operation', 'associated', 'continuous', 'distributions']", FiniteDimensional Product Measures,seg_47,"provided the iterated integral of |g(x, y)| is finite. this shows how an elementary conditional model with dependent rvs can fit into the lebesgue integral context. (to treat an elementary version of discrete rvs, just replace two-dimensional lebesgue measure λ2 by two-dimensional counting measure on the set of pairs (m,n) with m,n ≥ 0.) mainly, this example shows one way that some dependent models can be treated in the context of the product measure theorem using a “base measure” φ = μ × ν that seemingly can only be associated with independendent rvs. (the lebesgue integral allows us to treat absolutely continuous, discrete, and singular distributions simultaneously via the expectation operation e{·}. this effect will be extended to a general treatment of conditional expectation in chapter 7.)"
719,1,"['functions', 'probability']", Random Vectors on ΩA P ,seg_49,"we will now treat measurable functions from a probability space (ω,a, p ) to a euclidean space (rn,bn), with n ≥ 1. let x ≡ (x1, . . . , xn)′ denote a generic vector in the euclidean space rn."
720,1,"['distribution function', 'distribution', 'joint', 'random', 'function']", Random Vectors on ΩA P ,seg_49,"definition 2.1 (random vectors) suppose x ≡ (x1, . . . , xn)′ is such that x : ω → rn is bn-a-measurable. then x is called a random vector (which is also abbreviated rv). define the joint distribution function (or just df) of x by"
721,1,['random'], Random Vectors on ΩA P ,seg_49,"thus, a random vector is measurable if and only if each coordinate rv is measurable."
722,1,['functions'], Random Vectors on ΩA P ,seg_49,"proof. we give the details for finite-valued functions. (⇒) now,"
723,1,['set'], Random Vectors on ΩA P ,seg_49,"with the set inclusion shown in the first line. that is, x−1(bn) ⊂ a."
724,1,"['continuous', 'joint']", Random Vectors on ΩA P ,seg_49,exercise 2.1 (joint df) a joint df f is ↗ and right continuous and satisfies
725,1,"['continuous', 'probability measure', 'probability']", Random Vectors on ΩA P ,seg_49,exercise 2.2 suppose f : rn → r is ↗ and right continuous and satisfies (2) and (3). then there exists a unique probability measure p ≡ pf on bn that satisfies
726,1,"['function', 'joint', 'random']", Random Vectors on ΩA P ,seg_49,"this is a generalization of the correspondence theorem to n > 1. now note that the identity function x(ω) ≡ ω, for each ω ∈ rn, is a random vector on (rn,bn) that has as its joint df the function f above. thus, given any joint df f , there is a random vector x having f as its joint df. this is in the spirit of example 2.2.1."
727,1,"['distribution', 'joint']", Random Vectors on ΩA P ,seg_49,"definition 2.2 (joint density of rvs) let x ≡ (x1, . . . , xn)′ denote a rv. define pn(b) ≡ p (x ∈ b) for all b ∈ bn, so that pn defines the induced distribution of x on (rn,bn). let λn denote lebesgue measure on (rn,bn). if pn λn, then a finite-valued radon-nikodym derivative fn ≡ dpn/dλn exists (and is unique a.e. λn) for which"
728,1,"['distribution', 'joint']", Random Vectors on ΩA P ,seg_49,"when this is true, fn(· · · ) is called the joint density (or, the density) of the rv x. (for onedimensional rvs, we often denote the distribution, df, and density of x by px(·), fx(·), and fx(·). for two-dimensional rvs (x,y )′, we often use px,y (·), fx,y (·, ·), and fx,y (·, ·).)"
729,1,"['densities', 'marginal', 'distribution', 'joint']", Random Vectors on ΩA P ,seg_49,"exercise 2.3 (marginal densities) suppose that (x1, . . . , xn)′ has the induced distribution pn, and pn λn with joint density fn (as in the previous definition). let 1 ≤ i1 < · · · < im ≤ n, with m ≤ n, and let 1 ≤ j1 < · · · < jn−m ≤ n denote the complementary indices. show that the induced distribution pm of (xi1 , . . . , xim)′ satisfies pm λm, and that its joint density is given by"
730,1,"['marginal', 'marginal density']", Random Vectors on ΩA P ,seg_49,"on rm. we also call fm the marginal density of (xi1 , . . . , xim)′."
731,0,[], Random Vectors on ΩA P ,seg_49,"exercise 2.4 (lebesgue-stieltjes measures on (r2,b2)) (i) let f be a gdf on r2. using the notation of definition 2.1, let"
732,1,[], Random Vectors on ΩA P ,seg_49,in parallel with the correspondence theorem 1.3.1 and exercise 1.3.1. show that
733,1,"['continuous', 'disjoint']", Random Vectors on ΩA P ,seg_49,"for countable unions of disjoint subrectangles (ck, dk] of r̄2. as in definition 1.3.3, call f absolutely continuous if for all > 0 there exists a δ > 0 for which"
734,1,"['continuous', 'disjoint']", Random Vectors on ΩA P ,seg_49,"whenever n ≥ 1 and all of the subrectangles (ck, dk] of r2 are mutually disjoint. here, λ2(·) is the lebesgues-stieltjes measure generalization of area with f (x) = x. (ii) state and prove the two-dimensional analogs of exercise 3.2.6 (integrals as measures) and exercise 3.2.7 (absolutely continuous dfs)."
735,1,"['probability', 'probability measures']", Countably Infinite Product Probability Spaceso,seg_51,"we now begin to carry out the program discussed in section 2.5. that is, we will extend the notion of rvs and product probability measures to a countably infinite number of dimensions."
736,1,"['set', 'mean', 'interval']", Countably Infinite Product Probability Spaceso,seg_51,"let i denote an interval of the type (c, d], (−∞, d], (c,+∞), or (−∞,∞). an n-dimensional rectangle will mean any set of the form i1 × · · · × in × r × r × · · · , where each interval ii is of the type above. a finite-dimensional rectangle is an n-dimensional rectangle, for some n ≥ 1. a cylinder set is defined as a set of the form bn × r × r × · · · with bn in bn for some n ≥ 1. thus:"
737,0,[], Countably Infinite Product Probability Spaceso,seg_51,(2) ci ≡ {all finite-dimensional rectangles}
738,1,['disjoint'], Countably Infinite Product Probability Spaceso,seg_51,"(3) cf ≡ {all finite disjoint unions of finite-dimensional rectangles},"
739,1,['sets'], Countably Infinite Product Probability Spaceso,seg_51,"(4) c∞ ≡ {all cylinder sets} ≡ {bn × r × r × · · · : n ≥ 1, bn ∈ bn}."
740,0,[], Countably Infinite Product Probability Spaceso,seg_51,"both cf and c∞ are fields, and a trivial application of exercise 1.1.1 shows that"
741,0,['n'], Countably Infinite Product Probability Spaceso,seg_51,"thus, extending a measure from ci to b∞ will be of prime interest to us. we first extend the criterion for measurability from n dimensions to a countably infinite number of dimensions."
742,0,[], Countably Infinite Product Probability Spaceso,seg_51,exercise 3.1 prove proposition 3.1.
743,0,[], Countably Infinite Product Probability Spaceso,seg_51,notation 3.2 we will use the notation
744,0,[], Countably Infinite Product Probability Spaceso,seg_51,"to denote the minimal sub σ-fields of a relative to which the quantities xi and y ≡ (xi1 ,xi2 , . . .) are measurable."
745,1,"['probability measure', 'probability', 'distributions']", Countably Infinite Product Probability Spaceso,seg_51,"now suppose that pn is a probability measure on (rn,bn), for each n ≥ 1. the question is: when can we extend the collection {pn : n ≥ 1} to a measure on (r∞,b∞)? reasoning backwards to see what conditions the family of finite-dimensional distributions should satisfy leads to the following definition."
746,1,"['consistency', 'distributions']", Countably Infinite Product Probability Spaceso,seg_51,"definition 3.1 (consistency) finite-dimensional distributions {(rn,bn, pn)}∞n=1 are consistent if for every n ≥ 1, every b1, . . . , bn ∈ b, and every 1 ≤ i ≤ n,"
747,1,"['probability', 'probability measures']", Countably Infinite Product Probability Spaceso,seg_51,"theorem 3.1 (kolmogorov’s extension theorem) an extension of any consistent family of probability measures {(rn,bn, pn)}∞n=1 to a probability p (·) on (r∞,b∞) necessarily exists, and it is unique."
748,0,[], Countably Infinite Product Probability Spaceso,seg_51,we will first summarize the main part of this proof as a separately stated result that seems of interest in its own right.
749,0,[], Countably Infinite Product Probability Spaceso,seg_51,"(c) if d denotes any fixed n-dimensional rectangle, then there exists a sequence of compact ndimensional rectangles dj for which dj ↗ d and p (dj) ↗ p (d). (that is, p is well-defined and additive on n-dimensional rectangles and satisfies something like continuity from below.) then there exists a unique extension of p to b∞."
750,0,[], Countably Infinite Product Probability Spaceso,seg_51,"proof.∗ (recall the continuity result of proposition 1.1.3.) now,"
751,1,['disjoint'], Countably Infinite Product Probability Spaceso,seg_51,cf ≡ {all finite disjoint unions of finite-dimensional rectangles}
752,1,['disjoint'], Countably Infinite Product Probability Spaceso,seg_51,"i dij with di1, . . . , di,mi disjoint,"
753,1,['condition'], Countably Infinite Product Probability Spaceso,seg_51,"(using condition (b) in each of the last two equalities), since p is well-defined."
754,1,['continuous'], Countably Infinite Product Probability Spaceso,seg_51,"we will now show that p is continuous from above at ∅. let an’s in cf be such that an ↘ ∅. we must show that p (an) ↘ 0. assume not. then p (an) ↘ > 0; and by going to subsequences, we may assume that an = a∗n × ∏n"
755,1,"['disjoint', 'union', 'condition']", Countably Infinite Product Probability Spaceso,seg_51,"∞ +1 r, where each a∗n is a finite union of disjoint rectangles (repeat some members of the sequence if necessary in order to have an∗ ⊂ rn). by condition (c), choose bn"
756,1,"['disjoint', 'union']", Countably Infinite Product Probability Spaceso,seg_51,∗ ⊂ an∗ such that bn ∗ is a finite union of compact disjoint rectangles in rn with
757,0,['n'], Countably Infinite Product Probability Spaceso,seg_51,but cn ↘with p(cn) ≥ /2 for all n is not compatible with the conclusion that cn ↘ ∅:
758,0,['n'], Countably Infinite Product Probability Spaceso,seg_51,"(n), x2 (n), . . .). choose an initial subsequence"
759,0,[], Countably Infinite Product Probability Spaceso,seg_51,n1) → (some x1) ∈ c1∗; then choose a further subsequence n2 such that
760,0,[], Countably Infinite Product Probability Spaceso,seg_51,"now apply the continuity of measures in proposition 1.1.3, and then apply the carathéodory extension of theorem 1.2.1 to complete the proof."
761,0,[], Countably Infinite Product Probability Spaceso,seg_51,proof.∗ we now turn to the kolmogorov extension theorem. the p defined by
762,1,"['consistency', 'probability', 'condition']", Countably Infinite Product Probability Spaceso,seg_51,is a well-defined f.a. probability on ci = {all finite-dimensional rectangles}; this follows from the consistency condition (7). thus (a) and (b) of theorem 3.2 hold.
763,0,['n'], Countably Infinite Product Probability Spaceso,seg_51,we will now verify (c). fix n. let dn be an arbitrary but fixed n-dimensional rectangle. it is clearly possible to specify compact n-dimensional rectangles dnj for which dnj ↗ dn as j → ∞. write dj = dnj × ∏∞
764,0,[], Countably Infinite Product Probability Spaceso,seg_51,"since pn is a measure on (rn,bn). thus (c) holds. the conclusion follows from theorem 3.2."
765,1,"['probability measure', 'probability', 'probability measures']", Countably Infinite Product Probability Spaceso,seg_51,"example 3.1 (coordinate rvs) once consistent probability measures pn(·) on (rn,bn) have been extended to a probability measure p (·) on (r∞,b∞), it is appropriate then to define xn(x1, x2, . . .) = xn, for each n ≥ 1. these are rvs on the probability space (ω,b, p ) ≡ (r∞,b∞, p ). moreover,"
766,1,"['distribution', 'realization']", Countably Infinite Product Probability Spaceso,seg_51,"for all bn ∈ bn. we thus have a realization of x ≡ (x1,x2, . . .): ω → r∞ that is b∞-a- measurable, and each (x1, . . . , xn) induces the distribution pn on (rn,bn). this is the natural generalization of example 2.2.1 and the comment below exercise 5.2.2."
767,1,"['probability theory', 'probability', 'random', 'distributions']", Countably Infinite Product Probability Spaceso,seg_51,"theorem 3.3 (the finite dimensional dfs define probability theory) let x = (x1,x2, . . .)′ denote any random element on (r∞,b∞). then px can be determined solely by examination of the finite-dimensional distributions of x. also, whether or not there exists a finite rv x such that xn converges to x in the sense of →a.s.,→p,→r, or →d can be similarly determined."
768,1,['sets'], Countably Infinite Product Probability Spaceso,seg_51,proof. let c denote the π̄-system consisting of r∞ and of all sets of the form ∏n
769,1,"['convergence', 'distributions']", Countably Infinite Product Probability Spaceso,seg_51,"1 (−∞, xi] ∞ ×∏n+1 r, for some n ≥ 1 and all xi ∈ r. the finite-dimensional distributions (even the finite-dimensional dfs) determine p∞ on c, and hence on b∞ = σ[c] (appeal to dynkin’s π-λ theorem of proposition 1.1.5). to emphasize the fact further, we now consider each convergence mode separately. →d: obvious."
770,1,['function'], Countably Infinite Product Probability Spaceso,seg_51,"= lim lim p (∩nm=n[|xm − xn| ≤ ]) = lim lim{a function of fxn,...,xn }. n n n n"
771,0,[], Countably Infinite Product Probability Spaceso,seg_51,the proof is complete
772,1,"['standard', 'data', 'distribution', 'cases', 'random', 'experiments']", Countably Infinite Product Probability Spaceso,seg_51,"example 3.2 (equivalent experiments) perhaps i roll an ordinary die n times with the appearance of an even number called “success.” perhaps i draw a card at random n times, each time from a freshly shuffled deck of standard playing cards, with “red” called “success.” perhaps i flip a fair coin n times with “heads” called “success.” note that (x1, . . . , xn) has the same distribution in all three cases. thus, if i report only the data from one of these experiments, you can not hope to determine which of the three experiments was actually performed. these are called equivalent experiments."
773,1,"['functions', 'continuous', 'set']", Random Elements and Processes on ΩA P  o,seg_53,"definition 4.1 (projections and finite-dimensional subsets) let mt denote a collection of functions that associate with each t of some set t a real number denoted by either xt or x(t). (t is usually a euclidean set such as [0, 1], r, or [0, 1] × r. the collection mt is often a collection of “nice” functions, such as the continuous functions on t.) for each integer k and all (t1, . . . , tk) in t we let πt1,...,tk denote the projection mapping of mt into k-dimensional space rk defined by"
774,1,['set'], Random Elements and Processes on ΩA P  o,seg_53,"−1 then for any b in the set of all k-dimensional borel subsets bk of rk, the set πt1,...,tk(b) is called a finite-dimensional subset of mt ."
775,0,[], Random Elements and Processes on ΩA P  o,seg_53,exercise 4.1 show that the collection mt0 of all finite-dimensional subsets of mt is necessarily a field. (this is true no matter what collection mt is used.)
776,1,"['normal', 'random', 'function', 'processes', 'distributions']", Random Elements and Processes on ΩA P  o,seg_53,"definition 4.2 (measurable function spaces, finite-dimensional distributions, random elements, and normal processes) we let mt denote the σ-field generated by the field m0t . we call m0t and mt the finite-dimensional field and the finitedimensional σ-field, respectively. call the measurable space (mt ,mt ) a measurable function space over t."
777,1,"['probability', 'random']", Random Elements and Processes on ΩA P  o,seg_53,"given any probability space (ω,a, p ) and any measurable space (ω∗,a∗), an a∗-a- measurable mapping x : ω → ω∗ will be called a random element. we denote this by x : (ω,a) → (ω∗,a∗) or by x : (ω,a, p ) → (ω∗,a∗), or even by x : (ω,a, p ) → (ω∗,a∗, p ∗), where p ∗ denotes the induced probability on the image space."
778,1,"['normal', 'random', 'function', 'process', 'distributions']", Random Elements and Processes on ΩA P  o,seg_53,"a random element x : (ω,a, p ) → (mt ,mt , p ∗) in which the image space is a measurable function space will be called a process. the finite-dimensional distributions of a process are the distributions induced on the (rk,bk) by the projection mappings πt1,...,tk : (mt ,mt , p ∗) → (rk,bk). if all of the finite-dimensional distributions of a process x are multivariate normal (see section 9.3 below), then we call x a normal process."
779,1,"['probability', 'random', 'function', 'distributions']", Random Elements and Processes on ΩA P  o,seg_53,"definition 4.3 (realizations and versions) if two random elements x and y (possibly from different probability spaces to different measurable function spaces) have identical induced finite-dimensional distributions, then we refer to x and y as different realizations of the same random element and we call them equivalent random elements. we denote this by agreeing that"
780,1,['random'], Random Elements and Processes on ΩA P  o,seg_53,∼ x y means that x and y are equivalent random elements. =
781,1,"['functions', 'process', 'continuous']", Random Elements and Processes on ΩA P  o,seg_53,"(we will see in chapter 12 that a process called brownian motion can be realized on both the (r[0,1],b[0,1]) of (3) and (c, c), where c ≡ c[0,1] denotes the space of all continuous functions on [0, 1] and c ≡ c[0,1] denotes its finite-dimensional σ-field.)"
782,1,['probability'], Random Elements and Processes on ΩA P  o,seg_53,"if x and y are defined on the same probability space and p (xt = yt) = 1 for all t ∈ t , then x and y are called versions of each other. (in chapter 12 we will see versions x and y of brownian motion where x : (ω,a, p ) → (r[0,1],b[0,1]) and y : (ω,a, p ) → (c[0, 1]c[0,1]). of course, this x and y are also different realizations of brownian motion.)"
783,1,"['distribution', 'convergence', 'processes']", Random Elements and Processes on ΩA P  o,seg_53,"definition 4.4 (finite-dimensional convergence, →fd) suppose x,x1,x2, . . . denote processes with image space (mt ,mt ). if the convergence in distribution"
784,1,['distributions'], Random Elements and Processes on ΩA P  o,seg_53,"holds for all k ≥ 1 and all t1, . . . , tk in t , then we write xn →fd x as n → ∞, and we say that the finite-dimensional distributions of xn converge to those of x."
785,1,"['stochastic process', 'process']", Random Elements and Processes on ΩA P  o,seg_53,the general stochastic process
786,0,[], Random Elements and Processes on ΩA P  o,seg_53,"notation 4.1 ((rt ,bt )) we now adopt the convention that"
787,1,['function'], Random Elements and Processes on ΩA P  o,seg_53,"(3) (rt ,bt ) denotes the measurable function space with rt ≡ ∏t∈t rt,"
788,1,"['functions', 'stochastic process', 'process']", Random Elements and Processes on ΩA P  o,seg_53,"where each rt is a copy of the real line. thus rt consists of all possible real-valued functions on t , and bt is the smallest σ-field with respect to which all πt are measurable. we call a process x : (ω,a, p ) → (rt ,bt ) a general stochastic process. we note that a general stochastic process is also a process. but we do not yet know what bt looks like."
789,1,['set'], Random Elements and Processes on ΩA P  o,seg_53,"a set bt ∈ bt is said to have countable base t1, t2, . . . if"
790,1,['sets'], Random Elements and Processes on ΩA P  o,seg_53,here b∞ is the countably infinite-dimensional σ-field of section 5.3. let bc denote the class of countable base sets defined by
791,0,[], Random Elements and Processes on ΩA P  o,seg_53,"proposition 4.1 (measurability in (rt ,bt )) now, bc is a σ-field. in fact, bc is the smallest σ-field relative to which all πt are measurable; that is,"
792,0,[], Random Elements and Processes on ΩA P  o,seg_53,"also (generalizing proposition 5.2.1),"
793,1,[], Random Elements and Processes on ΩA P  o,seg_53,"proof. clearly, bt is the smallest σ-field containing bc ; so (6) will follow from showing that bc is a σ-field. now, c is closed under complements, since πt−"
794,1,['set'], Random Elements and Processes on ΩA P  o,seg_53,"suppose that b1, b2, . . . in bc have countable bases t1, t2, . . ., and let t0 = ∪∞m=1tm. then using the countable set of distinct coordinates in t0, reexpress each bm as bm = πt"
795,1,[], Random Elements and Processes on ΩA P  o,seg_53,1(∪∞m=1bm∞) is in bc . thus bc is closed under countable unions. thus bc is a σ-field.
796,0,[], Random Elements and Processes on ΩA P  o,seg_53,now to establish (7): suppose x is bt -a-measurable. then
797,1,"['consistency', 'stochastic process', 'process', 'distributions']", Random Elements and Processes on ΩA P  o,seg_53,"remark 4.1 (consistency of induced distributions in (rt ,bt )) any general stochastic process x : (ω,a, p ) → (rt ,bt ) has a family of induced distributions"
798,1,['distributions'], Random Elements and Processes on ΩA P  o,seg_53,"for all k ≥ 1 and all t1, . . . , tk ∈ t . these distributions are necessarily consistent in the sense that"
799,1,"['processes', 'stochastic processes', 'distributions']", Random Elements and Processes on ΩA P  o,seg_53,"for all k ≥ 1, all b1, . . . , bk ∈ b, all 1 ≤ i ≤ k, and all t1, . . . , tk ∈ t . (the next result gives a converse. it is our fundamental result on the existence of stochastic processes with specified distributions.)"
800,1,"['distribution', 'consistency', 'stochastic process', 'set', 'process', 'distributions']", Random Elements and Processes on ΩA P  o,seg_53,"theorem 4.1 (kolmogorov’s consistency theorem) given a consistent set of distributions as in (9), there exists a distribution p on (rt ,bt ) such that the identity map x(ω) = ω, for all ω ∈ rt , is a general stochastic process x : (rt ,bt , p ) → (rt ,bt ) whose family of induced distributions is the pt∗"
801,0,[], Random Elements and Processes on ΩA P  o,seg_53,−1(b)) for b ∈ bc and each counti able subset ti of t . use notational ideas from the proof of proposition 4.1 to show easily that p ∗(·) is well-defined and countably additive.)
802,1,"['function', 'functions', 'continuous']", Random Elements and Processes on ΩA P  o,seg_53,"example 4.1 (comment on (r[0,1],b[0,1])) the typical function x in rt has no smoothness properties. let t = [0, 1] and let c denote the subset of r[0,1] that consists of all functions that are continuous on [0, 1]. we now show that"
803,1,['distributions'], Random Elements and Processes on ΩA P  o,seg_53,"let (ω,a, p ) denote lebesgue measure on the borel subsets of [0, 1]. let ξ(ω) = ω. now let x : (ω,a, p ) → (r[0,1],b[0,1]) via xt(ω) = 0 for all ω ∈ ω and for all t ∈ t . let y : (ω,a, p ) → (r[0,1],b[0,1]) via yt(ω) = 1{t}(ξ(ω)). now, all finite-dimensional distributions of x and y are identical. note, however, that [ω : x(ω) ∈ c] = ω, while [ω : y (ω) ∈ c] = ∅. thus c cannot be in b[0,1]."
804,1,"['processes', 'stochastic processes']", Random Elements and Processes on ΩA P  o,seg_53,smoother realizations of general stochastic processes
805,1,"['functions', 'distribution', 'cases', 'process', 'distributions']", Random Elements and Processes on ΩA P  o,seg_53,"suppose now that x is a process of the type x : (ω,a, p ) → (rt ,bt , p ∗). as the previous example shows, x is not the unique process from (ω,a, p ) that induces the distribution p ∗ on (rt ,bt ). we now let mt denote a proper subset of rt and agree that mt denotes the σ- field generated by the finite-dimensional subsets of mt . suppose now that x(ω) ∈ mt for all ω ∈ ω. can x be viewed as a process x : (ω,a, p ) → (mt ,mt , p̃ ) such that (mt ,mt , p̃ ) has the same finite-dimensional distributions as does (rt ,bt , p ∗) ? we now show that the answer is necessarily yes. interesting cases arise when the functions of the mt above have smoothness properties such as continuity. the next result is very important and useful."
806,1,"['sample', 'variable', 'random variable', 'random', 'process', 'processes', 'distributions']", Random Elements and Processes on ΩA P  o,seg_53,"theorem 4.2 (smoother realizations of processes) consider an arbitrary measurable mapping x : (ω,a, p ) → (rt ,bt , p ∗). (i) let mt ⊂ rt . then we can view x as a process x : (ω,a) → (mt ,mt ) if and only if every sample path x.(ω) = x(·, ω) is in mt and every xt(·) ≡ x(t, ·) is a random variable. (ii) let x(ω) ⊂ mt ⊂ rt . then x : (ω,a, p ) → (mt ,mt , p̃ ). where the finitedimen- sional distributions of (mt ,mt , p̃ ) are the same as those of (rt ,bt , p ∗). (iii) comment: all this is true even when mt is not in the class bt ."
807,0,[], Random Elements and Processes on ΩA P  o,seg_53,"proof. (i) (⇐) note first that mt ∩ bt = mt (recall definition 4.2). moreover, when x(ω) ⊂ mt , it necessarily follows that"
808,1,"['sets', 'distributions']", Random Elements and Processes on ΩA P  o,seg_53,"−1 −1 pairs of generator sets (πt1,...,tk((−∞, r1] × · · · × (−∞, rk]) in bt , or πt1,...,tk((−∞, r1] × · · · × (−∞, rk]) ∩ mt in mt ) have the same inverse images under x; thus the finite dimensional distributions induced from (ω,a) to (rt ,bt ) and to (mt ,mt ) are identical. (⇒) clearly, x : ω → mt implies x : ω → rt . also, for any t ∈ t and any b ∈ b,"
809,0,[], Random Elements and Processes on ΩA P  o,seg_53,"thus each xt is a rv, and so x is bt − a−measurable by (7). (ii) this is now clear, and it summarizes the most useful part of this theorem."
810,0,[], Random Elements and Processes on ΩA P  o,seg_53,"exercise 4.3 let m denote any non-void class of subsets of ω, and let m denote any non-void subset. show that"
811,1,"['set', 'case', 'probability']", Random Elements and Processes on ΩA P  o,seg_53,"remark 4.2 it is interesting to consider the case where mt is a countable or finite set. the resulting (mt ,mt , p̃ ) is the natural probability space."
812,1,"['distribution', 'function', 'distribution function']", Character of Distribution Functions,seg_57,"let x : (ω,a, p ) → (r,b, px) be a rv with distribution function (df)fx , where"
813,0,[], Character of Distribution Functions,seg_57,then f ≡ fx was seen earlier to satisfy
814,1,['continuous'], Character of Distribution Functions,seg_57,"(2) f is ↗ and right continuous, with f (−∞) = 0 and f (+∞) = 1."
815,1,"['function', 'continuous']", Character of Distribution Functions,seg_57,"because of the proposition below, any function f satisfying (2) will be called a df. (if f is ↗, right continuous, 0 ≤ f (−∞), and f (+∞) ≤ 1, we earlier agreed to call f a sub-df. as usual, f (a, b] ≡ f (b) − f (a) denotes the increments of f , and f (x) ≡ f (x) − f−(x) = f (x) − f (x−) is the mass of f at x.)"
816,1,"['set', 'discrete']", Character of Distribution Functions,seg_57,"(a) call f discrete if f is of the form f (·) = ∑j bj1[aj ,∞)(·) with ∑j bj = 1, where the aj form a non-void finite or countable set. such measures μf have radon–nikodym derivative ∑j bj1{aj}(·) with respect to counting measure on the aj ’s."
817,1,['continuous'], Character of Distribution Functions,seg_57,(b) a df f is called absolutely continuous if f (·) = ∫∞
818,1,"['function', 'probability', 'continuous']", Character of Distribution Functions,seg_57,"integrates to 1 over r. the corresponding measure has radon-nikodym derivative f with respect to lebesgue measure λ; this f is also called a probability density. moreover, f is an absolutely continuous function and the ordinary derivative f ′ of the df f exists a.e. λ and satisfies f ′ = f as λ."
819,1,['set'], Character of Distribution Functions,seg_57,(c) a df f is called singular if μf (n c) = 0 for a λ-null set n .
820,1,['probability'], Character of Distribution Functions,seg_57,"proposition 1.1 (there exists an x with df f ) if f satisfies (2), then there exists a probability space (ω,a, p ) and a rv x : (ω,a, p ) → (r,b) for which the df of x is f . we"
821,0,[], Character of Distribution Functions,seg_57,theorem 1.1 (decomposition of a df) any df f can be decomposed as
822,0,[], Character of Distribution Functions,seg_57,"where fd, fc, fs, and fac are the unique sub-dfs of the following types (unique among those sub-dfs equal to 0 at −∞):"
823,1,"['function', 'step function']", Character of Distribution Functions,seg_57,"(4) fd is a step function of the form ∑j bj1[aj ,∞) (with all bj > 0)."
824,1,['continuous'], Character of Distribution Functions,seg_57,(5) fc is continuous.
825,0,[], Character of Distribution Functions,seg_57,(6) fs and fs + fd are both singular with respect to lebesgue measure λ.
826,1,['continuous'], Character of Distribution Functions,seg_57,and this fac(·) is absolutely continuous on the whole real line.
827,1,"['functions', 'continuous', 'set', 'limit', 'inequality']", Character of Distribution Functions,seg_57,"proof. let {aj} denote the set of all discontinuities of f , which can only be jumps; and let bj ≡ f (aj) − f−(aj). there can be only a countable number of jumps, since the number of jumps of size exceeding size 1/n is certainly bounded by n. now define fd ≡ ∑j bj1[aj ,∞), which is obviously ↗ and right continuous, since fd(x, y] ≤ f (x, y] ↘ 0 as y ↘ x (the inequality holds, since the sum of jump sizes over every finite number of jumps between a and b is clearly bounded by f (x, y], and then just pass to the limit). define fc = f − fd. now, fc is ↗, since for x ≤ y we have fc(x, y] = f (x, y] −fd(x, y] ≥ 0. now, fc is the difference of right-continuous functions, and hence is right continuous; it is left continuous, since for x ↗ y we have"
828,1,['continuous'], Character of Distribution Functions,seg_57,"we turn to the uniqueness of fd. assume that fc + fd = f = gc + gd for some other gd ≡ ∑j b̄j1[āj ,∞) with distinct āj ’s and ∑j b̄j ≤ 1. then fd − gd = gc − fc is continuous. if gd = fd, then either some jump point or some jump size disagrees. no matter which disagrees, at some a we must have"
829,0,[], Character of Distribution Functions,seg_57,"contradicting the continuity of gc − fc = fd − gd. thus gd = fd, and hence fc = gc. this completes the first decomposition."
830,0,[], Character of Distribution Functions,seg_57,"we now turn to the further decomposition of fc. associate a measure μc with fc via μc((−∞, x]) = fc(x). then the lebesgue decomposition theorem shows that μc = μs + μac, where μs(b) = 0 and μac(bc) = 0 for some b ∈ b; we say that μs and μac are orthogonal. moreover, this same lebesgue theorem implies the claimed uniqueness and shows that fac exists with the uniqueness claimed. now, fac(x) ≡ μac((−∞, x]) = ∫−"
831,1,['continuous'], Character of Distribution Functions,seg_57,"x ∞ fac(y) dy is continuous by fac(x, y] ≤ μac(x, y] → 0 as y → x or as x → y. thus fs ≡ fc −fac is continuous, and fs(x) = μs((−∞, x]). in fact, fac is absolutely continuous on r by the absolute continuity of the integral. (now fc = fs +fac decomposes fc with respect to λ, while f = (fd +fs)+fac decomposes f with respect to λ.)"
832,1,['set'], Character of Distribution Functions,seg_57,example 1.1 (lebesgue singular df) define the cantor set c by
833,1,"['set', 'interval']", Character of Distribution Functions,seg_57,"(thus the cantor set is obtained by removing from [0, 1] the open interval (1"
834,1,['intervals'], Character of Distribution Functions,seg_57,"3 , 2 3 ) at stage one, then the open intervals (1"
835,1,['function'], Character of Distribution Functions,seg_57,figure 1.1 lebesgue singular function.
836,1,['continuous'], Character of Distribution Functions,seg_57,"now note that {f (x) : x ∈ c} = [0, 1], since the right-hand side of (9) represents all of [0, 1] via dyadic expansion. we now define f “linearly” on cc (the first three “components” are shown in figure 1.1 above). since the resulting f is ↗ and achieves every value in [0, 1], it must be that f is continuous. now, f assigns no mass to the “flat spots” whose lengths"
837,0,[], Character of Distribution Functions,seg_57,"3 9 27 1−2/3 measure λ, using λ(cc) = 1 and μf (cc) = 0. call this f the lebesgue singular df . (the theorem in the next section shows that removing the flat spots does, even for a general df f , leave only the essentials.) we have, in fact, shown that"
838,1,['continuous'], Character of Distribution Functions,seg_57,"(10) f : c → [0, 1] is 1-1, is ↑, and is continuous; so f−1 : [0, 1] → c is 1 − 1."
839,1,['continuous'], Character of Distribution Functions,seg_57,"∼ exercise 1.1 let x = n(0, 1) (as in (a.1.22) below), and let y ≡ 2x. (a) is the df f (·, ·) of (x,y ) continuous? (b) does the measure μf on r2 have a density with respect to two-dimensional lebesgue measure? (hint. appeal to corollary 2 to fubini’s theorem.)"
840,1,"['interval', 'cardinality', 'set', 'continuous']", Character of Distribution Functions,seg_57,"exercise 1.2 show that the cantor set c is perfect (thus, each x ∈ c is an accumulation point of c) and totally disconnected (between any c1 < c2 in c there is an interval that lies entirely in cc). (note that the cardinality of c equals that of [0, 1]. at which points is 1c(·) continuous?"
841,0,[], Character of Distribution Functions,seg_57,definition 1.1 two rvs x and y are said to be of the same type if y ∼= ax + b for some a > 0. their dfs are also said to be of the same type.
842,1,"['set', 'interval']", Properties of Distribution Functions,seg_59,"definition 2.1 the support of a given df f ≡ fx is defined to be the minimal closed set c having p (x ∈ c) = 1. a point x is a point of increase of f if every open interval u containing x has p (x ∈ u) > 0. a realizable t-quantile of f , for 0 < t < 1, is any value z for which f (z) = t. (such a z need not exist.) define ut to be the maximal open interval of x’s for which f (x) = t (this flat spot will be an interval, since f is ↗)."
843,1,"['function', 'set']", Properties of Distribution Functions,seg_59,"theorem 2.1 (jumps and flat spots) let c denote the support of f . then: (a) c ≡ (⋃0≤t≤1 ut)c is a closed set having p (c) = 1. (b) c is equal to the set of all points of increase. (c) c is the support of f . (d) f has at most a countable number of discontinuities, and these discontinuities are all discontinuities of the jump type. (e) f has an at most countable number of flat spots (the nonvoid ut’s). these are exactly those t’s that have more than one realizable t-quantile. (we will denote jump points and jump sizes of f by ai’s and bi’s. the t values and the λ(ut) values of the multiply realizable t-quantiles will be seen in the proof of proposition 6.3.1 below to correspond to the jump points cj and the jump values dj of the function k(·) ≡ f−1(·), and there are at most countably many of them.)"
844,1,"['disjoint', 'interval', 'intervals', 'sets', 'union']", Properties of Distribution Functions,seg_59,"proof.* (a) for each t there is a maximal open interval ut (possibly void) on which f equals t, and it is bounded for each 0 < t < 1. now, p (x ∈ ut) = 0 using proposition 1.1.2. note that c ≡ (∪tut)c is closed (since the union of an arbitrary collection of open sets is open). hence cc = ∪0≤t≤1ut = ∪(an, bn), where (a1, b1), . . . are (at most countably many) disjoint open intervals, and all those with 0 < t < 1 must be finite. now, by proposition 1.1.2, for the finite intervals we have p (x ∈ (an, bn)) = lim →0 p (x ∈ [an+ , bn− ]) = lim →0 0 = 0, where p (x ∈ [an + , bn − ]) = 0 holds since this finite closed interval must have a finite subcover by ut sets. if (an, bn) = (−∞, bn), then p (x ∈ (−∞, bn)) = 0, since p (x ∈ [−1/ , bn − ]) = 0 as before. an analogous argument works if (an, bn) = (an,∞). thus p (x ∈ cc) = 0 and p (x ∈ c) = 1. note that the ut’s are just the (an, bn)’s in disguise; each ut ⊂ some (an, bn), and hence ut = that (an, bn). thus ut is nonvoid for at most countably many t’s."
845,1,['set'], Properties of Distribution Functions,seg_59,"(b) let x ∈ c. we will now show that it is a point of increase. let u denote a neighborhood of x, and let t ≡ f (x). assume p (u) = 0. then x ∈ u ⊂ ut ⊂ cc, which is a contradiction of x ∈ c. thus all points x ∈ c are points of increase. now suppose conversely that x is a point of increase. assume x ∈/ c. then x ∈ some (an, bn) having p (x ∈ (an, bn)) = 0, which is a contradiction. thus x ∈ c. thus the closed set c is exactly the set of points of increase."
846,1,"['set', 'interval', 'probability']", Properties of Distribution Functions,seg_59,"(c) assume that c is not the minimal closed set having probability 1. then p (c̃) = 1 for some closed c̃ c. let x ∈ c\c̃ and let t = f (x). since c̃c is open, there is an open interval vx with x ∈ vx ⊂ c̃c and p (x ∈ vx) = 0. thus x ∈ vx ⊂ (some ut) ⊂ cc. so x ∈/ c, which is a contradiction. thus c is minimal."
847,1,"['function', 'quantile']", The Quantile Transformation,seg_61,definition 3.1 (quantile function) for any df f (·) we define the quantile function (qf) (which is the inverse of the df) by
848,1,['transformation'], The Quantile Transformation,seg_61,theorem 3.1 (the inverse transformation) let
849,0,[], The Quantile Transformation,seg_61,the following are all true.
850,0,[], The Quantile Transformation,seg_61,failure occurs if and only if ξ(ω) equals the height of a flat spot of f .
851,1,['range'], The Quantile Transformation,seg_61,"if ξ(ω) = t where t is not in the range of f , then (6) holds. if ξ(ω) = t where f (x) = t for exactly one x, then (6) holds. if ξ(ω) = t where f (x) = t for at least two distinct x’s, then (6) fails; theorem 6.2.1 shows that this can happen for at most a countable number of t’s. (or: graph a df f that exhibits the three types of points t, and the rest is trivial with respect to (6), since the value of f at any other point is immaterial. specifically, (6) holds for ω unless f has a flat spot at height t ≡ ξ(ω). note figure 3.1.)"
852,1,"['quantile', 'distribution', 'mean', 'associated', 'convergence']", The Quantile Transformation,seg_61,"definition 3.2 (convergence in quantile) let kn denote the qf associated with df fn, for each n ≥ 0. we write kn →d k0 to mean that kn(t) → k0(t) at each continuity point t of k0 in (0, 1). we then say that kn converges in quantile to k0, or kn converges in distribution to k0."
853,1,"['distribution', 'convergence', 'quantile']", The Quantile Transformation,seg_61,proposition 3.1 (convergence in distribution equals convergence in quantile)
854,0,[], The Quantile Transformation,seg_61,exercise 3.0 give the proof of the converse for the previous proposition.
855,0,[], The Quantile Transformation,seg_61,"summary fn−1(t) → f−1(t) for all but at most a countably infinite number of t’s (namely, for all but those t’s that have multiply realizable t-quantiles; these correspond to the heights of flat spots of f , and these flat spot heights t are exactly the discontinuity points of k)."
856,1,"['continuous', 'associated']", The Quantile Transformation,seg_61,"exercise 3.1 (left continuity of k) show that k(t) = f−1(t) is left continuous on (0, 1). (note that k is discontinuous at t ∈ (0, 1) if and only if the corresponding ut is nonvoid (see theorem 6.2.1). likewise, the jump points cj and the jump sizes dj of k(.) are equal to the t values and the λ(ut) values of the multiply realizable t-quantiles. we earlier agreed to use ai and bi for the jump points and jump sizes of the associated df f .)"
857,1,"['range', 'probability', 'continuous', 'transformation']", The Quantile Transformation,seg_61,"and equality fails if and only if t ∈ (0, 1) is not in the range of f on [−∞,∞]. ∼ (ii) (the probability integral transformation) if x has a continuous df f , then f (x) = uniform(0, 1). in fact, for any df f,"
858,1,['range'], The Quantile Transformation,seg_61,with equality failing if and only if t is not in the closure of the range of f . (iii) for any df f we have
859,1,['continuous'], The Quantile Transformation,seg_61,"∼ ∼ (iv) if f is a continuous df and f (x) = uniform(0, 1), then x = f . (v) graph f ◦ f−1 and f−1 ◦ f for the df f in figure 6.3.1."
860,1,"['independent', 'probability', 'transformation']", The Quantile Transformation,seg_61,"proposition 3.2 (the randomized probability integral transformation) let x denote an arbitrary rv. let f denote its df, and let (aj , bj)’s denote an enumeration of whatever pairs (jump point, jump size) the df f possesses. let η1, η2, . . . denote iid uniform(0, 1) rvs (that are also independent of x). then both"
861,1,"['independent', 'variation']", The Quantile Transformation,seg_61,"(we have reproduced the original x from a uniform(0, 1) rv that was defined using both x and some independent extraneous variation. note figure 3.1.)"
862,1,['random'], The Quantile Transformation,seg_61,proof. we have merely smoothed out the mass bj that f (x) placed at f (aj) by subtracting the random fractional amount ηjbj of the mass bj .
863,1,"['change of variable', 'statistician', 'variable', 'continuous']", The Quantile Transformation,seg_61,"exercise 3.3 (change of variable) let y =∼ g and x = h−1(y ) =∼ f , where h is ↗ and right continuous on the real line with left-continuous inverse h−1. (a) use the theorem of the unconscious statistician to conclude that"
864,1,['functions'], The Quantile Transformation,seg_61,"(10) ∫(−∞,h(x)] m(h−1) dg = ∫(−∞,x] m df for measurable functions m ≥ 0,"
865,1,['function'], The Quantile Transformation,seg_61,"∼ (b) let g = i,h = f , and y = ξ = uniform(0, 1) above, for any df f . let g denote any measurable function. then (via part (a), or via (2) and (3)), show that"
866,0,[], The Quantile Transformation,seg_61,reverse the inequalities if h ↘.
867,1,['set'], The Quantile Transformation,seg_61,"proof. we finally prove proposition 1.2.3. let d be a subset of [0, 1] that is not lebesgue measurable; its existence is guaranteed by proposition 1.2.2. let b ≡ f−1(d) for the lebesgue singular df f . then (6.1.10) shows that b is a subset of the cantor set c. since λ(c) = 0 and b ⊂ c, then b is a lebesgue set with λ(b) = 0; that is, b ∈ b̂λ. we now assume that b is borel set (and seek a contradiction). now f−1 is measurable by (6.1.10), and so (f−1)−1(b) ∈ b. but"
868,0,[], The Quantile Transformation,seg_61,the elementary skorokhod construction theorem
869,1,['marginal'], The Quantile Transformation,seg_61,"let x0,x1,x2, . . . be iid f . then xn →d x0, but the xn do not converge to x0 in the sense of →a.s.,→p, or → r. however, whenever xn →d x0, it is possible to replace the xn’s by rvs yn having the same (marginal) dfs, for which the stronger result yn →a.s. y0 holds."
870,1,['set'], The Quantile Transformation,seg_61,"∼ [0, 1] so that ξ = uniform (0, 1) on (ω,a, p ) ≡ ([0, 1],b ∩ [0, 1], λ), for lebesgue measure λ. let fn denote the df of xn, and define yn ≡ fn−1(ξ) for all n ≥ 0. let dk0 denote the at most countable discontinuity set of k0. then both"
871,0,[], The Quantile Transformation,seg_61,proof. this follows trivially from proposition 3.1.
872,0,[], The Quantile Transformation,seg_61,exercise 3.5 (wasserstein distance) let k = 1 or 2. define
873,1,[], The Quantile Transformation,seg_61,"(a) show that all such (fk, dk) spaces are complete metric spaces, and that"
874,0,[], Integration by Parts Applied to Moments,seg_63,integration by fubini’s theorem or “integration by parts” formulas are useful in many contexts. here we record a few of the most useful ones.
875,1,"['function', 'functions']", Integration by Parts Applied to Moments,seg_63,proposition 4.1 (integration by parts formulas) suppose that both the leftcontinuous function u and the right-continuous function v are monotone functions. then for any a ≤ b we have both
876,0,[], Integration by Parts Applied to Moments,seg_63,figure 4.1 integration by parts.
877,0,[], Integration by Parts Applied to Moments,seg_63,proof. we can apply fubini’s theorem at steps (a) and (b) to obtain
878,1,['variances'], Integration by Parts Applied to Moments,seg_63,"useful formulas for means, variances, and covariances"
879,0,[], Integration by Parts Applied to Moments,seg_63,thinking of x as f−1(ξ) presents alternative ways to approach problems.
880,1,"['mean', 'random']", Integration by Parts Applied to Moments,seg_63,"x d1[x≤x], where 1[ξ≤t] is a random df that puts mass 1 at the point ξ(ω) and 1[x≤x] is a random df that puts mass 1 at the point x. if x has a finite mean μ, then (depending on which representation of x we use) (4) μ = ∫"
881,0,[], Integration by Parts Applied to Moments,seg_63,"x df (x) moreover, when μ is finite we can combine the two previous formulas to write (5) x − μ = ∫"
882,1,[], Integration by Parts Applied to Moments,seg_63,"[s ∧ t − st] df−1(s) df−1(t) (true, even if e|x| = ∞). (in fact, see exercise 4.2 and (6.6.2) to rigorize the steps of the proof of (7)−(8), even when e|x| = ∞.) the parallel formula (via the same type of argument) is (9) var[x ] = ∫−"
883,1,['variances'], Integration by Parts Applied to Moments,seg_63,"(x − μ)2 df (x). proposition 4.2 (other formulas for means, variances, and covariances) (i) if x ≥ 0 has df f , then (11) ∫0"
884,1,"['continuous', 'marginal', 'joint']", Integration by Parts Applied to Moments,seg_63,"[f−1(t)]rdt in fact, one of the two integrals is finite if and only if the other is finite. (iv) let (x, y ) have joint df f with marginal dfs fx and fy . let g and h be ↗ and left continuous. then"
885,1,"['continuous', 'covariance', 'loss', 'case']", Integration by Parts Applied to Moments,seg_63,"whenever this covariance is finite. note the special case g = h = i for cov [x, y ]. hint. without loss, g−(0) = g+(0) = h−(0) = h+(0). make use of the fact that g(x) = ∫[0,∞) 1[0,x)(s) dg−(s) in the first quadrant, etc. (v) let k be ↗ and left continuous and ξ ∼ uniform(0, 1) (perhaps k = h(f−1) for"
886,1,['function'], Integration by Parts Applied to Moments,seg_63,"= an ↗ left-continuous function h, and for x ≡ f−1(ξ) for a df f ). when finite, (15) var [k(ξ)] = ∫0"
887,1,['combination'], Integration by Parts Applied to Moments,seg_63,"(a) use the fubini/tonelli combination (as above) to check that e(k̃a2,a′(ξ)) − (e(k̃a,a′(ξ))2 = var[k̃a,a′(ξ)] = ∫0 1 ∫0 1 1(a,1−a′)(s)1(a,1−a′)(t)(s ∧ t − st) dk(s) dk(t); essentially, obtain (8) for x̃a,a′ . then let (a∨a′) → 0, and apply the mct, to obtain (7) for general x. (use (6.6.2) to see that (8) holds even if e|x| = ∞.) (b) establish (9) using similar methods. exercise 4.3 (a) prove formulas (11)–(13). [hint. use integration by parts.] (b) prove the formula (14). exercise 4.4 prove the formulas in (17). exercise 4.5 give an extension of (13) to arbitrary rvs. exercise 4.6 (a) use fubini and use integration by parts to show twice that for arbitrary f and for every x ≥ 0 we have (18) ∫[0,x] y2 df (y) = 2 ∫0"
888,1,['continuous'], Integration by Parts Applied to Moments,seg_63,"x tp (x > t) dt − x2p (x > x). (b) verify (6.3.12) and (6.3.13) once again, with the current methods. exercise 4.7 (integration by parts formulas) we showed in proposition 4.1 earlier that d(uv ) = u−dv + v+du (with left continuous u and right continuous v ). (i) now show (noting that du− = du+) that (19) du2 = d(u−u+) = u−du + u+du = (2u + u) du for u ≡ u − u−. (ii) apply proposition 4.1 to 1 = u · (1/u) to obtain (20) d(1/u) = −{1/(u+u−)} du = −{1/(u(u + u))} du. (iii) show by induction that for k = 1, 2, . . . we have (21) duk = (∑i"
889,1,['mean'], Important Statistical Quantitieso,seg_65,"1 k̃a,a′(t) dt, which is the (a, a′)-w̃insorized mean of the rv k(ξ), and let"
890,1,['variance'], Important Statistical Quantitieso,seg_65,"1 ∫0 1[s ∧ t − st] dk̃a,a′(s) dk̃a,a′(t) denote the (a, a′)-w̃insorized variance (recall (6.4.8)). for general x, let (4) μ̃(a) ≡ μ̃(a, a), σ̃2(a) ≡ σ̃2(a, a), and k̃a(·) ≡ k̃a,a(·); but μ̃(a) ≡ μ̃0,a if x ≥ 0, etc."
891,1,['mean'], Important Statistical Quantitieso,seg_65,"−ań k(t) dt, μ̌n ≡ μ̌k(an, a′n) ≡ μ̌n/(1 − an − a′n) , μ̃n ≡ μ̃k(an, a′n) ≡ μk̃n ≡ ek̃n(ξ) ≡ ∫0 1 k̃n(t) dt, so that μ̌n is the (an, a′n)-̌trimmed mean, μ̃n is the (an, a′n)-w̃ insorized mean, and μ̌n is herein called the (an, an′ )-̌truncated mean of the rv k(ξ). then let"
892,1,"['variance', 'mean']", Important Statistical Quantitieso,seg_65,"≡ var[k̃n(ξ)] = ∫0 1 ∫0 1[s ∧ t − st] dk̃n(s) dk̃n(t) denote the (an, a′n)-w̃insorized variance. when they are finite, the mean μ and variance σ2 satisfy"
893,1,['case'], Important Statistical Quantitieso,seg_65,"′ as the trimming/winsorizing numbers and an, a′n as the trimming/ winsorizing fractions. describe the case of (10) as slowly growing to ∞."
894,1,"['sample', 'statistics', 'order statistics', 'continuous']", Important Statistical Quantitieso,seg_65,"suppose xn1, . . . , xnn is an iid sample with df f and qf k. let xn:1 ≤ · · · ≤ xn:n denote the order statistics (that is, they are the ordered values of xn1, . . . , xnn). let kn(.) on [0, 1] denote the empirical qf that equals xn:i on ((i − 1)/n, i/n], for 1 ≤ i ≤ n, and that is right continuous at zero. now let"
895,1,"['sample', 'sample mean', 'mean']", Important Statistical Quantitieso,seg_65,denote the sample mean and the “sample variance.” we also let
896,1,"['sample', 'statistics', 'order statistics', 'mean']", Important Statistical Quantitieso,seg_65,"xn:i + knxn−kn ′ ⎦ = μ̃kn(an, an) denote the sample (an, a′n)-t̆runcated mean, the sample (an, a′n)-ťrimmed mean, and the sample (an, a′n)-w̃insorized mean. let x̃n:1, . . . , x̃n:n denote the (an, a′n)-w̃insorized order statistics, whose empirical qf is k̃n. now note that"
897,1,"['sample', 'variance']", Important Statistical Quantitieso,seg_65,"denote the sample (an, a′n)-w̃insorized variance. let"
898,1,"['estimate', 'standardized', 'estimators']", Important Statistical Quantitieso,seg_65,"2 ≡ σ̃n 2/(1 − an − an′ )2 and s̆n 2 ≡ s̃n 2/(1 − an − an′ )2. of course, x̄n, sn, x̆n, s̃n estimate μ, σ, μ̆n, σ̃n. we also define the standardized forms of these estimators as"
899,1,['estimators'], Important Statistical Quantitieso,seg_65,σ σ̆n σ̃n and the studentized forms of these estimators as
900,1,"['statistical', 'probabilistic']", Important Statistical Quantitieso,seg_65,"sn s̆n s̃n (the first formula for ťn is for statistical application, while the second formula is for probabilistic theory.)"
901,1,"['sample median', 'sample', 'independent', 'median']", Important Statistical Quantitieso,seg_65,"we define the sample median ẍn to equal xn:(n+1)/2 or (xn:n/2 + xn:n/2+1)/2, according as n is odd or even. representing rvs we will often assume that the independent rvs xn1, . . . , xnn having df f and qf k are defined in terms of iid uniform(0, 1) rvs ξn1, . . . , ξnn (as above (6.4.3)) via (19) xnk ≡ k(ξnk) = f−1(ξnk) for 1 ≤ k ≤ n."
902,1,['data'], Important Statistical Quantitieso,seg_65,"as one alternative, we start with data rvs x1, . . . , xn that are iid k and then define uniform(0, 1) rvs ξ1o, . . . ξn"
903,1,"['order statistics', 'statistics']", Important Statistical Quantitieso,seg_65,"o:1 < · · · < ξn o:n denote the order statistics of the iid uniform(0, 1) rvs ξk0 of (20). let rn ≡ (rn1, . . . , rnn)′ denote the ranks of these ξ1o, . . . , ξn"
904,1,"['permutation', 'random']", Important Statistical Quantitieso,seg_65,"o , and let dn ≡ (dn1, . . . , dnn)′ denote their antiranks. thus the rank vector rn is a random permutation of the vector (1, 2, . . . , n)’ while dn is the inverse permutation. these satisfy"
905,0,[], Important Statistical Quantitieso,seg_65,we will learn later that
906,1,"['independent', 'data', 'random', 'processes']", Important Statistical Quantitieso,seg_65,"o:1, . . . , ξn o:n) and (rn1, . . . , rnn) are independent random vectors. using the rvs ξko of this paragraph corresponds (when forming the rank vector of data xk coming from a discontinuous df) to breaking ties at random in forming the ranks. such notation is sometimes used throughout the remainder of this book. the empirical df notation 5.2 (empirical dfs and processes) let x1, x2, . . . be iid with df f and qf k. the empirical df fn of (x1, . . . , xn) is defined by"
907,1,"['estimate', 'observation', 'step function', 'function', 'process', 'estimator']", Important Statistical Quantitieso,seg_65,this is a step function on the real line r that starts at height 0 and jumps by height 1/n each time the argument reaches another observation as it moves from left to right along the line. we can think of fn as an estimate of f . the important study of the empirical process (24) en(x) ≡ √n[fn(x) − f (x)] for x ∈ r will allow us to determine how this estimator fn of f performs.
908,1,['function'], Important Statistical Quantitieso,seg_65,"we also let ξ1, ξ2, . . . be iid uniform(0, 1), with true df the identity function i on [0, 1] and with uniform empirical df"
909,1,['process'], Important Statistical Quantitieso,seg_65,"the corresponding uniform empirical process is given by (26) un(t) ≡ √n[gn(t) − t] for t ∈ [0, 1]. if we now define an iid f sequence x1, x2, . . . via xk ≡ f−1(ξk) = k(ξk), then the empirical df and empirical process of these (x1, . . . , xn) satisfy"
910,1,"['probability', 'distributions']", Important Statistical Quantitieso,seg_65,"(fn − f ) = [gn(f ) − i(f )] on r and en = un(f ) on r, (27) valid for every ω, as follows by (6.3.3). (if we use the ξko’s of (6.3.8), then the fn on the left in (22) is everywhere equal to the fn of the original xk’s.)thus our study of properties of en can proceed via a study of the simpler un, which is then evaluated at a deterministic f. (recall also in this regard theorem 5.3.3 about probability being determined by the finite dimensional distributions.)"
911,1,"['variance', 'mean', 'case']", Infinite Varianceo,seg_67,"whenever the variance is infinite, (1) will show that the ťruncated variance σ̌2 dominates the square μ̌2 of the ťruncated mean. the same is true in the w̃insorized case, as in (2). let k̃a,a′ denote k winsorized outside (a, 1 − a′). theorem 6.1 (gnedenko–kolmogorov) every nondegenerate qf k satisfies (1) lim sup{∫a"
912,1,"['continuous', 'symmetric']", Infinite Varianceo,seg_67,"proof. let h > 0 be continuous, symmetric about t = 1/2, ↑ to ∞ on [1/2, 1), and suppose it satisfies ch ≡ ∫0"
913,1,[], Infinite Varianceo,seg_67,1 h2(t) dt < ∞. let b ≡ 1 − a′. then cauchy–schwarz provides the bound
914,1,['variance'], Infinite Varianceo,seg_67,2 (a) + a′k2(1 − a′)] → 0. exercise 6.1 (comparing contributions to the variance) let k(.) be arbitrary. establish the following elementary properties of qfs (of non-trivial rvs):
915,1,"['function', 'moment']", Infinite Varianceo,seg_67,"1 k(t) dt < ∞. there exists a positive ↘ function h(·) on (0, 1) with h(t) → ∞ as t → 0 for which 1 h(t)k(t) dt < ∞. (note exercise 3.5.12.) ∫0 proposition 6.1 let x have sth absolute moment e|x|s = ∞, and let 0 < r < s. let |x|sn ≡ n"
916,1,"['continuous', 'symmetric']", Infinite Varianceo,seg_67,"1 ∑n 1 |xk|s for iid rvs x1, x2, . . . distributed as x, etc. then (4) [ |x|rn]s/r/ |x|sn →a.s. 0 (using the following remark 6.1). remark 6.1 it is useful to give this proof now even though it will not be until the slln (theorem 8.4.2) that we prove e|x|s < ∞ yields |x|ns →a.s. e|x|s. likewise, e|x|s = ∞ yields |x|sn →a.s. ∞. these will be used in the proofs below. proof. we follow the proof of gnedenko–kolmogorov’s theorem 6.6.1, as we give the proof of (4) for s = 2 and r = 1. let kn denote the empirical qf. let h be positive, continuous, symmetric about t = 1"
917,1,[], Infinite Varianceo,seg_67,1 h2(t) dt < ∞. define a = 1 − b = 1/(2n). then cauchy–schwarz gives the bound
918,1,['intervals'], Infinite Varianceo,seg_67,"n (the “4” comes from the definition of a and b, which gives only half of the two end intervals) ≤ 4ch ∫a b[k2n(t)/h2(t)] dt/xn 2 by (a) h {∫c"
919,1,['limit'], Infinite Varianceo,seg_67,"2 →a.s. ∞ in the final step. (thus the numerator of the leading term in (c) converges a.s. to a finite number, while the denominator has an a.s. limit exceeding 2/ times the numerator.) exercise 6.4 (a) prove proposition 6.1 for general 0 < r < s. (b) now for iid rvs, (4) implies that →p 0 as well. hence, →p 0 is immediate if xn1, . . . , xnn are iid as x, for each n. be clear on this. proposition 6.2 (equivalent versions of negligibility) for any vector x ≡ (x1, . . . , xn), let"
920,1,['set'], Infinite Varianceo,seg_67,"2 ≡ [max1≤k≤n |xk − x̄n|2]/sn 2 where sn 2 ≡ xn 2 − (x̄n)2 n let x1, x2, . . . be iid as x, and set x= (x1, . . . , xn)′. then"
921,1,['set'], Infinite Varianceo,seg_67,"2 →a.s. 0 if and only if [max1≤k≤n xk2]/xn 2 →a.s. 0. n let xn1, . . . , xnn be iid as x, for each n ≥ 1. set x= (xn1, . . . , xnn)′. then"
922,1,['inequality'], Infinite Varianceo,seg_67,"2 ≤ 1 always holds by the liapunov inequality, we have (b) |x̄n|/√"
923,1,['independent'], Infinite Varianceo,seg_67,"nxn 2 ≤ 1/√n →a.s. 0 always holds for all rvs. thus the second term in the numerator always goes to zero for all rvs (independent, or not). consult remark 6.1 for the following two claims. the denominator of (a) converges a.s. to 1 − 0 = 1 if ex2 = ∞ (by (4)), while the denominator of (a) converges a.s. to (1 − e2x/ex2) < 1 if ex2 < ∞. thus dn"
924,1,['moment'], Infinite Varianceo,seg_67,"2}1/2 → 0 for a.e. fixed ω. this gives (6). then (7) follows analogously using exercise 6.4b. proposition 6.3 let x ≥ 0 with exs = ∞ have df f and qf k. the rth partial absolute moment mr is defined on [0, 1] by (8) mr(t) ≡ ∫0"
925,1,['average'], Infinite Varianceo,seg_67,"1−t kr(u) du = ∫t 1 mr(u) du, where mr(t) ≡ [k(1 − t)]r. then, for 0 < r < s, (9) tms(t)/ms(t) → 0 implies tmr(t)/mr(t) → 0. (sections c.2–c.3) contain a good deal more in the spirit of (1)–(3) and (9).) proof. raising to a power increases a maximum more than an average, and so"
926,1,"['states', 'case', 'central limit theorem', 'varying', 'limit']", Infinite Varianceo,seg_67,"t ∫(t,1] ks(1 − u) du ms(t) for all t so close to 1 that k(1 − t) > 1. this establishes (9). the clt and slowly varying functions∗ let xn1, . . . , xnn be iid with df f and qf k ≡ f−1. the central limit theorem (clt) states that z̄n ≡ √n[x̄n − μ]/σ →d n(0, 1) when σ2 is finite. in this case"
927,1,"['case', 'second moment', 'moment']", Infinite Varianceo,seg_67,"2 (ct) ∨ k2(1 − ct)]/σ2 → 0 as t → 0, for each fixed c > 0; extend the calculation just below (6.4.6) to the second moment case to see this."
928,1,['mean'], Infinite Varianceo,seg_67,"what if σ2 is infinite? let ξ denote a uniform(0, 1) rv. let x ≡ k(ξ), so that x has df f . let x̃(a) ≡ k̃a,a(ξ), with mean μ̃(a) ≡ ∫0"
929,1,['variance'], Infinite Varianceo,seg_67,"1 k̃a,a(t) dt and variance (12) σ̃2(a) = ∫0"
930,1,['mean'], Infinite Varianceo,seg_67,"1 ∫0 1[r ∧ s − rs] dk̃a,a(r) dk̃a,a(s), which increases to σ2 as a ↘ 0, whether σ2 is finite or infinite. this expression makes no reference to any mean (such as μ or μ̃(a)). this is nice! it turns out that (13) z̄n ≡ √n{x̄n − μ̃(1/n)}/σ̃(1/n) →d n(0, 1) if and only if"
931,1,"['functions', 'quantile', 'independent', 'treatment', 'case', 'varying', 'variation', 'tails']", Infinite Varianceo,seg_67,"2 (ct) ∨ k2(1 − ct)]/σ̃2(t) → 0 as t → 0, for each fixed c > 0. this (14) holds if and only if σ̃2(t) does not grow too fast; that is, if and only if (15) σ̃2(ct)/σ̃2(t) → 1 as t → 0, for each fixed c > 0. it is appropriate to examine such slow variation of σ̃2(t) in the infinite-variance case. we shall do so very carefully in appendix c, in both the df domain and the quantile domain. often this slow variation question is examined in the context of the clt and is treated in the context of the general theory of slowly varying functions. but the equivalence of similar conditions in appendix c will follow from a careful treatment of the easier wlln and treating slowly varying functions in an elementary fashion from simple pictures and a dash of cauchy–schwarz. the connection to the clt (16) will not be made until the optional sections 10.5–40.6. to use this clt one might verify the limited contribution of the tails, via (18) or (17), and then claim (16), (20), and (19). theorem 6.2 (a studentized clt) let xn1, . . . , xnn be row independent, all with nondegenerate df f . conditions (14) and (15) are each equivalent to the conditions below. (16) √n{x̄n − μ̃(1/n)}/σ̃(1/n) →d n(0, 1). (17) r(x) ≡ x2p (|x| > x)/∫[y≤"
932,1,"['confidence', 'interval', 'confidence interval']", Infinite Varianceo,seg_67,"2/σ̃2(1/n) →p 1. or, xn 2/u1n →p 1, where u1n = u(x1n) for the (1 − 1/n)-quantile x1n of |x|. these claims imply the studentized result √n{x̄n − μ̃(1/n)}/sn →d n(0, 1), which easily leads to an asymptotically valid confidence interval for μ̃(1/n).) (see theorem 10.6.1 for a list containing many more equivalent versions.)"
933,1,"['independent', 'independence', 'events', 'independent events']", Independence,seg_71,the idea of independence of events a and b is that the occurrence or nonoccurrence of a has absolutely nothing to do with the occurrence or nonoccurrence of b. it is customary to say that a and b are independent events if
934,1,"['independent', 'independence', 'events']", Independence,seg_71,classes c and d of events are called independent classes if (1) holds for all a ∈ c and all b ∈ d. we need to define the independence of more complicated collections.
935,1,"['independence', 'independent', 'probability']", Independence,seg_71,"definition 1.1 (independence) consider a fixed probability space (ω,a, p ). (a) consider various sub σ-fields of a. call such σ-fields a1, . . . ,an independent σ-fields if they satisfy"
936,1,['independent'], Independence,seg_71,"the σ-fields a1,a2, . . . are called independent σ-fields if a1, . . . ,an are independent for each n ≥ 2. (use this definition for arbitrary classes a1, . . . ,an, too.) (b) rvs x1, . . . , xn are called independent rvs if the σ-fields f(xi) ≡ xi"
937,1,"['independent', 'events', 'independent events']", Independence,seg_71,"are independent. rvs x1,x2, . . . are called independent rvs if x1, . . . , xn are independent for each n ≥ 2. (c) events a1, . . . , an are called independent events if the σ-fields σ[a1], . . . , σ[an] are independent σ-fields; here note that"
938,1,['events'], Independence,seg_71,the next exercise is helpful because it will relate the rather formidable definition of independent events in (3) back to the simple definition (1).
939,1,['independent'], Independence,seg_71,"exercise 1.1 (a) show that p (ab) = p (a)p (b) if and only if {∅, a,ac, ω} and {∅, b,bc,ω} are independent σ-fields. [thus we maintain the familiar (1).] (b) show that a1, . . . , an are independent if and only if"
940,1,"['independent', 'combinations', 'table', 'contingency table', 'partitions', 'events', 'independent events', 'venn diagram', 'probability', 'venn']", Independence,seg_71,"remark 1.1 when discussing a pair of possibly independent events, one should draw the venn diagram as a square representing ω divided into half vertically (with respect to a,ac) and into half horizontally (with respect to b,bc) creating four cells (rather than as the familiar two-circle picture). also, if one writes on the table the probability of each of the four combinations ab, abc, acb,acbc, one has the contingency table superimposed on the picture. (see figure 1.1) [this extends to two partitions (a1, . . . , am) and (b1, . . . , bn), but not to three events.]"
941,1,['table'], Independence,seg_71,figure 1.1 the 2 × 2 table.
942,1,"['expectation', 'independent']", Independence,seg_71,"theorem 1.1 (expectation of products) suppose x and y are independent rvs for which g(x) and h(y ) are integrable. then g(x)h(y ) is integrable, and"
943,0,[], Independence,seg_71,"proof. the assertion is obvious for g = 1a and h = 1b . now hold g = 1a fixed, and proceed through simple and nonnegative h. then with h held fixed, proceed through simple, nonnegative, and integrable g. then extend to integrable h."
944,1,"['independence', 'independent']", Independence,seg_71,proposition 1.1 (extending independence on π-systems) (a) suppose the π-system c and a class d are independent. then
945,1,['independent'], Independence,seg_71,σ[c] and d are independent.
946,1,['independent'], Independence,seg_71,(b) suppose the π-systems c and d are independent. then
947,1,['independent'], Independence,seg_71,σ[c] and σ[d] are independent σ-fields.
948,1,['independent'], Independence,seg_71,"(c) if c1, . . . , cn are independent π-systems (see (2)), then"
949,1,['independent'], Independence,seg_71,"σ[c1], . . . , σ[cn] are independent σ-fields."
950,0,[], Independence,seg_71,"we now demonstrate that cd is a λ-system (that trivially contains c). trivially, ω ∈ cd. if a,b ∈ cd with a ⊂ b, then"
951,1,"['disjoint', 'independent', 'sets', 'disjoint sets']", Independence,seg_71,"theorem 1.2 let x1,x2, . . . be independent rvs on (ω,a, p ). let i ≡ (i1, i2, . . .) and j ≡ (j1, j2, . . .) be disjoint sets of integers. (a) then"
952,1,['independent'], Independence,seg_71,"(6) f(xi1 ,xi2 , . . .) and f(xj1 ,xj2 , . . .) are independent σ-fields."
953,1,"['sets', 'disjoint', 'disjoint sets']", Independence,seg_71,(b) this extends immediately to countably many disjoint sets of integers.
954,1,"['functions', 'disjoint', 'independent', 'independence', 'sets', 'disjoint sets']", Independence,seg_71,"corollary 1 (preservation of independence) any rvs h1(xi1 ,xi2 , . . .), h2(xj1 ,xj2 . . .), . . . (that are based on disjoint sets of the underlying independent rvs xk) are themselves independent rvs, for any choice of the b-b∞-measurable functions h1, h2, . . .."
955,1,['sets'], Independence,seg_71,"proof. let c denote all sets of the form c ≡ [xi1 ∈ b1, . . . , xim ∈ bm], for some m ≥ 1 and for b1, . . . , bm in b. let d denote all sets of the form d ≡ [xj1 ∈ b1"
956,1,['sets'], Independence,seg_71,for some n ≥ 1 and sets b1
957,1,['independence'], Independence,seg_71,n =1 p (xjl ∈ bl′) by independence
958,1,['independence'], Independence,seg_71,n =1[xjl ∈ bl′]) by independence
959,1,"['disjoint', 'independent', 'sets', 'disjoint sets']", Independence,seg_71,"so that c and d are independent classes. thus σ[c] and σ[d] are independent by proposition 1.1(b), as is required for (6). the extension to countably many disjoint sets of indices is done by induction using proposition 1.1(c), and is left to the exercises. (the corollary is immediate.)"
960,1,['independence'], Independence,seg_71,criteria for independence
961,1,['independent'], Independence,seg_71,"theorem 1.3 the rvs (x1, . . . , xn) are independent rvs if and only if"
962,1,"['factors', 'independence', 'joint']", Independence,seg_71,"proof. clearly, independence implies that the joint df factors. for the converse we suppose that the joint df factors. then for all x1, . . . , xn we have"
963,1,"['independence', 'independent']", Independence,seg_71,"that is, the classes ci ≡ {[xi ≤ xi] : xi ∈ r} are independent, and they are π-systems, with σ[ci] = f(xi). independence of x1, . . . , xn then follows from proposition 1.1(c)."
964,1,['independent'], Independence,seg_71,"exercise 1.4 rvs x,y that take on only a countable number of values are independent if and only if p ([x = ai][y = bj ]) = p (x = ai)p (y = bj) for all i, j."
965,1,"['joint', 'independent', 'factors']", Independence,seg_71,"exercise 1.5 show that rvs x,y having a joint density f(., .) are independent if and only if the joint density factors to give f(x, y) = fx(x)fy (y) for a.e. x, y."
966,1,"['characteristic function', 'independent', 'factors', 'function']", Independence,seg_71,"remark 1.2 that rv’s x,y are independent if and only if their characteristic function factors appears as theorem 9.5.3 of section 9."
967,1,"['tail', 'random']", The Tail σField,seg_73,"definition 2.1 (the tail σ-field) consider an arbitrary random element x ≡ (x1, x2, . . .) from (ω,a, p ) to (r∞,b∞). then t ≡ ⋂n"
968,1,"['tail', 'event']", The Tail σField,seg_73,"∞ =1 f(xn,xn+1, . . .) is called the tail σ-field, and any event d ∈ t is called a tail event."
969,1,"['tail', 'events', 'independent']", The Tail σField,seg_73,"theorem 2.1 (kolmogorov’s 0-1 law) if x1,x2, . . . are independent rvs, then p (d) equals 0 or 1 for all tail events d in the tail σ-field t ."
970,1,"['independent', 'approximation', 'set']", The Tail σField,seg_73,"proof. fix a set d ∈ t , and then note that d ∈ f(x) = x−1(b∞). by the halmos approximation lemma of exercise 1.2.3 and the introduction to section 2.5, there exists an integer n and a set dn in the nth member of ∪mf(x1, . . . , xm) = ∪mx−m1(bm) = (a field) such that p (dn d) → 0. thus both p (d ∩ dn) → p (d) and p (dn) → p (d) occur. happily, d ∈ t ⊂ f(xn+1, . . .). so that d and dn ∈ f(x1, . . . , xn) are independent. hence"
971,1,"['independent', 'sets']", The Tail σField,seg_73,"remark 2.1 (sequences and series of independent rvs converge a.s., or almost never) note that for any borel sets b1, b2, . . . in b,"
972,0,[], The Tail σField,seg_73,the following result has thus been established.
973,1,['independent'], The Tail σField,seg_73,theorem 2.2 sequences and series of independent rvs can only converge either a.s. or almost never.
974,1,['symmetric'], The Tail σField,seg_73,the symmetric σ-field
975,1,"['symmetric', 'sets', 'set']", The Tail σField,seg_73,"definition 2.2 (symmetric sets) let π denote any mapping of the integers onto themselves that (for some finite n) merely permutes the first n integers. let x ≡ (x1,x2, . . .) be b∞-measurable, and set xπ ≡ (xπ(1),xπ(2), . . .). then a ≡ x−1(b) for some b ∈ b∞ is called a symmetric set if a = xπ−1(b) for all such π. let s denote the collection of all symmetric sets."
976,1,"['symmetric', 'tail', 'approximation']", The Tail σField,seg_73,"exercise 2.1 (hewitt-savage 0-1 law) let x ≡ (x1,x2, . . .) have iid coordinates xk. (a) show that p (a) equals 0 or 1 for every a in s. (b) show that s is a σ-field, called the symmetric σ-field. (c) show that the tail σ-field t is a subset of the symmetric σ-field s. (d) give an example where t is a proper subset of s. (hint. use the approximation lemma of exercise 1.2.3 as it was used above.)"
977,1,"['variance', 'mean', 'probability']", Uncorrelated Random Variables,seg_75,"recall from definition 3.4.1 that x =∼ (μ, σ2) denotes that the rv x on a probability space (ω,a, p ) has mean μ and a variance σ2 that is assumed to be finite."
978,1,"['variances', 'correlation']", Uncorrelated Random Variables,seg_75,"definition 3.1 (correlation) if x1, . . . , xn have finite variances, then we call them uncorrelated if cov[xi,xj ] ≡ e{(xi − exi)(xj − exj)} = 0 for all i = j. define the dimensionless quantity"
979,1,"['covariance', 'covariance matrix', 'correlation']", Uncorrelated Random Variables,seg_75,"to be the correlation between xi and xj . if x ≡ (x1, . . . , xn), then the n × n covariance matrix of x is defined to be the matrix σ ≡ |[σij ]| whose (i, j)th element is σij ≡ cov[xi, xj ]."
980,1,"['independent', 'variances']", Uncorrelated Random Variables,seg_75,proposition 3.1 independent rvs with finite variances are uncorrelated.
981,1,[], Uncorrelated Random Variables,seg_75,by cauchy–schwarz.
982,1,['variances'], Uncorrelated Random Variables,seg_75,"(1) | corr [x,y ]| ≤ 1 for any x and y having finite variances."
983,0,[], Uncorrelated Random Variables,seg_75,"in particular, suppose x1, . . . , xn are uncorrelated (μ, σ2). then"
984,0,[], Uncorrelated Random Variables,seg_75,proof. this is all trivial.
985,1,"['conditional expectation', 'results', 'conditional', 'expectation']", Basic Properties of Conditional Expectation ,seg_77,"the lebesgue integral is a widely applicable tool that extended the value of the reimann approach. it allows more general “heavy duty results.” so too, we now need to extend and rigorize our elementary approach to conditional expectation, in a way that keeps the useful results intact. (illustrations follow the definitions.)"
986,1,"['conditional expectation', 'conditional', 'expectation', 'mean', 'probability', 'function']", Basic Properties of Conditional Expectation ,seg_77,"definition 4.1 (conditional expectation) let (ω,a, p ) denote a probability space. let d denote a sub σ-field of a. let y be a rv on (ω,a, p ) for which e|y | < ∞. by e(y |d)(·) we mean any d-measurable function on ω such that"
987,1,"['conditional expectation', 'conditional', 'expectation', 'function']", Basic Properties of Conditional Expectation ,seg_77,"such a function exists and is unique a.e. p , as is seen below; we call this the conditional expectation of y given d. if x is another rv on (ω,a, p ), then"
988,0,[], Basic Properties of Conditional Expectation ,seg_77,justification of definition 4.1. let e|y | < ∞. define a signed measure ν on d by
989,1,['function'], Basic Properties of Conditional Expectation ,seg_77,"now, ν is a signed measure on (ω,d) by example 4.1.1, and the restriction of p to d (denoted by p |d) is another signed measure on (ω,d). moreover, that ν p |d is trivial. thus the radon–nikodym theorem guarantees, uniquely a.e. p |d, a d-measurable function h such that (recall exercise 3.2.3 for the second equality)"
990,1,['function'], Basic Properties of Conditional Expectation ,seg_77,"now, being d-measurable and unique a.s. p |d implies that the function h is unique a.s. p . define e(y |d) ≡ h. radon–nikodym derivatives are only unique a.e., and any function that works is called a version of the radon–nikodym derivative."
991,1,['function'], Basic Properties of Conditional Expectation ,seg_77,"proposition 4.1 suppose that z is a rv on (ω,a) that is f(x)-measurable. then there exists a measurable function g on (r,b) such that z = g(x)."
992,0,[], Basic Properties of Conditional Expectation ,seg_77,proof. this is just proposition 2.2.5 again.
993,1,"['function', 'statistician']", Basic Properties of Conditional Expectation ,seg_77,"notation 4.1 since e(y |x) = e(y |f(x)) is f(x)-measurable, the previous proposition shows that h ≡ e(y |x) = g(x) for some measurable function g on (r,b). the theorem of the unconscious statistician gives ∫x"
994,1,"['function', 'set']", Basic Properties of Conditional Expectation ,seg_77,general set d ∈ f(x) as d = x−1(b) for some b ∈ b. thus we may define e(y |x = x) = g(x) to be a b-measurable function on r for which
995,1,['function'], Basic Properties of Conditional Expectation ,seg_77,"this function e(y |x = x) exists and is unique a.s. px , as above. in summary:"
996,1,"['standard', 'conditional', 'probability', 'conditional probability']", Basic Properties of Conditional Expectation ,seg_77,"definition 4.2 (conditional probability) since p (a) = e1a for standard probability, define the conditional probability of a given d, denoted by p (a|d), by"
997,1,['function'], Basic Properties of Conditional Expectation ,seg_77,"equivalently, p (a|d) is a d-measurable function on ω satisfying"
998,1,['function'], Basic Properties of Conditional Expectation ,seg_77,"thus p (a|x)(ω) = g(x(ω)), where g(x) ≡ p (a|x = x) is a b-measurable function satisfying"
999,1,['function'], Basic Properties of Conditional Expectation ,seg_77,this function exists and is unique a.s. px .
1000,1,"['sample', 'treatment', 'conditional probability', 'case', 'conditional', 'probabilistic', 'distribution', 'discrete', 'probability distribution', 'probability', 'event', 'sample space']", Basic Properties of Conditional Expectation ,seg_77,"discussion 4.1 (discrete case; the elementary treatment) given that the event b has occurred (with p (b) > 0), how likely is it now for the event a to occur. the classic elementary approach defines the conditional probability of a given b by p (a|b) ≡ p (ab)/p (b). thus we have taken a revisualized view of things, while regarding b as the updated sample space. for any event b, only that portion ab of the event a is relevant (as b was known to have occurred). thus all that matters is the probabilistic size p (ab) of ab relative to the probabilistic size p (b) of b. the resulting p (·|b) is a probability distribution over ab ≡ {ab : a ∈ a}."
1001,1,"['mass function', 'function', 'discrete']", Basic Properties of Conditional Expectation ,seg_77,"for discrete rvs x and y with mass function p(·, ·) this leads to"
1002,1,"['mass function', 'conditional', 'function']", Basic Properties of Conditional Expectation ,seg_77,for the conditional mass function. it is then natural to define
1003,1,"['treatment', 'conditional', 'sets', 'summation', 'probability', 'conditional probability']", Basic Properties of Conditional Expectation ,seg_77,"exercise 4.1 (“discrete” conditional probability; the general treatment) suppose ω = ∑i di, and then define d = σ[d1,d2, . . .]. show that (whether the summation is finite or countable) the different expressions needed on the different sets di of the partition d can be combined together via"
1004,1,['function'], Basic Properties of Conditional Expectation ,seg_77,p (di) function e(y |d) takes the form
1005,1,"['conditional probability', 'case', 'conditional', 'discrete', 'probability', 'standard']", Basic Properties of Conditional Expectation ,seg_77,"with the term in braces defined to be 0 if p (di) = 0 (just for definiteness). (we note that the standard elementary approach to conditional probability is, in the discrete case, embedded within (8) and (9)–but it sits in there “differently.”)"
1006,1,"['conditional expectation', 'conditional', 'expectation', 'probability', 'conditional probability']", Basic Properties of Conditional Expectation ,seg_77,figure 4.1 conditional probability and conditional expectation
1007,1,"['sample', 'with replacement', 'conditional distributions', 'conditional', 'replacement', 'set', 'probability', 'function', 'sample space', 'probability function', 'distributions']", Basic Properties of Conditional Expectation ,seg_77,"example 4.1 let an urn consist of six balls identical except for the numbers 1, 2, 2, 3, 3, 3. let x1 and x2 represent a sample of size two drawn with replacement, and set y = x2 and s = x1 + x2. consider figure 4.1 above. in the figure (a) we see the sample space ω of (x1,x2) with the values of s superimposed, while the figure (b) superimposes the probability function on the same representation of ω. in the figure (c) we picture the five “diagonal sets” that generate d ≡ s−1(b). the three-part figure (d) depicts p (y = i|d)(·) as a d-measurable function on ω for each of the three choices [y = 1], [y = 2], and [y = 3] for a, while the figure (e) depicts e(y |d)(ω) as a d-measurable function. (had we used the elementary definition of p (y = ·|s = k) as a function of y for each fixed k, then the conditional distributions would have been those shown along the five diagonals in the figure (f), while e(y |s = k) is shown at the end of each diagonal.)"
1008,0,[], Basic Properties of Conditional Expectation ,seg_77,remark 4.1 it will be seen that (with a and ai’s regarded as fixed)
1009,1,"['distribution', 'probability distribution', 'probability']", Basic Properties of Conditional Expectation ,seg_77,"(to see these, just apply parts (16)(monotonicity) and (17)(mct) of theorem 4.1 appearing below.) these properties remind us of a probability distribution."
1010,1,"['treatment', 'case', 'conditional', 'discrete', 'distribution', 'continuous', 'conditional distribution']", Basic Properties of Conditional Expectation ,seg_77,"discussion 4.2 (continuous case; the elementary treatment) for discrete rvs, the conditional distribution is specified by"
1011,1,['continuous'], Basic Properties of Conditional Expectation ,seg_77,(this is in line with discussion 4.1.) one “natural approximation” of this approach for continuous rvs considers
1012,1,['discrete'], Basic Properties of Conditional Expectation ,seg_77,"but making this approach rigorous fails without sufficient smoothness, and leads to a tedious and limited theory. so elementary texts just suggest the even more blatant and “less rigorous” imitation of the discrete result via"
1013,1,"['densities', 'case']", Basic Properties of Conditional Expectation ,seg_77,discussion 4.3 suggests that the general approach of this section should ultimately lead to this same elementary result in the case when densities do exist.
1014,1,"['curve', 'slope']", Basic Properties of Conditional Expectation ,seg_77,"moreover, if (x(t), y(t)), a ≤ t ≤ b, parametrizes a smooth curve (imagine a circle about the origin, or a line of slope 1350), it is definition 4.2 that leads rigorously to formulas of the type"
1015,1,"['curve', 'conditional density', 'conditional']", Basic Properties of Conditional Expectation ,seg_77,for the conditional density at the point t given that one is on the curve.
1016,1,"['treatment', 'case', 'conditional', 'set', 'probability', 'conditional probability', 'continuous']", Basic Properties of Conditional Expectation ,seg_77,"discussion 4.3 (continuous case; the general treatment) let us consider the current approach to conditional probability. we will illustrate it in a special case. let a ∈ b2 denote a two-dimensional borel set. let t ≡ t (x) ≡ (x12 + x22)1/2, so that t = t defines (in the plane ω = r2) the circle ct ≡ {(x1, x2) : x12 + x22 = t2}. let b ∈ b denote a one-dimensional borel set of t’s, and then let d ≡ t−1(b) = ∪{ct : t ∈ b}. requirements (6) and (7) (in a manner similar to exercise 4.1, but from a different point of view than used in discussion 4.1) become"
1017,1,['probabilistic'], Basic Properties of Conditional Expectation ,seg_77,"so if ga(·) is given a value at t indicating the probabilistic proportion of a ∩ ct that belongs to a (or h(x) is given this same value at all x ∈ ct), then the above equation ought to be satisfied."
1018,1,['densities'], Basic Properties of Conditional Expectation ,seg_77,"(when densities exist, such a value would seem to be ga(t) = ∫ct 1a(x)px(x)dx/∫ct px(x)dx,"
1019,1,['densities'], Basic Properties of Conditional Expectation ,seg_77,"(when densities exist, the value e(y |t = t) = g(t) = ∫ y (x)p(x) dx/∫ p(x) dx seems"
1020,0,[], Basic Properties of Conditional Expectation ,seg_77,"ct ct appropriate, with h(x) assigning this same value for all x in ct.)"
1021,1,"['conditional', 'discrete', 'probability', 'function', 'sample', 'probability distributions', 'mass function', 'cases', 'sets', 'standard', 'sample space', 'distributions', 'conditional expectation', 'distribution', 'expectation', 'summation', 'continuous']", Basic Properties of Conditional Expectation ,seg_77,"the reader is urged to draw an (x1, x2)-plane to serve as the sample space for x ≡ (x1,x2) and a half line [0,∞) to serve as a sample space for t ≡ (x12 + x22)1/2. then any d ≡ f(t ) measurable function, such as ga(t) ≡ p (a|t = t), leads to the function ha(x) = ga(t (x)) = ga(t) that is a constant on every circle ct in the (x1, x2)-plane. picture this on your diagram. we need to guarantee that our specific guess for the function ha(x) = ga(t) = p (a|t = t) works—that is, (6) and (7) hold. happily, theorems in section 7.5 can be summarized by saying that in all the old standard problems we need not bother. when we fix t, the function p (a|t = t) = ga(t) behaves just like the old probability distributions over all of the sets a in a. great! so why are we doing this? the answer is by analogy. we already knew how to take the expectation of a function of a rv that has a density (we used an integral), or a mass function (we used a summation), but by this point we have learned how to rigorously do all cases at once using only the expectation sign format e(·) (whether the distribution is absolutely continuous, discrete, singular, or a mixture of these). it can be very useful to learn how do general cases for conditional expectation as well. to this end we will now call on john wayne and the cavalry (and this has the distinct look of the halmos-savage approach via a radon-nikodym derivative) and the bullets can be found in the upcoming theorem 4.1."
1022,1,"['without replacement', 'case', 'replacement', 'sampling']", Basic Properties of Conditional Expectation ,seg_77,"exercise 4.2 (a) (i) mimic discussion 4.2 in case t ≡ x1 + x2, instead. (ii) make up another interesting example. (b) (iii) repeat example 4.1 and the accompanying figure, but now in the context of sampling without replacement. (iv) make up another interesting example."
1023,1,"['function', 'probabilities', 'table']", Basic Properties of Conditional Expectation ,seg_77,"exercise 4.3 let y be a rv on some (ω,a, p ) that takes on the eight values 1, . . . , 8 with probabilities 1/32, 2/32, 3/32, 4/32, 15/32, 4/32, 1/32, 2/32, respectively. let c ≡ f(y ), and let ci ≡ [y = i] and pi ≡ p (ci) for 1 ≤ i ≤ 8. let d ≡ σ[{c1 + c5, c2 + c6, c3 + c7, c4 + c8}], e ≡ σ[{c1 + c2 + c5 + c6, c3 + c4 + c7 + c8}] and f ≡ {ω, ∅}. (a) represent ω as a 2 × 4 rectangle having eight 1 × 1 cells representing c1, . . . , c4 in the first row and c5, . . . , c8 in the second row. enter the appropriate values of y (ω) and pi in each cell, forming a table. evaluate e(y ). (b) evaluate e(y |d). present this function in a similar table. evaluate e(e(y |d)). (c) evaluate e(y |e). present this function in a similar table. evaluate e(e(y |e)). (d) evaluate e(y |f). present this function in a similar table. evaluate e(e(y |f))."
1024,1,"['conditional expectation', 'conditional', 'expectation', 'expectations']", Basic Properties of Conditional Expectation ,seg_77,"theorem 4.1 (properties of conditional expectation) let x,y, yn be integrable rvs on (ω,a, p ). let d be a sub σ-field of a. let g be measurable. then for any versions of the conditional expectations, the following hold:"
1025,1,[], Basic Properties of Conditional Expectation ,seg_77,"(14) (linearity) e(ax + by |d) = ae(x|d) + be(y |d) a.s. p (or, a.s. (p |d))."
1026,1,[], Basic Properties of Conditional Expectation ,seg_77,(16) (monotonicity) x ≤ y a.s. p implies e(x|d) ≤ e(y |d) a.s. p.
1027,1,['independent'], Basic Properties of Conditional Expectation ,seg_77,"(21) if f(y ) and d are independent, then e(y |d) = ey a.s. p."
1028,1,['stepwise'], Basic Properties of Conditional Expectation ,seg_77,"(22) (stepwise smoothing). if d ⊂ e ⊂ a, then e[e(y |e)|d] = e(y |d) a.s. p."
1029,1,['independent'], Basic Properties of Conditional Expectation ,seg_77,"(23) if f(y,x1) is independent of f(x2), then e(y |x1,x2) = e(y |x1) a.s. p."
1030,0,[], Basic Properties of Conditional Expectation ,seg_77,"(24) cr,hölder, liapunov, minkowski, and jensen inequalities hold for e(·|d)."
1031,1,['expectation'], Basic Properties of Conditional Expectation ,seg_77,"proof.o we first prove (14). now, by linearity of expectation,"
1032,0,[], Basic Properties of Conditional Expectation ,seg_77,"to prove (15), simply note that"
1033,0,[], Basic Properties of Conditional Expectation ,seg_77,"statement (17) follows easily from (16), since we have"
1034,0,[], Basic Properties of Conditional Expectation ,seg_77,"and we can appeal to the uniqueness of radon–nikodym derivatives, or apply exercise 3.2.2. now we use (16) and (17) to prove (18). thus"
1035,1,[], Basic Properties of Conditional Expectation ,seg_77,≤ lim inf e(yk|d) by the monotonicity of (16) n k≥n
1036,1,"['indicator', 'case']", Basic Properties of Conditional Expectation ,seg_77,"to prove (20) we proceed through indicator, simple, nonnegative, and then general functions, and each time we apply exercise 3.2.2 at the final step. case 1: y = 1d∗ . then"
1037,0,['n'], Basic Properties of Conditional Expectation ,seg_77,n ai1di . then
1038,1,['case'], Basic Properties of Conditional Expectation ,seg_77,n ai ∫d 1dix dp by case 1
1039,1,['functions'], Basic Properties of Conditional Expectation ,seg_77,case 3: y ≥ 0. let simple functions yn ↗ y where yn ≥ 0. suppose first that x ≥ 0. then we have
1040,1,['case'], Basic Properties of Conditional Expectation ,seg_77,= ∫d limn yne(x|d) dp by case 2
1041,1,['case'], Basic Properties of Conditional Expectation ,seg_77,"for general x, use x = x+ − x− and the linearity of (14). case 4: general y . just write y = y + − y −"
1042,0,[], Basic Properties of Conditional Expectation ,seg_77,"to prove (21), simply note that for each d ∈ d one has"
1043,0,[], Basic Properties of Conditional Expectation ,seg_77,and apply exercise 3.2.2. assertion (22) is proved by noting that
1044,0,[], Basic Properties of Conditional Expectation ,seg_77,the integrands of the two extreme terms must be equal a.s. by the exercise 3.2.2.
1045,1,['sets'], Basic Properties of Conditional Expectation ,seg_77,"since ν1 and ν2 are measures on f(x1,x2) that agree on all sets in the π̄-system consisting of all sets of the form d = d1 ∩ d2, they agree on the σ-field f(x1,x2) by the dynkin π–λ theorem. thus the integrands satisfy e(y |x1,x2) = e(y |x1) a.s."
1046,0,[], Basic Properties of Conditional Expectation ,seg_77,"we next prove (25), leaving most of (24) and (26) to the exercises. we have"
1047,1,"['conditional', 'inequality']", Basic Properties of Conditional Expectation ,seg_77,(i) ≤ e[e(|yn − y |r|d)] by the conditional jensen inequality of (24)
1048,1,"['linear', 'inequality']", Basic Properties of Conditional Expectation ,seg_77,"(durrett) we now turn to the jensen inequality of (24). the result is trivial for linear g. otherwise, we define"
1049,1,"['null sets', 'sets', 'union']", Basic Properties of Conditional Expectation ,seg_77,"hence (as the union of a countable number of null sets is null), (l) and (m) give"
1050,1,"['dispersion', 'independent', 'inequality']", Basic Properties of Conditional Expectation ,seg_77,exercise 4.5 (dispersion inequality) suppose that x and y are independent rvs with μy = 0. let r ≥ 1. show that |x + y | is more dispersed than x in that
1051,1,"['distribution', 'inequality']", Basic Properties of Conditional Expectation ,seg_77,"(hint. use fubini on the induced distribution in (r2,b2) and then apply jensen’s inequality to gx(y) = |x + y|r to the inner integral. note also exercise 8.2.3 below.)"
1052,1,"['conditional probabilities', 'probabilities', 'conditional', 'distribution']", Basic Properties of Conditional Expectation ,seg_77,"exercise 4.6 (a) let p denote the uniform (−1, 1) distribution on the borel subsets b of ω = [−1, 1]. let w (ω) ≡ |ω|,x(ω) ≡ ω2, y (ω) ≡ ω3, and z(ω) ≡ ω4. fix a ∈ b. show that versions of various conditional probabilities are given by"
1053,1,"['disjoint', 'null sets', 'cases', 'sets', 'probability measure', 'null set', 'set', 'union', 'probability', 'function']", Regular Conditional Probability ,seg_79,"for fixed a, the function p (a|d)(·) is a d-measurable function on ω that is only a.s. unique. we wish that for each fixed ω the set function p (·|d)(ω) were a probability measure. but for each disjoint sequence of ai’s there is a null set where (7.4.10)–(7.4.12) may fail, and there typically are uncountably many such sets. the union of all such null sets need not be null. in the most important special cases, though, we may assume that p (·|d)(ω) behaves as we would like, where the nonuniqueness of p (a|d)(·) also provides the key, by allowing us to make whatever negligible changes are required. (for added useful generality, we will work on a sub σ-field ã of the basic σ-field a.)"
1054,1,"['probability', 'conditional', 'conditional probability']", Regular Conditional Probability ,seg_79,"definition 5.1 we will call p (a|d)(ω) a regular conditional probability on a sub σ-field ã of the σ-field a, given the σ-field d, if"
1055,1,['function'], Regular Conditional Probability ,seg_79,"(1) for each fixed a ∈ ã, the function p (a|d)(·) of ω satisfies definition 4.2,"
1056,1,"['probability measure', 'probability']", Regular Conditional Probability ,seg_79,"(2) for each fixed ω, pω(·|d) ≡ p (·|d)(ω) is a probability measure on ã."
1057,1,"['discrete', 'conditional', 'probability', 'conditional probability']", Regular Conditional Probability ,seg_79,exercise 5.1 verify that the discrete conditional probability of exercise 7.4.1 is a regular conditional probability.
1058,1,"['probabilities', 'conditional probabilities', 'conditional expectation', 'conditional', 'expectation', 'probability', 'conditional probability']", Regular Conditional Probability ,seg_79,"when a regular conditional probability exists, conditional expectation can be computed by integrating with respect to conditional probability, and we first show this general theorem 5.1. in theorem 5.2 and beyond we shall show specifically how to construct such conditional probabilities in some of the most important examples."
1059,1,"['conditional expectation', 'conditional', 'expectation', 'probability', 'conditional probability']", Regular Conditional Probability ,seg_79,"theorem 5.1 let p (a|d)(ω) be a regular conditional probability on ã, and let y ∈ l1(ω, ã, p ). then a version of the conditional expectation of y given d is formed by setting"
1060,1,"['functions', 'conditional expectation', 'conditional', 'expectation', 'function']", Regular Conditional Probability ,seg_79,"with the various steps true by definition. thus (3) is trivial for simple functions y . if y ≥ 0 and yn are simple functions for which yn ↗ y , then for any version of the conditional expectation function e(y |d)(·) we have"
1061,0,[], Regular Conditional Probability ,seg_79,"using the mct (ordinary, and of (17)) in the first and last steps. finally, let y = y + − y −."
1062,1,"['null sets', 'probabilities', 'conditional probabilities', 'conditional', 'sets', 'union', 'probability', 'conditional probability']", Regular Conditional Probability ,seg_79,"regular conditional probabilities need not exist; the null sets on which things fail may have a nonnull union. however, if y : (ω,a) → (r,b) is a rv, then things necessarily work out on (r,b), and this will be generalized to any “borel space.” we will now start from scratch with regular conditional probability, and will choose to regard it as a measure over the image σ-field."
1063,1,['sets'], Regular Conditional Probability ,seg_79,"exercise 5.2 (a) show that (rn,bn) is a borel space. (b) show that (r∞,b∞) is a borel space. (c) the spaces (c, c) and (d,d) to be encountered below are also borel spaces. (d) let (m,d) be a complete and separable metric space having borel sets m, and let m0 ∈ m. then (m0,m0 ∩ m) is a borel space. (this exercise is the only mathematically difficult part of the chapter that we have encountered so far.)"
1064,1,"['distribution', 'conditional distribution', 'conditional']", Regular Conditional Probability ,seg_79,"definition 5.3 (regular conditional distribution) suppose that z: (ω,a) → (m,g). let ã ≡ z−1(g), and let d be a sub σ-field of a. then pz(g|d)(ω) will be called a regular conditional distribution for z given d if"
1065,1,['function'], Regular Conditional Probability ,seg_79,"(4) the function pz(g|d)(·) is a version of p (z ∈ g|d)(·) on ω,"
1066,1,"['distribution', 'probability distribution', 'set', 'probability', 'function']", Regular Conditional Probability ,seg_79,"(5) the set function pz(·|d)(ω) is a probability distribution on (m,g);"
1067,1,"['distribution', 'probability distribution', 'probability']", Regular Conditional Probability ,seg_79,and pzω(·|d) ≡ pz(·|d)(ω) will be used to denote this probability distribution.
1068,1,"['conditional probability', 'conditional', 'distribution', 'probability distribution', 'probability', 'conditional probability distribution', 'conditional distribution']", Regular Conditional Probability ,seg_79,"theorem 5.2 (existence of a regular conditional distribution) suppose z : (ω,a) → (m,g) with (m,g) a borel space. then the existence of a regular conditional probability distribution pzω(g|d) ≡ pz(g|d)(ω) is guaranteed."
1069,1,"['set', 'null set', 'case']", Regular Conditional Probability ,seg_79,"proof. case 1: suppose first that z : (ω,a) → (r,b). let r1, r2, . . . denote the set of rational numbers. consider p (z ≤ ri|d) and note that except on a null set n , all of the following hold:"
1070,0,[], Regular Conditional Probability ,seg_79,"now define, for an arbitrary but fixed df f0,"
1071,1,['function'], Regular Conditional Probability ,seg_79,"then for every ω, the function f (·|d)(ω) is a df. also, (e) and the dct of theorem 7.4.1 show that f (z|d)(·) is a version of p (z ≤ z|d)(·)."
1072,1,['distribution'], Regular Conditional Probability ,seg_79,now extend p (z ≤ ·|d)(ω) to a distribution (labeled pz(b|d)(ω)) over all b ∈ b via the correspondence theorem. we now define
1073,1,['case'], Regular Conditional Probability ,seg_79,"1 (ai, bi], and m is closed under monotone limits. thus m = b, by the minimal monotone class result of proposition 1.1.6, completing the proof in this case."
1074,1,"['case', 'conditional', 'distribution', 'conditional distribution']", Regular Conditional Probability ,seg_79,"case 2: let y ≡ φ(z), so y is a rv. thus a regular conditional distribution py (b|d) exists by case 1. then for g ∈ g, define pz(g|d) ≡ py (φ(g)|d)."
1075,1,"['densities', 'conditional']", Regular Conditional Probability ,seg_79,example 5.1 (elementary conditional densities) suppose that
1076,1,['joint'], Regular Conditional Probability ,seg_79,"where f ≥ 0 is measurable; and then f(x, y) is called the joint density of x,y (or, the radon-nikodym derivative dp/dλ2). let b2 ≡ b × r, for all b ∈ b. we can conclude that px λ ≡ (lebesgue measure), with"
1077,1,"['marginal', 'marginal density']", Regular Conditional Probability ,seg_79,we call fx(x) the marginal density of x. we first define
1078,1,"['conditional density', 'conditional', 'distribution', 'set', 'conditional distribution']", Regular Conditional Probability ,seg_79,"call g(y|x) the conditional density of y given x = x, and this p (y ∈ a|x = x) will now be shown to be a regular conditional distribution (if modified appropriately on the set where fx(x) = 0). moreover, if e|h(y )| < ∞, then"
1079,0,[], Regular Conditional Probability ,seg_79,"thus (8) (also written as (10)) fulfills theorem 5.2, and (9) will be seen to fulfill theorem 5.3. (note that this example also holds for vectors x ∈ rm and y ∈ rn.)"
1080,0,[], Regular Conditional Probability ,seg_79,"proof. by fubini’s theorem,"
1081,0,[], Regular Conditional Probability ,seg_79,"moreover, fubini’s theorem tells us that fx(x) is b-measurable."
1082,1,['set'], Regular Conditional Probability ,seg_79,"and so g(y|x) works as a version. now, for any fixed set a ∈ b we note that"
1083,1,"['conditional probability', 'conditional', 'distribution', 'probability distribution', 'probability', 'conditional probability distribution', 'function']", Regular Conditional Probability ,seg_79,"is a measurable function on (r,b). it is clear that for each fixed x the function of (10) acts like a probability distribution. thus (10) defines completely a regular conditional probability distribution."
1084,1,"['conditional expectation', 'conditional', 'expectation']", Regular Conditional Probability ,seg_79,"theorem 5.3 (conditional expectation exists as an expectation) given a measurable mapping z : (ω,a) → (m,s), where (m,s) is a borel space, consider a transformation φ : (m,s) → (r,b) with e|φ(z)| < ∞. then a version of the conditional expectation of φ(z) given d is formed by setting"
1085,1,"['distribution', 'conditional distribution', 'conditional']", Regular Conditional Probability ,seg_79,proof. apply theorem 5.1 to the regular conditional distribution of theorem 5.2.
1086,1,"['expectation', 'conditional', 'conditional expectation']", Regular Conditional Probability ,seg_79,"theorem 5.4 (a most useful format for conditional expectation) suppose that x : (ω,a, p ) → (m1,g1) and y : (ω,a, p ) → (m2,g2) (with borel space images). then (x,y ) : (ω,a, p ) → (m1 × m2,g1 × g2). also (as above)"
1087,1,"['probability', 'conditional', 'conditional probability']", Regular Conditional Probability ,seg_79,"(12) a regular conditional probability p (a|x = x) exists,"
1088,1,['sets'], Regular Conditional Probability ,seg_79,"for sets a ∈ ã ≡ y −1(g2) ⊂ a and for x ∈ m1. let e|h(x,y )| < ∞. (a) then"
1089,1,['independent'], Regular Conditional Probability ,seg_79,"(b) if x and y are independent, then"
1090,1,"['functions', 'indicator']", Regular Conditional Probability ,seg_79,exercise 5.3 prove theorem 5.4 above. (give a separate trivial proof of (14).) hint. begin with indicator functions h = 1g11g2 .
1091,1,"['permutations', 'symmetric', 'statistics', 'order statistics', 'sufficiency', 'continuous']", Regular Conditional Probability ,seg_79,"example 5.2 (sufficiency of the order statistics) let x1, . . . , xn be iid with df f in the class fc of all continuous dfs. let t (x) ≡ (xn:1, . . . , xn:n) denote the vector of ordered values of x, and let t ≡ {x : xn:1 < · · · < xn:n}. exercise 5.5 below asks the reader to verify that pf (t (x) ∈ t ) = 1 for all f ∈ fc. let x denote those x ∈ rn having distinct coordinates. let a and b denote all borel subsets of x and t , respectively. then d ≡ t−1(b) denotes all symmetric subsets of a (in that x ∈ d ⊂ d implies π(x) ∈ d for all n! permutations π(x)"
1092,0,[], Regular Conditional Probability ,seg_79,(n)-integrable. then define
1093,1,['permutations'], Regular Conditional Probability ,seg_79,"1 (15) φ0(x) ≡ n! ∑all n! permutations φ(π(x)),"
1094,1,['function'], Regular Conditional Probability ,seg_79,which is a d-measurable function. since pf
1095,1,"['symmetric', 'set']", Regular Conditional Probability ,seg_79,"(n) is symmetric, for any symmetric set d ∈ d we have"
1096,1,['permutations'], Regular Conditional Probability ,seg_79,(n)(x) = ∫d φ(π(x)) dpf (n)(x) for all n! permutations π(·)
1097,1,[], Regular Conditional Probability ,seg_79,but this means that
1098,1,['function'], Regular Conditional Probability ,seg_79,"now, for any a ∈ a, the function 1a(·) is pf"
1099,0,[], Regular Conditional Probability ,seg_79,(n)-integrable for all f ∈ fc. (thus conclusion
1100,1,"['statistic', 'sufficient statistic', 'distributions']", Regular Conditional Probability ,seg_79,"note that the right-hand side of (15) does not depend on the particular f ∈ fc, and so t is said to be a sufficient statistic for the family of distributions fc. (note discussion 4.3 once again.)"
1101,1,"['sample', 'permutations', 'symmetry', 'set', 'probability']", Regular Conditional Probability ,seg_79,"example 5.3 (ranks) consider the ranks rn = (rn1, . . . , rnn)′ and the antiranks dn = (dn1, . . . , dnn)′ in a sample from some f ∈ fc (see the previous example, and (6.5.22)). let ∏n denote the set of all n! permutations of (1, . . . , n). now, rn takes on values in ∏n when f ∈ fc (since ties occur with probability 0). by symmetry, for every f ∈ fc we have"
1102,0,[], Regular Conditional Probability ,seg_79,"note that x is equivalent to (t,rn), in the sense that each determines the other. note also that"
1103,1,['independent'], Regular Conditional Probability ,seg_79,"(20) t and rn are independent rvs (for each fixed f ∈ fc),"
1104,1,"['independent', 'statistics', 'order statistics', 'statistic', 'sufficient statistic']", Regular Conditional Probability ,seg_79,"since p (rn = r) does not depend on the particular f ∈ fc. since the ranks are independent of the sufficient statistic, they are called ancillary rvs. (note that dn is also equally likely distributed over ∏n, and that it is distributed independent of the order statistics t .)"
1105,1,"['observations', 'probability', 'continuous']", Regular Conditional Probability ,seg_79,exercise 5.4 suppose that the n observations are sampled from a continuous distribution f . verify that with probability one all observations are distinct. (hint. use corollary 2 to theorem 5.1.3.)
1106,1,['bernoulli'], Regular Conditional Probability ,seg_79,"exercise 5.5 suppose x1, . . . , xn are iid bernoulli (p) rvs, for some p ∈ (0, 1). let"
1107,1,"['probability distributions', 'successes', 'family of probability distributions', 'probability', 'distributions']", Regular Conditional Probability ,seg_79,"1 xk denote the total number of successes. show that this rv t is sufficient for this family of probability distributions (that is, t is “sufficient for p”)."
1108,1,"['distribution', 'joint', 'independent']", Regular Conditional Probability ,seg_79,"exercise 5.6 let ξ1 and ξ2 be independent uniform(0, 1) rvs. let θ ≡ 2πξ1 and y ≡ − log ξ2. let r ≡ (2y )1/2. now let z1 ≡ r cos θ and z2 ≡ r sinθ. determine the joint distribution of (y,θ) and of (z1, z2)."
1109,1,"['law of large numbers', 'convergence']", Introduction,seg_83,"this is one of the classically important chapters of this text. the first three sections of it are devoted to developing the specific tools we will need. in the second section we also present khinchin’s weak law of large numbers (wlln), which can be viewed as anticipating both of the classical laws of large numbers (llns). both the classical weak law of large numbers (feller’s wlln) and classical strong law of large numbers (kolmogorov’s slln) are presented in section 8.4, where appropriate negligibility of the summands is also emphasized. this section is the main focus of the chapter. some applications of these llns are given in the following section 8.5 then we branch out. the law of the iterated logarithm (lil), the strong markov property, and convergence of infinite series are treated in sections 8.6 – 8.8. the choice was made to be rather specific in section 8.4, with easy generalizations in section 8.8. the usual choice is to begin more generally, and then specialize. martingales (mgs) are introduced briefly in section 8.9, both for limited use in chapter 12 and so that the inequalities in the following section 8.10 can be presented in appropriate generality."
1110,0,[], BorelCantelli and Kronecker lemmas,seg_85,"the first three sections will develop the required tools, while applications will begin with the llns (the first of which appears in section 8.2). we use the notation"
1111,1,"['independent', 'events', 'independent events', 'random', 'convergence']", BorelCantelli and Kronecker lemmas,seg_85,this concept is important in dealing with convergence of various random elements. the following lemmas exhibit a nice dichotomy relative to sequences of independent events.
1112,1,['events'], BorelCantelli and Kronecker lemmas,seg_85,"lemma 1.1 (borel–cantelli lemma) for any events an,"
1113,1,"['independent', 'events', 'independent events']", BorelCantelli and Kronecker lemmas,seg_85,"lemma 1.2 (second borel–cantelli lemma) for a sequence of independent events a1, a2, . . ., we have the converse that"
1114,1,"['independent', 'events', 'independent events']", BorelCantelli and Kronecker lemmas,seg_85,"thus independent events a1, a2, . . . have p (an i.o.) equal to 0 or 1 according as ∑∞"
1115,1,['independence'], BorelCantelli and Kronecker lemmas,seg_85,n =n[1 − p (am)] by independence
1116,1,['tail'], BorelCantelli and Kronecker lemmas,seg_85,remark 1.1 (kolmogorov’s 0-1 law) in theorem 7.2.1 we considered the tail σ-field
1117,1,['independent'], BorelCantelli and Kronecker lemmas,seg_85,"∞ =1 f(xn, xn+1, . . .) of an arbitrary sequence of independent rvs x1,x2, . . .. we learned that p (d) = 0 or 1 for all d ∈ t . (here, let xn ≡ 1an and obtain the characterization via the finiteness of ∑∞"
1118,1,"['tail', 'event']", BorelCantelli and Kronecker lemmas,seg_85,1 p (an) at the end of lemma 1.2. the tail event in question is [xn = 1 i.o.].)
1119,0,[], BorelCantelli and Kronecker lemmas,seg_85,lemmas about real numbers
1120,1,"['convergence', 'inequality']", BorelCantelli and Kronecker lemmas,seg_85,an important bridge going from the convergence of series to the convergence of averages is provided by kronecker’s lemma. (an alternative bridge is provided by the monotone inequality (8.10.1) (note also inequality 8.4.10).)
1121,0,[], BorelCantelli and Kronecker lemmas,seg_85,kxj with s0 ≡ 0 and b0 ≡ 0. summing by parts gives
1122,0,['n'], BorelCantelli and Kronecker lemmas,seg_85,≤ 3 for n sufficiently larger than n .
1123,0,[], BorelCantelli and Kronecker lemmas,seg_85,nbkxk/bn puts large weight only on
1124,0,[], BorelCantelli and Kronecker lemmas,seg_85,the later terms.]
1125,1,['convergence'], BorelCantelli and Kronecker lemmas,seg_85,"lemma 1.4 (convergence of sums and products) suppose a ∈ [0, ∞], all constants cnk ≥ 0, and mn ≡ [max1≤k≤n cnk] → 0. then"
1126,1,['mean'], BorelCantelli and Kronecker lemmas,seg_85,"proof. we will write a = b ⊕ c to mean that |a − b| ≤ c. for mn ≤ 1/2,"
1127,0,[], BorelCantelli and Kronecker lemmas,seg_85,[this exercise will not be employed anywhere in this text.]
1128,0,[], BorelCantelli and Kronecker lemmas,seg_85,exercise 1.4 show that
1129,1,['independent'], BorelCantelli and Kronecker lemmas,seg_85,"exercise 1.5 let x1, . . . , xn be independent rvs. (i) show that"
1130,1,['moments'], Truncation WLLN and Review of Inequalities,seg_87,"truncated rvs necessarily have moments, and this makes them easier to work with. but it is crucial not to lose anything in the truncation."
1131,0,[], Truncation WLLN and Review of Inequalities,seg_87,∞ =1p (xn = yn) < ∞ are called khinchin equivalent.
1132,1,"['convergence', 'probability']", Truncation WLLN and Review of Inequalities,seg_87,"since a sequence (such as xn, sn or sn/bn) converges in probability if and only if each subsequence n′ contains a further subsequence n′′ on which the convergence is a.s., the in probability statements follow directly from the a.s. statements."
1133,0,[], Truncation WLLN and Review of Inequalities,seg_87,inequality 2.1 (sandwiching e|x|) for any rv x we have
1134,1,['function'], Truncation WLLN and Review of Inequalities,seg_87,"for the greatest integer function [·], an arbitrary rv satisfies"
1135,0,[], Truncation WLLN and Review of Inequalities,seg_87,"moreover, (a) shows that"
1136,0,[], Truncation WLLN and Review of Inequalities,seg_87,while (consult figure 2.1 again)
1137,1,['moment'], Truncation WLLN and Review of Inequalities,seg_87,figure 2.1 the moment e|x| = ∫0
1138,0,[], Truncation WLLN and Review of Inequalities,seg_87,"example 2.1 (ťruncating and w̃insorizing) let x1,x2, . . . be iid as x. let us truncate and winsorize the rv xn by defining"
1139,1,['moment'], Truncation WLLN and Review of Inequalities,seg_87,so that these x̌n’s and x̃n’s are khinchin equivalent to the xn’s if and only if the absolute moment e|x| < ∞. (do not lose sight of this during the slln.)
1140,1,['inequality'], Truncation WLLN and Review of Inequalities,seg_87,"proof. using inequality 2.1, then iid, and then the borel–cantelli lemmas, we obtain that e|x| < ∞ if and only if ∑∞"
1141,0,[], Truncation WLLN and Review of Inequalities,seg_87,this final fact (5) is a useful supplementary result. recall (6.4.11).
1142,0,[], Truncation WLLN and Review of Inequalities,seg_87,we begin with an easy result that illustrates our path rather clearly.
1143,1,['mean'], Truncation WLLN and Review of Inequalities,seg_87,"theorem 2.1 (i) (wlln; khinchin) let x1, . . . , xn be iid with mean μ. then"
1144,1,['independent'], Truncation WLLN and Review of Inequalities,seg_87,"note that xp (|x| > x) → 0 as x → ∞ can hold even if e(x) fails to exist.) (ii) (more general wlln) suppose that xn1, . . . , xnn are independent. then"
1145,1,['condition'], Truncation WLLN and Review of Inequalities,seg_87,this condition holds if the rvs {xnk : n ≥ 1 and 1 ≤ k ≤ n} are uniformly integrable.
1146,1,[], Truncation WLLN and Review of Inequalities,seg_87,"proof. (ii) truncate these rvs via ynk ≡ xnk × 1[−n≤xnk≤n], and define the means μnk ≡ e(ynk). then define μ̄n ≡ ∑1"
1147,1,"['average', 'condition']", Truncation WLLN and Review of Inequalities,seg_87,"so, ȳn−μ̄n →p when the average in (b) converges to 0 (which is slightly weaker than requiring the condition in (9)); that is, (a) holds. then x̄n − μ̄n →p 0 since"
1148,1,"['mean', 'condition', 'case']", Truncation WLLN and Review of Inequalities,seg_87,"finally, e(x̄n) − μ̄n → 0 is exactly the conclusion in (g); so x̄n − e(x̄n) →p 0. (i) in this iid case with finite mean, the condition (9) is satisfied and e(x̄n) = μ. in fact, x̄n →l1μ; use vitali, since x̄n →p μ and the x̄n’s are uniformly integrable by exercise 8.4.16 below. the rest of (i) is justified in theorem 8.4.1 below."
1149,1,"['mean', 'case']", Truncation WLLN and Review of Inequalities,seg_87,remark 2.1 there are two natural ways to proceed to improve khinchin’s wlln in the iid case. one way is to obtain the conclusion x̄n →a.s. μ; and this is done in kolmogorov’s slln (theorem 8.4.2 below). another way is to continue to relax the assumption of a finite mean and center differently; and this is done in feller’s wlln (theorem 8.4.1 below). (other possibilities and other approaches will be outlined in the exercises of section 8.4.)
1150,1,['inequality'], Truncation WLLN and Review of Inequalities,seg_87,"in section 8.3 we will develop a number of inequalities (so called “maximal inequalities”) to help us to the stated goal. (at the end of this section the reader could go directly to section 8.4, and then go back to section 8.3 for the inequalities as they are needed.) (also, at the end of section 8.3 we improve on the technique used in the proof of the khintchine wlln to obtain the truncation inequality to be used in the wlln in section 8.4 below.)"
1151,0,[], Truncation WLLN and Review of Inequalities,seg_87,review of general inequalities from measure theory
1152,1,"['probability theory', 'probability', 'inequality']", Truncation WLLN and Review of Inequalities,seg_87,"having completed the transition from measure theory to probability theory, we take this opportunity to restate without comment a few of the most important inequalities presented earlier. (see the proof of theorem 2.1 for the khinchin inequality below.)"
1153,1,['probability'], Truncation WLLN and Review of Inequalities,seg_87,"inequality 2.2 (review) let x and y be rvs on a probability space (ω, a, p ). then:"
1154,1,"['independence', 'dispersion']", Truncation WLLN and Review of Inequalities,seg_87,"(14) dispersion: e|x|r ≤ e|x + y |r if independence, μy = 0, and r ≥ 1."
1155,1,['interval'], Truncation WLLN and Review of Inequalities,seg_87,jensen: g(ex) ≤ e g(x) if g is convex on an interval i ⊂ r (15) having p (x ∈ i) = 1 and a finite ex ∈ io.
1156,1,['independent'], Truncation WLLN and Review of Inequalities,seg_87,(18) for independent xnk with μnk ≡ e(xnk1|xnk|≤n]).
1157,1,"['limit', 'probability']", Truncation WLLN and Review of Inequalities,seg_87,"definition 2.2 (“big ohp,” and “little ohp,” =a, ∼, and “ at most” ⊕) (a) we say that zn is bounded in probability [and write zn = op(1) ] if for all > 0 there exists a constant m for which p (|zn| ≥ m ) < for all n ≥ (some n ). for a sequence an, we write zn = op(an) if zn/an = op(1); and we say that zn is of order an, in probability. (b) if zn →p 0, we write zn = op(1). we write zn = op(an) if zn/an →p 0. (c) this notation (without subscript p) was also used for sequences of real numbers zn and an. for example, zn = o(an) if zn/an → 0. (note that o(an) = op(an).) (d) write un =a vn if un − vn →p 0; and call un and vn asymptotically equal. (this is effectively a passage to the limit that still allows n to appear on the right-side.) (e) we write an ∼ bn if an/bn → 1. (f) we write a = b⊕c if |a−b| ≤ c. (this can be used in the same fashion as op(·), but it allows one to keep track of an absolute bound on the difference. especially, it allows inequalities to be strung together more effectively.)"
1158,1,['independent'], Truncation WLLN and Review of Inequalities,seg_87,"exercise 2.3 let x and y be independent rvs, and let r > 0. then"
1159,1,"['condition', 'inequality']", Truncation WLLN and Review of Inequalities,seg_87,"that is, (x +y ) ∈ lr if and only if both x ∈ lr and y ∈ lr, for any r > 0. hint. condition on y = y. or, note the symmetrization inequality 8.3.2 below."
1160,1,"['random variables', 'independent', 'variables', 'symmetric', 'statistics', 'independent random variables', 'moments', 'probability', 'random', 'inequality']", Maximal Inequalities and Symmetrizationo,seg_89,"sums of independent random variables play an important role in probability and statistics. our goal initially in this section is to develop probability bounds for the maximum of the first n partial sums. such inequalities are called maximal inequalities. the most famous of these is kolmogorov’s inequality. for symmetric rvs, lévy’s inequality is an extremely clean and powerful version of such a maximal inequality; it does not require the underlying rvs to have any moments. neither does the ottavani–skorokhod inequality, which is true for arbitrary rvs, though it is not nearly as clean. (recall (2.3.7) which shows that sn →a.s. (some rv s) if and only if p (maxn≤m≤n |sm − sn| ≥ ) ≤ for all n ≥ n ≥ (some n ).)"
1161,1,['independent'], Maximal Inequalities and Symmetrizationo,seg_89,"inequality 3.1 (kolmogorov) let x1,x2, . . . be independent, with xk =∼ (0, σk2). let sk ≡ x1 + · · · + xk. then"
1162,1,['inequality'], Maximal Inequalities and Symmetrizationo,seg_89,[this contains chebyshev’s inequality that p (|sn| ≥ λ) ≤ var[sn]/λ2 for all λ > 0.]
1163,1,['independence'], Maximal Inequalities and Symmetrizationo,seg_89,1 [∫ 0 dp + e(sn − sk)e(2sk1ak) + ∫ak sk2dp ] by independence
1164,1,['symmetric'], Maximal Inequalities and Symmetrizationo,seg_89,definition 3.1 (symmetric rvs) a rv x is called symmetric if x ∼ −x. note that this
1165,1,['independent'], Maximal Inequalities and Symmetrizationo,seg_89,= is equivalent to its df satisfying f (−x) = 1 − f−(x) for all x ≥ 0. suppose x =∼ x ′ are independent rvs; then xs ≡ x − x ′ is called the symmetrization of the initial rv x.
1166,1,['medians'], Maximal Inequalities and Symmetrizationo,seg_89,definition 3.2 (medians) let x be an arbitrary rv. then m ≡ median(x) is any number for which p (x ≥ m) ≥ 2
1167,1,"['tails', 'median']", Maximal Inequalities and Symmetrizationo,seg_89,1 and p (x ≤ m) ≥ 2 1 . [one median of the symmetrization xs of any rv x is always 0. and (2) below shows that the tails of xs behave roughly the same as do those of x.]
1168,1,"['independent', 'inequality']", Maximal Inequalities and Symmetrizationo,seg_89,inequality 3.2 (symmetrization inequality) let xs ≡ x − x ′ where x =∼ x ′ with x and x ′ independent. let r > 0 and let a be any real number. then both
1169,1,[], Maximal Inequalities and Symmetrizationo,seg_89,2−1p (|x − median(x)| ≥ λ) ≤ p (|xs| ≥ λ) ≤ 2p (|x − a| ≥ λ/2) and
1170,1,[], Maximal Inequalities and Symmetrizationo,seg_89,(2) 2−1e|x − median(x)|r ≤ e|xs|r ≤ 21+re|x − a|r.
1171,1,['events'], Maximal Inequalities and Symmetrizationo,seg_89,we may replace ≥ by > in the three events in the upper half of (2).
1172,1,['inequality'], Maximal Inequalities and Symmetrizationo,seg_89,"proof. let m ≡ median(x). now, the first inequality comes from"
1173,1,['inequality'], Maximal Inequalities and Symmetrizationo,seg_89,"the second inequality holds, since for any real a,"
1174,1,['moment'], Maximal Inequalities and Symmetrizationo,seg_89,plug (2) into (6.4.13) for the moment inequalities.
1175,1,"['symmetric', 'independent']", Maximal Inequalities and Symmetrizationo,seg_89,"inequality 3.3 (lévy) let x1, . . . , xn be independent and symmetric rvs. now define, sn ≡ x1 + · · · + xn. then both"
1176,1,['independence'], Maximal Inequalities and Symmetrizationo,seg_89,"by independence of x1, . . . , xk from xk+1, . . . , xn"
1177,1,['symmetry'], Maximal Inequalities and Symmetrizationo,seg_89,1p (ak)/2 by symmetry
1178,1,['symmetric'], Maximal Inequalities and Symmetrizationo,seg_89,"combine this with the symmetric result, and achieve the first claim."
1179,1,['distribution'], Maximal Inequalities and Symmetrizationo,seg_89,"see feller(1966) proof: let m ≡ xk , where k ≡ min{k : |xk| = max1≤j≤n |xj |}. let t ≡ sn − xk . then, for all four choices of + or − signs, the rvs (±m, ±t ) have the same distribution. then we require both"
1180,1,['symmetric'], Maximal Inequalities and Symmetrizationo,seg_89,and the symmetric result. [see exercise 3.2 below for more on (4).]
1181,1,"['inequality', 'normal', 'moment']", Maximal Inequalities and Symmetrizationo,seg_89,"∼ remark 3.1 kolmogorov’s inequality is a moment inequality. since the rv sn/stdev[sn] = (0, 1) is often approximately normal (2π)−1/2 exp(−x2/2) on the line, and since"
1182,1,['inequality'], Maximal Inequalities and Symmetrizationo,seg_89,both lévy’s inequality and the ottaviani–skorokhod inequality to follow offer the hope of a much better bound.
1183,1,['independent'], Maximal Inequalities and Symmetrizationo,seg_89,inequality 3.4 let sk ≡ x1 + · · · + xk for independent rvs xk. (ottaviani–skorokhod) for all 0 < c < 1 we have
1184,0,[], Maximal Inequalities and Symmetrizationo,seg_89,"(etemadi) alternatively,"
1185,0,['n'], Maximal Inequalities and Symmetrizationo,seg_89,n =1ak = [max1≤k≤n sk ≥ λ]. thus k is the smallest index for which sk exceeds λ. (this is now the third time we have used this same trick.) note that
1186,1,['inequality'], Maximal Inequalities and Symmetrizationo,seg_89,≥ 1 − max1≤k≤n var[sn − sk]/[(1 − c)λ]2 by chebyshev’s inequality
1187,1,[], Maximal Inequalities and Symmetrizationo,seg_89,"allows us to “improve” (7) to (8). meanwhile, (7) comes from"
1188,1,['independence'], Maximal Inequalities and Symmetrizationo,seg_89,n =1p (ak ∩ [|sn − sk| ≤ (1 − c)λ]) by independence
1189,0,[], Maximal Inequalities and Symmetrizationo,seg_89,combining (f) and (b) with the analogous result for −sn completes the proof.
1190,1,['inequality'], Maximal Inequalities and Symmetrizationo,seg_89,exercise 3.1 prove etemadi’s inequality.
1191,1,['independent'], Maximal Inequalities and Symmetrizationo,seg_89,exercise 3.2 consider independent rvs xks ≡ xk − xk
1192,1,['independent'], Maximal Inequalities and Symmetrizationo,seg_89,"′ independent, and with each xk"
1193,1,['median'], Maximal Inequalities and Symmetrizationo,seg_89,"′ ∼ x . let m denote a median of x , and let a denote = k k k any real number. let λ > 0 and r > 0. show that both:"
1194,1,['inequality'], Maximal Inequalities and Symmetrizationo,seg_89,"s | ≥ λ) ≤ 2p (|sn−a| ≥ λ/2) (for any real a), by inequality 3.3 and inequality 3.2.]"
1195,0,[], Maximal Inequalities and Symmetrizationo,seg_89,inequalities for rademacher rvs∗
1196,1,"['sample', 'independent']", Maximal Inequalities and Symmetrizationo,seg_89,"inequality 3.5 (symmetrization; giné–zinn) let x1, . . . , xn be iid rvs, and let 1, . . . , n denote an independent sample of iid radamacher rvs (that satisfy p ( k = ±1) = 1"
1197,1,[], Maximal Inequalities and Symmetrizationo,seg_89,proof. by conditioning on the rademacher rvs we obtain
1198,1,['inequality'], Maximal Inequalities and Symmetrizationo,seg_89,"exercise 3.3 (a) (khinchin inequality) suppose 1, . . . , n are iid rademacher rvs. let a1, . . . , an be real constants. then"
1199,1,"['independent', 'mean', 'inequality']", Maximal Inequalities and Symmetrizationo,seg_89,"for some constants ar and br. establish this for r = 1, with a1 = 1/√3 and b1 = 1. [hint. use littlewood’s inequality with r, s, t equal to 1, 2, 4.] (b) (marcinkiewicz–zygmund inequality) for x1, . . . , xn independent 0 mean rvs,"
1200,1,['independent'], Maximal Inequalities and Symmetrizationo,seg_89,"exercise 3.4 let x1, . . . , xn be independent with 0 means, and independent of the iid rademacher rvs 1, . . . , n. let φ be ↗ and convex on r. then"
1201,1,['average'], Maximal Inequalities and Symmetrizationo,seg_89,[hint. the left side is an average of terms like eφ(|∑1
1202,1,['independent'], Maximal Inequalities and Symmetrizationo,seg_89,"nek(xk − exk ′ )|/2), for independent"
1203,0,[], Maximal Inequalities and Symmetrizationo,seg_89,"weak negligibility, or maximal inequalities of another ilk∗"
1204,1,['independent'], Maximal Inequalities and Symmetrizationo,seg_89,"discussion 3.1 (weak negligibility) let yn1, . . . , ynn be independent rvs having dfs fn1, . . . , fnn. let θ > 0 be given. for any > 0, let pnk ≡ p (|ynk| > ). now, the maximum maxn ≡ [max1≤k≤n |ynk|] satisfies"
1205,0,[], Maximal Inequalities and Symmetrizationo,seg_89,[the equality uses ⋂n
1206,1,"['inequality', 'standard']", Maximal Inequalities and Symmetrizationo,seg_89,"1 ak]c, and the first bound follows from the inequality 1 − x ≤ exp(−x).] this gives (so does exercise 8.1.5) the standard result that"
1207,1,"['symmetric', 'interval']", Maximal Inequalities and Symmetrizationo,seg_89,"define xθn by requiring [−xθn, xθn] to be the smallest interval that is both closed and symmetric to which f̄n ≡ ∑n"
1208,1,['probability'], Maximal Inequalities and Symmetrizationo,seg_89,1fnk/n assigns probability at least 1 − θ/n. let p̄n(x) ≡
1209,1,"['tail', 'average', 'tail probability', 'probability']", Maximal Inequalities and Symmetrizationo,seg_89,"1p (|ynk| > x) denote the average tail probability, and then let kn denote the qf of the"
1210,1,['quantile'], Maximal Inequalities and Symmetrizationo,seg_89,df 1 − p̄n(·). note the quantile relationship xθn = kn(1 − θ/n). note that kn(1 − θ/n) = inf{x : 1 − p̄n(x) ≥ 1 − θ/n} = inf{x : p̄n(x) ≤ θ/n}. thus
1211,0,['n'], Maximal Inequalities and Symmetrizationo,seg_89,"fix 0 < ≤ 1 and 0 < θ ≤ 1, and suppose that we are considering all n exceeding some n ,θ. conclusions (14) and (15) give (the seemingly new emphasis)"
1212,1,"['quantile', 'average']", Maximal Inequalities and Symmetrizationo,seg_89,discussion 3.2 (weak negligibility in the lln context) let νn > 0 be constants. applying the previous paragraph to the rvs |ynk|/nνn (whose average df has the (1 − θ/n)th quantile xθn/nνn) gives the equivalencies
1213,1,['moment'], Maximal Inequalities and Symmetrizationo,seg_89,the truncated absolute moment u1n ≡ ∫[|y|≤x1n] |y|df̄n(y) as well as the winsorized absolute
1214,1,"['quantile', 'independent']", Maximal Inequalities and Symmetrizationo,seg_89,"moment ũ1n ≡ u1n + x1np̄n(x1n) are among the potential choices for νn that could lead to x̄n/νn →p 1 for independent arrays xn1, . . . , xnn on [0, ∞). (here x1n means the quantile xθn with θ = 1.)"
1215,1,"['linear', 'probability']", Maximal Inequalities and Symmetrizationo,seg_89,"inequality 3.6 (daniels’ equality) with high probability there is an upper linear bound on the uniform empirical df gn. that is, for each 0 < λ < 1,"
1216,1,"['statistics', 'order statistics', 'joint']", Maximal Inequalities and Symmetrizationo,seg_89,"proof. (robbins) the vector of uniform(0, 1) order statistics (ξn:1, . . . ξn:n) has joint density n! on its domain 0 < t1 < · · · < tn < 1. thus"
1217,1,"['linear', 'probability', 'inequality']", Maximal Inequalities and Symmetrizationo,seg_89,"inequality 3.7 (chang’s inequality) with high probability there is a lower linear bound on the uniform empirical df gn. that is, for all λ ≥ 1, we have"
1218,1,"['symmetry', 'inequality']", Maximal Inequalities and Symmetrizationo,seg_89,(this provides a nice symmetry with the previous inequality; see chapter 12.)
1219,1,['independent'], Maximal Inequalities and Symmetrizationo,seg_89,"let xn1, . . . , xnn be independent fn1, . . . , fnn, with p̄n(x) ≡ n"
1220,1,"['quantile', 'average']", Maximal Inequalities and Symmetrizationo,seg_89,"1∑n 1p (|xnk| > x). note that 1 − p̄n(·) is the average df of the rvs |xn1|, . . . , |xnn|, and recall from above (15) that xθn denotes the 1 − θ/n quantile of this df (for any 0 < θ ≤ 1). let τ̄n(x) ≡ xp̄n(x). let"
1221,1,"['independent', 'inequality']", Maximal Inequalities and Symmetrizationo,seg_89,"inequality 3.8 (truncation inequality i) for row independent xn1, . . . , xnn,"
1222,1,['set'], Maximal Inequalities and Symmetrizationo,seg_89,"to obtain (23), set cn = xθn and νn = uθn in (b) (which uses e(ȳn) = μθn), and then observe that ∫[|y|≤xθn] y2 df̄n(y) ≤ xθnuθn."
1223,1,"['independent', 'inequality']", Maximal Inequalities and Symmetrizationo,seg_89,"inequality 3.9 (truncation inequality ii) for row independent xn1, . . . , xnn, define ynk ≡ xnk × 1[|xnk|≤n], and then let μ̌n ≡ e(ȳn). then (for any 0 < ≤ 1)"
1224,0,['n'], Maximal Inequalities and Symmetrizationo,seg_89,proof. from line (25) in the previous proof (with cn = n and νn = 1) one gets
1225,0,[], Maximal Inequalities and Symmetrizationo,seg_89,exercise 3.5 write out the (nearly identical) details for the proof of (24).
1226,1,['inequality'], Maximal Inequalities and Symmetrizationo,seg_89,exercise 3.6 show that x̄n − μ̄n →p 0 in the context of the previous inequality provided sup{τ̄n(x) : 2√n ≤ x ≤ n} → 0.
1227,1,"['sample', 'law of large numbers', 'hypotheses', 'sample average', 'average']", The Classical Laws of Large Numbers LLNs,seg_91,"it is now time to present versions of the laws of large numbers under minimal hypotheses. the weak law of large numbers (wlln) will establish →p, while the strong law of large numbers (slln) will establish →a.e. of a sample average x̄n."
1228,1,[], The Classical Laws of Large Numbers LLNs,seg_91,"mn ≡ median(x̄n). if e|x| < ∞, then (1) holds with μn ≡ μ ≡ ex."
1229,1,"['symmetric', 'condition']", The Classical Laws of Large Numbers LLNs,seg_91,"conditions (4) and (8) show the sense in which these llns are tied to the size of the maximal summand. this is an important theme, do not lose sight of it. we now give a symmetric version of condition (4). (see also exercises 4.18–4.21.)"
1230,1,['independent'], The Classical Laws of Large Numbers LLNs,seg_91,"theorem 4.3 (the maximal summand) let xn1, . . . , xnn, n ≥ 1, be iid row independent rvs with df f . we then let xn"
1231,0,[], The Classical Laws of Large Numbers LLNs,seg_91,sk ≡ xnk − xn ′ k denote their symmetrized versions. fix r > 0. [most important is r = 1.] then
1232,1,['inequality'], The Classical Laws of Large Numbers LLNs,seg_91,"suppose e|x| < ∞. using inequality 8.2.1 in the second step and iid in the third, we obtain"
1233,1,['function'], The Classical Laws of Large Numbers LLNs,seg_91,with dominating function given by |x|). it thus suffices to show that ∑n
1234,1,['inequality'], The Classical Laws of Large Numbers LLNs,seg_91,kolmogorov’s inequality yields (e) via
1235,0,[], The Classical Laws of Large Numbers LLNs,seg_91,"now, this last is seen to be true via the following kolmogorov argument that"
1236,1,['inequality'], The Classical Laws of Large Numbers LLNs,seg_91,suppose e|x| = ∞. then the “sandwich” inequality 8.2.1 gives
1237,0,[], The Classical Laws of Large Numbers LLNs,seg_91,so that applying the second borel–cantelli lemma to (l) gives
1238,1,[], The Classical Laws of Large Numbers LLNs,seg_91,"thus (5)–(7) hold. [apply vitali with exercise 4.16 below for x̄n →l1 μ in (7). if x̄n → l1 (some rv w ), then x̄n →p w—and the averages x̄n are u.i. by vitali. thus ex is finite.]"
1239,0,[], The Classical Laws of Large Numbers LLNs,seg_91,"we merely repeat this last statement, writing"
1240,1,['inequality'], The Classical Laws of Large Numbers LLNs,seg_91,"thus inequality 8.2.1 (by applying iid, and then the second borel–cantelli) gives"
1241,0,['n'], The Classical Laws of Large Numbers LLNs,seg_91,thus for all n exceeding some even larger nω′ we have
1242,0,[], The Classical Laws of Large Numbers LLNs,seg_91,where we will have to increase the specification on nω′ for (t). thus mn →a.s. 0.
1243,1,['independent'], The Classical Laws of Large Numbers LLNs,seg_91,"remark 4.1 suppose x1, . . . , xn are independent, with xi =∼ (0, σi2). then"
1244,1,['inequality'], The Classical Laws of Large Numbers LLNs,seg_91,"chebyshev’s nequality and kolmogorov’s inequality give, respectively,"
1245,1,"['variance', 'inequality']", The Classical Laws of Large Numbers LLNs,seg_91,"for x1,x2, . . . iid (μ, σ2), the inequality (13)(a) gives x̄n →p μ, by chebyshev’s inequality. but the wlln conclusion x̄n →p μ should not require the variance σ2 to be finite, as this cheap proof based on (13)(a) requires. indeed, khintchine’s wlln of theorem 8.2.1 didn’t. exercise 4.8 below outlines one very cheap proof of the slln using “only” the borel–cantelli lemma, and exercise 4.9 outlines a slightly improved version that also uses kolmogorov’s inequality. kolmogorov’s proof of the full slln made the key step of incorporating truncation. exercise 4.10 describes an elementary way to avoid use of kronecker’s lemma."
1246,1,['inequality'], The Classical Laws of Large Numbers LLNs,seg_91,"by the truncation inequality (8.3.26). (note (6.4.18).) note that τ(x) ≤ x, and choose m > 0 so large that τ(x) < 3/4 for x > m . apply this to (14) for"
1247,1,['inequality'], The Classical Laws of Large Numbers LLNs,seg_91,"thus x̄n − μn →p 0. we also have x̄n − median(x̄n) →p 0, since the symmetrization inequality 8.3.2 gives"
1248,1,[], The Classical Laws of Large Numbers LLNs,seg_91,(15) p (|x̄n − median(x̄n)| ≥ ) ≤ 4p (|x̄n − μn| ≥ /2) → 0.
1249,1,['case'], The Classical Laws of Large Numbers LLNs,seg_91,"[the acceptability of the third exhibited choice for the centering constant is left to exercise 4.1 below.] in any case, we have shown that (2) implies (1)."
1250,1,['inequality'], The Classical Laws of Large Numbers LLNs,seg_91,"we still neeed to show that (1) implies (2), but we’ll wait a paragraph for this. consider next theorem 4.3. we will only provide a proof with r = 1 (we may just replace |x| by |y | ≡ |x|r, after raising |x| to the power r). now, τx(x) → 0 implies τs(x) → 0 by the right-hand side of inequality 8.3.2 with a = 0, while the left-hand side then gives τx−med(x) → 0. the equivalence of (4) and (2) then gives max |xnk − med|/n →p 0, which trivially gives max |xnk|/n →p 0, which gives τx(x) → 0 by the equivalence of (4) and (2). this completes theorem 4.3."
1251,1,['independent'], The Classical Laws of Large Numbers LLNs,seg_91,′ k =∼ xnk and with xnk’s and xn ′ ks independent. thus sn
1252,1,['inequality'], The Classical Laws of Large Numbers LLNs,seg_91,"sk|/n →p 0 by the (8.3.4) lévy inequality. thus τs(x) → 0 by theorem 4.3, and hence τ(x) → 0 by theorem 4.3."
1253,0,[], The Classical Laws of Large Numbers LLNs,seg_91,in the wlln as a centering constant in (1). the most natural choice is
1254,0,[], The Classical Laws of Large Numbers LLNs,seg_91,"we have just seen that good inequalities lead to good theorems! in sections 8.9 and 12.11 we will add to our growing collection of good inequalities. some will be used in this text, and some will not. but the author thinks it important to illustrate these possibilities."
1255,0,[], The Classical Laws of Large Numbers LLNs,seg_91,for every sequence of constants cn. (note exercise 4.23 below.)
1256,0,[], The Classical Laws of Large Numbers LLNs,seg_91,"exercise 4.4 (marcinkiewicz–zygmund) let x1,x2, . . . be iid. let 0 < r < 2. establish the equivalence"
1257,0,[], The Classical Laws of Large Numbers LLNs,seg_91,exercise 4.6 clarify the overlap between (17) and (18).
1258,1,"['sample size', 'sample', 'random', 'random sample']", The Classical Laws of Large Numbers LLNs,seg_91,"exercise 4.7 (random sample size) (a) let x1,x2, . . . be iid with τ(x) → 0 as x → ∞. let nn ≥ 0 be any integer-valued rv satisfying nn/n →p c ∈ (0,∞). then"
1259,1,['independent'], The Classical Laws of Large Numbers LLNs,seg_91,"exercise 4.8 (a weak slln) (a) for xn1, . . . , xnn independent (or, uncorrelated) with xnk =∼ (0, σn"
1260,1,"['independence', 'inequality']", The Classical Laws of Large Numbers LLNs,seg_91,"2 |2} ≤ 4n2m , so that p (δn/n2 > i.o.) = 0. (b) use kolmogorov’s inequality to obtain e 2n ≤ 2nm , under independence."
1261,1,['independent'], The Classical Laws of Large Numbers LLNs,seg_91,"exercise 4.9 let xn1, . . . , xnn be row independent rvs (here, merely uncorrelated is much harder to consider) with means 0 and having all exn"
1262,1,['inequality'], The Classical Laws of Large Numbers LLNs,seg_91,(a) (cantelli’s inequality) verify that x̄n ≡ sn/n ≡ (xn1 + · · · + xnn)/n satisfies
1263,1,"['replacement', 'inequality']", The Classical Laws of Large Numbers LLNs,seg_91,exercise 4.10 (alternative proof of the slln) apply either the hájek–rényi inequality (inequality 8.10.3) or the monotone inequality (inequality 8.10.1) as a replacement for the use of the kronecker lemma in the slln proof.
1264,1,[], The Classical Laws of Large Numbers LLNs,seg_91,"exercise 4.13 if x1,x2, . . . are iid exponential(l), then lim xn/ log n = 1 a.s. and xn:n/ log n → 1 a.s."
1265,1,"['distribution', 'cauchy', 'symmetric', 'cauchy distribution']", The Classical Laws of Large Numbers LLNs,seg_91,"exercise 4.15 (a) does the wlln hold for the cauchy distribution? (b) does the wlln hold if p (|x| > x) = e/[2x log x] for x ≥ e,x symmetric? (c) make up one more example of each of these two types."
1266,1,"['sample', 'independence']", The Classical Laws of Large Numbers LLNs,seg_91,"exercise 4.16 (uniform integrability of sample averages) let x1,x2, . . . be iid, and let x̄n ≡ (x1 + · · · + xn)/n. then the rvs {xn : n ≥ 1} are uniformly integrable if and only if the rvs {x̄n : n ≥ 1} are uniformly integrable. (relate this to the slln result in (7).) (we only need independence for u.i. xk’s to yield u.i. x̄n’s.)"
1267,1,"['independent', 'mean']", The Classical Laws of Large Numbers LLNs,seg_91,"exercise 4.17 (a) let row independent rvs xn1, . . . , xnn be iid with the df f (·). let f have finite mean μ ≡ ex. we know mn ≡ [max1≤k≤n |xnk|/n] →p 0 by the wlln. trivially, emn ≤ e|x|. show that"
1268,1,['case'], The Classical Laws of Large Numbers LLNs,seg_91,"exercise 4.18 (negligibility for r = 1, a.s.) (i) let x,x1,x2, . . . be iid rvs. let r > 0 (with r = 1 the most important case). prove that the following are equivalent:"
1269,1,['symmetric'], The Classical Laws of Large Numbers LLNs,seg_91,"(ii) since e|x|r < ∞ if and only if the symmetrized rv x − x ′ has e|x − x ′|r < ∞ (by exercise 8.2.3), we can add three analogous equivalences for iid symmetric rvs distributed as x − x ′."
1270,1,['case'], The Classical Laws of Large Numbers LLNs,seg_91,"exercise 4.20 (negligibility for r = 2, a.s.) (i) let x,x1,x2, . . . be iid rvs (that are not identically equal to 0). let r > 0 (with r = 2 the most important case). prove that the following are equivalent (you should use the difficult (26) for (30)):"
1271,1,['condition'], The Classical Laws of Large Numbers LLNs,seg_91,"(ii) when r = 2, we may add the equivalent condition (by (6.6.6))"
1272,1,['probability'], The Classical Laws of Large Numbers LLNs,seg_91,"∼ exercise 4.21 (neglibility, in probability) let xn1, . . . , xnn be iid f , for n ≥ 1. let x = f . let r > 0. prove that the following are equivalent:"
1273,1,"['case', 'condition']", The Classical Laws of Large Numbers LLNs,seg_91,in case r > 1 (and especially for r = 2) add to this list the equivalent condition
1274,0,[], The Classical Laws of Large Numbers LLNs,seg_91,"because (34) is included in this list, the iid xnk’s may be replaced by iid symmetrized xn"
1275,0,[], The Classical Laws of Large Numbers LLNs,seg_91,the remaining problems in this subsection are mainly quite substantial. they are here for “flavor.” some also make good exercises for section 8.8. (some of the martingale inequalities found in section 8.10 should prove useful (here and below).)
1276,1,['independent'], The Classical Laws of Large Numbers LLNs,seg_91,"exercise 4.26* let x1,x2, . . . be independent with 0 means. let r ≥ 1. then"
1277,1,[], The Classical Laws of Large Numbers LLNs,seg_91,"exercise 4.28* (stone) let x,x1,x2, . . . be iid nondegenerate rvs with 0 means. let sn ≡ x1 + · · · + xn. then"
1278,0,[], The Classical Laws of Large Numbers LLNs,seg_91,generalizations of the llns
1279,1,['results'], The Classical Laws of Large Numbers LLNs,seg_91,our results allow simple generalizations of both the wlln and slln.
1280,1,['independent'], The Classical Laws of Large Numbers LLNs,seg_91,"theorem 4.4 (general wlln and slln) let x1,x2, . . . be independent. then"
1281,1,['inequality'], The Classical Laws of Large Numbers LLNs,seg_91,"proof. the first claim is immediate from chebyshev’s inequality. also, (f) in the slln proof shows that"
1282,1,"['independent', 'set']", The Classical Laws of Large Numbers LLNs,seg_91,"exercise 4.29* (more general wlln) let xn1, . . . , xnn be independent, and set sn ≡ xn1 + · · ·+xnn. truncate via ynk ≡ xnk × 1[|xnk|≤bn], for some bn > 0 having bn ↗ ∞. let μnk and σn"
1283,1,"['variance', 'mean']", The Classical Laws of Large Numbers LLNs,seg_91,2k denote the mean and variance of ynk. then
1284,0,[], The Classical Laws of Large Numbers LLNs,seg_91,if and only if we have all three of
1285,0,[], The Classical Laws of Large Numbers LLNs,seg_91,[the converse is a substantial problem.]
1286,0,[], Applications of the Laws of Large Numbers,seg_93,theorem 5.1 (glivenko-cantelli) we have
1287,1,"['function', 'random']", Applications of the Laws of Large Numbers,seg_93,[this is a uniform slln for the random function fn(·).]
1288,1,['function'], Applications of the Laws of Large Numbers,seg_93,"proof. let xk = f−1(ξ̇k) for k ≥ 1 be iid f , with the ξ̇k’s iid uniform (0, 1). let gn denote the empirical df of the first n of these ξ̇k’s, and let fn denote the empirical df of the first n of these xk’s. let i denote the identity function. then"
1289,1,['case'], Applications of the Laws of Large Numbers,seg_93,"by (6.3.4). thus by theorem 5.3.3, it will suffice to prove the result in the special case of uniform empirical df’s gn’s. (recall the remark in bold above (6.4.3) that the representation of x as f−1(ξ) allows alternative ways to approach problems. moreover, using the ξ̇k’s of (6.3.8) gives us back the original xk’s.)"
1290,1,['bernoulli'], Applications of the Laws of Large Numbers,seg_93,"now, gn(k/m) − k/m →a.s. 0 as n → ∞ for 0 < k ≤ m by the slln applied to the iid bernoulli (k/m) rv’s 1[0,k/m ](ξ̇i). we now assume that m is so large that 1/m < . then for (k − 1)/m ≤ t ≤ k/m , with 1 ≤ k ≤ m , we have both"
1291,0,[], Applications of the Laws of Large Numbers,seg_93,these combine to give
1292,1,['independent'], Applications of the Laws of Large Numbers,seg_93,"exercise 5.1 let ξn1, . . . , ξnn denote any row independent uniform (0, 1) rvs, and let all xnk = f−1(ξnk) for a fixed df f . let fn and gn denote the empirical dfs of the nth rows of these two arrays. show that (2) still holds."
1293,1,"['continuous', 'approximation']", Applications of the Laws of Large Numbers,seg_93,"example 5.1 (weierstrass approximation theorem) if f is continuous on [0, 1], then there exist polynomials bn such that ‖bn − f‖ = sup0≤t≤1 |bn(t) − f(t)| → 0 as n → ∞."
1294,1,['bernoulli'], Applications of the Laws of Large Numbers,seg_93,proof. (bernstein) define the bernoulli polynomials
1295,1,[], Applications of the Laws of Large Numbers,seg_93,"∼ (a) = ef(t/n) where t = binomial(n, t)."
1296,1,['continuous'], Applications of the Laws of Large Numbers,seg_93,"since f is continuous, f is bounded by some m , and f is uniformly continuous on [0, 1] having |f(x) − f(y)| < whenever |x − y| < δ . then"
1297,1,"['convergence', 'inequality']", Applications of the Laws of Large Numbers,seg_93,"as the choice of n does not depend on t, the convergence is uniform. note that this is just an application of a weak form of the wlln (that is, of the chebyshev inequality)."
1298,1,['normal'], Applications of the Laws of Large Numbers,seg_93,"example 5.2 (borel’s normal numbers) a number x in [0, 1] is called normal to base d if when expanded to base d, the fraction of each of the digits 0, . . . , d− 1 converges to 1/d. the number is normal if it is normal to base d for each d > 1. we are able to conclude that"
1299,1,['normal'], Applications of the Laws of Large Numbers,seg_93,"a.e. number in [0, 1] is normal with respect to lebesgue measure λ."
1300,1,['normal'], Applications of the Laws of Large Numbers,seg_93,"1 1 [comment: = 0.010101 . . . in base 2 is normal in base 2, but = 0.1000 . . . in base 3 is"
1301,1,['normal'], Applications of the Laws of Large Numbers,seg_93,"3 3 not normal in base 3.] [this was a historically important example, which spurred some of the original development.]"
1302,1,['discrete'], Applications of the Laws of Large Numbers,seg_93,"note that the βn’s are iid discrete uniform on 0, 1, . . . , d − 1. thus, letting ηnk = 1 or 0 according as βn = k or βn = k, we have"
1303,1,['normal'], Applications of the Laws of Large Numbers,seg_93,"1 0ad,k has λ(ad) = 1; that is, a.e. ω in [0, 1] is normal to base d. then trivially, a ≡ ⋂d"
1304,1,['normal'], Applications of the Laws of Large Numbers,seg_93,"∞ =1ad has λ(a) = 1. and so, a.e. ω in [0, 1] is normal."
1305,1,"['sample size', 'sample', 'random', 'mean', 'random sample']", Applications of the Laws of Large Numbers,seg_93,"example 5.3 (slln for random sample size) let nn be positive-integer valued rvs for which nn/n →a.s. c ∈ (0, ∞), and let x1,x2, . . . be iid with mean μ. (a) then"
1306,1,[], Applications of the Laws of Large Numbers,seg_93,"(b) if x1,x2, . . . are iid bernoulli(p) and nn(ω) ≡ min{k : sk(ω) = n}, then the waiting times nn satisfy nn/n →a.s. 1/p."
1307,1,[], Applications of the Laws of Large Numbers,seg_93,"completing the proof. note that we could also view nn as the sum of n iid geometric(p) rvs, and then apply the slln."
1308,1,"['sample', 'estimation', 'sample average', 'continuous', 'average', 'estimator', 'consistent estimator']", Applications of the Laws of Large Numbers,seg_93,"exercise 5.2 (monte carlo estimation) let h : [0, 1] → [0, 1] be continuous. (i) let xk ≡ 1[h(ξk)≥θk], where ξ1, ξ2, . . . θ1,θ2, . . . are iid uniform (0, 1) rvs. show that this sample average is a strongly consistent estimator of the integral; that is, show that"
1309,1,"['set', 'limit']", Law of the Iterated Logarithm,seg_95,"[that is, for a.e. ω the limit set of sn/√2n log log n is exactly [−σ, σ] ]. (c) conversely, if"
1310,0,[], Law of the Iterated Logarithm,seg_95,"[we state this for fun only, as it has seen little application.]"
1311,1,"['case', 'cases', 'normal', 'exponential', 'process']", Law of the Iterated Logarithm,seg_95,"versions of both theorems are also known for cases other than iid. the classical proof of theorem 6.1 in full generality begins with truncation, and then carefully uses exponential bounds for bounded rvs. a more modern proof relies upon skorokhod embedding of the partial sum process in brownian motion. this general proof is outlined in the straightforward exercise 12.8.2, after embedding is introduced. but the proof below for the special case of normal rvs contains several of the techniques used in the classical proof of the general case (and in other related problems). and it is also a crucial component of the general case in exercise 12.8.2."
1312,1,['exponential'], Law of the Iterated Logarithm,seg_95,proof. let > 0. we will use the exponential bound
1313,1,['inequality'], Law of the Iterated Logarithm,seg_95,"(for some λ ) [see mills’ ratio exercise 6.1 below], and the lévy maximal inequality"
1314,0,[], Law of the Iterated Logarithm,seg_95,"let nk ≡ [ak] for a > 1; a sufficiently small a will be specified below. now,"
1315,0,['e'], Law of the Iterated Logarithm,seg_95,"since (e) is true, on any subsequence nk → ∞ we can claim that"
1316,1,"['independent', 'events', 'independent events']", Law of the Iterated Logarithm,seg_95,"now, the independent events"
1317,1,[], Law of the Iterated Logarithm,seg_95,"(k) = 1/k1− = (kth term of a series with an infinite sum),"
1318,1,[], Law of the Iterated Logarithm,seg_95,so that p (bk i.o.) = 1 by the second borel–cantelli lemma. but p (ak i.o.) = 0 and p (bk i.o.) = 1 means that
1319,1,['symmetric'], Law of the Iterated Logarithm,seg_95,"moreover, on ack ∩ bk we have, using (h), (i), and the symmetric version of (f),"
1320,0,[], Law of the Iterated Logarithm,seg_95,"for the constant a specified sufficiently large. thus, even focusing only on the subsequence nk in (f) with this large a ≡ a , since > 0 was arbitrary,"
1321,0,"['n', 'e']", Law of the Iterated Logarithm,seg_95,combining (e) and (n) gives the proposition.
1322,0,[], Law of the Iterated Logarithm,seg_95,which can be rewritten as
1323,1,"['standard normal', 'normal', 'standardized', 'standard']", Law of the Iterated Logarithm,seg_95,"where φ and φ denote the standard normal n(0, 1) density and df, respectively. show (5) follows from this. this is the end of this exercise. (∗) for a standardized rv zn, one might then hope that as λn → ∞"
1324,1,"['cases', 'normal', 'exponential']", Law of the Iterated Logarithm,seg_95,as was applied in (5). [this clean exponential bound for normal rvs was the key to the simple lil proof in proposition 6.1. the classic hartman–wintner proof uses truncation to achieve a reasonable facsimile of this in other cases.] (∗) (ito–mckean) it is even true that for all λ > 0 there are the tighter bounds
1325,1,['events'], Law of the Iterated Logarithm,seg_95,"exercise 6.3 let arbitrary events an and bn satisfy p (an i.o.) = 0 and p (bn i.o.) = 1. show that p (acn ∩ bn i.o.) = 1 (as in (1) above). summary suppose x,x1,x2, . . . are iid (μ, 1). then:"
1326,0,[], Law of the Iterated Logarithm,seg_95,suppose we go all the way to √n in the denominator. then the classical clt gives
1327,0,[], Law of the Iterated Logarithm,seg_95,even though we have divergence to ±∞ for a.e. ω (by the lil).
1328,1,"['convergence', 'mean', 'case']", Law of the Iterated Logarithm,seg_95,"exercise 6.4 (rth mean convergence theorem) let x,x1,x2, . . . be iid, and consider the partial sums sn ≡ x1 + · · · + xn. let 0 < r < 2 (and suppose ex = 0 in case 1 ≤ r < 2). the following are equivalent: (a) e|x|r < ∞. (b) sn/n1/r →a.s. 0. (c) e|sn|r = o(n). (d) e(max1≤k≤n |sk|r) = o(n). hint. for (a) and (b) imply (c), use the hoffmann-jorgensen (8.10.6) below."
1329,1,[], Strong Markov Property for Sums of IID RVs,seg_97,"since it is clearly closed under complements and countable intersections. (clearly, [n = k] can be replaced by [n ≤ k] in the definition of fn in (2).)"
1330,0,['n'], Strong Markov Property for Sums of IID RVs,seg_97,proposition 7.1 both n and sn are fn -measurable.
1331,1,['random'], Strong Markov Property for Sums of IID RVs,seg_97,"theorem 7.1 (the strong markov property) if n is a stopping time, then the increments continuing from the random time"
1332,1,['distribution'], Strong Markov Property for Sums of IID RVs,seg_97,"have the same distribution on (r∞, b∞) as does sk, k ≥ 1. moreover, defining s̃ ≡ (s̃1, s̃2, . . .),"
1333,1,['independent'], Strong Markov Property for Sums of IID RVs,seg_97,"(4) f(s̃) ≡ f(s̃1, s̃2, . . .) is independent of fn (hence of n and sn )."
1334,1,['independence'], Strong Markov Property for Sums of IID RVs,seg_97,which is the statement of independence.
1335,0,[], Strong Markov Property for Sums of IID RVs,seg_97,"exercise 7.1 (manipulating stopping times) let n1 and n2 denote stopping times relative to an ↗ sequence of σ-fields a1 ⊂ a2 ⊂ · · · . show that n1 ∧ n2, n1 ∨ n2, n1 + n2, and no ≡ i are all stopping times."
1336,0,[], Strong Markov Property for Sums of IID RVs,seg_97,definition 7.2 define waiting times for return to the origin by
1337,1,['set'], Strong Markov Property for Sums of IID RVs,seg_97,"w1 ≡ min{n : sn = 0} with w1 = +∞ if the set is empty, . . (5) . ."
1338,1,['set'], Strong Markov Property for Sums of IID RVs,seg_97,. . wk ≡ min{n > wk−1 : sn = 0} with wk = +∞ if the set is empty.
1339,0,[], Strong Markov Property for Sums of IID RVs,seg_97,"then define tk ≡ wk − wk−1, with w0 ≡ 0, to be the interarrival times for return to the origin."
1340,0,[], Strong Markov Property for Sums of IID RVs,seg_97,"now, t1 = w1 is clearly a stopping time. thus, by the strong markov property, t1 is"
1341,1,['independent'], Strong Markov Property for Sums of IID RVs,seg_97,s̃ =∼ s. thus t2 is independent of the rv s̃(2) with kth coordinate s̃k
1342,1,"['bernoulli', 'process']", Strong Markov Property for Sums of IID RVs,seg_97,"st1+t2+k − st1+t2 and s̃(2) =∼ s̃(1) =∼ s. continue with s̃(3), etc. [note the relationship to interarrival times of a bernoulli process.]"
1343,1,['mean'], Strong Markov Property for Sums of IID RVs,seg_97,"exercise 7.2 (wald’s identity) (a) suppose x1,x2, . . . are iid with mean μ, and n is a stopping time with finite mean. show that sn ≡ x1 + · · · + xn satisfies"
1344,1,"['independent', 'mean', 'probability']", Strong Markov Property for Sums of IID RVs,seg_97,"(b) suppose each xk equals 1 or −1 with probability p or 1 − p for some 0 < p < 1. then define the rv n ≡ min{n : sn equals −a or b}, where a and b are strictly positive integers. show that n is a stopping time that is a.s. finite. then evaluate the mean en. [hint. [n ≥ k] ∈ f(s1, . . . , sk−1), and is thus independent of xk, while sn = ∑∞"
1345,1,"['set', 'probability', 'event', 'convergence', 'tail', 'limit']", Convergence of Series of Independent RVs,seg_99,"in section 8.4 we proved the slln after recasting it (via kronecker’s lemma) as a theorem about a.s. convergence of infinite series. in this section we consider the convergence of infinite series directly. since the convergence set of a series is a tail event (recall remark 7.2.1), convergence can happen only with probability 0 or 1. moreover, the first theorem below seems to both limit the possibilities and broaden the possible approaches to them. all proofs are given at the end of this section."
1346,1,['independent'], Convergence of Series of Independent RVs,seg_99,"theorem 8.1 let x1,x2, . . . be independent. then, for some rv s, we have"
1347,0,[], Convergence of Series of Independent RVs,seg_99,"[we will show the first equivalence now, and leave the second until exercise 10.2.10.]"
1348,1,['independent'], Convergence of Series of Independent RVs,seg_99,"theorem 8.2 (the 2-series theorem) let x1,x2, . . . be independent rvs for which xk =∼ (μk, σk2). let sn ≡ ∑k"
1349,1,['convergence'], Convergence of Series of Independent RVs,seg_99,"if a series is to converge, the size of its individual terms must be approaching zero. thus the rvs must be effectively bounded. thus truncation should be particularly effective for demonstrating the convergence of series."
1350,1,['independent'], Convergence of Series of Independent RVs,seg_99,"theorem 8.3 (the 3-series theorem) let x1,x2, . . . be independent rvs."
1351,1,[], Convergence of Series of Independent RVs,seg_99,if and only if for some c > 0 the following three series all converge:
1352,1,['condition'], Convergence of Series of Independent RVs,seg_99,"(b) the condition (7) holds for some c > 0 if and only if it holds for all c > 0. (c) if either ic, iic, or iiic diverges for any c > 0, then ∑n"
1353,1,['independent'], Convergence of Series of Independent RVs,seg_99,"example 8.1 suppose x1,x2, . . . are independent and are uniformly bounded. they are assumed to be independent of the iid rademacher rvs 1, . . . , n. then"
1354,0,[], Convergence of Series of Independent RVs,seg_99,∞ =1σk2). [this is immediate from the 2-series theorem.] =
1355,1,"['characteristic function', 'mean', 'function', 'variance']", Convergence of Series of Independent RVs,seg_99,"k=1xk/3k →a.s. (some s), and determine the mean, variance, and the name of the df fs of s. also determine the characteristic function of s (at some point after chapter 9)."
1356,1,['independent'], Convergence of Series of Independent RVs,seg_99,"n =1akxk →a.s. (some s) when x1,x2, . . . are independent ∼ with xk uniform(−k, k) for k ≥ 1, and where 0 < a < 1."
1357,1,"['variance', 'mean']", Convergence of Series of Independent RVs,seg_99,= (b) evaluate the mean and the variance (give a simple expression) of s.
1358,1,['independent'], Convergence of Series of Independent RVs,seg_99,n =1xk →a.s. (some rv s). the converse holds for independent rvs.
1359,1,"['distribution', 'mean', 'variance']", Convergence of Series of Independent RVs,seg_99,"and determine (if possible) the mean, variance, and distribution of the limiting rvs. (b) let y1, y2, . . . be iid cauchy(0,1) rvs. does ∑k"
1360,1,"['distribution', 'limit']", Convergence of Series of Independent RVs,seg_99,"∞ =1yk/2k →a.s. (some rv)? if so, what is the distribution of the limit?"
1361,1,['inequality'], Convergence of Series of Independent RVs,seg_99,but ottaviani–skorokhod’s inequality 10.3.4 gives
1362,1,['limit'], Convergence of Series of Independent RVs,seg_99,"using sn − sn →p 0 for (d). thus (9) holds, and sn →a.s. (some rv s′). the a.s. limit s′ equals s a.s. by proposition 2.3.4."
1363,1,['inequality'], Convergence of Series of Independent RVs,seg_99,"proof. consider theorem 8.2, part (a): we first verify (2). by theorem 8.1, to establish that s0,n →a.s. (some s) we need only show that s0,m − s0,n →p 0. but this follows immediately from chebyshev’s inequality, since"
1364,1,"['variance', 'independent', 'mean']", Convergence of Series of Independent RVs,seg_99,"(as the two rvs are independent, the first has mean 0, and both have finite variance (as follows from (e)). thus e(s02) = σ2. inasmuch as both s0,n →a.s. s0 and e(s02,n) → e(s02), the vitali theorem gives s0,n →l2 s0. then exercise 3.5.1b and vitali show that e(s0) = lim e(s0,n) = lim 0 = 0. as s = s0 + μ, we have es = e(s0 + μ) = μ and var[s] = var[s0] = e(s02) = σ2 = ∑∞"
1365,1,['inequality'], Convergence of Series of Independent RVs,seg_99,the proof of part (b) of the 2-series theorem above will require a converse of kolmogorov’s inequality that is valid for bounded rvs.
1366,1,"['independent', 'set', 'inequality']", Convergence of Series of Independent RVs,seg_99,"inequality 8.1 (kolmogorov’s other inequality) consider independent zero-mean rvs xk, and set sk ≡ x1 + · · · + xk for 1 ≤ k ≤ n. suppose |xk| ≤ (some m) < ∞ for all k. then"
1367,1,['independence'], Convergence of Series of Independent RVs,seg_99,1{e(sk21ak) + 2 · 0 + p (ak)e(sn − sk)2} by independence
1368,1,['event'], Convergence of Series of Independent RVs,seg_99,using |sn| ≤ λ on the event [mn ≤ λ] to obtain (d). combining (c) and (d) and doing algebra gives
1369,0,[], Convergence of Series of Independent RVs,seg_99,"proof. consider theorem 8.2, part(b): consider first the forward half of (4). since s0,n →a.s. (some rv s0), for some sufficiently large λ we have"
1370,1,['inequality'], Convergence of Series of Independent RVs,seg_99,nσk2 by kolmogorov’s other inequality 8.1
1371,1,['independent'], Convergence of Series of Independent RVs,seg_99,"consider (5). again, (2) gives the converse half. consider the forward half. suppose that sn →a.s. s. the plan is first to symmetrize, so that we can use (4) to prove (5). let xn’s be independent, and independent of the xn’s with xn"
1372,1,['distributions'], Convergence of Series of Independent RVs,seg_99,"the symmetrized rv. since → a.s. depends only on the finite-dimensional distributions, the given fact that sn →a.s. s implies that the rv sn"
1373,1,['mean'], Convergence of Series of Independent RVs,seg_99,1 (xk − μk) → a.s. (some rv s0) with mean 0. thus
1374,0,[], Convergence of Series of Independent RVs,seg_99,"1 μk convergent, and the forward half of (5) holds."
1375,0,[], Convergence of Series of Independent RVs,seg_99,proof. consider the 3-series theorem. consider (a) and (b) in its statement: suppose that
1376,1,[], Convergence of Series of Independent RVs,seg_99,the 3 series converge for at least one value of c. then ii and iii imply that ∑n
1377,0,[], Convergence of Series of Independent RVs,seg_99,"(c), . . . are khinchin equivalent sequences."
1378,0,[], Convergence of Series of Independent RVs,seg_99,i < ∞ holds for all c > 0 by the second borel–cantelli lemma. thus ∑n
1379,0,[], Convergence of Series of Independent RVs,seg_99,since i < ∞ implies that x1
1380,1,[], Convergence of Series of Independent RVs,seg_99,consider (c). kolmogorov’s 0-l law shows that sn either converges a.s. or else diverges a.s.; and it is not convergent if one of the three series fails to converge.
1381,1,[], Convergence of Series of Independent RVs,seg_99,"l2-convergence of infinite series, and a.s. convergence∗"
1382,1,"['variance', 'independent', 'mean']", Convergence of Series of Independent RVs,seg_99,"exercise 8.5 (l2-convergence of series) let x1,x2, . . . be independent rvs in l2, where xk has mean μk and variance σk2. then the sum sn ≡ x1 + · · · + xn has mean mn ≡ ∑k"
1383,1,['variance'], Convergence of Series of Independent RVs,seg_99,n =1μk and variance vn
1384,1,['mean'], Convergence of Series of Independent RVs,seg_99,"exercise 8.6 (chow–teicher) let x1,x2, . . . be iid with finite mean. suppose the series of"
1385,0,[], Convergence of Series of Independent RVs,seg_99,"1ak converges, where the |ak| are uniformly bounded. show that"
1386,0,[], Convergence of Series of Independent RVs,seg_99,other generalizations of the llns∗
1387,1,['independent'], Convergence of Series of Independent RVs,seg_99,"exercise 8.7 the following (with r = 1) can be compared to theorem 8.4.4. if x1,x2, . . . are independent with 0 means, then"
1388,1,"['continuous', 'independent', 'variation']", Convergence of Series of Independent RVs,seg_99,"exercise 8.8 (chung) here is an even more general variation on theorem 8.??. suppose that φ > 0 is even and continuous, and either φ(x)/x ↗ but φ(x)/x2 ↘ or else φ(x) ↗ but φ(x)/x ↘. let bn ↗ ∞. let x1,x2, . . . be independent with 0 means. then"
1389,1,"['function', 'characteristic function']", Convergence of Series of Independent RVs,seg_99,"the wlln is taken up again in sections 10.1 and 10.2, after the characteristic function tool has been introduced in chapter 9."
1390,1,['probability'], Martingales,seg_101,"definition 9.1 (martingales) (a) consider the sequence of rvs s1, s2, . . . defined on a probability space (ω,a, p ) and adapted to an ↗ sequence of σ-fields a1 ⊂ a2 ⊂ · · · . call it a martingale (abbreviated mg) if e|sk| < ∞ for all k, and"
1391,1,['set'], Martingales,seg_101,(1) e(sk|ai) =a.s. si for all i ≤ k in the index set.
1392,1,"['independent', 'set']", Martingales,seg_101,"example 9.1 (the prototypical example) let x1, . . . , xn denote independent rvs with 0 means, and set sk ≡ x1 + · · · + xk and ak ≡ σ[x1, . . . , xk] for 1 ≤ k ≤ n. then the sequence of partial sums satisfies"
1393,1,['variance'], Martingales,seg_101,while (provided xk also has finite variance σk2)
1394,0,[], Martingales,seg_101,"the first claim is trivial, and the second holds, since"
1395,0,[], Martingales,seg_101,notation 9.1 we will use the following notational system:
1396,0,[], Martingales,seg_101,⎧mg and = for a martingale.
1397,1,[], Martingales,seg_101,"⎩ s-mg and for a s-mg (mg or submg, as the casemay be)."
1398,0,['n'], Martingales,seg_101,"exercise 9.2 turn (sk2, ak), 1 ≤ k ≤ n into a martingale in (4) by centering it appropriately."
1399,1,['inequality'], Maximal Inequalities Some with  Boundaries o,seg_103,"inequality 10.1 (monotone inequality) for arbitrary rvs x1, . . . , xn and for constants 0 < b1 ≤ · · · ≤ bn we let sk ≡ x1 + · · · + xk and obtain"
1400,0,[], Maximal Inequalities Some with  Boundaries o,seg_103,"if all xi ≥ 0, then we may replace 2 by 1. [this also holds in higher dimensions, when properly formulated. see shorack and smythe(1976).]"
1401,1,['average'], Maximal Inequalities Some with  Boundaries o,seg_103,≤ia≤xk |tik|) since an average does not exceed the maximum
1402,1,['variances'], Maximal Inequalities Some with  Boundaries o,seg_103,"if (sk,ak), 1 ≤ k ≤ n is a zero-mean mg with all of the variances esk2 < ∞, then we can conclude that (sk2,ak), 1 ≤ k ≤ n is a submg. this allows the maximum to be bounded by"
1403,1,['inequality'], Maximal Inequalities Some with  Boundaries o,seg_103,"[this last is kolmogorov’s inequality, valid for zero-mean mgs.]"
1404,1,"['expectation', 'conditional', 'conditional expectation']", Maximal Inequalities Some with  Boundaries o,seg_103,by (7.4.1) in the definition of conditional expectation. now let
1405,0,[], Maximal Inequalities Some with  Boundaries o,seg_103,so that k is the first index for which sk is ≥ λ. then
1406,1,['set'], Maximal Inequalities Some with  Boundaries o,seg_103,"as claimed. in a s-mg context, these are called “first passage time” proofs. to this end, set τ equal to k on ak for 1 ≤ k ≤ n, and set τ equal to n + 1 on (∑n"
1407,1,['level'], Maximal Inequalities Some with  Boundaries o,seg_103,1ak)c. then τ is the first passage time to the level λ.
1408,1,"['conditional expectation', 'conditional', 'expectation', 'inequality']", Maximal Inequalities Some with  Boundaries o,seg_103,"that {(exp(rsk),ak), 1 ≤ k ≤ n} is also a submg for any r > 0 follows from jensen’s inequality for conditional expectation with an ↗ g(·) via"
1409,1,"['moment', 'moment generating function', 'function', 'inequality']", Maximal Inequalities Some with  Boundaries o,seg_103,"applying doob’s first inequality (2) to (e) gives (3). [this is often sharper than (2), though it requires the existence of the moment generating function e exp(rsn).] when (sk, ak) is a mg, then (sk2, ak) is also a submg (by another application of the same jensen’s inequality), so applying (2) to the latter submg gives (4)."
1410,1,['variance'], Maximal Inequalities Some with  Boundaries o,seg_103,"inequality 10.3 (hájek–rényi) let (sk,ak), 1 ≤ k ≤ n , be a mg with all esk = 0. let xk ≡ sk − sk−1 have variance σk2. let 0 < b1 ≤ · · · ≤ bn . then"
1411,1,"['independent', 'inequality']", Maximal Inequalities Some with  Boundaries o,seg_103,proof. (we give the proof for independent rvs.) the monotone inequality bounds the maximum partial sum via
1412,1,['inequality'], Maximal Inequalities Some with  Boundaries o,seg_103,applying kolmogorov’s inequality (4) to (6) gives
1413,1,['factor'], Maximal Inequalities Some with  Boundaries o,seg_103,(a more complicated proof can eliminate the factor 4.)
1414,1,['inequality'], Maximal Inequalities Some with  Boundaries o,seg_103,"exercise 10.1 to complete the proof of the hájek-rényi inequality for mgs, one can show that tk ≡ sn/bn + ∑n"
1415,1,['continuous'], Maximal Inequalities Some with  Boundaries o,seg_103,"inequality 10.4 (birnbaum–marshall) let (s(t), a(t)), 0 ≤ t ≤ θ, be a mg having s(0) = 0,es(t) = 0, and ν/(t) = es2(t) finite and continuous on [0, θ]. suppose that paths of s are right (or left) continuous. let q(·) > 0 on (0, θ] be ↗ and right (or left) continuous. then"
1416,1,"['case', 'conditional', 'inequality']", Maximal Inequalities Some with  Boundaries o,seg_103,"+,ak), for 1 ≤ k ≤ n, is also a submg, by the conditional version of jensen’s inequality. (or, refer to (13.1.7) below.) [refer to (13.1.5) for case (ii).] thus in case (i) we have"
1417,1,['inequality'], Maximal Inequalities Some with  Boundaries o,seg_103,∞ rλr−1λ−1e{sn+1[mn≥λ]} dλ by doob’s inequality 10.2
1418,1,['inequality'], Maximal Inequalities Some with  Boundaries o,seg_103,"r})(r−1)/r by hölder’s inequality,"
1419,1,"['results', 'case']", Maximal Inequalities Some with  Boundaries o,seg_103,which gives the results. (just change sn+ to |sn| for case (ii).)
1420,1,"['independent', 'moments', 'control']", Maximal Inequalities Some with  Boundaries o,seg_103,the following inequalities show that “in probability” control of the overall sum and of the maximal summand actually gives control of moments of sums of independent rvs.
1421,1,"['independent', 'probability']", Maximal Inequalities Some with  Boundaries o,seg_103,"inequality 10.6 (hoffmann–jorgensen, probability form). let x1, . . . , xn be independent rvs, and let sk ≡ x1 + · · · + xk for 1 ≤ k ≤ n. let λ, η > 0. then"
1422,1,['symmetric'], Maximal Inequalities Some with  Boundaries o,seg_103,"if the xi’s are also symmetric, then both"
1423,1,['moment'], Maximal Inequalities Some with  Boundaries o,seg_103,"inequality 10.7 (hoffmann–jorgensen, moment form). let the rvs x1, . . . , xn be independent, and let sk ≡ x1 + · · · + xk for 1 ≤ k ≤ n. suppose that each xi ∈ lr(p ) for some r > 0. then"
1424,1,['symmetric'], Maximal Inequalities Some with  Boundaries o,seg_103,"where t0 ≡ inf{t > 0 : p (max1≤k≤n |sk| > t) ≤ 1/(2 · 4r)}. if the xi’s are also symmetric, then"
1425,1,['inequality'], Maximal Inequalities Some with  Boundaries o,seg_103,"proof. consider inequality 10.6. let τ ≡ inf{k ≤ n : |sk| > λ}. then [τ = k] depends only on x1, . . . , xk, and [maxk≤n |sk| > λ] = ∑n"
1426,1,['independence'], Maximal Inequalities Some with  Boundaries o,seg_103,"therefore, by independence,"
1427,1,['inequality'], Maximal Inequalities Some with  Boundaries o,seg_103,the second inequality follows from the first by lévy’s inequality 8.3.3.
1428,1,"['symmetric', 'case']", Maximal Inequalities Some with  Boundaries o,seg_103,"for the symmetric case, first note that"
1429,0,[], Maximal Inequalities Some with  Boundaries o,seg_103,and hence summing over k then yields
1430,1,['inequality'], Maximal Inequalities Some with  Boundaries o,seg_103,the third inequality again follows from lévy’s inequality.
1431,1,['inequality'], Maximal Inequalities Some with  Boundaries o,seg_103,"proof. consider inequality 10.7. here is the proof of (14); the proof of (13) is similar. let u > t0. then, using (12) for (i),"
1432,0,[], Maximal Inequalities Some with  Boundaries o,seg_103,simple algebra now gives (14).
1433,1,['case'], Maximal Inequalities Some with  Boundaries o,seg_103,exercise 10.2 provide the details in the case of (13)
1434,1,"['condition', 'case', 'set', 'continuous', 'convergence', 'limit']", Classical Convergence in Distribution,seg_107,"∼ definition 1.1 (sub-dfs) (a) suppose we have rvs xn = fn and x. we now wish to allow the possibility that x is an extended rv. in this case, we assume that h is a sub-df (we will not use the notation f in this context), and we will write x ∼= h. the interpretation in the case of an extended rv x is that h(−∞) = p (x = −∞), h(x) = p (−∞ ≤ x ≤ x) for all −∞ < x < ∞, and 1 − h(+∞) = p (x = +∞). the set ch of all points at which h is continuous is called the continuity set of h. (b) if fn(x) → h(x) as n → ∞ at each x ∈ ch of a sub-df h, then we say that xn (or fn) converges in sub-df to x (or h), and we write xn →sd x(or fn →sd h) as n → ∞. [what has happened in the case of sub-df convergence is that amounts h(−∞) and 1 − h(+∞) of mass have escaped to −∞ and +∞, respectively.] (c) we have agreed that fn, f , etc. denote a bona fide df, while hn,h, etc. may denote a sub-df. thus fn →d f (with letter f rather than letter h) will still imply that the limit is necessarily a bona fide df. [the next definition provides a condition that guarantees (in a totally obvious way, on r at least) that any possible limit is a bona fide df.]"
1435,1,"['set', 'distributions']", Classical Convergence in Distribution,seg_107,definition 1.2 (tightness) a family p of distributions p on r is called tight if for each > 0 there is a compact set (which for one-dimensional rvs is just a closed and bounded set) k with
1436,1,"['continuous', 'expectations']", Classical Convergence in Distribution,seg_107,"theorem 1.1 (helly–bray) if fn →d f and g is bounded and continuous a.s. f , then the expectations satisfy"
1437,1,['continuous'], Classical Convergence in Distribution,seg_107,"conversely, if (2) holds for all bounded continuous g, then fn →d f."
1438,1,['continuous'], Classical Convergence in Distribution,seg_107,[thus fn →d f if and only if ∫ g dfn → ∫ g df for all bounded and continuous g.]
1439,1,['continuous'], Classical Convergence in Distribution,seg_107,theorem 1.2 (continuous mapping theorem; mann–wald) suppose that xn →d x and suppose that g is continuous a.s. f . then g(xn) →d g(x).
1440,1,"['functions', 'condition', 'continuous', 'distribution', 'convergence']", Classical Convergence in Distribution,seg_107,how do we establish that fn →d f? we have the necessary and sufficient condition of the helly–bray theorem 1.1 (presented earlier as theorem 3.5.1). (we should now recall our definition of the determining class used in the context of the proof of theorem 3.5.1.) we can also show convergence in distribution of more complicated functions of rvs via mann–wald’s continuous mapping theorem 1.2 (presented earlier as theorem 3.5.2); an example is given by
1441,1,['limit'], Classical Convergence in Distribution,seg_107,where g(x) = x2. the concept of tightness was introduced above to guarantee that any possible limit is necessarily a bona fide df. this becomes more important in light of the next theorem.
1442,1,['limit'], Classical Convergence in Distribution,seg_107,"theorem 1.3 (helly’s selection theorem) let f1, f2, . . . be any sequence of dfs. there necessarily exists a subsequence fn′ and a sub-df h for which fn′ →sd h. if the subsequence of dfs is tight, then the limit is necessarily a bona fide df."
1443,0,[], Classical Convergence in Distribution,seg_107,"corollary 1 let f1, f2, . . . be any sequence of dfs. let h be a fixed sub-df. suppose every sd-convergent subsequence {fn"
1444,1,['distribution'], Classical Convergence in Distribution,seg_107,′} satisfies fn ′ →sd (this same h). then the whole sequence satisfies fn →sd h. (here is an alternative phrasing. suppose every subsequence n′ contains a further subsequence n′′ for which fn′′ converges in distribution to this one fixed sub-df h. then the whole sequence satisfies fn →sd h.)
1445,0,[], Classical Convergence in Distribution,seg_107,"proof. let r1, r2, . . . denote a sequence which is dense in r. using bolzano-weierstrass, choose a subsequence n1j such that fn1j (r1) → (some a1). a further subsequence n2j also satisfies fn2j (r2) → (some a2). continue in this fashion. the diagonal subsequence njj converges to ai at ri for all i ≥ 1. [this cantor diagonalization technique is important. learn it!] define ho on the ri’s via ho(ri) = ai. now define h on all real values via"
1446,1,[], Classical Convergence in Distribution,seg_107,"this h is clearly is ↗ and takes values in [0, 1]. we must now verify that h is also rightcontinuous, and that fn →sd h. that is, the diagonal subsequence, which we will now"
1447,1,[], Classical Convergence in Distribution,seg_107,"the monotonicity of ho trivially gives infy↘x h(y) ≥ h(x). meanwhile,"
1448,1,['continuous'], Classical Convergence in Distribution,seg_107,"yields infy↘x h(y) ≤ h(x). hence infy↘x h(y) = h(x), and h is right continuous."
1449,1,['limit'], Classical Convergence in Distribution,seg_107,passing to the limit on j gives
1450,1,['limit'], Classical Convergence in Distribution,seg_107,"consider the corollary. fact: any bounded sequence of real numbers contains a convergent subsequence; and the whole original sequence converges if and only if all subsequential limit points are the same. or, if every subsequence an′ contains a further subsequence an′′ that converges to the one fixed number ao, then we have an → ao. we effectively showed above that every subsequence fn"
1451,1,"['moments', 'expectations', 'convergence']", Classical Convergence in Distribution,seg_107,exercise 1.1 (convergence of expectations and moments) (a) suppose fn →sd h and that both fn−(a) → h−(a) and fn(b) → h(b) for some constants −∞ < a < b < ∞ in ch having h(a) < h(b). then
1452,1,['continuous'], Classical Convergence in Distribution,seg_107,"(3) ∫[a,b]g dfn → ∫[a,b] g dh for all g ∈ c[a,b] ≡ {g : g is continuous on [a, b]}."
1453,1,['continuous'], Classical Convergence in Distribution,seg_107,"where c0 ≡ {g : g is continuous on r and g(x) → 0 as |x| → ∞}. (b) suppose fn →d f and g is continuous on the line. suppose |g(x)|/ψ(x) → 0 as |x| → ∞, where ψ ≥ 0 has ∫ ψdfn ≤ k < ∞ for all n. then ∫ g dfn → ∫ g df. (c) if e|xn|r0 < (some m) < ∞ for all large n, then fn →d f implies that"
1454,1,['continuous'], Classical Convergence in Distribution,seg_107,"(d) let g be continuous. if fn →sd h, then lim inf ∫ |g|dfn ≥ ∫ |g| dh."
1455,1,['continuous'], Classical Convergence in Distribution,seg_107,"[actually, g continuous a.s.h suffices in (a), (b), and (d) above.]"
1456,1,['continuous'], Classical Convergence in Distribution,seg_107,"exercise 1.2 (pólya’s lemma) if fn →d f for a continuous df f , then"
1457,1,['continuous'], Classical Convergence in Distribution,seg_107,"thus if fn →d f with f continuous and xn → x, then fn(xn) → f (x)."
1458,0,['n'], Classical Convergence in Distribution,seg_107,∼ exercise 1.3 (verifying tightness) suppose xn = fn. show that {fn : n ≥ 1} is tight if either
1459,1,"['distribution', 'convergence']", Classical Convergence in Distribution,seg_107,equivalent definitions of convergence in distribution
1460,1,"['functions', 'condition', 'function', 'expectations']", Classical Convergence in Distribution,seg_107,"the condition fn(x) → f (x) can be rewritten both as pn((−∞, x]) → p ((−∞, x]) as well as e1(−∞,x](xn) → e1(−∞,x](x). thus →d is reduced to computing expectations of the particularly simple function 1(−∞,x]; but these simple functions have the disadvantage of being discontinuous."
1461,1,['set'], Classical Convergence in Distribution,seg_107,"definition 1.3 (closure, interior, and boundary) the closure of b is defined to be b̄ ≡ ∩{c : b ⊂ c and c is closed}, while b0 ≡ ∪{ u : u ⊂ b and u is open} is called the interior of b. these have the property that b̄ is the smallest closed set containing b, while b0 is the largest open set contained within b. the boundary of b is defined to be ∂b ≡ b̄\b0. a set b is called a p -continuity set if p (∂b) = 0. (these definitions are valid on a general metric space, not just on r.)"
1462,1,"['functions', 'continuous', 'associated', 'distributions']", Classical Convergence in Distribution,seg_107,"theorem 1.4 (→d equivalencies) let f, f1, f2, . . . be the dfs associated with the probability distributions p1, p2, . . .. let cb denote all bounded, continuous functions g on r, and then let cbu denote all bounded and uniformly continuous functions g on r. the following are equivalent:"
1463,1,['set'], Classical Convergence in Distribution,seg_107,(8) fn(x) → f (x) for all x in a dense set.
1464,1,['sets'], Classical Convergence in Distribution,seg_107,(11) lim pn(b) ≤ p (b) for all closed sets b.
1465,1,['sets'], Classical Convergence in Distribution,seg_107,(12) lim pn(b) ≥ p (b) for all open sets b.
1466,1,['sets'], Classical Convergence in Distribution,seg_107,(13) lim pn(b) = p (b) for all p -continuity sets b.
1467,1,['intervals'], Classical Convergence in Distribution,seg_107,(14) lim pn(i) = p (i) for all (even unbounded) p -continuity intervals i.
1468,1,[], Classical Convergence in Distribution,seg_107,"(15) l(fn, f ) → 0 for the lévy metric l (see below)."
1469,0,[], Classical Convergence in Distribution,seg_107,"exercise 1.4 that (7)–(10) are equivalent is either trivial, or done previously. cite the various reasons. then show that (11)–(15) are also equivalent to →d ."
1470,1,['set'], Classical Convergence in Distribution,seg_107,"show that l is a metric and that the set of all dfs under l forms a complete and separable metric space. also show that fn →d f is equivalent to l(fn, f ) → 0."
1471,1,['convergence'], Classical Convergence in Distribution,seg_107,"∼ theorem 1.5 (convergence of types) suppose (xn−bn)/an →d x = f, and (xn−βn)/"
1472,1,"['normal distributions', 'distribution', 'normal', 'distributions']", Classical Convergence in Distribution,seg_107,"remark 1.1 the classical clt implies that if x1,x2, . . . are iid (0, σ2), then sn/√n →d n(0, 1). the above theorem tells us that no matter how we normalize sn, the only possible nondegenerate limits in distribution are normal distributions. moreover, if sn/an →d (some rv), the limiting distribution can be nondegenerate only if an/√n → (some constant) ∈ (0,∞)."
1473,1,['convergence'], Classical Convergence in Distribution,seg_107,exercise 1.6 (proof of the convergence of types theorem) prove theorem 1.5 on the convergence of types.
1474,1,"['distribution', 'random']", Classical Convergence in Distribution,seg_107,"if x,x1,x2, . . . are k-dimensional random vectors with dfs f, f1, f2, . . ., then we say that xn converges in distribution to x if"
1475,0,[], Classical Convergence in Distribution,seg_107,just as in one dimension.
1476,1,['results'], Classical Convergence in Distribution,seg_107,"the helly–bray theorem, the mann–wald theorem, helly’s selection theorem, and polya’s lemma all hold in k dimensions; generalizations of the other results also hold. moreover, if xn ′ denotes the first j coordinates of xn, with 1 ≤ j < k, then xn →d x implies xn"
1477,0,[], Classical Convergence in Distribution,seg_107,"exercise 1.7 prove the k-dimensional helly–bray theorem (along the lines of exercise 3.5.2) using helly’s selection theorem and pólya’s lemma. prove that xn →d x implies xn ′ →d x ′. after reading section 2, prove the k-dimensional version of the mann–wald theorem."
1478,0,[], Classical Convergence in Distribution,seg_107,exercise 1.8 prove that theorem 1.4 holds in k dimensions.
1479,1,['functions'], Determining Classes of Functions,seg_109,"we can approximate the functions 1(−∞,z](·) to an arbitrary degree of accuracy within various classes of particularly smooth functions. within these classes of functions we do not have to worry about the continuity of the limiting measure at z, and this will make these classes more convenient. indeed, the specialized class h0 below is of this type."
1480,1,"['functions', 'continuous']", Determining Classes of Functions,seg_109,"definition 2.1 (determining class) a collection g of bounded and continuous functions g is called a determining class if for any choice of dfs f̃ and f , the requirement that ∫ g df̃ ="
1481,1,"['functions', 'continuous']", Determining Classes of Functions,seg_109,definition 2.2 (various classes of smooth functions) (i) let c (let cb) [let cbu] denote the class of continuous (bounded and continuous) [bounded and also uniformly continuous]
1482,0,[], Determining Classes of Functions,seg_109,functions on r. let cb
1483,0,[], Determining Classes of Functions,seg_109,(k) (let cb (∞)) denote the subclasses with k (with all) derivatives
1484,1,"['functions', 'linear', 'continuous']", Determining Classes of Functions,seg_109,"bounded and continuous. (ii) an extra c on these classes will indicate that all functions vanish outside some compact subset of r. (iii) let c0 denote the subclass of c that converge to 0 as |x| → ∞. (iv) let h0 denote the class of all hz, with z real and > 0; here hz, (x) equals 1, is linear, equals 0 according as x is in (−∞, z], is in [z, z + ], is in [z + ,∞) (this class was introduced in the proof of the helly–bray theorem 3.5.1). (v) let g0 denote the class of all continuous functions ga,b, with a < b and > 0; here ga,b, (x) equals 0, is linear, equals 1 according as x is in (−∞, a− ] ∪ [b+ ,∞), is in [a− , a]∪ [b, b+ ], is in [a, b]."
1485,1,['functions'], Determining Classes of Functions,seg_109,"class. (iii) so, too, if we add an extra subscript c to the various c-classes in (ii). (that is, we require they take on the value 0 outside some compact subset of r.) [for some proofs in the literature, functions g with sharp corners are unhandy.]"
1486,0,[], Determining Classes of Functions,seg_109,exercise 2.1 prove the previous theorem.
1487,1,['results'], Determining Classes of Functions,seg_109,exercise 2.2 (higher dimensions) show that the natural extension of each of the results of this section to rk is valid.
1488,0,[], Determining Classes of Functions,seg_109,exercise 2.3 exhibit at least one more determining class.
1489,1,"['limit', 'moment']", Determining Classes of Functions,seg_109,moments as a determining class for a moment unique limit
1490,1,"['moments', 'moment']", Determining Classes of Functions,seg_109,"theorem 2.2 (clt via moments; fréchet-shohat) (a) suppose f is the unique df having the specific finite moment values μk = ∫ xk df (x), for all integers k ≥ 1. then fn →d f whenever"
1491,1,"['normal', 'moments']", Determining Classes of Functions,seg_109,(b) any normal df is determined by its moments.
1492,1,['inequality'], Determining Classes of Functions,seg_109,"proof. let n′ denote an arbitrary subsequence. by the helly selection theorem we have fn′′ →sd h for some further subsequence n′′ and some sub-df h. however, lim e|xn|2 < ∞, so that {fn : n ≥ 1} is tight by markov’s inequality. thus h is a bona fide df, and fn"
1493,1,['hypotheses'], Determining Classes of Functions,seg_109,′′(x) by hypotheses
1494,1,['moments'], Determining Classes of Functions,seg_109,"thus ∫ xk dh(x) = ∫ xk df (x) for all k ≥ 1; and since only f has these moments, we conclude that h = f . thus fn"
1495,0,[], Determining Classes of Functions,seg_109,"′′ →d f . moreover, fn ′′ →d (this same f ) on any such convergent subsequence n′′ thus fn →d f , by the corollary to theorem1.3 of section 9. see exercise 2.6 of section 9 for part (b) of the theorem."
1496,1,"['distribution', 'moments']", Determining Classes of Functions,seg_109,"in general, moments do not determine a distribution uniquely; thus {xk : k ≥ 1} is not a determining class. this is shown by the following exercise."
1497,1,['moments'], Determining Classes of Functions,seg_109,"exercise 2.4∗ (moments need not determine the df; heyde) suppose that the rv log x =∼ n(0, 1); thus"
1498,1,"['function', 'density function']", Determining Classes of Functions,seg_109,"for each −1 ≤ a ≤ 1, let ya have the density function"
1499,1,"['moments', 'distributions']", Determining Classes of Functions,seg_109,show that x and each ya have exactly the same moments. [knowing that these particular distributions have this property is not worth much; it is knowing that some dfs have this property that matters.]
1500,1,['moments'], Determining Classes of Functions,seg_109,"though we have just seen that moments do not necessarily determine a df, it is often true that a given df f is the unique df having its particular moments (name them {μk : k ≥ 1}). here is an “exercise” giving various sufficient conditions."
1501,1,['moments'], Determining Classes of Functions,seg_109,exercise 2.5 (when moments do determine a df) suppose either of the following conditions hold:
1502,1,['interval'], Determining Classes of Functions,seg_109,1 μ2kt2k/(2k)! < ∞ in some interval of t values.
1503,1,['moment'], Determining Classes of Functions,seg_109,then at most one df f can possess the moment values μk = ∫ xk df (x). [wait to prove this until it appears again as part of exercise 6.1 of section 9].
1504,1,"['convergence', 'condition']", Determining Classes of Functions,seg_109,1/2k comment. a condition for convergence due to carleman μ = ∞ has often been claimed to be necessary and sufficient. it is not. see stoyanov (1977; p. 113).
1505,1,"['distribution', 'moments']", Determining Classes of Functions,seg_109,"exercise 2.6 show that the n(0, 1) distribution is uniquely determined by its moments."
1506,1,"['functions', 'characteristic function', 'condition', 'moment', 'results', 'associated', 'function']", Determining Classes of Functions,seg_109,summary the methods of this section that establish →d by verifying the moment condition that eg(xn) → eg(x) for all functions g in a given determining class g can be extended from the present setting of the real line to more general settings; note chapter 15. this chapter now turns to the development of results associated with the particular determining class g ≡ {gt(·) ≡ eit : t ∈ r}. the resulting function is called the characteristic function of the rv x. the rest of chapter 9 includes a specialized study of the characteristic function. chapter 10 will apply this characteristic function tool to the clt.
1507,1,"['function', 'characteristic function']", Characteristic Functions with Basic Results,seg_111,"definition 3.1 (characteristic function) let x be an arbitrary rv, and let f denote its df. the characteristic function of x (abbreviated chf) is defined (for all t ∈ r) by"
1508,1,"['transform', 'fourier transform']", Characteristic Functions with Basic Results,seg_111,"with df replaced by h dμ, we call this the fourier transform of the signed measure hdμ. (we note that the chf φx(t) exists for −∞ < t < ∞ for all rvs x, since |eitx(ω)| ≤ 1 for all t and all ω.)"
1509,0,[], Characteristic Functions with Basic Results,seg_111,proposition 3.1 (elementary properties) let φ denote an arbitrary chf.
1510,1,['independent'], Characteristic Functions with Basic Results,seg_111,"n =1φxi(·) when x1, . . . , xn are independent."
1511,1,['continuous'], Characteristic Functions with Basic Results,seg_111,(g) φ(·) is uniformly continuous on r.
1512,1,"['function', 'characteristic function', 'independent']", Characteristic Functions with Basic Results,seg_111,"= if φ is real, then φx = φ̄x = φ−x ; so x ∼= −x by the uniqueness theorem below. (f) if x and x ′ are independent with characteristic function φ, then φx−x′ = φxφ−x = φφ̄ = |φ|2. for (g), we note that for all t, |φ(t + h) − φ(t)| = |∫[expi(t+h)x −eitx] df (x)|"
1513,1,['function'], Characteristic Functions with Basic Results,seg_111,"as h → 0, by the dct with dominating function 2."
1514,1,[], Characteristic Functions with Basic Results,seg_111,"the converse of (c) is false. let x1 ≡ x2 and x3 be two iid cauchy(0, 1) rvs. we will see below that φcauchy(t) = exp(−|t|), giving φ2x1(t) = φx1+x2(t) = φx1+x3(t) for all t."
1515,1,"['functions', 'method', 'distribution', 'exponential', 'limit']", Characteristic Functions with Basic Results,seg_111,"motivation 3.1 (proving the clt via chfs) in this chapter we present an alternative method for establishing fn →d f . it is based on the fact (to be demonstrated below) that the complex exponential functions eit. on r, indexed by t ∈ r, form a limit determining class. saying this another way, the chf φ determines the distribution p , or the df f (or the density f, if there is one). thus (as is shown in the continuity theorem below) we can establish that fn →d f by showing that φn(·) → φ(·) on r. indeed, using just the elementary properties listed above, it is trivial to give an informal “proof” of the classical clt. thus, we begin by expanding the chf of one rv x as"
1516,1,"['errors', 'estimate']", Characteristic Functions with Basic Results,seg_111,"(in section 6 we will make such expansions rigorous, and in section 7 we will estimate more carefully the size of the errors that were made.)"
1517,1,['standardized'], Characteristic Functions with Basic Results,seg_111,"then the standardized sum of the iid rvs x1, . . . , xn is"
1518,0,[], Characteristic Functions with Basic Results,seg_111,"∼ since φzn(·) → φz(·) on r, where z = n(0, 1), the uniqueness theorem and the continuity theorem combine to guarantee that zn →d z. in principle, this is a rather elementary way to prove the clt."
1519,1,"['statistician', 'information', 'distribution', 'probability']", Characteristic Functions with Basic Results,seg_111,"think of it this way. to have all the information on the distribution of x, we must know p (x ∈ b) for all b ∈ b. we have seen that the df f also contains all this information, but it is presented in a different format; a statistician may well regard this f format as the “tabular probability calculating format.” when a density f exists, it also contains all the information about p ; but it is again presented in a different format, which the statistician may regard as the “distribution visualization format.” we will see that the chf presents all the information about p too. it is just one more format, which we may well come to regard as the “theorem proving format”."
1520,1,"['functions', 'characteristic functions']", Characteristic Functions with Basic Results,seg_111,table 3.1 some important characteristic functions
1521,1,[], Characteristic Functions with Basic Results,seg_111,poisson(λ) e−λλk/k!; for k ≥ 0 exp(λ(eit − 1)) geometricf(p) pqk; for k ≥ 0 p(1 − qeit)−1
1522,1,[], Characteristic Functions with Basic Results,seg_111,"normal(μ, σ2) e−(x−μ)2/2σ2/√2πσ on r exp(itμ − σ2t2/2) exponential(θ) e−x/θ/θ on r+ (1 − itθ)−1 chisquare(n) x(n/2)−1e−x/2/[2n/2γ(n/2)] (1 − 2it)−n/2 gamma(r, θ) xr−1e−x/θ/[θrγ(r)] on r+ (1 − itθ)−r uniform(0, 1) 1[0,1](x) [exp(it) − 1]/it double exp(θ) e−|x|/θ/2θ 1/(1 + θ2t2) cauchy(0, 1) 1/[π(1 + x2)] e−|t| de la vallée poussin (1 − cos x)/(πx2) on r [1 − |t|] × 1[−1,1](t) triangular(0, 1) [1 − |x|] × 1[−1,1](x) 2(1 − cos t)/(t2) on r"
1523,0,[], Characteristic Functions with Basic Results,seg_111,review of some useful complex analysis
1524,1,['function'], Characteristic Functions with Basic Results,seg_111,"a function f is called analytic on a region (a connected open subset of the complex plane) if it has a derivative at each point of the region; if it does, then it necessarily has derivatives of all orders at each point in the region. if z0 is an isolated singularity of f and f(z) ="
1525,1,['continuous'], Characteristic Functions with Basic Results,seg_111,"we also note that a smooth arc is described via equations x = φ(t) and y = ψ(t) for a ≤ t ≤ b when φ′ and ψ′ are continuous and not simultaneously zero. a contour is a continuous chain of a finite number of smooth arcs that do not cross the same point twice. closed means that the starting and ending points are identical. (see ahlfors (1953, pp. 102, 123) for what follows.)"
1526,0,[], Characteristic Functions with Basic Results,seg_111,"lemma 3.1 (residue theorem) if f is analytic on a region containing a closed contour c, except for a finite number of singularities z1, . . . , zn interior to c at which f has residues k1, . . . , kn, then (for counterclockwise integration over c)"
1527,1,"['functions', 'taylor series', 'set']", Characteristic Functions with Basic Results,seg_111,"lemma 3.2 let f and g be functions analytic in a regions ω. suppose that f(z) = g(z) for all z on a set s that has an accumulation point in ω. we then have the equality f(z) = g(z) for all z ∈ ω. (that is, f is determined on ω by its values on s. so if there is a taylor series"
1528,1,['coefficients'], Characteristic Functions with Basic Results,seg_111,"∞ =0 an(z − zo)j valid on some disk interior to ω, then the coefficients a1, a2, . . . determine f on all of ω.)"
1529,1,"['functions', 'characteristic functions']", Characteristic Functions with Basic Results,seg_111,evaluating various characteristic functions
1530,1,['cauchy'], Characteristic Functions with Basic Results,seg_111,"example 3.1 (derivation of the cauchy(0, 1)chf) let c denote the upper semicircle centered at the origin with radius r parametrized counterclockwise; and let a (for arc) denote c without its base. let t > 0. the cauchy chf is approached via"
1531,0,[], Characteristic Functions with Basic Results,seg_111,it further holds that
1532,1,['absolute value'], Characteristic Functions with Basic Results,seg_111,since the second integral in (b) is bounded in absolute value by
1533,1,"['cauchy', 'symmetric']", Characteristic Functions with Basic Results,seg_111,"since the cauchy is symmetric, φ(−t) = φ(t) = exp(−|t|); or, integrate the contour clockwise when t < 0. the tabular entry has been verified. that is,"
1534,1,[], Characteristic Functions with Basic Results,seg_111,"(4) φ(t) = exp(−|t|), for all t, gives the cauchy(0,1) chf."
1535,1,"['function', 'variable']", Characteristic Functions with Basic Results,seg_111,"let us instead think of φ as a function of a complex variable z. that is,"
1536,1,['function'], Characteristic Functions with Basic Results,seg_111,let us define a second function ψ on the complex plane by
1537,0,[], Characteristic Functions with Basic Results,seg_111,now φ and ψ are analytic on the whole complex plane. let us now consider the purely imaginary line z = iy. on this line it is clear that
1538,0,[], Characteristic Functions with Basic Results,seg_111,and since elementary calculations show that
1539,1,"['distribution', 'gamma distribution', 'gamma']", Characteristic Functions with Basic Results,seg_111,(a similar approach works for the gamma distribution in exercise 3.3 below.)
1540,1,['poisson'], Characteristic Functions with Basic Results,seg_111,"exercise 3.2 (a) derive the poisson (λ) chf (by summing power series). (b) derive the geometrict(p)chf. (c) derive the bernoulli(p), binomial(n, p), and negbit(m, p) chfs."
1541,1,[], Characteristic Functions with Basic Results,seg_111,"exercise 3.3 (a) derive the gamma(r, θ)chf. [hint. note example 3.2.] (b) derive the exponential(θ) and chisquare(n), and double exponential(θ) chfs."
1542,1,[], Characteristic Functions with Basic Results,seg_111,"exercise 3.4 derive the logistic(0, 1) chf. hint. use the lemma 3.2 approach."
1543,0,[], Characteristic Functions with Basic Results,seg_111,exercise 3.5 show that the real part of a chf (or re φ(·)) is itself a chf.
1544,1,"['function', 'hypothesis']", Uniqueness and Inversion,seg_113,"for the chf to be a useful tool, there must be a l-to-l correspondence between dfs and chfs. the fact that this is so is called the uniqueness theorem. we give a simple proof of the uniqueness theorem at the end of this subsection. but the simple proof does not establish an inversion formula that expresses the df as a function of the chf. in order to establish an inversion formula, we will need some notation, and an inversion formula useful for other purposes will require a hypothesis on the chf that is strong enough to allow some useful simplification."
1545,1,['continuous'], Uniqueness and Inversion,seg_113,let u denote a rv with continuous density fu (·) and let w denote a rv with a bounded and continuous density fw (·) and with chf φw (·); and suppose we are lucky enough to determine a complementary pair that (for some constant c) satisfy the relationship
1546,1,[], Uniqueness and Inversion,seg_113,(1) fu (t) = c φw (−t) for all real t. (complementary pair)
1547,1,[], Uniqueness and Inversion,seg_113,"(the cauchy(0, 1) and the double exponential(0, 1) then lead to two additional complementary pairs.) (the beauty of this is that we can nearly eliminate the use of complex analysis.) (in all such examples we have 2πcfw (0) = 1.)"
1548,1,['convolution'], Uniqueness and Inversion,seg_113,"an arbitrary rv x, having df fx(·) and chf φx(·) may not have a density. let us recall from the convolution formula (a.2.2) that (if u has a density) a slightly perturbed version xa of x is smoother than x, in that"
1549,1,['set'], Uniqueness and Inversion,seg_113,"by slutsky’s theorem, since au →p 0 as a → 0. thus f (·) = lim fa(·) at each point in the continuity set cf of f . this is the key to the approach we will follow to establish an inversion formula."
1550,0,[], Uniqueness and Inversion,seg_113,theorem 4.1 (uniqueness theorem) every df on the line has a unique chf.
1551,0,[], Uniqueness and Inversion,seg_113,theorem 4.2 (inversion formula) if an arbitrary rv x has df fx(·) and chf φx(·) we can always write
1552,1,"['densities', 'condition']", Uniqueness and Inversion,seg_113,theorem 4.3 (inversion formula for densities) if a rv x has a chf φx(·) that satisfies the integrability condition
1553,1,['continuous'], Uniqueness and Inversion,seg_113,then x has a uniformly continuous density fx(·) given by
1554,1,['set'], Uniqueness and Inversion,seg_113,"remark 4.1 the uniqueness theorem can be restated as follows: the set of complex exponentials g ≡ {eitx for x ∈ r : t ∈ r} is a determining class. this is so because knowing all values of φx(t) = eeitx allows the df f to be determined, via the inversion formula."
1555,1,['convolution'], Uniqueness and Inversion,seg_113,proof. from the convolution formula (a.2.2) and xa ≡ x + au we have
1556,0,[], Uniqueness and Inversion,seg_113,this establishes theorems 4.1 and 4.2.
1557,1,"['utility', 'hypothesis']", Uniqueness and Inversion,seg_113,"the particular formula given in (c) might look useless, but the mere fact that one can recover fx from φx via some formula is enough to establish the important property of uniqueness. (see exercise 4.3 for some utility for (7).) we now turn to theorem 4.3, in which we have added a hypothesis that allows the previous formula to be manipulated into a simple and useful form."
1558,1,"['function', 'hypothesis']", Uniqueness and Inversion,seg_113,"suppose that (8) holds, so that applying the dct to (b) (using a constant times |φx(·)| as a dominating function) gives [recall the hypothesis on the fw (·) of (1)] as a → 0 that"
1559,1,['continuous'], Uniqueness and Inversion,seg_113,since fw is bounded and is continuous at 0. note that uniform continuity of f follows from the bound
1560,1,"['function', 'convergence', 'interval']", Uniqueness and Inversion,seg_113,by applying the dct (with dominating function 2c‖fw ‖|φx(·)|). the uniform convergence of fa to f on any finite interval involves only an |fw (0) − fw (av)| term under the integral sign. that f really is the density of fx follows from applying this uniform convergence in (c) to obtain
1561,0,[], Uniqueness and Inversion,seg_113,the conclusion (9) holds since specifying u = w = z gives
1562,1,['inequality'], Uniqueness and Inversion,seg_113,"esseen’s inequality 9.7.1 below provides an important extension of theorem 4.2 by showing that if two chfs are sufficiently close over most of their domain, then the corresponding dfs will be uniformly close over their entire domain."
1563,1,[], Uniqueness and Inversion,seg_113,"exercise 4.1 show that setting w = z in line (c) of the previous proof leads, for any rv x, to the alternative inversion formula"
1564,0,[], Uniqueness and Inversion,seg_113,at all continuity points r1 < r2 of fx(·). [this is one possible alternative to (6).]
1565,1,"['function', 'interval']", Uniqueness and Inversion,seg_113,"exercise 4.2 derive the chf of the triangular(0, 1) density on the interval [−1, 1] (perhaps, add two appropriate uniform rvs). then use theorem 4.3 to derive the chf of the de la vallée poussin density, while simultaneously verifying that the non-negative and real integrable function (1 − cos x)/(πx2) really is a density. following section 6, determine e|x| when x has the de la vallée poussin density."
1566,1,"['kernel', 'estimator', 'estimate']", Uniqueness and Inversion,seg_113,"exercise 4.3 (kernel density estimator) since the rv x having df fx(·) and chf φx(·) may not have a density, we choose instead to estimate the density fa(·) of (5) and (7) using"
1567,0,[], Uniqueness and Inversion,seg_113,2 is finite] with the
1568,1,"['kernel', 'estimator']", Uniqueness and Inversion,seg_113,"(a) verify that f̂a(·) is actually a kernel density estimator, meaning that it can be expressed as"
1569,1,"['densities', 'observations', 'statistical']", Uniqueness and Inversion,seg_113,"[this has statistical meaning, since we are averaging densities centered at each of the observations.]"
1570,1,"['variance', 'mean', 'unbiased']", Uniqueness and Inversion,seg_113,(b) show that f̂a(x) is always unbiased (in that it has mean fa(x)) and has a finite variance we can calculate; thus for all x ∈ r we can show that
1571,1,['mean'], Uniqueness and Inversion,seg_113,"(2), determine the order of the mean squared"
1572,1,[], Uniqueness and Inversion,seg_113,(16) mse{f̂a(x)} ≡ bias2{f̂a(x)} + var[f̂a(x)] ≡ {e(f̂a(x)) − f(x)}2 + var[f̂a(x)]
1573,1,['estimator'], Uniqueness and Inversion,seg_113,"of f̂a(x), viewed as an estimator of f(x). (it is intended that you rewrite (16) by expanding"
1574,1,['taylor series'], Uniqueness and Inversion,seg_113,fa(x) in a taylor series in “a” (valid for f(·) ∈ cb
1575,0,[], Uniqueness and Inversion,seg_113,"(2)), and then analyze the magnitude of (16)"
1576,1,"['kernel', 'estimator']", Uniqueness and Inversion,seg_113,"for values of “a” near 0. it might also be useful to relabel fu (·) by ψ now so that your work refers to any kernel density estimator, right from the beginning. this will avoid “starting"
1577,1,['mse'], Uniqueness and Inversion,seg_113,−4/5 over” in part (f).) show that this mse expression is of order n for f(·) ∈ cb
1578,1,"['mse', 'densities', 'kernel', 'data', 'normal', 'parameter', 'function', 'estimator']", Uniqueness and Inversion,seg_113,"(2) when a is −1/5 of order n , and that this is the minimal attainable order. (d) note that the choice u = z (or u = t ) leads to an f̂a(·) that is the sum of n normal (or triangular) densities that are centered at the n data points and that have a scale parameter directly proportional to a. (e) obtain an expression for lima→0 a4/5 mse {f̂a(x)} in terms of f(x), f ′(x), and f ′′(x) when a = n−1/5 (and obtain it for both of the choices u = z and u = t ). (f) we could also motivate the idea of a kernel density estimator based on (13) alone. how much of what we have done still carries over for a general kernel? what properties should a good kernel exhibit? what can you prove in this more general setting? (now, for sure, replace fu by a function labeled ψ. a simple sentence that specifies the requirements on ψ should suffice.)"
1579,1,"['poisson', 'densities', 'independent', 'gamma', 'table', 'cauchy', 'convolution', 'normal', 'binomial', 'distributions']", Uniqueness and Inversion,seg_113,"exercise 4.4 use the table of chfs above to show in what sense the sums of independent binomial, poisson, negbit, normal, cauchy, chisquare and gamma rvs have distributions that again belong to the same family. (recall section a.2, noting that chfs have allowed the complicated operation of convolution of dfs or densities to be replaced by the simple operation of multiplication of chfs.)"
1580,1,"['continuous', 'interval']", The Continuity Theorem,seg_115,"theorem 5.1 (continuity theorem for chfs; cramér–lévy) (i) if φn → φ where φ is continuous at 0, then φ is the chf of a bona fide df f and fn →d f. (ii) fn →d f implies φn → φ uniformly on any finite interval |t| ≤ t."
1581,1,['tails'], The Continuity Theorem,seg_115,inequality 5.1 (chf bound on the tails of a df) for any df f we have
1582,1,['inequality'], The Continuity Theorem,seg_115,as claimed. (it may be interesting to compare this to the chebyshev inequality.) [this idea will be carried further in (10.5.9) and (10.5.10).]
1583,1,"['functions', 'exponential', 'expectations']", The Continuity Theorem,seg_115,"proof. consider theorem 5.1. (i) the uniqueness theorem for chfs shows that the collection g of complex exponential functions form a determining class, and the expectations of these are hypothesized to converge. it thus suffices (by the kinder and gentler helly–bray theorem (theorem 9.2.1(i)(a))) to show that {fn : n ≥ 1} is tight. now,"
1584,1,['function'], The Continuity Theorem,seg_115,"by the dct, with dominating function 2"
1585,1,['function'], The Continuity Theorem,seg_115,"by the dct, with dominating function 2."
1586,1,"['function', 'characteristic function']", The Continuity Theorem,seg_115,"if x1, . . . , xk are rvs on (ω,a, p ), then the bk-a-mapping x ≡ (x1, . . . , xk)′ from ω to rk induces a measure px on (rk, bk). the characteristic function of x is"
1587,1,"['distribution', 'convergence']", The Continuity Theorem,seg_115,"without further explanation, we state simply that the uniqueness theorem (that {gt ≡ exp(it′x) for all x ∈ rn : t ∈ rn} is a determining class) and the cramér-lévy continuity theorem still hold, based on minor modifications of the previous proof. we also remark that all equivalences of →d in theorem 1.1 are still valid. but we now take up an extremely useful approach to showing convergence in distribution in higher dimensions."
1588,1,"['linear', 'characteristic function', 'linear combination', 'combination', 'function']", The Continuity Theorem,seg_115,the characteristic function of the one-dimensional linear combination λ′x is
1589,1,['joint'], The Continuity Theorem,seg_115,comparison of this with (2) shows that knowing the joint chf φx(t) for all t ∈ rk is equivalent
1590,0,[], The Continuity Theorem,seg_115,′x(t) for all t ∈ r and λ ∈ rk for which |λ| = 1. this immediately yields the following useful result.
1591,1,['method'], The Continuity Theorem,seg_115,"that λ′xn →d λ′x for all such λ (no matter what method we use to show it), as such a result implies (4).]"
1592,1,"['joint', 'independent']", The Continuity Theorem,seg_115,"theorem 5.3 the rvs x1, . . . , xk are independent if and only if the joint chfs satisfy φx(t1, . . . , tk) = ∏k"
1593,0,[], The Continuity Theorem,seg_115,exercise 5.1∗ prove the claims made below (2) for the n-dimensional chf φx.
1594,1,"['function', 'taylor series']", Elementary Complex and Fourier Analysis,seg_117,"lemma 6.1 (taylor expansions of log(1 + z) and ez) [note that log z is a many-valued function of a complex z = reiθ; any of (log r)+ i[θ +2πm] for m = 0,±1,±2, . . . will work for log z. however, when we write log z = log r + iθ, we will always suppose that −π < θ ≤ π. moreover, we denote this unique determination by log z; this is the principal branch.] the taylor series expansion of log (1 + z) gives"
1595,1,['taylor series'], Elementary Complex and Fourier Analysis,seg_117,from another taylor series expansion we have for all z that
1596,1,['set'], Elementary Complex and Fourier Analysis,seg_117,"lemma 6.2 (taylor expansion of eit) let m ≥ 0 and 0 ≤ δ ≤ 1 (and set the constant k0,0 = 2, below). then for all real t we have"
1597,0,[], Elementary Complex and Fourier Analysis,seg_117,t eis ds and further note that
1598,1,['inequality'], Elementary Complex and Fourier Analysis,seg_117,[see chow and teicher (1997).] (the next inequality is immediate.)
1599,1,"['inequality', 'moment']", Elementary Complex and Fourier Analysis,seg_117,inequality 6.1 (moment expansion inequality) suppose e|x|m+δ < ∞ for some m ≥ 0 and 0 ≤ δ ≤ 1. then
1600,1,[], Elementary Complex and Fourier Analysis,seg_117,"lemma 6.3 (the first product lemma) for all n ≥ 1, let complex βn1, . . . , βnn satisfy the following conditions:"
1601,0,[], Elementary Complex and Fourier Analysis,seg_117,"then (compare this with the stronger lemma 8.1.4, which requires all βnk ≥ 0)"
1602,0,[], Elementary Complex and Fourier Analysis,seg_117,"moreover, (q) shows that"
1603,0,[], Elementary Complex and Fourier Analysis,seg_117,"lemma 6.4 (the second product lemma) if z1, . . . , zn and w1, . . . , wn denote complex numbers with modulus at most 1, then"
1604,0,['n'], Elementary Complex and Fourier Analysis,seg_117,"proof. this is trivial for n = 1. we will use induction. now,"
1605,0,[], Elementary Complex and Fourier Analysis,seg_117,by the induction step. [see most newer texts.]
1606,1,['moment'], Elementary Complex and Fourier Analysis,seg_117,inequality 6.2 (moment expansions of chfs) suppose 0 < e|x|m < ∞ for some m ≥ 0. then (for some 0 ≤ g(t) ≤ 1) the chf φ of x satisfies
1607,0,[], Elementary Complex and Fourier Analysis,seg_117,proof. use the real expansions for sin and cos to obtain
1608,1,['function'], Elementary Complex and Fourier Analysis,seg_117,by the dct with dominating function 3|x|m. [see breiman (1968).]
1609,0,[], Elementary Complex and Fourier Analysis,seg_117,"inequality 6.3 (summary of useful facts) let x =∼ (0, σ2). result (8) then gives the highly useful"
1610,1,"['continuous', 'moments', 'function', 'distributions']", Elementary Complex and Fourier Analysis,seg_117,"exercise 6.1 (distributions determined by their moments) (a) suppose that e|x|n < ∞. then the nth derivative φ(n)(·) is a continuous function given by φ(n)(t) = ine(xneitx), so that exn = i−nφ(n)(0). (b) the series ψ(t) = ∑0"
1611,1,['convergence'], Elementary Complex and Fourier Analysis,seg_117,"∞(it )ke(xk)/k! has radius of convergence r ≡ 1/(el), where"
1612,1,['test'], Elementary Complex and Fourier Analysis,seg_117,/2k/(2k); use the root test.
1613,1,[], Elementary Complex and Fourier Analysis,seg_117,(c) the series in (b) has the same r > 0 if ∑k
1614,1,"['distribution', 'moments', 'convergence']", Elementary Complex and Fourier Analysis,seg_117,"∞ =0 μ2k t2k/(2k)! < ∞ for some t > 0. (d) the series (b) converges for |t| < r if and only if e exp(t|x|) < ∞ for |t| < r if and only if e exp(tx) < ∞ for |t| < r (e) if the radius of convergence in (b) is strictly positive, then the distribution having the stated moments is uniquely determined by its moments μk. (f) show that the normal(0, 1) distribution is uniquely determined by its moments. (g) show that any gamma(r, 1) distribution is uniquely determined by its moments. (h) show that it is valid to expand the mgs of normal(0, 1) and gamma(r, 1) to compute their moments. do it."
1615,0,[], Elementary Complex and Fourier Analysis,seg_117,results from fourier analysis
1616,0,[], Elementary Complex and Fourier Analysis,seg_117,on some occasions we will need to know the behavior of φ(t) for |t| large.
1617,0,[], Elementary Complex and Fourier Analysis,seg_117,"it thus suffices to show that for any a, b in r we have"
1618,1,['interval'], Elementary Complex and Fourier Analysis,seg_117,a quick picture of sines and cosines oscillating very fast (and canceling out over the interval) shows that (12) is trivial. (or write eitx = cos(tx) + i sin(tx) and compute the integrals.)
1619,1,['tail'], Elementary Complex and Fourier Analysis,seg_117,lemma 6.6 (tail behavior of chfs)
1620,1,['continuous'], Elementary Complex and Fourier Analysis,seg_117,"proof. the fact that |φ(t)| → 0 as |t| → ∞ follows from the reimann–lebesgue lemma, since f is integrable. since f is absolutely continuous and a density, it follows that that f(x) → 0 as |x| → ∞. then use"
1621,1,"['functions', 'probability']", Elementary Complex and Fourier Analysis,seg_117,"then chf always exists, so it can always be used. however, if x ≥ 0 or if x is integer valued, then laplace transforms or probability generating functions offer more elementary tools."
1622,1,['transform'], Elementary Complex and Fourier Analysis,seg_117,exercise 6.5∗ (laplace transform) let f+ denote the class of all dfs f having f−(0) = 0. for any df f ∈ f+ we define the laplace transform l of f by
1623,1,"['moment', 'distribution', 'continuous', 'convergence', 'transform', 'inequality']", Elementary Complex and Fourier Analysis,seg_117,"(a) establish an analogue of proposition 3.1(a), (b), (c), and (g). (b) (uniqueness) show that each df in f+ has a unique laplace transform. (c) (continuity) let xn =∼ fn ∈ f+. if ln(λ) → (some l(λ)) for all λ ≥ 0 with l(·) right continuous at 0, then l is the laplace transform of a df f ∈ f+ for which the convergence in distribution fn →d f holds. (d) establish analogues of inequality 6.1 on moment expansions."
1624,1,"['function', 'probability']", Elementary Complex and Fourier Analysis,seg_117,"exercise 6.6∗ (probability generating function) let fi denote the class of all dfs f assigning mass 1 to the integers 0, 1, 2, . . .. for any df f ∈ fi we define the probability generating function g of f by"
1625,1,"['function', 'continuous']", Elementary Complex and Fourier Analysis,seg_117,"(a) establish an analogue of proposition 3.1. (b) (uniqueness) show that each df f in fi has a unique generating function. (c) (continuity) let xn ∼= fn ∈ fi . if gn(z) → (some g(z)) for all |z| ≤ 1 with g(·) continuous at 1, then g is the generating function of a df f in fi for which fn →d f."
1626,1,"['function', 'cumulant']", Elementary Complex and Fourier Analysis,seg_117,exercise 6.7o (cumulant generating function) the cumulant generating function ψx(·) of a rv x is defined by
1627,1,"['moments', 'moment']", Elementary Complex and Fourier Analysis,seg_117,"and is necessarily finite for t-values in some neighborhood of the origin. (a) temporarily suppose that all moments of x are finite. let μk ≡ e(x − μ)k denote the k -th central moment, for k ≥ 1. then when μ = ex = 0 and with σ2 ≡ μ2, we have the formal expansion"
1628,0,[], Elementary Complex and Fourier Analysis,seg_117,verify that further formal calculations based on this yield
1629,1,"['cumulant', 'independent']", Elementary Complex and Fourier Analysis,seg_117,"where κj is called the jth cumulant of x. note that for independent rvs x1, . . . , xn,"
1630,1,['cumulant'], Elementary Complex and Fourier Analysis,seg_117,(19) (the jth cumulant of ∑n
1631,1,['cumulant'], Elementary Complex and Fourier Analysis,seg_117,"k=1 (the jth cumulant of xk),"
1632,1,"['cumulants', 'case']", Elementary Complex and Fourier Analysis,seg_117,"which is nice. then verify that in the iid case, the third and fourth cumulants of the standardized rv zn ≡ √n(x̄n − μ)/σ are"
1633,1,"['rate', 'distribution', 'skewness', 'tail']", Elementary Complex and Fourier Analysis,seg_117,"where γ1 measures skewness and γ2 measures tail heaviness. [this is particularly nice; it shows that the effect (on the distribution of x̄n) of skewness disappears at rate 1/√n, while the effect of tail heaviness disappears at rate 1/n.] (b) finally, if only e|x|m < ∞ for some m ≥ 1 is known, show that in a sufficiently small neighborhood of the origin"
1634,0,[], Elementary Complex and Fourier Analysis,seg_117,for some universal constant c̄m. the exercise is to establish carefully that all of this is true.
1635,1,['function'], Esseens Lemma,seg_119,"let g denote a fixed function having g(−∞) = 0, g(+∞) = 1, having derivative g on the"
1636,1,"['characteristic function', 'estimate', 'mean', 'function', 'inequality']", Esseens Lemma,seg_119,"∞ ∞ eitxg(x) dx. let f denote a general df having mean 0, and let φ denote its characteristic function. we wish to estimate ‖f − g‖ = sup−∞<x<∞ |f (x) − g(x)| in terms of the distance between φ and ψ. roughly speaking, the next inequality says that if φ and ψ are sufficiently close over most of their domain, then ‖f − g‖ will be small. [in the initial application of this esseen’s lemma, we will take g, g, ψ to be the n(0, 1)df, density, and chf. in this context, the constant of (1) is 24‖g‖/π = 24m/π = 24/(√2ππ) = 3.047695 . . . .]"
1637,0,[], Esseens Lemma,seg_119,inequality 7.1 (esseen’s lemma) let f and g be as above. for any a > 0 we have the uniform bound
1638,1,"['function', 'characteristic function']", Esseens Lemma,seg_119,proof. the key to the technique is to smooth by convolving f and g with the df ha whose density ha and characteristic function γa are given by
1639,1,[], Esseens Lemma,seg_119,"this ha is the density of v/a, when v has the de la vallée density. let fa and ga denote the convolutions of f and g with ha, for “a” large. we will now show that"
1640,1,['loss'], Esseens Lemma,seg_119,"let δ ≡ f − g. now, δ(x) = δ+(x) and δ−(x) exist for all x; thus there exists xo such that either d ≡ ‖f − g‖ = |δ(xo)| or d = |δ−(xo)|. without loss of generality, we suppose that d = |δ(xo)|(just replace x,y by − x,−y if not). note figure 7.1. without loss of generality, we act below as though δ(xo) > 0, and we let zo > xo. (if δ(xo) < 0, then let zo < xo). now, since f is ↗ and g is bounded by m , we have"
1641,0,[], Esseens Lemma,seg_119,"where ≡ d/2m and zo ≡ xo + . trivially (since d was the supremum),"
1642,1,['convolution'], Esseens Lemma,seg_119,∞ ∞ δ(zo − x)ha(x) dx by the convolution formula
1643,0,[], Esseens Lemma,seg_119,figure 7.1 bounds for esseen’s lemma
1644,1,['continuous'], Esseens Lemma,seg_119,"we now bound ‖fa − ga‖. by the fourier inversion formula, fa and ga have bounded continuous “densities” that satisfy"
1645,0,[], Esseens Lemma,seg_119,from this we suspect that
1646,1,"['function', 'inequality', 'continuous']", Esseens Lemma,seg_119,"that the integrand is a continuous function that equals 0 at t = 0 (since f and g have 0 “means,” inequality 6.1 gives this) makes the right-hand side well-defined, and we may differentiate under the integral sign by the dct [with dominating function γa(·)] to get the previous equation (4). thus δa(x) can differ from the right-hand side of (5) by at most a constant; but this constant is 0, since obviously δa(x) → 0 as |x| → ∞, while the right-hand side does the same by the riemann–lebesgue lemma. equation (5) gives"
1647,0,[], Esseens Lemma,seg_119,"corollary 1 (stein) suppose that instead of convolving f and g with the ha of (2), we convolve with an arbitrary df h instead. in this situation we obtain"
1648,0,[], Esseens Lemma,seg_119,"∼ proof. picking up at line (d) of the previous proof (with y = h), we obtain"
1649,1,['inequality'], Esseens Lemma,seg_119,using markov’s inequality and ≡ d/2m in the last step.
1650,1,['probabilities'], Distributions on Grids,seg_121,"definition 8.1 we say that a rv x is distributed on a grid if there exist real numbers a, d such that the probabilities pn ≡ p (x = a + nd) satisfy ∑−"
1651,0,[], Distributions on Grids,seg_121,"proposition 8.1 if t0 = 0, the following are equivalent:"
1652,1,[], Distributions on Grids,seg_121,"since the integrand is nonnegative for all x, this means that"
1653,0,[], Distributions on Grids,seg_121,suppose that (b) holds. then
1654,0,[], Distributions on Grids,seg_121,proposition 8.2 one of the following possibilities must hold:
1655,1,"['consequences', 'case', 'continuous']", Distributions on Grids,seg_121,"proof. clearly, either (d), (e), or (f) holds, or else |φ(tn)| = 1 for some sequence tn → 0. in this latter case, |φ(mtn)| = 1 for all m, for each n by proposition 8.1. since {mtn : n ≥ 1,m = 0,±1,±2, . . .} is dense in r and since φ, and thus |φ|, is continuous, we must have case (f) again. it remains to establish the consequences of (e) and (f)."
1656,0,['e'], Distributions on Grids,seg_121,consider (e). proposition 8.1 shows that (e) holds if and only if both d is a span and no number exceeding d is a span.
1657,1,['case'], Distributions on Grids,seg_121,"in the case of (f), we have |φ(t1)| = 1 = |φ(t2)| for some t1 and t2 having t1/t2 = (an irrational number). but |φ(t1)| = 1 and |φ(t2)| = 1 imply that both 2π/t1 and 2π/t2 are spans. thus if at least two points have positive mass, then the distance between them must equal m12π/t1 for some integer m1 and it must equal m22π/t2 for some integer m2. that is, 2πm1/t1 = 2πm2/t2, or t1/t2 = m1/m2 = (a rational number). this contradiction shows that there can be at most one mass point a."
1658,1,['distributions'], Distributions on Grids,seg_121,exercise 8.1 (inversion formula for distributions on a grid) let x be distributed on a grid with pn = p (x = a + dn). then φ(t) = ∑∞
1659,1,"['function', 'density function']", Conditions for φ to Be a Characteristic Function,seg_123,is a de la vallée poussin density function with chf
1660,0,[], Conditions for φ to Be a Characteristic Function,seg_123,let fa denote the df. then
1661,1,"['function', 'characteristic function']", Conditions for φ to Be a Characteristic Function,seg_123,is a df with characteristic function
1662,1,['function'], Conditions for φ to Be a Characteristic Function,seg_123,"thus any even function φ ≥ 0 with φ(0) = 1 whose graph on [0,∞) is a convex polygon is a chf."
1663,1,['function'], Conditions for φ to Be a Characteristic Function,seg_123,"proposition 9.1 (pólya) let φ ≥ 0 be an even function with φ(0) = 1 whose graph on [0,∞) is convex and ↓. then φ is a chf."
1664,1,['limit'], Conditions for φ to Be a Characteristic Function,seg_123,"proof. pass to the limit in the obvious picture, using the continuity theorem to complete the proof."
1665,1,"['function', 'standard']", Conditions for φ to Be a Characteristic Function,seg_123,"bochner’s theorem below gives necessary and sufficient conditions for a function to be a chf. we merely state it, as a background fact. its proof can be found in a number of the standard texts."
1666,1,"['function', 'set']", Conditions for φ to Be a Characteristic Function,seg_123,definition 9.1 a complex-valued function φ(·) on r is nonnegative definite if for any finite set t and any complex-valued function h(·) we have
1667,1,"['function', 'continuous']", Conditions for φ to Be a Characteristic Function,seg_123,theorem 9.1 (bochner) a complex-valued function φ(·) is a chf if and only if it is nonnegative definite and continuous.
1668,1,['states'], Introduction,seg_127,"the classical clt states that if x1,x2, . . . are iid (μ, σ2), then"
1669,0,[], Introduction,seg_127,this chapter will also consider the following generalizations.
1670,0,[], Introduction,seg_127,lindeberg–feller (and the optional general clt theorem 10.5.1).
1671,1,"['convergence', 'rate']", Introduction,seg_127,"(ii) the rate of convergence of such dfs to the limiting df, via berry–esseen."
1672,0,[], Introduction,seg_127,(iii) the multidimensional clt.
1673,1,"['sample', 'random', 'quantiles', 'random sample']", Introduction,seg_127,"(iv) random sample sizes, sample quantiles, and many other examples."
1674,1,"['poisson', 'limit']", Introduction,seg_127,(v) non-normal limits (both the degenerate wlln and a poisson limit).
1675,1,"['functions', 'density functions', 'convergence']", Introduction,seg_127,(vi) convergence of the density functions as well (the local clt 10.4.1).
1676,1,[], Introduction,seg_127,(vii) necessary and sufficient conditions for the optional statistically formulated clt 10.5.2
1677,1,['independent'], Introduction,seg_127,for triangular arrays of row independent rvs.
1678,1,"['bootstrap', 'results', 'distribution', 'samples', 'normal', 'normal distribution']", Introduction,seg_127,possible results for iid rvs and for bootstrap samples. theorem 10.6.1 develops the domain of attraction d(normal) of the normal distribution.
1679,1,"['trimmed means', 'asymptotic', 'results', 'sampling', 'normality', 'inequality']", Introduction,seg_127,"in chapter 11 we consider situations that lead to stable and infinitely divisible rvs as limits. edgeworth and other approximations are also considered there. section 13.9 includes a discussion of martingale clts. chapter 15 has sections on asymptotic normality of trimmed means, of l-statistics, and of r-statistics (the latter includes a finite sampling clt). the proofs of some optional results above require knowledge of sections c.1–c.4. the chapter 15 examples require an inequality in sections c.6."
1680,1,"['poisson', 'contrast', 'random variables', 'variables', 'central limit theorems', 'central limit theorem', 'random', 'limit']", Basic Limit Theorems,seg_129,the goal of this section is to use a chf approach to present the classical central limit theorems for sums of iid random variables in r and in rk. we also compare and contrast the central limit theorem with the poisson limit theorem.
1681,1,"['average', 'mean', 'variance']", Basic Limit Theorems,seg_129,"theorem 1.1 (classical clt) for each n ≥ 1, let xn1, . . . , xnn be iid f (μ, σ2); this denotes that the df f (.) of the xnk’s has mean μ and finite variance σ2. define the total tn ≡ xn1 + · · · + xnn and the average x̄n ≡ tn/n. then as n → ∞,"
1682,1,['inequality'], Basic Limit Theorems,seg_129,σ2g(√n)]n by inequality 9.6.2
1683,0,[], Basic Limit Theorems,seg_129,the first product lemma 9.6.3 with θ = −σ2t2/2 trivially applies. thus
1684,1,['table'], Basic Limit Theorems,seg_129,"using table 9.1.1. thus √n(x̄n − μ) →d n(0, σ2) by the cramér–lévy continuity theorem 9.5.1 and the uniqueness theorem 9.4.1."
1685,0,[], Basic Limit Theorems,seg_129,"had we chosen to appeal to the second product lemma 9.6.4 instead, we would have instead claimed that"
1686,1,[], Basic Limit Theorems,seg_129,degenerate limits
1687,1,"['convergence', 'mean']", Basic Limit Theorems,seg_129,"exercise 1.1 (wlln, or classical degenerate convergence theorem) for each n ≥ 1, let xn1, . . . , xnn be iid with finite mean μ. use chfs to show the wlln result that x̄n →p μ as n → ∞. equivalently,"
1688,1,['distribution'], Basic Limit Theorems,seg_129,(2) x̄n →d (the degenerate distribution with mass 1 at μ).
1689,1,"['poisson', 'independent', 'parameters', 'bernoulli', 'limit']", Basic Limit Theorems,seg_129,"theorem 1.2 (classical poisson limit theorem; the plt) for each n ≥ 1, suppose that xn1, . . . , xnn are independent bernoulli (λnk) rvs for which the values of the parameters"
1690,1,[], Basic Limit Theorems,seg_129,(3) tn ≡ xn1 + · · · + xnn →d poisson(λ) as n → ∞.
1691,1,['table'], Basic Limit Theorems,seg_129,proof. from table 9.1.1 we have φxnk(t) = 1 + λnk(eit − 1). thus
1692,1,['table'], Basic Limit Theorems,seg_129,(c) = φpoisson(λ)(t) by table 13.1.1.
1693,0,[], Basic Limit Theorems,seg_129,now apply the cramér–lévy continuity theorem and the uniqueness theorem.
1694,1,"['poisson', 'limit']", Basic Limit Theorems,seg_129,exercise 1.2 (poisson local limit theorem) show that
1695,1,[], Basic Limit Theorems,seg_129,"(4) p (tn = k) → p ( poisson(λ) = k) as n → ∞, for k = 0, 1, . . . ,"
1696,1,[], Basic Limit Theorems,seg_129,where tn ∼= pn and poisson(λ) ∼= p . [exercise 12.5.4 will improve this.]
1697,1,['parameter'], Basic Limit Theorems,seg_129,"exercise 1.3 show that if tλ ∼= poisson(λ), then (tλ−λ)/√λ →d n(0, 1) as the parameter λ → ∞."
1698,1,"['poisson', 'convergence', 'normal']", Basic Limit Theorems,seg_129,a comparison of normal and poisson convergence
1699,1,['hypotheses'], Basic Limit Theorems,seg_129,exercise 1.4 (a) suppose the hypotheses of the classical clt hold. show that
1700,1,['hypotheses'], Basic Limit Theorems,seg_129,(b) suppose the hypotheses of the classical plt hold. show that
1701,1,[], Basic Limit Theorems,seg_129,≤ka≤xn |xnk|] →d bernoulli(1 − e−λ).
1702,1,['cases'], Basic Limit Theorems,seg_129,"(∗) there is something fundamentally different regarding the negligibility of the corresponding terms in these two cases! the clt involves summing many tiny pieces, but the plt arises from very occasionally having a “large” piece."
1703,1,['independent'], Basic Limit Theorems,seg_129,"remark 1.1 let yn1, . . . , ynn be independent. let pnk ≡ p (|ynk| > ). recall equation (8.3.14) for the conclusion"
1704,1,['inequality'], Basic Limit Theorems,seg_129,this was proved via the (8.3.13) inequality
1705,1,['random'], Basic Limit Theorems,seg_129,"theorem 1.3 (classical multivariate clt) let xn ≡ (xn1, . . . , xnk)′, n ≥ 1, be a sequence of iid (μ,σ) random vectors. then"
1706,0,[], Basic Limit Theorems,seg_129,thus the classic clt gives
1707,1,"['set', 'process']", Basic Limit Theorems,seg_129,"exercise 1.5 (empirical process; doob) let un ≡ √n[gn − i] be the uniform empirical process of sections 6.5 and 12.10, and let u denote the brownian bridge of (a.4.13). show that un →fd u as n → ∞; that is, show that for any set of points 0 < t1 < · · · < tk < 1 we have"
1708,1,['results'], Basic Limit Theorems,seg_129,"(essentially, all results in chapter 12 derive from this example—via a suggestion of doob (1949).)"
1709,1,"['set', 'transform', 'random', 'process']", Basic Limit Theorems,seg_129,"exercise 1.6 (partial sum process of iid rvs) let sn denote the partial sum process of iid (0, 1) rvs (see (11) below) and let s denote brownian motion (as in (a.4.12)) show that sn →fd s as n → ∞. [hint. set things up cumulating from the left, and then transform. or note that the random element you must consider can be written in a form equivalent to something simpler. or use the cramér–wold device. one of these methods is much simpler then the others.]"
1710,1,"['process', 'independent']", Basic Limit Theorems,seg_129,"exercise 1.7 (partial sum process) suppose that xn1, . . . , xnn are independent (0, σn"
1711,1,['condition'], Basic Limit Theorems,seg_129,"and satisfy lindeberg’s condition (10.2.11) below. define sn on [0, 1] via"
1712,0,[], Basic Limit Theorems,seg_129,"2i and s2n0 ≡ 0. show that sn →fd s, where s denotes brownian motion. (only attempt this problem following theorem 10.2.2.)"
1713,1,"['goodness of fit', 'statistic']", Basic Limit Theorems,seg_129,example 1.1 (chisquare goodness of fit statistic) suppose ω = ∑i
1714,1,"['covariance', 'mean', 'covariance matrix']", Basic Limit Theorems,seg_129,"(a) now, (z1j , . . . , zkj)′, with zij ≡ (1ai(xj) − pi)/√pi, has mean vector o and covariance matrix σ = |[σii′ ]| with σii = 1 − pi and σii′ = −√pipi"
1715,1,"['goodness of fit', 'statistic']", Basic Limit Theorems,seg_129,"1 zj/√n →d w =∼ nk(o,σ) as n → ∞, by theorem 1.3. (c) the usual chisquare goodness of fit statistic is"
1716,0,[], Basic Limit Theorems,seg_129,here g is k × k and orthogonal with first row √p
1717,1,"['statistic', 'estimator']", Basic Limit Theorems,seg_129,"i − (1, 0, . . . , 0)′(1, 0, . . . , 0). this has diagonal elements (0, 1, 1, . . . , 1) with all off-diagonal elements 0, and then gw =∼ n(o, gσg′) (by (7.3.5) and (a.3.6)). we also use (a.1.29) for (16). [if a value of expected is unknown, it should be replaced by an appropriate estimator êxpected.] (see exercise 10.3.26 below.) (this statistic is just a quadratic form.)"
1718,1,['table'], Basic Limit Theorems,seg_129,exercise 1.8* [independence in an i × j table] suppose both ω = ∑i
1719,1,['partitions'], Basic Limit Theorems,seg_129,"j=1 bj represent partitions of ω. (a) let pij ≡ p (aibj) = pi.p.j , where pi. ≡ p (ai) and p.j ≡ p (bj). let"
1720,1,['observations'], Basic Limit Theorems,seg_129,"nij ≡ (the number of iid observations x1, . . . , xn that fall in aibj)."
1721,1,['observations'], Basic Limit Theorems,seg_129,of iid p (·|bj) observations x1
1722,1,"['statistic', 'independent']", Basic Limit Theorems,seg_129,") that fall in aibj). let p̂i|j ≡ ∑j j=1 nij/n·j . show that when σ[a1, . . . , ai ] and σ[b1, . . . , bj ] are independent, the chisquare statistic satisfies"
1723,1,"['marginal', 'without replacement', 'replacement', 'sets', 'random']", Basic Limit Theorems,seg_129,"(∗) suppose that both sets of marginal totals n1., . . . , ni . and n.1, . . . , n.j are fixed, and that both sum to n. suppose that n balls are assigned to the ij cells at random without replacement, subject to the side conditions on the marginal totals stated above. let nij denote the number assigned to the (i, j)-th cell. it holds that"
1724,1,"['data', 'independence', 'statistic', 'level', 'test']", Basic Limit Theorems,seg_129,"as (n1. ∧ · · · ∧ ni .) ∧ (n.1 ∧ · · · ∧ n·j) → ∞. [suppose i = 5 different social groups are at work in broadcasting, where the sum of the i group sizes ni. of our data is n = 250. the number whose salaries fall in each decile (thus j = 10) of the observed salaries is necessarily n.j = n/j = 25. the statistic in (17) can be used to test for independence of group and salary level.]"
1725,1,['distributions'], Basic Limit Theorems,seg_129,limiting distributions of extremes
1726,1,"['sample', 'asymptotic', 'distribution', 'joint']", Basic Limit Theorems,seg_129,"exercise 1.9 (a) let ξn1, . . . , ξnn be iid uniform(0, 1) rvs. then the sample minimum ξn:n satisfies nξn:n → exp(1). (b) now, ξn:n is the sample maximum. determine the joint asymptotic distribution of nξn:1 and n(1 − ξn:n)."
1727,1,"['cases', 'sample']", Basic Limit Theorems,seg_129,"exercise 1.10 (special cases of gnedenko’s theorem) let xn:n be the maximum of an iid sample x1, . . . , xn from f (·)· then:"
1728,1,['results'], Basic Limit Theorems,seg_129,"[distributions that are “suitably similar” to these prototypes yield the same limiting results, and these limits are the only possible limits.]"
1729,1,['variances'], Variations on the Classical CLT,seg_131,"notation 2.1 let xnk, 1 ≤ k ≤ n for each n ≥ 1, be row-independent rvs having means μnk and variances σn"
1730,1,['moments'], Variations on the Classical CLT,seg_131,"2k, and let γnk ≡ e|xnk − μnk|3 < ∞ denote the third absolute central moments. let"
1731,1,"['rate', 'standardized', 'convergence']", Variations on the Classical CLT,seg_131,"theorem 2.1 (rate of convergence in the clt) consider the rvs above. the df fzn of the standardized zn is uniformly close to the n(0, 1) df φ, in that"
1732,0,[], Variations on the Classical CLT,seg_131,corollary 1 (liapunov clt)
1733,1,"['rate', 'loss', 'normality', 'convergence']", Variations on the Classical CLT,seg_131,"proof. here we first give a delicate proof of the rate of convergence to normality in (3) based on esseen’s lemma (with (4) shown to be a corollary to this proof). [a rather simple proof of (4) is asked for in exercise 2.4 below.] without loss, we assume that all μnk = 0. now, let a ≡ sd3n/γn; and assume throughout that a ≥ 9 (note that (3) is meaningless unless a > 13). (recall that a = b ⊕ c means |a − b| ≤ c.) note that"
1734,1,['inequality'], Variations on the Classical CLT,seg_131,using the liapunov inequality for (e|xnk|3/2)2 ≤ γnk. but validity of (7) required that all
1735,1,['moment'], Variations on the Classical CLT,seg_131,"consider for a moment the liapunov clt corollary 1 for any fixed t, the bound on |z| in (c) goes to 0 whenever 1/a = γn/sdn3 → 0. moreover, (e) always holds when γn/sdn3 → 0,"
1736,0,[], Variations on the Classical CLT,seg_131,we now turn back to theorem 2.1 itself. since the bound of (c) gives
1737,1,['range'], Variations on the Classical CLT,seg_131,(having the bound in (g) only over the range |t| ≤ a1/3 is not sufficient for what is too come; we extend it in the next paragraph.)
1738,1,"['variance', 'mean', 'moment']", Variations on the Classical CLT,seg_131,"s ≡ zn − zn ′ (and this rv has mean 0, variance 2, and third absolute moment bounded above by 8γn/sdn3 (via the cr-inequality)). thus"
1739,0,[], Variations on the Classical CLT,seg_131,as was desired. this leads to
1740,1,['inequality'], Variations on the Classical CLT,seg_131,"key chf inequality combining (6), (e), (f) and (h) gives (provided a ≥ 9)"
1741,1,"['distribution', 'normal', 'variance', 'normal distribution']", Variations on the Classical CLT,seg_131,"we next apply esseen’s lemma to (8) and get (3). since we know the variance of a normal distribution,"
1742,1,['case'], Variations on the Classical CLT,seg_131,"in the iid case use k1,1 = 2"
1743,1,['case'], Variations on the Classical CLT,seg_131,with (e) necessarily valid. thus (8) can be replaced in the iid case by
1744,0,[], Variations on the Classical CLT,seg_131,this yields 8γ/√nσ3 when the steps leading to (l) are repeated.
1745,1,['independent'], Variations on the Classical CLT,seg_131,"theorem 2.2 (lindeberg–feller) let xn1, . . . , xnn be row independent, with xnk ∼= (μnk, σn"
1746,0,[], Variations on the Classical CLT,seg_131,2k. the following statements are equivalent:
1747,1,"['inequality', 'sufficiency', 'moment']", Variations on the Classical CLT,seg_131,"proof. (lindeberg) we prove the sufficiency here, with the necessity considered in the following separate proof. we note that the moment expansion inequality (found in inequality 9.6.1) gives bounds on βnk(t), where"
1748,0,[], Variations on the Classical CLT,seg_131,defines θnk(t) and βnk(t). moreover (in preparation for the product lemma)
1749,1,['inequality'], Variations on the Classical CLT,seg_131,"the inequality of (9.6.4) (compare this with (9.6.10), and with (7)) gives"
1750,1,['normality'], Variations on the Classical CLT,seg_131,"thus normality holds, since the integral in (16) goes to 0 for all > 0 by (11)."
1751,0,[], Variations on the Classical CLT,seg_131,on the second term below to claim that
1752,1,"['interval', 'sufficiency', 'condition']", Variations on the Classical CLT,seg_131,proof. (feller) we proved sufficiency in the previous proof; we now turn to necessity. suppose that condition (10) holds. applying (9.6.2) [since the easy exercise 10.2.9 below applied to our uan rvs shows that the terms znk = φnk(t) − 1 converge uniformly to 0 on any finite interval] gives
1753,0,[], Variations on the Classical CLT,seg_131,we thus have (for any finite m)
1754,0,[], Variations on the Classical CLT,seg_131,but we also know that
1755,1,"['asymptotic', 'normality']", Variations on the Classical CLT,seg_131,since we have assumed asymptotic normality. [recall that a = b ⊕ c means that |a − b| ≤ c.] combining (b) and (c) shows that for every tiny > 0 and every huge m > 0 we have
1756,1,['condition'], Variations on the Classical CLT,seg_131,"where θ > 0 is arbitrary. thus, the lindeberg condition (11) holds."
1757,0,[], Variations on the Classical CLT,seg_131,exercise 2.1 (characterizations of “uan”) the following are equivalent:
1758,1,['interval'], Variations on the Classical CLT,seg_131,(23) [max1≤k≤n |φnk(t) − 1|] → 0 uniformly on every finite interval of t’s.
1759,1,['independent'], Variations on the Classical CLT,seg_131,exercise 2.2 phrase a simple consequence of the liapunov clt that applies to uniformly bounded row independent rvs xnk.
1760,0,[], Variations on the Classical CLT,seg_131,exercise 2.3 provide the steps leading to (9) referred to in the berry–esseen proof.
1761,1,['condition'], Variations on the Classical CLT,seg_131,"remark 2.1 (lindeberg’s condition) (i) if lindeberg’s condition fails, it may still be true that"
1762,1,"['independent', 'probabilities']", Variations on the Classical CLT,seg_131,"let y1, y2, . . . be iid (0, 1) rvs, so that √nȳn →d n(0, 1) by the clt. now let the rv’s uk be independent (0, c2) with uk equal to −ck, 0, ck with probabilities 1/(2k2), 1 − 1/k2, 1/(2k2)."
1763,1,['set'], Variations on the Classical CLT,seg_131,"rv sequence uk satisfies uk = 0 only finitely often. thus √kūk →a.s. 0 follows. for n ≥ 1 set xn ≡ yn + un, and let sn ≡ x1 + · · · + xn. note that sd2n ≡ var[sn] = (1 + c2)n. so, by slutsky’s theorem,"
1764,1,['condition'], Variations on the Classical CLT,seg_131,"so, lindeberg’s condition fails, since"
1765,0,[], Variations on the Classical CLT,seg_131,"the nonzero contribution shown in the last step is due to the uk’s, whereas we do already know that the contribution due to the yk’s is o(1). this example shows that it is possible to have xn →d x without having var[xn] → var[x]. note that var[n(0, 1)/(1 + c2)] = 1/(1 + c2) < 1 = lim 1 = lim var[sn/sdn] (via the fatou lemma and skorokhod’s theorem)."
1766,1,"['independent', 'condition']", Variations on the Classical CLT,seg_131,"∼ xnk = n(0, 1) for pn < k ≤ n for independent rvs xnk, then sn/sdn →d n(0, 1), while lindeberg’s condition fails and [max1≤k≤n σn"
1767,1,"['distribution', 'case']", Variations on the Classical CLT,seg_131,"remark 2.2 it is known that the constant 8 in (5) can be replaced by 0.7975. it is also known in the iid case with e|x|3 < ∞ that the “limiting distribution measure” d(f,φ) ≡ limn→∞ √n‖fzn −φ‖ exists, and that this measure achieves the bound supf (σ3/γ)d(f, φ) = (√10+3)/(6√2π) = 0.409. this sup is achieved by c[bernoulli(a)−a], where c = (√10−3)/2 and a = (4 − √10)/2. thus the constant 0.7975 cannot be greatly improved. many other improvements and refinements of the berry–esseen theorem are possible. the books by bhattacharya and rao (1976, pp. 110, 240) and petrov (1977) both give many. we list three as “exercises” in exercise 2.15 below."
1768,1,['condition'], Variations on the Classical CLT,seg_131,"exercise 2.5 construct an example with iid x1,x2, . . . for which the lindeberg condition holds, but for which liapunov’s (2 + δ)-condition fails for each 0 < δ ≤ 1."
1769,1,[], Variations on the Classical CLT,seg_131,"exercise 2.6 (liapunov-type wlln) let xn1, . . . , xnn, n ≥ 1, be a triangular array of row-independent rvs with 0 means. then"
1770,1,"['asymptotic', 'normality', 'condition']", Variations on the Classical CLT,seg_131,exercise 2.8 (i) show that lindeberg’s condition that all lfn → 0 implies feller’s condition (which is not strong enough to guarantee asymptotic normality) that
1771,1,"['poisson', 'probability', 'independent']", Variations on the Classical CLT,seg_131,"(ii) let xn1, . . . , xnn be row independent poisson (λ/n) rvs, with λ > 0. discuss which of lindeberg–feller, liapunov, and feller conditions holds in this context. (iii) repeat part (ii) when xn1, . . . , xnn are row independent and all have the probability density cx−3(log x)−2 on x ≥ e (for some constant c > 0). (iv) repeat part (ii) when p (xnk = ak) = p (xnk = −ak) = 1/2 for the row independent rvs. discuss this for general ak ≥ 0, and present interesting examples."
1772,1,['independent'], Variations on the Classical CLT,seg_131,"exercise 2.9 let xn1, . . . , xnn be row independent, with xnk =∼ (μnk, σn"
1773,1,['set'], Variations on the Classical CLT,seg_131,"zn1 + · · · + znn, and set μn ≡ ∑n"
1774,0,[], Variations on the Classical CLT,seg_131,2k = var[tn]. the following
1775,0,[], Variations on the Classical CLT,seg_131,statements are equivalent:
1776,1,['method'], Variations on the Classical CLT,seg_131,(example 11.1.2 below treats the current exercise by a different method.)
1777,1,['independent'], Variations on the Classical CLT,seg_131,"exercise 2.10 complete the proof of theorem 8.8.1 regarding the equivalence of →d,→p, and →a.s. for sums of independent rvs."
1778,0,[], Variations on the Classical CLT,seg_131,exercise 2.11 formulate a wlln in the spirit of the lindeberg–feller theorem.
1779,1,['moments'], Variations on the Classical CLT,seg_131,"exercise 2.12 establish the (2 + δ)-analogue of theorem 2.1. [hint. use both 2 + δ and 1 + δ/2 moments in line (b) of the theorem 2.1 proof, via lemma 9.6.2.]"
1780,1,"['variance', 'limit', 'variances']", Variations on the Classical CLT,seg_131,"exercise 2.13 construct a second example satisfying the key property of remark 2.1 (i), that the limiting variance is not the limit of the variances."
1781,1,"['moment', 'deviations', 'function']", Variations on the Classical CLT,seg_131,"exercise 2.14 (large deviations; cramér)) let xn1, . . . , xnn be iid f . suppose x ∼= f has a moment generation function m(t) ≡ e etx that is finite in some neighborhood of the"
1782,0,[], Variations on the Classical CLT,seg_131,(this exercise is repeated again later as exercise 11.6.6.)
1783,1,['independent'], Variations on the Classical CLT,seg_131,"exercise 2.15* (a) (petrov) suppose xn1, . . . xnk are row independent rvs for which xnk ∼ (0, σn"
1784,1,['set'], Variations on the Classical CLT,seg_131,"2k), and set σn 2 ≡ ∑k"
1785,0,[], Variations on the Classical CLT,seg_131,then for some absolute constant c we have the very nice result
1786,1,['case'], Variations on the Classical CLT,seg_131,(c) (nagaev) bounds on |fn(x)−φ(x)| that decrease as |x| → ∞ are given (in the iid case) in the expression
1787,0,[], Variations on the Classical CLT,seg_131,"exercise 2.16 beginning with (15), try to obtain the berry–esseen bound (but with a different constant) by appeal to the second product lemma."
1788,1,"['delta method', 'method']", Examples of Limiting Distributions o,seg_133,"example 3.1 (delta method) (a) suppose cn[wn − a] →d v where cn → ∞, and suppose g(·) is differentiable at a (recall (4.3.6) and (4.3.12)). then (as in the chain rule proof of calculus) immediately"
1789,1,[], Examples of Limiting Distributions o,seg_133,[recall that un =a vn means that un − vn →p 0.]
1790,0,[], Examples of Limiting Distributions o,seg_133,(b) the obvious vector version of this has the conclusion
1791,1,"['sample', 'asymptotic', 'normality', 'sample variance', 'variance']", Examples of Limiting Distributions o,seg_133,"example 3.2 (asymptotic normality of the sample variance) suppose the rvs x1, . . . , xn are iid (μ, σ2) with μ4 ≡ ex4 < ∞ and σ2 > 0. then"
1792,1,"['sample', 'variance', 'sample variance']", Examples of Limiting Distributions o,seg_133,2 ≡ ∑(xk − x̄)2 = (the sample variance). n − 1 k=1
1793,0,[], Examples of Limiting Distributions o,seg_133,"for a useful phrasing of conclusions, define"
1794,1,"['distribution', 'tail', 'kurtosis']", Examples of Limiting Distributions o,seg_133,where γ2 ≡ (μ4 − 3σ4)/σ4 ≡ (the kurtosis) measures the tail heaviness of the distribution of x. we will show that as n → ∞ both
1795,1,"['distribution', 'joint', 'case']", Examples of Limiting Distributions o,seg_133,"exercise 3.1 (a) determine the joint limiting distribution of √n(x̄n −μ) and √n(sn −σ) in the iid case, where sn"
1796,1,"['moments', 'condition']", Examples of Limiting Distributions o,seg_133,2 ≡ [∑n 1 (xk − x̄n)2/(n−1)] (consider the representation of sn in (6) as a normed sum of the rvs yk.) what condition on the moments is required for the result?
1797,1,"['coefficient of variation', 'normalized', 'asymptotic', 'case', 'distribution', 'normality', 'variation', 'coefficient']", Examples of Limiting Distributions o,seg_133,"(b) find the asymptotic distribution of the (appropriately normalized) coefficient of variation sn/x̄n in this iid case; that is, consider √n(sn/x̄n − σ/μ). obtain a useful representation by appealing to part (a). (suppose now that all xk ≥ 0.) (c) note that (6) provides a stronger conclusion than just asymptotic normality, in that it forms a superb starting point for the further asymptotic work in (a) and (b). note also (13) below."
1798,1,['moments'], Examples of Limiting Distributions o,seg_133,exercise 3.2 (moments of x̄n and sn
1799,1,"['distribution', 'delta method', 'method', 'asymptotic']", Examples of Limiting Distributions o,seg_133,"2 − μ2] →d 2μ × n(0, σ2) (by the delta method). what is the asymptotic distribution of nx̄n"
1800,1,['samples'], Examples of Limiting Distributions o,seg_133,"exercise 3.4 (two samples) if √m(sm − θ) →d n(0, 1) as m → ∞ and √n(tn − θ) →d"
1801,1,['independent'], Examples of Limiting Distributions o,seg_133,"n(0, 1) as n → ∞ for independent rvs sm and tn, then √m"
1802,1,['convolution'], Examples of Limiting Distributions o,seg_133,"as m ∧ n → ∞. [hint: suppose initially that λmn ≡ m/(m + n) → λ ∈ [0, 1]. use skorohod (or, use convolution or chfs) to extend it.] this is useful for the two-sample t-test and f -test."
1803,1,"['linear', 'statistics']", Examples of Limiting Distributions o,seg_133,exercise 3.5 (simple linear rank statistics) let tn ≡ √
1804,1,"['permutations', 'probability']", Examples of Limiting Distributions o,seg_133,"π(n)) achieves each of the n ! permutations of (1, . . . , n) with probability 1/n !. here, the ci and ai are constants. show that:"
1805,1,"['distribution', 'population', 'median']", Examples of Limiting Distributions o,seg_133,example 3.3 (the median ẍn) the population median of the distribution of a rv x’s
1806,1,"['sample median', 'sample', 'interval', 'median', 'statistics', 'order statistics']", Examples of Limiting Distributions o,seg_133,"2 ∼ xi = θ + i, for i’s that are iid f (·) with a unique median at 0.) the ordered values of the xk’s are denoted by xn:1 ≤ · · · ≤ xn:n, and are called the order statistics. the sample median ẍn is defined to be xn:m or any point in the interval [xn:m,xn:m+1] according as n equals 2m + 1 or 2m is odd or even. let xn"
1807,1,"['sample', 'interval', 'sample medians', 'medians']", Examples of Limiting Distributions o,seg_133,"l and xn r denote the left and right endpoints of the interval of possible sample medians (of course, xn"
1808,1,"['sample median', 'sample', 'median']", Examples of Limiting Distributions o,seg_133,l = xn r = xn:m+1 if n = 2m+1 is odd). let ẍn denote any sample median. (a) then
1809,1,"['distribution', 'covariance', 'normal', 'normal distribution']", Examples of Limiting Distributions o,seg_133,"(c) in fact, the limiting normal distribution is given by (z1n, z2n) → d(z1, z2), where the covariance of the limiting normal distribution is given by"
1810,1,['event'], Examples of Limiting Distributions o,seg_133,proof. by the event equality [xn
1811,1,['summation'], Examples of Limiting Distributions o,seg_133,note that all terms in the summation in an are of the same sign. then
1812,0,[], Examples of Limiting Distributions o,seg_133,thus the same argument as before gives
1813,0,[], Examples of Limiting Distributions o,seg_133,"now we squeeze the general ẍn in between, via"
1814,1,['events'], Examples of Limiting Distributions o,seg_133,where both ends converge to p (z1/f ′(0) ≤ y). this completes the proof. summary it has been demonstrated that the events (note (15))
1815,1,[], Examples of Limiting Distributions o,seg_133,(17) differ by a probabilistically negligible amount.
1816,1,['joint'], Examples of Limiting Distributions o,seg_133,"for the joint result, apply (17) and the multivariate clt to (wn, z2n)."
1817,1,"['quantile', 'asymptotic', 'quantiles', 'normality', 'joint']", Examples of Limiting Distributions o,seg_133,"exercise 3.6 (joint asymptotic normality of quantiles) for 0 < p < 1, the pth quantile xp of f is now defined as xp ≡ f−1(p). (a) show that if f has a derivative f ′(xp) > 0 at xp, then"
1818,1,"['asymptotic', 'quantiles', 'covariance matrix', 'distribution', 'normality', 'joint', 'covariance']", Examples of Limiting Distributions o,seg_133,"(b) establish joint normality for pi and pj quantiles, where the covariance matrix of the asymptotic distribution has (i, j)th entry"
1819,0,[], Examples of Limiting Distributions o,seg_133,"write out the analogue of (17), and use it."
1820,0,[], Examples of Limiting Distributions o,seg_133,exercise 3.7 what happens when you try to apply (12) to:
1821,1,"['cases', 'normal', 'case']", Examples of Limiting Distributions o,seg_133,show that √n[ẍn − θ] →d (a rv) in both cases. (in case (b) it is not normal.)
1822,0,['n'], Examples of Limiting Distributions o,seg_133,(the right side is an equality when n is odd.)
1823,1,"['normalized', 'asymptotic', 'distribution', 'hypothesis']", Examples of Limiting Distributions o,seg_133,exercise 3.9 consider (with hypothesis as weak as possible) the asymptotic distribution of (appropriately normalized forms of) both
1824,1,"['samples', 'median']", Examples of Limiting Distributions o,seg_133,"for iid samples x1, . . . , xn from a df f (μ, σ2) having median ν."
1825,1,['independent'], Examples of Limiting Distributions o,seg_133,"∼ exercise 3.10 let x1,x2, . . . be independent with xk = uniform(−k, k). then establish that sn/σn →d n(0, 1)."
1826,1,['distribution'], Examples of Limiting Distributions o,seg_133,exercise 3.11 determine the limiting distribution of
1827,1,['distribution'], Examples of Limiting Distributions o,seg_133,exercise 3.12 determine the .95-quantile of the limiting distribution of
1828,1,['independent'], Examples of Limiting Distributions o,seg_133,"∼ ∼ for independent rvs with xk = double exponential(0, 1) and uk = uniform(0, 1)."
1829,1,['set'], Examples of Limiting Distributions o,seg_133,"example 3.4 (weighted sums of iid rvs) suppose that rvs xn1, . . . , xnn are row independent and iid (μ, σ2). let cn ≡ (cn1, . . . , cnn)′ for n ≥ 1, and set"
1830,1,['condition'], Examples of Limiting Distributions o,seg_133,suppose we have the uan condition
1831,1,['mean'], Examples of Limiting Distributions o,seg_133,[we need not center the cnk’s if the xnk’s have mean 0.]
1832,1,"['loss', 'set', 'condition']", Examples of Limiting Distributions o,seg_133,"proof. without loss of generality, set μ = 0. now, lindeberg’s condition holds, as we demonstrate via"
1833,1,['regression'], Examples of Limiting Distributions o,seg_133,the preceding example is useful in regression situations.
1834,1,"['function', 'estimation']", Examples of Limiting Distributions o,seg_133,"exercise 3.13 (monte carlo estimation) let h : [0, 1] → [0, 1] be a measurable function,"
1835,1,['estimators'], Examples of Limiting Distributions o,seg_133,"1 h(t)dt. let x1, y1,x2, y2, . . . be iid uniform(0,1) rvs. define two different estimators of θ by"
1836,1,"['normalized', 'estimated', 'asymptotic', 'unbiased estimators', 'estimators', 'distribution', 'joint', 'variance', 'estimator', 'unbiased']", Examples of Limiting Distributions o,seg_133,"(a) show that both t1n and t2n are unbiased estimators of θ, and determine which estimator has the smaller variance. indicate how the variance of each estimator could be estimated. (b) determine the joint asymptotic distribution of appropriately normalized forms of t1n and t2n."
1837,1,"['quartiles', 'statistic']", Examples of Limiting Distributions o,seg_133,"exercise 3.14 (an analogue of the student-t statistic based on quartiles) let x1, . . . , xn be iid with df f (·). let m ≡ [n/4], for the greatest integer [·]. let"
1838,1,['median'], Examples of Limiting Distributions o,seg_133,"un ≡ xn:m, vn ≡ ẍn ≡ (the median), wn ≡ xn:n+1−m"
1839,1,"['quartiles', 'sample', 'median']", Examples of Limiting Distributions o,seg_133,denote the quartiles and median of the sample. make appropriate assumptions regarding f (·).
1840,1,"['distribution', 'joint', 'asymptotic']", Examples of Limiting Distributions o,seg_133,(a) determine the joint asymptotic distribution of
1841,1,[], Examples of Limiting Distributions o,seg_133,(b) simplify this if the xi are symmetrically distributed about 0.
1842,1,"['sample', 'asymptotic', 'quantiles', 'distribution', 'symmetry', 'statistic']", Examples of Limiting Distributions o,seg_133,(c) determine the asymptotic distribution under symmetry of the (student-t like) statistic (formed from three sample quantiles)
1843,1,[], Examples of Limiting Distributions o,seg_133,"exercise 3.15 let the xk’s be iid cauchy(0, 1) in the previous exercise."
1844,0,[], Examples of Limiting Distributions o,seg_133,(f) express your answers to (b) and (c) of the previous exercise in the present context.
1845,1,"['poisson', 'estimation']", Examples of Limiting Distributions o,seg_133,"exercise 3.16 (poisson estimation) let x1, . . . , xn be iid poisson (θ)."
1846,1,"['sample', 'sample mean', 'estimators', 'mean', 'sample variance', 'variance']", Examples of Limiting Distributions o,seg_133,"(a) reasonable estimators of θ include the sample mean t1n ≡ x̄n, the sample variance t2n ≡ sn 2 , and t3n ≡ ∑n"
1847,1,['observations'], Examples of Limiting Distributions o,seg_133,"1 k (which puts more emphasis on the more recent observations). evaluate lim var[tin] for i = 1, 2, 3."
1848,1,"['unbiased estimators', 'estimators', 'unbiased']", Examples of Limiting Distributions o,seg_133,"2 − x̄n/n and t5n ≡ xn 2 − x̄n are both unbiased estimators of θ2. evaluate lim var [tin] for i = 4, 5."
1849,1,"['distribution', 'asymptotic']", Examples of Limiting Distributions o,seg_133,(c) determine the asymptotic distribution of dn ≡ √n[x̄n − sn
1850,1,['observations'], Examples of Limiting Distributions o,seg_133,2 ]/x̄n when the observations
1851,1,"['poisson', 'distribution', 'poisson distribution']", Examples of Limiting Distributions o,seg_133,really do follow a poisson distribution.
1852,1,"['distribution', 'observations', 'asymptotic']", Examples of Limiting Distributions o,seg_133,"(d) what is the asymptotic distribution of dn when the observations xk actually follow a negbit (r, p) distribution?"
1853,1,"['random number', 'random']", Examples of Limiting Distributions o,seg_133,"theorem 3.1 (doeblin’s clt for a random number of rvs) consider iid (0, σ2) rvs x1,x2, . . .. let {νn}∞n=1 be integer-valued rvs such that the proportion νn/n →p c ∈ (0,∞) as n → ∞. let tn ≡ x1 + · · · + xn denote the total. then"
1854,1,['independent'], Examples of Limiting Distributions o,seg_133,"[note that νn and x1,x2, . . . need not be independent.]"
1855,0,[], Examples of Limiting Distributions o,seg_133,the theorem then follows from slutsky’s theorem.
1856,0,['n'], Examples of Limiting Distributions o,seg_133,"since νn/[cn] →p 1, for n sufficiently large we have"
1857,1,['inequality'], Examples of Limiting Distributions o,seg_133,"also, applying kolmogorov’s inequality twice,"
1858,1,"['sample', 'variance', 'sample variance']", Examples of Limiting Distributions o,seg_133,"exercise 3.17 let vn2 now denote the sample variance. show, in the context of doeblin’s clt, that tνn/vνn →d n(0, 1) as n → ∞."
1859,1,"['independent', 'condition']", Examples of Limiting Distributions o,seg_133,"exercise 3.18 prove a version of doeblin’s theorem for xnk’s independent but not iid; assume the lindeberg condition and νn/n →p c ∈ (0,∞). [revert to the liapunov condition, if necessary.]"
1860,1,"['sample', 'correlation', 'correlation coefficient', 'coefficient', 'sample correlation coefficient']", Examples of Limiting Distributions o,seg_133,exercise 3.19 (sample correlation coefficient rn; cramér and anderson) let us suppose that
1861,1,"['sample', 'correlation', 'correlation coefficient', 'coefficient', 'sample correlation coefficient']", Examples of Limiting Distributions o,seg_133,"and that the σ below has finite entries. consider √n[rn − ρ], where rn is the sample correlation coefficient. thus rn ≡ ssxy /{ssxxssy y }1/2 for the sums of squares ssxy ≡"
1862,1,"['case', 'variances']", Examples of Limiting Distributions o,seg_133,"1 (xi − x̄n)(yi − ȳn), etc. (a) reduce the case of general means, variances and covariances to this case. (b) note that"
1863,1,"['jointly', 'normal', 'independent']", Examples of Limiting Distributions o,seg_133,"2z2 − ρ 2z3 =∼ n(0, τ2), and evaluate τ2. (d) show that when x and y are independent, then √n[rn − ρ] →d n(0, 1). (e) if the (xi, yi)′ are jointly normal, show that"
1864,0,[], Examples of Limiting Distributions o,seg_133,then simplify the expression for τ2 and obtain
1865,1,['distribution'], Examples of Limiting Distributions o,seg_133,"+t t ). ρ (g) approximating the distribution of √n − 3[g(rn) − g(ρ) − 2(n−1) ] by n(0, 1) yields an"
1866,1,"['quantiles', 'extreme value', 'weibull']", Examples of Limiting Distributions o,seg_133,"exercise 3.20 (extreme value quantiles) let x and x1, . . . , xn be iid with the weibull (α, β) density f(x) = (βxβ−1/αβ) exp(−(x/α)β) on x ≥ 0. now, (x/α)β =∼ exponential(l), and thus y ≡ log x satisfies"
1867,1,"['statistics', 'extreme value', 'order statistics']", Examples of Limiting Distributions o,seg_133,"and w has the extreme value density for minima given by exp(w − ew) on (−∞,∞). let yn:1 ≤ · · · ≤ yn:n denote the order statistics of the rvs yk ≡ log xk. first, let 0 < p1 < p2 < 1, and then define un ≡ yn:[np1] and vn ≡ yn:[np2]. we seek values p1 and p2 such that"
1868,1,"['asymptotic', 'normality']", Examples of Limiting Distributions o,seg_133,"−1(p), fy (yp) and p(1 − p)/fy2 (yp). (b) determine values of p1 and p2 that achieve the objective. (c) establish the claimed asymptotic normality, and evaluate σ both symbolically and numerically."
1869,1,"['unbiased estimator', 'model', 'independent', 'parameters', 'minimum variance', 'normal', 'mean', 'variance', 'estimator', 'unbiased']", Examples of Limiting Distributions o,seg_133,"exercise 3.21 (estimating a common normal mean) consider independent rvs x1, . . . , xm and y1, . . . , yn from n(θ, σ2) and n(θ, τ2). when γ ≡ σ2/τ2 is known, the unbiased estimator of θ that has minimum variance (for all possible values of the parameters within this model) is known to be"
1870,1,"['sample', 'sample variances', 'variances']", Examples of Limiting Distributions o,seg_133,"2 , sy 2 ) depend only on the two sample variances sx"
1871,1,[], Examples of Limiting Distributions o,seg_133,(all limits below are to be taken as m ∧ n → ∞.) then define
1872,1,['distribution'], Examples of Limiting Distributions o,seg_133,(f) determine the distribution of α̃. does α̃/α →l2 1?
1873,1,"['sample', 'unbiased estimator', 'sample mean', 'estimation', 'exponential', 'mean', 'variance', 'estimator', 'unbiased']", Examples of Limiting Distributions o,seg_133,"exercise 3.22 (exponential estimation) let x1, . . . , xn be iid exponential (θ). the minimum variance estimator of θ is known to be the sample mean x̄n. another unbiased estimator of θ is tn ≡ ḡn/γn(1 + 1/n), where ḡn ≡ (∏n"
1874,1,"['observations', 'geometric mean', 'mean', 'geometric', 'variances']", Examples of Limiting Distributions o,seg_133,1 xk)1/n denotes the geometric mean of the observations. evaluate the limiting ratio of the variances lim var[x̄n]/var[tn].
1875,1,"['convergence', 'moment']", Examples of Limiting Distributions o,seg_133,"exercise 3.23 let x1, . . . , xn be iid poisson(λ). show the moment convergence e|x̄n − λ|3 → e|n(0, 1)|3."
1876,1,"['simple linear regression', 'linear', 'model', 'regression model', 'regression', 'linear regression', 'linear regression model']", Examples of Limiting Distributions o,seg_133,exercise 3.24 (simple linear regression) consider the simple linear regression model of (a.3.25); thus we are assuming that
1877,1,"['least squares estimators', 'least squares', 'estimators']", Examples of Limiting Distributions o,seg_133,and for known constants xnk for 1 ≤ k ≤ n. the least squares estimators (lses) α̂n and
1878,1,['sum of squares'], Examples of Limiting Distributions o,seg_133,β̂n of α and β are defined to be those values of a and b that minimize the sum of squares
1879,0,[], Examples of Limiting Distributions o,seg_133,use the cramér–wold device and the weighted sums of example 10.3.4 to show that
1880,1,"['noncentral', 'independent', 'distributions']", Examples of Limiting Distributions o,seg_133,"definition 3.1 (noncentral distributions) (a) let x1, . . . , xm be independent, and suppose that xi =∼ n(θi, σ2). let θ2 ≡ ∑m"
1881,1,['distribution'], Examples of Limiting Distributions o,seg_133,"where z1, . . . , zm are iid n(0, 1) rvs. denote this distribution by"
1882,1,"['degrees of freedom', 'independent', 'distribution', 'parameter', 'noncentral']", Examples of Limiting Distributions o,seg_133,"and say that u is distributed as noncentral chisquare with m degrees of freedom and noncentrality parameter δ. (b) let y =∼ n(θ, 1), u =∼ χ2m(δ2/2) and v =∼ χ2n be independent rvs. we define the noncentral student-tn(θ) distribution and the noncentral snedecor-fm,n(δ2/2) via"
1883,1,"['noncentral', 'distributions']", Examples of Limiting Distributions o,seg_133,"proposition 3.1 (form of the noncentral distributions) consider the rvs u, v, and y of the previous definition. let y > 0. (a) the rv u of (32) satisfies"
1884,1,[], Examples of Limiting Distributions o,seg_133,k=0 p (poisson(δ2/2) = j) × p (χ2m+2j > y).
1885,1,"['poisson', 'degrees of freedom', 'mean']", Examples of Limiting Distributions o,seg_133,"here, poisson(λ) denotes a poisson rv with mean λ, and χr2 denotes an ordinary chisquare rv with r degrees of freedom. (b) it is thus trivial that"
1886,1,[], Examples of Limiting Distributions o,seg_133,"∞ =0 p (poisson(δ2/2) = j) × p (fm+2j,n > y)."
1887,0,[], Examples of Limiting Distributions o,seg_133,exercise 3.25 prove proposition 3.1.
1888,1,"['goodness of fit', 'statistic', 'true parameter', 'parameter']", Examples of Limiting Distributions o,seg_133,"exercise 3.26 (chisquare goodness of fit, again) (a) (local alternatives) we suppose that the statistic qn ≡ qn(p0) of (10.1.13) is computed, but that in reality the true parameter"
1889,1,['estimate'], Examples of Limiting Distributions o,seg_133,"k ai = 0, so that the coordinates pni add to 1). let p̂ni ≡ nni/n estimate pni for 1 ≤ i ≤ k. show that the vector"
1890,0,[], Examples of Limiting Distributions o,seg_133,"(b) (fixed alternatives) suppose that qn ≡ qn(p0) is computed, but a fixed p is true. show that"
1891,1,['densities'], Local Limit Theorems o,seg_135,"recall from scheffé’s theorem that if fn and f are densities with respect to some dominating measure μ, then"
1892,1,"['densities', 'distribution', 'variation', 'continuous', 'convergence']", Local Limit Theorems o,seg_135,"thus convergence of densities implies convergence in total variation distance, which is stronger than convergence in distribution. we will now establish (1) in a clt context, for summands that are either suitably continuous or else are distributed on a grid."
1893,1,"['continuous', 'case', 'limit']", Local Limit Theorems o,seg_135,"theorem 4.1 (local limit theorem, continuous case) let x,x1,x2, . . . be iid (0, σ2)"
1894,1,"['case', 'discrete', 'limit']", Local Limit Theorems o,seg_135,"theorem 4.2 (local limit theorem, discrete case) let x1,x2, . . . be iid (0, σ2) rvs that take values on the grid a±md for m = 0,±1,±2, . . .. now let x∗ = (na+md)/(σ√n), and m = 0,±1,±2, . . . denote a possible value of sn/(σ√n), and let pn(x) ≡ p (sn/(σ√n) = x). then"
1895,1,[], Local Limit Theorems o,seg_135,"example 4.1 let x1, . . . , xn be iid bernoulli(θ) rvs. then (by (4))"
1896,0,[], Local Limit Theorems o,seg_135,exercise 4.1* verify (5) by direct computation.
1897,1,['loss'], Local Limit Theorems o,seg_135,proof. consider theorem 4.1. without loss of generality we may suppose that σ = 1. notice that
1898,0,[], Local Limit Theorems o,seg_135,thus the fourier inversion formula of (9.4.9) gives
1899,1,['distribution'], Local Limit Theorems o,seg_135,"this same formula also holds for the distribution of a n(0, 1) rv z. thus"
1900,1,['inequality'], Local Limit Theorems o,seg_135,"we first specify δ > 0 so small that |φx(t)| ≤ exp(−t2/4) for |t| ≤ δ. this is possible, since |φ(t)| = |1 − 0 − t2/2| + |o(t2)| ≤ 1 − t2/4 ≤ exp(−t2/4) in some neighborhood |t| ≤ δ, by inequality 9.6.2."
1901,0,[], Local Limit Theorems o,seg_135,for this fixed large a we have
1902,1,['convergence'], Local Limit Theorems o,seg_135,since the cramér–lévy theorem implies that the convergence of these chfs is uniform on any |t| ≤ a.
1903,0,['e'], Local Limit Theorems o,seg_135,"combining (e), (f), and (g) into (d) establishes the claim made in the theorem."
1904,1,['distributions'], Local Limit Theorems o,seg_135,"proof. consider theorem 4.2. by the inversion formula (9.8.1) given for distributions on grids,"
1905,1,['densities'], Local Limit Theorems o,seg_135,"by the inversion formula (9.4.9) given for densities,"
1906,0,[], Local Limit Theorems o,seg_135,"the proof of theorem 4.1 applies, virtually verbatim; the only thing worthy of note is that"
1907,0,[], Normality Via Winsorization and Truncation,seg_137,"definition 5.1 (a) call xn1, . . . , xnn weakly negligible (or, strongly negligible) if"
1908,1,[], Normality Via Winsorization and Truncation,seg_137,(b) call them uniformly asymptotically negligible (or uan) under the weaker
1909,1,"['consequences', 'statistical', 'results']", Normality Via Winsorization and Truncation,seg_137,"the objective is to now investigate when some version of the clt holds for uan pieces, as in (2). (that is, there are to be no monsters among the many, no giants among the peons, etc., and the law of the mob is to hold sway. these are cute descriptive phrases others have used to summarize the problem very cleverly.) the results in theorem 5.1 are classical, while theorem 5.2 and its iid consequences were developed with statistical intent in shorack (2000)—and are streamlined herein."
1910,1,"['asymptotic', 'normality', 'independent']", Normality Via Winsorization and Truncation,seg_137,"theorem 5.1 (asymptotic normality) let xn1, . . . , xnn be independent rvs. let a > 0 and b be arbitrary. as above, let mn ≡ [max1≤k≤n |xnk|]. fix a truncation constant; let it be any c > 0. let x̌nk be the ťruncated rv that equals xnk or 0 according as |xnk| ≤ c or as |xnk| > c. the following are equivalent:"
1911,1,['results'], Normality Via Winsorization and Truncation,seg_137,"if (4) holds for one c > 0, then it holds for each c > 0. these results also hold with w̃insorized quantities x̃nk, μ̃n, σ̃n"
1912,0,[], Normality Via Winsorization and Truncation,seg_137,1 xnk →d (some rv y ) for rvs xnk that are row independent and uan. then
1913,1,"['distribution', 'normal', 'normal distribution']", Normality Via Winsorization and Truncation,seg_137,(5) y has a normal distribution iff mn ≡ [1m
1914,1,['symmetric'], Normality Via Winsorization and Truncation,seg_137,corollary 2 let the xnk above be symmetric. then (3) (with b = 0) holds iff
1915,1,['normalized'], Normality Via Winsorization and Truncation,seg_137,"1 x̌nk. thus we need only show that the normalized rv žn ≡ (∑n 1 x̌nk − μ̌n)/σ̌n →d n(0, 1). it suffices to verify that lindeberg’s ľfn of (10.2.11) satisfies ľfn → 0. we will do so presently. first, note that mn →p 0, by (8.3.14). thus [max |x̌nk|] ≤ mn →p 0"
1916,1,['function'], Normality Via Winsorization and Truncation,seg_137,by the dct with dominating function “c.” thus for n ≥ (some n ) we have
1917,1,['case'], Normality Via Winsorization and Truncation,seg_137,"(the exact same bounds for l̃fn in the w̃insorized case hold in (c)–(f), showing that l̃fn → 0.) thus (4) (for either x̌nk’s or x̃n"
1918,0,[], Normality Via Winsorization and Truncation,seg_137,discussed in section 8.3. applying the continuity theorem 9.5.1 for chf’s then yields
1919,1,['symmetric'], Normality Via Winsorization and Truncation,seg_137,"s′ks are symmetric, their chf’s are real valued. moreover, all (1 −"
1920,1,[], Normality Via Winsorization and Truncation,seg_137,"thus for all n ≥ (some n ,m ) we have (recalling that a = b ⊕ c means that |a − b| ≤ c)"
1921,1,['inequality'], Normality Via Winsorization and Truncation,seg_137,whenever (8) holds. (recall the inequality 9.5.1 for “comparison.”) let t1 = 1 and t2 = 2.
1922,1,['function'], Normality Via Winsorization and Truncation,seg_137,"consider the function h(x) ≡ (3 − 4 sinx + sin 2x ). (on [0, 4] the function h increases from 0"
1923,1,['inequality'], Normality Via Winsorization and Truncation,seg_137,"x 2x to about 4, it then oscillates ever smaller about 3, and eventually converges to 3 at ∞.) the basic inequality (3.4.18) gives (for all tiny > 0)"
1924,1,['function'], Normality Via Winsorization and Truncation,seg_137,"since 5/h( ) ∼ 10 as ↓ 0 (simply expand sinx and sin 2x in the formula for the function h(·) in (10)). thus, (8.3.14) and (k) yield"
1925,0,[], Normality Via Winsorization and Truncation,seg_137,s →p 0 and the symmetrization
1926,1,['condition'], Normality Via Winsorization and Truncation,seg_137,"inequality (8.3.9), and the uan condition then guarantees that [max1≤k≤n |m̈nk|] → 0. (see kallenberg (1997) regarding this vital (11) via (9) and (10). very nice!) so far, we have shown that (3) implies (7) and (11). (note that the “only if” implication in corollary 1 is now established as well.)"
1927,0,[], Normality Via Winsorization and Truncation,seg_137,we now truncate these symmetrized rvs by letting x̌n
1928,0,[], Normality Via Winsorization and Truncation,seg_137,"note: the symbol x̌n sk(1) (symmetrize, and then truncate at c = 1) as well as the symbol x̌n sk(1) (truncate at c = 1, and then symmetrize) are henceforth replaced by the symbols x̌n"
1929,1,[], Normality Via Winsorization and Truncation,seg_137,"by the argument (begin at (12)) made in previous paragraph—that led to the third claim made in (13). that is (parallel to (13)), the x̌n"
1930,1,['hypothesis'], Normality Via Winsorization and Truncation,seg_137,while hypothesis (3) gives
1931,1,['hypothesis'], Normality Via Winsorization and Truncation,seg_137,follows from the analogous proof. from hypothesis (3) (and using mn →p 0 in (11) to replace xnk’s by x̌nk’s for the =a in (t)) we have
1932,1,"['hypotheses', 'mean']", Normality Via Winsorization and Truncation,seg_137,"note that the hypotheses of (4) hold for rvs x̌nk −μ̌nk (since they have mean 0, since σ̌n"
1933,1,['adjusted'], Normality Via Winsorization and Truncation,seg_137,"by (16), and since m̌n →p 0 by (17)). that is, these adjusted rvs"
1934,1,['extreme values'], Normality Via Winsorization and Truncation,seg_137,they are “nicer” than the general rvs to which the lindeberg-feller theorem 10.2.2 applies in that their most extreme values have been truncated. thus
1935,0,[], Normality Via Winsorization and Truncation,seg_137,"proof. consider corollary 1. in light of the remark below (11), the “only if part” of corollary 1 has already been established, so suppose that mn →p 0. since yn →d y , the symmetrized versions satisfy yns →d y s ≡ y − y ′. that mn →p 0 (and hence mn"
1936,0,[], Normality Via Winsorization and Truncation,seg_137,means that the versions truncated at c = 1 also satisfy y̌n ≡ ∑n
1937,1,[], Normality Via Winsorization and Truncation,seg_137,"1 (xnk − μ̌nk) + μ̌n = ∑n 1 xnk →d y is given. thus μ̌n must converge to some finite number—name it b, and so the limiting rv y is n(b, a2)."
1938,1,['symmetric'], Normality Via Winsorization and Truncation,seg_137,proof. consider corollary 2. let the rvs x̌nk represent the symmetric uan rvs xnk truncated at c = a. define the rvs wnk via
1939,0,[], Normality Via Winsorization and Truncation,seg_137,rewriting (20) in terms of the wnk’s gives
1940,1,['condition'], Normality Via Winsorization and Truncation,seg_137,"that is, condition (c.1.9) holds for the wnk. hence the equivalent (c.1.3) gives"
1941,1,"['case', 'observations']", Normality Via Winsorization and Truncation,seg_137,"thus (6) holds. next, suppose (6) = (z) holds and verify (4)—or its equivalent (3). basically, just read (20)–(z) backward. (still, can we make useful observations on the case of nonsymmetric rvs xnk? more on this later.)"
1942,1,['condition'], Normality Via Winsorization and Truncation,seg_137,corollary 3 (i) condition (3) that∑n
1943,1,['independent'], Normality Via Winsorization and Truncation,seg_137,"1 xnk →d n(b, a2) for row independent uan rvs xn1, . . . , xnn implies (see the proof of theorem 5.1, with μ̌n ≡ ∑n"
1944,0,[], Normality Via Winsorization and Truncation,seg_137,"(ii) conversely (and trivially from (4) implies (3)),"
1945,0,[], Normality Via Winsorization and Truncation,seg_137,(iii) the w̃insorized constants
1946,0,[], Normality Via Winsorization and Truncation,seg_137,(iv) the theorem of types then yields both σ̌n
1947,1,['results'], Normality Via Winsorization and Truncation,seg_137,2/σ̃n 2 → 1 and (μ̃n − μ̌n)/σ̃n → 0. all results hold for any choice of the truncation point “c” of the given rvs xnk.
1948,0,[], Normality Via Winsorization and Truncation,seg_137,statistical reformulation of the general clts
1949,1,"['symmetric', 'interval']", Normality Via Winsorization and Truncation,seg_137,"notation 5.1 (weak negligibility in the clt context) let xn1, . . . , xnn be independent rvs with dfs fn1, . . . , fnn. fix θ > 0. define xθn by requiring [−xθn, xθn] to be the shortest closed, symmetric interval to which f̄n ≡ n"
1950,1,['probability'], Normality Via Winsorization and Truncation,seg_137,1 ∑n 1 fnk assigns probability at least 1 − θ/n. let p̄n(x) ≡ n
1951,1,"['tail', 'average', 'tail probability', 'probability']", Normality Via Winsorization and Truncation,seg_137,"1 ∑n 1 p (|xnk| > x) denote the average tail probability, and then let"
1952,1,['quantile'], Normality Via Winsorization and Truncation,seg_137,kn denote the qf of the df 1 − p̄n(·). note the quantile relationship xθn = kn(1 − θ/n). let wn denote a rv with the df 1 − p̄n(·).
1953,1,"['moment', 'mean', 'variance', 'second moment']", Normality Via Winsorization and Truncation,seg_137,"2k, and ṽnk be the w̃insorized mean, variance and second moment of x̃nk. let"
1954,1,"['moments', 'average', 'variances']", Normality Via Winsorization and Truncation,seg_137,"1 ∑n 1 x̃nk. its average means, variances, and 2nd moments are"
1955,1,"['quantile', 'moments', 'average']", Normality Via Winsorization and Truncation,seg_137,"for ťruncated 2nd moments instead, etc. such μ̌n and σ̌n may replace the μ̃n and σ̃n in the obvious fashion—see corollary 1 below. applying discussion 8.3.1 to the rvs |xnk|/√nσ̃n (whose average df has (1− θ/n)th quantile xθn/√nσ̃n) shows the following conditions for the weak negligibility of the rvs xnk/√nσ̃n are equivalent:"
1956,0,[], Normality Via Winsorization and Truncation,seg_137,theorem 5.2 (notation for w̃insorization) consider both xn ≡ 1 ∑k
1957,1,['independent'], Normality Via Winsorization and Truncation,seg_137,2k for row independent rvs xnk(1 ≤ k ≤ n). let (note (25)) n
1958,1,['mean'], Normality Via Winsorization and Truncation,seg_137,"(i) (clt for the w̃insorized mean) first,"
1959,1,"['estimation', 'moment']", Normality Via Winsorization and Truncation,seg_137,(ii) (moment estimation) the following claims hold for xn
1960,1,['results'], Normality Via Winsorization and Truncation,seg_137,(other results in theorem c.1.1 (with r = 2) can also be recast in this context.) (iii) (statistically useful) let sn
1961,0,[], Normality Via Winsorization and Truncation,seg_137,then all of the following hold.
1962,1,['mean'], Normality Via Winsorization and Truncation,seg_137,"corollary 1 (clt for the ťruncated mean) all of theorem 5.2 still holds true when μ̌n and σ̌n replace μ̃n and σ̃n throughout. especially,"
1963,1,['condition'], Normality Via Winsorization and Truncation,seg_137,(condition (35) also yields both of the conclusions stated in (38)—as well as
1964,0,[], Normality Via Winsorization and Truncation,seg_137,"proof. for the proof of theorem 5.2, we will need notation similar to that above, for any θ. for 0 < θ ≤ θ0 let x̃n"
1965,1,['moments'], Normality Via Winsorization and Truncation,seg_137,"θk, and let z̃θn ≡ √n[x̃θn − μ̃θn]/σ̃θn. define third central moments"
1966,1,[], Normality Via Winsorization and Truncation,seg_137,"(recall that a = b ⊕ c means |a − b| ≤ c.) since ‖fz̄θn − φ‖ ≤ 13γ̃θn/√nσ̃θ3n by the berry– esseen result (10.2.3), conclusion (30) will follow at once from (41) and the slutsky theorem 2.4.1 by verifying (42) below."
1967,1,['quantile'], Normality Via Winsorization and Truncation,seg_137,inequality 5.1 (applying the liapunov clt from a quantile viewpoint) suppose only that mn →p 0 (hence (27) holds as well). fix any 0 < θ ≤ θ0. then
1968,1,['set'], Normality Via Winsorization and Truncation,seg_137,"proof. for convenience, set θ0 = 1. note that σ̃n = σ̃1n = σ̃θo,n now holds. bounding"
1969,1,['probability'], Normality Via Winsorization and Truncation,seg_137,"using (27). since the probability outside [−x1n, x1n] is at most 1/n,"
1970,1,"['average', 'moments']", Normality Via Winsorization and Truncation,seg_137,by (27). we need some notation before turning to (42)(j). let ṽθn denote the average of the second moments of the x̃n
1971,1,"['interval', 'complement', 'set', 'probability']", Normality Via Winsorization and Truncation,seg_137,"θk, and set ṽn ≡ ṽ1n. now, f̄n assigns at most 1/n probability to the complement of the interval [−x1n, x1n], and on [−xθn, xθn] the integrand of ṽθn never exceeds x2θn. and so"
1972,1,"['asymptotic', 'normality']", Normality Via Winsorization and Truncation,seg_137,"by (27). thus (42) holds, giving the asymptotic normality of z̄n in (41) and (30)."
1973,0,[], Normality Via Winsorization and Truncation,seg_137,this proof of (42)(j) (ending in (46)) also verifies that both
1974,0,[], Normality Via Winsorization and Truncation,seg_137,proof. we now consider (31) and (32). suppose
1975,1,['results'], Normality Via Winsorization and Truncation,seg_137,"2k/nv̌n being uan. this establishes (33). (all of the results (31), (32), and (33) hold in the context of corollary 1.) conclusion (34) holds since (c.1.4) and (c.1.8) are equivalent conditions in theorem c.1.1."
1976,0,['n'], Normality Via Winsorization and Truncation,seg_137,proof. let us prove (36). suppose first that only n
1977,1,['hypothesis'], Normality Via Winsorization and Truncation,seg_137,"by the equivalence of (c.1.4) with (c.1.8) (see (34)). we next wish to replace v̌n in (m) by σ̃n 2 in order to claim mn2 →p 0 under the hypothesis (35). to do the bookkeeping, we claim from the equivalence of (c.1.6) with (m) that"
1978,1,['average'], Normality Via Winsorization and Truncation,seg_137,"added stipulation makes complete sense. if all rvs xnk only take on one fixed value ‘c’, then the average value of the xn"
1979,1,['variance'], Normality Via Winsorization and Truncation,seg_137,"2k’s does indeed “converge” to ‘c2’—but there is no hope of a clt as the variance is 0. still, we did not use this for (m).)"
1980,0,[], Normality Via Winsorization and Truncation,seg_137,"analogous to (45), we now write"
1981,0,[], Normality Via Winsorization and Truncation,seg_137,"proof. consider (37), when (35) holds. conclusions from (36) and (33) include"
1982,1,"['set', 'inequality']", Normality Via Winsorization and Truncation,seg_137,"let > 0 be given, and set 0 < θ < /2. then for all n ≥ (some n ), the ťruncation inequality (8.3.25) gives"
1983,0,[], Normality Via Winsorization and Truncation,seg_137,remark 10.5.1 (simplification for iid rvs) any non-degenerate rv x satisfies
1984,0,[], Normality Via Winsorization and Truncation,seg_137,2 is finite or is infinite;
1985,1,"['variance', 'percentage', 'case']", Independent Identically Distributed RVs,seg_139,"notation 6.1 (magnitude and percentage w̃insorization) let f and k be the df and qf of a non-degenerate rv x. (there is no need for the following theorem if var[x] is finite. however, it does apply to this finite variance case as well.)"
1986,1,"['parameters', 'interval', 'symmetric', 'probability']", Independent Identically Distributed RVs,seg_139,"magnitude w̃insorization: let [−xθn, xθn] be the shortest interval symmetric about 0 that contains at least the proportion (1− θ/n) of the x probability. when |x| has df f|x|(·) and qf k|x|(·) then xθn ≡ k|x|(1−θ/n). we now define the magnitude modified parameters."
1987,1,"['variance', 'mean']", Independent Identically Distributed RVs,seg_139,the w̃insorized mean and variance notation is
1988,1,['moments'], Independent Identically Distributed RVs,seg_139,"define ťruncated second moments in terms of dfs on [0,∞) and qfs on (0, 1] via"
1989,1,"['second moment', 'moment']", Independent Identically Distributed RVs,seg_139,"winsorized second moment in the present context is ũ1n, where"
1990,1,"['second moment', 'moment']", Independent Identically Distributed RVs,seg_139,"equality holds in (3) even though u1n ≥ v1n always holds (see theorem c.1.1 (c)). (the magnitude ťruncated second moment is denoted by both v1n = v̌1n, so as to connect it more easily with theorem c.1.1. let σ̌12n ≡ v1n − μ̌21n = v̌1n − μ̌21n.)"
1991,1,['tail'], Independent Identically Distributed RVs,seg_139,"percentage w̃insorization: w̃insorize an equal proportion “a” from each tail. let a > 0 be tiny. we agree that dom(a, a) denotes [0, 1−a), (a, 1], or (a, 1−a) according as x ≥ 0,x ≤ 0, or otherwise, and that k̃a,a(·) denotes k w̃insorized outside dom(a, a). (for example, when"
1992,1,"['percentage', 'moment', 'mean', 'variance', 'second moment']", Independent Identically Distributed RVs,seg_139,"thus the percentage w̃ insorized second moment, mean, and variance (when we winsorize outside dom(1/n, 1/n) ) can also be written as ṽn = ek̃12/n,1/n(ξ), μ̃n = ek̃1/n,1/n(ξ),"
1993,1,"['moments', 'case']", Independent Identically Distributed RVs,seg_139,"σ̌n 2 ≡ v̌n − μ̌2n denote the ťruncated moments in this case, where"
1994,1,[], Independent Identically Distributed RVs,seg_139,"normed clt rvs now define the rvs (xn1, . . . , xnn) ≡ (k(ξn1), . . . ,k(ξnn)) for rowindependent uniform(0, 1) rvs ξn1, . . . , ξnn. they are row-independent rvs with df f and qf k (as shown in (6.3.2)). our interest is in both"
1995,1,"['variance', 'independent']", Independent Identically Distributed RVs,seg_139,"theorem 6.1 (clt for general iid rvs) let xn1, . . . , xnn be row independent iid rvs with variance in (0,∞]. possible choices for the constant νn herein include"
1996,0,[], Independent Identically Distributed RVs,seg_139,the following conclusions are equivalent.
1997,1,"['functions', 'function', 'varying']", Independent Identically Distributed RVs,seg_139,"definition 6.1 (slowly varying functions) (a) call l(·) > 0 slowly varying at 0 (written l ∈ r0 or l ∈ l) provided l(ct)/l(t) → 1 as t → 0, for each c > 0. (b) the function l(·) > 0 on (0,∞) is called slowly varying at ∞ (written as l ∈ u0) if it satisfies l(cx)/l(x) → 1 as x → ∞, for each positive number c > 0. (∗) the functions l(t) = log(1/t) and l(x) = log(x) are the prototypes."
1998,1,"['distribution', 'normal', 'normal distribution']", Independent Identically Distributed RVs,seg_139,"proposition 6.1 (equivalent conditions that give a clt for iid rvs) the following contains further “best” items from a long list of equivalents. when any one (hence all) hold, we write either f ∈ d(normal) or k ∈ d(normal) and say that f (or,k) is in the domain of attraction of the normal distribution. (we require that specific an ↘ 0 have lim(an/an+1) < ∞) in (28), and an = θ/n is one such.) the following are also equivalent to the conclusions listed in theorem 6.1."
1999,1,['varying'], Independent Identically Distributed RVs,seg_139,"(23) u(·) is slowly varying at infinity, where u(x) ≡ ∫[|y|≤x] y2df (y)."
2000,1,['varying'], Independent Identically Distributed RVs,seg_139,"(24) v (·) is slowly varying at zero, where v (t) ≡ ∫dom(t,t) k2(s)ds"
2001,1,['varying'], Independent Identically Distributed RVs,seg_139,"(25) v|x|(·) is slowly varying at zero, where v|x|(t) ≡ ∫[0,1−t) k|2x|(s)ds"
2002,1,['varying'], Independent Identically Distributed RVs,seg_139,"(26) σ̃2(·) is slowly varying at zero, where σ̃2(t) ≡ var[k̃t2,t(ξ)]."
2003,1,['symmetric'], Independent Identically Distributed RVs,seg_139,"(recall the gnedenko–kolmogorov theorem 6.6.1, (6.6.7), and all of that section.) (for symmetric xs ≡ x − x ′ rvs, see definition 8.3.1.)"
2004,1,"['asymptotic', 'normality', 'statistic']", Independent Identically Distributed RVs,seg_139,"corollary 1 (asymptotic normality of the student-t statistic) if any one (hence, all) of (9)–(20) and (23)–(35) holds, then"
2005,1,"['interval', 'confidence', 'confidence interval']", Independent Identically Distributed RVs,seg_139,"thus, we have a confidence interval available for any of μ̃n, μ̃1nμ̌n, or μ̌1n that is asymptotically valid for any df f ∈ d(normal)."
2006,1,[], Independent Identically Distributed RVs,seg_139,"proof. consider theorem 6.1 the equivalence of conditions (10)–(20) and (23)–(35) is specifically developed in section c.1-section c.3, where much longer lists of equivalents are given. theorem 6.1 needs only to tie the clt into this other combined list. this is accomplished via comparing (c.1.24)(a) (note also (c.1.24)(c)) with (10.5.32) and (10.5.38) in theorem 10.5.2. specifically, (10) ↔ (c.1.60) denotes the correspondence. likewise, (11) ↔ (c.1.59), (12) ↔ (c.1.58), (13) ↔ ((c.1.51)–(c.1.54), (c.1.32), (c.1.42)), (14) ↔ (c.1.55), (15) ↔ ((c.1.57) and (c.1.38)), (16) ↔ ((c.1.28) and (c.1.32)), (17) ↔ (c.1.56), (18) ↔ (c.1.43) with r = 2, (19) ↔ (c.1.25), (20) ↔ (c.1.26)."
2007,1,"['results', 'percentage']", Independent Identically Distributed RVs,seg_139,"the results in proposition 6.1 correspond to results in section c.2. in section c.2, theorem c.2.1 has parts (a)−(d) for magnitude w̃insorizing as in (1)–(3), while theorem c.2.2 repeats much of this for for percentage w̃insorizing as in (4)–(6). we will list a–d and f after an equation number (with the added symbol “f” referring to the use of the theorem c.2.2 version of a result (and not the theorem c.2.1 version). thus (22) ↔ (c.2.15), (23) ↔ (c.2.13), (24) ↔ (c.2.7)f, (25) ↔ (c.2.7), (26) ↔ (c.2.4)f, (27) ↔ (c.2.12)f, (28) ↔ (c.2.12)bf, (29) ↔ (c.2.6), (30) ↔ (c.2.6)f, (31) ↔ (c.2.10)cf, (32) ↔ (c.2.10)bcf, (33) ↔ (c.2.9)bcf, (34) ↔ (c.2.11)bcf, (35) ↔ (c.2.7)bcf."
2008,1,['condition'], Independent Identically Distributed RVs,seg_139,"as earlier pointed out, many of these equivalences are found in the literature. having a different denominator in a condition, or being required to verify it only on some sequence of values, could be useful. (the author finds all of the conditions in theorem 6.1 and proposition 6.1 to be interesting in their own right.)"
2009,1,"['set', 'normal']", A Converse of the Classical CLT,seg_141,"theorem 7.1 (domain of normal attraction of the normal df) consider iid rvs x1,x2, . . ., and set zn = ∑n"
2010,1,['independent'], A Converse of the Classical CLT,seg_141,"n kxk/√n for iid rademacher rvs 1, 2, . . . that are independent of the xk’s. by giné–zinn symmetrization of (8.3.10), we have"
2011,1,"['hypotheses', 'inequality']", A Converse of the Classical CLT,seg_141,"and thus p (zn > λ) = op(1) by our hypotheses. also, khinchin’s inequality in exercise 8.3.3 (regarding the xk’s as fixed constants, and with r = 1) gives"
2012,1,['inequality'], A Converse of the Classical CLT,seg_141,applying paley–zygmund’s inequality 3.4.9 to zn (conditioned on fixed values of the xk’s) for the first inequality and (b) for the second yields
2013,1,"['expectations', 'inequality']", A Converse of the Classical CLT,seg_141,taking expectations across the extremes of this inequality with respect to the xk’s gives the bound
2014,1,['hypothesis'], A Converse of the Classical CLT,seg_141,"thus sn = op(1), by combining (e), (a), and the hypothesis."
2015,1,['set'], A Converse of the Classical CLT,seg_141,"but →a.s. implies →d. thus, applying (9.1.12) to the open set (t,∞) gives"
2016,0,[], A Converse of the Classical CLT,seg_141,it follows that for each t > 0 we have
2017,1,"['indicator function', 'function', 'indicator']", A Converse of the Classical CLT,seg_141,2 = op(1); and this implies that we can specify a t value of t0 in (j) so large that the right-hand side of (j) at t0 is less than 1/2. but this implies that for this t0 the indicator function in (i) must equal zero uniformly in m . this means that
2018,0,[], A Converse of the Classical CLT,seg_141,"but this last supremum equals e(x12), and hence we must have e(x12) ≤ t0 < ∞."
2019,1,['hypothesis'], A Converse of the Classical CLT,seg_141,"to complete the proof, we must now show that e(x1) = 0. since ex12 < ∞, the wlln gives x̄n →p ex1. but the hypothesis that zn = op(1) implies that x̄n = zn/√n →p 0. combining these gives ex1 = 0."
2020,1,"['sample', 'variance', 'mean']", Bootstrapping o,seg_143,"suppose x1,x2, . . . are an iid sample from f . denote the empirical df of the sample xn ≡ (x1, . . . , xn)′ by fn(·). this empirical df fn has mean x̄n and variance sn"
2021,1,"['sample', 'bootstrap']", Bootstrapping o,seg_143,"∗1, . . . , xn ∗n) denote an iid sample from fn, called the bootstrap sample. let x̄n"
2022,1,"['deviation', 'sample', 'bootstrap', 'moments', 'mean', 'standard', 'standard deviation']", Bootstrapping o,seg_143,"denote the mean and the standard deviation of the bootstrap sample. since the moments of fn exist, we will work with normed summands. note that the normed summands of a bootstrap sample always constitute a uan array, since"
2023,1,"['mean', 'bootstrap']", Bootstrapping o,seg_143,all > 0. the maximum normed summand (when forming the bootstrap mean) is
2024,1,"['sampling', 'random sampling', 'random']", Bootstrapping o,seg_143,"now (by random sampling xi−x̄n values), we can view mn∗ as the value of either of mn = m̃n of (10.5.30) and (10.5.31), and note that (for each 0 < θ ≤ θ0 = 1/2) the quantity xθn/√nσ̃n in (10.6.18) now has a value of dn, where"
2025,1,"['sample', 'bootstrap']", Bootstrapping o,seg_143,"now, dn is formed from the original sample, while mn∗ is formed from the bootstrap sample. the following theorem is in the spirit of result (10.6.9). (note, moreover, that 0 ≤ mn∗ ≤ dn, while p (mn∗ = dn) ≥ 1 − (1 − 1/n)n → 1 − 1/e > 0 also holds.) the “standardized” rv of (10.6.9) is now equal to"
2026,1,['bootstrap'], Bootstrapping o,seg_143,agree that the weak bootstrap holds if
2027,1,"['joint', 'probability', 'joint probability']", Bootstrapping o,seg_143,(5) for the joint probability on ω × ω∗n.
2028,1,['bootstrap'], Bootstrapping o,seg_143,agree that the strong bootstrap holds if
2029,1,[], Bootstrapping o,seg_143,theorem 8.1 (bootstrapping) consider z̄n
2030,1,['case'], Bootstrapping o,seg_143,∗ in the iid case.
2031,1,['bootstrap'], Bootstrapping o,seg_143,(i) the weak bootstrap for z̄n
2032,0,[], Bootstrapping o,seg_143,∗ is equivalent to both
2033,1,['varying'], Bootstrapping o,seg_143,(7) dn →p 0 and/or σ̃2(·) is slowly varying at zero
2034,1,['bootstrap'], Bootstrapping o,seg_143,(ii) the strong bootstrap for z̄n
2035,0,[], Bootstrapping o,seg_143,∗ is equivalent to both
2036,1,['bootstrap'], Bootstrapping o,seg_143,corollary 1 (i) the weak bootstrap holds for tn∗ whenever dn →p 0.
2037,1,['bootstrap'], Bootstrapping o,seg_143,(ii) the strong bootstrap holds for tn∗ whenever dn → a.s. 0.
2038,1,['varying'], Bootstrapping o,seg_143,"proof. now (10.6.26) and (10.6.11) show that dn →p 0 is equivalent to σ̃2(·) being slowly varying. (thus, (7) is a true statement.) additionally, it is known from the slln of theorem 8.4.1 (or, from exercise 8.4.20(ii)) that dn → a.s. 0 is equivalent to var[x1] < ∞. (thus, (8) is a true statement.)"
2039,1,['normality'], Bootstrapping o,seg_143,we next verify normality. consider (ii). the liapunov bound of (10.2.5) is
2040,1,['sample'], Bootstrapping o,seg_143,"for a.e. sample value of x1,x2, . . .. thus (6) holds. we next consider (i). well, p (dn > /8) < for all n ≥ (some n ) means that p (‖fz̄∗ − φ‖ > ) < for all n ≥ n . that is, (5)"
2041,1,['normality'], Bootstrapping o,seg_143,consider the converse of the normality statements. suppose z̄n
2042,1,['bootstrap'], Bootstrapping o,seg_143,"∗ →d n(0, 1) for a fixed array (x1, . . . , xn, . . .). the summands are necessarily uan by (1). thus (10.6.18) (for any θ < 1) is equivalent to dn → 0 for this same fixed array (as already noted just above (3)). thus dn → a.s. 0 is implied by the strong bootstrap, and dn →p 0 is implied (by going to subsequences) by the weak bootstrap."
2043,1,['results'], Bootstrapping o,seg_143,these last two results are useful in their own right.
2044,0,[], Bootstrapping o,seg_143,exercise 8.1 establish the details.
2045,1,"['sample', 'mean']", Bootstrapping with Slowly  Winsorization,seg_145,′ )/n → 0. the clt necessarily applies to any such w̃insorized mean from any iid sample! unfortunately the rvs x̃n and sn
2046,0,[], Bootstrapping with Slowly  Winsorization,seg_145,2 below are not
2047,1,"['bootstrap', 'data']", Bootstrapping with Slowly  Winsorization,seg_145,"statistics–that is, they are not computable from the data. however, the bootstrap version of this theorem is useful."
2048,1,['associated'], Bootstrapping with Slowly  Winsorization,seg_145,"notation 9.1 let kn ≡ fn−1 denote the qf associated with some unknown but fixed nondegenerate df fn. we can always specify that dom (an, a′n) ≡ (an, 1 − a′n) for any n(an ∧ a′n) → ∞. however, if fn−(0) = 0, then specifying that dom (0, a′n) ≡ [0, 1 − a′n) is preferable; and if fn(0) = 1, then specifying that dom (an, 0) ≡ (an, 1] is preferable. so we agree to let an ∧ an′ denote an ∧ an′ , or an′ , or an according to the scheme used. let k̃n(·) denote knw̃insorized outside of dom (an, a′n), and let μ̃n and σ̃n"
2049,1,['mean'], Bootstrapping with Slowly  Winsorization,seg_145,2 denote the mean and vari-
2050,1,"['sample', 'sample mean', 'mean', 'variance']", Bootstrapping with Slowly  Winsorization,seg_145,1 (x̃nk − x̃n)2/n denote the sample mean and variance. the quantities of primary interest here are
2051,1,"['model', 'data']", Bootstrapping with Slowly  Winsorization,seg_145,"define γ̃n ≡ e|x̃n1 − μ̃n|3. suppose these x̃nk ≡ k̃n(ξnk), with either known or unknown df fn, denote our model for our data. also, let fn denote the collection of all dfs fn having σ̃n > 0."
2052,0,[], Bootstrapping with Slowly  Winsorization,seg_145,theorem 9.1 (universal clt for w̃insorized rvs) suppose that the w̃insorization fractions satisfy n(an ∧ a′n) = (kn ∧ kn
2053,1,[], Bootstrapping with Slowly  Winsorization,seg_145,requiring further that (an ∨ an′ ) → 0 guarantees that every fixed nondegenerate df f is eventually in all further collections fn.
2054,1,['bernoulli'], Bootstrapping with Slowly  Winsorization,seg_145,"example 9.1 if fn is bernoulli (10−10), then n must be huge before σ̃n(an) > 0."
2055,0,[], Bootstrapping with Slowly  Winsorization,seg_145,3 is immediate from the berry–esseen theorem for
2056,1,['inequality'], Bootstrapping with Slowly  Winsorization,seg_145,where chebyshev’s inequality gives both p (|i1n| ≥ ) ≤ 1/( 2n) → 0 and
2057,1,['set'], Bootstrapping with Slowly  Winsorization,seg_145,"note that p (an ) ≡ p ([|z̃n| ≤ m ]) can be made uniformly small via chebyshev (as just below (a) ), even though this set depends on f . also, p (|σ̃n/s̃n − 1| ≥ ) → 0 uniformly in f ∈ fn also (as exhibited in (c) ). thus (4) follows."
2058,1,['sample'], Bootstrapping with Slowly  Winsorization,seg_145,"notation 9.2 let xn ≡ (xn1, . . . , xnn)′ denote an iid sample from the qf kn, and let x̄n, sn"
2059,1,"['sample', 'sample mean', 'moment', 'mean', 'sample variance', 'variance']", Bootstrapping with Slowly  Winsorization,seg_145,"2 , gn, and kn(·) denote its sample mean, sample variance, sample third absolute central moment, and sample qf. let x̃n ≡ (x̃n1, . . . , x̃nn)′ denote the (kn, kn"
2060,1,['sample'], Bootstrapping with Slowly  Winsorization,seg_145,"′ )-w̃insorized sample,"
2061,0,[], Bootstrapping with Slowly  Winsorization,seg_145,for integers kn and kn
2062,1,"['sample', 'sample mean', 'moment', 'mean', 'population', 'sample variance', 'variance']", Bootstrapping with Slowly  Winsorization,seg_145,"′ /n. let x̃n, s̃n, g̃n, and k̃n denote the sample mean, sample variance, sample third central moment, and sample qf of the population x̃n. let xn"
2063,1,"['sample', 'bootstrap']", Bootstrapping with Slowly  Winsorization,seg_145,"∗ ≡ (xn ∗1, . . . , xn ∗n)′ denote the iid bootstrap sample from k̃n(·)"
2064,1,"['sample', 'sample mean', 'bootstrap', 'distribution', 'probability distribution', 'mean', 'probability', 'sample variance', 'variance']", Bootstrapping with Slowly  Winsorization,seg_145,∗2 be the sample mean and sample variance of the bootstrap sample x∗n. let pn∗ denote the bootstrap probability distribution. our rvs of interest are
2065,1,"['sample', 'sample mean', 'mean', 'sample variance', 'variance']", Bootstrapping with Slowly  Winsorization,seg_145,[we saw in the previous section that the sample mean and sample variance xn
2066,1,"['sample', 'bootstrap']", Bootstrapping with Slowly  Winsorization,seg_145,an iid bootstrap sample from kn are such that z̄n ≡ √n[xn
2067,1,"['sample', 'bootstrap']", Bootstrapping with Slowly  Winsorization,seg_145,"∗ − x̄n]/sn satisfies the strong (or the weak) bootstrap if and only if var[x] ∈ (0,∞) (or f ∈ d(normal)). but next we see the glories of winsorizing! winsorizing does do what winsorizing was supposed to do–in a w̃insorized sample. the bootstrap always works, provided that we winsorize at least a slowly increasing number.]"
2068,1,['bootstrap'], Bootstrapping with Slowly  Winsorization,seg_145,theorem 9.2 (universal strong bootstrap clt) suppose the w̃insorization fractions are such that (kn ∧ kn
2069,1,['conditional'], Bootstrapping with Slowly  Winsorization,seg_145,"′ ) = n(an ∧ a′n) → ∞ (as in notation 9.2). then uniformly in all xn for which s̃n > 0, we have that for a.e. xn, conditional on xn,"
2070,0,[], Bootstrapping with Slowly  Winsorization,seg_145,proof. this is immediate from the previous theorem.
2071,1,['bootstrap'], Bootstrapping with Slowly  Winsorization,seg_145,"remark 9.1 if we knew how to w̃insorize correctly in theorem 9.1, it would be a useful theorem. the point is, we do always know how to winsorize correctly in the bootstrap of theorem 9.2."
2072,1,"['sample', 'bootstrap', 'sampling', 'distributions']", Bootstrapping with Slowly  Winsorization,seg_145,"but should we instead do bootstrap sampling from the empirical qf kn itself, rather than k̃n, and then winsorize this sample? no! sampling from k̃n gives us the analog of theorem 9.1, while sampling from kn (it can be shown) does not. (sampling from kn could, however, be shown to work for any qf k in a very large class of distributions.)"
2073,1,['moments'], Bootstrapping with Slowly  Winsorization,seg_145,"n =1 xnk/n. compute vn, as well as the higher moments e|xnk|3 and exn"
2074,1,['condition'], Bootstrapping with Slowly  Winsorization,seg_145,"(a) show that zn ≡ √nx̄n/√vn →d n(0, 1) by verifying the lindeberg condition. (b) what conclusion does the berry–esseen theorem imply for zn? (c) show that xn"
2075,0,[], Bootstrapping with Slowly  Winsorization,seg_145,exercise 9.2 formulate and solve another example in the spirit of exercise 9.1.
2076,0,[], Bootstrapping with Slowly  Winsorization,seg_145,exercise 9.3 verify that sn
2077,1,['condition'], Bootstrapping with Slowly  Winsorization,seg_145,by verifying a lindeberg type condition in the context of theorem 10.5.1.
2078,1,['distributions'],,seg_149,1 infinitely divisible distributions o
2079,1,"['asymptotic', 'independent', 'condition']",,seg_149,"definition 1.1 (triangular arrays, and the uan condition) a triangular array is just a collection of rvs xn1, . . . , xnn, n ≥ 1, such that the rvs in the nth row are independent. call it a uan array if the uniform asymptotic negligibility condition holds, that is"
2080,1,['condition'],,seg_149,the uan condition is a natural one for preventing one term from dominating the whole sum.
2081,1,['limit'],,seg_149,(i) find the family of all possible limit laws of sn.
2082,1,['convergence'],,seg_149,(ii) find conditions for convergence to a specified law of this form.
2083,1,['results'],,seg_149,find specialized results for further restrictions on the uan array.
2084,1,['variances'],,seg_149,(a) suppose variances exist.
2085,1,"['poisson', 'normal', 'limit']",,seg_149,(b) suppose the limit law is normal or poisson.
2086,1,"['set', 'results', 'limit']",,seg_149,"some of the results in this chapter are stated with only indications of the proofs. the goal in this chapter is simply to develop some rough understanding of the subject. we will see in this section that the set of all possible limit laws of row sums sn ≡ xn1 + · · · + xnn of a uan array of xnk’s is exactly the class of infinitely divisible laws, which we now define."
2087,1,['distribution'],,seg_149,definition 1.2 (infinitely divisible) call both the rv y and its distribution infinitely divisible (id) if for every value of n it is possible to decompose y into n iid components as
2088,1,"['variance', 'distributions']",,seg_149,"we denote the class of all id distributions by i; the subclass with finite variance is denoted by i2. (we remark that the yni’s of this definition form a uan array, but this needs to be shown; note exercise 1.2.)"
2089,0,[],,seg_149,"exercise 1.1 (chf expansions for the uan array xn1 + · · · + xnn) consider a uan array of rvs xnk, as in (1). (a) let fnk and φnk denote the df and the chf of xnk. show that"
2090,1,['interval'],,seg_149,(3) [max1≤k≤n|φnk(t) − 1|] → 0 uniformly on every finite interval.
2091,0,[],,seg_149,[hint. integrate over |x| < and |x| ≥ separately to obtain
2092,0,[],,seg_149,from which point the result is minor.] we then define (as will be useful regarding an expansion of log (1 + (φnk(·) − 1)) below)
2093,0,[],,seg_149,"(b) verify the elementary fact that if xnk =∼ (0, σn"
2094,1,['interval'],,seg_149,"2k] → 0, then n(t) → 0 uniformly on each finite interval."
2095,1,"['independent', 'function']",,seg_149,motivation 1.1 (limits of uan arrays) suppose that sn ≡ xn1 + · · · + xnn for some row independent uan array. let fnk and φnk denote the df and chf of the rv xnk. then for the function n(·) of (4) we necessarily have for n sufficiently large (recall that a = b ⊕ c means that |a − b| ≤ c) that
2096,1,['variances'],,seg_149,"if we further assume that all the xnk’s have 0 means and finite variances, then we can rewrite expression (5) to obtain"
2097,1,['continuous'],,seg_149,"φ(x, t) is continuous on the (x, t)-plane with (7) each φ(·, t)bounded and continuous, and equal to − t2/2 at x = 0."
2098,0,[],,seg_149,"moreover, take the point of view that"
2099,1,['variance'],,seg_149,k=1fnk(y) = (the contribution to variance up to x).
2100,1,['distribution'],,seg_149,it is natural to hope that sn will converge in distribution to a rv y whose log chf is of the form
2101,1,['variances'],,seg_149,"when means and variances need not exist, we define"
2102,0,[],,seg_149,we then note that (5) can also be manipulated to give
2103,0,[],,seg_149,"where this new φ(x, t) still satisfies (7), and where we now define"
2104,1,['variance'],,seg_149,". (contribution to variance up to x),"
2105,1,['distribution'],,seg_149,"at least for x near 0, which is where all the action is in any uan array. it is natural to hope that sn will now converge in distribution to a rv y whose chf is of the form"
2106,0,[],,seg_149,"we are thus particularly interested in the behavior of kn and/or hn (and n(·)), both for the general uan xnk’s of (1) and for the special uan ynk’s of (2)."
2107,1,['distribution'],,seg_149,the next example enables us to show that any chf of the form (9) or (13) is the chf of some id distribution. the details are left to the easy exercise 1.3.
2108,1,"['poisson', 'compound', 'poisson distributions', 'distributions']",,seg_149,"example 1.1 (generalized poisson and compound poisson distributions) we suppose that the rvs xn1, . . . , xnn are iid with"
2109,1,['distribution'],,seg_149,thus the limiting distribution is that of y ≡ ∑j
2110,1,"['poisson', 'distribution', 'independent', 'poisson distribution']",,seg_149,j=1 ajyj for independent poisson(λj) rvs yj . this is called the generalized poisson distribution.
2111,0,[],,seg_149,note also that the chf on the right-hand side of (14) satisfies
2112,1,"['poisson', 'independent', 'multinomial', 'distribution', 'poisson distribution', 'poisson sum']",,seg_149,"j=1 λj , pj ≡ p (w = aj) = λj/λ for 1 ≤ j ≤ j, φw is the chf of w, and ∼ w1,w2, . . . are iid as w and n = poisson(λ). the distribution of the rv y is called the compound poisson distribution, and is distributed as a poisson sum of independent multinomial (k;λ1/λ, . . . , λk/λ) rvs."
2113,1,"['poisson', 'distribution', 'compound', 'poisson distribution']",,seg_149,"the compound poisson distribution of (15) is obviously id, as is clearly seen by using λ/n in place of λ in (15) for the iid ynk’s of (2). thus the generalized poisson distribution in (14) is also id. it is in the compound poisson format that we recognize that this distribution is id, but it will be in its generalized poisson format that we will put it to work for us."
2114,1,"['normal', 'limit']",,seg_149,"exercise 1.3 heuristics have suggested that if y is id, then its chf φ is of the form (13) (the reader should also note (29), where the normal component of the limit is removed). they have not yet suggested the converse. however,"
2115,1,['intervals'],,seg_149,"n 1ν/(ij)β(xj)/α(xj) and appropriate intervals ij (with xj , ij and λj all depending on n)"
2116,1,"['poisson', 'limit']",,seg_149,→ (a limit of generalized poisson rvs)
2117,1,['limit'],,seg_149,"(17) = (a limit of id rvs) = id,"
2118,1,['limit'],,seg_149,since the limit under →d of id rvs is also an id rv (as was stated in exercise 1.2(c)). [the present exercise is to make all this rigorous.]
2119,1,['variance'],,seg_149,"theorem 1.1 (kolmogorov’s representation theorem) let the rv y have chf φ and finite variance. we will use the symbol k to denote a generalized df with 0 = k(−∞) < k(+∞) < ∞. now, y is id if and only if"
2120,1,"['limit', 'variances']",,seg_149,"theorem 1.2 (bounded variances limit theorem for i2) (a) we start with a triangular array of row-independent rvs xnk =∼ (μnk, σn"
2121,0,[],,seg_149,(such a triangular array is necessarily uan). then
2122,1,[],,seg_149,where the limiting rv necessarily satisfies
2123,1,"['variance', 'limit']",,seg_149,"2 , and we will write kn →d k. (b) the family of all possible limit laws of such sn is the family i2 of all possible infinitely divisible laws that have finite variance."
2124,1,"['variance', 'mean']",,seg_149,"proof. (if one grants exercises 1.1 and 1.3, then the proof we give will be complete. these exercises are straightforward.) any chf of the form (18) or (21) is id, by exercise 1.3. differentiating twice in (21) shows that this chf has mean β and variance ∫−"
2125,1,"['functions', 'continuous', 'intervals', 'mean', 'variance']",,seg_149,"it remains to show that any id y with mean 0 and finite variance has a chf of the form (18) with β = 0. reconsider (6). for the special uan ynk’s of (2) we have var[ynk] = var[y ]/n, so that exercise 1.1(b) implies that n(t) → 0 uniformly on all finite intervals. moreover, the family of functions φ(·, t) in (7) are bounded and continuous functions that converge to 0 as |x| → ∞. applying the helly–bray exercise 11.1.1 to each φ(·, t) in (6) shows (for the first"
2126,0,[],,seg_149,"so we must show that kn →sd k. now, every subsequence n′ has a further n′′ on which kn"
2127,1,[],,seg_149,"′′ →sd (some k). but every resulting such ∫ φ(x, t)dk(x) with a limiting k inserted must equal log φy , and thus the first paragraph of this proof implies that k has 0 = k(−∞) < k(+∞) = var[y ]. in fact, all possible subsequential limits k must be equal, by the uniqueness of the representation in equation (21). thus kn →sd k on the whole sequence n. thus log φy = ∫ φ(x, t)dk(t). this completes the proof of theorem 1.1."
2128,1,"['intervals', 'hypothesis']",,seg_149,"we now turn to the proof of theorem 1.2. under the basic hypothesis (19), we have in (6) that n(t) → 0 uniformly on all finite intervals (by exercise 1.1(b)). thus whenever kn →sd (some k) and μn → μ, we have by applying helly–bray to each φ(·, t) in (7) that log φsn(t) → log φ(t) for each t, for the φ of (21) with β = μ. thus sn →d y for the id y with chf given by (21)."
2129,0,[],,seg_149,(using fatou and a skorokhod representation for which sn
2130,0,[],,seg_149,then kn →sd k reasonably becomes kn →d k.
2131,1,"['convergence', 'normal']",,seg_149,"example 1.2 (normal convergence) (i) (representation) the n(0, 1) chf φ has"
2132,0,[],,seg_149,if and only if
2133,1,"['poisson', 'convergence']",,seg_149,example 1.3 (poisson convergence) (i) (representation) the poisson(λ) chf φ has
2134,1,['convergence'],,seg_149,"(ii) (convergence) suppose the triangular array with xnk =∼ (μnk, σn"
2135,1,[],,seg_149,1xnk →d poisson(λ)
2136,0,[],,seg_149,if and only if
2137,1,"['poisson', 'independent', 'normal and poisson distributions', 'normal', 'poisson distributions', 'distributions']",,seg_149,"exercise 1.6 (decomposition of normal and poisson distributions) suppose that x ∼= x1+ x2, where x1 and x2 are independent i2 rvs. then:"
2138,1,['normal'],,seg_149,(27) x normal implies that x1 and x2 are both normal.
2139,1,['poisson'],,seg_149,(28) x poisson implies that x1 and x2 are both poisson.
2140,0,[],,seg_149,[this is also true if i replaces i2.]
2141,1,['results'],,seg_149,"from here to the end of this section we mainly just state results, mostly by analogy."
2142,0,[],,seg_149,theorem 1.3 (lévy–khinchin representation theorem) let y have chf φ. then y is infinitely divisible (id) if and only if
2143,1,"['normal', 'limit']",,seg_149,"for φ(x, t) as in (11) (and (7)). the representation is unique. (we write y =r (β,h) to denote this representation. we will think of iβt − σ2t2/2 as the normal component of the limit law.)"
2144,1,['limit'],,seg_149,theorem 1.4 (general limit theorem for i) let the rv’s xnk form a uan triangular array. then
2145,0,[],,seg_149,if and only if for some finite-measure generalized df h and for some real β we have
2146,1,"['limit', 'distributions']",,seg_149,"and where bnk ≡ exnk1(−δ,δ)(xnk), with δ > 0 arbitrary but fixed. the family of possible limit laws of such sn is the family i of all possible infinitely divisible distributions."
2147,1,['normal'],,seg_149,"theorem 1.5 (normal limits of uan arrays) (a) let sn ≡ xn1 + · · · + xnn for iid xnk’s, and suppose that sn →d s. then"
2148,1,['normal'],,seg_149,(34) mn →p 0 if and only if s is normal.
2149,1,['independent'],,seg_149,"(b) let sn ≡ xn1 + · · · + xnn for independent xnk’s, and suppose sn →d s. then"
2150,1,['normal'],,seg_149,(35) mn →p 0 if and only if s is normal and the xnk’s are uan.
2151,1,['interval'],,seg_149,"exercise 1.8 (a) show that all gamma(r, ν) rvs are infinitely divisible. (b) show that an infinitely divisible rv can not be concentrated on a finite interval."
2152,1,"['asymptotic', 'normality', 'condition']",,seg_149,exercise 1.9 use theorem 1.4 to prove the asymptotic normality condition of theorem 10.5.1.
2153,1,[],,seg_149,exercise 1.11 use chfs to determine necessary and sufficient conditions under which the wlln will hold.
2154,1,['cases'], Stable Distributions o,seg_151,"then f is said to belong to the domain of attraction of g, and we write f ∈ d(g). we also say that g possesses a domain of attraction. [if var[x] < ∞ above, then necessarily (sn − nex)/√n →d n(0,var[x]) by the ordinary clt; thus the only new and interesting cases have ex2 = ∞.]"
2155,0,['n'], Stable Distributions o,seg_151,definition 2.2 (stable law) call a df g stable if for all n there exist constants an > 0 and bn with
2156,0,[], Stable Distributions o,seg_151,we call g strictly stable if we may take all bn = 0 in (2).
2157,1,['distribution'], Stable Distributions o,seg_151,"theorem 2.1 (only stable dfs have domains of attraction) a df g will possess a domain of attraction if and only if g has a stable distribution. moreover, the an of (2) must satisfy"
2158,1,['results'], Stable Distributions o,seg_151,"we call α the characteristic exponent of g. [to compare with section c.4 results for r−β with β > 0, we define"
2159,1,['normal'], Stable Distributions o,seg_151,"definition 2.3 (basic domain of attraction) the df f is said to belong to the basic domain of attraction of g (or the domain of normal attraction of g), which is denoted by writing f ∈ dn (g), provided an = (constant)×n1/α works in (1)."
2160,1,['normal'], Stable Distributions o,seg_151,"(thus there is a domain of attraction for the normal type, but this is not so for a particular normal df.)"
2161,0,[], Stable Distributions o,seg_151,"suppose g possesses a domain of attraction. thus there exists x1,x2, . . . iid f where f ∈ d(g). hence for some an > 0 and some bn we have"
2162,0,[], Stable Distributions o,seg_151,which can be rewritten in the more useful format
2163,1,"['states', 'convergence']", Stable Distributions o,seg_151,let k be fixed. recall that the convergence of types theorem states that if
2164,0,[], Stable Distributions o,seg_151,"from (f), we see that g is stable. this completes the proof of the first statement."
2165,0,['e'], Stable Distributions o,seg_151,now we further exploit equation (e). from it we determine that
2166,1,['symmetric'], Stable Distributions o,seg_151,"′ , where y, y ′, y1, y1′, . . . are iid as g. then the rvs z and zn are symmetric, and (1) shows that"
2167,0,[], Stable Distributions o,seg_151,thus for some x > 0 fixed we have
2168,1,['convergence'], Stable Distributions o,seg_151,"was shown in (a). thus exercise 2.1 below shows that an = n1/α for some α > 0. suppose that α > 2. then var[y ] < ∞ by exercise 2.2 below. we can thus claim that √n(ȳ −μ) →d n(0, 1) by the ordinary clt. thus an = √n and ak = √k work above. by the convergence of types theorem, there are no other choices. that is, when α > 2, then only α = 2 works. (see breiman (1968, p. 202).)"
2169,1,['moments'], Stable Distributions o,seg_151,exercise 2.2 (moments) suppose that y ∼= g is stable with characteristic exponent α. then
2170,0,['n'], Stable Distributions o,seg_151,"[hint. use the inequalities of section 8.3 to show that np (|x| > anx) is bounded in n, where an ≡ n1/α. then bound the appropriate integral.]"
2171,0,['n'], Stable Distributions o,seg_151,"exercise 2.3 (strictly stable dfs) suppose that g is stable with characteristic exponent α = 1. then there is a number b such that g(·+ b) is strictly stable. [hint. show that b must satisfy b′n ≡ bn + (an − n)b = 0 for all n, and specify b such that b′2 = 0. or else b′2 = b = 0 immediately.]"
2172,0,[], Stable Distributions o,seg_151,"example 2.1 (hitting time as a stable law) watch brownian motion s until it first attains height a. the time this takes is denoted by ta. according to the strong markov property,"
2173,1,"['functions', 'covariance']", Stable Distributions o,seg_151,"checking the covariance functions, we see that s(a2·)/a =∼ s on [0,∞), and thus ta =∼ a2t1. putting these last two equations together gives"
2174,1,['normal'], Characterizing Stable Laws o,seg_153,"∼ theorem 3.1 (general stable chfs) suppose that y = g is stable. then either y is a normal rv or there is a number 0 < α < 2 and constants m1,m2 ≥ 0 and β for which"
2175,1,"['parameters', 'location', 'skewness']", Characterizing Stable Laws o,seg_153,"with c, d, θ as above. in fact, θ = (m1−m2)/(m1+m2) measures skewness, while the constants d and (1/c)(1/α) are just location and scale parameters. then let p ≡ m1/(m1 + m2) = (1 + θ)/2."
2176,1,"['distribution', 'symmetric']", Characterizing Stable Laws o,seg_153,corollary 1 (symmetric stable chfs) φ(·) is the chf of a nondegenerate and symmetric stable distribution with characteristic exponent α if and only if
2177,0,[], Characterizing Stable Laws o,seg_153,"∼ proof. we give only a direct proof of the corollary. this keeps things simple. let y = g be strictly stable with chf φ and let ψ ≡ log φ. since sn =∼ any , we have φn(t) = φ(ant). thus (modulo 2πi)"
2178,0,[], Characterizing Stable Laws o,seg_153,thus for all rationals r > 0 we have shown that
2179,1,['set'], Characterizing Stable Laws o,seg_153,"and by continuity, (5) also holds for all real r > 0. set t = 1 and r = τα in (5) for"
2180,0,[], Characterizing Stable Laws o,seg_153,we can summarize the two equations in (c) as
2181,1,['symmetric'], Characterizing Stable Laws o,seg_153,"since g is symmetric, φ is real, and so c2 = 0. thus (5) holds. all that remains is to be sure that φ is a valid chf. this follows from the next two exercises."
2182,0,[], Characterizing Stable Laws o,seg_153,∼ exercise 3.1 suppose that x = f with chf φ that satisfies
2183,1,['uniformly distributed'], Characterizing Stable Laws o,seg_153,"regard zn as the sum of forces exerted on a unit mass at the origin by n stars of mass m that are uniformly distributed on (−n, n) in a universe where an inverse pth power of attraction is operating. show that zn →d z, where the chf of z is given by φz(t) = exp(−c|t|α) for appropriate c and α."
2184,1,"['distribution', 'convergence', 'results']", The Domain of Attraction of a Stable Law o,seg_155,we now merely state some results that assert when convergence in distribution to a general stable law takes place.
2185,0,[], The Domain of Attraction of a Stable Law o,seg_155,(or equivalently)
2186,0,[], The Domain of Attraction of a Stable Law o,seg_155,"moreover, α and p determine g up to type, as follows from the theorem of types. (b) the constants an of (sn − bn)/an →d y ∼= g necessarily satisfy (according to the theorem of types)"
2187,1,['varying'], The Domain of Attraction of a Stable Law o,seg_155,(4) nu(an)/a2n ∼ na−n αl(an) → 1 for l(·) slowly varying at ∞.
2188,0,[], The Domain of Attraction of a Stable Law o,seg_155,"(c) the following are equivalent (for some constant 0 < α < 2, and then for some c > 0 and some 0 ≤ p ≤ 1):"
2189,0,[], The Domain of Attraction of a Stable Law o,seg_155,(5) f ∈ dn (g) for some stable g with characteristic exponent α.
2190,1,"['varying', 'normal']", The Domain of Attraction of a Stable Law o,seg_155,"theorem 4.2 (domain of attraction of the normal) f ∈ d(normal) if and only if either (hence, both) u(·) is slowly varying at ∞ or v (·) is slowly varying at 0. (theorem 10.6.1 and proposition 10.6.1 give a myriad of other equivalences.)"
2191,1,[], The Domain of Attraction of a Stable Law o,seg_155,exercise 4.1 use section c.4 to show that for 0 < α < 2 the following are equivalent conditions:
2192,1,"['functions', 'results', 'varying']", The Domain of Attraction of a Stable Law o,seg_155,other characterizations in terms of k can be found in or derived from theorem c.4.2. the theorems and this remark can also be proved via the lévy–khinchin theorem and results about regularly varying functions.
2193,1,"['cauchy', 'symmetric', 'function', 'tail', 'varying']", The Domain of Attraction of a Stable Law o,seg_155,exercise 4.2 (a) state necessary and sufficient conditions on f for f ∈ d(cauchy). (b) do the same for f ∈ dn (cauchy). (c) show by example that dn (cauchy) is a proper subset of d(cauchy). (d) observe that a symmetric df f (·) is in d(cauchy) if and only if the tail function defined by xp (x > x) = x(1 − f (x)) is slowly varying. (recall the tail function τ(.) of feller used in the wlln in (8.4.2).)
2194,1,['normal'], The Domain of Attraction of a Stable Law o,seg_155,"exercise 4.3 (a) show by example that the domain of normal attraction of the normal law dn (n(0, 1)) is a proper subset of the domain of attraction of the normal law d(n(0, 1)). to this end, let x1,x2, . . . be iid with density"
2195,0,['n'], The Domain of Attraction of a Stable Law o,seg_155,"and consider sn/(√n log n). (b) give a second example that works. (c) for both examples, determine an an that works."
2196,1,['normal'], The Domain of Attraction of a Stable Law o,seg_155,(11) f ∈ dn (normal) if and only if σ2 < ∞.
2197,1,"['cumulants', 'normal approximation', 'independent', 'gamma', 'approximation', 'symmetric', 'skewed', 'distribution', 'normal', 'limit']", Gamma Approximation ,seg_157,"if the underlying summands xnk in a clt approximation are symmetric, then a normal approximation may seem particularly appropriate. but what if the underlying distribution is positively skewed? (if x is negatively skewed, we just consider −x instead.) consider the rv gr ≡ [gamma(r) − r]/√r ∼= (0, 1), where r is chosen to make the third cumulants match. might this not give a better approximation for small n? and since gr →d n(0, 1) as r → ∞, there is no contradiction at the limit. we will show that this recipe works. the fact that the sum of independent gammas is again gamma is crucial to the technical details of the proof."
2198,1,"['cumulants', 'independent']", Gamma Approximation ,seg_157,"can we go one step further and match the first four cumulants? yes, because sums of independent rvs that are distributed as “gamma-gamma” again belong to the same family. [this will also work within our ability to match up the cumulants.]"
2199,1,"['cumulants', 'gamma', 'kurtosis', 'standardized', 'cumulant', 'skewness', 'tail']", Gamma Approximation ,seg_157,"the skewness γ1 ≡ μ3/σ3 = e(x − μ)3/σ3 is defined to be the third cumulant of the standardized rv, and the tail heaviness (or kurtosis) is the fourth such cumulant defined by γ2 ≡ μ4/σ4 − 3 = e(x − μ)4/σ4 − 3. we will use these formulas with μ = 0 and σ2 = 1, so that the skewness becomes μ3 and the tail heaviness becomes μ4 − 3. for the standardized gamma, the first four cumulants are given by"
2200,1,"['independent', 'set']", Gamma Approximation ,seg_157,"for the difference of two independent gammas we let p + q = 1 and c, d > 0 and set u ≡ p/c and v ≡ q/d, and then further define r = c2n and s = d2n and set"
2201,1,[], Gamma Approximation ,seg_157,"[this parameterization can match all (μ3, μ4) pairs for which μ23 ≤ 2"
2202,1,"['normal distributions', 'normal', 'tails', 'distributions']", Gamma Approximation ,seg_157,"3 (μ4 − 3), all of which have heavier tails than normal distributions.]"
2203,1,"['cumulants', 'approximation', 'gamma']", Gamma Approximation ,seg_157,"theorem 5.1 (gamma approximation; the glt) let xn1, . . . , xnn be iid as an x having df f with cumulants (0, 1;μ3, μ4 − 3), where μ3 ∈ (0,∞), so that"
2204,1,['results'], Gamma Approximation ,seg_157,"assume either (4)(a) for the df results below or (4)(b) for the density results, where:"
2205,1,['limit'], Gamma Approximation ,seg_157,"(ii) suppose r and s can be specified so gr,s =∼ (0, 1;μ3/√n, (μ4 − 3)/n). then n3/2 can replace n in (5) when μ5 < ∞. and n can replace √n in (6) when μ4 < ∞. (iii) the density gr(·) of gr(·) may replace the n(0, 1) density in the local limit theorems of section 10.3."
2206,1,['distribution'], Gamma Approximation ,seg_157,proof. we initially approximate the distribution of zn by that of
2207,1,['cumulants'], Gamma Approximation ,seg_157,"√2 √2 1 1 ∼= (0, 1; 0, 0) + (0, 1; 2/√r̄, 6/r̄) = (0, 1; 1/√2r̄, 3/2r̄) √2 √2 (b) =∼ (0, 1;μ3/√n, 3μ23/n) matching (3) to three cumulants"
2208,1,['independent'], Gamma Approximation ,seg_157,"where the nk =∼ n(0, 1) and the wk =∼ [gamma(a)−a]/√a with a = 1/(2μ23) are independent. let φy (t) ≡ eeity and ψy ≡ log φy , with φx and ψx defined analogously. then"
2209,1,"['cumulants', 'inequality']", Gamma Approximation ,seg_157,here (provided that |t|/√n is sufficiently small for the expansion of (9.6.22) to be valid) the inequality (9.6.22) then gives (since the first three cumulants of x and y match)
2210,1,"['approximation', 'normal', 'exponential']", Gamma Approximation ,seg_157,"where c and d depend on the β2 value of (x − μ)/σ. [the specification in (a) that z̄n has a normal component is not natural or practically useful, but it delivers the technically lovely exponential bound component exp(−t2/4) in both (d) and (e). since (a) is not useful for a practical approximation, we will overcome this objection by doing the approximation (a) again—to difference it out. (i believe this whole approach may be new.)]"
2211,1,['cumulants'], Gamma Approximation ,seg_157,"(k) =∼ (0, 1;μ3/√n, (3/2)μ23/n), matching (3) to three cumulants."
2212,0,[], Gamma Approximation ,seg_157,we now approximate gr by the z̄n of (a) [just as earlier we approximated zn by the z̄n of (a)]. this gives (with generic constants c and d)
2213,0,[], Gamma Approximation ,seg_157,now (7) bounds the integrand of the lead term to give
2214,0,[], Gamma Approximation ,seg_157,"since ∫ |φx(t)|dt < ∞ by (4)(b), the density inversion formula (9.4.9) shows that x has a density fx(·). since x is thus not distributed on a grid (and likewise y ), proposition 9.8.2 gives"
2215,0,[], Gamma Approximation ,seg_157,thus the second term in (m) satisfies
2216,1,[], Gamma Approximation ,seg_157,"since the θn term goes to 0 geometrically. likewise, i3n = o(n−r), for any r > 0, since |φg|k satisfies (4)(b), for some k. combine (n) and (o) into (m) to get (5)(b)."
2217,0,[], Gamma Approximation ,seg_157,consider (5)(a). we will apply esseen’s lemma. thus
2218,1,"['function', 'convergence']", Gamma Approximation ,seg_157,"consider (6), when μ4 is not assumed finite. use of (9.6.22) at line (f) must be replaced by use of (9.6.21). the |t|3e|x|3 in (9.6.22) would give a bound of only cf /√n at line (m) of the current proof; but the added δ3(t/√n) term in (9.6.21) that is valid on |t| ≤ d√n (now with a tiny d) allows a cf to be replaced by a cf,n → 0. dominated convergence is used for this, with dominating function guaranteed by e|x|3 < ∞."
2219,1,"['cumulants', 'parameters', 'approximation', 'kurtosis', 'convolution', 'skewness']", Gamma Approximation ,seg_157,"if we knew any appropriate two-parameter family closed under convolution, we could choose those two parameters to match both third and fourth cumulants. then cf /n3/2 under μ5 < ∞ and cf,n/n under μ4 < ∞ would be possible. the proof is essentially unchanged, and needs no further comment. the difference of two gammas can be specified in several different ways. all work. the only question is which has the greatest coverage of the (skewness, kurtosis)-plane. using gammas, we seem stuck with positive kurtosis, which leaves out some of the least important situations. [edgeworth expansions allow us to cover the whole (skewness, kurtosis)-plane, but they have some other deficiencies. for instance, the edgeworth approximation to a df or density is not necessarily a df or density itself.]"
2220,1,['approximation'], Gamma Approximation ,seg_157,poisson approximation
2221,1,"['gamma', 'case', 'discrete', 'discrete distributions', 'continuity correction', 'distributions']", Gamma Approximation ,seg_157,"most discrete distributions we care about live on the integers, and the write-up here will reflect that fact and make this case fit our notation with the least effort. rather than approximating sums of such rvs x by an appropriate gamma with a continuity correction, we will use a nice discrete analogue of the gamma."
2222,1,"['poisson', 'cumulants', 'standardized']", Gamma Approximation ,seg_157,"for the standardized poisson, the first four cumulants are"
2223,1,[], Gamma Approximation ,seg_157,"(8) gr ≡ poisson(r) − r 0, 1; 1 , 1 ."
2224,1,"['set', 'poissons']", Gamma Approximation ,seg_157,"for the difference of two poissons we let p + q = 1 and c, d > 0 and set u ≡ p/c and v ≡ q/d, and then define r = c2n and s = d2n and set"
2225,1,"['poisson', 'approximation']", Gamma Approximation ,seg_157,"theorem 5.2 (poisson approximation) consider a rv x on the integers and let zn be as in (3). let r ≡ n/μ32, so that"
2226,1,['standardized'], Gamma Approximation ,seg_157,"(i) then for some constants cf and cf,n → 0 (that may depend on the df of the standardized rv (x − μ)/σ):"
2227,1,['probabilities'], Gamma Approximation ,seg_157,"[most probabilities that one computes involve summing over the appropriate m√n number of terms that are each of the type pzn(·).] (ii) suppose r and s can be specified so that gr,s =∼ (0, 1;μ3/√n, (μ4 − 3)/n). then n2 can replace n3/2 in (10), provided that μ5 < ∞. and n3/2 can replace n in (11), provided that μ4 < ∞."
2228,1,['distribution'], Gamma Approximation ,seg_157,proof. the appropriate inversion formula now (for a distribution on the grid am + b) is given by
2229,1,['normal'], Gamma Approximation ,seg_157,"by the previous proof (including the previous step (a) normal component, but now appearing in step (u)) yields"
2230,0,[], Gamma Approximation ,seg_157,for c and d that may depend on the df of (x − μ)/σ. applying the inversion formula in (12) now gives
2231,0,[], Gamma Approximation ,seg_157,"as is now easily shown with the same arguments as before (because θ < 1, since π√n never reaches a full period of φx ; recall proposition 9.8.2)."
2232,1,"['poisson', 'moment', 'distribution']", Gamma Approximation ,seg_157,"exercise 5.3 we can replace the poisson by the negbit (r, p) distribution with the moment structure"
2233,1,"['poisson', 'numerical']", Gamma Approximation ,seg_157,this is probably more useful than the previous theorem. (a) verify the claim. (b) provide some numerical work to compare poisson and negbit approximations to a situation of interest.
2234,1,"['poisson', 'gamma', 'approximation', 'distribution', 'probability distribution', 'probability']", Gamma Approximation ,seg_157,"remark 5.1 (gamma approximation or edgeworth approximation?) in the next section we will derive the classical edgeworth approximations. the first-order gamma (or poisson, or negbit) approximations of the current section are of the same order as the first-order edgeworth approximations. moreover, approximation by the gr-distribution is an approximation by a probability distribution; but this is not true of the edgeworth approximation. happily, gamma approximations are easily and accurately implemented in s-plus, say."
2235,1,"['cumulants', 'statistician', 'set']", Gamma Approximation ,seg_157,"the situation is similar regarding the two second-order approximations, provided that the first two cumulants of the underlying rv can be matched within the family of grs-distributions. however, the grs-distributions are not available within any set of computer-generated routines i know, so that this would be hard to implement at present. however, this would seem to make a nice project for a computer-oriented statistician."
2236,1,"['sample', 'cumulants', 'gamma', 'approximation', 'random sample', 'sampling', 'distribution', 'population', 'random', 'sampling distribution']", Gamma Approximation ,seg_157,"example 5.1 (sampling distribution of x̄n and s2n) suppose x1, . . . , xn is a random sample from a population whose first four cumulants are (μ, κ2;κ3, κ4). [let k&s denote kendall and stuart (1977, vol. i).] how do we apply a gamma approximation? (a) consider first an infinite population, in which κ2 = σ2, κ3 = μ3, and κ4 = μ4 − 3. then (9.6.20) gives the first four cumulants of √n(x̄n − μ) as"
2237,1,"['unbiased estimators', 'estimators', 'unbiased']", Gamma Approximation ,seg_157,"now, unbiased estimators κ̂j of these κj are given (see k&s (p. 297, 300)) by"
2238,1,['distribution'], Gamma Approximation ,seg_157,"1 (xj −x̄n)j/n. we will combine these facts with the theorems of the previous sections to approximate the distribution of √n(x̄n −μ). additionally (by k&s (p. 306–307),"
2239,1,"['unbiased estimator', 'skewness', 'variance', 'estimator', 'unbiased']", Gamma Approximation ,seg_157,where correcting for skewness in (16) should probably be ignored. an unbiased estimator of the variance in (16) (unbiasedness verifiable from k&s (p. 296)) is
2240,1,"['distribution', 'bootstrap']", Gamma Approximation ,seg_157,(c) in approximating the bootstrap distribution of xn
2241,0,[], Gamma Approximation ,seg_157,"∗, it is exactly true that"
2242,1,"['unbiased estimators', 'estimators', 'population', 'unbiased']", Gamma Approximation ,seg_157,"(d) now consider a finite population x1, . . . , xn whose second, third, and fourth true cumulants kj are given by (15), with n replacing n. unbiased estimators k̂j are also given by (15), now with n again (see k&s (p. 320)). it is also true (by k&s (p. 321–322)) that"
2243,1,"['unbiased estimator', 'results', 'variance', 'estimator', 'unbiased']", Gamma Approximation ,seg_157,"then (by k&s (p. 323)) an unbiased estimator of this last variance is given by (17) (with kj replacing κ̂j). (though straightforward, the results cited from k&s are somewhat cumbersome.)"
2244,1,['noncentral'], Gamma Approximation ,seg_157,example 5.2 (hall) a noncentral chisquare rv χn2 (δ) satisfies
2245,1,['distribution'], Gamma Approximation ,seg_157,so we approximate this distribution by gr with r ≡ (n + 2δ)3/(2(n + 3δ)2). then
2246,1,[], Gamma Approximation ,seg_157,(23) p (χ2n(δ) ≤ x) = p (gamma(r) ≤ r + [x − (n + δ)](n + 2δ)/2(n + 3δ))
2247,1,['percentage'], Gamma Approximation ,seg_157,"where γα denotes the upper 1 − α percentage point of gamma(r). this is easy to implement in splus, for example. (hall found that the accuracy seemed quite good, especially in relation to previous proposals.)"
2248,1,"['poisson', 'approximation', 'binomial']", Gamma Approximation ,seg_157,"exercise 5.4 (poisson approximation of the generalized binomial) we suppose xn1, . . . , xnn"
2249,1,"['independent', 'distributions']", Gamma Approximation ,seg_157,∼ ∼ are independent rvs with xnk = bernoulli(pnk). suppose further that ynk = poisson(pnk) are independent for 1 ≤ k ≤ n. let pn and qn denote the distributions of xn ≡ ∑n
2250,1,['variation'], Gamma Approximation ,seg_157,1 ynk. show that the total variation distance between pn and qn satisfies
2251,1,['bernoulli'], Gamma Approximation ,seg_157,"if pnk = λk/n for 1 ≤ k ≤ n, then the bound becomes λ2/n. [hint. the first step is to replace the original bernoulli(pnk) rvs by different bernoulli"
2252,1,['independent'], Gamma Approximation ,seg_157,"∼ (pnk) rvs, to be denoted by xnk also. to this end we now define the new znk = bernoulli(1− (1 − pnk)epnk) rvs that are independent for 1 ≤ k ≤ n (and they are also independent of the ynk’s). now define"
2253,1,['jointly'], Gamma Approximation ,seg_157,"and verify that it is indeed a bernoulli(pnk) rv. (this choice of the jointly distributed pair (xnk, ynk) maximizes the mass on the diagonal x = y of an (x, y)-coordinate system.) now verify that"
2254,0,[], Gamma Approximation ,seg_157,"this type of proof is called a coupling proof, in that the (xnk, ynk) pairs are coupled together as closely as possible.]"
2255,1,[], Edgeworth Expansions ,seg_159,the setup
2256,0,[], Edgeworth Expansions ,seg_159,on the real line. these are related via the inversion formula for chfs as
2257,0,[], Edgeworth Expansions ,seg_159,"and in general,"
2258,0,[], Edgeworth Expansions ,seg_159,defines what we will call the kth hermite orthogonal polynomial hk (see exercise 6.1). equating the derivatives in (3) to derivatives of the right-hand side of (2) gives
2259,1,"['transform', 'fourier transform']", Edgeworth Expansions ,seg_159,which expresses hkf0 as the inverse fourier transform of (it )kφ0(t). this gives the key result that
2260,1,"['transform', 'fourier transform']", Edgeworth Expansions ,seg_159,(6) (it)kφ0(t) is the fourier transform of hk(·)f0(·)
2261,0,[], Edgeworth Expansions ,seg_159,"now suppose that x1, . . . , xn are iid where"
2262,1,"['approximation', 'error']", Edgeworth Expansions ,seg_159,"the idea is to expand fn in terms of the orthogonal polynomials hk. however, we choose instead to obtain a first-order or second-order approximation, together with an error analysis. also,"
2263,1,['case'], Edgeworth Expansions ,seg_159,in this latter case we will also seek to expand fn. the expansions we will derive for fn and fn are known as edgeworth expansions.
2264,1,['densities'], Edgeworth Expansions ,seg_159,edgeworth expansions for densities
2265,0,[], Edgeworth Expansions ,seg_159,"instead of just assuming that fn exists, we assume instead that the chf φ of the rv x =∼ (0, σ2) satisfies"
2266,0,['n'], Edgeworth Expansions ,seg_159,"this guarantees both that fn exists for all n ≥ m, and that it can be found from the fourier inversion formula (9.4.9)."
2267,1,"['skewness', 'tail', 'condition']", Edgeworth Expansions ,seg_159,"theorem 6.1 suppose condition (10) holds. let γ1 ≡ e(x/σ)3 denote the skewness, and let γ2 ≡ e(x/σ)4 − 3 denote the tail heaviness of x =∼ f (0, σ2). (a) then"
2268,1,['function'], Edgeworth Expansions ,seg_159,(11) = o(1/√n) (as a function of e|x/σ|3 and fx/σ) [if e|x|3 < ∞]
2269,1,['function'], Edgeworth Expansions ,seg_159,(12) = o(1/n) (as a function of e|x/σ|4 and fx/σ) [if ex4 < ∞].
2270,1,['function'], Edgeworth Expansions ,seg_159,2 h6(·)]}∥∥ ∥∥ (13) = o(1/n) (as a function of e|x/σ|4 and fx/σ) [if ex4 < ∞]
2271,1,['function'], Edgeworth Expansions ,seg_159,(14) = o(1/n3/2) (as a function of e|x/σ|5 and fx/σ) [if e|x|5 < ∞].
2272,0,[], Edgeworth Expansions ,seg_159,we specifically write out that h0(x) ≡ 1 and
2273,1,"['densities', 'condition', 'set', 'control', 'tails']", Edgeworth Expansions ,seg_159,"for use in the current set of theorems. the previous theorem was for densities. the next is for dfs. condition (10) is used to control the extreme tails in the fourier inversion formulas for densities. in proving analogues of (11) and (12) for dfs, we will be able to use esseen’s lemma to control these tails instead. however, the analogues of (13) and (14) run into other problems, and these are again overcome via a (now weaker) restriction on φ. all proofs are at the end of this section."
2274,1,"['functions', 'distribution', 'hypothesis']", Edgeworth Expansions ,seg_159,edgeworth expansions for distribution functions consider the potential hypothesis
2275,1,['continuous'], Edgeworth Expansions ,seg_159,"(this is weaker than (10). the riemann–lebesgue lemma shows that (16) holds if f has an absolutely continuous component, à la theorem 6.1.1.)"
2276,1,"['skewness', 'tail']", Edgeworth Expansions ,seg_159,"theorem 6.2 suppose that x is not distributed on a grid. let γ1 ≡ e(x/σ)3 denote the skewness, and let γ2 ≡ e(x/σ)4 − 3 denote the tail heaviness of the rv x =∼ f (0, σ2). (a) then"
2277,1,['function'], Edgeworth Expansions ,seg_159,1 nh2(·)}∥∥ ∥∥ (17) = o(1/√n) (as a function of e|x/σ|3 and fx/σ) [if e|x|3 < ∞]
2278,0,[], Edgeworth Expansions ,seg_159,or (additionally requiring (16) for (18))
2279,1,['function'], Edgeworth Expansions ,seg_159,(18) = o(1/n) (as a function of e|x/σ|4 and fx/σ) [if ex4 < ∞].
2280,1,['function'], Edgeworth Expansions ,seg_159,∥∥ ∥∥ (19) = o(1/n) (as a function of e|x/σ|4 and fx/σ) [if ex4 < ∞]
2281,1,['function'], Edgeworth Expansions ,seg_159,(20) = o(1/n3/2) (as a function of e|x/σ|5 and fx/σ) [if e|x|5 < ∞].
2282,1,['function'], Edgeworth Expansions ,seg_159,"exercise 6.2 let zn ≡ sn/σ√n as above, and let fn denote its density under (10). let zn ∗ ≡ [gamma(r) − r]/√r, with r ≡ 4n/γ12, and let gn(·) denote its density. show that ‖fn − gn‖ = o(1/n) as a function of e|x/σ|4 and fx/σ when e|x|4 < ∞."
2283,1,"['cumulants', 'standardized', 'cumulant', 'function']", Edgeworth Expansions ,seg_159,"we defined and expanded the cumulant generating function ψ(·) ≡ log φ(·) in exercise 9.6.6. the first few cumulants of the standardized rv x/σ were seen to be 0, 1, γ1, and γ2."
2284,1,['loss'], Edgeworth Expansions ,seg_159,proof. consider theorem 6.1(a). without loss of generality we suppose that σ = 1. we now agree that
2285,1,"['approximation', 'transform', 'fourier transform']", Edgeworth Expansions ,seg_159,denotes the difference between the true fn and our first approximation to it. note from (6) that dn has fourier transform
2286,0,[], Edgeworth Expansions ,seg_159,thus the fourier inversion formula (9.4.9) gives
2287,1,"['function', 'associated']", Edgeworth Expansions ,seg_159,"where δ(·) denotes the function δ3(·) function of (9.6.21) associated with the rv x/σ. using (h) and (i), the bound in (e) becomes (for some θ small enough)"
2288,1,['results'], Edgeworth Expansions ,seg_159,"since a tiny δ(θ) results from a sufficiently tiny θ. thus (11) holds. for (12), we replace the bound in line (i) above by"
2289,0,[], Edgeworth Expansions ,seg_159,"we now turn to (13), and then (14). we first redefine"
2290,1,"['transform', 'fourier transform']", Edgeworth Expansions ,seg_159,"taking the inverse of its fourier transform φn(·) gives (as in (e)) that for any fixed value of θ > 0 and all x,"
2291,0,[], Edgeworth Expansions ,seg_159,the final details are nearly the same as before.
2292,0,[], Edgeworth Expansions ,seg_159,exercise 6.3 finish the details of the previous proof of theorem 6.1(b).
2293,0,[], Edgeworth Expansions ,seg_159,proof. consider theorem 6.2(a). we note that
2294,1,['set'], Edgeworth Expansions ,seg_159,"where φn is the same φn appearing in (21). since the norm in the second term on the right of (r) is bounded, the second term in (r) is less that /√n whenever a ≡ a( , fx/σ) is chosen large enough. fix this a in the limits of integration of (r), and then break this integral into two pieces: the integral over [|t| ≤ θ√n/e|x|3] with θ as in (i), and the integral over [|t| > θ√n/e|x|3]. the integral over the set [|t| > θ√n/e|x|3] is o(n−r), for any r > 0 (à la (11.5.6), as before at line (e)). finally, the value of the integral over the set [|t| ≤ θ√n/e|x|3] is bounded by a term like the right-hand side of (j) (in which |t|3 and t6 are replaced in those integrals by t2 and |t|5, to account for division by |t| in the integrand of (r)). this completes the proof of (17) when x is not distributed on a grid. for (18), the initial region of integration in (r) must be [|t| ≤ an/ex4], and then an/ex4 will also appear below the norm term. moreover, we will now use θ for a, since only o(1/n) is required."
2295,0,[], Edgeworth Expansions ,seg_159,consider theorem 6.2(b). we note that
2296,0,[], Edgeworth Expansions ,seg_159,and this is the same dn as in (m) of the previous proof. thus the final details are nearly the same as before.
2297,0,[], Edgeworth Expansions ,seg_159,exercise 6.4 complete the details in the previous proof of theorem 6.2(b).
2298,1,"['results', 'moments', 'case']", Edgeworth Expansions ,seg_159,exercise 6.5 consider a non-iid case in which all dfs fnk have third and/or fourth moments that are of the same order. then all of the previous results still obtain.
2299,1,"['moment', 'moment generating function', 'deviations', 'function']", Edgeworth Expansions ,seg_159,"exercise 6.6 (large deviations) suppose the moment generating function (or mgf) mx(t) ≡ eetx of the rv x is finite for 0 ≤ |t| < . let x1,x2, . . . be iid (0, σ2). let fn(·) denote the df of √n(x̄n − μ) and let f0(·) denote the n(0, 1) df. show that"
2300,1,[], Special Spaces,seg_163,general metric spaces
2301,1,['convergence'], Special Spaces,seg_163,"let (m,d) denote an arbitrary metric space and let md denote its borel σ-field (that is, the σ-field generated by the collection of all d-open subsets of m). let mdb denote the σ- field generated by the collection of all open balls, where a ball is a subset of m of the form {y : d(y, x) < r} for some x ∈ m and some r > 0; call this the baire σ-field. [the important concept of weak convergence is best described in the context of metric spaces.]"
2302,1,[], Special Spaces,seg_163,"(1) mdb = md if (m,d) is a separable metric space."
2303,1,['functions'], Special Spaces,seg_163,"for functions x, y on [0, 1], define the uniform metric (or supremum metric) by"
2304,1,"['functions', 'continuous', 'set']", Special Spaces,seg_163,"let c denote the set of all continuous functions on [0, 1]. then"
2305,1,[], Special Spaces,seg_163,"(3) (c, ‖ ‖) is a complete and separable metric space."
2306,1,"['functions', 'continuous', 'cases', 'set']", Special Spaces,seg_163,"let d denote the set of all functions on [0, 1] that are right continuous and possess left-hand limits at each point. (in some applications below it will be noted that d is also used to denote the set of all left-continuous functions on [0, 1] that have right-hand limits at each point. this point will receive no further mention. in some cases we will admit to d, and/or to c, only"
2307,1,['case'], Special Spaces,seg_163,"functions x having x(0) = 0, etc. this, too, will receive little, if any, further mention.) in any case"
2308,1,[], Special Spaces,seg_163,"(5) (d, ‖ ‖) is a complete metric space that is not separable."
2309,0,[], Special Spaces,seg_163,"here d‖ ‖ will denote the borel σ-field of subsets of d, then d‖b‖ will denote the σ-field of subsets of d generated by the open balls, and d will denote the σ-field generated by the finite-dimensional subsets of d. it can be shown that"
2310,1,"['set', 'process', 'processes']", Special Spaces,seg_163,"we now digress briefly. the proper set inclusion of (6) caused difficulties in the historical development of the theory of empirical processes (note that the uniform empirical process un = √n(gn − i) takes values in d). to circumvent these difficulties, various authors showed that it is possible to define a metric d on d that has nice properties (see exercise 1.4 below); thus there is a d(·, ·) for which"
2311,1,[], Special Spaces,seg_163,"(8) (d, d) is a complete and separable metric space"
2312,0,[], Special Spaces,seg_163,whose borel σ-field dd satisfies
2313,1,[], Special Spaces,seg_163,"moreover, for all x, xn in d the metric d satisfies"
2314,1,['information'], Special Spaces,seg_163,"the metric d will not be important to us. we are able to replace d by ‖ ‖ in our theorems; however, we include some information on d as an aid to the reader who wishes to consult the original literature."
2315,1,['function'], Special Spaces,seg_163,exercise 1.3 (i) verify (5). [hint. for each 0 ≤ t ≤ 1 define a function xt in d by letting xt(s) equal 0 or 1 according as 0 ≤ s ≤ t or t ≤ s ≤ 1.] (ii) verify (6). [hint. consider
2316,1,['continuous'], Special Spaces,seg_163,"where λ denotes all ↑ continuous maps of [0, 1] onto itself. [roughly, this metric measures how closely x and a slightly perturbed (via λ) y line up, where too much perturbation is penalized. the (“log” bounds all λ-slopes away from both 0 and ∞.]"
2317,0,[], Special Spaces,seg_163,exercise 1.5 verify that
2318,0,[], Special Spaces,seg_163,[we will require the ‖ ‖-separability below.]
2319,1,['functions'], Special Spaces,seg_163,"let q ≥ 0 be positive on (0, 1). for functions x, y on [0, 1] we agree that"
2320,1,"['functions', 'continuous']", Special Spaces,seg_163,"exercise 1.6 it is useful to be able to view c∞ ≡ c[0,∞) as a metric space; of course, this denotes the class of all continuous functions on [0,∞). (we may sometimes require a subclass, such as the one consisting of functions that equal zero at zero; and we will make no further mention of this.) let c∞ ≡ c[0,∞) denote the finite-dimensional σ-field. consider (c∞, c∞) = (c[0, ∞)c[0,∞))."
2321,1,['functions'], Special Spaces,seg_163,"(a) for functions x and y on [0,∞), define"
2322,1,[], Special Spaces,seg_163,"where ρk(x, y) ≡ sup0≤t≤k |x(t) − y(t)|. show that (c[0,∞), ρ∞) is a metric space."
2323,1,[], Special Spaces,seg_163,"(c) show that (c[0,∞), ρ∞) is a complete and separable metric space. moreover, the σ-field"
2324,1,[], Special Spaces,seg_163,"(d) verify that (d[0,∞), ρ∞) is a complete metric space, and that the borel σ-field dρ∞"
2325,1,"['functions', 'continuous']", Special Spaces,seg_163,(e) other spaces of continuous and right-continuous functions are analogously treated. they
2326,0,[], Special Spaces,seg_163,will receive no specific mention.
2327,1,['stationarity'], Special Spaces,seg_163,independent increments and stationarity
2328,1,['interval'], Special Spaces,seg_163,"if t is an interval in (−∞,∞), then we will write"
2329,1,['independent'], Special Spaces,seg_163,"and we will refer to this as an increment of x. if x(t0),x(t0, t1], . . . ,x(tk−1, tk] are independent rvs for all k ≥ 1 and all t0 ≤ · · · ≤ tk in t , then we say that x has independent"
2330,1,"['stationary', 'process', 'stationary process']", Special Spaces,seg_163,"∼ increments. if x(s, t] = x(s + h, t + h] for all s, t, s + h, t + h in t with h ≥ 0, then x is said to have stationary increments. if (x(t1 + h), . . . ,x(tk + h)) ∼= (x(t1), . . . ,x(tk)) for all k ≥ 1, h ≥ 0, and all time points in t , then x is said to be a stationary process."
2331,1,"['sample', 'process', 'processes']", Existence of Processes on C C and DD,seg_165,"when dealing with processes, we would like to work with the smoothest version possible. this is the version that best models physical reality. it is important at this point to recall theorem 5.4.2 on the existence of smoother versions of processes. roughly, if all of the sample paths of a process are shown to lie in a (useful) subset of the current image space, then we can restrict ourselves to that subset."
2332,1,"['process', 'processes']", Existence of Processes on C C and DD,seg_165,"theorem 2.1 (existence of processes on (c , c)) begin with a process"
2333,0,[], Existence of Processes on C C and DD,seg_165,"suppose that for some a, b > 0 the increments of x satisfy"
2334,1,['continuous'], Existence of Processes on C C and DD,seg_165,"where f is a continuous df concentrated on [0, 1] and f (s, t] ≡ f (t) − f (s). then there exists an equivalent version z : (ω,a, p ) → (r[0,1],b[0, 1]pz) for which"
2335,1,"['sample', 'process']", Existence of Processes on C C and DD,seg_165,"corollary 1 (sample path properties) for any 0 < δ < a/b and any > 0, there exists a constant k ≡ k ,δ,a,b for which the process z of (2) satisfies"
2336,1,"['case', 'processes']", Existence of Processes on C C and DD,seg_165,"proof. case 1. suppose that the df f of (1) is f (t) = t on [0, 1]. let 0 < δ < a/b be fixed. let λ ≡ (a/b − δ)/2. define tni ≡ i/2n for 0 ≤ i ≤ 2n and n ≥ 1. for n ≥ 0 define processes zn : (ω,a, p ) → (c, c) by letting"
2337,1,"['linear', 'intervals']", Existence of Processes on C C and DD,seg_165,for each 0 ≤ i ≤ 2n −1; thus zn(·) equals xn(·) at each tni and zn(·) is linear on the intervals between these points. define
2338,0,[], Existence of Processes on C C and DD,seg_165,"let θ > 0 be arbitrary but fixed, and define"
2339,0,[], Existence of Processes on C C and DD,seg_165,recalling (f) shows that
2340,1,['inequality'], Existence of Processes on C C and DD,seg_165,bi/[θ 2−n(δ+λ)]b by markov’s inequality
2341,1,"['functions', 'continuous', 'function', 'limit']", Existence of Processes on C C and DD,seg_165,"converges uniformly on [0, 1] for a.e. ω; call the limit function z(t). since the uniform limit of continuous functions is continuous,"
2342,1,"['function', 'continuous']", Existence of Processes on C C and DD,seg_165,"n=0 δn = limzn is a continuous function on [0, 1] for a.e. ω."
2343,0,[], Existence of Processes on C C and DD,seg_165,"now, z = limzn, and since zn equals x at each tni, we have"
2344,1,"['sample', 'set', 'null set', 'continuous', 'distributions']", Existence of Processes on C C and DD,seg_165,"thus all finite-dimensional distributions with diadic rational coordinates are equal. for other t, we pick diadic rationals t1, t2, . . . such that tm → t. then x(tm) →p x(t) as m → ∞ by (1) and markov, while z(tm) →a.s. z(t) as m → ∞, since z has continuous sample paths. thus z(t) = x(t) a.s. by proposition 2.3.4. by redefining z ≡ 0 on the null set of (n), we may assume"
2345,1,"['sets', 'distributions']", Existence of Processes on C C and DD,seg_165,"by theorem 5.4.2. so all of the finite-dimensional distributions agree: in particular, we have pz([x ∈ c : xt ∈ b]) = px([x ∈ r[0,1] : xt ∈ b]) for all sets b ∈ bk and for all t ∈ [0, 1]k for any k ≥ 1."
2346,0,[], Existence of Processes on C C and DD,seg_165,case 2. general f . define
2347,1,"['continuous', 'process', 'case']", Existence of Processes on C C and DD,seg_165,"since f ◦ f−1 = i for continuous f by exercise 6.3.2. now use case 1 to replace y by an equivalent process ȳ : (ω,a) → (c, c). then define"
2348,1,"['case', 'interval']", Existence of Processes on C C and DD,seg_165,"now, f−1 ◦ f (t) = t, unless f (t − ) = f (t) for some > 0; see exercise 6.3.2. but in this case equation (1) shows that δx is 0 across that same interval. thus x(f−1 ◦ f ) ∼ x."
2349,1,['case'], Existence of Processes on C C and DD,seg_165,"= for the corollary, in case 1 we have (using (o) in line 2, (g) and (k) in line 3)"
2350,1,"['transformation', 'case']", Existence of Processes on C C and DD,seg_165,take k to be an appropriately large value of θ. use the transformation f−1 again in case 2.
2351,0,[], Existence of Processes on C C and DD,seg_165,"exercise 2.1 prove (2), by simplifying the proof of theorem 2.1 as much as possible with this simpler goal in mind."
2352,1,['processes'], Existence of Processes on C C and DD,seg_165,"we merely state an analogous result for the existence of processes on (d,d)."
2353,1,['processes'], Existence of Processes on C C and DD,seg_165,"theorem 2.2 (existence of processes on (d ,d); chentsov) let"
2354,1,['process'], Existence of Processes on C C and DD,seg_165,"be a general process. suppose that for some k > 0, b > 0, and a > 1 we have"
2355,0,[], Existence of Processes on C C and DD,seg_165,"[see billingsley (1968, pp. 130, 134), for example.]"
2356,1,"['poisson', 'process', 'poisson process']", Existence of Processes on C C and DD,seg_165,"exercise 2.2 verify the existence of the poisson process on (d,d)."
2357,1,"['continuous', 'sample', 'interval']", Existence of Processes on C C and DD,seg_165,"exercise 2.3 let x : (ω,a, p ) → (rt ,bt ) on some subinterval t of the line. let to denote a countable dense subset of t , and suppose p (ω : x(·, ω) is uniformly continuous on to ∩ i) = 1 for every finite interval subinterval i of t . then there exists a version z of x such that every sample path z(·, ω) of z is continuous."
2358,1,"['functions', 'moment', 'normal', 'process']", Brownian Motion and Brownian Bridge,seg_167,"brownian motion s on [0,1] we define {s(t) : 0 ≤ t ≤ 1} to be a brownian motion on [0,1] if s is a normal process having the moment functions"
2359,1,"['consistency', 'covariance', 'set', 'random', 'function', 'process']", Brownian Motion and Brownian Bridge,seg_167,"this covariance function is nonnegative definite (in the sense of (a.4.12)) and these distributions are consistent; thus kolmogorov’s consistency theorem shows that the process s exists as a random element on (r[0,1],b[0,1]). modifying this s on a set of measure zero (as in theorem 12.2.1), we may may create a version of s that satisfies"
2360,1,"['sample', 'functions', 'continuous']", Brownian Motion and Brownian Bridge,seg_167,"(2) all sample paths of s are continuous functions on [0, 1] that equal 0 at 0."
2361,1,['realization'], Brownian Motion and Brownian Bridge,seg_167,"thus (as with the smoother realizations of theorem 5.4.2) there is a nice realization of s having smoother paths; that is,"
2362,1,['process'], Brownian Motion and Brownian Bridge,seg_167,"(3) s exists as a process on (c, c)."
2363,1,"['distribution', 'consistency', 'realization']", Brownian Motion and Brownian Bridge,seg_167,"so, brownian motion exists as the coordinate map st(ω) ≡ ωt for some distribution p on (ω,a) = (c, c). this is a more convenient realization of s (than is the one guaranteed by kolmogorov’s consistency theorem). for either realization"
2364,1,"['stationary', 'independent']", Brownian Motion and Brownian Bridge,seg_167,(4) s has stationary and independent increments.
2365,1,['sample'], Brownian Motion and Brownian Bridge,seg_167,"in fact, its sample paths satisfy"
2366,1,['functions'], Brownian Motion and Brownian Bridge,seg_167,"1 as k → ∞. (note that (5) would allow a further application of the smoother realizations theorem using just this smaller subset of such functions in c.) [no appeal has been made to section 1.] brownian bridge u on [0, 1] let us now define"
2367,1,"['linear', 'linear combination', 'combination', 'normal', 'processes']", Brownian Motion and Brownian Bridge,seg_167,"then both u and v are obviously normal processes on (c, c) and satisfy (5); just observe that v(t) is a simple linear combination of two normal rvs. moreover, trivial calculations give"
2368,0,[], Brownian Motion and Brownian Bridge,seg_167,"call u a brownian bridge. and v is also a brownian bridge. brownian motion s on [0,∞) similarly, we establish the existence of brownian motion on (c∞, c∞). in particular, a brownian motion on (c∞, c∞) is given by"
2369,0,[], Brownian Motion and Brownian Bridge,seg_167,recall the proposition 8.6.1 lil result. in section 12.8 we will establish the companion lil result for brownian motion that
2370,1,"['normal', 'processes']", Brownian Motion and Brownian Bridge,seg_167,where b(t) ≡ √2(1 ∨ log log t)). (we will use it in very minor ways in the meantime.) the next exercise similarly defines some additional normal processes. these may provide a useful revisualization device that enables calculation.
2371,1,"['independent', 'transformations']", Brownian Motion and Brownian Bridge,seg_167,"∼ exercise 3.1 (transformations of brownian motion) let z = n(0, 1) and the brownian bridges v, u(1), and u(2) be independent. fix a > 0. show that:"
2372,0,[], Brownian Motion and Brownian Bridge,seg_167,"1 , is a brownian bridge."
2373,0,[], Brownian Motion and Brownian Bridge,seg_167,use the lil at infinity of (10) to show that this u(·) converges to 0 at t = 1.
2374,1,['sample'], Brownian Motion and Brownian Bridge,seg_167,apply the lil of (10) to verify that these sample paths converge to 0 at t = 0.
2375,1,"['normal', 'processes']", Brownian Motion and Brownian Bridge,seg_167,exercise 3.3 (integrals of normal processes are normal rvs)
2376,1,"['continuous', 'covariance', 'normal', 'mean', 'function', 'process']", Brownian Motion and Brownian Bridge,seg_167,"(a) suppose x is a normal process on (c, c). let x have mean function m(·) continuous on i ≡ [0, 1] and covariance function cov(·, ·) continuous on i×i. let g(·) ≥ 0 on i and q > 0 on i both be continuous on i. let k(·) be an ↗ and left continuous function for which"
2377,1,['process'], Brownian Motion and Brownian Bridge,seg_167,1 q|g|dk < ∞. show that the integrated process
2378,1,['continuous'], Brownian Motion and Brownian Bridge,seg_167,"provided that both m(s)/q(s) and cov[s, s]/q2(s) are continuous for s ∈ i."
2379,1,['distribution'], Brownian Motion and Brownian Bridge,seg_167,(b) determine the distribution of ∫0
2380,1,['results'], Brownian Motion and Brownian Bridge,seg_167,(c) develop results for ∫0
2381,1,['functions'], Brownian Motion and Brownian Bridge,seg_167,"1 s g dk, for appropriate functions g and k."
2382,1,['normally distributed'], Brownian Motion and Brownian Bridge,seg_167,[hint. (a) the riemann–stieltjes sums are normally distributed.]
2383,1,['functions'], Brownian Motion and Brownian Bridge,seg_167,"exercise 3.4 let z0, z1, z2, . . . be iid n(0, 1). let fj(t) ≡ √2 sin(jπt), for j ≥ 1; these are orthogonal functions. verify that"
2384,1,['process'], Brownian Motion and Brownian Bridge,seg_167,"thus the process s(t) ≡ u(t) + t z0 is a brownian motion on [0, 1]. moreover,"
2385,1,"['asymptotic', 'distribution', 'statistic', 'null distribution']", Brownian Motion and Brownian Bridge,seg_167,this rv has a distribution that is well tabled (the asymptotic null distribution of the cramér- von mises statistic).
2386,0,[], Brownian Motion and Brownian Bridge,seg_167,"exercise 3.5 show that z is a brownian motion on [0, 1], where"
2387,1,"['covariance', 'normal', 'mean', 'function', 'process']", Brownian Motion and Brownian Bridge,seg_167,"hint. since the reimann sums of normal rvs that define the integral are necessarily normal, the process {z(t) : 0 ≤ t ≤ 1} will be a normal process. then, its mean and covariance function will determine which normal process."
2388,1,['function'], Brownian Motion and Brownian Bridge,seg_167,"exercise 3.6 (white noise) (a) suppose that h and h̃ on [0, 1] are in l2. view white noise as an operator ds that takes the function h into a rv ∫[0,1] h(t)ds(t) in the sense of →l."
2389,1,"['functions', 'limit']", Brownian Motion and Brownian Bridge,seg_167,"define this integral first for step functions, and then use exercise 4.4.5 to define it in general. then show that ∫[0,1] h(t)ds(t) exists as such an →l limit for all h in l2."
2390,1,"['continuous', 'case']", Brownian Motion and Brownian Bridge,seg_167,"(b) in case h has a bounded continuous derivative h′ on [0, 1], show that"
2391,1,"['distribution', 'joint']", Brownian Motion and Brownian Bridge,seg_167,"(c) determine the joint distribution of ∫[0,1] h(t)ds(t) and ∫[0,1] h̃(t)ds(t)."
2392,1,['marginal'], Brownian Motion and Brownian Bridge,seg_167,"(d) define ∫[0,1] h(t)du(t) (appeal first to (7) for the definition), and obtain the marginal and"
2393,1,['distributions'], Brownian Motion and Brownian Bridge,seg_167,joint distributions of all three of the rvs in (c) and (d).
2394,1,"['distribution', 'conditional']", Brownian Motion and Brownian Bridge,seg_167,"exercise 3.7 (conditional brownian motion) let 0 ≤ r < s < t. determine the conditional distribution of s(s) given that s(r) = y and s(t) = z. draw a figure for this situation, and then put your answer in a format that allows some insight to be offered as to an interpretation."
2395,1,"['covariance', 'random', 'function', 'transform']", Brownian Motion and Brownian Bridge,seg_167,"exercise 3.8 find the solution v (t) of the stochastic differential equation with v ′(t) = −kv (t) + σs′(t). determine its covariance function. (think of a tiny particle suspended in a liquid whose velocity is impeded by the viscosity of the liquid and is additionally subjected to random changes from collisions with particles in the medium.) [hint. rewrite the equation first as ekt[v ′(t) + kv (t)] = σekts′(t), then transform it to"
2396,1,[], Brownian Motion and Brownian Bridge,seg_167,and then use integration by parts to give meaning to ds(·).]
2397,1,['condition'], Brownian Motion and Brownian Bridge,seg_167,"exercise 3.9 verify chentsov’s condition (12.2.5) for brownian bridge, that"
2398,0,[], Brownian Motion and Brownian Bridge,seg_167,specify a suitable specific k.
2399,1,"['process', 'case', 'condition']", Brownian Motion and Brownian Bridge,seg_167,"exercise 3.10 the partial sum process {sn(t) : 0 ≤ t < ∞} is defined below in (12.2.5). verify chentsov’s condition for the case of iid (0, σ2) rvs, that"
2400,1,"['process', 'random']", Stopping Times,seg_169,"we first paraphrase the main result of this section. if we observe a right-continuous process at a random time that depends on the process only through its past, then the result is a rv (that is, it is measurable)."
2401,1,"['set', 'events', 'probability', 'continuous', 'process']", Stopping Times,seg_169,"notation 4.1 let (ω,a, p ) denote our basic probability space. we suppose that our time set is a linearly ordered set such as [0, 1], [0,∞), [0,∞], {0, 1, 2, . . . }, {0, 1, 2, . . . ,∞}. let x denote a process with such an index set, defined on (ω,a, p ). we now suppose that the at’s are an ↗ collection of sub σ-fields of a, in that as ⊂ at whenever s < t. call such a collection of at’s a filtration. if it further holds that each xt is an at-measurable rv, then we say that the x-process is adapted to the at’s. the minimal such collection of ↗ σ-fields is the histories σt ≡ σ[xs−1(b) : s ≤ t]. roughly, σt denotes all events for the process up to time t. we let at+ ≡ ∩∞n=1at+1/n; and if at+ = at for all t ≥ 0, then we call the σ-fields at right continuous. let at− ≡ σ[as : s < t]. then let a∞ ≡ σ[∪t<∞at]."
2402,0,[], Stopping Times,seg_169,"definition 4.1 (stopping times) an (extended) rv τ ≥ 0 will be called an (extended) stopping time with respect to the at’s if [τ ≤ t] ∈ at for all t ≥ 0. (that is, one can determine whether τ ≤ t using only current knowledge at.)"
2403,1,"['events', 'probabilities']", Stopping Times,seg_169,"roughly, whether τ “stops” or “occurs” by time t or not depends only on those events at with probabilities within our knowledge base up through time t. we define the pre-τ σ-field"
2404,0,[], Stopping Times,seg_169,"roughly, at any instant we can decide whether or not a has occurred yet. note that if τ(ω) ≡ t for all ω, then aτ = at; that is, the fixed time t is a stopping time whose σ-field is at. we now develop some other technical properties."
2405,0,[], Stopping Times,seg_169,"proposition 4.1 (preservation of stopping times) suppose the rvs t1, t2, . . . are stopping times. then:"
2406,1,['continuous'], Stopping Times,seg_169,"(4) if tn ↘ and at’s are right continuous, then t ≡ lim tn is a stopping time."
2407,0,[], Stopping Times,seg_169,this proposition is also true for extended stopping times.
2408,0,[], Stopping Times,seg_169,proof. note that these four rvs satisfy
2409,1,['events'], Stopping Times,seg_169,"(c) [t ≤ u] = ∩∞n=1[tn ≤ u] = ∩∞n=1 (events in au) ∈ au,"
2410,1,['events'], Stopping Times,seg_169,(d) [t ≤ u] = ∩∞m=1 ∪∞n=1 [tn ≤ u + 1/m] = ∩∞m=1(au+1/m events) ∈ au+ = au.
2411,0,[], Stopping Times,seg_169,no change is needed for extended stopping times.
2412,0,['n'], Stopping Times,seg_169,proposition 4.2 (integral stopping times) integer-valued t ≥ 0 is a stopping time if and only if [t = n] ∈ an for all 0 ≤ n < ∞. this result is also true for extended stopping times.
2413,0,[], Stopping Times,seg_169,"exercise 4.1 (properties of stopping times) let t1, t2, . . . be (extended) stopping times; no ordering is assumed. then (using (9) and/or (10) below is ok):"
2414,1,['continuous'], Stopping Times,seg_169,(5) t1 + t2 is an (extended) stopping time if the at’s are right continuous.
2415,1,['continuous'], Stopping Times,seg_169,"(8) if tn ↘ t0 and the at’s are right continuous, then at0 = ∩∞n=1atn ."
2416,0,[], Stopping Times,seg_169,proposition 4.3 (stopping time measurability) suppose τ is a stopping time with respect to the at’s. then:
2417,1,['events'], Stopping Times,seg_169,(a) (∪ak) ∩ [τ ≤ t] = ∪(ak) ∩ [τ ≤ t]) = ∪ (events in at) ∈ at.
2418,1,['process'], Stopping Times,seg_169,"definition 4.2 (progressively measurable) let {x(t) : t ≥ 0} be a process. let bt denote the borel subsets of [0, t]. call x progressively measurable, and denote this type of measurability by writing"
2419,1,['set'], Stopping Times,seg_169,provided that for each t in the index set we have
2420,1,"['process', 'processes']", Stopping Times,seg_169,"proposition 4.4 (measurability of stopped right-continuous processes) let τ be a stopping time with respect to the at’s. aiso, let x : (ω,a, p ) → (d[0,∞),d[0,∞)) be a process adapted to the at’s. then:"
2421,0,[], Stopping Times,seg_169,(12) x is progressively measurable.
2422,1,['processes'], Stopping Times,seg_169,"we may replace [0,∞) by any [0, θ) or [0, θ] with 0 < θ < ∞. (essentially, one can work nicely with right-continuous processes.)"
2423,1,['process'], Stopping Times,seg_169,"clearly satisfies xn : ([0, t]×ω,bt ×at) → (r,b) is measurable. that is, xn is progressively measurable. thus the process x is also, by proposition 2.2.2, since xn(ω̃) → x(ω̃) for each ω̃ ≡ (s, ω). that is, x : ([0, t] × ω,bt × at) → (r,b) is measurable (or,x is progressively measurable)."
2424,1,['sets'], Stopping Times,seg_169,"the following type of truncation argument with stopping times is common; learn it. we must show that [x(τ) ∈ b] ∩ [τ ≤ t] ∈ at, for all borel sets b ∈ b. but setting τ∗ ≡ τ ∧ t, we see that"
2425,1,"['function', 'sets']", Stopping Times,seg_169,"and hence it suffices to show that [x(τ∗) ∈ b] ∈ at. note that the mapping ω → (τ∗(ω), ω) is a measurable mapping from (ω,at) to ([0, t]×ω,bt ×at), since for a ∈ at and the identity function i we have (for sets [0, s] × a generating bt × at) that"
2426,0,[], Stopping Times,seg_169,exercise 4.2 let t ≥ 0 be a rv and let {at : t ≥ 0} be an increasing sequence of σ-fields. establish the following facts.
2427,1,['continuous'], Stopping Times,seg_169,"(b) if [t < t] ∈ at for all t ≥ 0 and the at’s are right continuous, then t is a stopping time."
2428,0,[], Stopping Times,seg_169,"exercise 4.3 let t1 be a stopping time, and suppose t2 ≥ t1 where t2 is an at1- measurable rv, then t2 is a stopping time."
2429,1,"['sets', 'null sets']", Stopping Times,seg_169,"definition 4.3 (augmented filtration) let (ω, â, p ) denote the completion of the probability space (ω,a, p ). let n ≡ {n ∈ a : p (n) = 0} denote all null sets. let {at : t ≥ 0} be an ↗ sequence of σ-fields; that is, the at’s form a filtration. if all at = at+, the at’s are called right-continuous. if all at = σ[at,n ], then they are said to be a complete filtration. if a filtration {at : t ≥ 0} is both complete and right-continuous, such a collection of σ-fields is called an augmented filtration."
2430,1,"['null sets', 'sets', 'probability']", Stopping Times,seg_169,"proposition 4.5 (kallenberg) let {at : t ≥ 0} denote an ↗ sequence of σ-fields on the probability space (ω,a, p ); that is, the at’s form a filtration. consider the null sets"
2431,0,[], Stopping Times,seg_169,(a) then the ↗collection of σ-fields ât+ necessarily equals the completion ât+ of the right-
2432,0,[], Stopping Times,seg_169,"continuous filtration at+, and so forms an augmented filtration for (ω, â, p ). moreover, this is the minimal augmented filtration."
2433,1,['process'], Stopping Times,seg_169,"(b) if the at ≡ σt denote the histories of a right-continuous process x : (ω,a, p ) → (d[0,∞),d[0,∞)), then the completion of the right-continuized histories necessarily forms the minimal augmented filtration. (that is, complete each σt+ ≡ ∩n∞=1σt+1/n.)"
2434,1,['set'], Stopping Times,seg_169,"̂̂ proof. it is trivial that ât+ ⊂ at+ = ât+. to show the converse, consider a set a ∈ ât+."
2435,1,['set'], Stopping Times,seg_169,"then for each n ≥ 1 we have a ∈ ât+1/n, so p (aδan) = 0 for some set an ∈ at+1/n. note"
2436,0,[], Stopping Times,seg_169,{null}; thus a ∈ ât+. thus the main claim in (a) is established. let ft denote any other
2437,1,"['continuous', 'probability']", Stopping Times,seg_169,"example 4.1 (haeusler) let both a and ac be measurable subsets of some (ω,a, p ) that have probability exceeding 0. define xt(ω) on 0 ≤ t ≤ 1 to be identically 0 if ω ∈ a and to equal (t − 1/2) · 1[1/2,1](t) if ω ∈ ac. all paths of this x-process are continuous. since xt is always 0 for 0 ≤ t ≤ 1/2, we have σt = {∅,ω} for 0 ≤ t ≤ 1/2. however, σt = {∅, a,ac,ω} for 1/2 < t ≤ 1. these histories σt are not right continuous at t = 1/2. the right continuized histories σt+ equal {∅,ω} for 0 ≤ t < 1/2 and equal {∅, a,ac,ω} for 1/2 ≤ t ≤ 1. they are already complete, so σ̂t+ = σ̂t+ = σt+. now, proposition 4.5(c) could be applied."
2438,1,"['independent', 'stationary', 'processes']", Strong Markov Property,seg_171,we now extend the strong markov property (which was proved for discrete-time processes in section 8.6) to processes with stationary and independent increments.
2439,1,"['stochastic process', 'process', 'independent', 'stationary']", Strong Markov Property,seg_171,"theorem 5.1 (strong markov property) consider the stochastic process x : (ω,a, p ) → (d[0,∞),d[0,∞)) adapted to right-continuous at’s. suppose that x(0) = 0,x has stationary and independent increments, and suppose that the increment x(t + s) − x(t) is independent of at for all s ≥ 0. let τ be an extended stopping time for the at’s, and suppose p (τ < ∞) > 0. for some t ≥ 0 we define"
2440,1,"['process', 'independent', 'processes']", Strong Markov Property,seg_171,"thus if p (τ < ∞) = 1, then x and y are equivalent processes and the process y is independent of the σ-field aτ ."
2441,1,['continuous'], Strong Markov Property,seg_171,"(4) at′ ≡ aτ+t are ↗ and right continuous, with y adapted to the at′ ’s."
2442,1,['range'], Strong Markov Property,seg_171,"case 1. suppose the finite part of the range of τ is a countable subset {s1, s2, . . .} of [0,∞). let t1, . . . , tm ≥ 0, let b1, . . . , bm be borel subsets of the real line, and let a ∈ aτ . then"
2443,1,"['independent', 'event']", Strong Markov Property,seg_171,"where the third equality holds as a ∩ [τ = sk] = (a ∩ [τ ≤ sk]) ∩ [τ = sk] is in ask , and is thus independent of the other event by the independent increments of x. putting a = [τ < ∞] in (a) yields"
2444,0,[], Strong Markov Property,seg_171,substituting (b) into (a) and dividing by p (τ < ∞) yields
2445,1,['sets'], Strong Markov Property,seg_171,"thus (b) and (c) hold for the class g of sets of the form [y (t1) ∈ b1, . . . , y (tm) ∈ bm] and for all sets a in aτ . since g generates y −1(d[0,∞)), equation (b) implies (2). since g is also closed under finite intersections (that is, it is a π̄−system), (c) and proposition 7.1.1 imply the truth of (3)."
2446,0,['n'], Strong Markov Property,seg_171,"case 2. now consider a general stopping time τ . for n ≥ 1, define"
2447,0,[], Strong Markov Property,seg_171,"(so that τn is a stopping time), and also for a in aτ that"
2448,1,"['results', 'case']", Strong Markov Property,seg_171,"and let it equal 0 elsewhere. by case 1 results (b) and (c), both"
2449,1,"['joint', 'random']", Strong Markov Property,seg_171,"hold for all f in d[0,∞) and all a in aτ (recall that aτ ⊂ aτn as shown above, and [τ < ∞] = [τn < ∞]). let (r1, . . . , rm) denote any continuity point of the joint df of the finite dimensional random vector (y (t1), . . . , y (tm)), and define"
2450,1,['sample'], Strong Markov Property,seg_171,"by the right continuity of the sample paths, yn(t) → y (t) for every t and every ω in [τ < ∞]; thus"
2451,0,[], Strong Markov Property,seg_171,"and using proposition 7.1.1 again, we see that this is sufficient to imply (3)."
2452,0,[], Embedding a RV in Brownian Motion,seg_173,to be the first time s hits either −a or b. call τ a hitting time. [show that τ is a stopping time.] note figure 6.1.
2453,0,[], Embedding a RV in Brownian Motion,seg_173,figure 6.1 the stopping time τab.
2454,1,"['continuous', 'process', 'parameter']", Embedding a RV in Brownian Motion,seg_173,"definition 6.1 (martingale) a process {m(t) : t ≥ 0} is a continuous parameter martingale (mg) if e|m(t)| < ∞ for all t, m is adapted to the at’s, and"
2455,1,"['random', 'event']", Embedding a RV in Brownian Motion,seg_173,"definition 6.2 (stopping time) if τ is a random time (just a rv that is ≥ 0) for which the event [τ ≤ t] ∈ at for all t, then we call τ a stopping time. future theorem let τ be a stopping time. with appropriate regularity conditions on a mg m , we can claim that"
2456,1,"['sampling', 'cases']", Embedding a RV in Brownian Motion,seg_173,our present applications are simple special cases of a result called the optional sampling theorem for mgs. the general version will be proven in chapter 18. we will use it for such simple special cases now.
2457,1,['independent'], Embedding a RV in Brownian Motion,seg_173,"proof. the independent increments of s lead to satisfaction of the mg property stated in (6). also, s is suitably integrable (we will see later) for (7) to hold (note (13.6.9) and (13.6.16)). thus, with p ≡ p (s(τ) = b), we have"
2458,1,['process'], Embedding a RV in Brownian Motion,seg_173,"also, the process"
2459,1,"['sampling', 'process']", Embedding a RV in Brownian Motion,seg_173,"this process is also suitably integrable, so that optional sampling can be used to imply e[s(τ)2 − τ ] = 0. thus"
2460,1,"['variance', 'mean']", Embedding a RV in Brownian Motion,seg_173,"theorem 6.2 (skorokhod embedding of a zero-mean rv) suppose x is a rv with df f having mean 0 and variance 0 ≤ σ2 ≤ ∞. then there is a stopping time τ such that the stopped rv s(τ) is distributed as x; that is,"
2461,1,"['joint', 'independent']", Embedding a RV in Brownian Motion,seg_173,"proof. for degenerate f , just let τ ≡ 0. thus suppose f is nondegenerate. let (a,b) be independent of s, with joint df h having"
2462,1,['event'], Embedding a RV in Brownian Motion,seg_173,"the procedure is to observe (a,b) = (a, b) according to h, and then to observe τab, calling the result τ. (clearly, τab = 0 if a = 0 is chosen.) note that [τ ≤ t] can be determined by (a,b) and {s(s) : 0 ≤ s ≤ t}, and hence is an event in at ≡ σ[a,b,s(s) : 0 ≤ s ≤ t}. for t ≥ 0,"
2463,0,[], Embedding a RV in Brownian Motion,seg_173,"since ex = 0 with x nondegenerate implies ex+ = ex−. likewise, for t ≥ 0,"
2464,0,[], Barrier Crossing Probabilities,seg_175,for −a < 0 < b we defined the hitting time
2465,1,['probabilities'], Barrier Crossing Probabilities,seg_175,"where s denotes brownian motion on (c∞, c∞). we also considered the rv s(τab), which is called brownian motion stopped at τab. we saw that it took on the two values b and −a with the probabilities p ≡ a/(a + b) and q ≡ 1 − p = b/(a + b)."
2466,0,[], Barrier Crossing Probabilities,seg_175,for a > 0 we define the stopping time (the hitting time of a)
2467,0,[], Barrier Crossing Probabilities,seg_175,theorem 7.1 (the reflection principle; bachelier) both
2468,0,[], Barrier Crossing Probabilities,seg_175,proof. define the stopping time τa
2469,1,['event'], Barrier Crossing Probabilities,seg_175,"′ ≡ τa ∧c, and note that τa = τa ′ on the event [s(c) > a]. now, [τa"
2470,1,['independent'], Barrier Crossing Probabilities,seg_175,′ < c] ∈ aτ ′ is independent of the brownian motion {y(t) ≡ s(τa
2471,1,['events'], Barrier Crossing Probabilities,seg_175,since the events in (c) and (d) are identical.
2472,1,"['probabilities', 'probability', 'event']", Barrier Crossing Probabilities,seg_175,"the two-sided boundary of formula (4) follows from a more complicated reflection principle. let a+ ≡ [‖s+‖ > a] =[s exceeds a somewhere on [0, 1]] and a− ≡ [‖s−‖ > a] = [s falls below −a somewhere on [0, 1]]. though [‖s‖ > a] = a+∪a−, we have p (‖s‖ > a) < p (a+)+ p (a−), since we included paths that go above a and then below −a (or vice versa) twice. by making the first reflection in figure 7.1, we see that the probability of the former event equals that of a + − = [‖s+‖ > 3a], while that of the latter equals that of a−+ = [‖s−‖ > 3a]. but subtracting out these probabilities from p (a+) + p (a−) subtracts out too much, since the path may then have recrossed the other boundary; we compensate for this by adding back in the probabilities of a + −+ ≡ [‖s+‖ > 5a] and a−+− ≡ [‖s−‖ > 5a], which a second"
2473,1,"['process', 'probability']", Barrier Crossing Probabilities,seg_175,reflection shows to be equal to the appropriate probability. but we must continue this process ad infinitum. thus
2474,1,['symmetry'], Barrier Crossing Probabilities,seg_175,(f) = 2[p (a+) − p (a+−) + p (a+−+) − · · · ] by symmetry
2475,0,[], Barrier Crossing Probabilities,seg_175,as claimed. the final expression (5) is left for the reader; it is reputed to converge more quickly.
2476,0,[], Barrier Crossing Probabilities,seg_175,the first reflection 2a
2477,0,[], Barrier Crossing Probabilities,seg_175,figure 7.1 the reflection principle for brownian motion.
2478,1,['linear'], Barrier Crossing Probabilities,seg_175,"theorem 7.2 (the reflection principle for linear boundaries; doob) consider the line ct+d with c ≥ 0, d > 0. then:"
2479,1,['process'], Barrier Crossing Probabilities,seg_175,"proof. now, for any θ = 0 the process"
2480,1,['normal'], Barrier Crossing Probabilities,seg_175,"this holds with σt ≡ σ[s(s) : s ≤ t] (using the mgf of a normal rv), since"
2481,0,[], Barrier Crossing Probabilities,seg_175,but this same quantity also satisfies (by proposition 1.1.2)
2482,1,['distributions'], Barrier Crossing Probabilities,seg_175,theorem 7.3 (kolmogorov–smirnov distributions) both
2483,0,[], Barrier Crossing Probabilities,seg_175,"[hint. the vθ ≡ exp(θ[s(t) − θt2/2]), t ≥ 0 of (8) are martingales on [0,∞). differentiate formally under the integral sign in the martingale equality"
2484,0,[], Barrier Crossing Probabilities,seg_175,"then conclude that [∂k/∂θkvθ(t)]|θ=0 is a martingale for each k ≥ 1. for k = 4 this leads to s4(t) − 6ts2(t) + 3t2 = t2h4(s(t)/√t) being a martingale on [0,∞); here h4(t) = t4 − 6t2 + 3 is the “fourth hermite polynomial.” the reader needs to work only with the single specific martingale in part (a); the rest of this hint is simply an intuitive explanation of how this martingale arises.]"
2485,1,['process'], Embedding the Partial Sum Process o,seg_177,the partial sum process
2486,1,"['distribution', 'process']", Embedding the Partial Sum Process o,seg_177,"let xn1, . . . , xnn be row-independent rvs having a common f (0, 1) distribution, and let xn0 ≡ 0. we define the partial sum process sn on (d,d) by"
2487,1,['case'], Embedding the Partial Sum Process o,seg_177,"(or for all k ≥ 0, in case the nth row is xn1,xn2, ...). note that"
2488,1,"['function', 'process']", Embedding the Partial Sum Process o,seg_177,for the greatest integer function [·]. we suspect that sn“converges” to s. we will establish this shortly. embedding the partial sum process
2489,0,[], Embedding the Partial Sum Process o,seg_177,"by using the skorokhod embedding technique of the previous section repeatedly on the brownian motion zn, we may guarantee that for appropriate stopping times τn1, . . . , τnn (with all τn0 ≡ 0) we obtain that"
2490,1,['process'], Embedding the Partial Sum Process o,seg_177,"let sn denote the partial sum process of these xnk’s. then, for t ≥ 0 we have"
2491,1,[], Embedding the Partial Sum Process o,seg_177,"(7) tn1, . . . , tnn are iid with means = 1 = var[x], in each row."
2492,1,['process'], Embedding the Partial Sum Process o,seg_177,"theorem 8.1 (skorokhod’s embedding theorem) the partial sum process sn on (d,d) of row-independent f (0, 1) rvs formed as above satisfies"
2493,1,"['joint', 'joint distributions', 'limit', 'distributions']", Embedding the Partial Sum Process o,seg_177,"notice: the joint distributions of any sm,sn in theorem 8.1 are not the same as they would be if formed from a single sequence of iid rvs. in fact, we have no idea of what these joint distributions may be. however, the partial sums of an iid sequence do not generally converge to their limit in the sense of →p, so we have gained a great deal via the embedding."
2494,1,['rate'], Embedding the Partial Sum Process o,seg_177,theorem 8.2 (embedding at a rate) suppose that ex4 < ∞. let i denote the identity
2495,1,"['function', 'process']", Embedding the Partial Sum Process o,seg_177,"1 function. then for each 0 ≤ ν < 4 , the process sn of (5) satisfies"
2496,1,['function'], Embedding the Partial Sum Process o,seg_177,proof. consider theorem 8.1. let i denote the identity function. suppose we now show that
2497,0,[], Embedding the Partial Sum Process o,seg_177,"then on any subsequence n′ where →p 0 in (a) may be replaced by →a.s. 0, the continuity of the paths of s will yield"
2498,0,[], Embedding the Partial Sum Process o,seg_177,and thus (9) will follow. this is a useful argument; learn it. it therefore remains to prove (a). the wlln gives
2499,0,[], Embedding the Partial Sum Process o,seg_177,"using the diagonalization technique, we can extract from any subsequence a further subsequence n′ on which"
2500,1,"['functions', 'continuous', 'function', 'limit']", Embedding the Partial Sum Process o,seg_177,"but since all functions involved are monotone, and since the limit function is continuous, this implies that a.s."
2501,1,['limit'], Embedding the Partial Sum Process o,seg_177,"thus (a) follows from (e), since every n has a further n′ with the same limit. thus the conclusion (9) holds."
2502,0,[], Embedding the Partial Sum Process o,seg_177,"in the proof just given, the conclusion (9) can trivially be replaced by"
2503,0,[], Embedding the Partial Sum Process o,seg_177,"provided that the rvs xn1,xn2, . . . are appropriately iid (0, σ2). [we consider the proof of theorem 8.2 at the end of this section.]"
2504,1,"['continuous', 'set', 'process']", Embedding the Partial Sum Process o,seg_177,"let g : (d,d) → (r,b) and let δg denote the set of all x ∈ d for which g is not ‖ · ‖- continuous at x. if there exists a set δ ∈ d having δg ⊂ δ and p (s ∈ δ) = 0, then we say that g is a.s.‖ · ‖-continuous with respect to the process s."
2505,1,['distribution'], Embedding the Partial Sum Process o,seg_177,(13) g(sn) →d g(s) as n → ∞ for any sn having the same distribution.
2506,1,['continuous'], Embedding the Partial Sum Process o,seg_177,"(d-measurability is typically trivial, and hypothesizing it avoids the measurability difficulties discussed in section 12.1.) [theorem 8.2 allows (13) for d-measurable functionals g that are continuous in other ‖ · /q‖-metrics.]"
2507,1,[], Embedding the Partial Sum Process o,seg_177,"′′(ω)) → g(s(ω)) holds, as ‖sn ′′(ω) − s(ω)‖ → 0 and g is ‖ ‖-continuous at s(ω). thus g(sn) →p g(s) as n → ∞ for the sn of (5). thus g(sn) →d g(s) for the sn of (5), and hence of (13) also. [note that we are dealing only with functionals for which the compositions g(sn) and g(s) are (ω,a, p ) → (r,b) measurable.]"
2508,1,[], Embedding the Partial Sum Process o,seg_177,"example 8.1 since the functionals ‖ · ‖ and ‖ ·+ ‖ are a.s. ‖ · ‖-continuous,"
2509,1,['distributions'], Embedding the Partial Sum Process o,seg_177,the limiting distributions are those given in theorem 7.1.
2510,1,"['distribution', 'asymptotic']", Embedding the Partial Sum Process o,seg_177,(a) find the asymptotic distribution of (s1 + · · · + sn)/cn for an appropriate cn.
2511,1,"['distribution', 'asymptotic']", Embedding the Partial Sum Process o,seg_177,(b) determine a representation for the asymptotic distribution of the “absolute area” under
2512,1,['process'], Embedding the Partial Sum Process o,seg_177,"the partial sum process, as given by (|s1| + · · · + |sn|)/cn."
2513,1,['mean'], Embedding the Partial Sum Process o,seg_177,"notation 8.2 define stopping times t1, t2, . . . (with t0 = 0) having mean 1 for which the rvs"
2514,0,[], Embedding the Partial Sum Process o,seg_177,"[note that this embedding differs from that in notation 8.1. this one is based on a single sequence of rvs x1,x2, . . . .]"
2515,0,[], Embedding the Partial Sum Process o,seg_177,"we will now make rigorous this approach to the problem. first apply the slln to τ [t]/t as t → ∞. then define δk ≡ sup{|s(t) − s(tk)| : tk ≤ t ≤ tk+1}, with tk ≡ (1 + a)k for some suitably tiny a > 0. use a reflection principle and mills’ ratio to show that p (δk ≥ (an appropriate ck)) < ∞. complete the proof using borel–cantelli.]"
2516,1,['model'], Embedding the Partial Sum Process o,seg_177,"(b) now that you know how to deal with the “blocks” δk, model a proof of (16) on the proof"
2517,1,[], Embedding the Partial Sum Process o,seg_177,proof for embedding at a rate∗
2518,1,['inequality'], Embedding the Partial Sum Process o,seg_177,then the monotone inequality gives
2519,1,"['variance', 'independent', 'mean', 'inequality']", Embedding the Partial Sum Process o,seg_177,where the yni’s are independent with mean 0 and variance (ilog2i)−1. thus the kolmogorov inequality gives
2520,0,[], Embedding the Partial Sum Process o,seg_177,"4 (this final step holds, since ∫0"
2521,1,['inequality'], Embedding the Partial Sum Process o,seg_177,dx → 0 as c → ∞). the inequality (g) used
2522,1,['process'], Embedding the Partial Sum Process o,seg_177,exercise 8.3 suppose ex4 < ∞. show that the process sn of (5) satisfies
2523,0,['n'], Embedding the Partial Sum Process o,seg_177,"[hint. replace nν/(k/n)1/2−ν by n1/4/ log n in the definition of bn in (d). now determine the new form of the bounds in (20) and (21).] [while interesting and often quoted in the literature, this formulation has little value for us.]"
2524,1,['sample'], Other Properties of Brownian Motion o,seg_179,"here we collect some selected sample path properties of brownian motion, just to illustrate a sample of what is known. some proofs are outlined in the exercises."
2525,1,"['partitions', 'variation']", Other Properties of Brownian Motion o,seg_179,definition 9.1 (variation) for a sequence of partitions
2526,1,['variation'], Other Properties of Brownian Motion o,seg_179,"pn ≡ {(tn,k−1, tnk] : k = 1, . . . , n} of [0, 1] (with 0 ≡ tn0 < · · · < tnn ≡ 1), define the rth variation of s corresponding to pn by"
2527,1,['partitions'], Other Properties of Brownian Motion o,seg_179,"we call these partitions nested if pn ⊂ pn+1 for all n ≥ 1. we further define the mesh of the partitions to be ‖pn‖ ≡ sup1≤k≤n |tnk − tn,k−1|."
2528,0,[], Other Properties of Brownian Motion o,seg_179,theorem 9.1 (nondifferentiability)
2529,0,[], Other Properties of Brownian Motion o,seg_179,(a) almost every brownian path is nowhere differentiable.
2530,1,['variation'], Other Properties of Brownian Motion o,seg_179,(c) (finite squared variation) vn(2) →l2 1 if ‖pn‖ → 0.
2531,1,['variation'], Other Properties of Brownian Motion o,seg_179,(d) (finite squared variation) vn(2) →a.s. 1
2532,1,['condition'], Other Properties of Brownian Motion o,seg_179,theorem 9.2 (lévy) the hölder condition is true:
2533,1,['set'], Other Properties of Brownian Motion o,seg_179,"for almost all ω, the set zeros(ω) is a closed and perfect set of lebesgue measure zero. [a set a is called dense in itself if every point x of a is such that every neighborhood of x contains another point of a beyond x. a compact set that is dense in itself is called perfect.]"
2534,1,"['functions', 'continuous']", Other Properties of Brownian Motion o,seg_179,"set of absolutely continuous functions f on [0, 1] with f(0) = 0 and ∫0"
2535,1,"['set', 'limit']", Other Properties of Brownian Motion o,seg_179,"for almost all ω the sequence {zn(·, ω) : n = 3, 4, . . .} visualized within c[0, 1] is relatively compact with limit set k. that is,"
2536,1,['extreme values'], Other Properties of Brownian Motion o,seg_179,"we will write this conclusion symbolically as z k. [this can be used to establish a lil for various functionals g of sn, by determining the extreme values of g(k).]"
2537,1,['case'], Other Properties of Brownian Motion o,seg_179,"(γ) finally, demonstrate the truth of theorem 9.1(d), case (i)."
2538,1,['inequality'], Other Properties of Brownian Motion o,seg_179,exercise 9.3 prove theorem 9.1(b) when all tnk = k/2n. [hint. let 0 < λ < 1. the paley-zygmund inequality gives
2539,0,[], Various Empirical Process,seg_181,"suppose that ξn1, . . . , ξnn are iid uniform(0, 1). their empirical df gn is defined by"
2540,1,"['order statistics', 'statistics']", Various Empirical Process,seg_181,where 0 ≡ ξn:0 ≤ ξn:1 ≤ · · · ≤ ξn:n ≤ ξn:n+1 ≡ 1 are the order statistics; see figure 10.1. note
2541,1,['binomial'], Various Empirical Process,seg_181,"∼ ∼ that ngn(t) binomial (n, t) (t, t(1 − t)). the glivenko–cantelli theorem shows that gn"
2542,0,[], Various Empirical Process,seg_181,"= = converges uniformly to the true df i; that is,"
2543,1,"['moments', 'process']", Various Empirical Process,seg_181,(the cantelli proof of the slln based on fourth moments shows that gn(t) →a.s. t for each fixed t; even for triangular arrays. the rest of the proof is identical.) the uniform empirical process un is defined by
2544,1,['process'], Various Empirical Process,seg_181,"this process is also pictured in figure 10.1. the means and covariances of un are the same as those of brownian bridge u, in that"
2545,0,[], Various Empirical Process,seg_181,this follows easily from
2546,0,[], Various Empirical Process,seg_181,"{moreover, for any dk’s and ek’s we have immediately from this that"
2547,1,"['continuous', 'independent']", Various Empirical Process,seg_181,"1 e[dk, ek] instead, if these were rvs independent of the ξnk’s.} we note that gn−1(t) ≡ inf{x ∈ [0, 1] : gn(x) ≥ t} is left continuous, with"
2548,1,"['quantile', 'process']", Various Empirical Process,seg_181,"and gn−1(0) = 0, as in figure 10.1. the uniform quantile process vn is defined by"
2549,1,['function'], Various Empirical Process,seg_181,the key identities relating un and vn are (with i the identity function) the trivial
2550,0,[], Various Empirical Process,seg_181,it is sometimes convenient to use the smoothed versions g̈n and g̈n−1 defined by
2551,1,"['quantile', 'process', 'variation']", Various Empirical Process,seg_181,connected linearly between points. upon occasion the smoothed uniform quantile process v̈n(t) ≡ √n[g̈−n 1(t) − t] is a useful variation on vn. the glivenko–cantelli theorem implies that
2552,0,[], Various Empirical Process,seg_181,see figure 10.1. coupling these with the identities (10) and (11) shows that
2553,1,['normalized'], Various Empirical Process,seg_181,"let cn ≡ (cn1, . . . , cnn)′ denote a vector of known constants normalized so that"
2554,1,['condition'], Various Empirical Process,seg_181,we suppose that these constants also satisfy the uan condition
2555,1,['process'], Various Empirical Process,seg_181,the weighted uniform empirical process is defined by
2556,1,['process'], Various Empirical Process,seg_181,the wn process is pictured in figure 10.1. it is trivial from (7) that
2557,1,['independent'], Various Empirical Process,seg_181,"it is easy to show that wn →fd w, where w denotes another brownian bridge, one that is independent of u."
2558,1,"['permutation', 'random']", Various Empirical Process,seg_181,"let rn ≡ (rn1, . . . rnn)′ denote the ranks of ξn1, . . . , ξnn; and then denote the antiranks by dn ≡ (dn1, . . . , dnn)′. then rn is a random permutation of the vector (1, . . . , n)′, while dn is the inverse permutation. these satisfy"
2559,1,['independent'], Various Empirical Process,seg_181,"(21) (ξn:1, . . . , ξn:n) and (rn1, . . . , rnn) are independent rvs."
2560,1,"['sampling', 'process']", Various Empirical Process,seg_181,the empirical finite sampling process rn is defined by
2561,1,['process'], Various Empirical Process,seg_181,the rn process is also pictured in figure 10.1. the key identities are
2562,1,"['independent', 'processes']", Various Empirical Process,seg_181,(25) rn and vn are independent processes.
2563,1,['independent'], Various Empirical Process,seg_181,(26) w and v = −u are independent brownian bridges;
2564,0,[], Various Empirical Process,seg_181,"this is further corroborated, since (7) with ∑n"
2565,1,['covariance'], Various Empirical Process,seg_181,1 cnk/n = 0 imply that the cross covariance
2566,1,['rate'], Various Empirical Process,seg_181,"we will prove only part of theorems 10.1 and 10.3 (namely, that (28) holds). for the believability of the rest, we will rely on (28), our earlier proof that sn can be embedded at a rate, and the proof of theorem 10.2. (shorack(1991) contains these proofs, written in the current style and notation.) see section 12.11 for proofs of theorems 10.2 and 10.4."
2567,1,"['independent', 'probability', 'convergence', 'processes']", Various Empirical Process,seg_181,"theorem 10.1 (convergence of the uniform processes) we can define independent brownian bridges u = −v and w and row-independent uniform(0, 1) rvs ξn1, . . . , ξnn on a common probability space (ω,a, p ) in such a way that"
2568,1,"['approximation', 'processes']", Various Empirical Process,seg_181,theorem 10.3 (weighted approximation of the uniform processes) the embed-
2569,0,[], Various Empirical Process,seg_181,1 dings of the previous theorem are such that for any 0 ≤ ν < we have
2570,1,[], Various Empirical Process,seg_181,[the supremum limits in (32) and (34) may be changed to c/n and 1−c/n for any constant c > 0. this relates to exercise 10.3 below.]
2571,1,"['approximation', 'realization']", Various Empirical Process,seg_181,"theorem 10.4 (weighted approximation of gn; mason) for any realization of gn, any n ≥ 1, any 0 < ν < 1"
2572,1,"['linear', 'statistics']", Various Empirical Process,seg_181,example 10.1 (r-statistics) consider the simple linear rank statistics
2573,1,['continuous'], Various Empirical Process,seg_181,"where the last step holds if k = k1 − k2 with each ki ↗ and left continuous on (0, 1). as in (12.3.19), this suggests that"
2574,1,['condition'], Various Empirical Process,seg_181,"provided that the uan condition holds and provided that var [ki(ξ)] < ∞ for i = 1, 2. indeed, this can be shown to be true. (writing"
2575,1,"['function', 'case', 'variation']", Various Empirical Process,seg_181,provides a simple proof in case this integral is finite for some square integrable function q and for total variation measure d|k|.) we will return to this in section 15.2 below.
2576,1,[], Various Empirical Process,seg_181,proof. consider vn. we will represent our uniforms rvs as a normed sum of exponential(1) rvs. thus we really begin with a skorokhod embedding of iid exponential(1) rvs.∼
2577,1,['process'], Various Empirical Process,seg_181,"let f (x) = 1 − exp(−(x + 1)) for x ≥ −1, so that f is a (0, 1) df; and if x = f, ∼ then x + 1 = exponentil(1). according to skorokhod’s embedding theorem, there exist rowindependent rvs xn1, . . . , xnn with df f such that the partial sum process sn of the n th row satisfies ‖sn − s‖ →p 0 for some brownian motion s. we now define"
2578,1,"['statistics', 'order statistics', 'process']", Various Empirical Process,seg_181,"it is an elementary exercise below to show that these ξnk’s are distributed as n row-independent uniform(0, 1) order statistics. let gn denote their empirical df and un their uniform empirical process. the key identity relating vn to sn is"
2579,1,"['sample', 'process']", Various Empirical Process,seg_181,by continuity of all sample paths of the s process.
2580,1,"['continuous', 'sample']", Various Empirical Process,seg_181,"all sample paths of v are continuous, and the maximum jump size of |vn −v| is bounded above by [√n max1≤i≤n+1 δni]; so ‖vn − v‖ →p 0 and (12) imply"
2581,1,['sample'], Various Empirical Process,seg_181,using ‖gn − i|| →p 0 and uniform continuity of the sample paths of v.
2582,0,[], Various Empirical Process,seg_181,we will prove mason’s theorem in the next section.
2583,0,[], Various Empirical Process,seg_181,exercise 10.1 establish the claim made just below (40).
2584,1,"['function', 'continuous']", Various Empirical Process,seg_181,"example 10.2 (the supremum functionals) suppose g : (d,d) → (r,b) is an a.s. ‖ · ‖- continuous function. then"
2585,1,"['distribution', 'convergence', 'processes']", Various Empirical Process,seg_181,"for the special constructions of theorem 10.1. moreover, convergence in distribution holds for any versions of these processes. letting # denote +,−, or | · |, we can thus claim the convergence in distribution"
2586,1,"['processes', 'distributions']", Various Empirical Process,seg_181,for any versions of these processes. these limiting distributions of ‖u±‖ were given in theorem
2587,1,"['process', 'independent']", Various Empirical Process,seg_181,"exercise 10.2 (the two-sample uniform process) (i) let gm and hn be the empirical dfs of two independent uniform(0, 1) special constructions. let"
2588,1,['process'], Various Empirical Process,seg_181,"denote the corresponding empirical process, and let λmn ≡ n/(m + n). then"
2589,1,"['sample', 'method', 'combined sample', 'statistics', 'discrete', 'distribution', 'order statistics', 'random', 'limit']", Various Empirical Process,seg_181,"we thus have ‖wm#n‖ →d ‖w#‖ for brownian bridge w. write out the details. (ii) now use a discrete reflection principle to compute the exact distribution of p (‖w+nn‖ ≥ a), and pass to the limit in the resulting expression to obtain (12.7.10). (this provides an alternative to the earlier method.) [hint. go through the order statistics of the combined sample from smallest to largest. if it is from sample 2, step up one unit as you go to the right one unit. if it is from sample 2, step down one unit as you go to the right one unit. in this way, perform a random walk from (0, 0) to (2n, 0). what is the chance you ever cross a barrier of height a?]"
2590,1,"['continuous', 'process', 'statistics']", Various Empirical Process,seg_181,"example 10.3 (the kolmogorov–smirnov and cramér-von mises statistics) let ξn1, . . . , ξnn be the iid uniform (0, 1) rvs of the special construction, and let f denote an arbitrary df. then xnk ≡ f−1(ξnk), 1 ≤ k ≤ n, are iid f . let fn denote the empirical df of xn1, . . . , xnn and let en denote the empirical process defined by en(x) ≡ √n[fn(x) − f (x)]. now, en = un(f ). thus (28) implies ‖en − u(f )‖ ≤ ‖un − u‖ →p 0, where equality holds if f is continuous. thus"
2591,1,['continuous'], Various Empirical Process,seg_181,(48) √ndn# ≡ √n‖(fn − f )#‖ = ‖un#‖ → d‖u#‖ if f is continuous.
2592,1,"['variable', 'change of variable']", Various Empirical Process,seg_181,"likewise, a change of variable allows elimination of f , and gives"
2593,1,['continuous'], Various Empirical Process,seg_181,1 u2n(t)dt →d ∫0 1 u2(t)dt if f is continuous.
2594,1,"['estimate', 'asymptotic', 'statistics', 'distributions', 'test', 'null hypothesis', 'hypothesis']", Various Empirical Process,seg_181,"these statistics are used to test whether f is really the true df, and √ndn# and wn2 all measure how far the estimate fn of the true df differs from the hypothesized df f. [the percentage points of the asymptotic distributions of √ndn# and wn2, under the null hypothesis when f is really the true df, are available.]"
2595,0,[], Various Empirical Process,seg_181,consider now the two-sample problem in which the rvs xn
2596,1,['independent'], Various Empirical Process,seg_181,"1 ≤ j ≤ ni, of independent special constructions have empirical dfs f(n1"
2597,1,"['independent', 'processes']", Various Empirical Process,seg_181,that for independent uniform empirical processes
2598,1,['continuous'], Various Empirical Process,seg_181,"(51) =a wn1,n2(f ) if f is continuous,"
2599,1,"['asymptotic', 'null distribution', 'distribution', 'processes']", Various Empirical Process,seg_181,"this gives the asymptotic null distribution for the various supremum and integral functionals with which we have dealt, no matter which version of these processes is considered."
2600,1,[], Various Empirical Process,seg_181,exercise 10.3 show that n ξn:n → d exponential(1).
2601,0,[], Various Empirical Process,seg_181,(a) show that there exist rvs (to be labeled ∫0
2602,0,[], Various Empirical Process,seg_181,"[hint. this is too hard to be an “exercise,” but it is a very nice bound.]"
2603,1,"['processes', 'associated']", Inequalities for Various Empirical Processes o,seg_183,we wish to apply the birnbaum–marshall and hájek-rényi inequalities to various martingales (mgs) associated with the processes of the previous section.
2604,0,[], Inequalities for Various Empirical Processes o,seg_183,proposition 11.1 (various martingales)
2605,1,"['normal', 'mean']", Inequalities for Various Empirical Processes o,seg_183,since u(t)|u(s) is normal with mean μt + σtsς−ss1[u(s) − μs]. thus (3) holds.
2606,1,['set'], Inequalities for Various Empirical Processes o,seg_183,"consider (5). let znk ≡ rn(k/(n + 1))/(1 − k/n), and set δznk ≡ znk − zn,k−1 for integers 1 ≤ k ≤ n − 1. then"
2607,1,"['sampling', 'results']", Inequalities for Various Empirical Processes o,seg_183,apply the finite sampling results (a.1.8) and (a.1.9) to (d) to conclude that
2608,1,"['sample', 'conditional', 'distribution', 'statistic', 'conditional distribution', 'order statistic']", Inequalities for Various Empirical Processes o,seg_183,"since the conditional distribution of ξn:k given ξn:i is that of the (k − i)th order statistic in a sample of size n − i from uniform (ξn:i, 1), and (a.1.32) can be applied. thus (4) holds."
2609,1,"['continuous', 'probability', 'processes']", Inequalities for Various Empirical Processes o,seg_183,"inequality 11.1 (pyke–shorack) let x denote one of the processes un, v̈n,wn,rn, or u. let q > 0 on [0, θ] be ↗ and right continuous. then for all λ > 0 we have the probability bound"
2610,1,"['variance', 'mean', 'inequality']", Inequalities for Various Empirical Processes o,seg_183,"proof. let x denote any one of un,wn, or u. then x(t)/(1 − t) is a mg with mean 0 and variance ν(t) ≡ t/(1 − t). thus the birnbaum–marshall inequality gives"
2611,1,"['factor', 'inequality']", Inequalities for Various Empirical Processes o,seg_183,"(we can improve (a) and (c) by a factor of 4, as stated in the hájek–rényi inequality, but there is no real point to this.)"
2612,1,"['linear', 'probability', 'event']", Inequalities for Various Empirical Processes o,seg_183,inequality 11.2 (in probability linear bounds on gn and g−n 1) for all > 0 there exists λ ≡ λ so small that the event an on which
2613,1,"['indicator function', 'linear', 'function', 'indicator', 'realization']", Inequalities for Various Empirical Processes o,seg_183,has p (an ) ≥ 1 − for all n ≥ 1. let 1n denote the indicator function of an . (these conclusions hold for any realization of gn and gn−1.) (note that linear bounds on gn−1 are also established by this result.)
2614,1,['joint'], Inequalities for Various Empirical Processes o,seg_183,"proof. now, (ξn:1, . . . ξn:n) has joint density n! on its domain. thus"
2615,1,"['exponential', 'mean']", Inequalities for Various Empirical Processes o,seg_183,(h) = e2 infr>0 e−r/λ/(1 − r)2 from the mean of an exponential density
2616,1,"['symmetry', 'inequality']", Inequalities for Various Empirical Processes o,seg_183,"for λ ≡ λ small enough. thus the lower bound in (9) holds. then (10) follows from (9) by symmetry. finally, (11) holds since ‖un‖ = op(1). in fact, we have chang’s inequality"
2617,1,['inequality'], Inequalities for Various Empirical Processes o,seg_183,proof. consider mason’s theorem 12.10.4. apply the pyke–shorack inequality with divisor q(t) ≡ (a ∨ t)1−ν to obtain
2618,1,['symmetry'], Inequalities for Various Empirical Processes o,seg_183,1 and the symmetry about 2
2619,1,['symmetric'], Inequalities for Various Empirical Processes o,seg_183,"but [0, 1/n] is easy (and [1 − 1/n, 1] is symmetric), since on [0, 1/n] we have"
2620,1,"['interval', 'inequality']", Inequalities for Various Empirical Processes o,seg_183,"we can repeat this same proof up to (13) with g̈n−1 and v̈n replacing gn and un, because of the pyke-shorack inequality. then (0, 1/n] is trivial for g̈n−1, as the values on this whole interval are deterministically related to the value at 1/n."
2621,1,['model'], Inequalities for Various Empirical Processes o,seg_183,"exercise 11.2 prove the pyke-shorack theorem 12.10.2. [hint. model your proof on (a) of the previous proof, with a = 0 and b sufficiently small, and with theorem 12.10.1 sufficient on [b, 1 − b].]"
2622,1,['continuous'], Applications o,seg_185,[these conclusions hold for d-measurable functionals g that are continuous in other ‖ · /q‖- metrics as well.]
2623,0,[], Applications o,seg_185,exercise 12.1 write out the easy details to prove this donsker theorem.
2624,1,"['alternative hypothesis', 'hypothesis', 'tests', 'test', 'null hypothesis', 'inequality']", Applications o,seg_185,"example 12.1 (tests of fit) (i) call f stochastically larger than f0 when pf (x > x) ≥ pf0(x > x) for all x (with strict inequality for at least one x), and write f ≥s f0. then to test the null hypothesis h0 that f = f0 is true against the alternative hypothesis ha that f ≥s f0 it is reasonable to reject the truth of the h0 claim for large values of birnbaum’s"
2625,1,['continuous'], Applications o,seg_185,"∞ ∞ √n[fn(x)−f0(x)]df0(x). now suppose that h0 is true, with a continuous df f0. then"
2626,1,['statistic'], Applications o,seg_185,"(ii) alternatively, one could form the cramér-von mises statistic"
2627,1,['continuous'], Applications o,seg_185,"1 u2n(t)dt when f0 is continuous, by (6.3.10)"
2628,1,['functions'], Applications o,seg_185,"for the orthonormal functions φk(t) ≡ √2 sin(πkt) on [0, 1]"
2629,1,"['independent', 'distribution', 'tables']", Applications o,seg_185,"this shows that wn is asymptotically distributed as an infinite weighted sum of independent χ21rvs. this representation of the limiting distribution has been used to provide tables. if wn ≡ wn(f0) is computed but a different df f is true, then"
2630,1,['statistical'], Applications o,seg_185,"[in statistical parlance, this shows that the wn-test is consistent against any df alternative having f = f0.]"
2631,1,['statistic'], Applications o,seg_185,(iii) a third possibility is the anderson–darling statistic
2632,1,['continuous'], Applications o,seg_185,for f0 continuous
2633,1,"['change of variable', 'variable', 'normal', 'function']", Applications o,seg_185,"method 1: by (6.5.27) and then the change of variable theorem of (6.3.10) (with identity function h) one obtains the first two steps of (3). apply donsker for the third step. appeal to (12.3.19) for the → d to a normal rv z. finally, appeal to fubini’s theorem for both"
2634,0,[], Applications o,seg_185,the rest of the justification of example 12.1 is outlined in exercises 12.2 and 12.3.
2635,1,['statistic'], Applications o,seg_185,exercise 12.2 consider the cramér-von mises statistic wn.
2636,1,[], Applications o,seg_185,(i) verify step (5). use (12.10.28). (ii) we now seek to justify the step representing u as an infinite series. to this end formally write
2637,1,['functions'], Applications o,seg_185,"for iid n(0, 1) rvs zk and the orthonormal functions φk(·). first recall the group of trigonometric identities"
2638,0,[], Applications o,seg_185,use these to verify that ∫0
2639,1,"['function', 'coefficients']", Applications o,seg_185,"1 φj(t)φk(t)dt equals 0 or 1 according as j = k or j = k. [think of this formal u(t) as an odd function on [-1, 1], and thus only these φk(·) are needed.] then note that the fourier coefficients and the fourier series are"
2640,1,[], Applications o,seg_185,"so, verify that the series in (p) converges a.s. and then everything so far for the formal u is rigorous. then parseval’s identity (note theorem b.3.3) gives"
2641,0,[], Applications o,seg_185,"finally, one needs to verify the step (u) in the identity"
2642,1,['process'], Applications o,seg_185,and thus the (originally formal) process u is in fact a brownian bridge. where did this idea come from? verifying that
2643,1,['associated'], Applications o,seg_185,"1 shows that cov[s, t] ≡ cov[u(s),u(t)] = s∧t−st has eigenvalues with associated eigenfunc-"
2644,1,"['function', 'covariance', 'statistic', 'results']", Applications o,seg_185,"exercise 12.3 verify the results claimed for the anderson-darling statistic an. [verifying →d will be a little trickier this time, since (12.10.30) will now be needed in place of (12.10.28).] the rest is roughly similar in spirit, but the details are now a geat deal more complicated. fundamentally, one must now represent the covariance function"
2645,1,['functions'], Applications o,seg_185,"as a convergent infinite series of orthonormal functions. (hopefully, at least the approach is now clear. providing the details is hard work.)"
2646,1,"['case', 'discrete', 'cases', 'sets', 'continuous', 'processes']", Basic Technicalities for Martingales,seg_189,"notation 1.1 we will work with processes on the following time sets i : {0, . . . , n}, {0, 1, . . .}, {0, 1, . . . ,∞}, {. . . ,−1, 0}, {−∞, . . . ,−1, 0} in the discrete case and [0, t], [0,∞), [0,∞], (−∞, 0], [−∞, 0] in the continuous case. in the continuous cases we will consider only processes of the type x : (ω,a, p ) → (di ,di) that are adapted to an ↗ sequence of sub σ-fields at of a. we will use the notation {an}∞n=0, {an}∞n=0, {an}0n=−∞, {an}0n=−∞ to denote sequences over {0, 1, . . .}, {0, 1, . . . ,∞}, {. . . ,−1, 0}, {−∞, . . . ,−1, 0}, respectively."
2647,0,[], Basic Technicalities for Martingales,seg_189,"definition 1.1 (martingale and submartingale) suppose e|xt| < ∞ for all t. call the rvs {xt, at}t∈i a martingale (abbreviated mg) if"
2648,0,[], Basic Technicalities for Martingales,seg_189,"call {xt,at}t∈i a submartingale (abbreviated submg) if"
2649,1,"['results', 'set', 'process', 'inequality']", Basic Technicalities for Martingales,seg_189,"(if the inequality in (2) is reversed, the process {xt,at}t∈t is then called a super-martingale.) when the index set i is a subset of the negative numbers [−∞, 0], we refer to such a process as a reversed martingale or reversed submartingale. [most results in the first seven sections of this chapter are due to doob.]"
2650,1,['moments'], Basic Technicalities for Martingales,seg_189,"proposition 1.1 (equivalence) now, {xt,at}t∈i is a submg if and only if the moments e|xt| < ∞ for all t ∈ i and for every pair s ≤ t we have"
2651,0,[], Basic Technicalities for Martingales,seg_189,"similarly, {xt,at}t∈i is a mg if and only if equality holds in (3). notation as in section 8.9, we combine these two statements by writing"
2652,1,['case'], Basic Technicalities for Martingales,seg_189,"proof. clearly, φ(xt) is adapted to at. let s ≤ t. for the mg case,"
2653,1,"['conditional', 'inequality']", Basic Technicalities for Martingales,seg_189,(a) e[φ(xt)|as] ≥ φ(e(xt|as)) by the conditional jensen inequality
2654,1,['case'], Basic Technicalities for Martingales,seg_189,"for the submg case,"
2655,1,"['conditional', 'inequality']", Basic Technicalities for Martingales,seg_189,(c) e[φ(xt)|as] ≥ φ(e(xt|as)) by the conditional jensen inequality
2656,1,['set'], Basic Technicalities for Martingales,seg_189,"exercise 1.1 if {xtc,at}t∈i is a submg for all c in some index set c, then the maximum {xtc1 ∨ xtc2 ,at}t∈i is necessarily a submg for any c1, c2 ∈ c. likewise, {supc∈c xtc,at}t∈i is a submg, provided that e| supc∈c xtc| < ∞ for all t ∈ i."
2657,1,"['continuous', 'probability']", Basic Technicalities for Martingales,seg_189,"definition 1.3 (augmented filtration) let (ω,a, p ) be a complete probability space. let n ≡ {n ∈ a : p (n) = 0}. let {at : t ≥ 0} be such that the at’s are an ↗ sequence of σ-fields with at = ât = at+ for all t ≥ 0 (here ât ≡ σ[at,n ] and at+ ≡ ∩{ar : r > t}). [that is, they are complete and right continuous.] such a collection of σ-fields is called an augmented filtration."
2658,1,['processes'], Basic Technicalities for Martingales,seg_189,"notation 1.2 (completeness assumption) in this chapter we will assume that the σ- fields at form an augmented filtration in that completion has already been performed on the σ-fields labeled at. thus, from proposition 12.4.5(c), we see that s ≤ t a.s. implies as ⊂ at . for right-continuous processes on (d[0,∞),d[0,∞)) this effectively comes for free;"
2659,0,[], Basic Technicalities for Martingales,seg_189,exercise 1.2 verify the claim made in the previous assumption.
2660,1,"['continuous', 'process']", Basic Technicalities for Martingales,seg_189,"exercise 1.3 if x is a process on (d,d) or (d[0,∞),d[0,∞)), then the histories σt ≡ σ[xs : s ≤ t] are right continuous, as are the σ̂t ≡ σ[σt ∪ n ]. (recall (12.4.13) of proposition 12.4.4, proposition 12.4.5, and exercise 1.2.1.)"
2661,1,"['results', 'processes']", Basic Technicalities for Martingales,seg_189,"remark 1.1 all definitions and results in this section make sense for processes on the measurable space (ri ,bi)."
2662,1,[], Basic Technicalities for Martingales,seg_189,"example 1.2 (sums of iids) let x1,x2, . . . be iid with means e(xi) = 0, and then define sn ≡ x1 + · · · + xn and an ≡ σ[s1, . . . , sn]. then e|sn| ≤ ∑i"
2663,0,[], Basic Technicalities for Martingales,seg_189,"2 is a submg by proposition 1.2, and that we have written"
2664,1,['process'], Basic Technicalities for Martingales,seg_189,2 = (sn 2 − nσ2) + nσ2 = (martingale) + (increasing process).
2665,0,[], Basic Technicalities for Martingales,seg_189,"this is an example of the doob decomposition of a submartingale, which we will establish in section 5."
2666,1,[], Basic Technicalities for Martingales,seg_189,"example 1.5 (wald’s mg) consider again example 1.2, but now suppose that the xk’s have a mgf φ(t) = e exp(tx). let yn ≡ exp(csn)/φ(c)n. then {yn,an}n∞=1 is a mg. note that the mg of example 1.2 is recovered by differentiating once with respect to c and setting c = 0; the mg of example 1.4 is recovered by differentiating twice with respect to c and setting c = 0."
2667,1,['standardized'], Basic Technicalities for Martingales,seg_189,"example 1.6 (brownian motion) let {s(t) : t ≥ 0} be standardized brownian motion, and let at ≡ σ[s(s) : s ≤ t]. then {s(t),at : t ≥ 0} is a mg."
2668,1,['transform'], Basic Technicalities for Martingales,seg_189,"example 1.7 let y (t) ≡ s(t)2 − t in example 1.6. then the brownian motion transform {y (t),at : t ≥ 0} is a mg."
2669,1,"['set', 'exponential', 'standard']", Basic Technicalities for Martingales,seg_189,"example 1.8 (the exponential mg for brownian motion) as in example 1.6, let s denote standard brownian motion, and much as in example 1.5, set"
2670,1,[], Basic Technicalities for Martingales,seg_189,differentiating once with respect to c and setting c = 0 yields the mg of example 1.6; differentiating twice with respect to c and setting c = 0 yields the mg that appears in example 1.7; higher-order derivatives yield mgs based on the hermite polynomials. (recall (11.5.15) and (12.7.13).)
2671,1,"['process', 'counting process']", Basic Technicalities for Martingales,seg_189,"example 1.10 (cumulative hazard λ(·), and a simple counting process) (a) let x have df f on the reals r. then"
2672,1,"['function', 'hazard function']", Basic Technicalities for Martingales,seg_189,is called the cumulative hazard function. note that
2673,1,"['counting process', 'failure', 'probability', 'process']", Basic Technicalities for Martingales,seg_189,"∼ note that when x = bernoulli(p) with 0 < p < 1, then λ(t) = (1 − p)1[0,∞)(t) + 1[1,∞)(t). roughly, 1 − f−(t) is the probability that y still “lives” just prior to time t. given this, df (t) is the “instantaneous probability” of a failure at time t. thus dλ(t) = df (t)/[1 − f−(t)] represents the instantaneous hazard at time t. (b) define the counting process"
2674,1,['process'], Basic Technicalities for Martingales,seg_189,"note that n is an ↗ and right-continuous process on r that is adapted to the history σ-fields at, and hence is a submg. (c) the class cs ≡ {[x > r] : −∞ ≤ r ≤ s} is a π-system that generates as. so any two finite measures that agree on cs also agree on as by the dynkin π-λ theorem. (d) we start with a bit of practice. the reader is to show in exercise 1.4 below that"
2675,1,['set'], Basic Technicalities for Martingales,seg_189,"(where 0 is interpreted as 0 and f (s, t] ≡ f (t) − f (s) ) by verifying that for every set a"
2676,1,['process'], Basic Technicalities for Martingales,seg_189,"(e) next, the process"
2677,1,"['sets', 'process']", Basic Technicalities for Martingales,seg_189,then verify ∫a(nt − ns)dp is equal to ∫a(at − as)dp for all sets a ∈ cs. statement (12) gives the so called “doob-meyer decomposition” of the submg n into the mg m and the ↗ process a. the motivation for the definition of a is found in (13.5.3) and (13.8.3).
2678,1,"['process', 'counting process']", Basic Technicalities for Martingales,seg_189,"example 1.11 (another counting process) suppose that the rvs ξ1, ξ2, . . . are iid uniform(0, 1), and let nn(t) ≡ ngn(t) ≡ (the number of ξi’s ≤ t). then nn is a counting process, since it is ↗, and it increases only by jumps upward of size +1. hence it is a submartingale. the reader will be asked to show (giving another doob–meyer decomposition) that the uniform empirical process un satisfies"
2679,1,"['function', 'covariance', 'process']", Basic Technicalities for Martingales,seg_189,"the covariance function of this process is s ∧ t for all 0 ≤ s, t ≤ 1."
2680,1,"['poisson', 'rate', 'poisson process', 'counting process', 'process']", Basic Technicalities for Martingales,seg_189,"example 1.12 (poisson process) suppose n(t) is a poisson process with rate λ > 0. it is a counting process and hence a submartingale. moreover, the process m[(t) ≡ n(t) − λt is a martingale, and the process m2(t) − λt is also a martingale."
2681,1,"['likelihood', 'probability']", Basic Technicalities for Martingales,seg_189,"example 1.13 (likelihood ratios) let (ω,a, p ) and (ω,a, q) be probability spaces for q and p . suppose that an is an ↗ sequence of sub σ-fields of a. suppose qn and pn denote the measures q and p , respectively, restricted to an, and suppose that qn pn. let xn ≡ dqn/dpn. then for a ∈ am and n > m we have ∫a xn dp = qn(a) = qm(a) = ∫a xm dp , so that ∫a(xn − xm)dp = 0. this shows that"
2682,1,['likelihood'], Basic Technicalities for Martingales,seg_189,"(17) {xn,an}∞n=1 is a mg of likelihood ratios."
2683,1,['independent'], Basic Technicalities for Martingales,seg_189,"example 1.14 (kakutani’s mg) let x1,x2, . . . be independent rvs with each xk ≥ 0 and exk = 1. let mn ≡ ∏n"
2684,0,[], Basic Technicalities for Martingales,seg_189,exercise 1.4 verify the claims made in example 1.10.
2685,0,[], Basic Technicalities for Martingales,seg_189,exercise 1.5 verify the claims made in example 1.11.
2686,1,"['set', 'exponential']", Basic Technicalities for Martingales,seg_189,"exercise 1.6 find the exponential martingale that corresponds to the mg m(t) in example 1.12. then differentiate this twice with respect to c, set c = 0 each time, and obtain the two mgs given in the example."
2687,1,"['sampling', 'case']", Simple Optional Sampling Theorem,seg_191,the following proposition gives a particularly simple special case of the optional sampling theorem. (what are the implications for gambling?)
2688,1,['sampling'], Simple Optional Sampling Theorem,seg_191,"proposition 2.1 (optional sampling) if {xn,an}∞n=0 is a s-mg and stopping times s and t (relative to these an’s) satisfy 0 ≤ s ≤ t ≤ n a.s. for some fixed integer n , then"
2689,1,['case'], Simple Optional Sampling Theorem,seg_191,proof. case 1: 0 ≤ t − s ≤ 1. let a ∈ as . then
2690,1,['event'], Simple Optional Sampling Theorem,seg_191,(d) = (an event in ai) ∩ (an event in ai) ∈ ai.
2691,1,['case'], Simple Optional Sampling Theorem,seg_191,"thus (b) holds, and case 1 is completed."
2692,0,[], Simple Optional Sampling Theorem,seg_191,"recall that sums and minima of such stopping times are stopping times, as in exercise 8.7.1. note that"
2693,1,"['stepwise', 'case']", Simple Optional Sampling Theorem,seg_191,"e(xrn−1 |ar0) by case 1, and stepwise smoothing"
2694,1,['convergence'], The Submartingale Convergence Theorem,seg_193,"theorem 3.1 (s-mg convergence theorem) let {xn,an}∞n=0 be a s-mg. (a) suppose exn+ ↗ m < ∞ (i.e., the xn+-submg is integrable). then"
2695,1,[], The Submartingale Convergence Theorem,seg_193,"in fact, supposing {xn,an}∞n=0 is a submg, the conclusions (aa), (bb), and (ee) of part (c) below are equivalent. if all xn ≥ 0, then (cc) and (dd) are also equivalent. (closing the s-mg means that ∫a xn dp ∫a x∞ dp for all a ∈ an and all n, with the terminal rv x∞ ∈ l1.) (c) if the {xn,an}∞n=0 of (a) is actually a mg, then the following are equivalent."
2696,1,['process'], The Submartingale Convergence Theorem,seg_193,"(d) in all the above, if {xt,at}t∈[0,∞) is a process on (d[0,∞),d[0,∞)), then"
2697,1,[], The Submartingale Convergence Theorem,seg_193,"[closing an x-martingale on [0,∞) by x∞ is the same as closing an x-martingale on [0, θ) by the limiting rv xθ.]"
2698,1,['interval'], The Submartingale Convergence Theorem,seg_193,"notation 3.1 if a sequence does not converge to an extended real value, then it must necessarily be oscillating. if it does so oscillate, then some interval must be “upcrossed” infinitely often. we seek to take advantage of this. let x1,x2, . . . be a sequence of rvs. let a < b. then:"
2699,1,['inequality'], The Submartingale Convergence Theorem,seg_193,"inequality 3.1 (upcrossing inequality; doob) if {xk,ak}nk=0 is a submg, then"
2700,0,[], The Submartingale Convergence Theorem,seg_193,"proof. the number of upcrossings of [a, b] made by xk and the number of upcrossings of [0, b − a] made by (xk − a)+ are identical; since (xk − a)+ is also a submg, we may assume that xk ≥ 0 and a = 0 in this proof. define"
2701,1,['process'], The Submartingale Convergence Theorem,seg_193,"here we use the convention that min ∅ ≡ n. clearly, these are stopping times that do satisfy 0 = t0 ≤ t1 ≤ · · · ≤ tn+2 = n. thus proposition 13.2.1 shows that the process {xti ,ati}n"
2702,0,[], The Submartingale Convergence Theorem,seg_193,"and since xti is a submg, we have"
2703,0,[], The Submartingale Convergence Theorem,seg_193,now suppose that u[
2704,1,['inequality'], The Submartingale Convergence Theorem,seg_193,"and this is (7) in disguise (since xk really denotes (xk − a)+). finally, note the positive part inequality (xn − a)+ ≤ xn+ + |a|."
2705,0,[], The Submartingale Convergence Theorem,seg_193,thus fatou’s lemma implies
2706,1,['condition'], The Submartingale Convergence Theorem,seg_193,"thus {xn,an}n∞=0 is a s-mg by condition (13.1.4). that is, the rv x∞ closes the s-mg (and thus (b) holds, except for the equivalence of (cc) and (dd))."
2707,0,[], The Submartingale Convergence Theorem,seg_193,"(j) → 0 by absolute continuity of the integral,"
2708,1,['sets'], The Submartingale Convergence Theorem,seg_193,and since the sets satisfy
2709,1,"['case', 'discrete']", The Submartingale Convergence Theorem,seg_193,this completes the entire proof in the case of discrete time.
2710,1,['continuous'], The Submartingale Convergence Theorem,seg_193,"consider (d). (continuous time) our preliminaries will not assume the s-mg structure. now, lim xt(ω) could be +∞ for some ω’s, and this will cause difficulties with the present approach. thus (following doob) define"
2711,1,"['range', 'transform']", The Submartingale Convergence Theorem,seg_193,"to transform the range space from [−∞,∞] to [−1, 1]. for each m choose rational numbers tm1, . . . , tmk in [m,∞) so that (remember, x : (ω,a, p ) → (d,d))"
2712,1,['limit'], The Submartingale Convergence Theorem,seg_193,"this is possible, since the sup over all rationals r in [m,∞) equals the sup over all reals t in [m,∞), and the rationals are the limit of {r1, . . . , rk} for any ordering {r1, r2, . . .} of the rationals; thus,"
2713,0,[], The Submartingale Convergence Theorem,seg_193,we may assume that the tmj ’s were chosen so as to simultaneously satisfy
2714,1,['events'], The Submartingale Convergence Theorem,seg_193,letting am and bm denote the events in (q) we see that
2715,1,['transforming'], The Submartingale Convergence Theorem,seg_193,"now, transforming back via xt(ω) = tan((π/2)yt(ω)), (r) implies the next lemma:"
2716,1,"['continuous', 'discrete']", The Submartingale Convergence Theorem,seg_193,"armed with the (9) “lemma,” it is now easy to use the discrete version of this theorem to prove the continuous version. we will refer to the continuous versions of conclusions (1)–(3) as (1′)–(3′). we return to the proof of the theorem. (now, we again assume s-mg structure in what follows.)"
2717,1,['discrete'], The Submartingale Convergence Theorem,seg_193,"let yi ≡ xti and ãi ≡ ati for the ti’s in (9). then (yi, ãi)∞i=0 is a s-mg to which the discrete theorems can be applied. thus,"
2718,1,['continuous'], The Submartingale Convergence Theorem,seg_193,exercise 3.1 complete the proof of the continuous part of theorem 3.1.
2719,1,['process'], The Submartingale Convergence Theorem,seg_193,"exercise 3.3 let a−∞ ⊂ · · · ⊂ a−1 ⊂ a0 ⊂ a1 ⊂ · · · ⊂ a∞ be sub σ-fields of the basic σ-field a. suppose the rv x ∈ l1(ω,a, p ). let yn ≡ e(x|an). then the process (yn,an)∞n=−∞ is necessarily a uniformly integrable mg."
2720,0,[], The Submartingale Convergence Theorem,seg_193,(a): the xn+’s are uniformly integrable.
2721,0,[], The Submartingale Convergence Theorem,seg_193,(b): there exists a rv y that closes the submg.
2722,0,[], The Submartingale Convergence Theorem,seg_193,"(c): when these hold, then x∞(which necessarily exists a.s., and is in l1) closes the submg."
2723,0,[], The Submartingale Convergence Theorem,seg_193,uniformly integrable if and only if the xn
2724,0,[], The Submartingale Convergence Theorem,seg_193,r-process is integrable.
2725,0,[], The Submartingale Convergence Theorem,seg_193,"exercise 3.7 (martingale lr-convergence theorem) (i) let {xn,an}∞n=0 be a mg sequence. let r > 1. then the following are equivalent:"
2726,1,['process'], The Submartingale Convergence Theorem,seg_193,"definition 3.1 (reversed s-mg) letxn be adapted to an with n ∈ {. . . ,−1, 0}, and define the σ-field a−∞ ≡ ∩0n=−∞an. the process {xn,an}0n=−∞ is a reversed s-mg (as defined earlier) if all e|xn| < ∞ and"
2727,0,['n'], The Submartingale Convergence Theorem,seg_193,"(this is like the second law of thermodynamics run backward in time, since an brings more stability as n ↘ .)"
2728,1,['convergence'], The Submartingale Convergence Theorem,seg_193,"theorem 3.2 (reversed s-mg convergence theorem) let {xn,an}0n=−∞ be a s-mg sequence, or a reversed s-mg. (a) it necessarily holds that"
2729,0,[], The Submartingale Convergence Theorem,seg_193,"(b)–(c) furthermore, the following are equivalent (and all yield an x∞ ∈ l1):"
2730,0,[], The Submartingale Convergence Theorem,seg_193,(18) xn’s are uniformly integrable.
2731,1,['process'], The Submartingale Convergence Theorem,seg_193,"(d) in all the above, if {xt,at}t∈(−∞,0] is a process on (d(−∞,0],d(−∞,0]), then"
2732,1,['process'], The Submartingale Convergence Theorem,seg_193,") ] now denote the upcrossings of [r, s] by the process"
2733,1,['case'], The Submartingale Convergence Theorem,seg_193,"uniformly in n ∈ {−∞, . . . ,−1, 0} as λ → ∞. thus (an analogous xn proof works only in the case of a mg),"
2734,0,['n'], The Submartingale Convergence Theorem,seg_193,"implies that the xn+’s are uniformly integrable. now, for n < m we have"
2735,0,['n'], The Submartingale Convergence Theorem,seg_193,"for all n ≤ (a fixed m that is large enough), since exn ↘ m"
2736,0,[], The Submartingale Convergence Theorem,seg_193,"thus, the xn− are uniformly integrable. thus, the xn are uniformly integrable; that is, (18) holds."
2737,1,['case'], The Submartingale Convergence Theorem,seg_193,"note that (20) trivially implies (17). the extension of this theorem from n to t uses (9) to extend (16), just as in the case of theorem 3.1. then the proof that the t versions of (16)–(20) hold adds nothing new."
2738,1,['tails'], The Submartingale Convergence Theorem,seg_193,since the tails yield (since [±yn ≥ λ] ∈ an)
2739,1,['convergence'], The Submartingale Convergence Theorem,seg_193,we just applied the s-mg convergence theorem (theorem 3.1) as n → ∞ and the reversed s-mg convergence theorem (theorem 3.2) as n → −∞.
2740,1,['function'], The Submartingale Convergence Theorem,seg_193,"that is, ∫a y∞ dp = ∫a xdp for all a ∈ ∪∞n=1an, where ∪∞n=1an is a field and a π-system that generates a∞; thus equality also holds for all a ∈ a∞, by the carathéodory extension theorem. thus y∞ = e(x|a∞), by only the zero function."
2741,1,['convergence'], Applications of the Smg Convergence Theorem,seg_195,the following examples give just a few selected applications to show the power of the various s-mg convergence theorems.
2742,1,['process'], Applications of the Smg Convergence Theorem,seg_195,"example 4.1 (slln) let x1,x2, . . . be iid μ. then the partial sum process sn ≡ x1 + · · · + xn satisfies"
2743,1,['symmetry'], Applications of the Smg Convergence Theorem,seg_195,= ∑ e(xk|sn)/n by symmetry k=1
2744,1,['symmetric'], Applications of the Smg Convergence Theorem,seg_195,"but lim(sn/n) is measurable with respect to the symmetric σ-field, and so it is a.s. a constant by the hewitt-savage 0-1 law of exercise 7.2.1; hence e(x1|a−∞) is a.s. a constant, by (d). but e[e(x|a−∞)] = μ, so that the constant must be μ; that is e(x1|a−∞) = μ a.s. thus (d) implies sn/n → μ a.s. and l1."
2745,1,"['kernel', 'symmetric']", Applications of the Smg Convergence Theorem,seg_195,"exercise 4.1 (slln for u -statistics) let y−n ≡ un be a u -statistic based on x1,x2, . . ., with a symmetric kernel h for which eh(x1,x2) is finite. (thus, h(x, y) = h(y, x) for all"
2746,1,"['symmetric', 'statistics', 'order statistics']", Applications of the Smg Convergence Theorem,seg_195,"x, y.) consider the σ-field a−n ≡ σ[xn:n,xn+1,xn+2, . . .], for the vector xn:n of the first n order statistics of the sequence. [hint. as with the slln above, the proof will again be based of the hewitt-savage 0-1 law for the symmetric σ-field.]."
2747,1,['kernels'], Applications of the Smg Convergence Theorem,seg_195,(c) extend this to higher-dimensional kernels.
2748,1,"['tail', 'independent']", Applications of the Smg Convergence Theorem,seg_195,"example 4.2 (kolmogorov’s 0-1 law) suppose that y1, y2, . . . are iid rvs and let an ≡ σ[y1, . . . , yn]. suppose that a ∈ t ≡ (tail σ-field) = ∩∞n=1σ[yn+1, yn+2, . . .]. since an is independent of t ,"
2749,1,"['function', 'functions', 'approximation']", Applications of the Smg Convergence Theorem,seg_195,"example 4.3 (approximation of l1 and l2 functions) fix the function f ∈ l1([0, 1],"
2750,1,"['function', 'step function']", Applications of the Smg Convergence Theorem,seg_195,summary let f ∈ l1 and define the step function fn
2751,1,['cases'], Applications of the Smg Convergence Theorem,seg_195,[so in both cases fn
2752,0,[], Applications of the Smg Convergence Theorem,seg_195,s(·) can be thought of as approximating the derivative of the indefinite
2753,1,['independent'], Applications of the Smg Convergence Theorem,seg_195,"example 4.4 (kakutani’s mg) let x1,x2, . . . be independent with each xk ≥ 0 and exk = 1. define"
2754,1,['convergence'], Applications of the Smg Convergence Theorem,seg_195,"with m0 ≡ 1. then {mn,an}1∞ is a mg for which all emn = 1, where an is an appropriate ↗ sequence of σ-fields (such as the histories). since mn is bounded in the space l1, the sm-g convergence theorem of (13.3.1) shows that"
2755,0,[], Applications of the Smg Convergence Theorem,seg_195,for the appropriate rv m∞. we now show that the following are equivalent:
2756,0,[], Applications of the Smg Convergence Theorem,seg_195,"(8) mn’s are uniformly integrable,"
2757,0,[], Applications of the Smg Convergence Theorem,seg_195,"whenever one (hence all) of these equivalent statements fails, then necessarily"
2758,1,"['convergence', 'normalized']", Applications of the Smg Convergence Theorem,seg_195,"proof. because of (6), equivalence of (7)–(9) follows from vitali’s theorem (or from the submartingale convergence theorem). equivalence of (10) and (11) is called for in the easy exercise 4.2 below. we first show that (10) implies (8). suppose (10) holds. define the normalized product"
2759,1,[], Applications of the Smg Convergence Theorem,seg_195,"thus {nn,an}∞1 is a mean-1 mg that is bounded in l2. since all ∏1"
2760,1,['inequality'], Applications of the Smg Convergence Theorem,seg_195,inequality (inequality 8.10.5) and the mct give
2761,1,"['convergence', 'mean']", Applications of the Smg Convergence Theorem,seg_195,"we next show that when (10) fails (that is, when ∏∞ 1 an = 0), then (7) fails (and that, in fact, (12) holds). now (13) notes that the nn all have mean 1, and hence they form an integrable mg. thus nn →a.s. (some n∞) ∈ l1 by the submartingale convergence theorem. hence,"
2762,1,"['branching process', 'model', 'process']", Applications of the Smg Convergence Theorem,seg_195,a branching process model
2763,1,['processes'], Applications of the Smg Convergence Theorem,seg_195,"example 4.5 (branching processes) let x denote the number of offspring of a particular type of individual, and let pk ≡ p (x = k) for k = 0, 1, . . .. we start at generation zero with a single individual z0 = 1, and it produces the individuals in a first generation of size z1. these in turn produce a second generation of size z2, and so forth. thus,"
2764,0,[], Applications of the Smg Convergence Theorem,seg_195,"where xnj denotes the number of offspring of the jth individual present in the nth generation. we assume that all xnj ’s are iid as the x above. also, we suppose"
2765,1,"['branching process', 'model', 'process']", Applications of the Smg Convergence Theorem,seg_195,we call this a simple branching process model. let
2766,1,['process'], Applications of the Smg Convergence Theorem,seg_195,proposition 4.1 the process
2767,1,['mean'], Applications of the Smg Convergence Theorem,seg_195,"(17) {wn,an}∞n=0 is a mg with mean ewn = 1,"
2768,0,[], Applications of the Smg Convergence Theorem,seg_195,proof. we note that
2769,0,[], Applications of the Smg Convergence Theorem,seg_195,while the mg property follows from
2770,0,[], Applications of the Smg Convergence Theorem,seg_195,we leave (18) to the following exercise.
2771,1,['variance'], Applications of the Smg Convergence Theorem,seg_195,exercise 4.4 verify the variance formula (18). verify (20) below.
2772,1,['functions'], Applications of the Smg Convergence Theorem,seg_195,notation 4.1 we define the generating functions f and fn of x and zn by
2773,0,[], Applications of the Smg Convergence Theorem,seg_195,it is easy to verify that
2774,1,"['branching process', 'process']", Applications of the Smg Convergence Theorem,seg_195,theorem 4.1 (branching process) (i) suppose that m = ex > 1 and also σ2 ≡ var[x] < ∞. then
2775,1,['probability'], Applications of the Smg Convergence Theorem,seg_195,"(22) p (w∞ = 0) = (the probability of ultimate extinction) = π,"
2776,0,[], Applications of the Smg Convergence Theorem,seg_195,"moreover, the chf φ of w∞ is characterized as the unique solution of"
2777,1,['convergence'], Applications of the Smg Convergence Theorem,seg_195,"proof. (i) now, ewn2 ≤ 1 + σ2/[m(m − 1)] for all n, so that the mg {wn,an}∞n=1 is square-integrable. thus the mg lr convergence of exercise 13.3.7 gives (21)."
2778,1,['continuous'], Applications of the Smg Convergence Theorem,seg_195,= f(lim φn(t/m)) since f is continuous on |r| ≤ 1
2779,1,['mean'], Applications of the Smg Convergence Theorem,seg_195,"≤ |f ′(t∗)| × |ψ(t) − φ(t)| for t∗ ∈ (0, 1) by the mean value theorem"
2780,0,['e'], Applications of the Smg Convergence Theorem,seg_195,and iterating (e) gives
2781,1,"['set', 'limit']", Applications of the Smg Convergence Theorem,seg_195,"(ii) set s = 0 in (20) to get p (zn+1 = 0) = f(p (zn = 0)) here p (zn = 0) is necessarily ↗. passing to the limit gives π = lim p (zn = 0) = f(π). but if m ≤ 1, then π = 1 is the only solution of π = f(π)."
2782,1,"['process', 'processes']", Decomposition of a Submartingale Sequence,seg_197,"definition 5.1 (predictable process) a predictable process {an,an}∞n=0 is one in which each an is an−1-measurable for each n ≥ 0; here a0 is a constant (or a0 is {∅,ω}-measu- rable). [especially interesting are processes that are both ↗ and predictable, since any submg can be decomposed as the sum of a mg and such a predictable process.]"
2783,1,['process'], Decomposition of a Submartingale Sequence,seg_197,"(1) xn = yn + an = [a mg] + [an ↗ and predictable process],"
2784,1,['process'], Decomposition of a Submartingale Sequence,seg_197,"where {yn,an}∞n=0 is a 0-mean mg and an is a predictable process satisfying"
2785,1,['process'], Decomposition of a Submartingale Sequence,seg_197,"with δxk ≡ xk − xk−1. clearly, an is an ↗ process and each of the an is an−1-measurable. so, it remains only to show that yn ≡ xn − an is a mg. now,"
2786,0,[], Decomposition of a Submartingale Sequence,seg_197,"so {yn,an}∞n=0 is indeed a mg. consider the uniqueness. suppose xn = yn + an is one such decomposition that works. elementary computations give"
2787,1,['states'], Decomposition of a Submartingale Sequence,seg_197,but the specification of the decomposition also states that
2788,0,[], Decomposition of a Submartingale Sequence,seg_197,subtracting (d) from (c) gives uniqueness via
2789,0,[], Decomposition of a Submartingale Sequence,seg_197,"the converse holds, since"
2790,0,[], Decomposition of a Submartingale Sequence,seg_197,"exercise 5.1 if the x-process is integrable, then the a-process is uniformly integrable (in either theorem 5.1 above or theorem 5.2 below)."
2791,1,['process'], Decomposition of a Submartingale Sequence,seg_197,"(4) xn = yn + an = [a mg] + [an ↗ and predictable process that is ≥ 0],"
2792,1,['function'], Decomposition of a Submartingale Sequence,seg_197,"where {yn,an}0n=−∞ is a mean-m mg and an is an an−1-measurable function with"
2793,0,[], Decomposition of a Submartingale Sequence,seg_197,"this decomposition is a.s. unique. conversely, if xn = yn + an as above, then {xn,an}0n=−∞ is a submg. [call an the compensator.]"
2794,0,['n'], Decomposition of a Submartingale Sequence,seg_197,"then an is clearly ≥ 0,↗, and an−1-measurable, provided that it can be shown to be welldefined (that is, provided the sum converges a.s.). now, with n ≤ m,"
2795,1,['hypothesis'], Decomposition of a Submartingale Sequence,seg_197,"by hypothesis. also,"
2796,1,['limit'], Decomposition of a Submartingale Sequence,seg_197,"with a well-defined finite limit. since ãm ≥ 0 and eãm < ∞, we know that ãm is finite a.s.; so (6) is well-defined. the ãm’s are ↗ and bounded below by 0. thus a−∞ ≡ limm→−∞ ãm exists a.s., and it is ≥ 0. moreover, the equalities in (d) show that eãm → 0 as m → −∞; just use the mct via"
2797,0,[], Decomposition of a Submartingale Sequence,seg_197,"let yn ≡ xn–an. lines (a)–(f) of the previous proof complete this proof, using (13.3.20) about mgs for the existence of y−∞."
2798,1,"['conditional', 'conditional variance', 'variation', 'variance']", Decomposition of a Submartingale Sequence,seg_197,"example 5.1 (predictable variation, or conditional variance of a mg) let {xn, an}∞n=0 be a mg with each exn"
2799,1,['process'], Decomposition of a Submartingale Sequence,seg_197,here an is the predictable process (with a0 ≡ ex02 ≥ 0) defined by
2800,1,"['conditional', 'conditional variance', 'variation', 'variance']", Decomposition of a Submartingale Sequence,seg_197,"n given in (9) is called the conditional variance or the predictable variation of the xn-process. note that (for 〈x〉n ≡ an),"
2801,1,"['process', 'variation']", Decomposition of a Submartingale Sequence,seg_197,"since we agree to also use the notation 〈x〉n to denote the predictable variation process an that corresponds to the mg {xn,an}∞n=0. summary for any mg {xn,an}n∞=0 having all exn"
2802,1,"['process', 'conditional', 'conditional variance', 'variation', 'variance']", Decomposition of a Submartingale Sequence,seg_197,"is always the predictable variation (or conditional variance, or compensator), and the conditionally centered process"
2803,1,[], Decomposition of a Submartingale Sequence,seg_197,"2 − 〈x〉n is a 0-mean mg with respect to the an’s, for n ≥ 0. martingale transforms"
2804,1,"['process', 'outcomes']", Decomposition of a Submartingale Sequence,seg_197,"definition 5.2 (h-transforms) let {hn}∞n=0 be a predictable process with respect to the filtration {an}∞n=0. [think of hn being the amount a gambler will wager at stage n, based only on complete knowledge of the outcomes of the game up through time n − 1 (but not, of course, through time n).] for some other process {xn}∞n=0, define the h-transform of x (to be denoted by {(h · x)n}∞n=0) by"
2805,0,[], Decomposition of a Submartingale Sequence,seg_197,"(we agree that δx0 ≡ x0, and that h0 is a constant.)"
2806,1,"['process', 'mean', 'case']", Decomposition of a Submartingale Sequence,seg_197,"theorem 5.3 (s-mg transforms) (i) let {xn,an}∞n=0 be a s-mg (or supermg). if {hn}∞n=0 is predictable with each hn ≥ 0 and bounded, then {(h · x)n,an}∞n=0 is a s-mg (or supermg). (the supermartingale case shows that there is no system for beating the house in an unfavorable game.) (ii) if {xn,an}∞n=0 is a mg and {hn}∞n=0 is predictable and bounded, then note that the process {(h · x)n,an}∞n=0 is a mg with mean h0ex0."
2807,0,[], Decomposition of a Submartingale Sequence,seg_197,proof. we compute
2808,1,"['case', 'inequality']", Decomposition of a Submartingale Sequence,seg_197,since hn+1 ≥ 0 and e(δxn+1|an) 0. note that eδx0 = ex0 in the mg case. (the supermg case just reverses the inequality.)
2809,1,['process'], Decomposition of a Submartingale Sequence,seg_197,(b) (h · x)n = xt∧n = (the stopped process) is a s-mg.
2810,1,"['process', 'variation']", Decomposition of a Submartingale Sequence,seg_197,"with predictable variation process {〈x〉n}∞n=0. let {hn}∞n=0 denote a predictable, bounded, and ≥ 0 process. then we know that {(h · x)2n} is a submg. we will now give the form taken by its predictable variation process 〈h · x〉n. also, we will summarize everything so far in one place."
2811,1,[], Decomposition of a Submartingale Sequence,seg_197,"theorem 5.4 (martingale transforms) suppose {xn,an}∞n=0 is a mg with exn"
2812,1,"['process', 'variation']", Decomposition of a Submartingale Sequence,seg_197,"for each n, and let {hn}∞n=0 be bounded, predictable, and ≥ 0. then the predictable variation process 〈x}n is given by"
2813,1,['process'], Decomposition of a Submartingale Sequence,seg_197,then the conditionally centered process
2814,1,['transform'], Decomposition of a Submartingale Sequence,seg_197,for n ≥ 0. the martingale transform
2815,1,['mean'], Decomposition of a Submartingale Sequence,seg_197,"wn ≡ (h · x)n ≡ ∑n k=0hkδxk (16) is a mg with mean h0ex0 with respect to the an’s,"
2816,1,"['process', 'variation']", Decomposition of a Submartingale Sequence,seg_197,for n ≥ 0. its predictable variation process 〈w 〉n is
2817,0,['n'], Decomposition of a Submartingale Sequence,seg_197,"moreover, for n ≥ 0, the sequence"
2818,0,[], Decomposition of a Submartingale Sequence,seg_197,"proof. recall example 5.1 and theorem 5.3 for the first parts. then by straightforward calculation from (13), we have"
2819,0,[], Decomposition of a Submartingale Sequence,seg_197,since hk is ak−1-measurable
2820,1,['mean'], Decomposition of a Submartingale Sequence,seg_197,"note that l0 = w02 − 〈w0〉 = [h0δx0]2 − h02e(x02) = h02(x02 − ex02) has mean 0, while ln is a mg by example 5.1."
2821,1,['inequality'], Decomposition of a Submartingale Sequence,seg_197,"exercise 5.2 let (xn,an)n∞=0 be a submg. let mn ≡ sup{|xn| : n ≥ 1}. use the doobmeyer decomposition and doob’s inequality 8.10.2 to show that"
2822,1,['sampling'], Optional Sampling,seg_199,"we now extend the simple optional sampling theorem of section 13.2. our aim will be to relax the restrictive assumption used there, that the stopping times are bounded."
2823,0,[], Optional Sampling,seg_199,notation 6.1 suppose that
2824,1,['hypotheses'], Optional Sampling,seg_199,"so that the x̃n’s are adapted to the ãn’s. we would like to prove that {x̃n, ãn}∞n=0 is a s-mg, but this requires hypotheses. the weakest such hypotheses presented are"
2825,1,[], Optional Sampling,seg_199,(but these conditions (4) and (5) need to be replaced by useful conditions that are more easily verifiable.)
2826,1,"['sampling', 'process']", Optional Sampling,seg_199,"theorem 6.1 (optional sampling theorem) let (1)–(3) define the sequence {x̃n, ãn}∞n=0 with respect to the s-mg {xn,an}∞n=0. suppose (4) and (5) hold. then the optionally sampled process"
2827,1,['condition'], Optional Sampling,seg_199,"corollary 1 (a) condition (4) holds if {xn,an}n∞=0 is integrable."
2828,1,[], Optional Sampling,seg_199,(b) conditions (4) and (5) both hold if any of the following conditions holds:
2829,0,[], Optional Sampling,seg_199,(9) the xn’s are uniformly integrable.
2830,0,[], Optional Sampling,seg_199,"etj < ∞ for all j, and there exists a constant k such that for all j,"
2831,0,[], Optional Sampling,seg_199,notation 6.2 the theorem becomes much cleaner if our s-mg includes an entry at ∞ that closes the s-mg. suppose
2832,0,[], Optional Sampling,seg_199,"for extended stopping times t0, t1, . . . , t∞. we again define"
2833,1,['sampling'], Optional Sampling,seg_199,theorem 6.2 (optional sampling theorem) suppose (12)–(14) hold. then
2834,1,['sampling'], Optional Sampling,seg_199,"theorem 6.3 (optional sampling theorem) suppose that the x-process is integrable and satisfies x : (ω,a, p ) → (d[0,∞),d[0,∞)), and that it is adapted to some filtration {at}t∈[0,∞). then"
2835,0,[], Optional Sampling,seg_199,"in theorem 6.1, corollary 1 (only (11) must be omitted from the list of things that carry over with no change), and theorem 6.2."
2836,0,[], Optional Sampling,seg_199,proof. consider theorem 6.1. let a ∈ atn−1 . it suffices to show that
2837,0,[], Optional Sampling,seg_199,"basically, we wish to use proposition 13.2.1 and the dct. to this end we define"
2838,0,['n'], Optional Sampling,seg_199,"thus, for n fixed we have"
2839,1,['set'], Optional Sampling,seg_199,dp as the integrands are equal on the set
2840,1,[], Optional Sampling,seg_199,"tn e|xtn−1 | and e|xtn | are finite, the dct implies (recall that a = b ⊕ c means that |a − b| ≤ c)"
2841,0,[], Optional Sampling,seg_199,"it remains to show that extn supk exk. now,"
2842,0,[], Optional Sampling,seg_199,"proof. consider the first claim made in the corollary. that is, we verify that (4) holds if"
2843,1,['hypothesis'], Optional Sampling,seg_199,(d) ≤ (some m) < ∞ by hypothesis.
2844,1,['continuous'], Optional Sampling,seg_199,"proof. consider (9). uniformly integrable xn’s are uniformly bounded in l1 and are uniformly absolutely continuous, by theorem 3.5.4. uniformly bounded in l1 means that supk≥1 e|xk| < ∞; hence by part (a) of the corollary, we have (4). since tn < ∞ a.s., we have p (tn > k) → 0 as k → ∞, and hence uniform absolute continuity implies (5)."
2845,1,['process'], Optional Sampling,seg_199,"(a) < ∞, since the process is integrable and x∞ ∈ l1."
2846,0,[], Optional Sampling,seg_199,we add to each of (d) and (f) the equal terms of the equation
2847,0,[], Optional Sampling,seg_199,replace tn by t∞ in the previous paragraph to see that
2848,1,['expectation'], Optional Sampling,seg_199,"finally, (h), (i), and (13.1.4) show that {xtn ,atn}∞n=0 is a s-mg. add in ta ≡ 0 and tb ≡ ∞ for the expectation claim, with a = ω in (h) and (i)."
2849,0,[], Optional Sampling,seg_199,"proof. consider theorem 6.3. we must consider the extension of theorem 6.2. it suffices to consider the stopping times a pair at a time; we will do so, relabeling them so that s ≤ t a.s. let dn ≡ {k/2n : k = 0, 1, . . . , }, and note that"
2850,0,['n'], Optional Sampling,seg_199,define extended stopping times t (n) by
2851,0,[], Optional Sampling,seg_199,and make an analogous definition for s(n); it is trivial that these rvs are extended stopping times. note that a.s.
2852,0,[], Optional Sampling,seg_199,"now, by right continuity of the paths,"
2853,1,['limit'], Optional Sampling,seg_199,thus vitali’s theorem allows us to pass to the limit in (d) and obtain
2854,0,[], Optional Sampling,seg_199,provided that we show that
2855,0,['n'], Optional Sampling,seg_199,from this we need only the rather minor fact (since ex0 ext (n) for this s-mg pair) that
2856,1,['case'], Optional Sampling,seg_199,exercise 6.1 prove theorem 6.3 (for the case of integrable xt in theorem 6.1).
2857,1,['case'], Optional Sampling,seg_199,exercise 6.2 prove theorem 6.3 (for the corollary to theorem 6.1 case).
2858,0,[], Optional Sampling,seg_199,"exercise 6.3 write out all the details of step (h), in the context ot theorem 6.2."
2859,1,[], Applications of Optional Sampling,seg_201,"(2) sn is a mean-0 mg,"
2860,1,[], Applications of Optional Sampling,seg_201,(1) ≡ sn 2 − n is a mean-0 mg.
2861,1,[], Applications of Optional Sampling,seg_201,"(2) ≡ sn − n(p − q) is a mean-0 mg,"
2862,1,[], Applications of Optional Sampling,seg_201,(5) zn ≡ (q/p)sn = exp(c0sn) = exp(c0sn)/φn(c0) is a mean-1 mg.
2863,0,[], Applications of Optional Sampling,seg_201,we now make the claim (see exercise 7.1 below)
2864,1,['probability'], Applications of Optional Sampling,seg_201,"with probability 1, the rv sτ takes on one of the values −a or b. now, τ ∧ m ↗ τ a.s. and sτ∧m → sτ a.s., while (τ ∧ m) is a bounded stopping time to which proposition 13.2.1 or"
2865,0,[], Applications of Optional Sampling,seg_201,"1 theorem 6.1 necessarily applies. thus, for p = q = 2 we can conclude that"
2866,1,['function'], Applications of Optional Sampling,seg_201,by proposition 13.2.1 and the dct with dominating function a + b; and
2867,0,[], Applications of Optional Sampling,seg_201,"by the mct, proposition 13.2.1, and the dct. solving these gives"
2868,0,[], Applications of Optional Sampling,seg_201,justifying the other two equations in (6) (zn
2869,1,['condition'], Applications of Optional Sampling,seg_201,"(2) is analogous to zn (1), while zn uses condition"
2870,1,[], Applications of Optional Sampling,seg_201,"∼ note that if μ ≡ p − q < 0, then [max0≤n<∞ sn] = geometric(p/q). that is,"
2871,1,['probability'], Applications of Optional Sampling,seg_201,(just let a → ∞ in the formula for p (sτ = −a) to obtain the complementary probability.)
2872,1,['set'], Applications of Optional Sampling,seg_201,"then set θ = −2μ, and recall (12.7.8) to conclude that"
2873,1,[], Applications of Optional Sampling,seg_201,(12) exp(θ[sμ(t) − μt] − θ2t/2) = exp(−2μ[s(t) + μt]) is a mean-1 mg.
2874,1,['sampling'], Applications of Optional Sampling,seg_201,"applying the optional sampling theorem to (11) and (12), we obtain"
2875,1,"['tail', 'exponential', 'tail probability', 'probability']", Applications of Optional Sampling,seg_201,"let ‖s+μ ‖∞0 ≡ sup0≤t<∞ sμ(t). note that if μ < 0, then ‖s+μ ‖∞0 =∼ exponential(2|μ|). the exponential (2|μ|) tail probability gives"
2876,0,[], Applications of Optional Sampling,seg_201,note the complete analogy with example 7.1.
2877,0,[], Applications of Optional Sampling,seg_201,exercise 7.1 give all details in justifying the final two equalities in (6).
2878,0,[], Applications of Optional Sampling,seg_201,exercise 7.2 verify completely the claims of example 7.2. (recall theorem 12.7.1 and theorem 12.7.2.)
2879,1,"['poisson', 'process', 'poisson process']", Applications of Optional Sampling,seg_201,exercise 7.3 derive an analogue of the previous example 7.2 that is based on the poisson process {n(t) : t ≥ 0}.
2880,1,"['treatment', 'process', 'counting process']", Introduction to Counting Process Martingales,seg_203,heuristic treatment of counting process martingales
2881,1,['process'], Introduction to Counting Process Martingales,seg_203,"suppose now that the process {m(x),ax}x∈r is a martingale. then for every increment m(x + h) − m(x) we have e{m(x + h) − m(x)|ax} = 0. operating heuristically, this suggests that"
2882,0,[], Introduction to Counting Process Martingales,seg_203,"where ax− is the σ-field generated by everything up to (but not including) time x. with this background, we now turn to our problem."
2883,1,"['process', 'counting process']", Introduction to Counting Process Martingales,seg_203,(2) n(x) is a counting process;
2884,1,"['process', 'counting process']", Introduction to Counting Process Martingales,seg_203,a counting process is (informally) an ↗ process that can increase only by taking jumps of size +1. incremental change is modeled via
2885,0,[], Introduction to Counting Process Martingales,seg_203,it then seems that
2886,1,['process'], Introduction to Counting Process Martingales,seg_203,and ax−-measurable process.
2887,1,"['process', 'variation']", Introduction to Counting Process Martingales,seg_203,"we compute the predictable variation process 〈m〉 of the martingale m (as suggested by (13.5.3) or (13.5.8), and using integration by parts) via"
2888,1,['process'], Introduction to Counting Process Martingales,seg_203,"thus (note (13.5.12) and (5)), the process"
2889,0,[], Introduction to Counting Process Martingales,seg_203,"which suggests that, provided that each em2(x) < ∞,"
2890,0,[], Introduction to Counting Process Martingales,seg_203,"summary starting with a martingale m(.) having all em2(x) < ∞, it seems that"
2891,1,['variation'], Introduction to Counting Process Martingales,seg_203,"(11) 〈m〉(·) is the predictable variation of the submartingale {m2(x),ax}t∈r."
2892,1,['process'], Introduction to Counting Process Martingales,seg_203,"that is, 〈m〉 (·) is the ↗, ≥ 0, and ax−-measurable process whose existence is guaranteed by the doob-meyer decomposition (see 13.9.2 in shorack (2000)). [note that we (“discovered” this without using said doob-meyer theorem; that theorem will merely guarantee that our heuristic guess-and-verify approach (assuming that we can make it rigorous) gives us the “right answer” this is typical.] (note fleming and harrington (1991).)"
2893,1,['transform'], Introduction to Counting Process Martingales,seg_203,consider the martingale transform
2894,1,['variation'], Introduction to Counting Process Martingales,seg_203,"moreover, its predictable variation is given by"
2895,0,[], Introduction to Counting Process Martingales,seg_203,"this also suggests, provided that each ew 2(x) < ∞, that"
2896,1,['transform'], Introduction to Counting Process Martingales,seg_203,"processes h(.) that are ax−-measurable satisfy h(x) = e{h(x)|ax−}, and so h(x) can be determined by averaging h(.) over the past; such an h is thus called predictable. the martingale transform statement (12) can be summarized as"
2897,0,[], Introduction to Counting Process Martingales,seg_203,"x ∞ [predictable] d [martingale] = [martingale],"
2898,1,['expectations'], Introduction to Counting Process Martingales,seg_203,provided that expectations exist.
2899,1,"['normal', 'process', 'condition']", Introduction to Counting Process Martingales,seg_203,suppose now that we have a sequence of martingales mn whose increments satisfy a type of lindeberg condition; this suggests that any limiting process m ought to be a normal process. from the martingale condition we hope that
2900,1,"['independent', 'normal', 'mean', 'variance', 'process']", Introduction to Counting Process Martingales,seg_203,and for a normal process m(.) uncorrelated increments also mean independent increments. the variance process of m(.) should be em2(x) = limn emn2(x) = limn e〈mn〉(x) by (6). so it seems reasonable to hope that [recall (12.1.15)]
2901,0,[], Introduction to Counting Process Martingales,seg_203,"for a brownian motion s, provided that"
2902,1,['condition'], Introduction to Counting Process Martingales,seg_203,"(18) the increments of mn satisfy a type of lindeberg condition,"
2903,0,[], Introduction to Counting Process Martingales,seg_203,and provided that (note (9))
2904,1,['continuous'], Introduction to Counting Process Martingales,seg_203,(20) v is ↗ and right continuous with v (−∞) = 0.
2905,0,[], Introduction to Counting Process Martingales,seg_203,"as noted above,"
2906,0,[], Introduction to Counting Process Martingales,seg_203,"of course, the original martingales mn need to be square integrable. this “quasi theorem” is roughly rebolledo’s clt."
2907,1,['processes'], Introduction to Counting Process Martingales,seg_203,one other bit of heuristics seems in order. suppose now that we have several counting processes ni(x) and that we perform the above calculations and determine martingales mi(x) = ni(x) − ai(x) with 〈mi〉 (x) = ∫−
2908,1,[], Introduction to Counting Process Martingales,seg_203,"in fact, conditions under which all of the previous heuristics are actually true are given below. even without these, we can use these heuristics as the first step in a guess-and-verify approach."
2909,1,['sample'], Introduction to Counting Process Martingales,seg_203,the guess-and-verify approach in a single sample iid as f
2910,1,"['process', 'counting process']", Introduction to Counting Process Martingales,seg_203,then ni is a counting process with
2911,0,[], Introduction to Counting Process Martingales,seg_203,satisfies (as verified in exercise 13.1.4)
2912,1,"['process', 'variation']", Introduction to Counting Process Martingales,seg_203,the predictable variation process is (letting ∫−
2913,0,[], Introduction to Counting Process Martingales,seg_203,"since the sum of martingales is also a martingale, we have"
2914,0,[], Introduction to Counting Process Martingales,seg_203,"moreover, (23) tells us that"
2915,0,[], Introduction to Counting Process Martingales,seg_203,"our heuristic rebolledo clt thus suggests that for a brownian motion s,"
2916,1,['process'], Introduction to Counting Process Martingales,seg_203,it is clear from theorem 12.10.1 that a skorokhod embedding version m̃n of the original process mn should (and will) satisfy
2917,0,[], Introduction to Counting Process Martingales,seg_203,where (note exercise 8.1 below)
2918,1,[], Introduction to Counting Process Martingales,seg_203,"∼ exercise 8.1 (a) computing means and covariances to suggest that m[= s(v ). (b) verify that (zn ≡ m2n − 〈mn〉,ax)x∈r̄ is a u.i.0-mean mg, and identify zn(∞)."
2919,1,"['model', 'random']", Introduction to Counting Process Martingales,seg_203,the random censorship model
2920,1,"['independent', 'random']", Introduction to Counting Process Martingales,seg_203,"example 8.2 (random censorship) suppose now that x1, . . . , xn are iid nondegenerate survival times with df f on [0,∞). however, we are not able to observe the xi’s due to the following random censoring. let y1, . . . , yn be iid censoring times whose df g is an arbitrary df on [0,∞]. suppose also that the yi’s are independent of the xi’s. all that is observable is"
2921,0,[], Introduction to Counting Process Martingales,seg_203,let hnuc and huc denote the empirical df and the true df of the uncensored rvs; so
2922,1,"['process', 'counting process']", Introduction to Counting Process Martingales,seg_203,the basic counting process here is
2923,1,"['function', 'hazard function']", Introduction to Counting Process Martingales,seg_203,"in analogy with exercise 13.1.4, it can be shown (see exercise 8.2 below) that, for the cumulative hazard function defined by"
2924,0,[], Introduction to Counting Process Martingales,seg_203,the compensator must satisfy
2925,0,[], Introduction to Counting Process Martingales,seg_203,"this defines the basic martingale on [0,∞) to be"
2926,1,"['process', 'variation']", Introduction to Counting Process Martingales,seg_203,the predictable variation process should be
2927,0,[], Introduction to Counting Process Martingales,seg_203,where we have defined
2928,0,[], Introduction to Counting Process Martingales,seg_203,we would expect that a skorokhod version m̃n of mn would satisfy
2929,0,['e'], Introduction to Counting Process Martingales,seg_203,"where for appropriately defined euc and e,"
2930,1,"['function', 'hazard function', 'estimator']", Introduction to Counting Process Martingales,seg_203,the cumulative hazard function λ(.) and the aalen–nelson cumulative hazard function estimator λ̂n(.) are defined by
2931,1,"['function', 'estimator', 'table']", Introduction to Counting Process Martingales,seg_203,"for all t ∈ [0,∞). this is motivated by the deterministic halves of (39) and (40). we use this to form an estimator of the df f , that is, a type of instantaneous life table. the kaplan–meier product-limit estimator of the survival function 1 − f is defined by"
2932,1,['processes'], Introduction to Counting Process Martingales,seg_203,fundamental processes we define
2933,0,[], Introduction to Counting Process Martingales,seg_203,t [predictable] d [martingale] = [martingale](t).
2934,0,[], Introduction to Counting Process Martingales,seg_203,"exercise 8.2 suggest that on [0,∞) we have"
2935,0,[], Introduction to Counting Process Martingales,seg_203,exercise 8.4 use integration by parts to show that
2936,0,[], Introduction to Counting Process Martingales,seg_203,t[predictable] d [martingale] = [martingale](t).
2937,1,['process'], Introduction to Counting Process Martingales,seg_203,use the above heuristics to suggest (it can be proved by either mg or empirical process methods) that
2938,1,"['function', 'continuous']", Introduction to Counting Process Martingales,seg_203,"(which is a ≥ 0, right continuous, and ↗ function), since"
2939,0,[], Introduction to Counting Process Martingales,seg_203,these facts suggest that (for the d(.) of (64))
2940,0,[], Introduction to Counting Process Martingales,seg_203,we now prove the corresponding result for btn .
2941,1,"['convergence', 'distributions']", Introduction to Counting Process Martingales,seg_203,"so chentsov’s theorem gives relative compactness. we leave convergence of the finite-dimen- sional distributions to an exercise below. define yt ≡ bnt , and note that yt is of the form"
2942,1,['results'], Introduction to Counting Process Martingales,seg_203,we will require two basic results. it holds that
2943,1,['continuous'], Introduction to Counting Process Martingales,seg_203,"where c(.) as defined in (60) is ≥ 0, right continuous, and ↗, and where"
2944,0,[], Introduction to Counting Process Martingales,seg_203,"these will be established below. however, we first use them to give a proof of (67)."
2945,1,['conditional'], Introduction to Counting Process Martingales,seg_203,"(e) ≤ e{|y (r, s]|3/2e{y (s, t]2|as}3/4} by conditional liapunov"
2946,1,['inequality'], Introduction to Counting Process Martingales,seg_203,"3)1/4c(s, t]3/4(e{y (r, s]2})3/4 by holder’s inequality"
2947,1,['inequality'], Introduction to Counting Process Martingales,seg_203,"3)1/2c(r, s]3/4c(s, t]3/4 by liapunov’s inequality"
2948,1,[], Introduction to Counting Process Martingales,seg_203,"∼ where z(v) = binomia1(n[1 − hn−(s)], [(1 − h−(v)]/[1 − h−(s)])"
2949,1,[], Introduction to Counting Process Martingales,seg_203,"(69) e{z−11[z>0]} ≤ 2/(mp) when z =∼ binomial(m, p)."
2950,1,"['convergence', 'distributions']", Introduction to Counting Process Martingales,seg_203,"exercise 8.5 complete the proof of theorem 8.1, by showing convergence of the finitedimensional distributions."
2951,0,[], Introduction to Counting Process Martingales,seg_203,exercise 8.6 show that (69) may be extended to give
2952,1,[], Introduction to Counting Process Martingales,seg_203,"1 k 1[z>0]} ≤ k(k + 1)/(mp)k when z =∼ binomial(m, p), and k ≥ 1."
2953,1,"['continuous', 'treatment', 'data']", Introduction to Counting Process Martingales,seg_203,"remark 8.1 at this point, three sections of the 1st edition have been entirely omitted. the first two contained a general form of the doob–meyer decomposition for continuous parameter martingales followed by a treatment of martingales of the form ∫ hdm = ∫[predictable ]d[martingale]. the final omitted section treated the basic censored data martingale. together, these three sections, form the continuous analog to the current section 13.5 and section 13.8."
2954,1,['probability'], CLTs for Dependent RVs o,seg_205,"let {xnk : k = 1, 2, . . . and n = 1, 2, . . .} be an array of rvs on a basic probability space (ω,a, p ). for each n we suppose that xn1,xn2, . . . are adapted to an ↗ sequence of σ-fields an0 ⊂ an1 ⊂ an2 ⊂ · · · ⊂ a. for each n we suppose that κn in an integer-valued stopping time with respect to these (ank)∞k=0. we now introduce"
2955,0,[], CLTs for Dependent RVs o,seg_205,our interest is in the sum
2956,0,[], CLTs for Dependent RVs o,seg_205,we will have reason to consider
2957,1,"['asymptotic', 'normality']", CLTs for Dependent RVs o,seg_205,"what follows is the most basic clt in this monograph. for row-independent rvs and κn ≡ n it reduces to the asymptotic normality conclusion in (10.5.30), with c = 1. the second theorem implies the first, and is very much in the spirit of the lindeberg theorem."
2958,1,['dependent'], CLTs for Dependent RVs o,seg_205,"theorem 9.1 (basic dependent clt) conclude that sn →d n(0, 1) if"
2959,0,[], CLTs for Dependent RVs o,seg_205,comments if one replaces (5) by
2960,0,[], CLTs for Dependent RVs o,seg_205,then (6) may be replaced by
2961,0,[], CLTs for Dependent RVs o,seg_205,we now summarize these last claims.
2962,1,[], CLTs for Dependent RVs o,seg_205,"(15) conditions (7) and (9) hold,"
2963,1,[], CLTs for Dependent RVs o,seg_205,"(16) conditions (12) and (14) hold,"
2964,1,[], CLTs for Dependent RVs o,seg_205,"(17) conditions (4), (12), and (13) hold."
2965,1,['results'], CLTs for Dependent RVs o,seg_205,note hall and heyde (1980) for such results.
2966,1,"['probability measures', 'results', 'information', 'distribution', 'probability', 'variation', 'function', 'convergence']", Convergence in Distribution on Metric Spaces o,seg_209,"many results for convergence in distribution generalize to probability measures on a general metric space (m,d) equipped with its borel σ-field md. we call such measures borel measures. instead of using convergence of dfs to define convergence in distribution (or law), we use directly the helly-bray idea embodied in the →d equivalences of theorem 9.1.4, and that theorem is here extended to general metric spaces. skorokhod’s construction is also generalized to complete and separable metric spaces. section 12.1 gave specific information on the two very important metric spaces that gave rise to (c, c) and (d,d). in section 2 the dual bounded lipschitz metric will be introduced, along with hellinger, prohorov, and total variation metrics. these are useful on function spaces."
2967,1,"['probability measures', 'distribution', 'probability', 'convergence']", Convergence in Distribution on Metric Spaces o,seg_209,"definition 1.1 (convergence in distribution) if {pn : n ≥ 1} and p are probability measures on (m,d,md) satisfying"
2968,1,"['functions', 'continuous', 'distribution', 'random']", Convergence in Distribution on Metric Spaces o,seg_209,"[where cb(m) ≡ {all bounded and d-continuous functions g from m to r}, and cbu(m) denotes those functions that are additionally d-uniformly continuous], then we say that pn converges in distribution (or law) to p , or that pn converges weakly to p ; and we write pn →d p or pn →l p . similarly, if xn, x are random elements in m for which"
2969,1,"['probability', 'probability measures']", Convergence in Distribution on Metric Spaces o,seg_209,"theorem 1.1 (portmanteau theorem; billingsley) for probability measures {pn : n ≥ 1} and p on any metric space (m,d,md) the following are equivalent:"
2970,1,['sets'], Convergence in Distribution on Metric Spaces o,seg_209,(5) lim pn(b) ≤ p (b) for all closed sets b ∈ md.
2971,1,['sets'], Convergence in Distribution on Metric Spaces o,seg_209,(6) lim pn(b) ≥ p (b) for all open sets b ∈ md.
2972,1,['sets'], Convergence in Distribution on Metric Spaces o,seg_209,(7) lim pn(b) = p (b) for all p -continuity sets b ∈ md.
2973,1,['set'], Convergence in Distribution on Metric Spaces o,seg_209,"consider (4) implies (5): suppose that (4) holds and that b is closed. let > 0. then for integer m large enough, the set bm ≡ {x : d(x,b) < 1/m} satisfies"
2974,1,['continuous'], Convergence in Distribution on Metric Spaces o,seg_209,"and for each m ≥ 1, gm is lipschitz and uniformly continuous. hence, by (4) and also (a) and (b),"
2975,0,[], Convergence in Distribution on Metric Spaces o,seg_209,"since > 0 was arbitrary, (5) follows."
2976,1,"['set', 'transform']", Convergence in Distribution on Metric Spaces o,seg_209,"equivalence of (5) and (6) follows easily by taking complements. consider (5) implies (3): suppose that g ∈ cb(m) and that (5) holds. now, transform g linearly so that 0 ≤ g(x) ≤ 1. fix k ≥ 1, and define the closed set"
2977,0,[], Convergence in Distribution on Metric Spaces o,seg_209,then it follows that
2978,0,[], Convergence in Distribution on Metric Spaces o,seg_209,rewriting the sum on the right side and summing by parts gives
2979,1,['summation'], Convergence in Distribution on Metric Spaces o,seg_209,which together with a similar summation by parts on the left side yields
2980,1,['sets'], Convergence in Distribution on Metric Spaces o,seg_209,"applying the right side of (g) to pn, and then using (5) for the closed sets bj , and then applying the left side of (g) to p gives"
2981,0,[], Convergence in Distribution on Metric Spaces o,seg_209,applying (i) to (the nontransformed) −g yields
2982,1,['set'], Convergence in Distribution on Metric Spaces o,seg_209,"consider (5) implies (7): with b0 the interior of any set b ∈ m and b̄ its closure, (5) and (6) give"
2983,1,['set'], Convergence in Distribution on Metric Spaces o,seg_209,"if b is a p -continuity set, then p (∂b) = 0 and p (b̄) = p (b0), so the extreme terms in (k) are equal; thus limpn(b) = p (b), as required by (7)."
2984,1,"['sets', 'disjoint']", Convergence in Distribution on Metric Spaces o,seg_209,"consider (7) implies (5): since ∂{x : d(x,b) ≤ δ} ⊂ {x : d(x,b) = δ}, the boundaries are disjoint for different δ > 0, and hence at most countably many of them can have positive p -measure. therefore, for some sequence δk → 0, the sets bk ≡ {x : d(x,b) < δk} are p - continuity sets and bk ↘ b if b is closed. it follows from b ⊂ bk and then (7)"
2985,0,[], Convergence in Distribution on Metric Spaces o,seg_209,"then (5) follows from the monotone property of p , since bk ↘ b as k → ∞."
2986,0,[], Convergence in Distribution on Metric Spaces o,seg_209,proposition 1.1 pn →d p if and only if each subsequence {pn′} contains a further subsequence {pn
2987,0,[], Convergence in Distribution on Metric Spaces o,seg_209,"proof. this is easy from definition 1.1 (and the fact that a sequence of real numbers has xn → x if and only if each {xn′} contains a further subsequence {xn′′} such that xn′′ → x), as in the corollary to helly’s selection theorem 9.1.3."
2988,1,['random'], Convergence in Distribution on Metric Spaces o,seg_209,"theorem 1.2 (slutsky’s theorem) suppose that xn, yn are random elements taking values in a separable metric space (m,d,md), both defined on some ωn."
2989,1,['set'], Convergence in Distribution on Metric Spaces o,seg_209,"proof. for a closed set b and δ > 0 let bδ ≡ {x : d(x,b) < δ}. then"
2990,0,[], Convergence in Distribution on Metric Spaces o,seg_209,"the second term on the right side of (a) goes to zero, since d(xn, yn) →p 0 (note the following exercise). then xn →d x gives"
2991,1,"['continuous', 'set']", Convergence in Distribution on Metric Spaces o,seg_209,"theorem 1.3 (continuous mapping theorem) let xn →d x on (m,d,md), and suppose g : m → m̄ (where (m̄, d̄) is another metric space) is continuous a.s. with respect to p ≡ px (that is, p (x ∈ cg) = 1 for the continuity set cg of g). then, necessarily, g(xn) →d g(x)."
2992,1,[], Convergence in Distribution on Metric Spaces o,seg_209,"proof. we simply note that this is essentially no different from the proof of the mann–wald theorem. (only now we apply the general skorokhod construction of the following theorem, instead of the elementary skorokhod theorem.) [this proof, however, requires that the metric space be complete and separable. the next exercise asks the reader to provide a general proof.]"
2993,1,"['set', 'continuous', 'transformation']", Convergence in Distribution on Metric Spaces o,seg_209,exercise 1.2 prove the continuous mapping theorem above by appeal to (5) and then proposition 2.2.4 (that the discontinuity set of a transformation between metric spaces is always measurable). this proof requires neither completeness nor separability! (and this is actually a much more elementary proof.)
2994,0,[], Convergence in Distribution on Metric Spaces o,seg_209,skorokhod’s construction
2995,1,"['random variables', 'variables', 'probability', 'random']", Convergence in Distribution on Metric Spaces o,seg_209,"we earlier established the elementary form of skorokhod’s theorem: if random variables xn →d x0, then there exist random variables {yn : n ≥ 0} defined on a common probability"
2996,1,"['sets', 'transformation']", Convergence in Distribution on Metric Spaces o,seg_209,"∼ space satisfying yn = xn for all n ≥ 0 and yn →a.s. y0. that proof relied on the inverse transformation. we now turn to the extension of the elementary skorokhod theorem from r to a complete and separable metric space (m,d), whose open sets generate the borel σ- field. [the first step in the proof will be to establish a preliminary result for just one p on (m,d,md).]"
2997,1,['probability'], Convergence in Distribution on Metric Spaces o,seg_209,"theorem 1.4 (skorokhod construction) suppose (m,d,md) is a complete and separable metric space and the measures {pn : n ≥ 0} satisfy pn →d p0. then there exist random elements {xn : n ≥ 0} taking values in the space m (thus xn(ω) ∈ m for all ω ∈ ω) and all defined on the common probability space (ω,a, p ) ≡ ([0, 1],b([0, 1]) lebesgue), with"
2998,0,[], Convergence in Distribution on Metric Spaces o,seg_209,∼ xn = pn and satisfying
2999,1,"['distribution', 'probability measure', 'probability', 'random']", Convergence in Distribution on Metric Spaces o,seg_209,"proposition 1.2 suppose p is a probability measure on (m,d,md). then there is a random element x defined on (ω,a, p ) = ([0, 1],b[0, 1], lebesgue) and taking values in m that has distribution p ."
3000,1,"['disjoint', 'interval', 'sets', 'disjoint sets']", Convergence in Distribution on Metric Spaces o,seg_209,"proof. for each k, decompose m into a countable number of disjoint sets ak1, ak2, . . . whose diameter is less than 1/k. then arrange it so that ak+1 refines ak ≡ {ak1, ak2, . . .}. make a corresponding decomposition of the unit interval as ik ≡ {ik1, ik2, . . .} where the subintervals ikj satisfy p (akj) = λ(ikj) and where the decomposition ik+1 refines ik."
3001,0,[], Convergence in Distribution on Metric Spaces o,seg_209,"let xkj be a point in akj , and define"
3002,1,['cauchy'], Convergence in Distribution on Metric Spaces o,seg_209,"since {xk(ω),xk+1(ω), . . .} ⊂ (some one akj), its diameter is bounded by 1/k. thus, {xk(ω)} is cauchy for each ω, limk xk(ω) ≡ x(ω) exists, and"
3003,1,"['sets', 'set']", Convergence in Distribution on Metric Spaces o,seg_209,"for a given set b, write ∑∗ ≡ ∑{j:ajk∩b=∅}, and similarly for unions of sets. then"
3004,1,['set'], Convergence in Distribution on Metric Spaces o,seg_209,"where bδ ≡ {x : d(x,b) ≡ infy∈b d(x, y) < δ}. for a closed set b we have ⋂k"
3005,1,['sets'], Convergence in Distribution on Metric Spaces o,seg_209,"(c) limk p (xk ∈ b) ≤ limk p (xk ∈ b1/k) ≤ p (b) for closed sets b,"
3006,1,['distribution'], Convergence in Distribution on Metric Spaces o,seg_209,and hence the distribution of xk converges to p by (5) of the portmanteau theorem. it follows
3007,1,"['sets', 'set']", Convergence in Distribution on Metric Spaces o,seg_209,"proof. consider skorokhod’s theorem. first construct the decompositions ak of the proof of the previous proposition, but now do it in a way that makes each akj a p -continuity set. because ∂{y : d(x, y) < δ} ⊂ {y : d(y, x) = δ}, the spheres about x are p -continuity sets for all but countably many radii; hence m can be covered by countably many p -continuity sets all with diameter at most 1/k. the usual disjointification procedure preserves p -continuity because ∂(b ∩ c) ⊂ (∂b) ∪ (∂c)."
3008,1,[], Convergence in Distribution on Metric Spaces o,seg_209,"consider the decompositions ik as before, and, for each n, construct successively finer"
3009,0,[], Convergence in Distribution on Metric Spaces o,seg_209,)) = pn(akj). inductively arrange the indexing
3010,1,['intervals'], Convergence in Distribution on Metric Spaces o,seg_209,") if and only if iki < ikj ; here i < j for intervals i and j means that the right endpoint of i does not exceed the left endpoint of j . in other words, we ensure that for"
3011,0,[], Convergence in Distribution on Metric Spaces o,seg_209,"each k the families ik, ik"
3012,0,[], Convergence in Distribution on Metric Spaces o,seg_209,"define xk as before, where xkj ∈ akj , and define"
3013,1,['distribution'], Convergence in Distribution on Metric Spaces o,seg_209,and again x has distribution p and x(n) has distribution pn.
3014,1,"['function', 'sets']", Convergence in Distribution on Metric Spaces o,seg_209,"where the next to the last sum extends over those j for which the summand is positive. each summand goes to 0 as n → ∞ because the akj are p -continuity sets, and it follows by the dct (with dominating function identically equal to the constant function with value 1) that"
3015,0,[], Convergence in Distribution on Metric Spaces o,seg_209,"fix k and j0, let α and αn be the left endpoints of ikj0 and ik"
3016,1,"['set', 'summation']", Convergence in Distribution on Metric Spaces o,seg_209,"), respectively, and let ∑′ indicate summation over the set for which ikj < ikj0 , which is the same as the set for which"
3017,1,['interval'], Convergence in Distribution on Metric Spaces o,seg_209,"similarly, the right endpoint of the interval ik"
3018,1,['interval'], Convergence in Distribution on Metric Spaces o,seg_209,) converges as n → ∞ to the right endpoint of the interval ikj .
3019,0,[], Convergence in Distribution on Metric Spaces o,seg_209,"hence, if ω is interior to ikj (which now fixes k and j), then ω lies in ik"
3020,0,['n'], Convergence in Distribution on Metric Spaces o,seg_209,) for all n large
3021,1,"['distribution', 'convergence', 'set']", Convergence in Distribution on Metric Spaces o,seg_209,"thus, if ω is not an endpoint of any ikj , then for each k we have that (g) holds for all sufficiently large n. in other words, limn x(n)(ω) = x(ω) if ω is not in the set of endpoints of the ikj . this last set, being countable, has lebesgue measure 0; thus if x(n)(ω) is redefined as x(ω) on this set, x(n) still has distribution pn and there is now convergence for all ω. this completes the proof, with x(n) denoting xn and with x denoting x0."
3022,1,"['functions', 'process']", Convergence in Distribution on Metric Spaces o,seg_209,"exercise 1.3 recall the partial sum process sn = {sn(t) : t ≥ 0} defined earlier. now, sn →d s by donsker’s theorem (theorem 12.8.3), where s is a brownian motion process in m = c[0, 1]. consider the following four functions:"
3023,1,"['continuous', 'sets', 'set']", Convergence in Distribution on Metric Spaces o,seg_209,"for each of these real-valued functionals g of x ∈ c[0, 1], find the discontinuity set dg of g. [if we can show that the p measure of these discontinuity sets is zero, where p denotes the measure of s on c[0, 1], then it follows immediately from the continuous mapping theorem that g(sn) →d g(s).]"
3024,1,['continuous'], Convergence in Distribution on Metric Spaces o,seg_209,n =1 |sk| as a functional g(sn). is the resulting g continuous?
3025,1,"['distribution', 'convergence']", Convergence in Distribution on Metric Spaces o,seg_209,"tightness and relative compactness the notion of tightness comes into play in a crucial way in the general theory of convergence in distribution on a metric space (m,d), since there now are more ways to “leave” the space than simply for mass to drift off to infinity."
3026,1,"['probability measures', 'set', 'probability']", Convergence in Distribution on Metric Spaces o,seg_209,"definition 1.2 (tightness) let p0 denote a collection of probability measures on some (m,d, md). then p0 is tight (or uniformly tight) if and only if for every > 0 there is a compact set k ⊂ m with"
3027,1,"['probability', 'probability measures']", Convergence in Distribution on Metric Spaces o,seg_209,"definition 1.3 (sequential compactness, or relative compactness) let p0 be a family of probability measures on (m,d,md). we call p0 relatively compact (or sequentially compact) if every sequence {pn} ⊂ p0 contains a weakly convergent subsequence. that is, every sequence {pn} ⊂ p0 contains a subsequence {pn′} with pn′ →d (some probability q) (not necessarily in p0)."
3028,1,[], Convergence in Distribution on Metric Spaces o,seg_209,"proposition 1.3 let (m,d,md) be a separable metric space."
3029,1,"['set', 'limit']", Convergence in Distribution on Metric Spaces o,seg_209,"(a) if {pn}∞n=1 is relatively compact with limit set {p}, then pn →d p ."
3030,1,"['distribution', 'convergence']", Convergence in Distribution on Metric Spaces o,seg_209,(c) we have thus related convergence in distribution to relative compactness.
3031,0,[], Convergence in Distribution on Metric Spaces o,seg_209,"proof. see proposition 1.1 for both (a) and (b). (that is, we have merely rephrased things we already know.)"
3032,1,"['probability', 'probability measures']", Convergence in Distribution on Metric Spaces o,seg_209,"theorem 1.5 (prohorov) let p0 denote a collection of probability measures on the metric space (m,d,md). (a) if p0 is tight, then it is relatively compact. (b) suppose that (m,d,md) is separable and complete. if the collection p0 is relatively compact, then it is tight. (c) we have thus related relative compactness to tightness, at least on complete and separable metric spaces."
3033,1,['case'], Convergence in Distribution on Metric Spaces o,seg_209,"proof. a full proof progresses from m = rk to r∞, to sigma compact m, and finally to general m , at each step using the next simpler case. we present only the proof of (a) for the case m = rk."
3034,0,[], Convergence in Distribution on Metric Spaces o,seg_209,"if {pn} is any sequence in p0, then helly’s selection theorem implies that the corresponding sequence of dfs {fn} defined by fn(×) ≡ pn(−∞,×] contains a subsequence {fn′} satisfying"
3035,1,['continuous'], Convergence in Distribution on Metric Spaces o,seg_209,"where the sub df f is continuous from above. now, there is a measure p on rk such that p (a, b] equals f differenced around the vertices of the k-dimensional rectangle (a, b]. now pn"
3036,1,[], Convergence in Distribution on Metric Spaces o,seg_209,"′ →d p will follow if we can show that p (rk) = 1. given > 0, choose k ⊂ rk compact with pn′(k) > 1 − for all n′; this is possible by tightness of p0. now choose a, b in rk such that k ⊂ (a, b] and all 2k vertices of (a, b] are continuity points of f (we can do this because at most a countable number of parallel (k − 1)-dimensional hyperplanes can possibly have positive p -measure. since pn′(a, b] equals fn"
3037,1,"['probability measure', 'probability']", Convergence in Distribution on Metric Spaces o,seg_209,"′ differenced around (a, b], (a) yields pn ′(a, b] ≥ pn ′(k) ≥ 1 − , so p (a, b] ≥ 1 − . since was arbitrary, p (rk) = 1. hence p is a probability measure, pn′ →d p , and p0 is relatively compact."
3038,1,['distribution'], Convergence in Distribution on Metric Spaces o,seg_209,"convergence in distribution on (d,d)"
3039,1,['results'], Convergence in Distribution on Metric Spaces o,seg_209,"we phrase results carefully in this subsection, so as to mention primarily the familiar metric ‖ · ‖ on d (while limiting mention of the contrived metric d of exercise 12.1.4 for which (d, d) is complete and separable with dd = d). recall, d denotes the finite-dimensional σ-field."
3040,1,['process'], Convergence in Distribution on Metric Spaces o,seg_209,"theorem 1.6 (criterion for →d on (d,d); chentsov) let each xn denote a process on (d,d). suppose that for some a > 2"
3041,1,['processes'], Convergence in Distribution on Metric Spaces o,seg_209,1 and b > 0 the increments of the xn processes satisfy
3042,1,"['continuous', 'condition']", Convergence in Distribution on Metric Spaces o,seg_209,"for some finite measure μn on the borel subsets of [0, 1]. (often, a = 1 and b = 2, and note the relaxation of this condition in the remark below.) suppose that μ is a continuous measure"
3043,0,[], Convergence in Distribution on Metric Spaces o,seg_209,"on the borel subsets of [0, 1], and that either"
3044,1,[], Convergence in Distribution on Metric Spaces o,seg_209,(a) then for the metric d of exercise 12.1.4 we have that
3045,1,"['condition', 'intervals', 'process', 'processes']", Convergence in Distribution on Metric Spaces o,seg_209,"remark 1.1 for processes like partial sum process sn condition (11) is troublesome; but since sn is constant on the intervals [(i − 1)/n, i/n), it should be enough to verify (11) for r, s, t restricted to be of the form i/n. we now make this rough idea precise."
3046,1,['function'], Convergence in Distribution on Metric Spaces o,seg_209,"for m ≥ 1 we let tm ≡ {0 ≡ tm0 < tm1 < · · · < tmkm ≡ 1}, and measure the coarseness of this partition by defining mesh(tm) ≡ max{tmi − tm,i−1 : 1 ≤ i ≤ km}. let x ∈ d and let am(x) denote the function in d that equals x(tmi) at tmi for each 0 ≤ i ≤ km and that is constant in between these points. we agree to call am(x) the tm-approximation of x. suppose now that one of the two conditions (12) or (13) holds, that xn = an(xn) with mesh(tn) → 0 as n → ∞, and that (11) holds for all r, s, t in tn. then both (14) and (15) hold. [that xn = an(xn) means that xn is equal to its own tn-approximation; and this clearly holds for sn when all tni = i/n.]"
3047,1,"['functions', 'set', 'inequality']", Convergence in Distribution on Metric Spaces o,seg_209,"“proof.” by analogy with helly’s selection theorem, it should suffice to show the tightness. thus, for each > 0 we must exhibit a compact set k of functions on [0, 1] with p (xn ∈ k ) > 1 − for all n. according to arzelà’s theorem (see exercise b.2.11), a compact set of functions consists of a uniformly bounded set of functions that have a uniform bound on their “wiggliness.” a complex and delicate argument (slightly in the spirit of kolmogorov’s inequality) based on (6) can be given to bound this “wiggliness.” since the details are long and hard and are only used for the present theorem, they will be skipped. see billingsley (1968)."
3048,1,"['condition', 'function', 'process', 'processes']", Convergence in Distribution on Metric Spaces o,seg_209,"exercise 1.5 (prohorov) for any real-valued function on [0, 1] we now define the modulus of continuity ωδ(x) of x by ωδ(x) ≡ max{|xt − xs| : |t − s| ≤ δ} for each δ > 0. let x,x1,x2, . . . denote processes on (c, c). then xn →d x on (c, ‖ · ‖) if and only if both xn →f.d. x and limδ→0 lim supn→∞ p (ωδ(xn) > ) = 0 for all > 0. (the modulus of continuity also measures the “wiggliness” of the process, and prohorov’s condition implies that the processes are “not too wiggly.”)"
3049,1,[], Convergence in Distribution on Metric Spaces o,seg_209,exercise 1.6 (doob) use theorem 1.6 to establish that g(un) →d g(u) for all d-measur- able and a.s. ‖ · ‖-continuous functionals g on d. see (12.10.4) for un.
3050,1,[], Convergence in Distribution on Metric Spaces o,seg_209,exercise 1.7 (donsker) use theorem 1.6 to establish that g(sn) →d g(s) for all d-measur- able and a.s. ‖ · ‖-continuous functionals g on d.
3051,1,"['distribution', 'process', 'linearized']", Convergence in Distribution on Metric Spaces o,seg_209,"exercise 1.8 (prohorov) linearize sn between the i/n-points so as to make it a process on (c, ‖ · ‖), and then use exercise 1.3 to show that this linearized process converges in distribution to brownian motion."
3052,1,['set'], Metrics for Convergence in Distribution o,seg_211,"definition 2.1 (prohorov metric) [the lévy metric of exercise 9.1.5 extends in a nice way to give a metric for →d more generally.] for any borel set b ∈ md and > 0, define"
3053,1,"['probability', 'set', 'probability measures']", Metrics for Convergence in Distribution o,seg_211,"let p,q be two probability measures on (m,d,md). if we set"
3054,1,['symmetric'], Metrics for Convergence in Distribution o,seg_211,then ρ is the prohorov metric (see exercise 2.1). (we note that this definition is not formed in a symmetric fashion.)
3055,1,"['functions', 'set']", Metrics for Convergence in Distribution o,seg_211,"definition 2.2 (dudley metric) (i) label as bl(m,d) the set of all real-valued functions g on the metric space (m,d) that are bounded and lipschitz (in the sense that both of the quantities"
3056,1,['functions'], Metrics for Convergence in Distribution o,seg_211,"are finite). for functions g in bl(m,d) we define"
3057,1,"['probability', 'set', 'probability measures']", Metrics for Convergence in Distribution o,seg_211,"and so bl(m,d) = {g : ‖g‖bl < ∞}. (ii) now let p,q be two probability measures on (m,md), and set"
3058,1,['distributions'], Metrics for Convergence in Distribution o,seg_211,then β is called the dual bounded lipschitz distance (or dudley distance) between the probability distributions p and q.
3059,1,"['probability distributions', 'probability', 'distributions']", Metrics for Convergence in Distribution o,seg_211,"proposition 2.1 let p ≡ {all probability distributions p on (m,m) }. then both ρ and β are metrics on p. (equation (12) below will show that mρ = mβ , which we abbreviate here as m.)"
3060,0,[], Metrics for Convergence in Distribution o,seg_211,exercise 2.1 prove the previous proposition.
3061,1,"['distribution', 'convergence']", Metrics for Convergence in Distribution o,seg_211,"the following theorem says that both ρ and β metrize →d on (m,d,md) just as the lévy distance l metrized the convergence in distribution →d of dfs on r1."
3062,1,[], Metrics for Convergence in Distribution o,seg_211,"theorem 2.1 (metrizing →d; dudley) for any separable metric space (m,d) and probability measures {pn : n ≥ 1} and p on the borel σ-field md, the following are equivalent conditions:"
3063,1,[], Metrics for Convergence in Distribution o,seg_211,"proof. let > 0. by the separability of m , for each m ≥ 1 there is a sequence am1, am2, . . . of open 1/m spheres covering m . choose im such that p (⋃i≤im ami) > 1 − /2m."
3064,1,['set'], Metrics for Convergence in Distribution o,seg_211,"now, the set b ≡ ⋂m"
3065,1,['set'], Metrics for Convergence in Distribution o,seg_211,"∞ =1 ⋃i≤im ami is totally bounded in m , meaning that for each > 0 it has a finite -net (that is, a set of points xk with d(x, xk) < for some xk, for each x ∈ b). [nice trick!] by completeness of m,k ≡ b̄ is complete and is also compact; see exercises b.2.3(c) and b.2.4(e). since"
3066,0,[], Metrics for Convergence in Distribution o,seg_211,the conclusion follows.
3067,1,"['functions', 'set']", Metrics for Convergence in Distribution o,seg_211,"proof. we now prove theorem 2.1, since ulam’s theorem is in hand, but only under the additional assumption that m is complete. clearly, (5) implies (6). we will now show that (6) implies (7). by ulam’s theorem, for any > 0 we can choose k compact so that p (k) > 1 − . let f |k denote f restricted to k. now, the set of functions f ≡ {f |k : ‖f‖bl ≤ 1} forms a ‖ · ‖-totally bounded subset of the functions cb(k) (by the arzelà theorem of exercise b.2.11(a)). thus, for every > 0 there is some finite k ≡ k and functions f1, . . . , fk ∈ f such that for any f ∈ f there is an fj with"
3068,0,[], Metrics for Convergence in Distribution o,seg_211,since f and fj are in f (note the second half of (2)). let
3069,0,['n'], Metrics for Convergence in Distribution o,seg_211,for n chosen large enough. hence (7) holds.
3070,1,['set'], Metrics for Convergence in Distribution o,seg_211,"we next show that (7) implies (8). suppose a borel set b and an > 0 are given. let −1 f (x) ≡ max{0, (1 − d(x, b)/ )}. then f ∈ bl(m,d), ‖f ‖bl ≤ 1 + , and 1b ≤ f ≤ 1b . therefore, for any p and q on m we have from (4) that"
3071,0,[], Metrics for Convergence in Distribution o,seg_211,and it follows that
3072,1,['inequality'], Metrics for Convergence in Distribution o,seg_211,"hence, if β(p,q) < 2, then ρ(p,q) < + 2 < 2 . hence, for all p,q we have ρ(p,q) ≤ 2√β(p,q). thus, (7) implies (8). [it is also possible to establish the inequality β(p,q)/2 ≤ ρ(p,q). this would give"
3073,0,[], Metrics for Convergence in Distribution o,seg_211,showing that ρ and β are equivalent metrics (see (12) and exercise 2.3 below).]
3074,1,['set'], Metrics for Convergence in Distribution o,seg_211,"finally, we will show that (8) implies (5). suppose (8) holds. let b denote a p -continuity set, and let > 0. then for 0 < δ < with δ small enough, we have p (bδ \b) < and p ((bc)δ \bc) < . then for n large enough we have"
3075,0,[], Metrics for Convergence in Distribution o,seg_211,strassen’s coupling theorem
3076,1,"['rate', 'random']", Metrics for Convergence in Distribution o,seg_211,"suppose pn →d p0 on a separable metric space (m,d). then theorem 2.1 gives ρ(pn, p0) → 0, for prohorov’s metric ρ, while skorokhod’s theorem gives existence of random elements xn on a common (ω,a, p ) satisfying d(xn,x0) →a.s. 0. claiming less than is true, d(xn,x0) →p 0, or p (d(xn,x0) ≥ ) → 0 as n → ∞. we are then naturally led to ask how rapidly this convergence occurs. it turns out that this is essentially determined by ρ(pn, p ). alternatively, the following theorem can be used to bound ρ(pn, p ), provided that a rate is available regarding skorokhod."
3077,1,['sets'], Metrics for Convergence in Distribution o,seg_211,"theorem 2.3 (strassen) suppose that p and q are measures on the borel sets of a separable metric space (m,d). then ρ(p,q) < if and only if there exist x and y defined"
3078,1,['probability'], Metrics for Convergence in Distribution o,seg_211,"∼ ∼ on a common probability space with x = p and y = q and coupled closely enough that p (d(x,y ) ≥ ) ≤ ."
3079,0,[], Metrics for Convergence in Distribution o,seg_211,"proof. see dudley (1976, section 18)."
3080,1,['convergence'], Metrics for Convergence in Distribution o,seg_211,"as shown in theorem 2.1, both the prohorov metric ρ and also the dual-bounded lipschitz metric β metrize weak convergence (→d). other stronger metrics are also available and are often useful."
3081,1,"['probability', 'probability measures', 'variation']", Metrics for Convergence in Distribution o,seg_211,"definition 2.3 (total variation metric) for probability measures p and q on the measurable space (m,md), let"
3082,1,['variation'], Metrics for Convergence in Distribution o,seg_211,dtv is called the total variation metric.
3083,1,['variation'], Metrics for Convergence in Distribution o,seg_211,proposition 2.2 the total variation metric dtv is equal to
3084,1,['probabilities'], Metrics for Convergence in Distribution o,seg_211,"definition 2.4 (hellinger metric) for probabilities p and q on (m,md), let"
3085,1,[], Metrics for Convergence in Distribution o,seg_211,"where f = dp/dμ, g = dq/dμ, and μ is any measure dominating both p and q (for example, p + q); then dh is called the hellinger metric."
3086,0,[], Metrics for Convergence in Distribution o,seg_211,exercise 2.2 dh does not depend on the choice of μ.
3087,1,"['functions', 'continuous', 'sets', 'variation']", Metrics for Convergence in Distribution o,seg_211,"here is a theorem relating these metrics and the prohorov and bounded lipschitz metrics. the inequalities in (12) show that ρ and β induce identical topologies. the inequalities (13) show that the total variation metric dtv and the hellinger metric dh induce identical topologies. moreover, (14) shows that the identical β and ρ topologies are finer than the dtv topology (with more open sets and admitting more continuous functions). [note exercise b.2.1 dealing with equivalent metrics.]"
3088,1,"['probability', 'probability measures']", Metrics for Convergence in Distribution o,seg_211,"theorem 2.4 (inequalities among the metrics) (a) for p and q probability measures on (m,md), the following inequalities necessarily hold:"
3089,1,['inequality'], Metrics for Convergence in Distribution o,seg_211,exercise 2.3 prove the first inequality in (12).
3090,1,['inequality'], Metrics for Convergence in Distribution o,seg_211,"exercise 2.4 prove (13). [hint. to prove the left inequality, establish the inequality ∫ √fg dμ ≥ ∫ f ∧ gdμ and use the second equality in (11). to show the right inequality, write |f − g| = |√f − √g||√f + √g|."
3091,1,"['probabilities', 'errors', 'test', 'statistical', 'error']", Metrics for Convergence in Distribution o,seg_211,"exercise 2.6 (statistical interpretation of the dtv metric) consider testing p versus q. find the test that minimizes the sum of the error probabilities, and show that the minimum sum of errors is ‖p ∧ q‖ ≡ ∫ f ∧ gdμ. note that p and q are orthogonal if and only if ‖p − q‖ ≡ dtv (p,q) = 2 if and only if ‖p ∧ q‖ = 0 if and only if ∫ √fg dμ ≡"
3092,1,"['observations', 'random', 'number of observations', 'process', 'linear', 'results', 'samples', 'permutation', 'mean', 'trimmed mean', 'population', 'tests', 'distributions', 'distribution', 'permutation tests', 'convergence', 'sampling', 'normality', 'normal', 'tail', 'normal distribution']", Introduction,seg_215,"in section 15.1 we rederive the usual clt for iid samples from any distribution in the domain of attraction of the normal distribution, but now using empirical process methods. we then obtain a corresponding result for the trimmed mean that is valid for samples from any df in any domain of attraction, provided only that the number of observations trimmed from each tail grows to infinity so slowly that the fraction of observations trimmed from each tail goes to zero. when the qfs of all distributions in a class are bounded by an appropriate envelope qf, then all of these convergence results are uniform across the class of qfs. this uniformity allows random trimming. in section 15.2 similar results are derived for linear rank tests and permutation tests, and a uniform studentized clt is given for sampling from a finite population. also, two very interesting ways of creating normality are discussed in this section. in section 15.3 results are presented for l-statistics."
3093,1,"['quantile', 'rate', 'method', 'results', 'statistic', 'normal', 'process']", Introduction,seg_215,"all of the results are based on the empirical process construction of section 12.10 combined with the quantile method inequalities from sections c.5–c.6. we will frequently obtain conclusions of the form tn →p z for a special construction version of an interesting statistic tn and its limiting normal rv z. or, we may even obtain supk |tn − z(k)| →p 0 for a family of normal rvs z(k) indexed by the qf k in a class k. these strong →p conclusions are only true for the special construction versions of the statistic tn. however, tn →p z for the special construction tn implies that tn →d z for any version of tn. in like fashion, supk |tn −z(k)| →p 0 for a special construction version of tn implies that the rate at which tn →d z is uniform across the entire class of qfs in k for any version of the statistic tn."
3094,1,"['order statistics', 'statistics']", Trimmed and Winsorized Means ,seg_217,"their order statistics, with empirical qf kn. define μ ≡ μk ≡ ∫0"
3095,1,"['sample', 'mean', 'sample mean']", Trimmed and Winsorized Means ,seg_217,"2 ≡ var[k(ξ)], when these exist. we also let x̄n = ∑n 1 xni/n denote the sample mean and sn"
3096,1,[], Trimmed and Winsorized Means ,seg_217,2 = ∑n 1 (xni − x̄n)2/n denote the “sample variance” for trimming numbers
3097,1,"['deviation', 'sample', 'standard', 'mean', 'population', 'trimmed mean', 'standard deviation']", Trimmed and Winsorized Means ,seg_217,"here s̃n ≡ σ̃kn(an, a′n) denotes the sample (an, a′n)-winsorized standard deviation, and the rv x̌n = μ̌kn(an, a′n) is being called the sample (an, a′n)-truncated mean, while x̆n ≡ x̌n/(1 − an − a′n) denotes the sample (an, a′n)-trimmed mean. also, μ̆n ≡ μ̌n/(1 − an − a′n) is the population trimmed mean, and σ̆n ≡ σ̃n/(1 − an − a′n). now, x̌n is our vehicle of convenience for studying the trimmed mean x̆n, since"
3098,1,['statistical'], Trimmed and Winsorized Means ,seg_217,"summary: x̆n has statistical meaning, while x̌n does not; but x̌n is much more convenient to work with notationally and probabilistically."
3099,1,"['function', 'continuous']", Trimmed and Winsorized Means ,seg_217,"so long as kn ∧ kn ′ ≥ 1, we always have (integrating brownian bridge u, which for each fixed ω is just a continuous function)"
3100,1,[], Trimmed and Winsorized Means ,seg_217,"convention we now specify that throughout this chapter xni ≡ k(ξni) = f−1(ξni), for 1 ≤ i ≤ n, for the ξni described in notation 1.3 below. (recall the sentence above (6.4.3) noting that this representation of rvs allows alternative methods of proof.) note the conclusions of being “=a” in (4) and (6) below are true only for these particular rvs f−1(ξni) that are iid f (·) but the implied →d conclusion is true for any iid rvs xn1, . . . , xnn having df f (·)."
3101,1,[], Trimmed and Winsorized Means ,seg_217,"theorem 1.1 (the clt for x̄n) let k ∈ d(normal), which is equivalent to"
3102,1,['mean'], Trimmed and Winsorized Means ,seg_217,"by (10.6.31). define μ̌n ≡ μk(1/n, 1/n) and σ̃n ≡ σ̃k(1/n, 1/n), and let k̃n(·) denote k(·) winsorized outside (1/n, 1 − 1/n). then the mean x̄n of the iid rvs xn1, . . . , xnn above satisfies (as is also shown in theorem 10.6.1)"
3103,0,[], Trimmed and Winsorized Means ,seg_217,"notation 1.2 for the following theorem, suppose the integers kn and kn"
3104,1,"['trimmed means', 'variance', 'varying']", Trimmed and Winsorized Means ,seg_217,"theorem 1.2 (the clt for trimmed means) suppose the qf k(·) is such that the partial variance σ̃2(t) ≡ var[k̃t,t(ξ)] is in one of the regularly varying classes r−β , for some"
3105,1,"['union', 'inequality']", Trimmed and Winsorized Means ,seg_217,"β ≥ 0. [this union of all the r−β classes was labeled as d̃ in definition (c.5.33), and r0 ≡ d(normal). note inequality c.5.4.] if (5) holds, then"
3106,1,['continuous'], Trimmed and Winsorized Means ,seg_217,corollary 1 (trimming fixed fractions) suppose an → a and a′n → a′ for 0 < a < 1 − a′ < 1. then (4) holds for any qf k(·) continuous at both a and 1 − a′.
3107,1,"['statistics', 'order statistics', 'probability', 'process']", Trimmed and Winsorized Means ,seg_217,"notation 1.3 be clear that we are working on the specific probability space (ω,a, p ) of theorem 12.10.3 on which are defined a fixed brownian bridge u and a triangular array of rvs whose nth row members ξn1, . . . , ξnn are iid uniform(0, 1) with order statistics 0 ≤ ξn:1 ≤ · · · ≤ ξn:n ≤ 1, empirical df gn, and empirical process un = √n[gn −i] that not only satisfies ‖un = u‖ →p 0, but in fact, for each fixed 0 ≤ ν < 1"
3108,1,['realization'], Trimmed and Winsorized Means ,seg_217,"for any ξn1, . . . , ξnn (and hence for this realization also), for each fixed 0 < ν < 1,"
3109,1,"['range', 'random']", Trimmed and Winsorized Means ,seg_217,"remark 1.1 we prove these theorems in such a way that the uniformity available is apparent. to see the full range of uniformity possible, consult the 1st edition of this text, where this topic is pursued much more completely. random kn and kn"
3110,1,['statisticians'], Trimmed and Winsorized Means ,seg_217,"′ , (useful to applied statisticians)"
3111,0,[], Trimmed and Winsorized Means ,seg_217,are also considered therein.
3112,1,['case'], Trimmed and Winsorized Means ,seg_217,"′n only for the theorem 1.1 identity, because in this case only we have used the “non-natural for the identity” value μ̌(1/n, 1/n) in the theorem statement. the “natural value” μ = μ̌(0, 0) would have caused trouble in the proof. note the definition of kn = kn"
3113,0,[], Trimmed and Winsorized Means ,seg_217,"we begin an examination of the various γ-terms. concerning γ1n, we note that"
3114,1,"['factorization', 'distribution', 'random', 'inequality']", Trimmed and Winsorized Means ,seg_217,"and it is this sort of factorization into a random term times a deterministic term that is key to the proof. the randomness refers only to the uniform(0, 1) distribution, and the only reference to k(·) is in the deterministic term (to which inequality c.6.1 applies). in particular, νn = op(1) by (7). then (c.6.3) gives (the second term appearing in (d) is the γn of (c.6.3), and the γn"
3115,1,"['symmetric', 'tail']", Trimmed and Winsorized Means ,seg_217,′ term in (d) is the symmetric term from the right tail)
3116,1,[], Trimmed and Winsorized Means ,seg_217,note with regard to γ2n that monotonicity of gn(·) implies that
3117,1,"['indicator function', 'probability', 'event', 'function', 'indicator']", Trimmed and Winsorized Means ,seg_217,"where it is trivially true that en ≡ |un(an)/√an| = op(1). we also let 1n denote the indicator function of an event, having probability exceeding 1 − , on which (with λ ≡ λ small enough) we can specify our choice of one of the following:"
3118,1,"['linear', 'moment', 'distribution', 'moments', 'probability', 'second moment', 'inequality']", Trimmed and Winsorized Means ,seg_217,"[the bounds in (f) are derived as follows: for (ii), the chebyshev (second moment) inequality with beta rv moments of ξn:kn+1. or for (i), the markov (first moment) inequality, or from the in probability linear bounds of inequality 12.11.2. or for choice (iii), the exact distribution of ξn:kn+1.]"
3119,1,[], Trimmed and Winsorized Means ,seg_217,"consider theorem 1.1, when an = an′ = 1/n. now, γ1n is controlled via (d) (at the left end) solely by |k+(an)|/√nσ̃n (just choose r first to be very large in the first term in the bound of (d)). to summarize, γ1n →p 0 whenever"
3120,1,"['symmetric', 'tail', 'condition']", Trimmed and Winsorized Means ,seg_217,"thus the clt claim of (4) holds, provided only (adding the symmetric condition in the right tail)"
3121,1,"['normality', 'condition']", Trimmed and Winsorized Means ,seg_217,"for each 0 < λ ≤ 1, and it holds uniformly in any class of qfs in which (12) holds uniformly. but (12) follows from (3) (or any of the equivalent conditions like (10.6.32), or (10.6.35), or (10.6.13), or (10.6.12), or (c.2.6), or (c.2.13), or (c.2.15)–for example.) thus the normality in theorem 1.1 is established again—using a fundamentally different proof from that in section 10.6. [note that (the crude upper bound) condition (12) implies (11), (g), and (h).]"
3122,0,[], Trimmed and Winsorized Means ,seg_217,"consider theorem 1.2. the first term in (d) converges to 0, and the other two terms that appear in (d) equal zero. thus, again γ1n →p 0 whenever kn ∧kn"
3123,0,[], Trimmed and Winsorized Means ,seg_217,"uniformly in all qfs). in the present context the γ3n term always equals 0. thus, only γ2n must be shown to be negligible; but we must now be much more careful than the crude upper bound (12). now, using (f)(ii) in the definition of 1n in (f), we obtain"
3124,1,"['symmetric', 'tail']", Trimmed and Winsorized Means ,seg_217,"where iλn ≡ (an(1 − λ/√kn), an(1 + λ/√kn)). thus the clt claim of (6) holds, provided that (with symmetric requirements in the right tail)"
3125,1,"['normality', 'tail']", Trimmed and Winsorized Means ,seg_217,"for each λ > 0. thus, normality holds uniformly in any class ku in which both (13) and its right tail analogue hold uniformly; call such a class ku a uniformity class."
3126,0,[], Trimmed and Winsorized Means ,seg_217,summary (so far) whenever kn ∧ kn
3127,1,"['estimation', 'tail', 'variance']", Trimmed and Winsorized Means ,seg_217,"for any class ku in which both (13) and its right tail analogue hold uniformly. (also, we may replace σ̃n by s̃n in (14) under this same requirement (as was shown in the variance estimation proof given in the 1st edition))."
3128,1,"['asymptotic', 'estimation', 'normality', 'variance']", Trimmed and Winsorized Means ,seg_217,"now, (13) does hold for a fixed k whenever both k is in any r−β and the trimming numbers of (5) are employed (appeal to theorem c.5.1), and this gives theorem 1.2. (two uniformity classes ku are exhibited in theorem c.5.2.) aside from variance estimation, the proofs of theorems 1.1 and 1.2 are complete. the 1st edition carefully considers both variance estimation and the uniformity of the asymptotic normality, via making (12) hold uniformly."
3129,1,['tails'], Trimmed and Winsorized Means ,seg_217,so that both the contributions from the two extreme tails {(from 1 to kn) and (from n−kn
3130,0,['n'], Trimmed and Winsorized Means ,seg_217,to n)} can be balanced. (recall the de haan result in exercise c.4.2.) but we do not need to balance them; we threw them away.
3131,1,"['independent', 'statistics', 'order statistics']", Trimmed and Winsorized Means ,seg_217,"exercise 1.2* (rossberg) suppose 0 ≤ ξn:1 ≤ · · · ≤ ξn:n are the order statistics of the first n of the infinite sequence of independent uniform(0, 1) rvs ξ1, ξ2, . . .. let αn ≡ {j : 1 ≤ j ≤ kn}, δn ≡ {j : kn + 1 ≤ j ≤ ln}, βn ≡ {j : ln + 1 ≤ j ≤ n − ln"
3132,1,['independent'], Trimmed and Winsorized Means ,seg_217,′ } are asymptotically independent when
3133,1,['tail'], Trimmed and Winsorized Means ,seg_217,1+2ι/ for 0 < ν < 1 (with analogous conditions in 4 the right tail).
3134,1,"['sample', 'sample mean', 'asymptotic', 'normality', 'mean']", Trimmed and Winsorized Means ,seg_217,"remark 1.3 consider asymptotic normality of the sample mean. let un, vn,wn, vn"
3135,0,[], Trimmed and Winsorized Means ,seg_217,"the previous exercise shows that wn, un, and un"
3136,1,['independent'], Trimmed and Winsorized Means ,seg_217,′ are asymptotically independent. exercise
3137,1,"['condition', 'asymptotic', 'independence', 'normal', 'tails']", Trimmed and Winsorized Means ,seg_217,"′ are always asymptotically negligible. we saw in appendix c that the condition σ̃2(·) ∈ l is also necessary for (4). since the vast middle is “nearly always” normal, one needs to determine what is happening only in the extreme tails (as the mid-tails were always negligible). this asymptotic independence is at the heart of the very general asymptotics for x̄n found in s. csörgő, haeusler, and mason (1989)."
3138,1,"['sample', 'mean', 'sample mean']", Trimmed and Winsorized Means ,seg_217,"exercise 1.3 (winsorized mean) let z̃n ≡ √n(x̃n−μ̃n)/σ̃n for the sample mean x̃n of the winsorized sample x̃n1, . . . , x̃nn. for the n(0, 1) rv z(k̃n) of (2), the identity (9) becomes"
3139,1,['mean'], Trimmed and Winsorized Means ,seg_217,′ < a′n or 1 − ξn:n−kn ′ ≥ a′n. show that this quantity γ̃2n for the winsorized mean
3140,1,"['graphical', 'trimmed means', 'factor', 'mean', 'trimmed mean']", Trimmed and Winsorized Means ,seg_217,"essentially exceeds the γ2n of the trimmed mean proof by the factor √kn. this is just enough to prevent analogues of the theorems for trimmed means that we proved in this chapter. (a) prove what you can for the winsorized mean. (b) graph a typical k on (0, 1). locate an and ξn:kn+1 near the 0 endpoint (and suppose k(0) < 0). obtain the natural graphical upper bounds on the magnitudes of γ2n and γ̃2n, and note how the second quantity is inherently larger than the first (pictorially, a “trapezoid” versus a “triangle”)."
3141,0,[], Trimmed and Winsorized Means ,seg_217,"exercise 1.4 (uniform wlln) suppose the qf k0(·) is of order one, in that"
3142,0,[], Trimmed and Winsorized Means ,seg_217,then one can claim the uniform wlln
3143,1,"['linear', 'statistics', 'mean', 'process']", Linear Rank Statistics and Finite Sampling,seg_219,"example 2.1 (linear rank statistics) consider the rn process of section 12.10, with the same notation and assumptions as were made there. thus (for cni’s with mean c̄n = 0,"
3144,1,"['deviation', 'moment', 'standardized', 'standard']", Linear Rank Statistics and Finite Sampling,seg_219,"standard deviation σc2n = 1, and standardized fourth central moment c4n/σc4n ≤ m < ∞ for all n) we define (for the antiranks d ≡ (dn1, . . . , dnn ))"
3145,1,"['linear', 'process', 'moment', 'statistics', 'regression', 'mean', 'scores', 'variance']", Linear Rank Statistics and Finite Sampling,seg_219,"and this process satisfies νn = op(1), as in (12.10.35). the known constants cn ≡ (cn1, . . . , cnn )′ are called regression constants. let an ≡ (an1, . . . , ann )′ specify known scores. let an ., σa2n > 0, and μ4(an ) denote their mean, variance, and fourth central moment. the class of simple linear rank statistics is defined by"
3146,1,['scores'], Linear Rank Statistics and Finite Sampling,seg_219,n−1 that the scores are ordered as
3147,1,['probability'], Linear Rank Statistics and Finite Sampling,seg_219,the basic probability space and rvs are as defined in notation 15.1.3 and in (12.10.35). let
3148,1,"['linear', 'linear combination', 'combination', 'normal', 'jointly', 'mean']", Linear Rank Statistics and Finite Sampling,seg_219,"clearly, zn is a normal rv with mean 0, since it is just a linear combination of the jointly normal rvs w(1/n), . . . ,w(1 − 1/n). fubini’s theorem gives"
3149,1,['independent'], Linear Rank Statistics and Finite Sampling,seg_219,we will also consider the sum of independent rvs given by
3150,1,[], Linear Rank Statistics and Finite Sampling,seg_219,"0 =a zn =∼ n(0, 1) under rather mild conditions."
3151,0,[], Linear Rank Statistics and Finite Sampling,seg_219,definition 2.1 (d(an )-negligibility) call such an a negligible array if
3152,1,['random'], Linear Rank Statistics and Finite Sampling,seg_219,"we let a denote any collection of such arrays that uses the same n ’s. when the ani are random, we call them"
3153,1,"['linear', 'statistics', 'regression']", Linear Rank Statistics and Finite Sampling,seg_219,theorem 2.1 (uniform clt for linear rank statistics) suppose that the regression
3154,0,['n'], Linear Rank Statistics and Finite Sampling,seg_219,"constants satisfy cn4 ≤ m < ∞ for all n . let a be a collection of uniformly negligible arrays, in that n ≡ supa d(an ) → 0. then"
3155,0,['n'], Linear Rank Statistics and Finite Sampling,seg_219,"by first choosing r large and then letting n → ∞, we see that"
3156,1,['rate'], Linear Rank Statistics and Finite Sampling,seg_219,"note that we have separated the randomness from properties of the ani’s, so that the convergence is uniform over a. [the rate for tn in (9) depends on the sequence n → 0, and on the cni’s only through the m ’s in the statement p ( νn ≥ m ) ≤ for all n.] since (see (12.10.34))"
3157,1,['case'], Linear Rank Statistics and Finite Sampling,seg_219,comparing (e) with (a) and (10) shows that the proof is the same in this case.
3158,1,"['regression', 'independent', 'scores', 'random']", Linear Rank Statistics and Finite Sampling,seg_219,[one can also allow random regression constants cn and scores an that are independent
3159,1,['normality'], Linear Rank Statistics and Finite Sampling,seg_219,"example 2.2 (creating normality) the following are known, and intuitive."
3160,1,"['normal', 'regression']", Linear Rank Statistics and Finite Sampling,seg_219,"(a) (using normal regression constants) when lim c4n ≤ ∞, present methods give"
3161,1,['hypotheses'], Linear Rank Statistics and Finite Sampling,seg_219,"result 1 thus, with absolutely no hypotheses,"
3162,1,['sample'], Linear Rank Statistics and Finite Sampling,seg_219,"(b) (winsorizing a finite sample) let ãn ., σ̃an , t̃n , and z̃n be defined as before, but now"
3163,1,['population'], Linear Rank Statistics and Finite Sampling,seg_219,′ )-winsorized population an consisting of
3164,0,[], Linear Rank Statistics and Finite Sampling,seg_219,"of course, theorem 2.1 also applies to t̃n . but note that now"
3165,1,"['sample', 'asymptotic', 'normality']", Linear Rank Statistics and Finite Sampling,seg_219,"summary asymptotic normality is guaranteed by winsorizing a number that slowly increases to infinity, provided only that we do not collapse the whole sample."
3166,0,[], Linear Rank Statistics and Finite Sampling,seg_219,exercise 2.2 argue heuristically why (13) should be true. [see shorack(1996).]
3167,1,"['sample', 'sample mean', 'statistics', 'order statistics', 'permutation', 'mean', 'population', 'sample variance', 'variance']", Linear Rank Statistics and Finite Sampling,seg_219,"example 2.3 (permutation statistics) suppose x1, . . . , xn are iid rvs with nondegenerate df f on (ω,a, p ). then let xn ≡ (x1, . . . , xn )′ denote the full population of observed values, having order statistics xn :1 ≤ · · · ≤ xn :n , antiranks (dn1, . . . , dnn ), sample mean x̄n , sample variance sn"
3168,0,['n'], Linear Rank Statistics and Finite Sampling,seg_219,"2 , empirical df fn , and empirical qf kn ≡ f− n"
3169,1,"['parameters', 'mean', 'population', 'variance']", Linear Rank Statistics and Finite Sampling,seg_219,"′ ≤ n , and let x̃n denote the (kn , kn ′ )-winsorized population x̃n :1 ≤ · · · ≤ x̃n :n (as in (14)) whose parameters are the winsorized mean t̃n , the winsorized variance s̃n"
3170,0,[], Linear Rank Statistics and Finite Sampling,seg_219,empirical qf k̃n . we note that
3171,1,['independent'], Linear Rank Statistics and Finite Sampling,seg_219,"xn :1 ≤ · · · ≤ xn :n and (dn1, . . . , dnn ) are independent rvs, (18) if tied xi ’s are randomly assigned their ranks."
3172,1,['condition'], Linear Rank Statistics and Finite Sampling,seg_219,′ that are either fixed integer sequences or integer-valued rvs that are independent of the antiranks dn . condition (22) necessarily holds if
3173,1,"['independent', 'statistics', 'order statistics', 'realization']", Linear Rank Statistics and Finite Sampling,seg_219,"[we shall maintain the order statistics (which are on (ω,a, p )), but we can replace the independent antiranks by a realization (on some (ω∗,a∗, p ∗) independent of (ω,a, p )) for which ∗νn = op(1) on (ω∗, a∗, p ∗) (from (12.10.25)) for some brownian bridge w. this"
3174,1,"['permutation', 'statistic', 'mean']", Linear Rank Statistics and Finite Sampling,seg_219,by a permutation statistic we mean a rv of the form
3175,1,"['standardized', 'distribution', 'population', 'realization']", Linear Rank Statistics and Finite Sampling,seg_219,"2 = σx 2 n = ∑n 1 (xk − x̄n )2/n and with a cn ≡ (cn1, . . . , cnn )′ population that is standardized. (note that the distribution of tn is unaltered by using this different realization of the antiranks.)"
3176,1,"['asymptotic', 'normality', 'permutation', 'permutation tests', 'tests']", Linear Rank Statistics and Finite Sampling,seg_219,"theorem 2.2 (permutation tests) if ∗νn = op(1) (as when lim c4n < ∞), then the asymptotic normality"
3177,1,['normal'], Linear Rank Statistics and Finite Sampling,seg_219,"⎪⎩ on (ω × ω∗,a × a∗, p × p ∗) if f ∈ d (normal)."
3178,1,['convergence'], Linear Rank Statistics and Finite Sampling,seg_219,"[the convergence is uniform over classes f of dfs f in which d(xn ) →p or a.s.0 uniformly.] also, whenever (22) (or (23)) holds a.e. we have"
3179,1,['results'], Linear Rank Statistics and Finite Sampling,seg_219,"similar results hold for tn (yn ) and t̃n (yn ), where yn ≡ (yn1, . . . , ynn )′ with"
3180,1,"['function', 'independent']", Linear Rank Statistics and Finite Sampling,seg_219,(27) yni ≡ ĝn (xi) for any function ĝn (·) independent of the antiranks dn .
3181,0,[], Linear Rank Statistics and Finite Sampling,seg_219,proof. equation (11) now becomes (recall (10) for mνn (·))
3182,1,['populations'], Linear Rank Statistics and Finite Sampling,seg_219,sampling from finite populations
3183,1,"['sample', 'simple random sampling', 'without replacement', 'random sample', 'replacement', 'sampling', 'population', 'random', 'random sampling']", Linear Rank Statistics and Finite Sampling,seg_219,"example 2.4 (simple random sampling) let x1, . . . , xn be a random sample without replacement from an an ≡ (an1, . . . , ann )′ population. as usual, let x̄n and sn"
3184,1,"['sample', 'sample mean', 'mean']", Linear Rank Statistics and Finite Sampling,seg_219,"denote the sample mean and “sample variance.” suppose that an1 ≤ · · · ≤ ann , that n ≡ nn , and that the d(an ) of (8) satisfy both"
3185,1,[], Linear Rank Statistics and Finite Sampling,seg_219,"prior to normalizing, the cni’s consist of n values of 1 and m ≡ n − n values of 0, with cn . = −n/n and σc2n = mn/n2. after normalizing,"
3186,1,[], Linear Rank Statistics and Finite Sampling,seg_219,"thus (29) implies that all c4n ≤ (some m) < ∞. since cndn1 →d n(0, 1) clearly fails, (12) shows that tn →d n(0, 1) if and only if d(an ) → 0. the limiting rv (as in (5)) will be"
3187,1,"['sampling', 'simple random sampling', 'random sampling', 'random']", Linear Rank Statistics and Finite Sampling,seg_219,"theorem 2.3 (simple random sampling) suppose (29) holds, and suppose the arrays a are uniformly negligible with supa d(an ) → 0 (as in (30)). then"
3188,1,['convergence'], Linear Rank Statistics and Finite Sampling,seg_219,"if (36) also holds. that is, the uniform convergence conclusion in (35) holds if"
3189,1,['moment'], Linear Rank Statistics and Finite Sampling,seg_219,exercise 2.3 (a) show that (30) and (36) both hold whenever the qfs kn (·) have a uniformly bounded 2 + δ moment for any δ > 0. (b) devise a “logarithmic moment” that will suffice.
3190,1,"['sampling', 'variance', 'inequality']", Linear Rank Statistics and Finite Sampling,seg_219,using chebyshev’s inequality with the finite sampling variance of (a.1.9) yields
3191,0,[], Linear Rank Statistics and Finite Sampling,seg_219,thus (34) holds. then (33) and (34) gives the first claim in (35) via slutsky’s theorem. (note that (29) uniformly bounds the ratio √m/n.) the second claim made in (35) will now follow from the identity
3192,0,[], Linear Rank Statistics and Finite Sampling,seg_219,provided we show that
3193,0,[], Linear Rank Statistics and Finite Sampling,seg_219,the proof that (36) implies (37) is found in the 1st edition.
3194,1,"['results', 'bootstrap']", Linear Rank Statistics and Finite Sampling,seg_219,"remark 2.1 at this point in the 1st edition, the next section was used to rederive the bootstrap results of chapter 10 using the present methods instead."
3195,1,"['statistician', 'statistics', 'order statistics', 'function']", LStatistics ,seg_221,"let k ≡ f−1, and define xni ≡ k(ξni), for 1 ≤ i ≤ n, in terms of the uniform (0, 1) rvs ξn1, . . . , ξnn of notation 15.1.3. then xn1, . . . , xnn are iid f , and we let xn:1 ≤ · · · ≤ xn:n denote the order statistics. suppose the statistician specifies a known ↗ and left-continuous function h, known constants cn1, . . . , cnn, and known integers 0 ≤ kn < n − kn"
3196,1,"['asymptotic', 'normality']", LStatistics ,seg_221,to establish the asymptotic normality of the trimmed l-statistic
3197,1,"['results', 'loss', 'cases', 'continuous']", LStatistics ,seg_221,"where h ≡ h(f−1) = h(k) is also ↗ and left continuous. [other useful cases such as h(x) = x2 are dealt with by considering (h−)2 and (h+)2 separately, and then adding the results. here h− ≡ −h · 1[h≤0] and h+ ≡ h · 1[h≥0] denote the negative and positive parts of h. thus there is no theoretical loss in now assuming that h(x) = x and h = f−1.]"
3198,1,['process'], LStatistics ,seg_221,"now, gn and un denote the empirical df and the empirical process of those specially constructed ξni’s of notation 15.1.2, whose empirical process un converges pathwise to the brownian bridge u in the manner described. this will figure heavily in our proofs and in =a claims, but not in any →d claims."
3199,0,[], LStatistics ,seg_221,we need a centering constant μn for ln. we define
3200,0,[], LStatistics ,seg_221,"where the value of jn at the i/n points is totally inconsequential. suppose that jn “converges” to j in some sense. define an ≡ kn/n, a′n ≡ kn"
3201,0,['n'], LStatistics ,seg_221,"′ /n as before, and then define centering constants"
3202,1,['data'], LStatistics ,seg_221,"′ = 0 and cnn > 0) entails the added requirement that eh−(ξ) be finite (that eh+(ξ) be finite), for ξ =∼uniform (0, 1). [our main interest is in μn, while μn0 is secondary; μn is the data analysis constant, while μn0 is just a constant for theory.]"
3203,0,[], LStatistics ,seg_221,"it is convenient to assume that on (0, 1)"
3204,1,['continuous'], LStatistics ,seg_221,"(4) jn ≥ 0, j ≥ 0 is continuous, and h is ↗ and left continuous."
3205,1,"['functions', 'results']", LStatistics ,seg_221,"[more generally, we can apply our results for two separate j functions and then just subtract the results.] now specify a. ∈ (0, 1) to satisfy h+(a.) = 0, and define"
3206,1,['set'], LStatistics ,seg_221,"where ∫(a,t) ≡ −∫[t,a] .(but set a. = 0 if h(·) ≥ 0, and use ∫[0,t) in (5) ; and set a. = 1 if"
3207,1,['case'], LStatistics ,seg_221,"h(·) ≤ 0, and use ∫[t,1] in (5).) thus (in case (5))"
3208,1,['continuous'], LStatistics ,seg_221,"(6) k is ↗ and left continuous on (0, 1) with k+(a.) = 0,"
3209,1,['loss'], LStatistics ,seg_221,"where k + is the right-continuous version, and k ≡ k+ − k. [since ln − μn is invariant under vertical shift, there is actually no theoretical loss in also assuming as we did above that h satisfies h+(a.) = 0.] since k is a qf, the unobservable rvs"
3210,1,['case'], LStatistics ,seg_221,the most historically important case obtains when
3211,1,"['case', 'probability']", LStatistics ,seg_221,"in this case we would desire to show that (on the same probability space where the special ξni’s above are defined) for some n(0, 1) rv that we will denote by zk (or alternatively, and"
3212,0,[], LStatistics ,seg_221,"suggestively, we will also denote by 1 kdu/σ) we have"
3213,1,"['estimator', 'consistent estimator']", LStatistics ,seg_221,"we would also like a consistent estimator of σ, and we might want to be able to replace"
3214,0,[], LStatistics ,seg_221,"let k̃n denote k winsorized outside (an, 1−an′ ), and define the unobservable winsorized rvs"
3215,1,"['variance', 'mean']", LStatistics ,seg_221,"then ỹn1, . . . , ỹnn are iid with qf k̃n and mean μ̃n and variance σ̃n"
3216,1,['variance'], LStatistics ,seg_221,′ = 0 if the variance double integral is finite.) let
3217,1,['case'], LStatistics ,seg_221,in this case it is our desire to show that
3218,1,['estimator'], LStatistics ,seg_221,"we also seek an appropriate estimator of σ̃n, and we may want to be able to replace μn by"
3219,1,['mean'], LStatistics ,seg_221,1 k̃ndu to mean −∫0
3220,0,[], LStatistics ,seg_221,"is, pathwise integration for each ω) for all qfs."
3221,1,"['moment', 'statistician', 'quantiles', 'variation']", LStatistics ,seg_221,"make throughout without further comment the rather modest assumptions that an and a′n satisfy lim inf(1 − an − a′n) > 0 and lim inf σ̃n = lim inf σk(an, a′n) > 0 for the df f or f0 under consideration at the particular moment. the first says that we will deal with averaging, rather than just “quantiles” (though we could easily have added in fixed quantiles had we chosen to do so). the second says that the statistician has at least enough insight to avoid removing all the variation."
3222,1,"['sample', 'asymptotic', 'treatment', 'results', 'normality', 'samples', 'mean', 'score function', 'function', 'convergence']", LStatistics ,seg_221,"we now state the two most elementary theorems about l-statistics found in the 1st edition. the results found there establish uniform convergence to normality over large classes of dfs, and they present studentized versions of such results. this is a very complete treatment of l-statistics. roughly, suppose the finite sample score function jn function is sufficiently close to a limiting score function j , as defined in the 1st edition. then any asymptotic normality theorem for the mean (whether trimmed or untrimmed) of a sample from the df k defined in (5) is also true for the corresponding l-statistic of (1) based on samples from the df f . consult the 1st edition for proofs of the following results."
3223,1,"['function', 'statistic', 'score function']", LStatistics ,seg_221,"theorem 3.1 (clt for l-statistics) suppose the score function j(·) of (4) is approximated “sufficiently closely” by a sequence jn(·). let the statistic ln in (1) be untrimmed. suppose also that y ≡ k(ξ) =∼ (μ, σ2) with σ2 ∈ (0, ∞) for the k of (5). let μ̃n and σ̃n be"
3224,1,['estimator'], LStatistics ,seg_221,"(moreover, vn/σ →p 1 for an estimator vn2 of σ2 presented in the 1st edition.)"
3225,1,['statistician'], LStatistics ,seg_221,theorem 3.2 (clt for trimmed l-statistics) suppose that j(·) as in (4) is approximated “sufficiently closely” by a sequence jn(·). suppose the statistician protects himself by specifying trimming numbers kn and kn
3226,1,['statistical'], LStatistics ,seg_221,"′ for which kn ∧ kn ′ → ∞, while an ∨ a′n → 0 with an/a′n → 1, and suppose that k is in the statistical domain of attraction d̃ (recall (c.5.33), (15.1.3), and proposition 10.6.1). then (in the context of notation 15.1.3)"
3227,1,['estimator'], LStatistics ,seg_221,"(moreover, ṽn/σ̃n →p 1 for an estimator ṽn2 of σ̃n"
3228,1,[], LStatistics ,seg_221,"2 presented in the 1st edition.) [if k ∈ d(normal), only (kn ∧ kn"
3229,1,"['bernoulli', 'trials', 'bernoulli trials']", Elementary Probability,seg_225,independent bernoulli trials
3230,1,"['independent', 'successes', 'trials', 'event']", Elementary Probability,seg_225,"if p (x = 1) = p = 1 − p (x = 0), then x is said to be a bernoulli(p) rv. we refer to the event [x = 1] as “success,” and [x = 0] as “failure.” let x1, . . . , xn be iid bernoulli(p), and let tn ≡ x1 + · · · + xn denote the number of successes in n independent bernoulli(p) trials. now,"
3231,1,"['distribution', 'joint']", Elementary Probability,seg_225,"this formula gives the joint distribution of x1, . . . , xn. from this we obtain"
3232,0,['n'], Elementary Probability,seg_225,n since each of the ( k ) different possibilities that place k of the 1’s in specific positions in an
3233,1,"['probability', 'outcomes']", Elementary Probability,seg_225,"n-vector containing k outcomes 1 and n − k outcomes 0 has probability pk(1 − p)n−k, from the earlier display. we denote this by writing tn ∼= binomial(n, p) when (1) holds. note that binomial(1, p) is the same as bernoulli(p)."
3234,1,"['process', 'event']", Elementary Probability,seg_225,"let x1,x2, . . . be iid bernoulli(p); call this a bernoulli(p) process. interesting rvs include y1 ≡ w1 ≡ min{n : tn = 1}. since we can rewrite the event [y1 = k] = [x1 = · · · = xk−1 = 0,xk = 1], we have"
3235,1,"['success', 'geometric']", Elementary Probability,seg_225,"∼ we write y1 = geometric t(p). now let wm ≡ min{n : tn = m}. we call wm the waiting time to the mth success; wm counts the number of turns until the mth success. we let ym ≡ wm −wm−1 for m ≥ 1, with w0 ≡ 0, and we call the ym’s the interarrival times. note that [wm = k] = [tk−1 = m − 1 and xk = 1]. hence"
3236,1,"['failures', 'distribution', 'negative binomial', 'success', 'binomial']", Elementary Probability,seg_225,"∼ we write wm = negative binomial turns(m, p) ≡ negbit(m, p). [we agree that negbif(m, p) denotes the distribution of wm − m, and that this “f” connotes “failures”; the rv wm − m counts the number of failures prior to the mth success.]"
3237,1,[], Elementary Probability,seg_225,"exercise 1.1 explain why y1, y2, . . . are iid geometrict(p)."
3238,1,"['trials', 'successes']", Elementary Probability,seg_225,"since the number of successes in the first n1 + n2 trials is the same as the number of successes in the first n1 trials plus the number of successes in the next n2 trials, it is clear that"
3239,1,['independent'], Elementary Probability,seg_225,"(4) t1 + t2 ∼= binomial(n1 + n2, p) for independent rvs ti ∼= binomial(ni, p)."
3240,1,['successes'], Elementary Probability,seg_225,"likewise, waiting for m1 successes and then waiting for m2 more successes is the same as waiting for m1 + m2 successes in the first place. hence,"
3241,1,['independent'], Elementary Probability,seg_225,"∼ ∼ (5) w1 + w2 = negbit(m1 + m2, p) for independent rvs wi = negbit(mi, p)."
3242,1,[], Elementary Probability,seg_225,urn models
3243,1,['random'], Elementary Probability,seg_225,"suppose an urn contains n balls that are identical, except that m bear the number 1 and n − m bear the number 0. thoroughly mix the balls in the urn. draw one ball at random."
3244,1,"['independent', 'with replacement', 'replacement', 'sampling', 'success', 'trials', 'probability of success', 'binomial', 'probability', 'random', 'process']", Elementary Probability,seg_225,"∼ let x1 denote the number on the ball drawn. then x1 = bernoulli(p) with p ≡ m/n . now replace the ball in the urn, thoroughly mix, and draw at random a second ball with number x2. continue the process. this is the sampling with replacement scheme. then tn ≡ x1 + · · · + xn ∼= binomial (n, p), where p = m/n represents the probability of success in n independent bernoulli(p) trials."
3245,1,"['without replacement', 'dependent', 'replacement', 'sampling']", Elementary Probability,seg_225,"suppose now that the same scheme is repeated, except that the balls are not replaced. in this sampling without replacement scheme x1, . . . , xn are dependent bernoulli(p) rvs with p = m/n . also,"
3246,1,[], Elementary Probability,seg_225,"∼ we write tn = hypergeometric(m,n − m ;n)."
3247,1,"['model', 'without replacement', 'replacement', 'sampling']", Elementary Probability,seg_225,"suppose now that sampling is done without replacement, but the n balls in the urn bear the numbers a1, . . . , an . let x1, . . . , xn denote the numbers on the first n balls drawn, and let tn ≡ x1 + · · ·+xn. we call this the general finite sampling model. call ā ≡∑1"
3248,1,['mean'], Elementary Probability,seg_225,population mean and σa2 ≡ ∑1
3249,1,"['population variance', 'variance', 'population']", Elementary Probability,seg_225,"n (ai − ā)2/n the population variance. note that xi =∼ (ā, σa2) for all 1 ≤ i ≤ n, since we now assume n ≤ n . from (7.3.4), we have"
3250,0,[], Elementary Probability,seg_225,nxi is a constant. solving (7) yields
3251,1,"['correction factor', 'factor', 'sampling', 'population']", Elementary Probability,seg_225,where [1 − (n − 1)/(n − 1)] is called the correction factor for finite population sampling.
3252,1,"['independent', 'conditional', 'distribution', 'binomial', 'conditional distribution']", Elementary Probability,seg_225,"∼ ∼ exercise 1.3 suppose that t1 = binomial (m, p) and t2 = binomial(n, p) are independent. then the conditional distribution of the first component t1 given the total t1 + t2 = k is hypergeometric(k,m + n − k;m)."
3253,1,"['poisson', 'process', 'poisson process']", Elementary Probability,seg_225,the poisson process
3254,1,[], Elementary Probability,seg_225,"suppose now that xn1, . . . , xnn are iid bernoulli(pn), where npn → λ as n → ∞. let tn ≡ xn1 + · · · + xnn, so that tn ∼= binomial(n, pn). simple calculations give"
3255,1,[], Elementary Probability,seg_225,"when p (t = k) = λke−λ/k! for k = 0, 1, . . ., we write t =∼ poisson(λ)."
3256,1,"['poisson', 'model', 'independent', 'poisson process', 'interval', 'experiment', 'treatment', 'process']", Elementary Probability,seg_225,"this is now used to model a geiger counter experiment. a radioactive source with large half-life is placed near a geiger counter. let n(t) denote the number of particles registered by time t. we will say that {n(t) : t ≥ 0} is a poisson process. (do note that our treatment is purely informal.) physical considerations lead us to believe that the increments n(t1), n(t1, t2], . . . , n(tk−1, tk] should be independent rvs; here, the increment n(ti−1, ti] ≡ n(ti)−n(ti−1) is the number of particle counts across the interval (ti−1, ti]. we say that n has independent increments. let us now define"
3257,1,"['intensity', 'process']", Elementary Probability,seg_225,(11) ν ≡ en(1) ≡ [the intensity of the process].
3258,1,"['poisson', 'model', 'independent', 'registers', 'approximation', 'bernoulli', 'stationary', 'mean']", Elementary Probability,seg_225,"let m denote the number of radioactive particles in our source, and let xi equal 1 or 0 depending on whether or not the ith particle registers by time t = 1. it seems possible to assume that x1, . . . , xm are iid bernoulli. since n(1) = x1 + · · · + xm has mean ν = en(1) = mex1, this leads to n(1) ∼= binomial(m,ν/m). by the first paragraph of this section, n(1) is thus approximately a poisson (ν) rv. we now alter our point of view slightly, and agree that we will use this approximation as our model. thus n(1) is a poisson(ν) rv. since m is huge, the accuracy should be superb. because of the stationary and independent increments we thus have"
3259,1,[], Elementary Probability,seg_225,"∼ (12) n(s, t] ≡ n(t) − n(s) = poisson(ν(t − s)) for all 0 ≤ s ≤ t, and"
3260,1,['independent'], Elementary Probability,seg_225,(13) n has independent increments.
3261,1,"['poisson', 'process', 'poisson process']", Elementary Probability,seg_225,we agree also that n(0) ≡ 0. (this is actually enough to rigorously specify a poisson process.) let y1 ≡ w1 ≡ inf{t : n(t) = 1}. since
3262,1,[], Elementary Probability,seg_225,we write y1 ∼ exponential(ν). now let wm ≡ inf{t : n(t) = m}; we call wm the mth
3263,1,['model'], Elementary Probability,seg_225,"= waiting time. we call ym ≡ wm − wm−1,m ≥ 1, the mth interarrival time. in light of the physical properties of our geiger counter model, and using (13), it seems reasonable that"
3264,1,[], Elementary Probability,seg_225,"(16) y1, y2, . . . are iid exponential(ν) rvs."
3265,0,[], Elementary Probability,seg_225,our assumption of the previous sentence could be expressed as follows:
3266,1,"['poisson', 'intensity', 'independent', 'poisson process', 'process']", Elementary Probability,seg_225,"y1 and n1(t) ≡ n(y1, y1 + t] = n(y1 + t) − n(y1) are independent, (17) n1is again a poisson process, with intensity ν."
3267,1,"['poisson', 'process', 'poisson process']", Elementary Probability,seg_225,"we will call this the strong markov property of the poisson process. additionally,"
3268,0,[], Elementary Probability,seg_225,"telescopes, and shows that wm has density"
3269,1,[], Elementary Probability,seg_225,"we write wm ∼= gamma(m, ν). since waiting for m1 counts and then waiting for m2 more counts is the same as waiting for m1 + m2 counts in the first place,"
3270,1,['independent'], Elementary Probability,seg_225,"∼ ∼ (20) z1 + z2 = gamma(m1 + m2, ν) for independent zi = gamma(mi, ν)."
3271,0,[], Elementary Probability,seg_225,"it is true that (19) is a density for any real number m > 0, and the property (20) still holds for all positive mi’s."
3272,1,['binomial'], Elementary Probability,seg_225,"exercise 1.4 verify (10), that binomial (n, pn) → poisson(λ) as npn → λ."
3273,0,[], Elementary Probability,seg_225,"exercise 1.5 verify (19), that fwm has derivative fwm ."
3274,0,[], Elementary Probability,seg_225,exercise 1.6 verify that (20) holds for arbitrary real mi > 0.
3275,1,"['conditional', 'distribution', 'conditional distribution']", Elementary Probability,seg_225,"∼ ∼ exercise 1.7 if x = poisson(ν1) and y = poisson(ν2), then the conditional distribution of x given that x + y = n is binomial(n, ν1/(ν1 + ν2))."
3276,1,"['poisson', 'sample', 'poisson process', 'continuous', 'process']", Elementary Probability,seg_225,"exercise 1.8 use kolmogorov’s extension theorem to show that a poisson process n exists on (r[0,∞),b[0,∞)). then apply the smoother realizations theorem 5.4.2 to claim that a.e. sample path is right continuous with integral jumps."
3277,0,[], Elementary Probability,seg_225,holds for any fz(·). thus for any density fz(·) the rv az + b has density
3278,1,['distributions'], Elementary Probability,seg_225,normal distributions
3279,0,[], Elementary Probability,seg_225,suppose the rv z has density
3280,1,"['standard normal', 'normal', 'standard']", Elementary Probability,seg_225,"then z is said to be a standard normal rv. thus the rv x ≡ μ + σz =∼ (μ, σ2) has density (by (21))"
3281,1,[], Elementary Probability,seg_225,"and we write x =∼ normal(μ, σ2), or just x =∼ n(μ, σ2)."
3282,1,"['variance', 'mean']", Elementary Probability,seg_225,exercise 1.9 show that the formula fz(·) of (22) is a density. then show that this density has mean 0 and variance 1. [transform to polar coordinates to compute (∫fz(x) dx)2 = 1.]
3283,1,"['sample', 'distribution', 'normal', 'moments', 'sample average', 'average', 'normal distribution']", Elementary Probability,seg_225,"the importance of the normal distribution derives from the following theorem. recall that if x1, . . . , xn are iid (μ, σ2) with 0 < σ < ∞, then √n(x̄ − μ)/σ =∼ (0, 1) for the sample average x̄n ≡ (x1 + · · · + xn)/n. this is only a statement about moments. but much more is true. the proof of the powerful result we now state is found in chapter 10. we will use it here for motivational purposes."
3284,1,['normal'], Elementary Probability,seg_225,"let 0 < σ < ∞. then the rv zn below is asymptotically normal, in that"
3285,0,[], Elementary Probability,seg_225,thus z2 has density
3286,0,[], Elementary Probability,seg_225,[note that formula (27) is true for any density fz(·).] plugging into (27) for this z shows that
3287,1,"['distribution', 'gamma']", Elementary Probability,seg_225,this is called the chisquare(1) distribution. note that chisquare(1) is the same as gamma
3288,0,[], Elementary Probability,seg_225,1 ). thus (20) establishes that
3289,1,[], Elementary Probability,seg_225,where chisquare(m) ≡ gamma(m
3290,1,['distributions'], Elementary Probability,seg_225,uniform and related distributions
3291,1,['case'], Elementary Probability,seg_225,"by far the most important special case is uniform(0, 1). a generalization of this is the"
3292,1,"['order statistics', 'statistics']", Elementary Probability,seg_225,"suppose that ξ1, . . . , ξn are iid uniform(0, 1). let 0 ≤ ξn:1 ≤ · · · ≤ ξn:n ≤ 1 denote the ordered values of the ξi’s; we call the ξn:i’s the uniform order statistics. it seems intuitive that ξn:i equals x if (i − 1) of the ξi’s fall in [0, x), 1 of the ξi’s is equal to x, and n − i of the"
3293,1,[], Elementary Probability,seg_225,"ξi’s fall in (x, 1]. there are n!/[(i−1)!(n− i)!] such designations of the ξni’s, and for each such designation the “chance” of the rv’s falling in the correct parts of [0, 1] is xi−1(1·dx)(1−x)n−i. thus"
3294,0,[], Elementary Probability,seg_225,exercise 1.10 give a rigorous derivation of (32) by computing 1 − fξni(x) and then differentiating it.
3295,1,"['joint', 'sphere', 'probability', 'random']", Elementary Probability,seg_225,exercise 1.11 choose a point at random on the surface of the unit sphere (with probability proportional to area). let θ denote the longitude and φ denote the latitude (relative to some fixed axes) of the point so chosen. determine the joint density of θ and φ.
3296,1,"['distribution', 'cauchy', 'cauchy distribution']", Elementary Probability,seg_225,the cauchy distribution
3297,1,[], Elementary Probability,seg_225,"∼ write x = cauchy(b, a) if"
3298,1,"['cauchy', 'case']", Elementary Probability,seg_225,"∼ by far the most important special case is cauchy(0, 1); we then say simply that x cauchy,"
3299,1,"['sample', 'cauchy', 'sample average', 'average']", Elementary Probability,seg_225,"= and its density is given by 1/[π(1 + x2)] on (−∞,∞). verify that e|x| = ∞. we will see below that if x1, . . . , xn are iid cauchy, then the sample average x̄n ≡ (x1 + · · · + xn)/n ∼= cauchy. these two facts make the cauchy ideal for many counterexamples."
3300,1,"['logistic', 'exponential', 'exponential and logistic distributions', 'distributions']", Elementary Probability,seg_225,double exponential and logistic distributions
3301,1,[], Elementary Probability,seg_225,"we say x =∼ double exponential(b, a) when (x − b)/a has density 2"
3302,1,[], Elementary Probability,seg_225,"we say x =∼ logistic(b, a) when (x − b)/a has density ex/(1 + ex)2 = 1/(e−x/2 + ex/2)2 on the line."
3303,1,"['transformation', 'distributions']", Elementary Probability,seg_225,"exercise 1.12 now, x ≡ f−1(ξ) has df f by the inverse transformation. so, compute f−1 for the logistic(0, 1) and the double exponential(0, 1) distributions."
3304,1,"['random variables', 'variables', 'random']", Elementary Probability,seg_225,rademacher random variables and symmetrization
3305,0,[], Elementary Probability,seg_225,many problems become simpler if the problem is symmetrized. one way of accomplishing this is by the appropriate introduction of rademacher rvs. we say that is a rademacher rv if p ( = 1) = p ( = −1) = 2
3306,1,['symmetric'], Elementary Probability,seg_225,"1 . thus =∼ 2 bernoulli(1 2 ) − 1. we say that x is a symmetric rv if x =∼ −x. if x and x ′ are iid, then xs ≡ (x −x ′) =∼ (x ′ − x) = −(x − x ′) = −xs; hence xs is a symmetric rv."
3307,1,"['symmetric', 'independent']", Elementary Probability,seg_225,"∼ exercise 1.13 if x is a symmetric rv independent of the rademacher rv , then x = x always holds."
3308,1,"['distribution', 'multinomial distribution', 'multinomial']", Elementary Probability,seg_225,the multinomial distribution
3309,1,"['sets', 'set']", Elementary Probability,seg_225,"suppose that b1 + · · · + bk = r for borel sets bi ∈ b; recall that we call this a partition of r. let y1, . . . , yn be iid rvs on (ω,a, p ). let xi ≡ (xi1, . . . , xik) ≡ (1b1(yi), . . . , 1bk(yi)) for 1 ≤ i ≤ n, and set"
3310,1,"['multinomial', 'dependent', 'distribution', 'joint', 'coefficient']", Elementary Probability,seg_225,"∼ note that x1j , . . . , xnj are iid bernoulli(pj) with pj ≡ p (yi ∈ bj), and thus tj = binomial(n, pj) (marginally). however, t1, . . . , tn are dependent rvs. the joint distribution of (t1, . . . , tn)′ is called the multinomial(n, p) distribution. we now derive it. the number of ways to designate n1 of the yi’s to fall in b1, . . ., and nk of the yi’s to fall in bk is the multinomial coefficient"
3311,1,['probability'], Elementary Probability,seg_225,each such designation occurs with probability ∏1
3312,0,['n'], Elementary Probability,seg_225,"k pini . hence for each possible n,"
3313,0,[], Elementary Probability,seg_225,it is now a trivial calculation that
3314,0,[], Elementary Probability,seg_225,thus (with dp a diagonal matrix having each dii = pi)
3315,0,['n'], Elementary Probability,seg_225,stirling’s formula for n! for all n > 1 we have
3316,1,"['probability', 'conditional', 'conditional probability']", Elementary Probability,seg_225,elementary conditional probability
3317,1,"['independent', 'conditional probability', 'probability of the event', 'conditional', 'probability', 'event']", Elementary Probability,seg_225,"one defines the conditional probability of the event a given that the event b has occurred via p (a|b) ≡ p (ab)/p (b) when p (b) = 0. one then calls a and b independent if p (a|b) = p (a), because the probability of a is then unaffected by whether or not b occurred. thus both of the following statements hold:"
3318,1,"['independence', 'independent']", Elementary Probability,seg_225,"definition: independence means p (a|b) = p (a), and (43) leads to theorem: p (ab) = p (a)p (b) if a and b are independent."
3319,1,"['with replacement', 'without replacement', 'replacement', 'sampling', 'probability', 'random']", Elementary Probability,seg_225,"the big advantage of computation of p (a|b) via the theorem of (42) is that one can often revisualize p (a|b) in the context of a much simpler problem. thus the probability of drawing two reds when drawing at random without replacement from an urn containing 6 reds and 4 whites is p (r1r2) = p (r1)p (r2|r1) = (6/10) × (5/9), where we revisualized to an urn containing 5 reds and 4 whites to compute p (r2|r1) = 5/9. [had we used sampling with replacement, our answer would have been (6/10) ×(6/10) via (43).] [in the next exercise, revisualization works superbly to trivialize the problem.]"
3320,1,"['loss', 'probability']", Elementary Probability,seg_225,"exercise 1.15 (craps, according to hoyle) (a) the “shooter” rolls two dice, and obtains a total (called the “point”). if “point” equals “seven” or “eleven,” the game is over and “shooter” wins. if point equals “two” or “twelve,” the game is over and “shooter” loses. otherwise, the game continues. it is now a race between “point” and “seven.” if “point” comes first, the “shooter” wins; otherwise, he loses. determine the probability that the “shooter” wins in the game of craps. [when trying to “convert” a “point” of “ten” (say), we can revisualize and say that on the turn on which the game ends the dice will be showing either one of the 3 tens or one of the 6 sevens, and the probability of this conversion is clearly 3/(3 + 6).] (b) (the las vegas game) the above game is favorable to the “shooter.” thus the version played in las vegas has different rules. specifically, a “three” on the first roll of the two dice is also an immediate loss for “shooter.” determine the probability that “shooter” wins the las vegas version of craps."
3321,1,['independent'], Distribution Theory for Statistics,seg_227,"if x and y are independent rvs on (ω,a, p ), then"
3322,1,"['convolution', 'case']", Distribution Theory for Statistics,seg_227,"is a formula, called the convolution formula, for fx+y in terms of fx and fy (the symbol ∗ defined here stands for “convolution”). in case y has density fy with respect to lebesgue measure, then so does x + y . in fact, since"
3323,0,[], Distribution Theory for Statistics,seg_227,we see that x + y has a density given by
3324,1,"['densities', 'case']", Distribution Theory for Statistics,seg_227,"in case both x and y have densities, we further note that"
3325,1,['independent'], Distribution Theory for Statistics,seg_227,"exercise 2.1 use (2) to show that for x and y independent: (i) x =∼ n(μ1, σ12) and y =∼ n(μ2, σ22) implies x + y =∼ n(μ1 + μ2, σ12 + σ22)."
3326,1,[], Distribution Theory for Statistics,seg_227,"∼ ∼ ∼ (ii) x cauchy(0, a1) and y cauchy(0, a2) has x + y cauchy(0, a1 + a2)."
3327,1,[], Distribution Theory for Statistics,seg_227,"= = = ∼ ∼ ∼ (iii) x = gamma(r1, θ) and y = gamma(r2, θ) has x + y = gamma(r1 + r2, θ)."
3328,1,"['sample', 'average', 'sample average']", Distribution Theory for Statistics,seg_227,"exercise 2.2 (i) let x1, . . . , xn be iid n(0, 1). show that the normed sample average"
3329,1,[], Distribution Theory for Statistics,seg_227,"∼ necessarily satisfies (x1 + · · · + xn)/√n = n(0, 1). ∼ (ii) let x1, . . . , xn be iid cauchy(0, 1). show (x1 + · · · + xn)/n = cauchy(0, 1)."
3330,1,['independent'], Distribution Theory for Statistics,seg_227,"if x and y are independent rvs taking values in 0, 1, 2, . . ., then clearly"
3331,1,['independent'], Distribution Theory for Statistics,seg_227,exercise 2.3 use (3) to show that for x and y independent:
3332,1,[], Distribution Theory for Statistics,seg_227,∼ ∼ ∼ x = poisson(λ1) and y = poisson(λ2) has x + y = poisson(λ1 + λ2).
3333,1,"['probability theory', 'probability']", Distribution Theory for Statistics,seg_227,"a fundamental problem in probability theory is to determine constants bn and an > 0 for which iid rvs x1, . . . , xn, . . . satisfy"
3334,1,"['cases', 'convergence', 'convolution']", Distribution Theory for Statistics,seg_227,"for some nondegenerate df g. exercise 2.2 gives us two examples of such convergence; each was derived via the convolution formula. except in certain special cases, such as exercises"
3335,1,['convolution'], Distribution Theory for Statistics,seg_227,"2.1 – 2.3, the various convolution formulas are too difficult to deal with directly. for this reason we need to develop a more oblique, but ultimately more convenient, approach if we are to solve problems of the form (5). this is taken up in chapters 9, 10, and 11."
3336,1,['independent'], Distribution Theory for Statistics,seg_227,exercise 2.4 suppose that x and y are independent with p (y > 0) = 1. show that products and quotients of these rvs satisfy
3337,1,['densities'], Distribution Theory for Statistics,seg_227,"if fx has a density fx , then changing the order of integration above shows that fxy and fx/y have densities given by"
3338,1,"['results', 'independent']", Distribution Theory for Statistics,seg_227,"exercise 2.5 let z =∼ n(0, 1), u =∼ χ2m, and v =∼ χ2n be independent. (a) establish these classically important results:"
3339,1,"['moment', 'distributions']", Distribution Theory for Statistics,seg_227,(b) compute the kth moment of each of these three distributions.
3340,1,[], Distribution Theory for Statistics,seg_227,"exercise 2.6 if y1, . . . , yn+1 are iid exponential(θ), then"
3341,1,['independent'], Distribution Theory for Statistics,seg_227,2 are independent rvs. ∼ (d) show that tn ≡ √n(x̄n − μ)/sn = student’s tn−1.
3342,1,"['confidence intervals', 'intervals', 'confidence']", Distribution Theory for Statistics,seg_227,statistical confidence intervals
3343,1,"['sample', 'model', 'independent', 'experiment', 'sample variance', 'variance', 'estimator']", Distribution Theory for Statistics,seg_227,"example 2.1 suppose we model the performances of n independent repetitions x1, . . . , xn of an experiment as iid n(μ, σ2)rvs. the previous exercise shows that √n(x̄n − μ)/σ is a n(0, 1) rv independent of the sample variance estimator sn"
3344,1,['probability'], Distribution Theory for Statistics,seg_227,"specify tp/2 such that p (−tp/2 ≤ tn−1 ≤ tp/2) = 1 − p; perhaps, with p = .05. then with the “large” probability of 1 − p = .95 we have"
3345,1,"['random interval', 'interval', 'random']", Distribution Theory for Statistics,seg_227,the random interval x̄n ± tp/2sn/√n (20) will contain the unknown value of μ
3346,1,['average'], Distribution Theory for Statistics,seg_227,an average of (1 − p) × 100% of the time.
3347,1,"['confidence', 'interval', 'data']", Distribution Theory for Statistics,seg_227,"so when we apply this to the data values x1, . . . , xn, we can have (1 − p) × 100% confidence that the interval x̄n ± tp/2 sn/√n did enclose the true (but unknown) value of μ. we say that"
3348,1,"['confidence', 'interval', 'confidence interval']", Distribution Theory for Statistics,seg_227,(21) x̄n ± tp/2 sn/√n provides a (1 − p) × 100% confidence interval
3349,1,['mean'], Distribution Theory for Statistics,seg_227,for the unknown mean μ. or we say that
3350,1,"['numerical', 'confidence', 'interval', 'confidence interval']", Distribution Theory for Statistics,seg_227,(22) x̄n ± tp/2 sn/√n provides a (1 − p) × 100% numerical confidence interval
3351,1,"['experiment', 'data', 'mean', 'probability', 'confidence', 'error', 'numerical']", Distribution Theory for Statistics,seg_227,"for the unknown mean μ. there is a probability of 1 − p (or a (1 − p) × 100% chance) that the former will contain the unknown value of μ when the x-experiment is repeated n times. there is a (1 − p) × 100% confidence (or degree of belief) that the latter did contain the unknown value of μ after the x-experiment was repeated n times giving the actual data values x1, . . . , xn. we call tp/2 sn/√n the numerical margin for error exhibited by our experiment."
3352,1,"['random variables', 'variables', 'random']", Distribution Theory for Statistics,seg_227,transformations of random variables
3353,0,[], Distribution Theory for Statistics,seg_227,exercise 2.8 suppose x has density fx(·) with respect to lebesgue measure λn(·) on n-dimensional euclidean space rn.
3354,1,"['jacobian', 'linear', 'linear transformation', 'transformation']", Distribution Theory for Statistics,seg_227,(a) let y ≡ ax denote a linear transformation with a a nonsingular matrix. the jacobian of this linear transformation is
3355,0,[], Distribution Theory for Statistics,seg_227,verify that the rv y has a density fy (·) with respect to lebesgue measure that is given by fy (y) = fx(a−1y)/|a|+ on rn.
3356,1,"['continuous', 'transformation', 'jacobian']", Distribution Theory for Statistics,seg_227,(b) suppose now that x has density fx(·) with respect to lebesgue measure on a region rx in rn. suppose the 1-to-1 transformation y ≡ g(x) from rx to the region ry ≡ g(rx) has a nonsingular jacobian with continuous elements at each point of the region. show that y has a density given by
3357,1,"['transformation', 'linear']", Distribution Theory for Statistics,seg_227,"(that is, any “nice” transformation is locally linear.)"
3358,1,"['transformation', 'joint', 'jacobian']", Distribution Theory for Statistics,seg_227,"exercise 2.9 suppose that u ≡ xy and v ≡ x/y for rvs having joint density fxy (·, ·) on the region where x > 0 and y > 0. the inverse transformation is x = √uv and y =√u/v with a “nice” jacobian that is equal to 2v. thus the joint density of u, v is"
3359,1,"['independent', 'cases', 'transformation']", Distribution Theory for Statistics,seg_227,"provided that the transformation is 1-to-1. (obtaining the appropriate region is often the hardest part.) now evaluate fuv (·, ·) and fv (·) in the following cases. (a) x and y are independent exponential(1). (b) x and y are independent with density 1/(xy)2 on x, y ≥ 1. evaluate fu (·). (c) x and y are independent n(0, 1). [note that this transformation is not 1-1.]"
3360,1,['independent'], Distribution Theory for Statistics,seg_227,"∼ ∼ (d) x = n(0, 1) and y = uniform(0, 1) are independent. [this exercise demonstrates vividly the important role played by the regions rx and ry .]"
3361,1,"['covariance matrix', 'covariance', 'mean', 'inequality']", Linear Algebra Applications,seg_229,"notation 3.1 (mean vector and covariance matrix) let x ≡ (x1, . . . , xn)′ be a rv. then e(x) ≡ μ ≡ (μ1, . . . , μn)′, where μi ≡ e(xi) is called the mean vector. and σ ≡ |[σij ]| ≡ |[cov[xi,xj ]]| is called the covariance matrix. (by the cauchy–schwarz inequality, both of μ and σ are well-defined provided that each of σii ≡ var[xi] ≡ cov[xi,xi] is finite.)"
3362,1,"['linear', 'combinations', 'symmetric', 'standardized', 'linear combinations', 'set', 'transformation']", Linear Algebra Applications,seg_229,"definition 3.1 (linear algebra) we will operate on n-dimensional space rn with n × n matrices and n × 1 vectors. (i) a matrix γ with column vectors γi (that is, γ = [γ1, . . . , γn]) is called orthogonal if γ′γ = i. [thus γi′γj equals 1 or 0 according as i = j or i = j; when γi′γj = 0 we say that these vectors are orthogonal, and we write γi ⊥ γj .] under the orthogonal transformation of rn onto itself defined by y = γx, the image of each γi is the standardized basis vector ei ≡ (0, . . . , 0, 1, 0, . . . , 0)′ with the 1 in the ith slot. (ii) call a symmetric matrix a positive definite (written a > 0) if x′ax > 0 for all vectors x = 0. call it nonnegative definite (written a ≥ 0) if x′ax ≥ 0 for all vectors x = 0. (iii) if a is symmetric and idempotent (that is, if aa = a), then a is called a projection matrix (the symbol p is often used for a projection matrix). (iv) let da be the diagonal matrix with dii = ai (and dij = 0 for all i = j). (v) let r[a] denote the column space of a; that is, it is the set of all vectors that can be written as linear combinations of the column vectors of a. (vi) call x′ax =∑n"
3363,1,['linear'], Linear Algebra Applications,seg_229,"i=1xiaijxj a quadratic form in the vector x. what follows is the statistician’s main result from linear algebra. we simply state it, then interpret it geometrically in discussion 3.1, and then put it into a very useful format in discussion 3.2."
3364,1,['symmetric'], Linear Algebra Applications,seg_229,"theorem 3.1 (principal axes theorem) let a denote an arbitrary real and symmetric matrix of rank r. (a) there exists an orthogonal matrix γ ≡ [γ1, . . . , γn] and a diagonal matrix d for which we have the representation"
3365,1,['transformation'], Linear Algebra Applications,seg_229,discussion 3.1 (spectral decomposition) consider a projection matrix p of rank r. then the transformation y = px can be broken down as
3366,1,['transformation'], Linear Algebra Applications,seg_229,"where (γi′x)γi is the projection of x onto γi in the direction of γi, and where this term is present when dii = 1 and is absent when dii = 0. also, px ⊥ (i − p )x, where the transformation"
3367,1,['transformation'], Linear Algebra Applications,seg_229,with pi ≡ γiγi′. this is called the spectral decomposition of the transformation y = px.
3368,0,[], Linear Algebra Applications,seg_229,"exercise 3.1 (a) show that for compatible matrices b and c,"
3369,0,[], Linear Algebra Applications,seg_229,proposition 3.1 (properties of e(·)) (a) it holds that
3370,1,"['covariance', 'covariance matrix']", Linear Algebra Applications,seg_229,"(b) any covariance matrix σx ≡ |[cov[xi,xj ]]| satisfies σx ≥ 0."
3371,0,[], Linear Algebra Applications,seg_229,exercise 3.2 prove proposition 3.1.
3372,0,[], Linear Algebra Applications,seg_229,"discussion 3.2 (versions of σ− and σ−1/2) let x =∼ (μ,σ). according to the principal axes theorem, we can make the decomposition (for any orthogonal matrix δ whatsoever)"
3373,0,[], Linear Algebra Applications,seg_229,where d1/2 has the numbers di
3374,0,['n'], Linear Algebra Applications,seg_229,/2 on its diagonal and where a is n × k. the presence of δ′δ (which equals i) shows that this decomposition is not unique. continuing on gives
3375,1,['results'], Linear Algebra Applications,seg_229,these last two results are in keeping with the definition of generalized inverses.
3376,0,[], Linear Algebra Applications,seg_229,recall that the generalized inverse b− of the matrix b is defined to be any matrix b− that satisfies bb−b = b. a generalized inverse always exists. it has the following interpretation. fix the matrix b and the vector c. then
3377,0,[], Linear Algebra Applications,seg_229,"(it is clear that such a solution does always exist, for a fixed c.) suppose such a b− exists, in general; which we accept, and will use freely. then replace c in (13) by each column of b, and see that such a b− must necessarily satisfy bb−b = b."
3378,1,"['covariance', 'results']", Linear Algebra Applications,seg_229,theorem 3.2 (properties of covariance matrices) (a) the following results are equivalent for real matrices:
3379,1,"['covariance', 'covariance matrix']", Linear Algebra Applications,seg_229,(14) σ is the covariance matrix of some rv y.
3380,1,['symmetric'], Linear Algebra Applications,seg_229,(15) σ is symmetric and nonnegative definite.
3381,1,"['covariance', 'symmetric', 'independent', 'covariance matrix']", Linear Algebra Applications,seg_229,"proof. now, (14) implies (15): σ is symmetric, since eyiyj = eyjyi. also, a′σa = var[a′y ] ≥ 0 for all vectors a, so that σ ≥ 0. also, (15) implies (16): just recall (17). also, (16) implies (14): let x ≡ (x1, . . . , xn)′, where x1, . . . , xn are independent n(0, 1). let y ≡ ax. then y has covariance matrix σ = aa′ by (6)."
3382,1,['symmetric'], Linear Algebra Applications,seg_229,"exercise 3.4 let x ∼= (θ,σ) and let b be symmetric. (a) e{(x − b)′b(x − b)} = tr(bς) + (θ − b)′b(θ − b) . (b) if σ = σ2i, then tr(bς) = σ2tr(b) = σ2∑i"
3383,1,['symmetric'], Linear Algebra Applications,seg_229,"exercise 3.5 for symmetric a there exists an upper (or lower) triangular matrix h for which a = hh ′. if a > 0 (or a ≥ 0), we may suppose that all hii > 0 (or that all hii ≥ 0)."
3384,1,"['linear', 'linear predictor', 'correlation', 'random', 'predictor']", Linear Algebra Applications,seg_229,discussion 3.3 (best linear predictor and multiple correlation) consider the partitioned random vector
3385,1,"['linear', 'predictor', 'linear predictor']", Linear Algebra Applications,seg_229,the best linear predictor of y0 based on y is
3386,1,[], Linear Algebra Applications,seg_229,"in parallel with this,"
3387,1,"['coefficient', 'correlation coefficient', 'correlation']", Linear Algebra Applications,seg_229,"the maximized value of the correlation (that is, the multiple correlation coefficient) is given by"
3388,1,"['predictor', 'linear', 'variance', 'linear predictor']", Linear Algebra Applications,seg_229,and the variance of the best linear predictor is also easily seen to equal
3389,0,[], Linear Algebra Applications,seg_229,"[proof. the first holds, since"
3390,0,[], Linear Algebra Applications,seg_229,"the second holds, since"
3391,1,[], Linear Algebra Applications,seg_229,with equality only at β = cς−1σ0 (as follows from application of cauchy–schwarz).]
3392,1,"['model', 'linear', 'regression model', 'linear predictor', 'conditional', 'regression', 'distribution', 'linear regression', 'predictor', 'conditional distribution', 'linear regression model']", Linear Algebra Applications,seg_229,simple linear regression model we now want the best linear predictor of y based on x. the conditional distribution of y given that x = x is given by
3393,1,['moments'], Linear Algebra Applications,seg_229,expressing the moments in terms of
3394,1,"['simple linear regression', 'linear', 'model', 'regression model', 'observations', 'regression', 'linear regression', 'linear regression model']", Linear Algebra Applications,seg_229,this leads directly to the simple linear regression model that conditionally on x = x the observations yi satisfy
3395,1,"['moments', 'conditional']", Linear Algebra Applications,seg_229,discussion 3.4 (conditional moments and projections) suppose that
3396,1,"['conditional', 'distribution', 'moments', 'conditional distribution']", Linear Algebra Applications,seg_229,then the moments of the conditional distribution of y (1) given that y (2) = y(2) are summarized in
3397,0,[], Linear Algebra Applications,seg_229,"to see this, just define"
3398,0,[], Linear Algebra Applications,seg_229,it is a minor calculation that
3399,0,[], Linear Algebra Applications,seg_229,the exercises will show that
3400,1,[], Linear Algebra Applications,seg_229,"since y (1) = μ(1) + z(1) + σ12σ2−21z(2) with y (2) − μ(2) = z(2), conditionally"
3401,0,[], Linear Algebra Applications,seg_229,as required.] [see exercise 3.7 below for (31).]
3402,0,[], Linear Algebra Applications,seg_229,discussion 3.5 (partitioned matrices) let
3403,0,[], Linear Algebra Applications,seg_229,when the inverse exists. we agree that a11 is k × k.
3404,0,[], Linear Algebra Applications,seg_229,for appropriate choices.]
3405,1,['symmetric'], Linear Algebra Applications,seg_229,exercise 3.8 (a) show that for a symmetric a having |a11| = 0 and |a22| = 0:
3406,0,[], Linear Algebra Applications,seg_229,[hint. start multiplying the partitioned form of a a−1 = i.] (b) obtain analogous formulas from a−1a = i. (c) show that
3407,1,['symmetric'], Linear Algebra Applications,seg_229,"exercise 3.9 show that for symmetric a,"
3408,1,['symmetric'], Linear Algebra Applications,seg_229,"discussion 3.6 (simultaneous decomposition) for a real symmetric matrix a that is nonnegative definite (that is, a ≥ 0) we wrote"
3409,0,[], Linear Algebra Applications,seg_229,"all have the same solutions d11, . . . , drr, 0, and thus d11, . . . , drr are indeed the nonzero eigenvalues of a. moreover, (37) gives"
3410,0,[], Linear Algebra Applications,seg_229,"so that γ1, . . . , γr are the corresponding eigenvectors. (b) suppose a > 0 and b ≥ 0. then"
3411,0,[], Linear Algebra Applications,seg_229,this last formula is called the simultaneous decomposition of a and b.
3412,1,[], Linear Algebra Applications,seg_229,"discussion 3.7 (a) (cauchy–schwarz) for all vectors x, y:"
3413,1,['symmetric'], Linear Algebra Applications,seg_229,with equality (for y = 0) if and only if x = cy for some constant c. (b) for any real symmetric matrix a > 0
3414,1,['case'], Linear Algebra Applications,seg_229,"here, (a′ca) = (a′b)2 is an important special case (already solved via (46)). (d) let a > 0, let bk×n have rank (b) = k, and let bk×1 = 0. then"
3415,0,[], Linear Algebra Applications,seg_229,exercise 3.10 prove (42)–(47) (the equality in (42) needs some attention). [the harder (48) is proven below.]
3416,0,[], Linear Algebra Applications,seg_229,"yielding a bound not depending on a, which proves (48)."
3417,1,"['linear', 'model', 'linear model']", Linear Algebra Applications,seg_229,discussion 3.8 (general linear model) consider the general linear model
3418,1,['parameters'], Linear Algebra Applications,seg_229,"where xn×p is a matrix of known constants, where βp×1 is a vector of unknown parameters, and where the rv n×1 =∼ (0, σ2i) with σ2 unknown. recall that"
3419,1,"['estimator', 'least squares']", Linear Algebra Applications,seg_229,"is a vector space (of rank r, say). noting that θ = xβ ∈ r[x], the least squares estimator"
3420,0,[], Linear Algebra Applications,seg_229,this minimization clearly occurs when θ̂ is the projection of y onto r[x]; so
3421,0,[], Linear Algebra Applications,seg_229,"we note that β̂ need not be unique, since"
3422,1,"['normal equations', 'normal']", Linear Algebra Applications,seg_229,"since (y − θ̂) is ⊥ to r[x], it must be that θ̂ and β̂ satisfies the normal equations"
3423,1,"['normal equations', 'normal']", Linear Algebra Applications,seg_229,"conversely, suppose β̂ satisfies the normal equations. then xβ̂ ∈ r[x] with x ′(y −xβ̂) = 0,"
3424,1,['residuals'], Linear Algebra Applications,seg_229,(58) ≡ (the residuals) ≡ (y − ŷ ) = [i − pω]y = pω⊥y.
3425,1,"['sum of squares', 'residual sum of squares', 'residual']", Linear Algebra Applications,seg_229,call rss ≡ ‖ ‖2 = ′ = ‖y − ŷ ‖2 = y ′[i − pω]y the residual sum of squares.
3426,1,"['residuals', 'expectation']", Linear Algebra Applications,seg_229,"x(x ′x)−x ′y ; so pω = x(x ′x)−x ′. also, e(θ̂) = e(pωy ) = pωey = pωθ = θ. finally, the residuals satisfy ‖y −ŷ ‖2 = (y −ŷ )′(y −ŷ ) = y ′(i−pω)′(i−pω)y = y ′(i−pω)y , with expectation (given by exercise 3.4) σ2tr(i−pω)+θ′(i−pω)θ = σ2tr(i−pω)+θ′0 = (n−r)σ2."
3427,1,"['normal equations', 'normal']", Linear Algebra Applications,seg_229,"is the unique solution of the normal equations (β̂ is now called the lse of β), and"
3428,1,"['unbiased estimator', 'estimator', 'unbiased']", Linear Algebra Applications,seg_229,"(61) eβ̂ = (x ′x)−1x ′xβ = β, so that β̂ is an unbiased estimator of β, and"
3429,1,['unbiased'], Linear Algebra Applications,seg_229,"(63) es2 = σ2 for s2 ≡ ‖y − ŷ ‖2/(n − p), so that s2 is unbiased for σ2."
3430,1,['case'], Linear Algebra Applications,seg_229,"we thus say that β is identifiable (and estimable) in this full rank case when rank(x) = p (that is, when x is non-singular, or |x| = 0)."
3431,1,['linear'], Linear Algebra Applications,seg_229,"exercise 3.11 (gauss–markov) let y = xβ+ = θ+ (as in (49)) with the rv =∼ (0, σ2i) and with rank(x) = r. consider some c′θ(= c′xβ). show that among the class of all linear"
3432,1,"['minimum variance', 'variance', 'estimator', 'estimators']", Linear Algebra Applications,seg_229,"unbiased estimators of c′θ, the estimator c′θ̂ is the unique one having minimum variance (so, it is best). determine its variance."
3433,1,"['distribution', 'normality']", Linear Algebra Applications,seg_229,"exercise 3.12 (distribution theory under normality) (a) let y = xβ + = θ + (as in equation (49)) with =∼ n(0, σ2i), and with r ≡ rank(x). show that"
3434,0,[], Linear Algebra Applications,seg_229,"when r ≡ rank(x) = p (so that β̂ is unique, identifiable, and estimable), show that"
3435,1,['independent'], Linear Algebra Applications,seg_229,"(69) β̂ is independent of = (y − ŷ ) = (y − θ̂), and hence of ‖y − θ̂‖2 also."
3436,1,"['linear', 'model', 'linear model']", Linear Algebra Applications,seg_229,"(b) suppose instead that =∼ (0, v ), with rank(v) = n. define z ≡ v −1/2y, and show that this z satisfies the linear model equation z = x∗β + ∗ where x∗ ≡ v −1/2x and ∗ =∼ (0, σ2i). so, analogs of all the formulas in (a) are trivial."
3437,1,"['linear', 'model', 'linear model', 'covariance matrix', 'covariance']", Linear Algebra Applications,seg_229,exercise 3.13 (alternative minimization in the general linear model) in the context of the model y = xβ + = θ + (as in (49)) we now let θ̃ denote that θ ∈ r[x] that minimizes (for some positive definite covariance matrix m)
3438,1,"['normal equations', 'normal']", Linear Algebra Applications,seg_229,"(instead of minimizing ‖ ‖2 (as in (49))). show that (y − θ̃) ⊥m r[x], and so this resulting weighted lse θ̃ = xβ̃ must satisfy the weighted normal equations"
3439,1,"['normal equations', 'normal']", Linear Algebra Applications,seg_229,"summary: xβ̃ = θ̃ if and only if β̃ satisfies the weighted normal equations. also,"
3440,1,"['normal equations', 'normal']", Linear Algebra Applications,seg_229,(72) β̃ = (x ′mx)−(x ′my ) does satisfy the weighted normal equations and
3441,1,"['linear', 'minimum variance', 'estimators', 'mean', 'variance', 'unbiased']", Linear Algebra Applications,seg_229,"exercise 3.14 (minimum variance unbiased linear estimators) (a) let x1, . . . , xn be uncorrelated with common mean μ and common finite variance σ2. all linear estimators t ≡∑1"
3442,1,"['linear', 'minimum variance', 'unbiased estimators', 'estimators', 'mean', 'variance', 'estimator', 'unbiased', 'variances']", Linear Algebra Applications,seg_229,"naixi having∑1 nai = 1 are unbiased estimators of μ (that is, et = μ). show that the choice with all ai = 1/n has minimum variance within this class of linear unbiased estimators. (b) determine the minimum variance unbiased linear estimator of the common mean μ when the variances are σ2/c1, . . . , σn"
3443,0,[], Linear Algebra Applications,seg_229,"2/cn, with the ck being known constants."
3444,1,"['jointly', 'normal']", The Multivariate Normal Distribution,seg_231,"definition 4.1 (jointly normal) call y = (y1, . . . , yn)′ jointly normal with 0 means if there exist iid n(0, 1) rvs x1, . . . , xk and an n × k matrix a of known constants for which"
3445,1,"['covariance', 'covariance matrix', 'random']", The Multivariate Normal Distribution,seg_231,"y = ax. [we again write y in this section, rather than y , when the context seems clear.] note that the n × n covariance matrix σy ≡ σ of the random vector y is"
3446,1,"['covariance', 'covariance matrix']", The Multivariate Normal Distribution,seg_231,"the covariance matrix of x is the k × k identity matrix ik. we will write x ∼ n(0, ik), and"
3447,1,"['independent', 'determinant', 'covariance matrix', 'covariance', 'normal', 'mean']", The Multivariate Normal Distribution,seg_231,"= ∼ ∼ ∼ we will write y = n(0,σ). then write y = n(μ,σ) if y − μ = n(0,σ). call y multivariate normal with mean vector μ and covariances matrix σ, or just normal with mean vector μ and covariance matrix σ. call y nondegenerate when |σ| = 0 (that is, the determinant of σ is not equal to 0). say that y1, . . . , yn are linearly independent if (rank σ) = n. of course, this means that"
3448,1,['symmetric'], The Multivariate Normal Distribution,seg_231,"now, σ is symmetric. also aσa′ = var[ay ] ≥ 0 for all vectors a. when aσa′ ≥ 0 for all vectors a, the symmetric matrix σ is called nonnegative definite, and one writes σ ≥ 0."
3449,1,['densities'], The Multivariate Normal Distribution,seg_231,"∼ theorem 4.1 (densities) if y = n(0,σ) is nondegenerate, then y has density (with respect to lebesgue measure on rn) given by"
3450,1,"['distribution', 'normal', 'normal distribution']", The Multivariate Normal Distribution,seg_231,[note that each possible normal distribution is completely determined by μ and σ.]
3451,0,[], The Multivariate Normal Distribution,seg_231,this is the required statement.
3452,1,"['functions', 'characteristic functions', 'representations', 'random']", The Multivariate Normal Distribution,seg_231,"theorem 4.2 (characteristic functions and representations) (a) if we are given a random vector y = an×kxk×1 where x =∼ n(0, ik), we have"
3453,1,"['function', 'characteristic function']", The Multivariate Normal Distribution,seg_231,"with σ ≡ aa′ and rank(σ) = rank(a) . (b) if y has characteristic function φy (t) ≡ eeity = exp(−t′σt/2) with σ ≥ 0 of rank k, then"
3454,1,['independent'], The Multivariate Normal Distribution,seg_231,(thus the number of independent rvs xi’s needed is equal to the rank of a.)
3455,1,"['function', 'density function', 'characteristic function']", The Multivariate Normal Distribution,seg_231,"proof. our proof will use the fact that the characteristic function φy of any rv y is unique (this is shown in theorem 9.4.1.) [when a density function does not exist, one can use this characteristic function for many of the same purposes.] we observe that"
3456,1,"['function', 'characteristic function', 'normal']", The Multivariate Normal Distribution,seg_231,"even when a multivariate normal rvy does not have a density, the characteristic function can often be manipulated to establish a desired result."
3457,1,"['linear', 'linear combinations', 'independence', 'combinations']", The Multivariate Normal Distribution,seg_231,"theorem 4.3 (marginals, independence, and linear combinations) suppose that"
3458,1,"['marginal', 'covariance', 'covariance matrix']", The Multivariate Normal Distribution,seg_231,"(i) the marginal covariance matrix of (y1, . . . , yk)′ is the k × k matrix σ11, and"
3459,1,"['linear', 'independent', 'linear combinations', 'covariance', 'normal', 'jointly', 'combinations']", The Multivariate Normal Distribution,seg_231,"(ii) if σ12 = 0, then (y1, . . . , yk)′ and (yk+1, . . . , yn)′ are independent. (iii) if (y1, y2) is a jointly normal rv, then y1 and y2 are independent if and only if they have the zero covariance cov[y1, y2] = 0. (iv) linear combinations of multivariate normals are multivariate normal."
3460,1,['factor'], The Multivariate Normal Distribution,seg_231,proof. (i) use the first k coordinates of the representation y = ax. (ii) use the fact that one can factor
3461,1,"['conditional', 'conditional distributions', 'distributions']", The Multivariate Normal Distribution,seg_231,theorem 4.4 (conditional distributions) if
3462,0,[], The Multivariate Normal Distribution,seg_231,proof. the vector
3463,1,"['linear', 'linear combination', 'combination', 'normal', 'variances']", The Multivariate Normal Distribution,seg_231,"is just a linear combination of the yi’s, and so it is normal. we need only verify the means and variances. but we did this in discussion a.3.4."
3464,1,"['symmetric', 'distribution', 'jointly', 'joint', 'normal']", The Multivariate Normal Distribution,seg_231,"exercise 4.1 show that (y1, y2) can have normal marginals without being jointly normal. [hint. consider starting with a joint n(0, i) density on r2 and move mass in a symmetric fashion to make the joint distribution nonnormal, but still keeping the marginals normal.]"
3465,1,['symmetric'], The Multivariate Normal Distribution,seg_231,"exercise 4.2 let y n×1 =∼ n(0, i), and suppose that a is symmetric and of rank r. then y ′ay =∼ χr2 if and only if a is a projection matrix (that is, a2 = a)."
3466,1,"['symmetric', 'independent', 'distributions']", The Multivariate Normal Distribution,seg_231,"exercise 4.3 let y n×1 =∼ n(0, i). suppose that a and b are symmetric and both y ′ay and y ′by have chisquare distributions. show that y ′ay and y ′by are independent if and"
3467,0,['n'], The Multivariate Normal Distribution,seg_231,"exercise 4.4 suppose a and b are n × n projection matrices with ranks ra and rb , and suppose ab = 0 and i − a − b ≥ 0. then: (a) i − a is a projection matrix of rank n − ra."
3468,1,['symmetric'], The Multivariate Normal Distribution,seg_231,"exercise 4.5 suppose y n×1 =∼ n(0,σ), and let a be an arbitrary symmetric matrix of rank r. show that y ′ay =∼ χr2 if and only if aσa = a."
3469,0,[], The Multivariate Normal Distribution,seg_231,"the following result is theorem 14.1.3, but we also list it here for convenient referral."
3470,1,['random'], The Multivariate Normal Distribution,seg_231,"theorem 4.5 suppose that the random vectors x1, . . . , xn are iid (μ,σ). then"
3471,1,['processes'], The Multivariate Normal Distribution,seg_231,normal processes
3472,1,"['covariance matrix', 'consistency', 'covariance', 'normal', 'mean', 'function', 'process', 'distributions']", The Multivariate Normal Distribution,seg_231,"to specify a normal process, we must specify consistent distributions (in the sense of kolmogorov’s consistency theorem). but μ and σ completely specify n(μ,σ), while the marginals of n(μ,σ) are n(μ(1),σ11). thus a normal process exists, provided only that the mean value function μ(·) on i and the covariance function cov(·, ·) on i × i are well-defined and are such that cov(·, ·) is nonnegative definite (meaning that every n-dimensional covariance matrix formed from it is nonnegative definite)."
3473,1,"['normal', 'process']", The Multivariate Normal Distribution,seg_231,we call {s(t) : 0 ≤ t < ∞} a brownian motion if s is a normal process having
3474,1,"['condition', 'consistency', 'covariance', 'function', 'process']", The Multivariate Normal Distribution,seg_231,"since this covariance function is nonnegative definite, a version of the process s exists on (r[0,∞),b[0,∞)) by the kolmogorov consistency condition. then"
3475,1,"['normal', 'process']", The Multivariate Normal Distribution,seg_231,"it is a normal process on (r[0,1],b[0,1]) for which"
3476,1,['results'], General Topology ,seg_235,only the definitions and the statements of the major results are presented. (only a small amount will be referenced; so see this as a handy summary.)
3477,1,['set'], General Topology ,seg_235,"definition 1.1 (topology, open set, neighborhood, and boundary) (a) a topological space (or a space) is a pair (m,u) where m is a set and u is a family of subsets that satisfies"
3478,1,"['sets', 'set']", General Topology ,seg_235,"u is called a topology, and the sets in u are called open sets. the complements of all open sets are called closed sets. an open set u containing a point x ∈ m is called a neighborhood of x. (b) the interior ao of a is int (a) ≡ ao ≡ ∪{ u : u ⊂ a with u open}. the closure ā of a is defined by c1(a) ≡ ā ≡ ∩{ f : f ⊃ a with f closed}. the boundary ∂a of a is ∂a ≡ ā\ao. (c) call x an accumulation point of a if a\{x} contains x."
3479,1,"['intervals', 'sets']", General Topology ,seg_235,"definition 1.2 (bases and subbases) (a) call b a base for the topology u if b ⊂ u and if each nonvoid u ∈ u satisfies u = ∪v for some v ⊂ b. we then say that b generates u . (all the open intervals (a, b) generate the open subsets of r.) (b) a family of sets s is a subbase for u if ∪ s = m and the family of all finite intersections of members of s is a base for u . (all (a, b) × r and r × (a, b) serve as a subbase for all the open sets in r2.)"
3480,1,['function'], General Topology ,seg_235,"definition 1.3 (sequences) a sequence is a function x whose domain is the natural numbers {1, 2, . . . } and we let xn ≡ x(n). suppose x takes values xn in some space (m,u). then the sequence is said to converge to some x0 ∈ m if for each neighborhood u of x0 there exists a natural number nu such that xn ∈ u for all n ≥ nu . we write xn → x0 or limn xn = x0 to denote that the sequence xn converges to x0."
3481,1,"['interval', 'partitions', 'set', 'function']", General Topology ,seg_235,"definition 1.4 (nets) a partial ordering is a relationship r that is reflexive (xrx), antisymmetric (xry and yrx implies x = y), and transitive (xry and yrz implies xrz). a directed set is a set i under a relationship for which, for any i, j ∈ i there is a k ∈ i satisfying i k and j k. a net is any function whose domain is such a directed set i (with xi ≡ x(i)) taking values in some topological space (m,u). the net {xi}i∈i is said to converge to a value x0 in m if for each neighborhood u of x0 there is an iu ∈ i for which xi ∈ u for all i iu . [the set of all neighborhoods of a point constitutes a directed set. so does the set of all partitions of an interval.]"
3482,1,"['sets', 'set', 'intersection']", General Topology ,seg_235,"definition 1.5 (compactness) (a) a collection a of subsets of m covers the subset b of m if b ⊂ ∪a; and a is called an open cover of b if all sets a in a are open. a set a is called compact if every open cover a of a has a finite subcover v (that is, a ⊂ ∪v, where v ⊂ a contains only finitely many subsets of a). [this is called the heine-borel property.] (b) a ⊂ m is compact (relatively compact) if it (its closure) is compact in the relative topology. (c) the collection a has the finite intersection property if ∩f = ∅ for each finite subcollection f of subsets of a."
3483,1,"['sets', 'intersection']", General Topology ,seg_235,"exercise 1.4 (a) a closed subset of a compact space (m,u) is compact. (b) let b be a base for (m,u), and let a ⊂ m . then a is compact if and only if every cover of a by members of the base b admits a finite subcover. (c) (alexander’s lemma) let s be a subbase for the topological space (m,u). then m is a compact space if and only if each cover of m by members of the subbase s admits a finite subcover. (d) a space (m,u) is compact if and only if each family of closed sets with the finite intersection property has a nonempty intersection. (e) (m,u) is compact if and only if every net on m has at least one accumulation point in m."
3484,1,"['sets', 'normal', 'disjoint']", General Topology ,seg_235,"definition 1.6 (separation) (a) (m,u) is called a hausdorff space if for each x = y in m there are disjoint open sets u and v such that x ∈ u and y ∈ v. (b) (m,u) is called a normal space if for each pair of disjoint closed sets a and b there are disjoint open sets u and v having a ⊂ u and b ⊂ v. (c) (m,u) is called separable if it contains a countable subset d that is dense in m ; here d is called dense if d̄ = m. (d) (m,u) is called first countable if for each x ∈ m there is a family nx of neighborhoods nx of x such that for each neighborhood u of x we have x ∈ nx ⊂ u for some neighborhood nx in nx. (e) (m,u) is called second countable or perfectly separable if the topology u admits a countable base b."
3485,0,[], General Topology ,seg_235,"exercise 1.5 (a) a sequence with values in a hausdorff space converges to at most one point. (b) if all values xn are in a and xn → x0, then x0 ∈ ā."
3486,0,[], General Topology ,seg_235,"exercise 1.6 (a) a perfectly separable (m,u) is separable. (b) (lindelöf) an open cover of a perfectly separable space necessarily admits a countable subcover."
3487,1,['normal'], General Topology ,seg_235,"exercise 1.7 (a) a compact subset of a hausdorff space (m,u) is closed. (b) a compact hausdorff space is normal. (c) a space (m,u) is normal if and only if for each closed c and open u with c ⊂ u there exists an open v with c ⊂ v ⊂ v̄ ⊂ u. (d) a subset of a hausdorff space (m,u) is hausdorff in its relative topology. (e) a subset of a normal space (m,u) is normal in its relative topology."
3488,1,"['function', 'functions', 'continuous']", General Topology ,seg_235,"definition 1.7 (continuous functions and homeomorphisms) (a) let f : m → n for topological spaces (m,u) and (n,v). call the function (or mapping) f continuous at x if for each neighborhood v of f(x) there is a neighborhood u of x for which f(u) ⊂ v. (b) a 1-to-1 function that maps onto the image space is called a bijection. (c) a 1-to-1 bicontinuous function f (that is, both f and f−1 are continuous and onto) is called a homeomorphism."
3489,1,"['continuous', 'sets', 'function']", General Topology ,seg_235,"exercise 1.8 (conditions for continuity) the following are equivalent: (a) f is a continuous function. (b) inverse images of all open sets are open. (c) inverse images of all closed sets are closed. (d) inverse images of all subbasic open sets are open. (e) for each x ∈ m and each net {xi}i∈i converging to this x, {f(xi)}i∈i → f(x) converges. (f) f(ā) ⊂ f(a) for all subsets a of m. (g) f−1(b) ⊂ f−1(b−) for all subsets b of n. (h) f−1(bo) ⊂ [f−1(b)]o for all subsets b of n."
3490,1,"['functions', 'continuous']", General Topology ,seg_235,"exercise 1.9 (a) (dini) if fn are continuous real-valued functions on a compact (m,u) for which fn(x) ↘ f(x) for each x ∈ m , then fn converges uniformly to f."
3491,1,"['functions', 'limit', 'continuous']", General Topology ,seg_235,(b) a uniform limit of bounded and continuous functions fn is also bounded and continuous.
3492,1,"['functions', 'continuous']", General Topology ,seg_235,"exercise 1.10 (continuity and compactness) (a) if f : m → n is continuous and m is compact, then f(m) is compact. [thus [0, 1] cannot be mapped continuously onto r.] (b) the composition of continuous functions is continuous. (c) if f : m → n is 1-to-1 and continuous (that is, it is a bijection) where m is compact and n is hausdorff, then f is a homeomorphism."
3493,1,"['disjoint', 'continuous', 'sets', 'normal', 'function']", General Topology ,seg_235,"exercise 1.11 (urysohn’s lemma) (m,u) is normal if and only if for each pair of disjoint closed sets a and b there is a continuous function f : m → [0, 1] having f(x) = 0 for all x ∈ a and f(x) = 1 for all x ∈ b."
3494,1,"['continuous', 'normal']", General Topology ,seg_235,"exercise 1.12 (tietze) let (m,u) be a normal space, and let a be a closed subset. if f : a → [−1, 1] is continuous, then f admits a continuous extension from m to [–1, 1]."
3495,1,"['function', 'set']", General Topology ,seg_235,"definition 1.8 (product topology) let p{ma} ≡∏a∈ama denote the product set associated with the individual topological spaces (ma,ua) over the index set a; this product set is defined to be { x : x is a function on a with xa ≡ x(a) ∈ ma}. the a0th projection function πa0 : p{ma} → ma0 is defined by πa0(x) = xa0 . let s ≡ {πa−1(u) : u ∈ ua and a ∈ a}. then s is the subbase for a topology called the product topology."
3496,1,"['functions', 'continuous', 'sets', 'function']", General Topology ,seg_235,"exercise 1.13 (a) projection functions are continuous in the product topology. (recall definition 5.4.1 for the projection maps πt1,...,tk(·).) (b) (tychonoff) p{ma} with the product topology is compact if and only if each (ma,ua) is compact. (c) the product of countably many separable topological spaces is a separable space in the product topology. (d) p{ba} = p{b̄a} for all choices ba ⊂ ma, and thus the product of closed sets is closed in the product topology. (e) (p{ba})o = p{bao} for all choices ba ⊂ ma, provided that a is finite. (f) if each (ma,ua) is hausdorff, then the product topology is hausdorff. (g) let (m,u) be a topological space and suppose fa : m → ma is a continuous function for each a ∈ a. define f : m → p{ma} by πaf(x) = fa(x) for each x ∈ m . then f is continuous in the product topology."
3497,0,[], General Topology ,seg_235,"definition 1.9 (locally compact) let (m,u) be a topological space. let x ∈ m. the space is locally compact at x if x has a neighborhood with compact closure (that is, there exists a v ∈ u with x ∈ v for which v̄ is compact). the space is locally compact if it is locally compact at each point. [rn and (0, 1) are locally compact.]"
3498,1,"['functions', 'continuous', 'intersection', 'sets', 'set', 'union', 'function']", General Topology ,seg_235,"exercise 1.14 (locally compact hausdorff spaces are nice) let m be a locally compact hausdorff space. let c ⊂ u ⊂ m , with c compact and u open. (a) then there is an open set v whose closure is compact having c ⊂ v ⊂ v̄ ⊂ u. (b) (urysohn’s lemma) there is a continuous function f : m → [0, 1] for which f(x) equals 1 for x ∈ c and 0 for x ∈ u c. (c) there is an open set v and a compact set d where c ⊂ v ⊂ d ⊂ u with v a countable union of compact sets and d a countable intersection of open sets. sets of the type v form a base for the topology. (d) let cc denote the set of all continuous functions f on m that vanish outside compact sets (that is, f(x) = 0 for all x ∈/ (some compact kf )). let co be the set of all continuous functions f on m that vanish at infinity (that is, |f(x)| < for all x ∈/ (some compact kf,∈)), for each > 0). show that the closure (in the sup norm metric on m) of cc equals co."
3499,1,['sets'], General Topology ,seg_235,"exercise 1.15 (one-point compactification) the one-point compactification of any locally compact hausdorff space (m,u) makes it a compact hausdorff space. that is, let m̄ ≡ m ∪ {p}, and designate the open sets ū to be all open sets u plus all sets {p} ∪ cc with c compact and {p} the one new point. then (m̄, ū) is a compact hausdorff space."
3500,1,"['sets', 'union']", General Topology ,seg_235,"definition 1.10 (category) a subset a of (m,u) is nowhere dense if (ā)o = ∅. if a can be written as a countable union of nowhere dense sets, then it is said to be of the first category; otherwise, it is said to be of the second category."
3501,1,"['functions', 'continuous']", General Topology ,seg_235,"exercise 1.16 (baire category theorem) (a) a locally compact hausdorff space is second category. (b) let c denote the class on all continuous functions on [0, 1], and let c(1) denote the subclass of those that have continuous derivatives on [0, 1]. show that (c, ‖·‖ is a complete and separable metric space of category two, and that c(1) is a subset of category one. that is, “most” continuous functions on [0, 1] are not differentiable on [0, 1]."
3502,1,"['sets', 'set', 'normal']", General Topology ,seg_235,"definition 1.11 (topologically equivalent) if there exists a homeomorphism between the two topological spaces (m,u) and (n,v) (that is, there exists a 1-to-1 bicontinuous mapping between them), then there exist 1-to-1 correspondences both between points and between open sets. we then say that these two topological spaces are topologically equivalent. thus any properties that are defined solely in terms of the open sets either hold or fail simultaneously in the two spaces. such concepts are called topological concepts. [these include closed set, closure, interior, boundary, accumulation point, compactness, separability, perfect separability, local compactness, category, continuity, and being either hausdorff or normal.]"
3503,1,[], General Topology ,seg_235,"we are about to introduce metric spaces. we will soon see that two different metrics can induce the same topology. thus distance in a metric space is not a topological property. a homeomorphism f that leaves distance unchanged (thus, d2(f(x), f(y)) = d1(x, y)) is called an isometry."
3504,1,['set'], Metric Spaces ,seg_237,"definition 2.1 (metric space) (a) a semimetric space (m,d) consists of a set m together with a semimetric d that satisfies"
3505,1,['set'], Metric Spaces ,seg_237,"for all x, y, z ∈ m . if d(x, y) = 0 implies x = y, then d is called a metric and (m,d) is called a metric space. [if all d(x, y) ≤ (some finite c), then d is called a bounded metric.] call the set"
3506,1,['sphere'], Metric Spaces ,seg_237,"(2) sr(x) ≡ {y ∈ m : d(x, y) < r} an open sphere or ball about x of radius r."
3507,1,[], Metric Spaces ,seg_237,"(b) the collection of all such spheres is the base for a topology, called the metric topology, and is denoted by ud. the abbreviation (m,d,ud) refers to the metric space with its metric topology, and we call it a topological metric space. (c) let (m,u) be a topological space. suppose there exists a metric d on m whose open balls form a base for the topology u . then the topology u is said to be metrizable."
3508,1,['set'], Metric Spaces ,seg_237,"exercise 2.1 (equivalent metrics) (a) two metrics d and ρ on a set m lead to equivalent topologies if and only if for each x ∈ m and each 1, 2 > 0 there are δ1, δ2 > 0 such that for all y ∈ m,"
3509,1,"['functions', 'continuous', 'sets']", Metric Spaces ,seg_237,"[if only the first holds, then ud ⊂ uρ, meaning that the ρ topology is finer.] such equivalent metrics lead to the same continuous functions. if both sets of δ values do not depend on x, then the metrics are called uniformly equivalent metrics and lead to the same uniformly continuous functions. (b) define four equivalent metrics on rn. (c) ρ ≡ d/(1 + d) defines a metric equivalent to the d metric on (m,d); and ρ is a bounded metric."
3510,1,"['normal', 'sphere']", Metric Spaces ,seg_237,"exercise 2.2 (a) the closed sphere {y : d(x, y) ≤ r} equals sr(x) with respect to the metric topology ud. (b) any metric space is homeomorphic to a metric space with a bounded metric. (c) a metric space is perfectly separable if and only if it is separable. (d) a compact metric space is separable and perfectly separable. (e) any metric space is hausdorff, normal, and first countable. (f) the product of countably many metrizable spaces is metrizable in the product topology, with the bounded metric d(x, y) ≡ ∑∞"
3511,1,['union'], Metric Spaces ,seg_237,"1 ψ(dn(xn, yn))/2n, where we define ψ(t) ≡ t/(1 + t). if each is separable, then so is the product. (g) the relative topology on a subset a of the metric space (m,d,ud) is the same as the metric topology for (a, d) . (h) every subspace of a separable metric space is separable in its relative topology. (i) a separable metric space is locally compact if and only if it is the union of u1 ⊂ u2 ⊂ · · · with each ūn a compact subset of un+1."
3512,1,['cauchy'], Metric Spaces ,seg_237,"definition 2.2 (complete) (a) we will call a sequence x in a metric space (m,d) a cauchy sequence if d(xm, xn) → 0 as m ∧ n → ∞."
3513,1,['cauchy'], Metric Spaces ,seg_237,"(b) the topological metric space (m,d,ud) is called complete when every cauchy sequence x having values xn in m converges to a member of m."
3514,1,[], Metric Spaces ,seg_237,"exercise 2.3 (a) a sequence x taking values in a metric space (m,d) converges to a member xo of m if and only if d(xn, xo) → 0. (b) every compact metric space is complete, separable, and perfectly separable. (c) every closed subspace of a complete metric space is complete. (d) a metric space in which every sequence has a convergent subsequence is perfectly separable."
3515,1,[], Metric Spaces ,seg_237,"definition 2.3 (a) a metric space (m,d) is totally bounded if for every > 0 there is a finite subset f of m such that for each x ∈ m there exists a y ∈ f sufficiently close to x to have d(x, y) < . (b) let diam(a) ≡ sup{d(x, y) : x, y ∈ a} denote the diameter of a. the metric space is called bounded if diam(m) is finite."
3516,1,[], Metric Spaces ,seg_237,"exercise 2.4 (compactness) the following conditions are equivalent to the compactness of a subset k of the metric space (m,d,ud)."
3517,0,[], Metric Spaces ,seg_237,"(heine–borel property = compactness,by definition): (a) every open cover ofk has a finite subcover."
3518,1,['limit'], Metric Spaces ,seg_237,(bolzano–weierstrass property): (b) every infinite subset ofk has a limit point ink.
3519,0,[], Metric Spaces ,seg_237,(sequential compactness (or relative compactness)): (c) every sequence in k has a subsequence converging to a point ink.
3520,0,[], Metric Spaces ,seg_237,(countable compactness): (d) every countable open cover ofk has a finite subcover.
3521,0,['e'], Metric Spaces ,seg_237,(e) k is totally bounded and complete.
3522,1,[], Metric Spaces ,seg_237,"k is totally bounded, and (f) any ↘ sequence of closed spheres sn ink whose diameters → 0 has ∩1∞sn = {some singleton x} ∈ k."
3523,0,[], Metric Spaces ,seg_237,exercise 2.5 a subset of rn is compact if and only if it is closed and bounded.
3524,1,['disjoint'], Metric Spaces ,seg_237,"exercise 2.6 if the closed subset a and the compact subset b are disjoint, then we have d(a, b) ≡ inf{d(x, y) : x ∈ a, y ∈ b} > 0."
3525,1,['sets'], Metric Spaces ,seg_237,"definition 2.4 (covering numbers) let (m,d) be a totally bounded metric space. let > 0. let n( ,m) denote the minimum n for which m = ∪1nak for sets ak all having diam (ak) ≤ 2 . let d( ,m) be the largest number m of points xk in m having d(xi, xj) > for all i = j. these are called covering numbers."
3526,1,['functions'], Metric Spaces ,seg_237,spaces of functions
3527,1,"['functions', 'continuous', 'limit']", Metric Spaces ,seg_237,"exercise 2.8 (uniform norm) let cb(m) denote the collection of all bounded and continuous functions from the topological space (m,u) to r, and define the uniform norm ‖ · ‖ on the functions of cb(m) by ‖f‖ ≡ sup{|f(x)| : x ∈ m}. (a) a uniform limit of bounded, continuous functions is bounded and continuous. (b) moreover, (cb(m), ‖ · ‖ defines a complete metric space. (c) let (m,u) be a compact hausdorff space. then (cb(m), ‖ · ‖ is separable if and only if (m,u) is metrizable."
3528,1,['continuous'], Metric Spaces ,seg_237,"definition 2.5 (uniform continuity) a mapping f from a metric space (m1, d1) to a metric space (m2, d2) is uniformly continuous if for all > 0 there exists δ > 0 for which d2(f(x), f(y)) < whenever d1(x, y) < δ ."
3529,1,"['functions', 'continuous']", Metric Spaces ,seg_237,"definition 2.6 (equicontinuity) (a) a collection f of bounded and continuous functions from a topological space (m,u) to a metric space (n, d) is equicontinuous if for every > 0 and each x ∈ m there is a neighborhood u of x for which d(f(x), f(y)) < for all y ∈ u and all f ∈ f . (b) in fact, if (m,ρ,u = uρ) is a metric space for which d(f(x), f(y)) < for all f ∈ f whenever ρ(x, y) < δ , then the functions of f are called uniformly equicontinuous."
3530,1,"['functions', 'continuous', 'function']", Metric Spaces ,seg_237,exercise 2.9 a continuous function (an equicontinuous family of functions) from a compact metric space into another metric space is uniformly continuous (is uniformly equicontinuous).
3531,1,"['functions', 'continuous', 'convergence']", Metric Spaces ,seg_237,"exercise 2.10 (ascoli’s theorem) let f be an equicontinuous class of functions from a separable metric space (m,d) to a metric space (n, ρ). let fn denote a sequence of such functions for which c1({fn(x) : n ≥ 1}) is compact for each x ∈ m. then there is a subsequence fn′ that converges pointwise to an f that is continuous on m , and this convergence is uniform on any compact subset k of m."
3532,1,['functions'], Metric Spaces ,seg_237,"exercise 2.11 (arzelà’s theorem) (a) a collection f of real-valued, bounded, and continuous functions f on a compact topological space (m,u) is a totally bounded subset of (cb(m), ‖ · ‖ if and only if it is a collection of uniformly bounded and uniformly equicontinuous functions. (b) a subset f of (cb([a, b]), ‖ · ‖). is compact if and only if f is closed, bounded, and equicontinuous with respect to ‖ · ‖."
3533,1,['linear'], Hilbert Space ,seg_239,"(d) (normed linear space) the pair (h, ‖ · ‖ has the following three properties, and these are the three properties that define a normed linear space. thus"
3534,1,['linear'], Hilbert Space ,seg_239,a complete normed linear space is called a banach space.
3535,1,['complement'], Hilbert Space ,seg_239,"(e) (orthogonal, and orthogonal complement) we agree that"
3536,1,[], Hilbert Space ,seg_239,"(i) x ⊥ y means that 〈x, y〉 = 0,"
3537,0,[], Hilbert Space ,seg_239,and we then say that x is orthogonal to y. we also let
3538,1,['complement'], Hilbert Space ,seg_239,and we call it the orthogonal complement of m.
3539,1,[], Hilbert Space ,seg_239,"(f) (hilbert space) if h is an inner product space that is complete (with respect to the metric defined by the norm ‖x − y‖), then h is called a hilbert space."
3540,1,"['linear', 'linear map']", Hilbert Space ,seg_239,(g) (bounded linear map) a linear map l : h → r satisfies
3541,1,"['linear', 'linear map']", Hilbert Space ,seg_239,"the linear map l is said to be bounded if ‖l‖ is finite, where"
3542,1,['linear'], Hilbert Space ,seg_239,call ‖l‖ the norm of l. the collection of all linear maps on h is denoted by h∗.
3543,1,['linear'], Hilbert Space ,seg_239,"example 3.1 (l2 is a hilbert space) let μ denote a fixed positive measure. then let l2(μ) ≡ {x : ∫x2 dμ < ∞}, which is a hilbert space with inner product 〈x, y 〉 = ∫xy dμ = e(xy ). recall the completeness presented in exercise 3.5.1. (note part (g) below to identify the bounded linear functionals on this h. this is called the reisz representation theorem.)"
3544,1,"['linear', 'continuous']", Hilbert Space ,seg_239,"exercise 3.1 (a) show that a linear functional l on (h, ‖ · ‖ is bounded if and only if it is continuous if and only if it is continuous at any one point. (b) show that ‖l‖ = sup{x:‖x‖=1} |l(x)|/‖x‖ is also valid."
3545,0,[], Hilbert Space ,seg_239,"proposition 3.1 (elementary properties) let x, y ∈ h, an inner product space."
3546,1,[], Hilbert Space ,seg_239,"(a) (cauchy–schwarz) |〈x, y〉 | ≤ ‖x‖‖y‖."
3547,1,['inequality'], Hilbert Space ,seg_239,(b) (triangle inequality) ‖x + y‖ ≤ ‖x‖ + ‖y‖.
3548,1,"['linear', 'continuous']", Hilbert Space ,seg_239,"(f) (linear functionals) x → 〈x, y〉 and x → 〈y, x〉 are uniformly continuous linear"
3549,1,['linear'], Hilbert Space ,seg_239,(g) (reisz) the only bounded linear functionals on h are those in (f).
3550,1,['continuous'], Hilbert Space ,seg_239,(h) x → ‖x‖ is a bounded and uniformly continuous functional on h.
3551,0,[], Hilbert Space ,seg_239,exercise 3.2 prove proposition 3.1.
3552,1,['linear'], Hilbert Space ,seg_239,exercise 3.3 an inner product space over the reals is a normed linear space iff the inner product satisfies the parallelogram law. [hint. use proposition 3.1(e).]
3553,1,['limit'], Hilbert Space ,seg_239,"definition 3.2 (subspaces) we say that m ⊂ h is a subspace of the vector space h if m is also a vector space. [it is enough if x + y and cx are in m for all x, y ∈ m and all scalars c.] [recall that a subset m of the metric space h is closed if and only if it is complete (that is, it contains all of its own limit points).]"
3554,0,[], Hilbert Space ,seg_239,exercise 3.4 let m be a subset of the hilbert space h.
3555,0,[], Hilbert Space ,seg_239,"(c) if m1 and m2 are closed subspaces of h with m1 ⊥ m2, then the sum space"
3556,1,"['linear', 'continuous']", Hilbert Space ,seg_239,(e) let l denote a continuous linear functional on h. then m = {x : l(x) = 0} is a closed
3557,0,[], Hilbert Space ,seg_239,"subspace, and either m⊥ has dimension 1 (or else m⊥ = h)."
3558,0,[], Hilbert Space ,seg_239,theorem 3.1 (orthogonal projections) let m be a closed subspace of h.
3559,0,[], Hilbert Space ,seg_239,(i) unique mappings p and q on h necessarily exist such that
3560,0,[], Hilbert Space ,seg_239,"this p and q are called the orthogonal projections of h onto m and onto m⊥. that is, h = m + m⊥ with m ∩ m⊥ = {0} when m is a closed subspace."
3561,0,[], Hilbert Space ,seg_239,"(ii) specifically and additionally, this p ≡ pm and q ≡ pm ⊥ satisfy:"
3562,1,"['continuous', 'linear']", Hilbert Space ,seg_239,"(4) p and q are bounded, uniformly continuous, and idempotent linear maps."
3563,1,"['linear', 'linear map']", Hilbert Space ,seg_239,"(iii) moreover, pmx may be uniquely determined by solving x − pmx ⊥ m. or, pm is the unique linear map for which 〈pmx, y〉 = 〈pmx, pmy〉 for all x, y ∈ h."
3564,0,[], Hilbert Space ,seg_239,"(iv) if m is a proper subset of h, then there exists a y = 0 in h such that y ⊥ m. moreover, the space spanned by m and y is closed."
3565,0,[], Hilbert Space ,seg_239,"exercise 3.6 let m,m1, and m2 denote non-trivial subspaces of h. verify:"
3566,1,"['linear', 'independent', 'linear combinations', 'set', 'combinations', 'coefficient']", Hilbert Space ,seg_239,"definition 3.3 (independent, span, orthonormal, fourier coefficient, and basis) (i) call vectors {x1, . . . , xn} in v linearly independent if c1x1+· · ·+cnxn = 0 implies c1 = · · · = cn = 0. call the set z linearly independent if all of its finite subsets are linearly independent. let the span of z (denoted by s[z]) denote the set of all finite linear combinations of elements of the set z. (ii) if z ≡ {za : a ∈ a} is a subset of h indexed by a, then the elements of z are called orthonormal if 〈za, zb〉 = 0 for all a, b ∈ a, while ‖za‖ = 1 for all a ∈ a. moreover, the values"
3567,1,['coefficients'], Hilbert Space ,seg_239,"(6) xa(a) ≡ 〈x, za〉, for all a ∈ a, are called thefourier coefficients of x"
3568,1,['set'], Hilbert Space ,seg_239,relative to z. a maximal orthonormal set is called an orthonormal basis.
3569,1,['independent'], Hilbert Space ,seg_239,"(7) ck = 〈x, zk〉, the z1, . . . , zn are linearly independent, and ‖x‖2 =∑n"
3570,1,"['set', 'inequality']", Hilbert Space ,seg_239,"(iii) (bessel’s inequality) for any orthonormal set {za : a ∈ a},"
3571,1,['coefficients'], Hilbert Space ,seg_239,"(iv) thus, each x ∈ h has at most countably many nonzero fourier coefficients. (v) let y1, y2, . . . be orthogonal vectors in h. then"
3572,1,"['set', 'inequality']", Hilbert Space ,seg_239,"let z1, z2, . . . be any orthonormal set, and distance is the ‖ · ‖ 2 below. (vii) let 2(a) denote l2(a, 2a, counting measure), as in exercise 3.5.11. bessel’s inequality can be rewritten. for any orthonormal set {za : a ∈ a} we have"
3573,1,['set'], Hilbert Space ,seg_239,"(viii) an orthonormal set z1, z2, . . . is closed and bounded, but it is not compact. thus h is not locally compact."
3574,1,['set'], Hilbert Space ,seg_239,theorem 3.3 (orthonormal bases) let {za : a ∈ a} be an orthonormal set. the following are equivalent:
3575,1,['coefficients'], Hilbert Space ,seg_239,(20) every x ∈ h is uniquely determined by its fourier coefficients
3576,1,[], Hilbert Space ,seg_239,theorem 3.4 (h is isomorphic to 2) (a) suppose the hilbert space h has an orthonormal basis {za : a ∈ a}. then the mapping x → xa maps h onto 2(a). and it is an isomorphism in that it preserves inner products via the correspondence
3577,1,"['cardinality', 'set']", Hilbert Space ,seg_239,(b) every hilbert space possesses an orthonormal basis. thus every hilbert space h is isomorphic to some 2(a). the cardinality of a equals that of the basis. (c) every orthonormal basis for the hilbert space h has the same cardinality. (d) every orthonormal set can be extended to an orthonormal basis. (e) h is separable if and only if h contains an orthonormal basis that is finite or countably infinite.
3578,1,['independent'], Hilbert Space ,seg_239,exercise 3.8 (gram–schmidt) show that from any n vectors that are linearly independent one can define n orthonormal vectors that have the same span.
3579,1,"['functions', 'case', 'representations']", Hilbert Space ,seg_239,"consider fourier series representations defined on [−π, π]. in this case the functions"
3580,1,"['functions', 'set', 'function', 'coefficients']", Hilbert Space ,seg_239,"form a set (denote it by z) of orthonormal functions. the corresponding fourier coefficients for a function f in l1 ≡ l1([−π, π],b, λ) are given by"
3581,1,[], Hilbert Space ,seg_239,"the partial sums of the fourier series representation of f are defined on [−π, π] by"
3582,1,"['case', 'coefficients']", Hilbert Space ,seg_239,"in case f ∈ l1([0, 1],b, λ), we define fourier coefficients"
3583,1,[], Hilbert Space ,seg_239,"for n ≥ 1, with the fourier series representation gn(t) ≡ ∑0"
3584,1,"['functions', 'set']", Hilbert Space ,seg_239,"the set (denote it by z) of orthonormal functions φn, n ≥ 0. let ω ≡ [0, 1] and let lp ≡ lp([0, 1],b, λ)."
3585,1,"['case', 'coefficients']", Hilbert Space ,seg_239,"in case f ∈ l1([0, 1],b, λ), we also define fourier coefficients"
3586,1,[], Hilbert Space ,seg_239,with the fourier series representation gn(t) ≡∑1
3587,1,"['functions', 'set']", Hilbert Space ,seg_239,"n ajφj(t) on [0, 1] in terms of this set (denote it by z) of orthonormal functions φn, n ≥ 1. let ω ≡ [0, 1] and let lp ≡ lp([0, 1],b, λ)."
3588,1,"['convergence', 'representations']", Hilbert Space ,seg_239,"we are interested in when each of these three fourier series representations gn “converges” to f . we will consider various modes of convergence on these [c, d]."
3589,1,"['functions', 'cases', 'set', 'coefficients']", Hilbert Space ,seg_239,"theorem 3.5 (reisz–fischer) consider any f ∈ l2 ≡ l2(ω,b, λ), where we define ω ≡ [c, d]. let gn denote the fourier series representation of f based on the first n orthonormal functions z (in any one of the three cases considered above). let φn generically denote the full set of orthonormal functions in z, and then let an ≡ ∫ω f(t)φn(t) dt denote the corresponding fouier coefficients. (a) the class z is a complete orthonormal basis for l2. (b) moreover:"
3590,1,['coefficients'], Hilbert Space ,seg_239,where ãn ≡ ∫ω f̃(t)φn(t) dt denotes the fourier coefficients of the other f̃ ∈ l2.
3591,1,['continuous'], Hilbert Space ,seg_239,"definition 3.4 (piecewise continuous) suppose f is continuous at all but a finite number of points in [c, d], while right and left hand limits of f do exist at each of these exceptional points. then call f piecewise continuous on [c, d]."
3592,1,['continuous'], Hilbert Space ,seg_239,"theorem 3.7 (a) let f be piecewise continuous on ω = [c, d], and suppose the two onesided derivatives f+"
3593,1,"['condition', 'variation']", Hilbert Space ,seg_239,"(jordan condition) (b) let f ∈ l1([c, d],b, λ), and suppose that f is of bounded variation in some neighborhood of the point t ∈ (c, d). then"
3594,1,"['interval', 'variation', 'continuous', 'convergence']", Hilbert Space ,seg_239,"(c) let f be continuous and of bounded variation on [c, d]. then gn → f uniformly on every subinterval [co, do] having c < co < do < d. (uniform convergence of gn to f on any interval containing a discontinuity of f is impossible. gibb’s phenomenon is the name given to what goes wrong. to prevent it, endpoint conditions are assumed in the next theorem.)"
3595,1,"['continuous', 'case']", Hilbert Space ,seg_239,"theorem 3.8 let f be continuous on ω = [c, d]. we require f(−π) = f(π) in case (23) on [−π, π], f(0) = f(1) = 0 in (27) on [0, 1], nothing in (26) on [0, 1]. suppose that f ′ is piecewise continuous on ω. then:"
3596,1,['continuous'], Hilbert Space ,seg_239,"if f has m− 1 continuous derivatives and a piecewise continuous mth derivative, then gn"
3597,1,['continuous'], Hilbert Space ,seg_239,"theorem 3.9 let f be piecewise continuous on ω = [c, d]. then"
3598,0,[], Hilbert Space ,seg_239,(no matter how badly the gn may fail to converge to f).
3599,1,['case'], Hilbert Space ,seg_239,"proposition 3.2 let f ∈ l1. if all an = 0, then f(t) ≡ 0 on ω (in each case)."
3600,1,['representations'], Hilbert Space ,seg_239,"theorem 3.10 (cesàro summability of fourier series) let lr([−π, π],b, λ), for some fixed r ≥ 1. for the fourier series representations gn above, let"
3601,0,[], Hilbert Space ,seg_239,then the cesàro sums hn converge to f in the sense:
3602,1,[], Hilbert Space ,seg_239,"exercise 3.9 obtain the fourier series representation of f(t) = t on [−π, π]. use the previous theorems to claim that ∑1"
3603,0,[], Hilbert Space ,seg_239,"2 vergence in the sense of l2 and of (35)–(37)). in particular, now show that"
3604,0,[], Hilbert Space ,seg_239,"exercise 3.11 (legender polynomials) consider l2([−1, 1],b, λ). define"
3605,1,['process'], Hilbert Space ,seg_239,"these are the legender polynomials. (a) show that by beginning with 1, t, t2, . . . and then applying the gram–schmidt orthogonalization process, one arrives at these orthogonal polynomials. (b) also, verify that these polynomials are orthonormal. (c) verify that they form a basis."
3606,1,['set'], Hilbert Space ,seg_239,"exercise 3.12 (hermite polynomials) see (11.6.4), (11.6.15), and exercise 11.6.1 for the orthonormal set of polynomials hn/√n (for n ≥ 0). (a) show that they are orthonormal relative to (r,b, p ), with p denoting the n(0, 1) measure. (b) verify that they form a basis."
3607,1,['linear'], Hilbert Space ,seg_239,bounded linear functionals on lr
3608,1,"['function', 'linear']", Hilbert Space ,seg_239,"exercise 3.13 (reisz representation theorem) fix 1 < r < ∞, and define s via 1/r+1/s = 1. a linear functional g on lr ≡ lr(ω,a, μ) is bounded if and only if there is a measurable function y ∈ ls ≡ ls(ω,a, μ) for which g(x) = e(xy ) for all x ∈ lr. the norm of this bounded linear functional g is given by ‖g‖ = ‖y ‖s. thus the collection lr∗ of all bounded linear functionals on lr is isomorphic to ls (in that norms are preserved)."
3609,1,"['independent', 'estimate', 'moment', 'estimation', 'case', 'tail', 'second moment']", Introduction,seg_243,"the beginning section c.1 can be viewed as a continuation of the wlln theme, that now allows the case of a possibly infinite moment. to do so, we choose to estimate a moment when trimming a vanishingly small proportion from the tail of the moment. this leads to what could be referred to as the ratio lln . such estimation of a second moment is intimately connected to determining the most general possible setting for a clt for sums of independent rvs that are each individually rather negligible. (a version of this material appeared in the 1st edition as the sixth section of the llns chapter ten, though it has now been streamlined and improved.)"
3610,1,"['statistics', 'linear', 'results', 'functions', 'asymptotic', 'tails', 'distribution', 'linear combinations', 'moments', 'convergence', 'treatment', 'normality', 'order statistics', 'normal', 'combinations', 'normal distribution']", Introduction,seg_243,"such results depend on a careful understanding of the contributions of the tails of a df to its moments. this topic is also developed in sections c.2–c.3 and some of that is used in section c.1. (sections 6.5 and 6.6 derive inequalities capable of yielding necessary and sufficient conditions for asymptotic normality for untrimmed, trimmed, and winsorized averages, as well as the extension of such results to linear combinations of functions of order statistics and linear rank statistics.) these six sections could be viewed as a continuation of chapters 6 and 8 of the present text. a short treatment of the applications referred to above appears in chapter 15 of this text. these six sections on the df appeared in the 1st edition as sections seven through eleven of chapter seven, section six of chapter 10, and a more complete treatment of the applications that carefully treated uniform convergence to normality of the various statistics under very general conditions appeared in a chapter sixteen. roughly then, nearly all this theoretical material has been moved to appendix c, with a greatly reduced version of the applications moved to chapter 15. the exception is chapter 11 where some of this remaining material is cited in developing the domain of attraction of the normal distribution."
3611,1,"['average', 'case']", General Moment Estimationo,seg_245,"notation 1.1 (general case on [0, ∞)) let yn1, . . . , ynn denote rvs that are row independent with dfs fn1, . . . , fnn and with average df f̄n ≡ n"
3612,1,"['average', 'median']", General Moment Estimationo,seg_245,"1 ∑n 1 fnk. let xnk ≡ |ynk|, with median m̈nk, for 1 ≤ k ≤ n. the average df of xn1, . . . , xnn is 1 − p̄n, where"
3613,1,"['function', 'quantile']", General Moment Estimationo,seg_245,"1 ∑n 1 p (|ynk| > x), with quantile function kn(·). let xθn denote the (1 − θ/n)th"
3614,1,"['symmetric', 'interval', 'probability']", General Moment Estimationo,seg_245,"quantile of the df 1 − p̄n. (thus x1n ≡ kn(1 − 1/n) is the (1 − 1/n)-quantile of 1 − p̄n, while [−x1n, x1n] is the shortest symmetric interval with at least 1 − 1/n of the probability.) let the rv wn have df 1 − p̄n."
3615,0,[], General Moment Estimationo,seg_245,"rk, for a fixed r > 0. we do not require that exn rk < ∞. define the"
3616,1,"['functions', 'moment']", General Moment Estimationo,seg_245,"truncated rth absolute moment functions on [0, ∞) and on (0, 1] by"
3617,1,['tails'], General Moment Estimationo,seg_245,let uθn ≡ un(xθn) and mθn ≡ mn(θ/n) for small values 0 < θ ≤ 1. measure the relative heaviness of the tails of both of the dfs f̄n and 1 − p̄n by either
3618,1,['parameters'], General Moment Estimationo,seg_245,1 ) are natural parameters to associate with the
3619,1,"['estimation', 'consistency', 'moments']", General Moment Estimationo,seg_245,theorem 1.1 (consistency of the absolute moments; estimation) (a) statements (3)-(10) below are equivalent.
3620,1,[], General Moment Estimationo,seg_245,"(b) if (3), then (9) yields u1n/νn → 1, ǔn/νn → 1, and (e.g.) un(nu1n)/νn → 1. (c) the w̃insorized means ũθn ≡ uθn + xrθnp̄n(xθn) and m̃θn ≡ mθn + n"
3621,1,['hypothesis'], General Moment Estimationo,seg_245,corollary 1 under the hypothesis of the preceding theorem we necessarily have
3622,1,"['average', 'case']", General Moment Estimationo,seg_245,"discussion 1.1 (general case on [0, ∞)) let yn1, . . . , ynn denote rvs that are row independent with dfs fn1, . . . , fnn and average df f̄n ≡ n"
3623,1,"['functions', 'quantile', 'numerical']", General Moment Estimationo,seg_245,"(use the top formulas in (a) and (b) to picture the numerical values of un(x) and un−(x) on the quantile half of figure 8.4.1, and then use the two bottom formulas to picture the corresponding areas in the df half of figure 8.4.1.) measure the values of these functions at the 1 − θ/n−quantiles xθn(using f̄n(xθn) ≥ 1 − n"
3624,1,['estimator'], General Moment Estimationo,seg_245,natural constants to associate with the estimator xn
3625,0,[], General Moment Estimationo,seg_245,exercise 1.1 show that for each fixed 0 < θ ≤ 1 both
3626,0,[], General Moment Estimationo,seg_245,for the truncated rv zn
3627,0,['n'], General Moment Estimationo,seg_245,for all n ≥ (some large n θ). this implies that
3628,1,['inequality'], General Moment Estimationo,seg_245,we now show that (6) implies (4) (and we are entitled to use (12) to do so). fix 0 < θ < . the truncation inequality (8.3.23) (the proof is repeated below) gives
3629,1,['probability'], General Moment Estimationo,seg_245,"with probability at least 1 − 2 . thus (4) holds. that is, (6) implies (4)."
3630,1,['inequality'], General Moment Estimationo,seg_245,for the symmetrized rvs wnsk. then levy’s inequality (8.3.4) and (k) give
3631,0,[], General Moment Estimationo,seg_245,it further holds that
3632,1,[], General Moment Estimationo,seg_245,p (maxk|(xnk/nu1n) − median(xnk/nu1n)| > )
3633,1,[], General Moment Estimationo,seg_245,≤∑n 1p (|(xnk/nu1n) − median(xnk/nu1n)| > )
3634,1,['inequality'], General Moment Estimationo,seg_245,np (|wnsk| > ) by the symmetrization inequality (8.3.2)
3635,1,[], General Moment Estimationo,seg_245,(n) maxk|(xnk/nu1n) − median(xnk/nu1n)| →p 0.
3636,1,['condition'], General Moment Estimationo,seg_245,"since the uan condition in (4) yields {maxk |median(xnk)/nu1n)|} → 0, we have shown from"
3637,0,[], General Moment Estimationo,seg_245,exercise 1.2 prove corollary 1 to theorem 1.1 by adapting (d)–(j) in the previous proof in light of the truncation equality (8.3.23).
3638,0,[], General Moment Estimationo,seg_245,but this contradicts (15). thus
3639,0,[], General Moment Estimationo,seg_245,"combined with (a), this gives"
3640,1,['inequality'], General Moment Estimationo,seg_245,then for n ≥ (some n ) the truncation inequality (8.3.25) gives
3641,1,['expectation'], General Moment Estimationo,seg_245,has the expectation bound
3642,1,['inequality'], General Moment Estimationo,seg_245,so markov’s inequality now yields δn →p 0. this δn →p 0 and (17) yield
3643,1,['replacement'], General Moment Estimationo,seg_245,"we will now prove that (9) implies (3). begin rereading at (16), and replace μ̌n in the definitions of z̄n and znk by νn (which necessitates this same replacement at various points in the arguments that follow (16)—these include the denominators of (17), (g), and (i)). (statement (f) is not needed.) since μ̌n = μ̄n = μ̄1n, (g) now yields (μ̄1n − μ̄ n)/νn → 0, using (9) for μ̄1n/νn → 1. thus (h) and (i) give z̄n/μ̄ n →p 1 as is implied by (h), and μ̄ n/νn → 1 as in (i). hence z̄n/νn →p 1. then p (x̄n = z̄n) ≤ np̄ (nνn) → 0 from (9), and gives x̄n/νn → 1 so, (3) holds true."
3644,0,[], General Moment Estimationo,seg_245,"proof. we will now show that (7) and (10) are equivalent. suppose (7), which we already know implies (4). then"
3645,1,['condition'], General Moment Estimationo,seg_245,"using (7) and then (4) in the last step. the uan condition in (10) is implied by the stronger (7). this shows that (7) implies (10); that is, it establishes that"
3646,1,['independent'], General Moment Estimationo,seg_245,"suppose, alternatively, that (7) fails for some uan array of row independent xnk. (note o’brien (1980) for the iid analog.) then for a fixed tiny > 0,"
3647,0,[], General Moment Estimationo,seg_245,"let amk ≡ [xmk > mu1m], let cm ≡ [exactly 1 of the xmk’s exceed mu1m], and let dkm ≡ [at least k of the xmk’s exceed mu1m]. bounding p (d1m) below by inclusion/exclusion gives"
3648,1,['condition'], General Moment Estimationo,seg_245,"k=1p (amk) for all n ≥ (some n3 ), by the uan condition,"
3649,1,['inequality'], General Moment Estimationo,seg_245,since uan gives (18) once the markov inequality gives (f) via
3650,1,['inequality'], General Moment Estimationo,seg_245,fix β ≡ α 3/2 (note that 1/β ≥ 2). then the markov inequality gives
3651,1,['failure'], General Moment Estimationo,seg_245,"so, the failure of (7) implies the failure of (10); or (10) implies (7). thus (c) and (n) give the equivalence of (7) and (10). (this present proof replaces the proof that added (10) to the list in the first edition of this text.)"
3652,1,['case'], General Moment Estimationo,seg_245,"independent identically distributed rvs (or, the iid case)"
3653,1,"['functions', 'moment', 'case', 'moments']", General Moment Estimationo,seg_245,"notation 1.2 (absolute moments in the iid case) let y and all rvs ynk have a common df f (·) ≡ fy (·) and qf k(·) ≡ ky (·). (let x ≡ |y |. the rvs xnk ≡ |ynk| may be represented as kx(ξnk) for row-independent uniform(0, 1) rvs ξn1, . . . , ξnn, as in (6.3.5).) then p (·) ≡ 1 − fx(·). the truncated rth absolute moment functions m(·) and u(·) (as defined in discussion 1.1) are defined by"
3654,1,"['quantile', 'moment']", General Moment Estimationo,seg_245,"let mθn ≡ m(θ/n) and uθn ≡ u(xθn), where xθn ≡ kx(1 − θ/n) is defined to be the (1 − θ/n)th quantile of fx . the winsorized absolute moment"
3655,1,[], General Moment Estimationo,seg_245,comment section c.3 is devoted to showing that the four conditions in (25) and (26) are equivalent. (this section could be read now.) note also exercise 6.3.3.
3656,1,"['estimation', 'consistency', 'moments']", General Moment Estimationo,seg_245,"theorem 1.2 (consistency of absolute moments for iid rvs; estimation) for iid rvs xnk ≥ 0 as above, conditions (3)–(10), and (23)–(32) are equivalent."
3657,1,"['case', 'condition']", General Moment Estimationo,seg_245,"proof. of course, all of (3)–(10) are still equivalent in this iid case. (however, the uan condition is now redundant, since the claim just below (g) in the proof of theorem 1.1 that implied {maxk median(xnk)/nu1n} → 0 is now trivial in this iid case. the uan consequence in (18) is also trivial.) part (d) of theorem 1.1 showed that ũ1n can everywhere replace u1n, and this justifies the inclusion of (24)(b). note that (23), (24)(a), (30), and (31) are identical to (3), (4), (9), and (10)."
3658,1,['inequality'], General Moment Estimationo,seg_245,"fix > 0. then for a fixed tiny θ ≡ θ < , the truncation inequality (8.4.25) gives"
3659,0,['n'], General Moment Estimationo,seg_245,"for n sufficiently large, by (27). in like fashion"
3660,1,['probability'], General Moment Estimationo,seg_245,"with probability at least 1 − 2 . that is, x̄n/m1n →p 1, giving (24)(c)."
3661,0,[], General Moment Estimationo,seg_245,"that each of the two parts of (25) is equivalent to each part of (26) follows from (c.3.6), which is established simply by appealing to pictures of the graphs of f and k. then (26) trivially implies (27), while the proof that (27) implies (26) just mimics (m)–(n) on page 490. finally, (24)(c) implies (23), then (24)(a), then (5), which then implies (27)."
3662,0,[], General Moment Estimationo,seg_245,"comment at this point the reader should peruse sections c.2−c.3, and only then come back to complete the reading of this section."
3663,1,"['moments', 'independent', 'case']", General Moment Estimationo,seg_245,"notation 1.3 (moments in the iid case) let yn1, . . . , ynn be a triangular array of row independent rvs, all with common df f and qf k. now define the rvs yn"
3664,1,['plots'], General Moment Estimationo,seg_245,"make crude plots of m−(·), m+(·), m̄(·) and m(·) ≡ k|rx|(1 − ·) before going on."
3665,1,"['case', 'estimators', 'consistency', 'moments']", General Moment Estimationo,seg_245,theorem 1.3 (consistency of moments in the iid case; as estimators) fix an r > 0. just some of the possible equivalent conditions are as follows:
3666,1,[], General Moment Estimationo,seg_245,"moreover, these are also equivalent to each of the conditions in the lists of theorem 1.1 and theorem 1.2 such as"
3667,1,['condition'], General Moment Estimationo,seg_245,"θk ≡ |k(ξnk)|1[θ/n<ξnk<1−θ/n]. trivially, (35) implies (23). from theorem 1.1 we know that (23) is equivalent to (10). however, (10) = (30) = (39), as they are identical in content. now, (39) = (30) is equivalent to (26) (by theorem 1.2) and which is equivalent to (36) and to (37) (by theorem 2.2). having the condition (10) = (30) = (39) on this list allowed us to add all previous equivalent conditions given in (23)–(31) in theorem 1.2, and this is what all four of the conditions given in the list (42) = (24)(c), (43) = (6), (44) = (7), and (45) = (25) were chosen to exemplify."
3668,0,[], General Moment Estimationo,seg_245,"consider (46). with the zn θk of the previous paragraph, we now consider"
3669,1,['inequality'], General Moment Estimationo,seg_245,using the truncation inequality (8.4.15) yet again (with mθn ≡ e(zn
3670,1,"['estimation', 'variance']", General Moment Estimationo,seg_245,theorem 1.4 (equivalencies for consistent estimation of the variance σ̃n
3671,1,[], General Moment Estimationo,seg_245,"let y and yn1, . . . , ynn be iid with df f (·) and qf k(·) let f|y |(·) and k|y |(·) denote the df and qf of |y |. let dom(a, a) denote one of (a, 1], [0, 1 − a), or (a, 1 − a) according as y ≤ 0, y ≥ 0, or y is general. define v̄(t) ≡ [k+−(t)]2 +[k+(1− t)]2 and v(t) ≡ k|2y |(t). the following are just some of the possible equivalent conditions:"
3672,1,['variance'], General Moment Estimationo,seg_245,"1 ) is the variance of ky (ξ) winsorized outside dom(n 1 , n"
3673,1,['varying'], General Moment Estimationo,seg_245,(61) u(x) = ∫[|y|≤x] y2df (y) is slowly varying at ∞. (see (c.2.13)).
3674,1,[], General Moment Estimationo,seg_245,"now, (51)–(54) imply u1n/v1n → 1, ũ1n/v1n = ṽ1n/v1n → 1, and v̄1n/v1n → 1. (if (50) holds for one such sequence an, then it holds for all such sequences an.) [theorems c.2.1 and c.2.2 present many more equivalencies similar to (47)–(50) and (61), all of which follow from using just geometrical considerations and cauchy–schwarz.] (see theorems 1.1–1.3 for additional equivalencies.)"
3675,1,[], General Moment Estimationo,seg_245,"proof. the equivalence of (47) through (59) (and of many other conditions from theorems 1.1–1.3, (with r = 2 for iid rvs), like (3)–(10), (23)–(32), and (35)–(45)) has already been established. we were able combine the lists of theorems 1.2–1.3 since (58) appeared on all of these lists. we added (59) via propositions 6.6.1 and 6.6.2. we added (61) via theorems c.2.1 and c.2.2."
3676,1,"['variance', 'variances']", General Moment Estimationo,seg_245,"remark 1.1 (alternatively winsorized variances) let σ̃2(x) denote the variance of y winsorized outside [−x, x]. let σ̃2(t) denote the variance of ky (ξ) winsorized outside dom(t, t). note from theorem 1.4 that ũ1n = ṽ1n. note also that"
3677,1,['results'], General Moment Estimationo,seg_245,"(b) state the analogous results for u1n,m1n, ũ1n, m̃1n, and m"
3678,0,[], General Moment Estimationo,seg_245,"theorem 1.5 (strong negligibility) let y, y1, y2, . . . be iid rvs (that are not identically equal to 0). then"
3679,1,"['functions', 'parameters', 'condition', 'estimation', 'varying', 'variance']", Slowly Varying Partial Variance When σ   o,seg_247,many facts about slowly varying functions can be learned from simple pictures. we concentrate here on just such facts. lévy’s condition (14) has emerged as the necessary and sufficient condition of choice for the clt; and we will now derive many equivalent ways to demonstrate it. (necessary and sufficient conditions for consistent estimation of the variance parameters v (1/n) and σ̃2(1/n) as defined below are also equivalent to the conditions on the current list.) (note (21) below.)
3680,1,['moments'], Slowly Varying Partial Variance When σ   o,seg_247,"notation 2.1 let y denote an arbitrary rv (with df f and qf k); let x ≡ |y |. let fx and kx denote the df and qf of x. for 0 < t < 1, let xt ≡ kx(1 − t). let ỹt denote y w̃insorized) outside [−xt, xt]. define various partial moments, via"
3681,1,"['symmetric', 'variance', 'case']", Slowly Varying Partial Variance When σ   o,seg_247,"theorem 2.1 (partial variance, with symmetric w̃insorizing) only the case σ2 = ∞ has interest; all conclusions below are trivial if σ2 < ∞. (a): the following (also referred to as (4)(a)–(12)(a)) are equivalent (as t → 0) :"
3682,1,['condition'], Slowly Varying Partial Variance When σ   o,seg_247,"(b): specify a sequence an ↘ 0 as n → ∞ that satisfies lim sup an/an+1 < ∞. conditions (4)(b)–(12)(b) are obtained by replacing t by an in (4)(a)–(12)(a). these conditions (4)(b)– (12)(b) are also equivalent to condition (4)(a). (c): conditions (5)(c)–(11)(c) are obtained by replacing v (an) or ṽ (an) by σ̃2(an) in the denominators of (5)(b)–(11)(b). then (5)(c)–(11)(c) are also equivalent to the condition (4)(a). (also, (21) below is an equivalent; (20) and (22) are useful.) (d): the most useful choices are an ≡ /n(equivalently, an ≡ 1/n with c ≡ ), or the alternative an ≡ n/n with n ↘ 0 subject to lim sup n/ n+1 < ∞. (e): the following are equivalent (as x → ∞) to the previous conditions. [any sequence xn to be specified below is assumed to satisfy lim(xn+1/xn) < ∞).]"
3683,1,['varying'], Slowly Varying Partial Variance When σ   o,seg_247,"(13) u ∈ u0 (that is, u is slowly varying at ∞)."
3684,1,['function'], Slowly Varying Partial Variance When σ   o,seg_247,(16) ũ(x) ≡ u(x) + x2p (x > x) defines a function in u0.
3685,1,['variance'], Slowly Varying Partial Variance When σ   o,seg_247,"theorem 2.2 (partial variance, equal fractions) consider an arbitrary rv y with df f and qf k. let k̃t,t(·) denote k(·) winsorized outside dom(t, t) (recall notation 6.5.1), and now redefine so that"
3686,1,"['second moment', 'moment']", Slowly Varying Partial Variance When σ   o,seg_247,"all equivalences stated in parts (a), (b), and (c) of the previous theorem are still valid for the new definitions of q, v, σ̃2, and ṽ (t) ≡ e[k̃t2,t(ξ)]. (in the previous theorem we had q2(t) = v(t). in the present theorem we have v(t) ≤ q2(t) ≤ 2v(t), which is still convenient and meets all our needs. what is crucial is that m(·), q(·), v(·) and v (·) have an appropriate meaning. note that this v (t) is the ťruncated second moment (with respect to dom(t, t)).) (also, (21) below is equivalent.)"
3687,1,"['absolute value', 'statisticians', 'data', 'tail']", Slowly Varying Partial Variance When σ   o,seg_247,"note (to w̃insorize the absolute value, or to w̃insorize symmetrically) theorem 2.1 w̃insorizes symmetrically about zero, while the companion theorem 2.2 w̃insorizes equal fractions from each tail. statisticians use both strategies in the modification of their data, and these theorems are prepared to deal with both."
3688,1,"['moment', 'asymptotic', 'estimation', 'bootstrap', 'results', 'distribution', 'normality', 'moments', 'mean', 'variance', 'second moment']", Slowly Varying Partial Variance When σ   o,seg_247,"remark 2.1 (why develop all these equivalent conditions?) quite a number of the useful conditions that appear in the literature are developed herein. these have application in estimation of moments and partial moments and in the determination of necessary and sufficient conditions for asymptotic normality of iid rvs. some of these conditions are best developed in the context of a careful examination of the df and the qf of the underlying distribution. these are considered in this section and the next. some are best developed in the context of the ratio lln , and this was just done in the previous section in theorem c.1.4 for the estimation of a partial second moment and a partial variance when the underlying variance may be infinite. in sections 10.5–10.6 all of these results are connected to necessary and conditions for asymptotic normality of the mean of iid rvs. in sections 10.8–10.9 they are tied into the bootstrap via the quantity dn appearing in (c.1.60). (once started, it was just fun to see how completely it could be done.)"
3689,0,[], Slowly Varying Partial Variance When σ   o,seg_247,figure 7.1 comparison of areas.
3690,1,[], Slowly Varying Partial Variance When σ   o,seg_247,"suppose (7), that v ∈ r0. this means that [v (ct/2) − v (t)]/v (t) → 0, and figure 2.1(c) then demonstrates that"
3691,0,[], Slowly Varying Partial Variance When σ   o,seg_247,"supposing (8) about v(·), we will establish (9) about d(·). now,"
3692,1,[], Slowly Varying Partial Variance When σ   o,seg_247,then (7) implies (11) via cauchy–schwarz in
3693,0,[], Slowly Varying Partial Variance When σ   o,seg_247,"we next show that (4) is equivalent to the simpler (5). suppose (4) holds, so that σ̃2(·) ∈ r0. we use gnedenko and kolmogorov’s theorem 6.6.1 to write"
3694,1,['condition'], Slowly Varying Partial Variance When σ   o,seg_247,"since(6)(a) trivially implies (6)(b), the condition (6)(b) is now on the list."
3695,1,"['consequences', 'replacement']", Slowly Varying Partial Variance When σ   o,seg_247,"(c): note that (6)(c) is exactly (12)(b), and so (6)(c) is on our list, and thus it implies (12)(a). we may reread (e) (with denominator v (can) replaced by σ̃2(an) throughout) to see that (6)(c) (that is, its (12)(a) consequences) implies (7)(c). then rereading (f)–(h) (with the same denominator replacement) shows that (7)(c) implies (8)(c), which implies (9)(c), which implies (5)(c). we now close the circle on (5)(c)–(9)(c) by noting that (5)(c) implies (5)(b), again using the gnedenko and kolmogorov result (6.6.2). we can add (10)(c) by the same trivial argument as before. rereading (i)–(j) (with the new denominator) then allows us to add (11)(c)."
3696,1,['factor'], Slowly Varying Partial Variance When σ   o,seg_247,"the proof of theorem 2.2 is essentially identical. all but lines (h) and (j) are identical; line (i) is identical because m is still the integral of q. but (h) and (j) are not identical because we no longer have q2 = v. but we do have v2 = [q+]2 + [q−]2, where q = q+ + q−, and that is enough to complete the proof. just factor the two pieces separately in (h) and (j), and apply the trivial inequalities (a + b)2 ≤ 2(a2 + b2) and a ∨ b ≤ a + b ≤ 2(a ∨ b)."
3697,1,['varying'], Slowly Varying Partial Variance When σ   o,seg_247,"and for c > 1 it is analogous that [u(cx) − u(x)]/u(x) → 0. thus, u is slowly varying, as in (13)."
3698,1,['condition'], Slowly Varying Partial Variance When σ   o,seg_247,"and the extreme left term going to 0 forces r(cx) = (cx)2p (x > cx)/u(cx) → 0. thus (13)–(16) are equivalent. in fact, the second condition in (14) suffices, since"
3699,1,[], Slowly Varying Partial Variance When σ   o,seg_247,the equivalence of (14) and (6)(a) is shown in the next section. conditions (13)–(16) will then be added to the big list with the rest in theorems 2.1–2.2.
3700,0,[], Slowly Varying Partial Variance When σ   o,seg_247,"though already established, we will still give a simple proof that v (·) ∈ l does indeed imply ṽ (·) ∈ l. if v ∈ l, figure 2.1 (b) shows that"
3701,1,['condition'], Slowly Varying Partial Variance When σ   o,seg_247,remark 2.2 the condition (6.6.11) in theorem 6.6.2 is
3702,1,['condition'], Slowly Varying Partial Variance When σ   o,seg_247,"we now verify that (21) can be added to our list of equivalent conditions. (this condition gets heavy use in dealing with l-statistics.) just note that when σ2 = ∞, the gnedenko– kolmogorov theorem 6.6.1 gives"
3703,1,"['estimation', 'variance', 'moment']", Specific Tail Relationships o,seg_249,we list two relationships that could prove important. the first concerns the wlln and the second concerns the clt and variance estimation. they compare the height of the qf with the magnitude of a partial moment.
3704,0,[], Specific Tail Relationships o,seg_249,"proof. consider figure 8.4.1. when r = 1, the two quantities in (2) correspond to areas; so we will use words appropriate to r = 1."
3705,1,"['sets', 'associated']", Specific Tail Relationships o,seg_249,"at any point (x, f (x)) the quantity xp (x > x) is just the area above and to the left of this point (in the leftmost half of following figure). at any point (t, k(1 − t)) the quantity tk(1− t) is just the area below and to the right (in the rightmost half of the following figure). just trace out these two areas in the figures, and note that all the local extreme points of these two sets of area values (that are associated with jumps and flatspots) are identical."
3706,0,[], Specific Tail Relationships o,seg_249,"the same is true for the lim, and for the lim (if it exists)."
3707,1,[], Specific Tail Relationships o,seg_249,figure 4.1 conditions for the wlln and for the slln. (use (a ∨ b) ≤ a + b ≤ 2(a ∨ b) for general x.)
3708,0,[], Specific Tail Relationships o,seg_249,"proof. suppose that [x1, x2) is a maximal flat spot of f (·). then"
3709,1,[], Specific Tail Relationships o,seg_249,"for all x ∈ [x1, x2]. thus the supremum of r(·) across [x1, x2] is r(x2−). the three values r(x1), r(x2−), and r(x2) are numerically equal to the three values r(f (x1)), r(f (x1)+), and r(f (x2)); use (a) and picture the claims in terms of the flat spots in figure 4.1. suppose next that x is a discontinuity point of f (·) with jump size f (x) − f (x−). then r(x−) > r(x). moreover the maximum of r(t) across [f (x−), f (x)] is equal to r(f (x)+) = r(x−). finally, at all other points there are unique pairs that solve r(x) = r(t). thus (5) holds. this establishes the equivalence of the four different conditions that are stated in (25) and (26)."
3710,1,"['functions', 'function', 'varying']", Regularly Varying Functions ,seg_251,"definition 4.1 (regularly varying functions, at 0) call v (·) > 0 regularly varying at 0 with characteristic exponent r (written v ∈ rr) if l(t) ≡ t−rv (t) satisfies l(ct)/l(t) → 1 for each c > 0 (such a function l is called slowly varying, and we agree to write l ∈ l ≡ r0). [clearly, shifting a qf up or down has absolutely no effect on whether or not it is varies regularly.]"
3711,0,[], Regularly Varying Functions ,seg_251,(karamata theorem)
3712,0,[], Regularly Varying Functions ,seg_251,the left side of (a) thus gives
3713,1,[], Regularly Varying Functions ,seg_251,by setting a = 1 and letting b ↗ 1. the right side of (a) analogously gives
3714,1,[], Regularly Varying Functions ,seg_251,"by setting b = 1 and letting a ↘ 1. combining (c) and (d) gives tv(t)/v (t) → β, so the “only if” half of (1) holds. then (2) and (4) are immediate."
3715,1,['continuous'], Regularly Varying Functions ,seg_251,"consider the converse part of (1). let r(t) ≡ tv(t)/v (t), so that the ratio r(t) ∈ [β − , β+ ] for all 0 < t ≤ (some t ). now, v is absolutely continuous on every closed subinterval of (0, t ] by the fundamental theorem of calculus. and thus log v is absolutely continuous on the same closed subintervals (see exercise 4.4.5), and so we may claim that"
3716,0,[], Regularly Varying Functions ,seg_251,"thus for any 0 < t ≤ t , we can integrate to get"
3717,1,[], Regularly Varying Functions ,seg_251,(recall that a = b ⊕ c means that |a − b| ≤ c.) we write
3718,1,"['functions', 'function', 'varying']", Regularly Varying Functions ,seg_251,"definition 4.2 (regularly varying functions, at ∞) a function u > 0 on (0, ∞) is called regularly varying with exponent ρ (written u ∈ uρ) when l(x) ≡ x−ρu(x) satisfies l(cx)/l(x) → 1 for each c > 0 (such a function l(·) is called slowly varying at ∞)."
3719,1,['variance'], Regularly Varying Functions ,seg_251,"theorem 4.2 (partial variance, β > 0) [let α ≡ 2/(β + 1) and β = (2 − α)/α.] (i) based on the definitions in (c.2.1) and (c.2.3), the following are equivalent:"
3720,1,[], Regularly Varying Functions ,seg_251,"(ii) based on definition (c.2.17), the conditions (6)–(10) are equivalent."
3721,0,[], Regularly Varying Functions ,seg_251,"proof. the equivalence of (8), (9), and (10) follows from theorem 4.1. then add (7) using theorem 4.1. that (6) is also equivalent is the subject of the next exercise."
3722,1,"['results', 'variation', 'standard']", Regularly Varying Functions ,seg_251,"the following exercises summarize some standard general regular variation results. [that is, h is not assumed to be monotone. karamata’s result is now harder.]"
3723,1,"['variation', 'function', 'convergence', 'varying']", Regularly Varying Functions ,seg_251,"exercise 4.3 (regular variation holds uniformly) (a) let h denote any function that is regularly varying at 0, of order r ∈ r. then for 0 < a < b ≤ 1 we have the uniform convergence of both:"
3724,1,['varying'], Regularly Varying Functions ,seg_251,"exercise 4.4 (karamata) let h be regularly varying at 0, of order r ∈ r. (i) suppose r ≤ −1. then"
3725,1,"['functions', 'quantile']", Regularly Varying Functions ,seg_251,"question theorem 4.1 shows that when β > 0, the collection of quantile functions {k : v (·) ∈ r−β} form a subset of the collection {k : ṽ (·) ∈ r−β}. are these two collections actually the same collection?"
3726,1,"['tail', 'case']", Some Winsorized Variance Comparisons ,seg_253,"′(t) equals k+(a), k(t),k(1 − a′) according as t is in (0, a], (a, 1 − a′), [1 − a′, 1), while only the right tail of k(·) is modified if x ≥ 0, and only the left tail of k(·) is modified if x ≤ 0. (recall also that ao ≡ a ∧ (1 − a) for a· ≡ inf{t : k(t) ≥ 0}, from just below (6.5.8).) in any case,"
3727,1,['variance'], Some Winsorized Variance Comparisons ,seg_253,"for 0 ≤ a < 1 − a′ ≤ 1, which shows that σ̃2(a, a′) always ↗ var[k(ξ)] as a ↘ 0 and a′ ↘ 0. setting a = a′ = 0 gives a valid representation of the variance in [0, ∞] without mention of μ."
3728,0,[], Some Winsorized Variance Comparisons ,seg_253,"figure 10.1 if k ∈ k, then the graph of (k − μk)/σk lies entirely in the shaded region."
3729,1,"['variance', 'mean']", Some Winsorized Variance Comparisons ,seg_253,"fix a qf k0 having mean 0 and finite variance σ02, and (for some 0 < a0 < 1 fixed) define the"
3730,0,[], Some Winsorized Variance Comparisons ,seg_253,2 class of qfs
3731,1,"['tails', 'standardized']", Some Winsorized Variance Comparisons ,seg_253,to be all qfs whose standardized form is bounded in the tails by the fixed qf k0. (see the figure above.) let μ ≡ μk and σ ≡ σk for each k ∈ k.
3732,1,['variance'], Some Winsorized Variance Comparisons ,seg_253,"inequality 5.1 (uniform winsorized variance comparisons) note that as (a∨a′) → 0,"
3733,1,['moment'], Some Winsorized Variance Comparisons ,seg_253,"∼ proof. let k = (0, 1) with k ∈ k. the following bounds are uniform over k ∈ k and all 0 ≤ (a ∨ a′) ≤ (some a ). for the first moment comparison we use"
3734,1,['moments'], Some Winsorized Variance Comparisons ,seg_253,follows from √tk0(t) → 0 as t → 0 whenever σ2 < ∞. comparing second moments shows that for all 0 ≤ (a ∨ a′) ≤ (some a ) we have
3735,0,[], Some Winsorized Variance Comparisons ,seg_253,simple algebra completes the proof.
3736,1,"['variance', 'inequality']", Some Winsorized Variance Comparisons ,seg_253,"inequality 5.2 (basic winsorized variance inequality) we suppose that 0 ≤ a ≤ 1 − a′ ≤ 1 (with a = 0 allowed only if x ≥ 0, and a′ = 0 allowed only if x ≤ 0). fix 0 < c, c′ < 1. let k̃c(·) ≡ k̃ca,c′a′(·) and let μ̃c, σ̃c2, ṽc, and q̃c denote the same quantities for this new qf. it is immediate from the next figure (using |μ̃| ≤ q̃) that for a ∨ a′ sufficiently small,"
3737,1,['case'], Some Winsorized Variance Comparisons ,seg_253,inequalities aimed at the infinite-variance case
3738,1,['tail'], Some Winsorized Variance Comparisons ,seg_253,inequality 5.3 (tail relationships) suppose k+(a) < 0 < k(1 − a). then
3739,0,[], Some Winsorized Variance Comparisons ,seg_253,proof. the figure immediately gives all but (20) and (23). for (23) observe that σ̃2 ≥
3740,1,"['distribution', 'normal', 'inequality']", Some Winsorized Variance Comparisons ,seg_253,there are many equivalent ways of expressing that a df or qf is in the domain of attraction of a normal or stable distribution. the next inequality enables us to go back and forth between
3741,1,['variance'], Some Winsorized Variance Comparisons ,seg_253,various of these equivalent conditions and to establish new ones. we now prove that (24)–(28) below are equivalent. [we write (24)u–(28)u to denote uniformity of the inequalities in the qfs over some collection ku of qfs k(·).] it matters not here whether the variance of k(·) is finite or infinite.
3742,1,['tail'], Some Winsorized Variance Comparisons ,seg_253,"inequality 5.4 (tail equivalencies) (a) let the qf k be arbitrary. with fixed 0 < c, c′ < ∞ (bounded from 0 and ∞), as (a ∨ a′) → 0 the following are equivalent:"
3743,1,['condition'], Some Winsorized Variance Comparisons ,seg_253,"′ → 1 as the maximum (a ∨ a′) → 0. (c) the condition a ∨ a′ → 0 may be replaced by a specific an ∨ a′n → 0 as n → ∞. (d) if any one of (24)–(27) holds uniformly over a class ku of qfs k, then all of them hold uniformly over the same class ku. (e) everywhere in (24)–(27) that a σ̃2 appears in a denominator it may be replaced everywhere simultaneously by ṽ . (f) suppose x ≥ 0. we may let a ≡ 0, and claim everything above with respect only to a′."
3744,0,[], Some Winsorized Variance Comparisons ,seg_253,"proof. [the proofs are written assuming 0 < c, c′ < 1, with only minor adjustments needed otherwise] now, (27) is equivalent to (26) by (18), and (26) implies (24) by (19) (all implications holding with the claimed uniformity). then (24) implies (25) by (20) (also with the claimed uniformity). we will show in the next paragraph that (25) implies (27) and (24) (also with the claimed uniformity)."
3745,1,['statistical'], Some Winsorized Variance Comparisons ,seg_253,the statistical domain of attraction
3746,0,[], Some Winsorized Variance Comparisons ,seg_253,and call d the classical total domain of attraction. [it is customary to focus instead on the value α related to β by α ≡ 2/(β + 1).] let
3747,1,"['observations', 'statistician', 'tails', 'statistical']", Some Winsorized Variance Comparisons ,seg_253,"and we call d̃ the statistical domain of attraction. [in trimming observations the two extreme tails do not have to be in balance, as they will be thrown away. that is, the natural clt for the applied statistician can apply more generally than the probabilist’s natural clt. we are preparing for this.]"
3748,1,"['tail', 'varying']", Some Winsorized Variance Comparisons ,seg_253,theorem 5.1 (tail comparisons for regularly varying qfs) suppose the qf k is in the class d̃ ≡ {all qfs k : ṽ (·) ∈ r−β for some β ≥ 0}. suppose further that the following two conditions hold:
3749,0,[], Some Winsorized Variance Comparisons ,seg_253,then all of the conclusions (24)–(28) hold.
3750,1,"['symmetric', 'case', 'inequality']", Some Winsorized Variance Comparisons ,seg_253,"proof. because of inequality 5.4, we need only establish (28). we suppose that 0 < a, a′ < ao ≡ (a. ∧ (1 − a.)). we assume a′ < a in deriving an inequality (the other case of a < a′ is symmetric). let ṽ (a) ≡ ṽ (a, a), etc. now,"
3751,0,[], Some Winsorized Variance Comparisons ,seg_253,definition 5.2 (uniformity class ku) consider the uniformity classes ku of qfs defined by
3752,1,['varying'], Some Winsorized Variance Comparisons ,seg_253,where q is any class of qfs q contained in l = r0 that are uniformly slowly varying at 0 and at 1.
3753,0,[], Some Winsorized Variance Comparisons ,seg_253,"theorem 5.2 (uniformity class ku) as a ∨ a′ → 0, we have for either of the classes ku above that"
3754,0,[], Some Winsorized Variance Comparisons ,seg_253,thus all of (24)–(28) hold uniformly over such classes ku.
3755,1,"['law of large numbers', 'estimators', 'moments', 'central limit theorem', 'tail', 'variance', 'limit']", Inequalities for Winsorized Quantile Functions ,seg_255,"the key to the smoothly functioning power of the following inequalities is the formulation of the tail terms in (3) and (7) below. the inequalities look clumsy, but they work extremely efficiently. note that the upper bounds that appear in equations (1) and (6) do not depend at all on the unknown qf k. roughly, parts (i), (ii), and (iii) prepare for the central limit theorem (clt), the law of large numbers (lln) (whether for second moments or for first moments), and general integral variance estimators. [if σ̃n = 0, then just multiply through"
3756,0,[], Inequalities for Winsorized Quantile Functions ,seg_255,by this symbol. we agree that ∫a
3757,0,[], Inequalities for Winsorized Quantile Functions ,seg_255,"b ≡ ∫(a,b) throughout the course of this section.] use the"
3758,0,[], Inequalities for Winsorized Quantile Functions ,seg_255,notation of section 7.5. all qfs are assumed to be nondegenerate.
3759,1,['variance'], Inequalities for Winsorized Quantile Functions ,seg_255,inequality 6.1 (winsorized variance inequalities) let k(·) be arbitrary. let 0 < c ≤ an < 1 − an′ ≤ 1 − c′ < 1. let 1/n ≤ r/n ≤ 1 ≤ 1 − r′/n ≤ 1 − 1/n.
3760,0,[], Inequalities for Winsorized Quantile Functions ,seg_255,2 ). the following statements hold:
3761,1,"['estimation', 'distribution function', 'distribution', 'function', 'variance']", Inequalities for Winsorized Quantile Functions ,seg_255,"(ii) for the wlln and variance estimation fix ν ∈ (0, 1). recall (6.5.7) for the distribution function k̄n"
3762,1,"['estimation', 'variance']", Inequalities for Winsorized Quantile Functions ,seg_255,(iii) for variance estimation involving double integrals let
3763,1,['set'], Inequalities for Winsorized Quantile Functions ,seg_255,"where we set 0 ≤ ν0, ν1 < 1"
3764,1,"['estimated', 'variances']", Inequalities for Winsorized Quantile Functions ,seg_255,2 with 0 < ν0 + ν1 < 1 (to “dominate” the difference between the (“limiting” and estimated variances). then
3765,1,['inequality'], Inequalities for Winsorized Quantile Functions ,seg_255,"var[k̃(ξ)]. integration by parts, then cauchy–schwarz, and the elementary inequality (|a| + |b| + |c|)2 ≤ (3max(|a|, |b|, |c|))2 ≤ 9(a2 + b2 + c2) give (with ã. ≡ inf{t : k(t) − μ̃ ≥ 0}) that"
3766,0,[], Inequalities for Winsorized Quantile Functions ,seg_255,integration by parts gives (5) via
3767,1,"['symmetric', 'tail']", Inequalities for Winsorized Quantile Functions ,seg_255,"letting (c, r/n] ≡ (c, an] in (12), we obtain (2). there is a symmetric right tail result."
3768,0,[], Inequalities for Winsorized Quantile Functions ,seg_255,"consider part (iii). now,"
3769,1,['symmetry'], Inequalities for Winsorized Quantile Functions ,seg_255,and this bound extends to an ≤ t ≤ s ≤ r/n by symmetry. thus
3770,1,['symmetric'], Inequalities for Winsorized Quantile Functions ,seg_255,"and the integral over (an, 1 − a′n) × (an, r/n] gives the same value. whenever 1 − r′/n ≤ t ≤ 1 − an′ , then both the integral over (an, 1 − an′ ) × [1 − r′/n, 1 − an′ ) and that over [1− r′/n, 1− a′n) ×(an, 1− a′n) give equal values symmetric to (e). adding the four bounds, together with (6), then gives (7)."
3771,0,[], Inequalities for Winsorized Quantile Functions ,seg_255,"1+2ν , and let en ≡ ln/n. show that for every nondegenerate qf k we necessarily have"
3772,1,['tails'], Inequalities for Winsorized Quantile Functions ,seg_255,(thus the tails will be separated from the vast middle by a vanishingly small piece that is inconsequential.)
3773,1,['consequences'],Remarks,seg_257,"i would like to use this discussion of the literature to say a very heartfelt “thank you!” to a number of people who have figured prominently in my professional life. especially, i want to thank my professors fred andrews (university of oregon), donald truax (university of oregon), and lincoln moses (stanford university), whose voluntary efforts on my behalf had far-reaching consequences on most aspects of my life. i shall offer some thoughts on my own personal history as well as the subject matter of this book. my view is strongly affected by how i came to learn about these things. others have undoubtedly had different experiences."
3774,1,['probability'],Remarks,seg_257,"measure theory this text begins with five chapters devoted to measure theory. halmos (1950) has had a major influence on what future books on measure theory and real analysis would contain and how they would present the subject. other books on measure theory and real analysis that i have found to be especially useful include royden (1963), hewitt and stromberg (1965), rudin (1966), and the nicely simplified presentation of bartle (1966). many theorems in this introductory part are to some degree recognizable from several of these sources (and/or from the other sources listed in the probability section below). certainly, halmos’s book was a popular one while i was getting my m.s. degree in mathematics at the university of oregon, 1960–1962. my own introduction to “real and abstract analysis” came from a beautiful course taught by karl stromberg. later, edwin hewitt was a colleague at the university of washington. so it is a real pleasure for me to cite their work at various points. lou ward taught the topology course that i took at oregon. he gave us a list of theorems, and we had to come up with proofs and present them. that was the most fun i ever had in the classroom. a good deal of appendix b reflects what i learned in his course. kelly (1955), copson (1968), and housain (1977) are useful published sources. watching over the oregon graduate students in mathematics was andrew moursand, chairman. he really cared about all of us, and i owe him my thanks."
3775,1,"['treatment', 'probability theory', 'probability', 'processes', 'rates']",Remarks,seg_257,"probability loève’s (1977–78, originally 1955) presentation has been a very influential work on probability, certainly from the pedagogical point of view. to me, it refines and specializes much general analysis to probability theory, and then treats a broad part of this subject. clearly, many learned probability from his text. also, many seem to follow notational conventions used in his book. but i was rather late in learning from it. my original training was at stanford from lectures that became chung (1974), and those lectures also reflected chung’s efforts regarding translation of gnedenko and kolmogorov (1954). i truly enjoyed chung’s course, and his book. breiman’s (1968) style coincided most closely with my own. i particularly liked his treatment of partial sum and empirical processes, as one would suspect from my own research. i have sometimes used his text as a “permanent reference” to stand beside my own notes in my courses on probability theory. my choice of notation has been most influenced by loève and breiman. feller (1966) has a different flavor from most probability texts, and it includes various interesting approaches not found elsewhere. and it is informative on rates"
3776,1,"['gamma', 'approximation', 'conditional', 'expectation', 'probability', 'conditional probability', 'function', 'gamma function']",Remarks,seg_257,"of approximation. billingsley (1968) created some excitement and spawned much interesting work, and a bit of that is included here. doob’s (1954) work on martingales has had a huge influence on the subject. i had the privilege of sitting in on a course on martingales taught at the university of washington about 1968 by visiting professor ito. i find meyer (1966) and hall and heyde (1980) particularly significant. lectures by tom fleming that led to fleming and harrington (1991) sparked part of my martingale presentation here. whittaker and watson (1963) is still a superb source for the gamma function. lehmann (1959) has greatly influenced my view of conditional probability and expectation. this brings me back to the university of oregon, and to fred andrews. fred “recruited me to statistics” and then taught a year-long course out of lehmann’s book (even though i was the only student), and he was one of those who lined me up for a national science foundation fellowship that made it possible for me to go to stanford university. don truax also figured heavily in this. he cared about me, and i learned a lot from him. thank you both!"
3777,1,"['statistics', 'central limit theorem', 'nonparametric statistics', 'limit']",Remarks,seg_257,"the scene shifts southward. my years at stanford were very fruitful, and i met some fine people. ingram olkin is fun and a good teacher, and he went out of his way to be helpful to me. the multivariate topics in appendix a represent things i learned from him. lincoln moses was my thesis advisor. this relationship grew out of a course in nonparametric statistics that i took from him. one of the topics in his course was charles stein’s approach to the central limit theorem. lin spoke on it for three days, even though he had to leave a couple of well-acknowledged gaps in his presentation—because he believed it was good work. that gave me a profound respect for him as a teacher. lin was also my assigned advisor when i arrived at stanford. his second sentence to me was, “ok, shorack, what’s important to you in life”? my answer had a lot to do with the geography of the pacific northwest. two months before i graduated he responded on my behalf to a university of washington opening. wow!"
3778,1,"['functions', 'quantile', 'trimmed means', 'gamma', 'association', 'statistics', 'central limit theorem', 'probability', 'varying', 'limit']",Remarks,seg_257,"at washington i had a chance to teach courses in probability and statistics. and i learned a lot from my association with ron pyke, and later with jon wellner. the presentations in parts of chapters 12 and 14 reflect this to varying degrees. fritz scholz got me started on gamma approximations in the central limit theorem. likewise, work with david mason on quantile functions, embedding, and trimmed means is reflected in parts of chapters 6 and appendix c. i offer them all my thanks."
3779,1,['sampling'],Remarks,seg_257,"obviously, i also owe a huge debt to “the literature” in regard to all these topics, and i will list some of those sources below. however, this is a textbook. it is not a research monograph. my emphasis is on presentation, not attribution. often, my citation concerns where i learned something rather than who did it originally. and in some areas (especially, chapters 6 and 12) i have offered only a sampling of the citations that i could have given. moreover, i have often chosen to cite a book instead of an original paper. my own work is cited “too heavily” because it is written in the same style and notation as this book."
3780,1,"['probability theory', 'statistical', 'probability']",Remarks,seg_257,"the bibliography contains quite a number of other books on probability theory, and many are very good books. but it is the ones listed above that have had the most influence on me. i hope that the reader will find that my book also has a somewhat different flavor—a statistical flavor. that flavor will be enhanced if you think of chapters 16 and 17 and the first appendix of the original 2000 edition as part of the total package."
3781,0,[],Remarks,seg_257,"special thanks to chari boehnke, roger and heather shorack, the michael boehnke family, the barbara aalund family, kathleen shorack, david mason, michael perlman, fritz and roberta scholz, jon wellner, the jan beirlant family, piet groeneboom, frits ruymgaart, derek dohn, and pauline reed for enabling me to write this book."
3782,1,"['probability theory', 'independence', 'probability']",References,seg_259,"chapters 1–5 and appendix b ash, r. (1972) real analysis and probability. academic press, new york. bartle, r. (1966) the elements of integration. john wiley & sons, new york. breiman, l. (1968) probability. addision-wesley, reading, ma. chow, y. and h. teicher (1997) probability theory: independence, interchangeability, martingales 3rd ed. springer-verlag, new york. cohn, d. (1980) measure theory. birkhäuser, boston. copson, e. (1968) metric spaces. cambridge university press, cambridge. halmos, p. (1950) measure theory. van nostrand, princeton, nj. hewitt, e. and k. stromberg (1965) real and abstract analysis. springer-verlag, new york. husain, t. (1977) topology and maps. plenum press, new york. loève, m. (1977–78) probability theory. springer-verlag, new york. meyer, p. (1966) probability and potentials. blaisdell, waltham, ma. royden, h. (1967) real analysis. macmillan, new york. rudin, w. (1966) real and complex analysis. mcgraw-hill, new york."
3783,1,"['statistics', 'probability', 'random', 'process', 'inequality', 'sample', 'approximation', 'probability theory', 'exponential', 'limit', 'stochastic processes', 'asymptotic', 'distribution', 'stationary', 'probabilistic', 'convergence', 'processes', 'quantile', 'random variables', 'independent', 'probability measures', 'variables', 'normal']",References,seg_259,"chapters 6, 12, 15 and appendix c billingsley, p. (1968) convergence of probability measures. john wiley & sons, new york. breiman, l. (1968) probability. addision-wesley, reading, ma. chung, k. (1974) a course in probability theory 2nd ed. academic press, new york. cramér, h. and m. leadbetter (1967) stationary and related stochastic processes. john wiley & sons, new york. csörgö, m., s. csörgö, l. horváth, and d. mason (1986) what portion of the sample makes a partial sum asymptotically stable or normal? probab. theory related fields 72, 1–16. csörgö m., s. csörgö, l. horváth, and d. mason (1986) weighted empirical and quantile processes. ann. probab. 14, 31–85. csörgö, s., e. haeusler, and d. mason (1988) the asymptotic distribution of trimmed sums. ann. probab. 16, 672–699. csörgö s., e. haeusler, and d. mason (1989) a probabilistic approach to the asymptotic distribution of sums of independent identically distributed random variables. adv. in appl. math. 9, 259–333. doob, j. (1949) heuristic approach to the kolmogorov–smirnov theorems. ann. math. statist. 20, 393–403. freedman, d. (1971) brownian motion and diffusion. holden-day, san fran. haeusler, e. and d. mason (1989) a law of the iterated logarithm for modulus trimming. colloquia math. soc. j. bolyai, 57, limit theorems in probability and statistics. mason, d. (2001) an exponential inequality for a weighted approximation to the uniform empirical process with applications. institute of mathematical statistics: lecture notesmonograph series 36, 477–498. mason, d. and g. shorack (1990) necessary and sufficient conditions for asymptotic normality of trimmed l-statistics. j. statist. plan. inf. 25, 111–139."
3784,1,"['statistics', 'probability', 'process', 'sample', 'linear', 'rate', 'data', 'permutation', 'tests', 'limit', 'functions', 'condition', 'asymptotic', 'permutation tests', 'processes', 'quantile', 'sampling', 'normality', 'order statistics']",References,seg_259,"mason, d. and g. shorack (1992) necessary and sufficient conditions for asymptotic normality of l-statistics. ann. probab. 20, 1779–1804. meyer, p. (1966) probability and potentials. blaisdell, waltham, ma. o’brien, r. (1980) a limit theorem for sample maxima and heavy branches in galton– watson trees. j. appl. prob. 17539–545. ross, s. (1997) an introdaction to probability models, 6th ed., academic press, san diego. shorack, g. (1972) functions of order statistics. ann. math. statist. 43, 412–427. shorack, g. (1998) the asymptotic normality condition. univ. washington dept. of statistics technical reports 323, 336, 337, and 338. shorack, g. (1991) embedding the finite sampling process at a rate. ann. probability 19, 826–842. shorack, g. (1996) linear rank statistics, finite sampling, permutation tests and winsorizing. ann. statist. 24, 1371–1385. shorack, g. (1997a) inequalities for quantile functions with a uniform studentized clt that includes trimming. nonparametric statist. 8, 307–335. shorack, g. (1997b) uniform clt, wlln, lil and bootstrapping in a data analytic approach to trimmed l-statistics. j. statist. plan. inf. 60, 1–44. shorack, g. (1998) applications of weighted approximations via quantile inequalities. asymptotic methods in probability and statistics, a volume in honor of miklós csörgő. b. szyszkowicz, ed., 151–167, elsevier, amsterdam. shorack, g. and j. wellner (1986) empirical processes with applications to statistics. john wiley & sons, new york."
3785,1,"['testing statistical', 'statistical hypotheses', 'hypotheses', 'probability', 'statistical']",References,seg_259,"chapter 7 breiman, l. (1968) probability. addision-wesley, reading, ma. lehmann, e. (1959) testing statistical hypotheses. john wiley& sons, new york."
3786,1,"['statistics', 'independence', 'probability', 'random', 'probability theory', 'independent random variables', 'standard', 'limit', 'distributions', 'standard normal', 'random variables', 'independent', 'variables', 'normal']",References,seg_259,"chapter 8 breiman, l. (1968) probability. addision-wesley, reading, ma. chow, y. and h. teicher (1997) probability theory. independence, interchangeability, martingales 3rd ed. springer-verlag, new york. chung, k. (1974) a course in probability theory 2nd ed. academic press, new york. feller, w. (1966) an introduction to probability theory and its applications. vol. 2. john wiley & sons, new york. giné, e., f. gótze, and d. mason (1998) when is the student t-statistic asymptotically standard normal? ann. probab. 25, 1514–1531. gnedenko, b. and a. kolmogorov (1954) limit distributions for sums of independent random variables. addison-wesley, cambridge, mass. hoffman-jorgensen, j. (1974) sums of independent banach space valued random variables. studia math. 52, 159–186. kallenberg, o. (1997) foundations of modern probability. springer-verlag, new york. loève, m. (1977–78) probability theory. springer-verlag, new york. resnick, s. (1999) a probability path. birkhäuser, boston. strassen, v. (1967). almost sure behavior of sums of independent random variables and martingales. proceeding of the fifth berkeley symposium on mathematical statistics and probability 2, 315–343, university of california press, berkeley, ca."
3787,1,['statistical'],References,seg_259,"chapters 9–11 and appendix a ahlfors, l. (1979) complex analysis mcgraw-hill, inc., new york. anderson, t. (1984) introduction to multivariate statistical analysis. john wiley & sons., new york."
3788,1,"['normal approximation', 'statistics', 'probability', 'random', 'linear', 'bootstrap', 'approximation', 'probability theory', 'independent random variables', 'mean', 'standard', 'statistical', 'limit', 'distributions', 'functions', 'statistical inference', 'asymptotic', 'estimation', 'standard normal', 'distribution', 'continuous', 'point estimation', 'random variables', 'independent', 'variables', 'normal']",References,seg_259,"bickel, p. and k. doksum (1977) mathematical statistics: basic ideas and special topics. holden-day, san francisco. bhattacharya, r. and r. rao (1976) normal approximation and asymptotic expansions. john wiley & sons., new york. billingsley, p. (1986) probability and measure. john wiley & sons, new york. breiman, l. (1968) probability. addision-wesley, reading, ma. breiman, l. (1968) probability. addision-wesley, reading, ma. csörgö, s. and d. mason (1989) bootstrapping empirical functions. ann. statist. 17, 1447–1471. durrett, r. (1996). probability: theory and examples. wadsworth, belmont, ca. feller, w. (1966) an introduction to probability theory and its applications. vol. 2. john wiley & sons, new york. galambos, j. (1995) advanced probability theory 2nd ed. marcel dekker, new york. giné, e., f. götze, and d. mason (1997) when is the student t-statistic asymptotically standard normal? ann. probab. 25, 1514–1531. giné, e. and j. zinn (1990) necessary conditions for the bootstrap of the mean. ann. statist. 17, 684–691. gnedenko, b. and a. kolmogorov (1954) limit distributions for sums of independent random variables. addison-wesley, cambridge, mass. hall, p. (1983) chi squared approximations to the distribution of a sum of independent random variables. ann. statist. 11, 1028–1036. hall, p. (1990) asymptotic properties of the bootstrap for heavy tailed distributions. ann. statist. 18, 1342–1360. johnson, n., a. kotz, and n. balakrishnan (1994) continuous univariate distributions volumes 1 and 2. john wiley & sons, new york. kendall, m. and a. stuart (1977) the advanced theory of statistics vol. 2. macmillan, new york. lehmann, e. and g. casella (1998) theory of point estimation 3rd ed. springer-verlag, new york. lévy, p. (1937) théorie de l’addition des variables aléatoires. bautier-villars, paris. loève, m. (1977–78) probability theory. springer-verlag, new york. mason, d. and g. shorack (1992) necessary and sufficient conditions for asymptotic normality of l-statistics. ann. probab. 20, 1799–1804. petrov, v. (1975) sums of independent random variables. springer-verlag, new york. rao, c. (1965) linear statistical inference and its applications 2nd ed. john wiley & sons, new york. shiryayev, a. (1984) probability. springer-verlag, new york."
3789,1,"['random processes', 'statistics', 'processes', 'statistical', 'probability', 'random', 'limit', 'stochastic processes']",References,seg_259,"chapter 13 andersen, p., o. borgan, r. gill, and n. keiding (1993) statistical models based on counting processes. springer-verlag, new york. breiman, l. (1968) probability. addision-wesley, reading, ma. doob, j. (1953) stochastic processes. john wiley & sons, new york. fleming, t. and d. harrington (1991) counting processes and survival analysis. john wiley & sons, new york. hall, p. and c. heyde (1980) martingale limit theory and its application. academic press, new york. liptser, r. and a. shiryaev (1977) statistics of random processes i. springer-verlag, new york. meyer, p. (1966) probability and potentials. blaisdell, waltham, ma. williams, d. (1991) probability with martingales. cambridge university press, cambridge."
3790,1,"['probability measures', 'processes', 'statistical', 'probability', 'convergence', 'limit', 'stochastic processes']",References,seg_259,"chapter 14 billingsley, p. (1968) convergence of probability measures. john wiley & sons, new york. dudley, r. (1976) convergence of laws on metric spaces, with a view to statistical testing. matematisk institut aarhus universitet. dudley, r. (1989) real analysis and probability. wadsworth & brooks/cole, pacific grove, ca. skorokhod, a. (1956) limit theorems for stochastic processes. theor. probab. appls. 1, 261–290."
