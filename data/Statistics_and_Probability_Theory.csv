,Relevance,Tags,Heading,Seg,Sentence,Enc Relevance,Enc Tags,Enc Heading,Enc Sentence
0,1,"['functions', 'risk']",Chapter  Engineering Decisions Under Uncertainty,seg_1,lecture 1 (aim of the present lecture) the aim of the present lecture is to introduce the problem context of societal decision making and to outline how the concept of risk may provide a means for rational decisions in engineering. focus is directed to the understanding of the role of the engineer for the development and maintenance of societal functions. on the basis of the lecture it is expected that the reader will acquire knowledge on the following issues:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3127,  3330,  6567,  2104, 12503])","tensor([  101,  8835,  1015,  1006,  6614,  1997,  1996,  2556,  8835,  1007,
         1996,  6614,  1997,  1996,  2556,  8835,  2003,  2000,  8970,  1996,
         3291,  6123,  1997, 23382,  3247,  2437,  1998,  2000, 12685,  2129,
         1996,  4145,  1997,  3891,  2089,  3073,  1037,  2965,  2005, 11581,
         6567,  1999,  3330,  1012,  3579,  2003,  2856,  2000,  1996,  4824,
         1997,  1996,  2535,  1997,  1996,  3992,  2005,  1996,  2458,  1998,
         6032,  1997, 23382,  4972,  1012,  2006,  1996,  3978,  1997,  1996,
         8835,  2009,  2003,  3517,  2008,  1996,  8068,  2097,  9878,  3716,
         2006,  1996,  2206,  3314,  1024,   102])"
1,1,"['probability', 'uncertainties', 'consequences', 'risk']",Chapter  Engineering Decisions Under Uncertainty,seg_1,"• what is sustainability? • what is the role of engineering in society? • how can aspects of sustainability be related to life safety and cost optimal decision making? • what are the main different types of consequences to be considered in risk assessment? • why do possible conflicts exist between economy, safety and environment? • why is engineering decision making influenced by uncertainties? • what is the role of probability and consequence in decision making? • what is the definition of risk? • which are the main phases to be considered in life cycle risk assessments in engineering decision making?",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 3127,  3330,  6567,  2104, 12503])","tensor([  101,  1528,  2054,  2003, 15169,  1029,  1528,  2054,  2003,  1996,
         2535,  1997,  3330,  1999,  2554,  1029,  1528,  2129,  2064,  5919,
         1997, 15169,  2022,  3141,  2000,  2166,  3808,  1998,  3465, 15502,
         3247,  2437,  1029,  1528,  2054,  2024,  1996,  2364,  2367,  4127,
         1997,  8465,  2000,  2022,  2641,  1999,  3891,  7667,  1029,  1528,
         2339,  2079,  2825,  9755,  4839,  2090,  4610,  1010,  3808,  1998,
         4044,  1029,  1528,  2339,  2003,  3330,  3247,  2437,  5105,  2011,
         9662,  7368,  1029,  1528,  2054,  2003,  1996,  2535,  1997,  9723,
         1998,  9509,  1999,  3247,  2437,  1029,  1528,  2054,  2003,  1996,
         6210,  1997,  3891,  1029,  1528,  2029,  2024,  1996,  2364, 12335,
         2000,  2022,  2641,  1999,  2166,  5402,  3891, 20794,  1999,  3330,
         3247,  2437,  1029,   102])"
2,1,['sustainable'], Introduction,seg_3,"during the last two decades, there has been a growing awareness that our world only has limited non-renewable natural resources such as energy and materials but also limited renewable resources such as drinking water, clean air etc. this led the brundtland commission in 1987 to the conclusion that sustainable development is defined as a development “that meets the needs of the present without compromis-",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])",tensor([4955]),"tensor([  101,  2076,  1996,  2197,  2048,  5109,  1010,  2045,  2038,  2042,
         1037,  3652,  7073,  2008,  2256,  2088,  2069,  2038,  3132,  2512,
         1011, 13918,  3019,  4219,  2107,  2004,  2943,  1998,  4475,  2021,
         2036,  3132, 13918,  4219,  2107,  2004,  5948,  2300,  1010,  4550,
         2250,  4385,  1012,  2023,  2419,  1996,  7987,  8630, 19270,  3222,
         1999,  3055,  2000,  1996,  7091,  2008,  9084,  2458,  2003,  4225,
         2004,  1037,  2458,  1523,  2008,  6010,  1996,  3791,  1997,  1996,
         2556,  2302,  4012, 21572, 15630,  1011,   102])"
3,1,"['design', 'joint', 'sustainable']", Introduction,seg_3,"ing the ability of future generations to meet their own needs”. sustainable decision making is thus presently understood as based on a joint consideration of society, economy and environment. in regard to environmental impacts the immediate implications for the planning, design and operation of civil engineering infrastructures are clear: save energy, save non-renewable resources and find out about re-cycling of building materials, do not pollute the air, water or soil with toxic substances, save or even regain arable land and much more.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([4955]),"tensor([  101, 13749,  1996,  3754,  1997,  2925,  8213,  2000,  3113,  2037,
         2219,  3791,  1524,  1012,  9084,  3247,  2437,  2003,  2947, 12825,
         5319,  2004,  2241,  2006,  1037,  4101,  9584,  1997,  2554,  1010,
         4610,  1998,  4044,  1012,  1999,  7634,  2000,  4483, 14670,  1996,
         6234, 13494,  2005,  1996,  4041,  1010,  2640,  1998,  3169,  1997,
         2942,  3330,  6502,  2015,  2024,  3154,  1024,  3828,  2943,  1010,
         3828,  2512,  1011, 13918,  4219,  1998,  2424,  2041,  2055,  2128,
         1011,  9670,  1997,  2311,  4475,  1010,  2079,  2025,  8554, 10421,
         1996,  2250,  1010,  2300,  2030,  5800,  2007, 11704, 13978,  1010,
         3828,  2030,  2130, 12452,  5424,  2571,  2455,  1998,  2172,  2062,
         1012,   102])"
4,1,['replacement'], Introduction,seg_3,"for civil engineering infrastructure and facilities in general the financial aspect is also of crucial importance. civil engineering infrastructure is financed by the public via taxes, public charges or other. in the end, it is the individuals of society who pay and, of course, also enjoy the benefits derived from their existence. however, seen in the light of the conclusions of the brundtland report (brundtland [5]), intergenerational equity must be accounted for. our generation must not leave the burden of maintenance or replacement of too short-lived structures to future generations and it must not use more of the financial resources than those really available. in this sense, civil engineering facilities should be optimal not only from a technological point of view but also from a sustainability point of view.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([4955]),"tensor([  101,  2005,  2942,  3330,  6502,  1998,  4128,  1999,  2236,  1996,
         3361,  7814,  2003,  2036,  1997, 10232,  5197,  1012,  2942,  3330,
         6502,  2003, 13790,  2011,  1996,  2270,  3081,  7773,  1010,  2270,
         5571,  2030,  2060,  1012,  1999,  1996,  2203,  1010,  2009,  2003,
         1996,  3633,  1997,  2554,  2040,  3477,  1998,  1010,  1997,  2607,
         1010,  2036,  5959,  1996,  6666,  5173,  2013,  2037,  4598,  1012,
         2174,  1010,  2464,  1999,  1996,  2422,  1997,  1996, 15306,  1997,
         1996,  7987,  8630, 19270,  3189,  1006,  7987,  8630, 19270,  1031,
         1019,  1033,  1007,  1010,  6970,  6914, 16754,  2389, 10067,  2442,
         2022, 14729,  2005,  1012,  2256,  4245,  2442,  2025,  2681,  1996,
        10859,  1997,  6032,  2030,  6110,  1997,  2205,  2460,  1011,  2973,
         5090,  2000,  2925,  8213,  1998,  2009,  2442,  2025,  2224,  2062,
         1997,  1996,  3361,  4219,  2084,  2216,  2428,  2800,  1012,  1999,
         2023,  3168,  1010,  2942,  3330,  4128,  2323,  2022, 15502,  2025,
         2069,  2013,  1037, 10660,  2391,  1997,  3193,  2021,  2036,  2013,
         1037, 15169,  2391,  1997,  3193,  1012,   102])"
5,1,"['cost benefit analysis', 'decision problem']", Introduction,seg_3,"it is in general a concern how society may maintain and even improve the quality of life. all activities in society should thus aim at improving the life expectancy and increasing the gross domestic product (gdp), resulting in the conclusion that investments into life saving activities must be in balance with the resulting increase in life expectancy. for the present, it is just stated that this problem constitutes a decision problem that can be analyzed using cost benefit analysis (see chap. 7).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])",tensor([4955]),"tensor([  101,  2009,  2003,  1999,  2236,  1037,  5142,  2129,  2554,  2089,
         5441,  1998,  2130,  5335,  1996,  3737,  1997,  2166,  1012,  2035,
         3450,  1999,  2554,  2323,  2947,  6614,  2012,  9229,  1996,  2166,
         5987, 11656,  1998,  4852,  1996,  7977,  4968,  4031,  1006, 14230,
         1007,  1010,  4525,  1999,  1996,  7091,  2008, 10518,  2046,  2166,
         7494,  3450,  2442,  2022,  1999,  5703,  2007,  1996,  4525,  3623,
         1999,  2166,  5987, 11656,  1012,  2005,  1996,  2556,  1010,  2009,
         2003,  2074,  3090,  2008,  2023,  3291, 17367,  1037,  3247,  3291,
         2008,  2064,  2022, 16578,  2478,  3465,  5770,  4106,  1006,  2156,
        15775,  2361,  1012,  1021,  1007,  1012,   102])"
6,1,['risk'], Introduction,seg_3,"at present, approximately 10 to 20% of the gdp of developed countries is being re-invested into life saving activities such as public health, risk reduction and safety. for example, the economic burden of degradation of infrastructure amounted to about 10% of the gdp for the usa in 1997 (see alsalam et al. [1]). from these numbers it becomes apparent that the issue of safety and well being of the individuals in society as well as the durability of infrastructure facilities has a high importance for the performance of society and the quality of life.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])",tensor([4955]),"tensor([  101,  2012,  2556,  1010,  3155,  2184,  2000,  2322,  1003,  1997,
         1996, 14230,  1997,  2764,  3032,  2003,  2108,  2128,  1011, 11241,
         2046,  2166,  7494,  3450,  2107,  2004,  2270,  2740,  1010,  3891,
         7312,  1998,  3808,  1012,  2005,  2742,  1010,  1996,  3171, 10859,
         1997, 16627,  1997,  6502, 18779,  2000,  2055,  2184,  1003,  1997,
         1996, 14230,  2005,  1996,  3915,  1999,  2722,  1006,  2156, 25520,
         7911,  2213,  3802,  2632,  1012,  1031,  1015,  1033,  1007,  1012,
         2013,  2122,  3616,  2009,  4150,  6835,  2008,  1996,  3277,  1997,
         3808,  1998,  2092,  2108,  1997,  1996,  3633,  1999,  2554,  2004,
         2092,  2004,  1996,  4241,  2527,  8553,  1997,  6502,  4128,  2038,
         1037,  2152,  5197,  2005,  1996,  2836,  1997,  2554,  1998,  1996,
         3737,  1997,  2166,  1012,   102])"
7,1,"['design', 'case']", Introduction,seg_3,"the present book attempts to provide the basic tools for supporting decision making in the context of planning, design and maintenance of civil engineering activities and structures. engineering facilities such as bridges, power plants, dams and offshore platforms are all intended to benefit, in one way or another the quality of life of the individuals of society. therefore, whenever such a facility is planned, it is a prerequisite that the benefit of the facility can be proven considering all phases of the life of the facility, i.e. including design, manufacturing, construction, operation and eventually decommissioning. if this is not the case, clearly the facility should not be established.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])",tensor([4955]),"tensor([  101,  1996,  2556,  2338,  4740,  2000,  3073,  1996,  3937,  5906,
         2005,  4637,  3247,  2437,  1999,  1996,  6123,  1997,  4041,  1010,
         2640,  1998,  6032,  1997,  2942,  3330,  3450,  1998,  5090,  1012,
         3330,  4128,  2107,  2004,  7346,  1010,  2373,  4264,  1010, 17278,
         1998, 12195,  7248,  2024,  2035,  3832,  2000,  5770,  1010,  1999,
         2028,  2126,  2030,  2178,  1996,  3737,  1997,  2166,  1997,  1996,
         3633,  1997,  2554,  1012,  3568,  1010,  7188,  2107,  1037,  4322,
         2003,  3740,  1010,  2009,  2003,  1037,  3653,  2890, 24871,  2008,
         1996,  5770,  1997,  1996,  4322,  2064,  2022, 10003,  6195,  2035,
        12335,  1997,  1996,  2166,  1997,  1996,  4322,  1010,  1045,  1012,
         1041,  1012,  2164,  2640,  1010,  5814,  1010,  2810,  1010,  3169,
         1998,  2776, 21933,  7382, 14643,  3258,  2075,  1012,  2065,  2023,
         2003,  2025,  1996,  2553,  1010,  4415,  1996,  4322,  2323,  2025,
         2022,  2511,  1012,   102])"
8,1,['level'], Societal Decision Making and Risk,seg_5,on a societal level a beneficial engineered facility is normally understood as:,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([23382,  3247,  2437,  1998,  3891])","tensor([  101,  2006,  1037, 23382,  2504,  1037, 15189, 13685,  4322,  2003,
         5373,  5319,  2004,  1024,   102])"
9,1,['efficient'], Societal Decision Making and Risk,seg_5,• being economically efficient in serving a specific purpose,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([23382,  3247,  2437,  1998,  3891])","tensor([  101,  1528,  2108, 15318,  8114,  1999,  3529,  1037,  3563,  3800,
          102])"
10,1,['limit'], Societal Decision Making and Risk,seg_5,• fulfilling given requirements with regard to the safety of the personnel directly involved with or indirectly exposed to the facility • fulfilling given requirements to limit the adverse effects of the facility on the environment.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([23382,  3247,  2437,  1998,  3891])","tensor([  101,  1528, 21570,  2445,  5918,  2007,  7634,  2000,  1996,  3808,
         1997,  1996,  5073,  3495,  2920,  2007,  2030, 17351,  6086,  2000,
         1996,  4322,  1528, 21570,  2445,  5918,  2000,  5787,  1996, 15316,
         3896,  1997,  1996,  4322,  2006,  1996,  4044,  1012,   102])"
11,0,[], Societal Decision Making and Risk,seg_5,"based on these requirements, the ultimate task of the engineer is to make decisions or to provide the decision basis for others in order to ensure that engineered facilities are established in such a way as to provide the largest possible benefit.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([23382,  3247,  2437,  1998,  3891])","tensor([  101,  2241,  2006,  2122,  5918,  1010,  1996,  7209,  4708,  1997,
         1996,  3992,  2003,  2000,  2191,  6567,  2030,  2000,  3073,  1996,
         3247,  3978,  2005,  2500,  1999,  2344,  2000,  5676,  2008, 13685,
         4128,  2024,  2511,  1999,  2107,  1037,  2126,  2004,  2000,  3073,
         1996,  2922,  2825,  5770,  1012,   102])"
12,1,"['associated', 'decision problem']", Example Feasibility of Hydraulic Power Plant,seg_7,"consider as an example the decision problem of exploitation of hydraulic power. a hydraulic power plant project involving the construction of a water reservoir in a mountain valley is planned. the benefit of the hydraulic power plant is for simplicity assumed to be associated only with the monetary income from selling electricity to consumers. the decision problem thus is simplified to compare the costs of establishing, operating and eventually decommissioning the hydraulic power plant with the incomes to be expected during the service life of the plant. in addition, it must be ensured that the safety of the personnel involved in the construction and operation of the plant and the safety of third persons, i.e. the individuals of the society in general, is satisfactorily high.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  5136,  2004,  2019,  2742,  1996,  3247,  3291,  1997, 14427,
         1997, 14761,  2373,  1012,  1037, 14761,  2373,  3269,  2622,  5994,
         1996,  2810,  1997,  1037,  2300,  8071,  1999,  1037,  3137,  3028,
         2003,  3740,  1012,  1996,  5770,  1997,  1996, 14761,  2373,  3269,
         2003,  2005, 17839,  5071,  2000,  2022,  3378,  2069,  2007,  1996,
        12194,  3318,  2013,  4855,  6451,  2000, 10390,  1012,  1996,  3247,
         3291,  2947,  2003, 11038,  2000, 12826,  1996,  5366,  1997,  7411,
         1010,  4082,  1998,  2776, 21933,  7382, 14643,  3258,  2075,  1996,
        14761,  2373,  3269,  2007,  1996, 29373,  2000,  2022,  3517,  2076,
         1996,  2326,  2166,  1997,  1996,  3269,  1012,  1999,  2804,  1010,
         2009,  2442,  2022, 16316,  2008,  1996,  3808,  1997,  1996,  5073,
         2920,  1999,  1996,  2810,  1998,  3169,  1997,  1996,  3269,  1998,
         1996,  3808,  1997,  2353,  5381,  1010,  1045,  1012,  1041,  1012,
         1996,  3633,  1997,  1996,  2554,  1999,  2236,  1010,  2003,  2938,
         2483,  7011, 16761,  6588,  2152,  1012,   102])"
13,1,"['efficiency', 'factors']", Example Feasibility of Hydraulic Power Plant,seg_7,"different solutions for establishing the power plant may be considered and their efficiency can be measured in terms of the expected income relative to the cost of establishing the power plant. however, a number of factors are important for the evaluation of the income and the costs of establishing the power plant. these are e.g. the period of time when the plant will be operating and produce electricity and the capacity of the power plant. moreover, the future income from selling electricity will depend on the availability of water, which depends on the future snow and rainfall. further, the market situation may change and competing energy recourses such as thermal and solar power may cause a reduction of the market price on electricity in general.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  2367,  7300,  2005,  7411,  1996,  2373,  3269,  2089,  2022,
         2641,  1998,  2037,  8122,  2064,  2022,  7594,  1999,  3408,  1997,
         1996,  3517,  3318,  5816,  2000,  1996,  3465,  1997,  7411,  1996,
         2373,  3269,  1012,  2174,  1010,  1037,  2193,  1997,  5876,  2024,
         2590,  2005,  1996,  9312,  1997,  1996,  3318,  1998,  1996,  5366,
         1997,  7411,  1996,  2373,  3269,  1012,  2122,  2024,  1041,  1012,
         1043,  1012,  1996,  2558,  1997,  2051,  2043,  1996,  3269,  2097,
         2022,  4082,  1998,  3965,  6451,  1998,  1996,  3977,  1997,  1996,
         2373,  3269,  1012,  9308,  1010,  1996,  2925,  3318,  2013,  4855,
         6451,  2097, 12530,  2006,  1996, 11343,  1997,  2300,  1010,  2029,
         9041,  2006,  1996,  2925,  4586,  1998, 10101,  1012,  2582,  1010,
         1996,  3006,  3663,  2089,  2689,  1998,  6637,  2943, 28667, 22957,
         2229,  2107,  2004,  9829,  1998,  5943,  2373,  2089,  3426,  1037,
         7312,  1997,  1996,  3006,  3976,  2006,  6451,  1999,  2236,  1012,
          102])"
14,1,"['failure', 'case']", Example Feasibility of Hydraulic Power Plant,seg_7,"in addition, the different possible solutions for establishing the power plant will have different costs and different implications for the safety for persons. obviously, the more the capacity the power plant will have, the higher the dam and the larger the construction costs will be. at the same time, the potential flooding (consequence of dam failure) will be larger in case of dam failure and more people would be injured or die, see fig. 1.1.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  1999,  2804,  1010,  1996,  2367,  2825,  7300,  2005,  7411,
         1996,  2373,  3269,  2097,  2031,  2367,  5366,  1998,  2367, 13494,
         2005,  1996,  3808,  2005,  5381,  1012,  5525,  1010,  1996,  2062,
         1996,  3977,  1996,  2373,  3269,  2097,  2031,  1010,  1996,  3020,
         1996,  5477,  1998,  1996,  3469,  1996,  2810,  5366,  2097,  2022,
         1012,  2012,  1996,  2168,  2051,  1010,  1996,  4022,  9451,  1006,
         9509,  1997,  5477,  4945,  1007,  2097,  2022,  3469,  1999,  2553,
         1997,  5477,  4945,  1998,  2062,  2111,  2052,  2022,  5229,  2030,
         3280,  1010,  2156, 20965,  1012,  1015,  1012,  1015,  1012,   102])"
15,1,['level'], Example Feasibility of Hydraulic Power Plant,seg_7,"the safety of the people in a town downstream of the reservoir will also be influenced by the load carrying capacity of the dam structure relative to the pressure loading due to the water level in the reservoir. the strength of the dam structure depends in turn on the material characteristics of the dam structure and the properties of the soil and rock on which it is founded. as these properties are subject to uncertainty of various sources as shall be seen later, the load carrying capacity relative to",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  1996,  3808,  1997,  1996,  2111,  1999,  1037,  2237, 13248,
         1997,  1996,  8071,  2097,  2036,  2022,  5105,  2011,  1996,  7170,
         4755,  3977,  1997,  1996,  5477,  3252,  5816,  2000,  1996,  3778,
        10578,  2349,  2000,  1996,  2300,  2504,  1999,  1996,  8071,  1012,
         1996,  3997,  1997,  1996,  5477,  3252,  9041,  1999,  2735,  2006,
         1996,  3430,  6459,  1997,  1996,  5477,  3252,  1998,  1996,  5144,
         1997,  1996,  5800,  1998,  2600,  2006,  2029,  2009,  2003,  2631,
         1012,  2004,  2122,  5144,  2024,  3395,  2000, 12503,  1997,  2536,
         4216,  2004,  4618,  2022,  2464,  2101,  1010,  1996,  7170,  4755,
         3977,  5816,  2000,   102])"
16,1,"['probability', 'failure']", Example Feasibility of Hydraulic Power Plant,seg_7,"the loading may be expressed in terms of the probability that the loading will exceed the load carrying capacity, or equivalently, the probability of dam failure.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  1996, 10578,  2089,  2022,  5228,  1999,  3408,  1997,  1996,
         9723,  2008,  1996, 10578,  2097, 13467,  1996,  7170,  4755,  3977,
         1010,  2030,  5662,  2135,  1010,  1996,  9723,  1997,  5477,  4945,
         1012,   102])"
17,1,"['level', 'range']", Example Feasibility of Hydraulic Power Plant,seg_7,"finally, the environmental impact of the power plant will depend on the water level in the reservoir: the higher the water level, the more land will be flooded upstream of the dam structure and various habitats for animals and birds will be destroyed. on the other hand, the water reservoir itself will provide a living basis for new species of fish and birds and may provide a range of recreational possibilities for people such as sailing and fishing.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  2633,  1010,  1996,  4483,  4254,  1997,  1996,  2373,  3269,
         2097, 12530,  2006,  1996,  2300,  2504,  1999,  1996,  8071,  1024,
         1996,  3020,  1996,  2300,  2504,  1010,  1996,  2062,  2455,  2097,
         2022, 10361, 13909,  1997,  1996,  5477,  3252,  1998,  2536, 10746,
         2005,  4176,  1998,  5055,  2097,  2022,  3908,  1012,  2006,  1996,
         2060,  2192,  1010,  1996,  2300,  8071,  2993,  2097,  3073,  1037,
         2542,  3978,  2005,  2047,  2427,  1997,  3869,  1998,  5055,  1998,
         2089,  3073,  1037,  2846,  1997, 10517, 12020,  2005,  2111,  2107,
         2004,  8354,  1998,  5645,  1012,   102])"
18,1,"['factors', 'level']", Example Feasibility of Hydraulic Power Plant,seg_7,"in order to evaluate whether or not the power plant is feasible it is useful to make a list of the various factors influencing the benefit and their effects. as the problem may be recognized to be rather complex, only the interrelation of the water level in the reservoir with the following factors will be considered: the load carrying capacity of the dam structure, the costs of constructing the dam structure and the implications on the safety of the people living in a town down-stream the power plant.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  1999,  2344,  2000, 16157,  3251,  2030,  2025,  1996,  2373,
         3269,  2003, 22945,  2009,  2003,  6179,  2000,  2191,  1037,  2862,
         1997,  1996,  2536,  5876, 25870,  1996,  5770,  1998,  2037,  3896,
         1012,  2004,  1996,  3291,  2089,  2022,  3858,  2000,  2022,  2738,
         3375,  1010,  2069,  1996,  6970, 16570,  3370,  1997,  1996,  2300,
         2504,  1999,  1996,  8071,  2007,  1996,  2206,  5876,  2097,  2022,
         2641,  1024,  1996,  7170,  4755,  3977,  1997,  1996,  5477,  3252,
         1010,  1996,  5366,  1997, 15696,  1996,  5477,  3252,  1998,  1996,
        13494,  2006,  1996,  3808,  1997,  1996,  2111,  2542,  1999,  1037,
         2237,  2091,  1011,  5460,  1996,  2373,  3269,  1012,   102])"
19,1,"['table', 'factors']", Example Feasibility of Hydraulic Power Plant,seg_7,"from table 1.1 which is clearly a simplified summary of the complex interrelations of the various factors influencing the benefit of realizing the power plant, it is seen that the various factors have different influences and that the different attributes",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  2013,  2795,  1015,  1012,  1015,  2029,  2003,  4415,  1037,
        11038, 12654,  1997,  1996,  3375,  6970, 16570, 10708,  1997,  1996,
         2536,  5876, 25870,  1996,  5770,  1997,  9301,  1996,  2373,  3269,
         1010,  2009,  2003,  2464,  2008,  1996,  2536,  5876,  2031,  2367,
         8092,  1998,  2008,  1996,  2367, 12332,   102])"
20,1,"['consequences', 'risk', 'decision problem', 'probability', 'table', 'failure', 'level', 'factors', 'combinations', 'case']", Example Feasibility of Hydraulic Power Plant,seg_7,"such as income, costs and safety are conflicting. in the table it is assumed that the medium load carrying capacity of the dam structure corresponds to a medium probability of dam failure but, of course, other combinations are also possible. consider the case with a high water level in the reservoir. in this case, the potential income is large but the costs of constructing the dam structure will also be high. furthermore, the potential consequences in case of dam failure will be large as well. table 1.1 clearly points to the true character of the decision problem, namely that the optimal decision depends on the consequences should something go wrong, and moreover, the probability that something goes wrong. the product of these two factors is called the risk, a measure that will be considered in much more detail in the chapters to follow.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  2107,  2004,  3318,  1010,  5366,  1998,  3808,  2024, 19326,
         1012,  1999,  1996,  2795,  2009,  2003,  5071,  2008,  1996,  5396,
         7170,  4755,  3977,  1997,  1996,  5477,  3252, 14788,  2000,  1037,
         5396,  9723,  1997,  5477,  4945,  2021,  1010,  1997,  2607,  1010,
         2060, 14930,  2024,  2036,  2825,  1012,  5136,  1996,  2553,  2007,
         1037,  2152,  2300,  2504,  1999,  1996,  8071,  1012,  1999,  2023,
         2553,  1010,  1996,  4022,  3318,  2003,  2312,  2021,  1996,  5366,
         1997, 15696,  1996,  5477,  3252,  2097,  2036,  2022,  2152,  1012,
         7297,  1010,  1996,  4022,  8465,  1999,  2553,  1997,  5477,  4945,
         2097,  2022,  2312,  2004,  2092,  1012,  2795,  1015,  1012,  1015,
         4415,  2685,  2000,  1996,  2995,  2839,  1997,  1996,  3247,  3291,
         1010,  8419,  2008,  1996, 15502,  3247,  9041,  2006,  1996,  8465,
         2323,  2242,  2175,  3308,  1010,  1998,  9308,  1010,  1996,  9723,
         2008,  2242,  3632,  3308,  1012,  1996,  4031,  1997,  2122,  2048,
         5876,  2003,  2170,  1996,  3891,  1010,  1037,  5468,  2008,  2097,
         2022,  2641,  1999,  2172,  2062,  6987,  1999,  1996,  9159,  2000,
         3582,  1012,   102])"
21,1,"['uncertainties', 'uncertainty']", Example Feasibility of Hydraulic Power Plant,seg_7,"furthermore, not only the load carrying capacity of the dam structure is associated with uncertainty, but, as indicated previously, also the income expected from the power plant, due to uncertainties in the future market situation. in a similar way, the costs of constructing the power plant are also uncertain as various difficulties encountered during the construction, such as unexpected rock formations, delay in construction works due to problems with material supplies, etc. may lead to additional costs.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  7297,  1010,  2025,  2069,  1996,  7170,  4755,  3977,  1997,
         1996,  5477,  3252,  2003,  3378,  2007, 12503,  1010,  2021,  1010,
         2004,  5393,  3130,  1010,  2036,  1996,  3318,  3517,  2013,  1996,
         2373,  3269,  1010,  2349,  2000,  9662,  7368,  1999,  1996,  2925,
         3006,  3663,  1012,  1999,  1037,  2714,  2126,  1010,  1996,  5366,
         1997, 15696,  1996,  2373,  3269,  2024,  2036,  9662,  2004,  2536,
         8190,  8567,  2076,  1996,  2810,  1010,  2107,  2004,  9223,  2600,
        13197,  1010,  8536,  1999,  2810,  2573,  2349,  2000,  3471,  2007,
         3430,  6067,  1010,  4385,  1012,  2089,  2599,  2000,  3176,  5366,
         1012,   102])"
22,1,"['consequences', 'decision problem', 'factors']", Example Feasibility of Hydraulic Power Plant,seg_7,when deciding on whether or not to establish the hydraulic power plant it is thus necessary to be able to assess consequences and probabilities—two key factors for the decision problem.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,
        0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  2043, 10561,  2006,  3251,  2030,  2025,  2000,  5323,  1996,
        14761,  2373,  3269,  2009,  2003,  2947,  4072,  2000,  2022,  2583,
         2000, 14358,  8465,  1998,  4013,  3676, 14680,  1517,  2048,  3145,
         5876,  2005,  1996,  3247,  3291,  1012,   102])"
23,1,"['vary', 'risk', 'consequences', 'probabilities']", Example Feasibility of Hydraulic Power Plant,seg_7,"both consequences and probabilities vary through the life of the power plant and this must be taken into account as well. in the planning phase it is necessary to consider the risk contributions from all subsequent phases of its life-cycle including decommissioning, see fig. 1.2.",tensor(1),"tensor([0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  2119,  8465,  1998,  4013,  3676, 14680,  8137,  2083,  1996,
         2166,  1997,  1996,  2373,  3269,  1998,  2023,  2442,  2022,  2579,
         2046,  4070,  2004,  2092,  1012,  1999,  1996,  4041,  4403,  2009,
         2003,  4072,  2000,  5136,  1996,  3891,  5857,  2013,  2035,  4745,
        12335,  1997,  2049,  2166,  1011,  5402,  2164, 21933,  7382, 14643,
         3258,  2075,  1010,  2156, 20965,  1012,  1015,  1012,  1016,  1012,
          102])"
24,1,"['events', 'design', 'failures', 'errors']", Example Feasibility of Hydraulic Power Plant,seg_7,"it is important to recognize that different things may go wrong during the different phases of the service life, including events such as mistakes and errors during design and failures and accidents during construction, operation and decommissioning. the potential causes of errors, mistakes, failures and accidents may be numerous, including human errors, failures of structural components, extreme load situa-",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  2009,  2003,  2590,  2000,  6807,  2008,  2367,  2477,  2089,
         2175,  3308,  2076,  1996,  2367, 12335,  1997,  1996,  2326,  2166,
         1010,  2164,  2824,  2107,  2004, 12051,  1998, 10697,  2076,  2640,
         1998, 15428,  1998, 13436,  2076,  2810,  1010,  3169,  1998, 21933,
         7382, 14643,  3258,  2075,  1012,  1996,  4022,  5320,  1997, 10697,
         1010, 12051,  1010, 15428,  1998, 13436,  2089,  2022,  3365,  1010,
         2164,  2529, 10697,  1010, 15428,  1997,  8332,  6177,  1010,  6034,
         7170, 26179,  2050,  1011,   102])"
25,1,"['associated', 'events', 'control', 'risks']", Example Feasibility of Hydraulic Power Plant,seg_7,tions and not least natural hazards. careful planning during the very first phase of a project is the only way to control the risks associated with such events.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101, 14841,  5644,  1998,  2025,  2560,  3019, 22010,  1012,  6176,
         4041,  2076,  1996,  2200,  2034,  4403,  1997,  1037,  2622,  2003,
         1996,  2069,  2126,  2000,  2491,  1996, 10831,  3378,  2007,  2107,
         2824,  1012,   102])"
26,1,"['uncertainty', 'control', 'quality control', 'factor', 'condition']", Example Feasibility of Hydraulic Power Plant,seg_7,"as an illustration, the dam structure must be designed such that the safety of the dam is ensured in all phases of the service life, taking into account yet another factor of uncertainty, namely the future deterioration, but also the quality of workmanship, the degree of quality control implemented during construction and not least the foreseen strategies for the inspection and maintenance of the structures and mechanical equipment during the operation of the power plant. as a final aspect concerning the structures, these should at the end of the service life be in such a condition that the work to be performed during the decommissioning of the power plant can be performed safely for both the persons involved and the environment.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  2004,  2019, 14614,  1010,  1996,  5477,  3252,  2442,  2022,
         2881,  2107,  2008,  1996,  3808,  1997,  1996,  5477,  2003, 16316,
         1999,  2035, 12335,  1997,  1996,  2326,  2166,  1010,  2635,  2046,
         4070,  2664,  2178,  5387,  1997, 12503,  1010,  8419,  1996,  2925,
        26118,  1010,  2021,  2036,  1996,  3737,  1997,  2147, 21530,  1010,
         1996,  3014,  1997,  3737,  2491,  7528,  2076,  2810,  1998,  2025,
         2560,  1996, 18921, 19763,  2078,  9942,  2005,  1996, 10569,  1998,
         6032,  1997,  1996,  5090,  1998,  6228,  3941,  2076,  1996,  3169,
         1997,  1996,  2373,  3269,  1012,  2004,  1037,  2345,  7814,  7175,
         1996,  5090,  1010,  2122,  2323,  2012,  1996,  2203,  1997,  1996,
         2326,  2166,  2022,  1999,  2107,  1037,  4650,  2008,  1996,  2147,
         2000,  2022,  2864,  2076,  1996, 21933,  7382, 14643,  3258,  2075,
         1997,  1996,  2373,  3269,  2064,  2022,  2864,  9689,  2005,  2119,
         1996,  5381,  2920,  1998,  1996,  4044,  1012,   102])"
27,1,"['uncertainty', 'decision problem']", Example Feasibility of Hydraulic Power Plant,seg_7,a final fundamental problem arises with regard to the question—how large are the acceptable risks?—what is one prepared to invest and/or pay for the purpose of getting a potential benefit? the decision problem of whether or not to establish the hydraulic power plant is thus seen to be a decision problem involving a significant element of uncertainty.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  1037,  2345,  8050,  3291, 18653,  2007,  7634,  2000,  1996,
         3160,  1517,  2129,  2312,  2024,  1996, 11701, 10831,  1029,  1517,
         2054,  2003,  2028,  4810,  2000, 15697,  1998,  1013,  2030,  3477,
         2005,  1996,  3800,  1997,  2893,  1037,  4022,  5770,  1029,  1996,
         3247,  3291,  1997,  3251,  2030,  2025,  2000,  5323,  1996, 14761,
         2373,  3269,  2003,  2947,  2464,  2000,  2022,  1037,  3247,  3291,
         5994,  1037,  3278,  5783,  1997, 12503,  1012,   102])"
28,1,"['uncertainty', 'risk', 'probabilities', 'treatment', 'decision problems', 'decision theory']", Example Feasibility of Hydraulic Power Plant,seg_7,the mathematical basis for the treatment of such decision problems is known as decision theory. important aspects of decision theory are the assessment of consequences and probabilities. in a very simplified manner one can say that risk and reliability analysis in civil engineering is concerned with the problem of decision making subject to uncertainty.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0.])","tensor([ 2742, 24010,  1997, 14761,  2373,  3269])","tensor([  101,  1996,  8045,  3978,  2005,  1996,  3949,  1997,  2107,  3247,
         3471,  2003,  2124,  2004,  3247,  3399,  1012,  2590,  5919,  1997,
         3247,  3399,  2024,  1996,  7667,  1997,  8465,  1998,  4013,  3676,
        14680,  1012,  1999,  1037,  2200, 11038,  5450,  2028,  2064,  2360,
         2008,  3891,  1998, 15258,  4106,  1999,  2942,  3330,  2003,  4986,
         2007,  1996,  3291,  1997,  3247,  2437,  3395,  2000, 12503,  1012,
          102])"
29,1,"['probability', 'likelihood', 'risk']", Definition of Risk,seg_9,"in daily conversation risk is a rather common notion used interchangeably with words like chance, likelihood and probability to indicate that people are uncertain about the state of the activity, item or issue under consideration. for example, the risk of getting cancer due to cigarette smoking is discussed, the chance of succeeding to develop a vaccine against the hiv virus in 2020, the likelihood of getting a “royal flush” in a poker game and the probability of a major earthquake occurring in the bay area of san francisco within the next decade.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6210, 1997, 3891])","tensor([  101,  1999,  3679,  4512,  3891,  2003,  1037,  2738,  2691,  9366,
         2109,  8989,  8231,  2007,  2616,  2066,  3382,  1010, 16593,  1998,
         9723,  2000,  5769,  2008,  2111,  2024,  9662,  2055,  1996,  2110,
         1997,  1996,  4023,  1010,  8875,  2030,  3277,  2104,  9584,  1012,
         2005,  2742,  1010,  1996,  3891,  1997,  2893,  4456,  2349,  2000,
         9907,  9422,  2003,  6936,  1010,  1996,  3382,  1997, 13034,  2000,
         4503,  1037, 17404,  2114,  1996,  9820,  7865,  1999, 12609,  1010,
         1996, 16593,  1997,  2893,  1037,  1523,  2548, 13862,  1524,  1999,
         1037, 11662,  2208,  1998,  1996,  9723,  1997,  1037,  2350,  8372,
        10066,  1999,  1996,  3016,  2181,  1997,  2624,  3799,  2306,  1996,
         2279,  5476,  1012,   102])"
30,1,"['consequences', 'risk', 'associated']", Definition of Risk,seg_9,"even though it may be understandable from the context of the discussion what is meant by the different words, it is necessary in the context of engineering decision making that those involved are precise in the understanding of risk. risk is to be understood as the expected consequences associated with a given activity, the activity being e.g. the construction, operation and decommissioning of a power plant.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6210, 1997, 3891])","tensor([  101,  2130,  2295,  2009,  2089,  2022,  3305,  3085,  2013,  1996,
         6123,  1997,  1996,  6594,  2054,  2003,  3214,  2011,  1996,  2367,
         2616,  1010,  2009,  2003,  4072,  1999,  1996,  6123,  1997,  3330,
         3247,  2437,  2008,  2216,  2920,  2024, 10480,  1999,  1996,  4824,
         1997,  3891,  1012,  3891,  2003,  2000,  2022,  5319,  2004,  1996,
         3517,  8465,  3378,  2007,  1037,  2445,  4023,  1010,  1996,  4023,
         2108,  1041,  1012,  1043,  1012,  1996,  2810,  1010,  3169,  1998,
        21933,  7382, 14643,  3258,  2075,  1997,  1037,  2373,  3269,  1012,
          102])"
31,1,"['consequences', 'risk', 'probability', 'event']", Definition of Risk,seg_9,considering an activity with only one event with potential consequences c the risk r is the probability that this event will occur p multiplied with the consequences given the event occurs i.e.:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([6210, 1997, 3891])","tensor([  101,  6195,  2019,  4023,  2007,  2069,  2028,  2724,  2007,  4022,
         8465,  1039,  1996,  3891,  1054,  2003,  1996,  9723,  2008,  2023,
         2724,  2097,  5258,  1052, 28608,  2007,  1996,  8465,  2445,  1996,
         2724,  5158,  1045,  1012,  1041,  1012,  1024,   102])"
32,1,"['consequences', 'risk', 'associated', 'events', 'probabilities']", Definition of Risk,seg_9,"if e.g. n events with consequences ci and occurrence probabilities pi may result from the activity, the total risk associated with the activity is assessed through the",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6210, 1997, 3891])","tensor([  101,  2065,  1041,  1012,  1043,  1012,  1050,  2824,  2007,  8465,
        25022,  1998, 14404,  4013,  3676, 14680, 14255,  2089,  2765,  2013,
         1996,  4023,  1010,  1996,  2561,  3891,  3378,  2007,  1996,  4023,
         2003, 14155,  2083,  1996,   102])"
33,1,"['events', 'risks']", Definition of Risk,seg_9,"sum of the risks from the individual possible events, i.e.:",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([6210, 1997, 3891])","tensor([  101,  7680,  1997,  1996, 10831,  2013,  1996,  3265,  2825,  2824,
         1010,  1045,  1012,  1041,  1012,  1024,   102])"
34,1,"['consequences', 'risk', 'events', 'utility', 'risks', 'case']", Definition of Risk,seg_9,"this definition of risk is consistent with the interpretation of risk used e.g. in the insurance industry. risks may e.g. be given in terms of euros, dollars or the number of human fatalities. even though most risk assessments have some focus on the possible negative consequences of events, the definitions in eqs. 1.1–1.2 are also valid in the case where benefits are taken into account. in fact, and as will be elaborated in chap. 7, in this case the definitions in eqs. 1.1–1.2 are more general and consistent with expected utility as basis for decision analysis.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([6210, 1997, 3891])","tensor([  101,  2023,  6210,  1997,  3891,  2003,  8335,  2007,  1996,  7613,
         1997,  3891,  2109,  1041,  1012,  1043,  1012,  1999,  1996,  5427,
         3068,  1012, 10831,  2089,  1041,  1012,  1043,  1012,  2022,  2445,
         1999,  3408,  1997, 19329,  1010,  6363,  2030,  1996,  2193,  1997,
         2529, 20871,  1012,  2130,  2295,  2087,  3891, 20794,  2031,  2070,
         3579,  2006,  1996,  2825,  4997,  8465,  1997,  2824,  1010,  1996,
        15182,  1999,  1041,  4160,  2015,  1012,  1015,  1012,  1015,  1516,
         1015,  1012,  1016,  2024,  2036,  9398,  1999,  1996,  2553,  2073,
         6666,  2024,  2579,  2046,  4070,  1012,  1999,  2755,  1010,  1998,
         2004,  2097,  2022, 25187,  1999, 15775,  2361,  1012,  1021,  1010,
         1999,  2023,  2553,  1996, 15182,  1999,  1041,  4160,  2015,  1012,
         1015,  1012,  1015,  1516,  1015,  1012,  1016,  2024,  2062,  2236,
         1998,  8335,  2007,  3517,  9710,  2004,  3978,  2005,  3247,  4106,
         1012,   102])"
35,0,[], Self Assessment QuestionsExercises,seg_11,1. what is meant by the term “sustainable development” and why is it important,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([ 101, 1015, 1012, 2054, 2003, 3214, 2011, 1996, 2744, 1523, 9084, 2458,
        1524, 1998, 2339, 2003, 2009, 2590,  102])"
36,0,[], Self Assessment QuestionsExercises,seg_11,for engineering decision making? 2. what is normally understood by the term a beneficial engineered facility or ac-,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2005,  3330,  3247,  2437,  1029,  1016,  1012,  2054,  2003,
         5373,  5319,  2011,  1996,  2744,  1037, 15189, 13685,  4322,  2030,
         9353,  1011,   102])"
37,1,"['consequences', 'risk', 'event']", Self Assessment QuestionsExercises,seg_11,"tivity? 3. how can the risk of an event be defined and how can it be expressed analytically? 4. what is meant by the term “acceptance risks”? 5. considering an activity with only one event with potential consequences, the",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101, 14841, 24872,  1029,  1017,  1012,  2129,  2064,  1996,  3891,
         1997,  2019,  2724,  2022,  4225,  1998,  2129,  2064,  2009,  2022,
         5228, 17826,  2135,  1029,  1018,  1012,  2054,  2003,  3214,  2011,
         1996,  2744,  1523,  9920, 10831,  1524,  1029,  1019,  1012,  6195,
         2019,  4023,  2007,  2069,  2028,  2724,  2007,  4022,  8465,  1010,
         1996,   102])"
38,1,"['consequences', 'risk', 'associated', 'probability', 'events', 'event']", Self Assessment QuestionsExercises,seg_11,associated risk is the probability that this event will occur multiplied with the consequences given the event occurs. which of the following events is associated with the highest risk?,tensor(1),"tensor([0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  3378,  3891,  2003,  1996,  9723,  2008,  2023,  2724,  2097,
         5258, 28608,  2007,  1996,  8465,  2445,  1996,  2724,  5158,  1012,
         2029,  1997,  1996,  2206,  2824,  2003,  3378,  2007,  1996,  3284,
         3891,  1029,   102])"
39,1,"['associated', 'probability theory', 'set', 'probability', 'bayesian', 'probabilities', 'axioms of probability theory', 'results', 'conditional probabilities', 'conditional', 'bayesian probability']",Chapter  Basic Probability Theory,seg_13,lecture 2 (aim of the present lecture) the aim of the present lecture is to introduce the basics of set and probability theory. different interpretations of the important concept of probability are provided and it is outlined that the bayesian probability interpretation facilitates an integration of the other interpretations. the basic axioms of probability theory are given and the important results regarding conditional probabilities and the associated bayes’ rule are outlined. on the basis of the lecture it is expected that the reader should acquire knowledge and skills with regard to:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3127, 3937, 9723, 3399])","tensor([  101,  8835,  1016,  1006,  6614,  1997,  1996,  2556,  8835,  1007,
         1996,  6614,  1997,  1996,  2556,  8835,  2003,  2000,  8970,  1996,
        24078,  1997,  2275,  1998,  9723,  3399,  1012,  2367, 15931,  1997,
         1996,  2590,  4145,  1997,  9723,  2024,  3024,  1998,  2009,  2003,
        14801,  2008,  1996,  3016, 25253,  9723,  7613, 27777,  2019,  8346,
         1997,  1996,  2060, 15931,  1012,  1996,  3937, 22260, 29178,  1997,
         9723,  3399,  2024,  2445,  1998,  1996,  2590,  3463,  4953, 18462,
         4013,  3676, 14680,  1998,  1996,  3378,  3016,  2229,  1521,  3627,
         2024, 14801,  1012,  2006,  1996,  3978,  1997,  1996,  8835,  2009,
         2003,  3517,  2008,  1996,  8068,  2323,  9878,  3716,  1998,  4813,
         2007,  7634,  2000,  1024,   102])"
40,1,"['sets', 'probability theory', 'conditional', 'conditional probability', 'events', 'mutual exclusivity', 'probability', 'complementary event', 'independence', 'sample space', 'axioms of probability theory', 'event', 'sample']",Chapter  Basic Probability Theory,seg_13,"• which are the different interpretations of probability? • what is a sample space and how can events be illustrated? • what is an event and what is a complementary event? • how are intersections and unions of sets defined? • how can operations involving intersections and unions of events be performed? • which are the axioms of probability theory? • which are the implications of mutual exclusivity between events? • what is a conditional probability and how may it be evaluated? • which are the implications of independence? • what is bayes’ rule, and how can it be interpreted? • how can bayes’ rule be applied for probability updating?",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0.])","tensor([3127, 3937, 9723, 3399])","tensor([  101,  1528,  2029,  2024,  1996,  2367, 15931,  1997,  9723,  1029,
         1528,  2054,  2003,  1037,  7099,  2686,  1998,  2129,  2064,  2824,
         2022,  7203,  1029,  1528,  2054,  2003,  2019,  2724,  1998,  2054,
         2003,  1037, 21053,  2724,  1029,  1528,  2129,  2024, 26540,  1998,
         9209,  1997,  4520,  4225,  1029,  1528,  2129,  2064,  3136,  5994,
        26540,  1998,  9209,  1997,  2824,  2022,  2864,  1029,  1528,  2029,
         2024,  1996, 22260, 29178,  1997,  9723,  3399,  1029,  1528,  2029,
         2024,  1996, 13494,  1997,  8203,  4654, 20464,  2271,  7730,  2090,
         2824,  1029,  1528,  2054,  2003,  1037, 18462,  9723,  1998,  2129,
         2089,  2009,  2022, 16330,  1029,  1528,  2029,  2024,  1996, 13494,
         1997,  4336,  1029,  1528,  2054,  2003,  3016,  2229,  1521,  3627,
         1010,  1998,  2129,  2064,  2009,  2022, 10009,  1029,  1528,  2129,
         2064,  3016,  2229,  1521,  3627,  2022,  4162,  2005,  9723,  2039,
        16616,  1029,   102])"
41,1,"['risk', 'events', 'probabilities', 'treatment']", Introduction,seg_15,probability theory forms the basis of the assessment of probabilities of occurrence of uncertain events and thus constitutes a cornerstone in risk and decision analysis. only when a consistent basis has been established for the treatment of the uncer-,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])",tensor([4955]),"tensor([  101,  9723,  3399,  3596,  1996,  3978,  1997,  1996,  7667,  1997,
         4013,  3676, 14680,  1997, 14404,  1997,  9662,  2824,  1998,  2947,
        17367,  1037, 23354,  1999,  3891,  1998,  3247,  4106,  1012,  2069,
         2043,  1037,  8335,  3978,  2038,  2042,  2511,  2005,  1996,  3949,
         1997,  1996,  4895, 17119,  1011,   102])"
42,1,"['consequences', 'associated', 'probability', 'events', 'risks']", Introduction,seg_15,"tainties influencing the probability that events with possible adverse consequences may occur, it is possible to assess the risks associated with a given activity and thus to establish a rational basis for decision making.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])",tensor([4955]),"tensor([  101, 13843, 16778,  2229, 25870,  1996,  9723,  2008,  2824,  2007,
         2825, 15316,  8465,  2089,  5258,  1010,  2009,  2003,  2825,  2000,
        14358,  1996, 10831,  3378,  2007,  1037,  2445,  4023,  1998,  2947,
         2000,  5323,  1037, 11581,  3978,  2005,  3247,  2437,  1012,   102])"
43,1,"['uncertainty', 'associated', 'probability', 'level', 'likelihood', 'percentages']", Introduction,seg_15,"the level of uncertainty associated with a considered activity or phenomenon may be expressed by means of purely qualitative statements such as “the chance is good” or “the likelihood is low” but may also be quantified in terms of numbers or percentages. however, the different words all carry the meaning of probability. in the following section this notion, and especially the theoretical framework for its quantification, will be investigated in more detail.",tensor(1),"tensor([0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])",tensor([4955]),"tensor([  101,  1996,  2504,  1997, 12503,  3378,  2007,  1037,  2641,  4023,
         2030,  9575,  2089,  2022,  5228,  2011,  2965,  1997, 11850, 24209,
        11475, 27453,  8635,  2107,  2004,  1523,  1996,  3382,  2003,  2204,
         1524,  2030,  1523,  1996, 16593,  2003,  2659,  1524,  2021,  2089,
         2036,  2022, 24110,  3775, 10451,  1999,  3408,  1997,  3616,  2030,
         7017,  2015,  1012,  2174,  1010,  1996,  2367,  2616,  2035,  4287,
         1996,  3574,  1997,  9723,  1012,  1999,  1996,  2206,  2930,  2023,
         9366,  1010,  1998,  2926,  1996,  9373,  7705,  2005,  2049, 24110,
         3775, 10803,  1010,  2097,  2022, 10847,  1999,  2062,  6987,  1012,
          102])"
44,1,"['cases', 'probability theory', 'probability', 'probabilities', 'quantitative']", Definition of Probability,seg_17,"the purpose of the theory of probability is to enable the quantitative assessment of probabilities but the real meaning and interpretation of probabilities and probabilistic calculations is not a part of the theory. consequently, two people may have completely different interpretations of the probability concept, but still use the same calculus. in the following sections three different interpretations of probability are introduced and discussed based on simple cases. a formal presentation of the axioms of probability theory is provided in sect. 2.4.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6210, 1997, 9723])","tensor([  101,  1996,  3800,  1997,  1996,  3399,  1997,  9723,  2003,  2000,
         9585,  1996, 20155,  7667,  1997,  4013,  3676, 14680,  2021,  1996,
         2613,  3574,  1998,  7613,  1997,  4013,  3676, 14680,  1998,  4013,
         3676, 27965,  4588, 16268,  2003,  2025,  1037,  2112,  1997,  1996,
         3399,  1012,  8821,  1010,  2048,  2111,  2089,  2031,  3294,  2367,
        15931,  1997,  1996,  9723,  4145,  1010,  2021,  2145,  2224,  1996,
         2168, 19276,  1012,  1999,  1996,  2206,  5433,  2093,  2367, 15931,
         1997,  9723,  2024,  3107,  1998,  6936,  2241,  2006,  3722,  3572,
         1012,  1037,  5337,  8312,  1997,  1996, 22260, 29178,  1997,  9723,
         3399,  2003,  3024,  1999, 17831,  1012,  1016,  1012,  1018,  1012,
          102])"
45,1,"['experimentalist', 'probability', 'frequency', 'event', 'trials', 'experiment', 'experiments', 'probability of an event']", Frequentistic Definition,seg_19,"the frequentistic definition of probability is the typical interpretation of probability by the experimentalist. in this interpretation the probability p(a) is simply the relative frequency of occurrence of the event a as observed in an experiment with n trials, i.e. the probability of an event a is defined as the number of times that the event a occurs divided by the number of experiments that are carried out:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([6976, 6553, 6210])","tensor([  101,  1996,  6976,  6553,  6210,  1997,  9723,  2003,  1996,  5171,
         7613,  1997,  9723,  2011,  1996,  6388,  2923,  1012,  1999,  2023,
         7613,  1996,  9723,  1052,  1006,  1037,  1007,  2003,  3432,  1996,
         5816,  6075,  1997, 14404,  1997,  1996,  2724,  1037,  2004,  5159,
         1999,  2019,  7551,  2007,  1050,  7012,  1010,  1045,  1012,  1041,
         1012,  1996,  9723,  1997,  2019,  2724,  1037,  2003,  4225,  2004,
         1996,  2193,  1997,  2335,  2008,  1996,  2724,  1037,  5158,  4055,
         2011,  1996,  2193,  1997,  7885,  2008,  2024,  3344,  2041,  1024,
          102])"
46,1,['experiments'], Frequentistic Definition,seg_19,"na = number of experiments where a occurred, nexp = total number of experiments.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.])","tensor([6976, 6553, 6210])","tensor([  101,  6583,  1027,  2193,  1997,  7885,  2073,  1037,  4158,  1010,
        11265,  2595,  2361,  1027,  2561,  2193,  1997,  7885,  1012,   102])"
47,1,"['probability', 'experiments']", Frequentistic Definition,seg_19,"if a frequentist is asked what is the probability of achieving a “head” when flipping a coin, she/he would principally not know what to answer until she/he would have performed a large number of experiments. if say after 1000 experiments (flips with the coin) it is observed that “head” has occurred 563 times, the answer would be that the probability of achieving a “head” is 0.563. however, as the number of experiments is increased the probability would converge towards 0.5. in the mind of a frequentist, probability is a characteristic of nature.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([6976, 6553, 6210])","tensor([  101,  2065,  1037,  6976,  2923,  2003,  2356,  2054,  2003,  1996,
         9723,  1997, 10910,  1037,  1523,  2132,  1524,  2043, 18497,  1037,
         9226,  1010,  2016,  1013,  2002,  2052, 16552,  2025,  2113,  2054,
         2000,  3437,  2127,  2016,  1013,  2002,  2052,  2031,  2864,  1037,
         2312,  2193,  1997,  7885,  1012,  2065,  2360,  2044,  6694,  7885,
         1006, 11238,  2015,  2007,  1996,  9226,  1007,  2009,  2003,  5159,
         2008,  1523,  2132,  1524,  2038,  4158,  5179,  2509,  2335,  1010,
         1996,  3437,  2052,  2022,  2008,  1996,  9723,  1997, 10910,  1037,
         1523,  2132,  1524,  2003,  1014,  1012,  5179,  2509,  1012,  2174,
         1010,  2004,  1996,  2193,  1997,  7885,  2003,  3445,  1996,  9723,
         2052, 28314,  2875,  1014,  1012,  1019,  1012,  1999,  1996,  2568,
         1997,  1037,  6976,  2923,  1010,  9723,  2003,  1037,  8281,  1997,
         3267,  1012,   102])"
48,1,"['probability', 'probability definition']", Classical Definition,seg_21,the classical probability definition originates from the days when probability cal-,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([4556, 6210])","tensor([  101,  1996,  4556,  9723,  6210, 16896,  2013,  1996,  2420,  2043,
         9723, 10250,  1011,   102])"
49,1,"['probability', 'probability of the event', 'event']", Classical Definition,seg_21,1 culus was founded by pascal and fermat. the inspiration for this theory can be found in the games of cards and dice. the classical definition of the probability of the event a can be formulated as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([4556, 6210])","tensor([  101,  1015, 12731,  7393,  2001,  2631,  2011, 17878,  1998, 10768,
        17830,  2102,  1012,  1996,  7780,  2005,  2023,  3399,  2064,  2022,
         2179,  1999,  1996,  2399,  1997,  5329,  1998, 18740,  1012,  1996,
         4556,  6210,  1997,  1996,  9723,  1997,  1996,  2724,  1037,  2064,
         2022, 19788,  2004,  1024,   102])"
50,1,['experiment'], Classical Definition,seg_21,"na = number of equally likely ways by which an experiment may lead to a, ntot = total number of equally likely ways in the experiment.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([4556, 6210])","tensor([  101,  6583,  1027,  2193,  1997,  8053,  3497,  3971,  2011,  2029,
         2019,  7551,  2089,  2599,  2000,  1037,  1010, 23961,  4140,  1027,
         2561,  2193,  1997,  8053,  3497,  3971,  1999,  1996,  7551,  1012,
          102])"
51,1,"['probability', 'outcomes', 'experiment']", Classical Definition,seg_21,"according to the classical definition of probability, the probability of achieving a “head” when flipping a coin would be 0.5 as there is only one possible way to achieve a “head” and there are two equally likely outcomes of the experiment.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.])","tensor([4556, 6210])","tensor([  101,  2429,  2000,  1996,  4556,  6210,  1997,  9723,  1010,  1996,
         9723,  1997, 10910,  1037,  1523,  2132,  1524,  2043, 18497,  1037,
         9226,  2052,  2022,  1014,  1012,  1019,  2004,  2045,  2003,  2069,
         2028,  2825,  2126,  2000,  6162,  1037,  1523,  2132,  1524,  1998,
         2045,  2024,  2048,  8053,  3497, 13105,  1997,  1996,  7551,  1012,
          102])"
52,0,[], Classical Definition,seg_21,"in fact there is no real contradiction to the frequentistic definition, but the following differences may be observed:",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([4556, 6210])","tensor([  101,  1999,  2755,  2045,  2003,  2053,  2613, 26917,  2000,  1996,
         6976,  6553,  6210,  1010,  2021,  1996,  2206,  5966,  2089,  2022,
         5159,  1024,   102])"
53,1,['experiment'], Classical Definition,seg_21,• the experiment does not need to be carried out as the answer is known in advance. • the classical theory gives no solution unless all equally possible ways can be derived analytically.,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([4556, 6210])","tensor([  101,  1528,  1996,  7551,  2515,  2025,  2342,  2000,  2022,  3344,
         2041,  2004,  1996,  3437,  2003,  2124,  1999,  5083,  1012,  1528,
         1996,  4556,  3399,  3957,  2053,  5576,  4983,  2035,  8053,  2825,
         3971,  2064,  2022,  5173, 17826,  2135,  1012,   102])"
54,1,"['probability', 'bayesian', 'event']", Bayesian Definition,seg_23,in the bayesian interpretation the probability p(a) of the event a is formulated as a degree of belief that a will occur:,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3016, 25253,  6210])","tensor([  101,  1999,  1996,  3016, 25253,  7613,  1996,  9723,  1052,  1006,
         1037,  1007,  1997,  1996,  2724,  1037,  2003, 19788,  2004,  1037,
         3014,  1997,  6772,  2008,  1037,  2097,  5258,  1024,   102])"
55,1,"['probability', 'bayesian']", Bayesian Definition,seg_23,"coming back to the coin-flipping problem the bayesian would argue that there are two possibilities, and as she has no preferences as to “head” or “tail” she would judge the probability of achieving a “head” to be 0.5.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3016, 25253,  6210])","tensor([  101,  2746,  2067,  2000,  1996,  9226,  1011, 18497,  3291,  1996,
         3016, 25253,  2052,  7475,  2008,  2045,  2024,  2048, 12020,  1010,
         1998,  2004,  2016,  2038,  2053, 18394,  2004,  2000,  1523,  2132,
         1524,  2030,  1523,  5725,  1524,  2016,  2052,  3648,  1996,  9723,
         1997, 10910,  1037,  1523,  2132,  1524,  2000,  2022,  1014,  1012,
         1019,  1012,   102])"
56,1,"['probability', 'bayesian', 'probabilities', 'event', 'subjective']", Bayesian Definition,seg_23,"the degree of belief is a reflection of the state of mind of the individual person in terms of experience, expertise and preferences. in this respect the bayesian interpretation of probability is subjective or more precise—person-dependent. this opens up the possibility that two different persons may assign different probabilities to a given event and thereby contradicts the frequentist interpretation that probabilities are a characteristic of nature.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3016, 25253,  6210])","tensor([  101,  1996,  3014,  1997,  6772,  2003,  1037,  9185,  1997,  1996,
         2110,  1997,  2568,  1997,  1996,  3265,  2711,  1999,  3408,  1997,
         3325,  1010, 11532,  1998, 18394,  1012,  1999,  2023,  4847,  1996,
         3016, 25253,  7613,  1997,  9723,  2003, 20714,  2030,  2062, 10480,
         1517,  2711,  1011,  7790,  1012,  2023,  7480,  2039,  1996,  6061,
         2008,  2048,  2367,  5381,  2089, 23911,  2367,  4013,  3676, 14680,
         2000,  1037,  2445,  2724,  1998,  8558, 24528, 29201,  2015,  1996,
         6976,  2923,  7613,  2008,  4013,  3676, 14680,  2024,  1037,  8281,
         1997,  3267,  1012,   102])"
57,1,"['probability', 'bayesian', 'symmetry', 'experiments', 'statistical']", Bayesian Definition,seg_23,the bayesian statistical interpretation of probability includes the frequentistic and the classical interpretation in the sense that the subjectively assigned probabilities may be based on experience from previous experiments (frequentistic) as well as considerations of e.g. symmetry (classical).,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 3016, 25253,  6210])","tensor([  101,  1996,  3016, 25253,  7778,  7613,  1997,  9723,  2950,  1996,
         6976,  6553,  1998,  1996,  4556,  7613,  1999,  1996,  3168,  2008,
         1996, 20714,  2135,  4137,  4013,  3676, 14680,  2089,  2022,  2241,
         2006,  3325,  2013,  3025,  7885,  1006,  6976,  6553,  1007,  2004,
         2092,  2004, 16852,  1997,  1041,  1012,  1043,  1012, 14991,  1006,
         4556,  1007,  1012,   102])"
58,1,"['probability', 'bayesian', 'prior belief', 'treatment', 'prior probability', 'bayesian statistics', 'statistics']", Bayesian Definition,seg_23,"the degree of belief is also referred to as a prior belief or prior probability, i.e. the belief, which may be assigned prior to obtaining any further knowledge. it is interesting to note that immanuel kant2 developed the philosophical basis for the treatment of subjectivity at the same time as thomas bayes3 developed the mathematical framework later known as bayesian statistics.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0.])","tensor([ 3016, 25253,  6210])","tensor([  101,  1996,  3014,  1997,  6772,  2003,  2036,  3615,  2000,  2004,
         1037,  3188,  6772,  2030,  3188,  9723,  1010,  1045,  1012,  1041,
         1012,  1996,  6772,  1010,  2029,  2089,  2022,  4137,  3188,  2000,
        11381,  2151,  2582,  3716,  1012,  2009,  2003,  5875,  2000,  3602,
         2008, 10047,  2386, 16284, 26044,  2475,  2764,  1996,  9569,  3978,
         2005,  1996,  3949,  1997,  3395,  7730,  2012,  1996,  2168,  2051,
         2004,  2726,  3016,  2229,  2509,  2764,  1996,  8045,  7705,  2101,
         2124,  2004,  3016, 25253,  6747,  1012,   102])"
59,1,"['risk', 'probability', 'bayesian', 'degree of freedom', 'subjective', 'data']", Bayesian Definition,seg_23,"modern structural reliability and risk analysis is based on the bayesian interpretation of probability. however, the degree of freedom in the assignment of probabilities is in reality not as large as indicated in the above. in a formal bayesian framework the subjective element should be formulated before the relevant data are observed. arguments of objective symmetrical reasoning and physical constraints, of course, should be taken into account.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3016, 25253,  6210])","tensor([  101,  2715,  8332, 15258,  1998,  3891,  4106,  2003,  2241,  2006,
         1996,  3016, 25253,  7613,  1997,  9723,  1012,  2174,  1010,  1996,
         3014,  1997,  4071,  1999,  1996,  8775,  1997,  4013,  3676, 14680,
         2003,  1999,  4507,  2025,  2004,  2312,  2004,  5393,  1999,  1996,
         2682,  1012,  1999,  1037,  5337,  3016, 25253,  7705,  1996, 20714,
         5783,  2323,  2022, 19788,  2077,  1996,  7882,  2951,  2024,  5159,
         1012,  9918,  1997,  7863, 23476, 13384,  1998,  3558, 14679,  1010,
         1997,  2607,  1010,  2323,  2022,  2579,  2046,  4070,  1012,   102])"
60,1,"['cases', 'probability of failure', 'probability', 'failure', 'information', 'probabilities', 'failures', 'independent', 'case']", Practical Implications of the Different Interpretations of Probability,seg_25,"in some cases probabilities may adequately be assessed by means of frequentistic information. this is e.g. the case when the probability of failure of mass produced components are considered, such as pumps, light bulbs and valves. however, in order to utilize reported failures for the assessment of probability of failure for such components it is a prerequisite that the components are in principle identical, that they have been subject to the same operational and/or loading conditions and that the failures can be assumed to be independent.",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 6742, 13494,  1997,  1996,  2367, 15931,  1997,  9723])","tensor([  101,  1999,  2070,  3572,  4013,  3676, 14680,  2089, 23613,  2022,
        14155,  2011,  2965,  1997,  6976,  6553,  2592,  1012,  2023,  2003,
         1041,  1012,  1043,  1012,  1996,  2553,  2043,  1996,  9723,  1997,
         4945,  1997,  3742,  2550,  6177,  2024,  2641,  1010,  2107,  2004,
        15856,  1010,  2422, 25548,  1998, 17355,  1012,  2174,  1010,  1999,
         2344,  2000, 16462,  2988, 15428,  2005,  1996,  7667,  1997,  9723,
         1997,  4945,  2005,  2107,  6177,  2009,  2003,  1037,  3653,  2890,
        24871,  2008,  1996,  6177,  2024,  1999,  6958,  7235,  1010,  2008,
         2027,  2031,  2042,  3395,  2000,  1996,  2168,  6515,  1998,  1013,
         2030, 10578,  3785,  1998,  2008,  1996, 15428,  2064,  2022,  5071,
         2000,  2022,  2981,  1012,   102])"
61,1,"['cases', 'probability', 'bayesian']", Practical Implications of the Different Interpretations of Probability,seg_25,"in other cases when the considered components are e.g. bridges, high-rise buildings, ship structures or unique configurations of pipelines and pressure vessels, these conditions are not fulfilled. this is because the number of identical structures may be very small (or even just one) and the conditions in terms of operational and loading conditions are normally significantly different from structure to structure. in such situations, the bayesian interpretation of probability is far more appropriate.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0.])","tensor([ 6742, 13494,  1997,  1996,  2367, 15931,  1997,  9723])","tensor([  101,  1999,  2060,  3572,  2043,  1996,  2641,  6177,  2024,  1041,
         1012,  1043,  1012,  7346,  1010,  2152,  1011,  4125,  3121,  1010,
         2911,  5090,  2030,  4310, 22354,  1997, 13117,  2015,  1998,  3778,
         6470,  1010,  2122,  3785,  2024,  2025, 16829,  1012,  2023,  2003,
         2138,  1996,  2193,  1997,  7235,  5090,  2089,  2022,  2200,  2235,
         1006,  2030,  2130,  2074,  2028,  1007,  1998,  1996,  3785,  1999,
         3408,  1997,  6515,  1998, 10578,  3785,  2024,  5373,  6022,  2367,
         2013,  3252,  2000,  3252,  1012,  1999,  2107,  8146,  1010,  1996,
         3016, 25253,  7613,  1997,  9723,  2003,  2521,  2062,  6413,  1012,
          102])"
62,1,"['uncertainty', 'bayesian', 'probabilistic', 'bayesian statistics', 'process', 'statistics']", Practical Implications of the Different Interpretations of Probability,seg_25,"the basic idea behind bayesian statistics is that lack of knowledge should be treated by probabilistic reasoning, similarly to other types of uncertainty. in reality, decisions have to be made despite the lack of knowledge and probabilistic tools are a great help in this process.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.])","tensor([ 6742, 13494,  1997,  1996,  2367, 15931,  1997,  9723])","tensor([  101,  1996,  3937,  2801,  2369,  3016, 25253,  6747,  2003,  2008,
         3768,  1997,  3716,  2323,  2022,  5845,  2011,  4013,  3676, 27965,
         4588, 13384,  1010,  6660,  2000,  2060,  4127,  1997, 12503,  1012,
         1999,  4507,  1010,  6567,  2031,  2000,  2022,  2081,  2750,  1996,
         3768,  1997,  3716,  1998,  4013,  3676, 27965,  4588,  5906,  2024,
         1037,  2307,  2393,  1999,  2023,  2832,  1012,   102])"
63,1,"['standardized', 'estimated', 'results', 'experiments', 'test results', 'test']", Sample Space and Events,seg_27,"considering e.g. the compressive strength of concrete, this material characteristic may be estimated by performing laboratory experiments on standardized test specimens (cylinders or cubes). the test results will, however, be different from one",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 1998, 2824])","tensor([  101,  6195,  1041,  1012,  1043,  1012,  1996,  4012, 27484,  3997,
         1997,  5509,  1010,  2023,  3430,  8281,  2089,  2022,  4358,  2011,
         4488,  5911,  7885,  2006, 16367,  3231,  9908,  1006, 18729,  2030,
        14291,  2015,  1007,  1012,  1996,  3231,  3463,  2097,  1010,  2174,
         1010,  2022,  2367,  2013,  2028,   102])"
64,1,"['continuous', 'cases', 'sample space', 'set', 'interval', 'outcomes', 'outcome', 'sample', 'random', 'experiments', 'discrete', 'case']", Sample Space and Events,seg_27,"another and the concrete compressive strength shall thus be assumed to be an uncertain quantity or a random quantity. the set of all possible outcomes of the concrete compressive strength experiments is called the sample space (denoted ω) for the random quantity—the concrete compressive strength. in this example the sample space is the open interval ω =]0;∞[, i.e. the set of all positive real numbers. in this case, the sample space is furthermore continuous but in other cases (e.g. when considering the outcome of throwing a dice) the sample space can also be discrete and countable.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 1998, 2824])","tensor([  101,  2178,  1998,  1996,  5509,  4012, 27484,  3997,  4618,  2947,
         2022,  5071,  2000,  2022,  2019,  9662, 11712,  2030,  1037,  6721,
        11712,  1012,  1996,  2275,  1997,  2035,  2825, 13105,  1997,  1996,
         5509,  4012, 27484,  3997,  7885,  2003,  2170,  1996,  7099,  2686,
         1006, 19537,  1179,  1007,  2005,  1996,  6721, 11712,  1517,  1996,
         5509,  4012, 27484,  3997,  1012,  1999,  2023,  2742,  1996,  7099,
         2686,  2003,  1996,  2330, 13483,  1179,  1027,  1033,  1014,  1025,
         1601,  1031,  1010,  1045,  1012,  1041,  1012,  1996,  2275,  1997,
         2035,  3893,  2613,  3616,  1012,  1999,  2023,  2553,  1010,  1996,
         7099,  2686,  2003,  7297,  7142,  2021,  1999,  2060,  3572,  1006,
         1041,  1012,  1043,  1012,  2043,  6195,  1996,  9560,  1997,  6886,
         1037, 18740,  1007,  1996,  7099,  2686,  2064,  2036,  2022, 16246,
         1998,  4175,  3085,  1012,   102])"
65,1,"['sample space', 'set', 'event', 'sample']", Sample Space and Events,seg_27,an event is defined as a subset of a sample space and thus a set of sample points. if the subset is empty (i.e. contains no sample points) it is said to be impossible. an event is said to be certain if it contains all sample points in the sample space (i.e. the event is identical to the sample space).,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 0.])","tensor([7099, 2686, 1998, 2824])","tensor([  101,  2019,  2724,  2003,  4225,  2004,  1037, 16745,  1997,  1037,
         7099,  2686,  1998,  2947,  1037,  2275,  1997,  7099,  2685,  1012,
         2065,  1996, 16745,  2003,  4064,  1006,  1045,  1012,  1041,  1012,
         3397,  2053,  7099,  2685,  1007,  2009,  2003,  2056,  2000,  2022,
         5263,  1012,  2019,  2724,  2003,  2056,  2000,  2022,  3056,  2065,
         2009,  3397,  2035,  7099,  2685,  1999,  1996,  7099,  2686,  1006,
         1045,  1012,  1041,  1012,  1996,  2724,  2003,  7235,  2000,  1996,
         7099,  2686,  1007,  1012,   102])"
66,1,"['events', 'event', 'union', 'sample']", Sample Space and Events,seg_27,consider the events e1 and e2 shown in fig. 2.1. the subset of sample points belonging to the event e1 or the event e2 is denoted as the union of the events e1 and e2 written as e1 ∪ e2.,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([7099, 2686, 1998, 2824])","tensor([  101,  5136,  1996,  2824,  1041,  2487,  1998,  1041,  2475,  3491,
         1999, 20965,  1012,  1016,  1012,  1015,  1012,  1996, 16745,  1997,
         7099,  2685,  7495,  2000,  1996,  2724,  1041,  2487,  2030,  1996,
         2724,  1041,  2475,  2003, 19537,  2004,  1996,  2586,  1997,  1996,
         2824,  1041,  2487,  1998,  1041,  2475,  2517,  2004,  1041,  2487,
         1605,  1041,  2475,  1012,   102])"
67,1,"['events', 'intersection', 'sample']", Sample Space and Events,seg_27,the subset of sample points belonging to e1 and e2 is called the intersection of e1 and e2 and is written as e1 ∩ e2. the intersection of these two events is illustrated in the right portion of fig. 2.1.,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 1998, 2824])","tensor([  101,  1996, 16745,  1997,  7099,  2685,  7495,  2000,  1041,  2487,
         1998,  1041,  2475,  2003,  2170,  1996,  6840,  1997,  1041,  2487,
         1998,  1041,  2475,  1998,  2003,  2517,  2004,  1041,  2487,  1604,
         1041,  2475,  1012,  1996,  6840,  1997,  2122,  2048,  2824,  2003,
         7203,  1999,  1996,  2157,  4664,  1997, 20965,  1012,  1016,  1012,
         1015,  1012,   102])"
68,1,"['set', 'disjoint', 'events', 'intersection', 'sample', 'mutually exclusive', 'case']", Sample Space and Events,seg_27,"the two events are said to be mutually exclusive if they are disjoint (i.e. if they have no common sample points). in this case, the intersection of e1 and e2 is empty (i.e.e1 ∩ e2 = ∅), where ∅ denotes the empty set.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([7099, 2686, 1998, 2824])","tensor([  101,  1996,  2048,  2824,  2024,  2056,  2000,  2022, 20271,  7262,
         2065,  2027,  2024,  4487,  2015,  5558, 18447,  1006,  1045,  1012,
         1041,  1012,  2065,  2027,  2031,  2053,  2691,  7099,  2685,  1007,
         1012,  1999,  2023,  2553,  1010,  1996,  6840,  1997,  1041,  2487,
         1998,  1041,  2475,  2003,  4064,  1006,  1045,  1012,  1041,  1012,
         1041,  2487,  1604,  1041,  2475,  1027,  1593,  1007,  1010,  2073,
         1593, 14796,  1996,  4064,  2275,  1012,   102])"
69,1,"['sample space', 'complementary event', 'event', 'sample']", Sample Space and Events,seg_27,"consider the event e in the sample space ω . the event containing all sample points in ω , which are not included in e is called the complementary event to e and denoted ē. it then follows directly that e ∪ ē = ω and that e ∩ ē = ∅.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([7099, 2686, 1998, 2824])","tensor([  101,  5136,  1996,  2724,  1041,  1999,  1996,  7099,  2686,  1179,
         1012,  1996,  2724,  4820,  2035,  7099,  2685,  1999,  1179,  1010,
         2029,  2024,  2025,  2443,  1999,  1041,  2003,  2170,  1996, 21053,
         2724,  2000,  1041,  1998, 19537,  1041,  1012,  2009,  2059,  4076,
         3495,  2008,  1041,  1605,  1041,  1027,  1179,  1998,  2008,  1041,
         1604,  1041,  1027,  1593,  1012,   102])"
70,1,"['union', 'intersection', 'distributive laws', 'associative and distributive laws']", Sample Space and Events,seg_27,"it can be shown that the intersection and union operations obey the following commutative, associative and distributive laws:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.])","tensor([7099, 2686, 1998, 2824])","tensor([  101,  2009,  2064,  2022,  3491,  2008,  1996,  6840,  1998,  2586,
         3136, 15470,  1996,  2206,  4012, 28120,  8082,  1010,  4632, 10085,
         2401,  6024,  1998,  4487,  3367,  3089,  8569,  6024,  4277,  1024,
          102])"
71,0,[], Sample Space and Events,seg_27,from which the following laws (called de morgan’s laws) may be derived:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([7099, 2686, 1998, 2824])","tensor([ 101, 2013, 2029, 1996, 2206, 4277, 1006, 2170, 2139, 5253, 1521, 1055,
        4277, 1007, 2089, 2022, 5173, 1024,  102])"
72,0,[], The Three Axioms of Probability Theory,seg_29,probability theory is based on the following three axioms:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 1996,  2093, 22260, 29178,  1997,  9723,  3399])","tensor([  101,  9723,  3399,  2003,  2241,  2006,  1996,  2206,  2093, 22260,
        29178,  1024,   102])"
73,1,"['probability measure', 'probability', 'event']", The Three Axioms of Probability Theory,seg_29,for any given event e ⊂ ω where p is the probability measure.,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 1996,  2093, 22260, 29178,  1997,  9723,  3399])","tensor([ 101, 2005, 2151, 2445, 2724, 1041, 1610, 1179, 2073, 1052, 2003, 1996,
        9723, 5468, 1012,  102])"
74,1,"['sample', 'sample space']", The Three Axioms of Probability Theory,seg_29,where ω is the sample space.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 1996,  2093, 22260, 29178,  1997,  9723,  3399])","tensor([ 101, 2073, 1179, 2003, 1996, 7099, 2686, 1012,  102])"
75,1,"['events', 'mutually exclusive events', 'mutually exclusive']", The Three Axioms of Probability Theory,seg_29,"axiom 3 given that e1,e2, . . . is a sequence of mutually exclusive events (i.e. e1 ∩ e2 = ∅ etc.) then:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 1996,  2093, 22260, 29178,  1997,  9723,  3399])","tensor([  101, 22260, 18994,  1017,  2445,  2008,  1041,  2487,  1010,  1041,
         2475,  1010,  1012,  1012,  1012,  2003,  1037,  5537,  1997, 20271,
         7262,  2824,  1006,  1045,  1012,  1041,  1012,  1041,  2487,  1604,
         1041,  2475,  1027,  1593,  4385,  1012,  1007,  2059,  1024,   102])"
76,1,"['probability', 'axioms of probability theory', 'probability theory']", The Three Axioms of Probability Theory,seg_29,these three axioms of probability theory form the sole basis of the theory of probability.,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0.])","tensor([ 1996,  2093, 22260, 29178,  1997,  9723,  3399])","tensor([  101,  2122,  2093, 22260, 29178,  1997,  9723,  3399,  2433,  1996,
         7082,  3978,  1997,  1996,  3399,  1997,  9723,  1012,   102])"
77,1,"['risk', 'probability', 'information', 'probabilities', 'estimates']", Conditional Probability and Bayes Rule,seg_31,"conditional probabilities are of special interest in risk and reliability analysis as they form the basis of the updating of probability estimates based on new information, knowledge and evidence.",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([  101, 18462,  4013,  3676, 14680,  2024,  1997,  2569,  3037,  1999,
         3891,  1998, 15258,  4106,  2004,  2027,  2433,  1996,  3978,  1997,
         1996,  2039, 16616,  1997,  9723, 10035,  2241,  2006,  2047,  2592,
         1010,  3716,  1998,  3350,  1012,   102])"
78,1,"['conditional probability', 'probability', 'event', 'probability of the event', 'conditional']", Conditional Probability and Bayes Rule,seg_31,the conditional probability of the event e1 given that the event e2 has occurred is written as:,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([  101,  1996, 18462,  9723,  1997,  1996,  2724,  1041,  2487,  2445,
         2008,  1996,  2724,  1041,  2475,  2038,  4158,  2003,  2517,  2004,
         1024,   102])"
79,1,"['conditional probability', 'set', 'probability', 'event', 'conditional']", Conditional Probability and Bayes Rule,seg_31,"it is seen that the conditional probability is not defined if the conditioning event is the empty set, i.e. when p(e2) = 0.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([  101,  2009,  2003,  2464,  2008,  1996, 18462,  9723,  2003,  2025,
         4225,  2065,  1996, 14372,  2724,  2003,  1996,  4064,  2275,  1010,
         1045,  1012,  1041,  1012,  2043,  1052,  1006,  1041,  2475,  1007,
         1027,  1014,  1012,   102])"
80,1,"['event', 'independent']", Conditional Probability and Bayes Rule,seg_31,the event e1 is said to be probabilistically independent of the event e2 if:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0.])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([  101,  1996,  2724,  1041,  2487,  2003,  2056,  2000,  2022,  4013,
         3676, 27965, 25084,  2981,  1997,  1996,  2724,  1041,  2475,  2065,
         1024,   102])"
81,1,"['probability', 'event']", Conditional Probability and Bayes Rule,seg_31,implying that the occurrence of the event e2 does not affect the probability of e1.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0.])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([  101, 20242,  2008,  1996, 14404,  1997,  1996,  2724,  1041,  2475,
         2515,  2025,  7461,  1996,  9723,  1997,  1041,  2487,  1012,   102])"
82,1,"['probability', 'probability of the event', 'event']", Conditional Probability and Bayes Rule,seg_31,from eq. 2.9 the probability of the event e1 ∩ e2 may be given as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([ 101, 2013, 1041, 4160, 1012, 1016, 1012, 1023, 1996, 9723, 1997, 1996,
        2724, 1041, 2487, 1604, 1041, 2475, 2089, 2022, 2445, 2004, 1024,  102])"
83,1,"['events', 'independent']", Conditional Probability and Bayes Rule,seg_31,"and it follows immediately that if the events e1 and e2 are independent, then:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([ 101, 1998, 2009, 4076, 3202, 2008, 2065, 1996, 2824, 1041, 2487, 1998,
        1041, 2475, 2024, 2981, 1010, 2059, 1024,  102])"
84,1,"['sample space', 'events', 'sample', 'mutually exclusive events', 'mutually exclusive', 'case']", Conditional Probability and Bayes Rule,seg_31,"based on the above findings, the important bayes’ rule can be derived. consider the sample space ω divided into n mutually exclusive events e1,e2, . . . ,en (see also fig. 2.2, where the case of n = 8 is considered).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([  101,  2241,  2006,  1996,  2682,  9556,  1010,  1996,  2590,  3016,
         2229,  1521,  3627,  2064,  2022,  5173,  1012,  5136,  1996,  7099,
         2686,  1179,  4055,  2046,  1050, 20271,  7262,  2824,  1041,  2487,
         1010,  1041,  2475,  1010,  1012,  1012,  1012,  1010,  4372,  1006,
         2156,  2036, 20965,  1012,  1016,  1012,  1016,  1010,  2073,  1996,
         2553,  1997,  1050,  1027,  1022,  2003,  2641,  1007,  1012,   102])"
85,1,"['sample space', 'event', 'sample']", Conditional Probability and Bayes Rule,seg_31,"furthermore, let the event a be an event in the sample space ω . then the probability of the event a, i.e. p(a), can be written as:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([ 101, 7297, 1010, 2292, 1996, 2724, 1037, 2022, 2019, 2724, 1999, 1996,
        7099, 2686, 1179, 1012, 2059, 1996, 9723, 1997, 1996, 2724, 1037, 1010,
        1045, 1012, 1041, 1012, 1052, 1006, 1037, 1007, 1010, 2064, 2022, 2517,
        2004, 1024,  102])"
86,1,"['probability', 'total probability theorem', 'total probability']", Conditional Probability and Bayes Rule,seg_31,this is also referred to as the total probability theorem.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([ 101, 2023, 2003, 2036, 3615, 2000, 2004, 1996, 2561, 9723, 9872, 1012,
         102])"
87,1,['results'], Conditional Probability and Bayes Rule,seg_31,"now by inserting eq. 2.14 into eq. 2.13, the bayes’ rule results:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([  101,  2085,  2011, 19274,  2075,  1041,  4160,  1012,  1016,  1012,
         2403,  2046,  1041,  4160,  1012,  1016,  1012,  2410,  1010,  1996,
         3016,  2229,  1521,  3627,  3463,  1024,   102])"
88,1,"['posterior', 'probability', 'likelihood', 'posterior probability', 'conditional']", Conditional Probability and Bayes Rule,seg_31,"in eq. 2.14 p(ei |a) is called the posterior probability of ei , the conditional term p(a|ei) is often referred to as the likelihood (i.e. the probability of observing",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([  101,  1999,  1041,  4160,  1012,  1016,  1012,  2403,  1052,  1006,
         1041,  2072,  1064,  1037,  1007,  2003,  2170,  1996, 15219,  9723,
         1997,  1041,  2072,  1010,  1996, 18462,  2744,  1052,  1006,  1037,
         1064,  1041,  2072,  1007,  2003,  2411,  3615,  2000,  2004,  1996,
        16593,  1006,  1045,  1012,  1041,  1012,  1996,  9723,  1997, 14158,
          102])"
89,1,"['probability', 'event', 'prior probability', 'probability of the event']", Conditional Probability and Bayes Rule,seg_31,a certain state given the true state). the term p(ei) is the prior probability of the event ei (i.e. prior to the knowledge about the event a).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0.])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([ 101, 1037, 3056, 2110, 2445, 1996, 2995, 2110, 1007, 1012, 1996, 2744,
        1052, 1006, 1041, 2072, 1007, 2003, 1996, 3188, 9723, 1997, 1996, 2724,
        1041, 2072, 1006, 1045, 1012, 1041, 1012, 3188, 2000, 1996, 3716, 2055,
        1996, 2724, 1037, 1007, 1012,  102])"
90,0,[], Conditional Probability and Bayes Rule,seg_31,"as mentioned previously, the bayes’ rule is extremely important, and in order to facilitate its appreciation a few illustrative applications will be given in the following section.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([18462,  9723,  1998,  3016,  2229,  3627])","tensor([  101,  2004,  3855,  3130,  1010,  1996,  3016,  2229,  1521,  3627,
         2003,  5186,  2590,  1010,  1998,  1999,  2344,  2000, 10956,  2049,
        12284,  1037,  2261,  5665, 19966, 18514,  5097,  2097,  2022,  2445,
         1999,  1996,  2206,  2930,  1012,   102])"
91,1,"['probability', 'event', 'method', 'condition']", Example Using Bayes Rule for Concrete Assessment,seg_33,"a reinforced concrete beam is considered. from experience it is known that the probability that corrosion of the reinforcement has initiated (the event ci) is p(ci) = 0.01. however, in order to know the condition more precisely an inspection method (non-destructive) has been developed.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 2478, 3016, 2229, 3627, 2005, 5509, 7667])","tensor([  101,  1037, 11013,  5509,  7504,  2003,  2641,  1012,  2013,  3325,
         2009,  2003,  2124,  2008,  1996,  9723,  2008, 24625,  1997,  1996,
        23895,  2038,  7531,  1006,  1996,  2724, 25022,  1007,  2003,  1052,
         1006, 25022,  1007,  1027,  1014,  1012,  5890,  1012,  2174,  1010,
         1999,  2344,  2000,  2113,  1996,  4650,  2062, 10785,  2019, 10569,
         4118,  1006,  2512,  1011, 15615,  1007,  2038,  2042,  2764,  1012,
          102])"
92,1,"['probability', 'method', 'likelihood']", Example Using Bayes Rule for Concrete Assessment,seg_33,the quality of the inspection method may be characterized by the probability that the inspection method will indicate initiated corrosion given that corrosion has initiated p(i |ci) (the probability of detection or equivalently the likelihood of an indication i given corrosion initiation ci) and the probability that the inspection method will indicate initiated corrosion given that no corrosion has initiated p(i |ci) (the probability of erroneous findings or the likelihood of an indication given no corrosion initiation).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([2742, 2478, 3016, 2229, 3627, 2005, 5509, 7667])","tensor([  101,  1996,  3737,  1997,  1996, 10569,  4118,  2089,  2022,  7356,
         2011,  1996,  9723,  2008,  1996, 10569,  4118,  2097,  5769,  7531,
        24625,  2445,  2008, 24625,  2038,  7531,  1052,  1006,  1045,  1064,
        25022,  1007,  1006,  1996,  9723,  1997, 10788,  2030,  5662,  2135,
         1996, 16593,  1997,  2019, 12407,  1045,  2445, 24625, 17890, 25022,
         1007,  1998,  1996,  9723,  2008,  1996, 10569,  4118,  2097,  5769,
         7531, 24625,  2445,  2008,  2053, 24625,  2038,  7531,  1052,  1006,
         1045,  1064, 25022,  1007,  1006,  1996,  9723,  1997,  9413, 20793,
         3560,  9556,  2030,  1996, 16593,  1997,  2019, 12407,  2445,  2053,
        24625, 17890,  1007,  1012,   102])"
93,1,['method'], Example Using Bayes Rule for Concrete Assessment,seg_33,for the inspection method at hand the following characteristics have been established:,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 2478, 3016, 2229, 3627, 2005, 5509, 7667])","tensor([  101,  2005,  1996, 10569,  4118,  2012,  2192,  1996,  2206,  6459,
         2031,  2042,  2511,  1024,   102])"
94,1,"['probability', 'method']", Example Using Bayes Rule for Concrete Assessment,seg_33,"an inspection of the concrete beam is conducted with the result that the inspection method indicates that corrosion has initiated. based on the findings from the inspection, what is the probability that corrosion of the reinforcement has initiated?",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 2478, 3016, 2229, 3627, 2005, 5509, 7667])","tensor([  101,  2019, 10569,  1997,  1996,  5509,  7504,  2003,  4146,  2007,
         1996,  2765,  2008,  1996, 10569,  4118,  7127,  2008, 24625,  2038,
         7531,  1012,  2241,  2006,  1996,  9556,  2013,  1996, 10569,  1010,
         2054,  2003,  1996,  9723,  2008, 24625,  1997,  1996, 23895,  2038,
         7531,  1029,   102])"
95,0,[], Example Using Bayes Rule for Concrete Assessment,seg_33,the answer is readily found by application of bayes’ rule:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2742, 2478, 3016, 2229, 3627, 2005, 5509, 7667])","tensor([  101,  1996,  3437,  2003, 12192,  2179,  2011,  4646,  1997,  3016,
         2229,  1521,  3627,  1024,   102])"
96,1,['probability'], Example Using Bayes Rule for Concrete Assessment,seg_33,"with p(i), the probability of obtaining an indication of corrosion at the inspection:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([2742, 2478, 3016, 2229, 3627, 2005, 5509, 7667])","tensor([  101,  2007,  1052,  1006,  1045,  1007,  1010,  1996,  9723,  1997,
        11381,  2019, 12407,  1997, 24625,  2012,  1996, 10569,  1024,   102])"
97,1,['probability'], Example Using Bayes Rule for Concrete Assessment,seg_33,and p(i ∩ ci) the probability of receiving an indication of initiated corrosion and at the same time to have initiated corrosion:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 2478, 3016, 2229, 3627, 2005, 5509, 7667])","tensor([  101,  1998,  1052,  1006,  1045,  1604, 25022,  1007,  1996,  9723,
         1997,  4909,  2019, 12407,  1997,  7531, 24625,  1998,  2012,  1996,
         2168,  2051,  2000,  2031,  7531, 24625,  1024,   102])"
98,1,"['probability', 'method']", Example Using Bayes Rule for Concrete Assessment,seg_33,"thus, the probability that corrosion of the reinforcement has initiated given an indication of initiated corrosion by the inspection method is:",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0.])","tensor([2742, 2478, 3016, 2229, 3627, 2005, 5509, 7667])","tensor([  101,  2947,  1010,  1996,  9723,  2008, 24625,  1997,  1996, 23895,
         2038,  7531,  2445,  2019, 12407,  1997,  7531, 24625,  2011,  1996,
        10569,  4118,  2003,  1024,   102])"
99,1,"['probability', 'method']", Example Using Bayes Rule for Concrete Assessment,seg_33,"the probability of initiated corrosion, given an indication of initiated corrosion, is surprisingly low. this is due to the high probability of an erroneous indication of initiated corrosion by the inspection method relative to the small probability of initiated corrosion (i.e. the inspection method is not sufficiently accurate for the considered application).",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 2478, 3016, 2229, 3627, 2005, 5509, 7667])","tensor([  101,  1996,  9723,  1997,  7531, 24625,  1010,  2445,  2019, 12407,
         1997,  7531, 24625,  1010,  2003, 10889,  2659,  1012,  2023,  2003,
         2349,  2000,  1996,  2152,  9723,  1997,  2019,  9413, 20793,  3560,
        12407,  1997,  7531, 24625,  2011,  1996, 10569,  4118,  5816,  2000,
         1996,  2235,  9723,  1997,  7531, 24625,  1006,  1045,  1012,  1041,
         1012,  1996, 10569,  4118,  2003,  2025, 12949,  8321,  2005,  1996,
         2641,  4646,  1007,  1012,   102])"
100,1,['samples'], Example Using Bayes Rule for Bridge Upgrading,seg_35,an old reinforced concrete bridge is reassessed in connection with an upgrading of the allowable traffic (see also schneider [11]). the concrete compressive strength class is unknown but concrete cylinder samples may be taken from the bridge and tested in the laboratory.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  2478,  3016,  2229,  3627,  2005,  2958, 25925])","tensor([  101,  2019,  2214, 11013,  5509,  2958,  2003,  2128, 27241, 29417,
         1999,  4434,  2007,  2019, 25925,  1997,  1996,  3499,  3085,  4026,
         1006,  2156,  2036, 15159,  1031,  2340,  1033,  1007,  1012,  1996,
         5509,  4012, 27484,  3997,  2465,  2003,  4242,  2021,  5509,  7956,
         8168,  2089,  2022,  2579,  2013,  1996,  2958,  1998,  7718,  1999,
         1996,  5911,  1012,   102])"
101,0,[], Example Using Bayes Rule for Bridge Upgrading,seg_35,the following classification of the concrete is assumed:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2742,  2478,  3016,  2229,  3627,  2005,  2958, 25925])","tensor([ 101, 1996, 2206, 5579, 1997, 1996, 5509, 2003, 5071, 1024,  102])"
102,0,[], Example Using Bayes Rule for Bridge Upgrading,seg_35,where σc is the compressive strength of concrete.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2742,  2478,  3016,  2229,  3627,  2005,  2958, 25925])","tensor([  101,  2073,  1173,  2278,  2003,  1996,  4012, 27484,  3997,  1997,
         5509,  1012,   102])"
103,1,"['probability', 'information', 'experiment']", Example Using Bayes Rule for Bridge Upgrading,seg_35,"even though the concrete class is unknown, experience with similar bridges suggests that the probability of the concrete of the bridge belonging to class b1, b2 and b3 is 0.65, 0.24 and 0.11, respectively. this information comprises the prior information—prior to any experiment result.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 2742,  2478,  3016,  2229,  3627,  2005,  2958, 25925])","tensor([  101,  2130,  2295,  1996,  5509,  2465,  2003,  4242,  1010,  3325,
         2007,  2714,  7346,  6083,  2008,  1996,  9723,  1997,  1996,  5509,
         1997,  1996,  2958,  7495,  2000,  2465, 29491,  1010,  1038,  2475,
         1998,  1038,  2509,  2003,  1014,  1012,  3515,  1010,  1014,  1012,
         2484,  1998,  1014,  1012,  2340,  1010,  4414,  1012,  2023,  2592,
         8681,  1996,  3188,  2592,  1517,  3188,  2000,  2151,  7551,  2765,
         1012,   102])"
104,1,"['table', 'probability', 'method', 'test']", Example Using Bayes Rule for Bridge Upgrading,seg_35,"the test method is not perfect in the sense that even though the test indicates a value of the concrete compressive strength belonging to a certain class, there is a certain probability that the concrete belongs to another class. the likelihoods for the considered test method are given in table 2.1.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0.])","tensor([ 2742,  2478,  3016,  2229,  3627,  2005,  2958, 25925])","tensor([  101,  1996,  3231,  4118,  2003,  2025,  3819,  1999,  1996,  3168,
         2008,  2130,  2295,  1996,  3231,  7127,  1037,  3643,  1997,  1996,
         5509,  4012, 27484,  3997,  7495,  2000,  1037,  3056,  2465,  1010,
         2045,  2003,  1037,  3056,  9723,  2008,  1996,  5509,  7460,  2000,
         2178,  2465,  1012,  1996, 16593,  2015,  2005,  1996,  2641,  3231,
         4118,  2024,  2445,  1999,  2795,  1016,  1012,  1015,  1012,   102])"
105,1,"['interval', 'test']", Example Using Bayes Rule for Bridge Upgrading,seg_35,"it is assumed that one test is performed and it is found that the concrete compressive strength is equal to 36.2 mpa, i.e. in the interval of class b2.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0.])","tensor([ 2742,  2478,  3016,  2229,  3627,  2005,  2958, 25925])","tensor([  101,  2009,  2003,  5071,  2008,  2028,  3231,  2003,  2864,  1998,
         2009,  2003,  2179,  2008,  1996,  5509,  4012, 27484,  3997,  2003,
         5020,  2000,  4029,  1012,  1016,  6131,  2050,  1010,  1045,  1012,
         1041,  1012,  1999,  1996, 13483,  1997,  2465,  1038,  2475,  1012,
          102])"
106,1,"['probability', 'posterior probability', 'posterior']", Example Using Bayes Rule for Bridge Upgrading,seg_35,"using bayes’ rule, the probability that the concrete belongs to one of the different classes may now be updated. the posterior probability that the concrete belongs to class b2 is given by:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 2742,  2478,  3016,  2229,  3627,  2005,  2958, 25925])","tensor([  101,  2478,  3016,  2229,  1521,  3627,  1010,  1996,  9723,  2008,
         1996,  5509,  7460,  2000,  2028,  1997,  1996,  2367,  4280,  2089,
         2085,  2022,  7172,  1012,  1996, 15219,  9723,  2008,  1996,  5509,
         7460,  2000,  2465,  1038,  2475,  2003,  2445,  2011,  1024,   102])"
107,1,"['posterior', 'table', 'probabilities', 'results', 'posterior probabilities']", Example Using Bayes Rule for Bridge Upgrading,seg_35,"the posterior probabilities for the other classes may be calculated in a similar manner, the results are given in table 2.1.",tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 2742,  2478,  3016,  2229,  3627,  2005,  2958, 25925])","tensor([  101,  1996, 15219,  4013,  3676, 14680,  2005,  1996,  2060,  4280,
         2089,  2022, 10174,  1999,  1037,  2714,  5450,  1010,  1996,  3463,
         2024,  2445,  1999,  2795,  1016,  1012,  1015,  1012,   102])"
108,1,['probability'], Self Assessment QuestionsExercises,seg_37,1. a person is asked what is the probability for achieving a “head” when flip-,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1015,  1012,  1037,  2711,  2003,  2356,  2054,  2003,  1996,
         9723,  2005, 10910,  1037,  1523,  2132,  1524,  2043, 11238,  1011,
          102])"
109,1,"['conditional probability', 'estimation', 'probability', 'event', 'experiments', 'conditional', 'probability of an event']", Self Assessment QuestionsExercises,seg_37,"ping a coin. the person after 1000 experiments (flips with the coin) observes that “head” has occurred 333 times and hence answers that the probability for “head” is 0.333. on which interpretation of probability is this estimation based on? 2. how can the conditional probability of an event e1, given that the event e2 has",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101, 17852,  1037,  9226,  1012,  1996,  2711,  2044,  6694,  7885,
         1006, 11238,  2015,  2007,  1996,  9226,  1007, 24451,  2008,  1523,
         2132,  1524,  2038,  4158, 21211,  2335,  1998,  6516,  6998,  2008,
         1996,  9723,  2005,  1523,  2132,  1524,  2003,  1014,  1012, 21211,
         1012,  2006,  2029,  7613,  1997,  9723,  2003,  2023, 24155,  2241,
         2006,  1029,  1016,  1012,  2129,  2064,  1996, 18462,  9723,  1997,
         2019,  2724,  1041,  2487,  1010,  2445,  2008,  1996,  2724,  1041,
         2475,  2038,   102])"
110,1,"['probability', 'event', 'probability theory']", Self Assessment QuestionsExercises,seg_37,"occurred, be written? 3. in probability theory the probability, p(a), of an event a can take any value",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([ 101, 4158, 1010, 2022, 2517, 1029, 1017, 1012, 1999, 9723, 3399, 1996,
        9723, 1010, 1052, 1006, 1037, 1007, 1010, 1997, 2019, 2724, 1037, 2064,
        2202, 2151, 3643,  102])"
111,1,"['set', 'events', 'intersection']", Self Assessment QuestionsExercises,seg_37,"within the following boundaries: 0 ≤ p(a) ≤ 1 −1 ≤ p(a) ≤ 1 −∞ < p(a) < ∞ 4. if the intersection of two events, a and b corresponds to the empty set ∅, i.e.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2306,  1996,  2206,  7372,  1024,  1014,  1608,  1052,  1006,
         1037,  1007,  1608,  1015,  1597,  2487,  1608,  1052,  1006,  1037,
         1007,  1608,  1015,  1597, 30128,  1026,  1052,  1006,  1037,  1007,
         1026,  1601,  1018,  1012,  2065,  1996,  6840,  1997,  2048,  2824,
         1010,  1037,  1998,  1038, 14788,  2000,  1996,  4064,  2275,  1593,
         1010,  1045,  1012,  1041,  1012,   102])"
112,1,"['events', 'mutually exclusive', 'independent']", Self Assessment QuestionsExercises,seg_37,"a ∩ b = ∅, the two events are: mutually exclusive. independent. empty events. 5. which the following expressions is(are) correct?",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1037,  1604,  1038,  1027,  1593,  1010,  1996,  2048,  2824,
         2024,  1024, 20271,  7262,  1012,  2981,  1012,  4064,  2824,  1012,
         1019,  1012,  2029,  1996,  2206, 11423,  2003,  1006,  2024,  1007,
         6149,  1029,   102])"
113,1,"['probability', 'events', 'event', 'intersection', 'probability of the intersection of two events', 'union', 'mutually exclusive events', 'mutually exclusive', 'probability of event', 'independent']", Self Assessment QuestionsExercises,seg_37,"the probability of the union of two events a and b is equal to the sum of the probability of event a and the probability of event b , given that the two events are mutually exclusive. the probability of the union of two events a and b is equal to the probability of the sum of the two events a and event b , given that the two events are mutually exclusive. the probability of the intersection of two events a and b is equal to the product of the probability of event a and the probability of event b , given that the two events are mutually exclusive. the probability of the intersection of two events a and b is equal to the product of the probability of event a and the probability of event b , given that the two events are independent. 6. the probability of the intersection of two mutually exclusive events is equal to:",tensor(1),"tensor([0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1996,  9723,  1997,  1996,  2586,  1997,  2048,  2824,  1037,
         1998,  1038,  2003,  5020,  2000,  1996,  7680,  1997,  1996,  9723,
         1997,  2724,  1037,  1998,  1996,  9723,  1997,  2724,  1038,  1010,
         2445,  2008,  1996,  2048,  2824,  2024, 20271,  7262,  1012,  1996,
         9723,  1997,  1996,  2586,  1997,  2048,  2824,  1037,  1998,  1038,
         2003,  5020,  2000,  1996,  9723,  1997,  1996,  7680,  1997,  1996,
         2048,  2824,  1037,  1998,  2724,  1038,  1010,  2445,  2008,  1996,
         2048,  2824,  2024, 20271,  7262,  1012,  1996,  9723,  1997,  1996,
         6840,  1997,  2048,  2824,  1037,  1998,  1038,  2003,  5020,  2000,
         1996,  4031,  1997,  1996,  9723,  1997,  2724,  1037,  1998,  1996,
         9723,  1997,  2724,  1038,  1010,  2445,  2008,  1996,  2048,  2824,
         2024, 20271,  7262,  1012,  1996,  9723,  1997,  1996,  6840,  1997,
         2048,  2824,  1037,  1998,  1038,  2003,  5020,  2000,  1996,  4031,
         1997,  1996,  9723,  1997,  2724,  1037,  1998,  1996,  9723,  1997,
         2724,  1038,  1010,  2445,  2008,  1996,  2048,  2824,  2024,  2981,
         1012,  1020,  1012,  1996,  9723,  1997,  1996,  6840,  1997,  2048,
        20271,  7262,  2824,  2003,  5020,  2000,  1024,   102])"
114,1,"['events', 'probabilities']", Self Assessment QuestionsExercises,seg_37,the product of the probabilities of the individual events. the sum of the probabilities of the individual events.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1996,  4031,  1997,  1996,  4013,  3676, 14680,  1997,  1996,
         3265,  2824,  1012,  1996,  7680,  1997,  1996,  4013,  3676, 14680,
         1997,  1996,  3265,  2824,  1012,   102])"
115,1,"['events', 'probabilities']", Self Assessment QuestionsExercises,seg_37,the difference between the probabilities of the individual events. one (1). zero (0). 7. which of the following statements is correct?,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1996,  4489,  2090,  1996,  4013,  3676, 14680,  1997,  1996,
         3265,  2824,  1012,  2028,  1006,  1015,  1007,  1012,  5717,  1006,
         1014,  1007,  1012,  1021,  1012,  2029,  1997,  1996,  2206,  8635,
         2003,  6149,  1029,   102])"
116,1,"['sample space', 'probability', 'events', 'event', 'union', 'sample', 'mutually exclusive events', 'mutually exclusive']", Self Assessment QuestionsExercises,seg_37,an event a is defined as a subset of a sample space ω . a sample space ω is defined as a subset of an event a. 8. the probability of the union of two not mutually exclusive events a and b,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2019,  2724,  1037,  2003,  4225,  2004,  1037, 16745,  1997,
         1037,  7099,  2686,  1179,  1012,  1037,  7099,  2686,  1179,  2003,
         4225,  2004,  1037, 16745,  1997,  2019,  2724,  1037,  1012,  1022,
         1012,  1996,  9723,  1997,  1996,  2586,  1997,  2048,  2025, 20271,
         7262,  2824,  1037,  1998,  1038,   102])"
117,1,"['sample space', 'probability', 'event', 'sample', 'probability of event']", Self Assessment QuestionsExercises,seg_37,"is given as: p(a ∪ b) = p(a) + p(b) − p(a ∩ b). it is provided that the probability of event a is equal to 0.1, the probability of event b is 0.1 and the probability of event b given event a, i.e. p(b|a) is 0.8. which result is correct? p(a ∪ b) = −0.6 p(a ∪ b) = 0.12 p(a ∪ b) = 0.04 9. for an event a in the sample space ω , event ā represents the complementary",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2003,  2445,  2004,  1024,  1052,  1006,  1037,  1605,  1038,
         1007,  1027,  1052,  1006,  1037,  1007,  1009,  1052,  1006,  1038,
         1007,  1597,  1052,  1006,  1037,  1604,  1038,  1007,  1012,  2009,
         2003,  3024,  2008,  1996,  9723,  1997,  2724,  1037,  2003,  5020,
         2000,  1014,  1012,  1015,  1010,  1996,  9723,  1997,  2724,  1038,
         2003,  1014,  1012,  1015,  1998,  1996,  9723,  1997,  2724,  1038,
         2445,  2724,  1037,  1010,  1045,  1012,  1041,  1012,  1052,  1006,
         1038,  1064,  1037,  1007,  2003,  1014,  1012,  1022,  1012,  2029,
         2765,  2003,  6149,  1029,  1052,  1006,  1037,  1605,  1038,  1007,
         1027,  1597,  2692,  1012,  1020,  1052,  1006,  1037,  1605,  1038,
         1007,  1027,  1014,  1012,  2260,  1052,  1006,  1037,  1605,  1038,
         1007,  1027,  1014,  1012,  5840,  1023,  1012,  2005,  2019,  2724,
         1037,  1999,  1996,  7099,  2686,  1179,  1010,  2724,  1037,  5836,
         1996, 21053,   102])"
118,1,"['event', 'distributive laws', 'associative and distributive laws']", Self Assessment QuestionsExercises,seg_37,"event of event a. which one(s) of the following expressions are correct? a ∪ ā = ω a ∩ ā = ω a ∪ ā = ∅ 10. the commutative, associative and distributive laws describe how to:",tensor(1),"tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2724,  1997,  2724,  1037,  1012,  2029,  2028,  1006,  1055,
         1007,  1997,  1996,  2206, 11423,  2024,  6149,  1029,  1037,  1605,
         1037,  1027,  1179,  1037,  1604,  1037,  1027,  1179,  1037,  1605,
         1037,  1027,  1593,  2184,  1012,  1996,  4012, 28120,  8082,  1010,
         4632, 10085,  2401,  6024,  1998,  4487,  3367,  3089,  8569,  6024,
         4277,  6235,  2129,  2000,  1024,   102])"
119,1,['sets'], Self Assessment QuestionsExercises,seg_37,operate with intersections of sets. operate with unions of sets. none of the above. 11. research in eth is often funded by the swiss national foundation of research,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  5452,  2007, 26540,  1997,  4520,  1012,  5452,  2007,  9209,
         1997,  4520,  1012,  3904,  1997,  1996,  2682,  1012,  2340,  1012,
         2470,  1999,  3802,  2232,  2003,  2411,  6787,  2011,  1996,  5364,
         2120,  3192,  1997,  2470,   102])"
120,1,"['normal', 'probabilities', 'associated']", Self Assessment QuestionsExercises,seg_37,"(snf). the normal procedure is that a professor submits a proposal for a new project. experts working for snf read the proposal and they may come to one of the following decisions: d1: the proposal is accepted and the project will be funded. d2: the proposal should be revised by the professor and resubmitted to snf. d3: the proposal is not accepted and hence no funding is provided. professor muster works at eth. during the past few years he has submitted many proposals to snf. based on experience, over many years, professor muster in general assesses that when he submits a proposal the probabilities associated with the possible final decisions of snf are as follows:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1006,  1055,  2078,  2546,  1007,  1012,  1996,  3671,  7709,
         2003,  2008,  1037,  2934, 12040,  2015,  1037,  6378,  2005,  1037,
         2047,  2622,  1012,  8519,  2551,  2005,  1055,  2078,  2546,  3191,
         1996,  6378,  1998,  2027,  2089,  2272,  2000,  2028,  1997,  1996,
         2206,  6567,  1024,  1040,  2487,  1024,  1996,  6378,  2003,  3970,
         1998,  1996,  2622,  2097,  2022,  6787,  1012,  1040,  2475,  1024,
         1996,  6378,  2323,  2022,  8001,  2011,  1996,  2934,  1998, 24501,
        12083, 22930,  3064,  2000,  1055,  2078,  2546,  1012,  1040,  2509,
         1024,  1996,  6378,  2003,  2025,  3970,  1998,  6516,  2053,  4804,
         2003,  3024,  1012,  2934, 20327,  2573,  2012,  3802,  2232,  1012,
         2076,  1996,  2627,  2261,  2086,  2002,  2038,  7864,  2116, 10340,
         2000,  1055,  2078,  2546,  1012,  2241,  2006,  3325,  1010,  2058,
         2116,  2086,  1010,  2934, 20327,  1999,  2236, 14358,  2229,  2008,
         2043,  2002, 12040,  2015,  1037,  6378,  1996,  4013,  3676, 14680,
         3378,  2007,  1996,  2825,  2345,  6567,  1997,  1055,  2078,  2546,
         2024,  2004,  4076,  1024,   102])"
121,1,"['probabilities', 'conditional', 'conditional probabilities']", Self Assessment QuestionsExercises,seg_37,"by coincidence, just at the time when professor muster considers to submit a new proposal to snf, he meets dr. beispiel. dr. beispiel used to work at snf as one of the experts who review proposals and make the final decisions. professor muster kindly asks dr. beispiel to have a look at the new proposal before submitting it to snf with the purpose of assessing the probabilities that the proposal would be accepted as it is. of course dr. beispiel cannot say with certainty what will be the final snf decision. however, his assessment can be considered as an indication, ij , of the final decision of snf. based on experience from previous assessments and final decisions the conditional probabilities, p(ij |di),",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2011, 16507,  1010,  2074,  2012,  1996,  2051,  2043,  2934,
        20327, 10592,  2000, 12040,  1037,  2047,  6378,  2000,  1055,  2078,
         2546,  1010,  2002,  6010,  2852,  1012, 21388, 13102,  9257,  1012,
         2852,  1012, 21388, 13102,  9257,  2109,  2000,  2147,  2012,  1055,
         2078,  2546,  2004,  2028,  1997,  1996,  8519,  2040,  3319, 10340,
         1998,  2191,  1996,  2345,  6567,  1012,  2934, 20327, 19045,  5176,
         2852,  1012, 21388, 13102,  9257,  2000,  2031,  1037,  2298,  2012,
         1996,  2047,  6378,  2077, 12040,  3436,  2009,  2000,  1055,  2078,
         2546,  2007,  1996,  3800,  1997, 20077,  1996,  4013,  3676, 14680,
         2008,  1996,  6378,  2052,  2022,  3970,  2004,  2009,  2003,  1012,
         1997,  2607,  2852,  1012, 21388, 13102,  9257,  3685,  2360,  2007,
        15855,  2054,  2097,  2022,  1996,  2345,  1055,  2078,  2546,  3247,
         1012,  2174,  1010,  2010,  7667,  2064,  2022,  2641,  2004,  2019,
        12407,  1010,  1045,  3501,  1010,  1997,  1996,  2345,  3247,  1997,
         1055,  2078,  2546,  1012,  2241,  2006,  3325,  2013,  3025, 20794,
         1998,  2345,  6567,  1996, 18462,  4013,  3676, 14680,  1010,  1052,
         1006,  1045,  3501,  1064,  4487,  1007,  1010,   102])"
122,1,['table'], Self Assessment QuestionsExercises,seg_37,of the indications ij of dr. beispiel given the final decisions di of snf are as summarized in the following table.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1997,  1996, 24936,  1045,  3501,  1997,  2852,  1012, 21388,
        13102,  9257,  2445,  1996,  2345,  6567,  4487,  1997,  1055,  2078,
         2546,  2024,  2004, 22539,  1999,  1996,  2206,  2795,  1012,   102])"
123,1,"['probability', 'table']", Self Assessment QuestionsExercises,seg_37,a. complete the above table. b. having read the new proposal dr. beispiel explains to professor muster that if he would still have been working with snf he would have asked for revisions and resubmission. based on this new information—what is the probability that the final decision of snf is the same as the assessment of dr. beispiel?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1037,  1012,  3143,  1996,  2682,  2795,  1012,  1038,  1012,
         2383,  3191,  1996,  2047,  6378,  2852,  1012, 21388, 13102,  9257,
         7607,  2000,  2934, 20327,  2008,  2065,  2002,  2052,  2145,  2031,
         2042,  2551,  2007,  1055,  2078,  2546,  2002,  2052,  2031,  2356,
         2005, 24699,  1998, 24501, 12083, 25481,  1012,  2241,  2006,  2023,
         2047,  2592,  1517,  2054,  2003,  1996,  9723,  2008,  1996,  2345,
         3247,  1997,  1055,  2078,  2546,  2003,  1996,  2168,  2004,  1996,
         7667,  1997,  2852,  1012, 21388, 13102,  9257,  1029,   102])"
124,1,"['representations', 'data', 'descriptive statistics', 'statistics', 'numerical']",Chapter  Descriptive Statistics,seg_39,lecture 3 (aim of the present lecture) the aim of the present lecture is to introduce descriptive statistics in terms of numerical summaries and graphical representations. it is outlined how data can be represented in a standardized manner numerically as well as in the form of different graphs. on the basis of the lecture it is expected that the reader should acquire knowledge and skills with regard to:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 3127, 22726,  6747])","tensor([  101,  8835,  1017,  1006,  6614,  1997,  1996,  2556,  8835,  1007,
         1996,  6614,  1997,  1996,  2556,  8835,  2003,  2000,  8970, 22726,
         6747,  1999,  3408,  1997, 15973,  7680,  7849,  3111,  1998, 20477,
        15066,  1012,  2009,  2003, 14801,  2129,  2951,  2064,  2022,  3421,
         1999,  1037, 16367,  5450, 15973,  2135,  2004,  2092,  2004,  1999,
         1996,  2433,  1997,  2367, 19287,  1012,  2006,  1996,  3978,  1997,
         1996,  8835,  2009,  2003,  3517,  2008,  1996,  8068,  2323,  9878,
         3716,  1998,  4813,  2007,  7634,  2000,  1024,   102])"
125,1,"['sets', 'representations', 'correlation', 'box plot', 'frequency', 'skewness', 'histogram', 'statistics', 'tukey box plot', 'plot', 'data', 'graphical representations', 'descriptive statistics', 'information', 'dispersion', 'significance', 'graphical', 'sample', 'numerical', 'data sets']",Chapter  Descriptive Statistics,seg_39,• what is the purpose of descriptive statistics? • in what principally different ways can data be assessed and communicated? • what are the assumptions underlying descriptive statistics? • which are the different “central numerical measures” and what do they describe? • what is a measure of dispersion and which such measures are available? • what does peakedness and skewness refer to? • what is the significance of correlation and how may it be calculated? • which are typical graphical representations of data sets? • what is the difference between a sample histogram and a frequency distribution? • what information is contained in a quantile-quantile plot? • what are the main components of a tukey box plot? • in what way can numerical summaries be related to graphical representations?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 3127, 22726,  6747])","tensor([  101,  1528,  2054,  2003,  1996,  3800,  1997, 22726,  6747,  1029,
         1528,  1999,  2054, 16552,  2367,  3971,  2064,  2951,  2022, 14155,
         1998, 24162,  1029,  1528,  2054,  2024,  1996, 17568, 10318, 22726,
         6747,  1029,  1528,  2029,  2024,  1996,  2367,  1523,  2430, 15973,
         5761,  1524,  1998,  2054,  2079,  2027,  6235,  1029,  1528,  2054,
         2003,  1037,  5468,  1997,  4487, 17668, 10992,  1998,  2029,  2107,
         5761,  2024,  2800,  1029,  1528,  2054,  2515,  6601,  2791,  1998,
        15315,  7974,  2791,  6523,  2000,  1029,  1528,  2054,  2003,  1996,
         7784,  1997, 16902,  1998,  2129,  2089,  2009,  2022, 10174,  1029,
         1528,  2029,  2024,  5171, 20477, 15066,  1997,  2951,  4520,  1029,
         1528,  2054,  2003,  1996,  4489,  2090,  1037,  7099,  2010,  3406,
        13113,  1998,  1037,  6075,  4353,  1029,  1528,  2054,  2592,  2003,
         4838,  1999,  1037, 24110, 15286,  1011, 24110, 15286,  5436,  1029,
         1528,  2054,  2024,  1996,  2364,  6177,  1997,  1037, 10722, 14839,
         3482,  5436,  1029,  1528,  1999,  2054,  2126,  2064, 15973,  7680,
         7849,  3111,  2022,  3141,  2000, 20477, 15066,  1029,   102])"
126,1,"['descriptive statistics', 'observations', 'results', 'statistics', 'data', 'test results', 'test']", Introduction,seg_41,"in order to assess the knowledge of a given quantity of interest, one of the first steps is to investigate the data available, such as observations and test results. for this purpose, descriptive statistics is useful. descriptive statistics do not assume anything",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])",tensor([4955]),"tensor([  101,  1999,  2344,  2000, 14358,  1996,  3716,  1997,  1037,  2445,
        11712,  1997,  3037,  1010,  2028,  1997,  1996,  2034,  4084,  2003,
         2000,  8556,  1996,  2951,  2800,  1010,  2107,  2004,  9420,  1998,
         3231,  3463,  1012,  2005,  2023,  3800,  1010, 22726,  6747,  2003,
         6179,  1012, 22726,  6747,  2079,  2025,  7868,  2505,   102])"
127,1,"['standardized', 'data']", Introduction,seg_41,"in terms of the degree or nature of the randomness underlying the data analyzed but are merely a convenient tool to reduce the data to a manageable form suitable for further analysis, as well as for communication of the data in a standardized format to other professionals.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.])",tensor([4955]),"tensor([  101,  1999,  3408,  1997,  1996,  3014,  2030,  3267,  1997,  1996,
         6721,  2791, 10318,  1996,  2951, 16578,  2021,  2024,  6414,  1037,
        14057,  6994,  2000,  5547,  1996,  2951,  2000,  1037,  6133,  3085,
         2433,  7218,  2005,  2582,  4106,  1010,  2004,  2092,  2004,  2005,
         4807,  1997,  1996,  2951,  1999,  1037, 16367,  4289,  2000,  2060,
         8390,  1012,   102])"
128,1,"['representations', 'risk', 'associated', 'uncertainties', 'statistics', 'standardized', 'data', 'graphical representations', 'uncertainty', 'descriptive statistics', 'information', 'graphical', 'sample', 'numerical']", Introduction,seg_41,"in the following section numerical summaries will first be introduced. these can be considered to be numerical characteristics of the observed data containing important information about the data and the nature of uncertainty associated with these. these are also referred to as sample characteristics. thereafter, graphical representations are introduced as means of visual characterization and as a useful tool for data analysis. descriptive statistics play an important role in engineering risk analysis as a standardized basis for assessing and documenting data obtained for the purpose of understanding and representing uncertainties in risk assessment.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.])",tensor([4955]),"tensor([  101,  1999,  1996,  2206,  2930, 15973,  7680,  7849,  3111,  2097,
         2034,  2022,  3107,  1012,  2122,  2064,  2022,  2641,  2000,  2022,
        15973,  6459,  1997,  1996,  5159,  2951,  4820,  2590,  2592,  2055,
         1996,  2951,  1998,  1996,  3267,  1997, 12503,  3378,  2007,  2122,
         1012,  2122,  2024,  2036,  3615,  2000,  2004,  7099,  6459,  1012,
         6920,  1010, 20477, 15066,  2024,  3107,  2004,  2965,  1997,  5107,
        23191,  1998,  2004,  1037,  6179,  6994,  2005,  2951,  4106,  1012,
        22726,  6747,  2377,  2019,  2590,  2535,  1999,  3330,  3891,  4106,
         2004,  1037, 16367,  3978,  2005, 20077,  1998, 23138,  2951,  4663,
         2005,  1996,  3800,  1997,  4824,  1998,  5052,  9662,  7368,  1999,
         3891,  7667,  1012,   102])"
129,1,"['mean', 'set', 'numerical', 'sample', 'data set', 'data', 'sample mean']", Central Measures,seg_45,"one of the most useful numerical summaries is the sample mean. if the data set is collected in the vector x̂ = (x̂1, x̂2, . . . , x̂n)t the sample mean x̄ is simply given as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([2430, 5761])","tensor([  101,  2028,  1997,  1996,  2087,  6179, 15973,  7680,  7849,  3111,
         2003,  1996,  7099,  2812,  1012,  2065,  1996,  2951,  2275,  2003,
         5067,  1999,  1996,  9207,  1060,  1027,  1006,  1060,  2487,  1010,
         1060,  2475,  1010,  1012,  1012,  1012,  1010,  1060,  2078,  1007,
         1056,  1996,  7099,  2812,  1060,  2003,  3432,  2445,  2004,  1024,
          102])"
130,1,"['mean', 'set', 'sample', 'data set', 'data', 'sample mean']", Central Measures,seg_45,"the sample mean may be interpreted as a central value of the data set. if, on the basis of the data set, one should give only one value characterizing the data, one would normally use the sample mean.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([2430, 5761])","tensor([  101,  1996,  7099,  2812,  2089,  2022, 10009,  2004,  1037,  2430,
         3643,  1997,  1996,  2951,  2275,  1012,  2065,  1010,  2006,  1996,
         3978,  1997,  1996,  2951,  2275,  1010,  2028,  2323,  2507,  2069,
         2028,  3643,  2839,  6026,  1996,  2951,  1010,  2028,  2052,  5373,
         2224,  1996,  7099,  2812,  1012,   102])"
131,1,"['graphical representations', 'graphical', 'representations', 'set', 'central measure', 'samples', 'data set', 'data']", Central Measures,seg_45,"another central measure is the mode of the data set i.e. the most frequently occurring value in the data set. when data samples are real values, the mode in general cannot be assessed numerically, but may be assessed from graphical representations of the data as will be illustrated in sect. 3.3.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2430, 5761])","tensor([  101,  2178,  2430,  5468,  2003,  1996,  5549,  1997,  1996,  2951,
         2275,  1045,  1012,  1041,  1012,  1996,  2087,  4703, 10066,  3643,
         1999,  1996,  2951,  2275,  1012,  2043,  2951,  8168,  2024,  2613,
         5300,  1010,  1996,  5549,  1999,  2236,  3685,  2022, 14155, 15973,
         2135,  1010,  2021,  2089,  2022, 14155,  2013, 20477, 15066,  1997,
         1996,  2951,  2004,  2097,  2022,  7203,  1999, 17831,  1012,  1017,
         1012,  1017,  1012,   102])"
132,1,"['set', 'data set', 'data']", Central Measures,seg_45,"it is often convenient to work with an ordered data set which is readily established by rearranging the original data set x̂ = (x̂1, x̂2, . . . , x̂n)t such that the data are arranged in increasing order as x̂1o ≤ x̂2o ≤ · · · ≤ x̂io ≤ · · · ≤ x̂no−1 ≤ x̂no. the ith value of an ordered data set is denoted by x̂io.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([2430, 5761])","tensor([  101,  2009,  2003,  2411, 14057,  2000,  2147,  2007,  2019,  3641,
         2951,  2275,  2029,  2003, 12192,  2511,  2011,  4373, 24388,  2075,
         1996,  2434,  2951,  2275,  1060,  1027,  1006,  1060,  2487,  1010,
         1060,  2475,  1010,  1012,  1012,  1012,  1010,  1060,  2078,  1007,
         1056,  2107,  2008,  1996,  2951,  2024,  5412,  1999,  4852,  2344,
         2004,  1060,  2487,  2080,  1608,  1060,  2475,  2080,  1608,  1087,
         1087,  1087,  1608,  8418,  2080,  1608,  1087,  1087,  1087,  1608,
         1060,  3630, 27944,  1608,  1060,  3630,  1012,  1996,  2009,  2232,
         3643,  1997,  2019,  3641,  2951,  2275,  2003, 19537,  2011,  8418,
         2080,  1012,   102])"
133,1,"['data set', 'set', 'average', 'data', 'median']", Central Measures,seg_45,the median of the data set is defined as the middle value in the ordered list of data if n is odd. if n is even the median is taken as the average value of the two middle values (see also the examples below).,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2430, 5761])","tensor([ 101, 1996, 3991, 1997, 1996, 2951, 2275, 2003, 4225, 2004, 1996, 2690,
        3643, 1999, 1996, 3641, 2862, 1997, 2951, 2065, 1050, 2003, 5976, 1012,
        2065, 1050, 2003, 2130, 1996, 3991, 2003, 2579, 2004, 1996, 2779, 3643,
        1997, 1996, 2048, 2690, 5300, 1006, 2156, 2036, 1996, 4973, 2917, 1007,
        1012,  102])"
134,1,"['table', 'set', 'measurements', 'data set', 'data']", Example Concrete Compressive Strength Data,seg_47,"consider the data set given in table 3.1 corresponding to concrete cube compressive strength measurements. in the table the data are listed both unordered, e.g. in the order they were observed and ordered according to increasing values.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  5509,  4012, 27484,  3997,  2951])","tensor([  101,  5136,  1996,  2951,  2275,  2445,  1999,  2795,  1017,  1012,
         1015,  7978,  2000,  5509, 14291,  4012, 27484,  3997, 11702,  1012,
         1999,  1996,  2795,  1996,  2951,  2024,  3205,  2119, 27776, 26764,
         2098,  1010,  1041,  1012,  1043,  1012,  1999,  1996,  2344,  2027,
         2020,  5159,  1998,  3641,  2429,  2000,  4852,  5300,  1012,   102])"
135,1,"['mean', 'set', 'observations', 'sample', 'intervals', 'sample mean', 'data set', 'data', 'median']", Example Concrete Compressive Strength Data,seg_47,"the sample mean for the data set is readily evaluated using eq. 3.1 and found to be equal to 32.67 mpa. all the observed values are different and therefore the mode cannot be determined without dividing the observations into intervals as will be shown in sect. 3.3. however, the median is readily determined as being equal to 33.05 mpa.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 2742,  5509,  4012, 27484,  3997,  2951])","tensor([  101,  1996,  7099,  2812,  2005,  1996,  2951,  2275,  2003, 12192,
        16330,  2478,  1041,  4160,  1012,  1017,  1012,  1015,  1998,  2179,
         2000,  2022,  5020,  2000,  3590,  1012,  6163,  6131,  2050,  1012,
         2035,  1996,  5159,  5300,  2024,  2367,  1998,  3568,  1996,  5549,
         3685,  2022,  4340,  2302, 16023,  1996,  9420,  2046, 14025,  2004,
         2097,  2022,  3491,  1999, 17831,  1012,  1017,  1012,  1017,  1012,
         2174,  1010,  1996,  3991,  2003, 12192,  4340,  2004,  2108,  5020,
         2000,  3943,  1012,  5709,  6131,  2050,  1012,   102])"
136,1,"['data', 'table']", Example Traffic Flow Data,seg_49,consider the data shown in table 3.2. the data correspond to the daily traffic flow in both directions through the gotthard tunnel for the month of january 1997 obtained within a project carried out by the swiss federal highways office (astra).,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 4026, 4834, 2951])","tensor([  101,  5136,  1996,  2951,  3491,  1999,  2795,  1017,  1012,  1016,
         1012,  1996,  2951, 17254,  2000,  1996,  3679,  4026,  4834,  1999,
         2119,  7826,  2083,  1996,  2288,  8322,  4103,  5234,  2005,  1996,
         3204,  1997,  2254,  2722,  4663,  2306,  1037,  2622,  3344,  2041,
         2011,  1996,  5364,  2976, 10292,  2436,  1006,  2004,  6494,  1007,
         1012,   102])"
137,1,"['mean', 'sets', 'observation', 'set', 'observations', 'sample', 'data sets', 'sample mean', 'data set', 'data', 'median']", Example Traffic Flow Data,seg_49,"for this data set the sample mean values of the traffic flow in direction 1 and direction 2 may be calculated from either the unordered or ordered data sets to be equal to 4697.39 and 5660.77 respectively. the corresponding median values can be read from the ordered data sets as 4419 and 5100 respectively (there are in total 31 observations in the data sets, so the median corresponds to observation 16).",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.])","tensor([2742, 4026, 4834, 2951])","tensor([  101,  2005,  2023,  2951,  2275,  1996,  7099,  2812,  5300,  1997,
         1996,  4026,  4834,  1999,  3257,  1015,  1998,  3257,  1016,  2089,
         2022, 10174,  2013,  2593,  1996, 27776, 26764,  2098,  2030,  3641,
         2951,  4520,  2000,  2022,  5020,  2000,  4805,  2683,  2581,  1012,
         4464,  1998,  5179, 16086,  1012,  6255,  4414,  1012,  1996,  7978,
         3991,  5300,  2064,  2022,  3191,  2013,  1996,  3641,  2951,  4520,
         2004, 28015,  2683,  1998, 23475,  2692,  4414,  1006,  2045,  2024,
         1999,  2561,  2861,  9420,  1999,  1996,  2951,  4520,  1010,  2061,
         1996,  3991, 14788,  2000,  8089,  2385,  1007,  1012,   102])"
138,1,"['variability', 'mean', 'set', 'dispersion', 'sample', 'data set', 'data', 'sample mean']", Dispersion Measures,seg_51,the variability or the dispersion of the data set around the sample mean is also an,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0.])","tensor([ 4487, 17668, 10992,  5761])","tensor([  101,  1996, 28436,  2030,  1996,  4487, 17668, 10992,  1997,  1996,
         2951,  2275,  2105,  1996,  7099,  2812,  2003,  2036,  2019,   102])"
139,1,"['set', 'dispersion', 'data set', 'data']", Dispersion Measures,seg_51,important characteristic of the data set. the dispersion may be characterized by the,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 4487, 17668, 10992,  5761])","tensor([  101,  2590,  8281,  1997,  1996,  2951,  2275,  1012,  1996,  4487,
        17668, 10992,  2089,  2022,  7356,  2011,  1996,   102])"
140,1,['variance'], Dispersion Measures,seg_51,sample variance s2 given by:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 4487, 17668, 10992,  5761])","tensor([  101,  7099, 23284,  1055,  2475,  2445,  2011,  1024,   102])"
141,1,"['variability', 'mean', 'deviation', 'observations', 'sample', 'sample standard deviation', 'standard deviation', 'standard', 'sample mean']", Dispersion Measures,seg_51,the sample standard deviation s is defined as the square root of the sample variance. from eq. 3.2 it is seen that the sample standard deviation s is assessed in terms of the variability of the observations around the sample mean value x̄.,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([ 4487, 17668, 10992,  5761])","tensor([  101,  1996,  7099,  3115, 24353,  1055,  2003,  4225,  2004,  1996,
         2675,  7117,  1997,  1996,  7099, 23284,  1012,  2013,  1041,  4160,
         1012,  1017,  1012,  1016,  2009,  2003,  2464,  2008,  1996,  7099,
         3115, 24353,  1055,  2003, 14155,  1999,  3408,  1997,  1996, 28436,
         1997,  1996,  9420,  2105,  1996,  7099,  2812,  3643,  1060,  1012,
          102])"
142,1,"['mean', 'moment', 'variance', 'sample', 'deviations', 'sample variance', 'sample mean']", Dispersion Measures,seg_51,"thus, the sample variance is the mean of the squared deviations from the sample mean and is in this way analogous to the moment of inertia as used in structural engineering.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 4487, 17668, 10992,  5761])","tensor([  101,  2947,  1010,  1996,  7099, 23284,  2003,  1996,  2812,  1997,
         1996, 19942, 24353,  2015,  2013,  1996,  7099,  2812,  1998,  2003,
         1999,  2023,  2126, 19639,  2000,  1996,  2617,  1997,  1999,  8743,
         2401,  2004,  2109,  1999,  8332,  3330,  1012,   102])"
143,1,"['sample coefficient of variation', 'sets', 'coefficient', 'mean', 'dispersions', 'standard deviation', 'data', 'coefficient of variation', 'deviation', 'variation', 'standard', 'sample', 'sample standard deviation', 'data sets', 'sample mean']", Dispersion Measures,seg_51,"as a means of comparison of the dispersions of different data sets, the dimensionless sample coefficient of variation ν is convenient. the sample coefficient of variation ν is defined as the ratio of the sample standard deviation to the sample mean, i.e. given by:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 4487, 17668, 10992,  5761])","tensor([  101,  2004,  1037,  2965,  1997,  7831,  1997,  1996,  4487, 17668,
        27466,  1997,  2367,  2951,  4520,  1010,  1996,  9812,  3238,  7099,
        19064,  1997,  8386,  1167,  2003, 14057,  1012,  1996,  7099, 19064,
         1997,  8386,  1167,  2003,  4225,  2004,  1996,  6463,  1997,  1996,
         7099,  3115, 24353,  2000,  1996,  7099,  2812,  1010,  1045,  1012,
         1041,  1012,  2445,  2011,  1024,   102])"
144,1,"['sample coefficient of variation', 'deviation', 'variance', 'table', 'variation', 'coefficient', 'sample', 'sample standard deviation', 'standard deviation', 'standard', 'sample variance', 'data', 'coefficient of variation']", Dispersion Measures,seg_51,the sample variance for the concrete cube compressive strengths of table 3.1 may be evaluated using eq. 3.2 and is found to be 16.36 mpa2. the sample standard deviation is thus 4.04 mpa. for the considered concrete cube compressive strength data the sample coefficient of variation is equal to 0.12. in the same manner the sample coefficient of variation for the traffic flow data in table 3.2 is equal to 0.21 and 0.30 for direction 1 and direction 2 respectively. it is seen that the coefficient of variation for direction 2 is higher than for direction 1. that indicates that the data observed in direction 2 are more dispersed than in direction 1.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4487, 17668, 10992,  5761])","tensor([  101,  1996,  7099, 23284,  2005,  1996,  5509, 14291,  4012, 27484,
        20828,  1997,  2795,  1017,  1012,  1015,  2089,  2022, 16330,  2478,
         1041,  4160,  1012,  1017,  1012,  1016,  1998,  2003,  2179,  2000,
         2022,  2385,  1012,  4029,  6131,  2050,  2475,  1012,  1996,  7099,
         3115, 24353,  2003,  2947,  1018,  1012,  5840,  6131,  2050,  1012,
         2005,  1996,  2641,  5509, 14291,  4012, 27484,  3997,  2951,  1996,
         7099, 19064,  1997,  8386,  2003,  5020,  2000,  1014,  1012,  2260,
         1012,  1999,  1996,  2168,  5450,  1996,  7099, 19064,  1997,  8386,
         2005,  1996,  4026,  4834,  2951,  1999,  2795,  1017,  1012,  1016,
         2003,  5020,  2000,  1014,  1012,  2538,  1998,  1014,  1012,  2382,
         2005,  3257,  1015,  1998,  3257,  1016,  4414,  1012,  2009,  2003,
         2464,  2008,  1996, 19064,  1997,  8386,  2005,  3257,  1016,  2003,
         3020,  2084,  2005,  3257,  1015,  1012,  2008,  7127,  2008,  1996,
         2951,  5159,  1999,  3257,  1016,  2024,  2062, 15484,  2084,  1999,
         3257,  1015,  1012,   102])"
145,1,"['symmetry', 'coefficient', 'data set', 'median', 'mean', 'skewness', 'central measures', 'data', 'dispersion', 'sample variance', 'variance', 'set', 'sample', 'coefficient of skewness', 'sample mean']", Other Measures,seg_53,"whereas the sample mean, mode and median are central measures of a data set and the sample variance is a measure of the dispersion around the sample mean, it is also useful to have some characteristic indicating the degree of symmetry of the data set. to this end, the sample coefficient of skewness, which is a simple logical extension of the sample variance is suitable. the sample coefficient of skewness η is defined as:",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([2060, 5761])","tensor([  101,  6168,  1996,  7099,  2812,  1010,  5549,  1998,  3991,  2024,
         2430,  5761,  1997,  1037,  2951,  2275,  1998,  1996,  7099, 23284,
         2003,  1037,  5468,  1997,  1996,  4487, 17668, 10992,  2105,  1996,
         7099,  2812,  1010,  2009,  2003,  2036,  6179,  2000,  2031,  2070,
         8281,  8131,  1996,  3014,  1997, 14991,  1997,  1996,  2951,  2275,
         1012,  2000,  2023,  2203,  1010,  1996,  7099, 19064,  1997, 15315,
         7974,  2791,  1010,  2029,  2003,  1037,  3722, 11177,  5331,  1997,
         1996,  7099, 23284,  2003,  7218,  1012,  1996,  7099, 19064,  1997,
        15315,  7974,  2791,  1161,  2003,  4225,  2004,  1024,   102])"
146,1,"['skewness coefficient', 'table', 'coefficient', 'data set', 'mean', 'coefficients', 'observations', 'skewness', 'data', 'skewed', 'set', 'distributions', 'sample']", Other Measures,seg_53,this coefficient is positive if the mode of the data set is less than its mean value (skewed to the right) and negative if the mode is larger than the mean value (skewed to the left). for the concrete cube compressive strengths (table 3.1) the sample coefficient of skewness is −0.12. for the traffic flow data (table 3.2) the observations in directions 1 and 2 have a skewness coefficient of 1.54 and 2.25 respectively. the coefficients are positive and that show that both distributions are skewed to the right.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.])","tensor([2060, 5761])","tensor([  101,  2023, 19064,  2003,  3893,  2065,  1996,  5549,  1997,  1996,
         2951,  2275,  2003,  2625,  2084,  2049,  2812,  3643,  1006, 15315,
         7974,  2098,  2000,  1996,  2157,  1007,  1998,  4997,  2065,  1996,
         5549,  2003,  3469,  2084,  1996,  2812,  3643,  1006, 15315,  7974,
         2098,  2000,  1996,  2187,  1007,  1012,  2005,  1996,  5509, 14291,
         4012, 27484, 20828,  1006,  2795,  1017,  1012,  1015,  1007,  1996,
         7099, 19064,  1997, 15315,  7974,  2791,  2003,  1597,  2692,  1012,
         2260,  1012,  2005,  1996,  4026,  4834,  2951,  1006,  2795,  1017,
         1012,  1016,  1007,  1996,  9420,  1999,  7826,  1015,  1998,  1016,
         2031,  1037, 15315,  7974,  2791, 19064,  1997,  1015,  1012,  5139,
         1998,  1016,  1012,  2423,  4414,  1012,  1996, 21374,  2024,  3893,
         1998,  2008,  2265,  2008,  2119, 20611,  2024, 15315,  7974,  2098,
         2000,  1996,  2157,  1012,   102])"
147,1,"['kurtosis', 'sample', 'coefficient']", Other Measures,seg_53,in a similar way the sample coefficient of kurtosis κ is defined as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([2060, 5761])","tensor([  101,  1999,  1037,  2714,  2126,  1996,  7099, 19064,  1997,  9679,
        12650,  1164,  2003,  4225,  2004,  1024,   102])"
148,1,"['normal distribution', 'table', 'set', 'coefficient', 'sample', 'normal', 'distribution', 'kurtosis', 'data set', 'data']", Other Measures,seg_53,"which is a measure of how closely the data are distributed around the mode (peakedness). typically one would compare the sample coefficient of kurtosis to that of a normal distribution (introduced in chap. 4), which is equal to 3.0. the kurtosis for the concrete cube compressive strength (table 3.1) is evaluated as equal to 2.23, i.e. the considered data set is less peaked than the normal distribution. for the traffic flow data (table 3.2) it is equal to 5.48 and 7.44 for directions 1 and 2 respectively.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2060, 5761])","tensor([  101,  2029,  2003,  1037,  5468,  1997,  2129,  4876,  1996,  2951,
         2024,  5500,  2105,  1996,  5549,  1006,  6601,  2791,  1007,  1012,
         4050,  2028,  2052, 12826,  1996,  7099, 19064,  1997,  9679, 12650,
         2000,  2008,  1997,  1037,  3671,  4353,  1006,  3107,  1999, 15775,
         2361,  1012,  1018,  1007,  1010,  2029,  2003,  5020,  2000,  1017,
         1012,  1014,  1012,  1996,  9679, 12650,  2005,  1996,  5509, 14291,
         4012, 27484,  3997,  1006,  2795,  1017,  1012,  1015,  1007,  2003,
        16330,  2004,  5020,  2000,  1016,  1012,  2603,  1010,  1045,  1012,
         1041,  1012,  1996,  2641,  2951,  2275,  2003,  2625,  6601,  2084,
         1996,  3671,  4353,  1012,  2005,  1996,  4026,  4834,  2951,  1006,
         2795,  1017,  1012,  1016,  1007,  2009,  2003,  5020,  2000,  1019,
         1012,  4466,  1998,  1021,  1012,  4008,  2005,  7826,  1015,  1998,
         1016,  4414,  1012,   102])"
149,1,"['mean', 'observations', 'moments', 'sample', 'sample moments']", Sample Moments and Sample Central Moments,seg_55,"in eqs. 3.2, 3.4 and 3.5 the sample central moments are used. these are denoted as “central” as they are always taken about mean. the equations show that the sample mean x̄ is subtracted from the particular observations x̂i . in general the central sample moments can be defined as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([7099, 5312, 1998, 7099, 2430, 5312])","tensor([  101,  1999,  1041,  4160,  2015,  1012,  1017,  1012,  1016,  1010,
         1017,  1012,  1018,  1998,  1017,  1012,  1019,  1996,  7099,  2430,
         5312,  2024,  2109,  1012,  2122,  2024, 19537,  2004,  1523,  2430,
         1524,  2004,  2027,  2024,  2467,  2579,  2055,  2812,  1012,  1996,
        11380,  2265,  2008,  1996,  7099,  2812,  1060,  2003,  4942, 24301,
         2013,  1996,  3327,  9420,  8418,  1012,  1999,  2236,  1996,  2430,
         7099,  5312,  2064,  2022,  4225,  2004,  1024,   102])"
150,1,"['mean', 'moment', 'deviation', 'skewness', 'sample', 'moments', 'standard deviation', 'sample moment', 'standard', 'sample moments', 'kurtosis', 'sample mean']", Sample Moments and Sample Central Moments,seg_55,"in eqs. 3.4 and 3.5 the 3rd and 4th central sample moment are divided by the quadratic standard deviation to assess the skewness and the kurtosis. the “sim- ple” (non-central) sample moments do not use the sample mean in their definition. hence, the general definition is:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 5312, 1998, 7099, 2430, 5312])","tensor([  101,  1999,  1041,  4160,  2015,  1012,  1017,  1012,  1018,  1998,
         1017,  1012,  1019,  1996,  3822,  1998,  4343,  2430,  7099,  2617,
         2024,  4055,  2011,  1996, 17718, 23671,  3115, 24353,  2000, 14358,
         1996, 15315,  7974,  2791,  1998,  1996,  9679, 12650,  1012,  1996,
         1523, 21934,  1011, 20228,  2063,  1524,  1006,  2512,  1011,  2430,
         1007,  7099,  5312,  2079,  2025,  2224,  1996,  7099,  2812,  1999,
         2037,  6210,  1012,  6516,  1010,  1996,  2236,  6210,  2003,  1024,
          102])"
151,1,"['scatter diagram', 'jointly', 'data']", Measures of Correlation,seg_57,observations are often made of two characteristics simultaneously as shown in fig. 3.1 where pairs of data observed simultaneously are plotted jointly along the x-axis and the y-axis (this representation is also called a two-dimensional scatter diagram as outlined in sect. 3.3).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 5761,  1997, 16902])","tensor([  101,  9420,  2024,  2411,  2081,  1997,  2048,  6459,  7453,  2004,
         3491,  1999, 20965,  1012,  1017,  1012,  1015,  2073,  7689,  1997,
         2951,  5159,  7453,  2024, 27347, 10776,  2247,  1996,  1060,  1011,
         8123,  1998,  1996,  1061,  1011,  8123,  1006,  2023,  6630,  2003,
         2036,  2170,  1037,  2048,  1011,  8789,  8040, 20097, 16403,  2004,
        14801,  1999, 17831,  1012,  1017,  1012,  1017,  1007,  1012,   102])"
152,1,"['sets', 'sample', 'correlation', 'data sets', 'covariance', 'sample covariance', 'data']", Measures of Correlation,seg_57,"as a characteristic indicating the tendency toward high-high pairings and lowlow pairings, i.e. a measure of the correlation between the observed data sets, the sample covariance sxy is useful, and is defined as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 5761,  1997, 16902])","tensor([  101,  2004,  1037,  8281,  8131,  1996, 11765,  2646,  2152,  1011,
         2152, 22778,  2015,  1998,  2659,  8261, 22778,  2015,  1010,  1045,
         1012,  1041,  1012,  1037,  5468,  1997,  1996, 16902,  2090,  1996,
         5159,  2951,  4520,  1010,  1996,  7099,  2522, 10755, 28335,  1055,
        18037,  2003,  6179,  1010,  1998,  2003,  4225,  2004,  1024,   102])"
153,1,"['data set', 'linear', 'set', 'sample', 'correlation', 'sample covariance', 'covariance', 'data']", Measures of Correlation,seg_57,"the sample covariance has the property that, if there is a tendency in the data set that the values of x̂i and ŷi are both higher than x̄ and ȳ at the same time, and the trend is linear, then most of the terms in the sum will be positive and the sample covariance will be positive. the opposite trend will result in a negative sample covariance. such behavior is referred to as correlation.",tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 5761,  1997, 16902])","tensor([  101,  1996,  7099,  2522, 10755, 28335,  2038,  1996,  3200,  2008,
         1010,  2065,  2045,  2003,  1037, 11765,  1999,  1996,  2951,  2275,
         2008,  1996,  5300,  1997,  8418,  1998, 12316,  2024,  2119,  3020,
         2084,  1060,  1998,  1061,  2012,  1996,  2168,  2051,  1010,  1998,
         1996,  9874,  2003,  7399,  1010,  2059,  2087,  1997,  1996,  3408,
         1999,  1996,  7680,  2097,  2022,  3893,  1998,  1996,  7099,  2522,
        10755, 28335,  2097,  2022,  3893,  1012,  1996,  4500,  9874,  2097,
         2765,  1999,  1037,  4997,  7099,  2522, 10755, 28335,  1012,  2107,
         5248,  2003,  3615,  2000,  2004, 16902,  1012,   102])"
154,1,"['correlation', 'scatter diagram', 'data']", Measures of Correlation,seg_57,in the scatter diagram to the left in fig. 3.1 there is only little correlation between the observed data pairs whereas the opposite is evident in the example to the right.,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 5761,  1997, 16902])","tensor([  101,  1999,  1996,  8040, 20097, 16403,  2000,  1996,  2187,  1999,
        20965,  1012,  1017,  1012,  1015,  2045,  2003,  2069,  2210, 16902,
         2090,  1996,  5159,  2951,  7689,  6168,  1996,  4500,  2003, 10358,
         1999,  1996,  2742,  2000,  1996,  2157,  1012,   102])"
155,1,"['sets', 'coefficient', 'sample standard deviations', 'correlation coefficient', 'correlation', 'covariance', 'sample correlation coefficient', 'sample covariance', 'data', 'standard deviations', 'standard', 'normalized', 'sample', 'deviations', 'data sets']", Measures of Correlation,seg_57,the sample covariance may be normalized with respect to the sample standard deviations of the individual data sets sx and sy and the result is called the sample correlation coefficient rxy defined as:,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0.])","tensor([ 5761,  1997, 16902])","tensor([  101,  1996,  7099,  2522, 10755, 28335,  2089,  2022,  3671,  3550,
         2007,  4847,  2000,  1996,  7099,  3115, 24353,  2015,  1997,  1996,
         3265,  2951,  4520,  1055,  2595,  1998, 25353,  1998,  1996,  2765,
         2003,  2170,  1996,  7099, 16902, 19064,  1054, 18037,  4225,  2004,
         1024,   102])"
156,1,"['extreme values', 'correlated', 'sample correlation coefficient', 'interval', 'coefficient', 'correlation coefficient', 'sample', 'correlation', 'scatter diagram', 'data', 'case']", Measures of Correlation,seg_57,"the sample correlation coefficient has the property that it is limited to the interval −1 ≤ rxy ≤ 1 and the extreme values of the interval are only achieved in case the data pairs are perfectly correlated, implying that the points on the scatter diagram lie on a straight line. for the example shown in fig. 3.1 there is almost zero correlation at the left hand side and almost full positive correlation at the right hand side.",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 5761,  1997, 16902])","tensor([  101,  1996,  7099, 16902, 19064,  2038,  1996,  3200,  2008,  2009,
         2003,  3132,  2000,  1996, 13483,  1597,  2487,  1608,  1054, 18037,
         1608,  1015,  1998,  1996,  6034,  5300,  1997,  1996, 13483,  2024,
         2069,  4719,  1999,  2553,  1996,  2951,  7689,  2024,  6669, 23900,
         1010, 20242,  2008,  1996,  2685,  2006,  1996,  8040, 20097, 16403,
         4682,  2006,  1037,  3442,  2240,  1012,  2005,  1996,  2742,  3491,
         1999, 20965,  1012,  1017,  1012,  1015,  2045,  2003,  2471,  5717,
        16902,  2012,  1996,  2187,  2192,  2217,  1998,  2471,  2440,  3893,
        16902,  2012,  1996,  2157,  2192,  2217,  1012,   102])"
157,1,"['graphical representations', 'sets', 'graphical', 'representations', 'observations', 'data sets', 'data']", Graphical Representations,seg_59,"graphical representations provide a convenient basis for assessing data as well as communicating these to other persons. there exist a relatively large number of different possible graphical representations of data, of which some are better suited than others depending on the purpose of the representations. some are better for representing the characteristics of data sets containing observations of one characteristic, e.g. the concrete compressive strength and others are better for representing the characteristics of two or more data sets e.g. the simultaneously observed traffic flows. in the following sub-sections the most frequently applied graphical representations are introduced and discussed with the help of examples.",tensor(1),"tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([20477, 15066])","tensor([  101, 20477, 15066,  3073,  1037, 14057,  3978,  2005, 20077,  2951,
         2004,  2092,  2004, 20888,  2122,  2000,  2060,  5381,  1012,  2045,
         4839,  1037,  4659,  2312,  2193,  1997,  2367,  2825, 20477, 15066,
         1997,  2951,  1010,  1997,  2029,  2070,  2024,  2488, 10897,  2084,
         2500,  5834,  2006,  1996,  3800,  1997,  1996, 15066,  1012,  2070,
         2024,  2488,  2005,  5052,  1996,  6459,  1997,  2951,  4520,  4820,
         9420,  1997,  2028,  8281,  1010,  1041,  1012,  1043,  1012,  1996,
         5509,  4012, 27484,  3997,  1998,  2500,  2024,  2488,  2005,  5052,
         1996,  6459,  1997,  2048,  2030,  2062,  2951,  4520,  1041,  1012,
         1043,  1012,  1996,  7453,  5159,  4026,  6223,  1012,  1999,  1996,
         2206,  4942,  1011,  5433,  1996,  2087,  4703,  4162, 20477, 15066,
         2024,  3107,  1998,  6936,  2007,  1996,  2393,  1997,  4973,  1012,
          102])"
158,1,"['cases', 'sets', 'graphical', 'histograms', 'set', 'plotting', 'observations', 'scatter diagram', 'data sets', 'plot', 'data set', 'data', 'case']", OneDimensional Scatter Diagrams,seg_61,"the simplest graphical representation is the scatter diagram which provides a means to represent observations contained in one or more data sets. the scatter diagram may be constructed by plotting the observed values of the data set along an axis labeled according to the scale of the observations. in a one-dimensional scatter diagram the minimum and maximum values of the data set can be readily observed. furthermore, as long as the number of data points is not very large, the central value of the observed data may be observed directly from the plot. in the case where a data set contains a large number of data, some of these may be overlapping and this makes it difficult to distinguish the individual observations. in such cases, it may be beneficial to apply another graphical representation such as histograms, as described subsequently.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2028, 22172,  6132, 19301,  8040, 20097, 26309])","tensor([  101,  1996, 21304, 20477,  6630,  2003,  1996,  8040, 20097, 16403,
         2029,  3640,  1037,  2965,  2000,  5050,  9420,  4838,  1999,  2028,
         2030,  2062,  2951,  4520,  1012,  1996,  8040, 20097, 16403,  2089,
         2022,  3833,  2011, 20699,  1996,  5159,  5300,  1997,  1996,  2951,
         2275,  2247,  2019,  8123, 12599,  2429,  2000,  1996,  4094,  1997,
         1996,  9420,  1012,  1999,  1037,  2028,  1011,  8789,  8040, 20097,
        16403,  1996,  6263,  1998,  4555,  5300,  1997,  1996,  2951,  2275,
         2064,  2022, 12192,  5159,  1012,  7297,  1010,  2004,  2146,  2004,
         1996,  2193,  1997,  2951,  2685,  2003,  2025,  2200,  2312,  1010,
         1996,  2430,  3643,  1997,  1996,  5159,  2951,  2089,  2022,  5159,
         3495,  2013,  1996,  5436,  1012,  1999,  1996,  2553,  2073,  1037,
         2951,  2275,  3397,  1037,  2312,  2193,  1997,  2951,  1010,  2070,
         1997,  2122,  2089,  2022, 20567,  1998,  2023,  3084,  2009,  3697,
         2000, 10782,  1996,  3265,  9420,  1012,  1999,  2107,  3572,  1010,
         2009,  2089,  2022, 15189,  2000,  6611,  2178, 20477,  6630,  2107,
         2004,  2010,  3406, 13113,  2015,  1010,  2004,  2649,  3525,  1012,
          102])"
159,1,"['range', 'table', 'plotting', 'observations', 'scatter diagram', 'data', 'concentration']", OneDimensional Scatter Diagrams,seg_61,"consider again the traffic flow data from table 3.2. for each of the two directions a one-dimensional scatter diagram can be produced by plotting the data along one axis. in fig. 3.2 the resulting scatter diagram is shown for direction 1. it can be seen from fig. 3.2 that the lowest value of the data lies close to 3000 while the highest lies close to 8000. moreover, a high concentration of observations is observed in the range 4000 to 5000, indicating that the central value of this data is in that range.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0.])","tensor([ 2028, 22172,  6132, 19301,  8040, 20097, 26309])","tensor([  101,  5136,  2153,  1996,  4026,  4834,  2951,  2013,  2795,  1017,
         1012,  1016,  1012,  2005,  2169,  1997,  1996,  2048,  7826,  1037,
         2028,  1011,  8789,  8040, 20097, 16403,  2064,  2022,  2550,  2011,
        20699,  1996,  2951,  2247,  2028,  8123,  1012,  1999, 20965,  1012,
         1017,  1012,  1016,  1996,  4525,  8040, 20097, 16403,  2003,  3491,
         2005,  3257,  1015,  1012,  2009,  2064,  2022,  2464,  2013, 20965,
         1012,  1017,  1012,  1016,  2008,  1996,  7290,  3643,  1997,  1996,
         2951,  3658,  2485,  2000, 11910,  2096,  1996,  3284,  3658,  2485,
         2000,  5385,  2692,  1012,  9308,  1010,  1037,  2152,  6693,  1997,
         9420,  2003,  5159,  1999,  1996,  2846, 20143,  2000, 13509,  1010,
         8131,  2008,  1996,  2430,  3643,  1997,  2023,  2951,  2003,  1999,
         2008,  2846,  1012,   102])"
160,1,"['cases', 'sets', 'table', 'estimated', 'interval', 'observations', 'estimate', 'frequency', 'histogram', 'plot', 'data', 'frequencies', 'number of observations', 'graphical', 'cumulative frequencies', 'cumulative frequency', 'data sets']", Histograms,seg_63,"a frequently applied graphical representation of data sets is the histogram. consider again as an example the traffic flow data from table 3.2 for direction 2. the data are further processed and the observed number of cars is subdivided into intervals, see table 3.3. for each interval the midpoint is determined and the number of observations within each interval is counted. thereafter, the frequencies of the measurements within each interval are evaluated as the number of observations within one interval divided by the total number of observations. the cumulative frequencies are estimated by summing up the frequencies for each interval in increasing order. this is a common way to estimate the cumulative frequencies especially in cases where the exact observations are not known but instead the frequency of observations within an interval is known. in the following, for illustration purposes, the cumulative frequencies are estimated from available observations. however, when observations are readily available the cumulative frequency plot can be replaced by",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  1037,  4703,  4162, 20477,  6630,  1997,  2951,  4520,  2003,
         1996,  2010,  3406, 13113,  1012,  5136,  2153,  2004,  2019,  2742,
         1996,  4026,  4834,  2951,  2013,  2795,  1017,  1012,  1016,  2005,
         3257,  1016,  1012,  1996,  2951,  2024,  2582, 13995,  1998,  1996,
         5159,  2193,  1997,  3765,  2003, 15369,  2046, 14025,  1010,  2156,
         2795,  1017,  1012,  1017,  1012,  2005,  2169, 13483,  1996,  3054,
         8400,  2003,  4340,  1998,  1996,  2193,  1997,  9420,  2306,  2169,
        13483,  2003,  8897,  1012,  6920,  1010,  1996, 13139,  1997,  1996,
        11702,  2306,  2169, 13483,  2024, 16330,  2004,  1996,  2193,  1997,
         9420,  2306,  2028, 13483,  4055,  2011,  1996,  2561,  2193,  1997,
         9420,  1012,  1996, 23260, 13139,  2024,  4358,  2011,  7680,  6562,
         2039,  1996, 13139,  2005,  2169, 13483,  1999,  4852,  2344,  1012,
         2023,  2003,  1037,  2691,  2126,  2000, 10197,  1996, 23260, 13139,
         2926,  1999,  3572,  2073,  1996,  6635,  9420,  2024,  2025,  2124,
         2021,  2612,  1996,  6075,  1997,  9420,  2306,  2019, 13483,  2003,
         2124,  1012,  1999,  1996,  2206,  1010,  2005, 14614,  5682,  1010,
         1996, 23260, 13139,  2024,  4358,  2013,  2800,  9420,  1012,  2174,
         1010,  2043,  9420,  2024, 12192,  2800,  1996, 23260,  6075,  5436,
         2064,  2022,  2999,  2011,   102])"
161,1,"['quantile', 'graphical', 'table', 'quantile plot', 'plot', 'data']", Histograms,seg_63,a plot similar to a quantile plot (see sect. 3.3.3) but a slightly different representation. figures 3.3 and 3.4 show the graphical representation of the processed data of table 3.3.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  1037,  5436,  2714,  2000,  1037, 24110, 15286,  5436,  1006,
         2156, 17831,  1012,  1017,  1012,  1017,  1012,  1017,  1007,  2021,
         1037,  3621,  2367,  6630,  1012,  4481,  1017,  1012,  1017,  1998,
         1017,  1012,  1018,  2265,  1996, 20477,  6630,  1997,  1996, 13995,
         2951,  1997,  2795,  1017,  1012,  1017,  1012,   102])"
162,1,"['interval', 'observations', 'information', 'histogram', 'data']", Histograms,seg_63,"it has to be noted that a histogram may reduce the information provided by the data examined. the interval width plays an important role for the resolution of the representation of the observations. however, there are no general guidelines con-",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  2009,  2038,  2000,  2022,  3264,  2008,  1037,  2010,  3406,
        13113,  2089,  5547,  1996,  2592,  3024,  2011,  1996,  2951,  8920,
         1012,  1996, 13483,  9381,  3248,  2019,  2590,  2535,  2005,  1996,
         5813,  1997,  1996,  6630,  1997,  1996,  9420,  1012,  2174,  1010,
         2045,  2024,  2053,  2236, 11594,  9530,  1011,   102])"
163,1,"['interval', 'observations', 'results', 'intervals', 'process']", Histograms,seg_63,"cerning the choice of the interval width. in most applications, the goal is to identify an interval, which with a sufficient resolution, can represent the observations and this may comprise an iterative process where several different subdivisions are applied and the results are evaluated. in benjamin and cornell [4] it is suggested to subdivide the interval between the maximum and minimum value into k intervals where k is given by:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  8292,  6826,  2075,  1996,  3601,  1997,  1996, 13483,  9381,
         1012,  1999,  2087,  5097,  1010,  1996,  3125,  2003,  2000,  6709,
         2019, 13483,  1010,  2029,  2007,  1037,  7182,  5813,  1010,  2064,
         5050,  1996,  9420,  1998,  2023,  2089, 15821,  2019,  2009, 25284,
         2832,  2073,  2195,  2367, 22095,  2024,  4162,  1998,  1996,  3463,
         2024, 16330,  1012,  1999,  6425,  1998, 10921,  1031,  1018,  1033,
         2009,  2003,  4081,  2000,  4942,  4305, 17258,  2063,  1996, 13483,
         2090,  1996,  4555,  1998,  6263,  3643,  2046,  1047, 14025,  2073,
         1047,  2003,  2445,  2011,  1024,   102])"
164,1,"['set', 'data set', 'data']", Histograms,seg_63,where n is the number of data points in the data set.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([ 101, 2073, 1050, 2003, 1996, 2193, 1997, 2951, 2685, 1999, 1996, 2951,
        2275, 1012,  102])"
165,1,"['table', 'frequency distribution', 'observations', 'frequency', 'intervals', 'distribution', 'data']", Histograms,seg_63,"using the above formula for the observations in table 3.2, k equals 5.92. by rounding up, 6 intervals should have been applied for the subdivision of observations while as shown in table 3.3 the number of intervals used is equal to 17. figure 3.5 illustrates the frequency distribution of the traffic flow data using the 6 intervals given in table 3.4.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  2478,  1996,  2682,  5675,  2005,  1996,  9420,  1999,  2795,
         1017,  1012,  1016,  1010,  1047, 19635,  1019,  1012,  6227,  1012,
         2011, 26939,  2039,  1010,  1020, 14025,  2323,  2031,  2042,  4162,
         2005,  1996, 12572,  1997,  9420,  2096,  2004,  3491,  1999,  2795,
         1017,  1012,  1017,  1996,  2193,  1997, 14025,  2109,  2003,  5020,
         2000,  2459,  1012,  3275,  1017,  1012,  1019, 24899,  1996,  6075,
         4353,  1997,  1996,  4026,  4834,  2951,  2478,  1996,  1020, 14025,
         2445,  1999,  2795,  1017,  1012,  1018,  1012,   102])"
166,1,"['graphical', 'frequency distribution', 'frequency', 'intervals', 'distribution']", Histograms,seg_63,compared with the frequency distribution in fig. 3.5 it can be seen that using a smaller number of intervals the resolution of the graphical representation is significantly reduced.,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  4102,  2007,  1996,  6075,  4353,  1999, 20965,  1012,  1017,
         1012,  1019,  2009,  2064,  2022,  2464,  2008,  2478,  1037,  3760,
         2193,  1997, 14025,  1996,  5813,  1997,  1996, 20477,  6630,  2003,
         6022,  4359,  1012,   102])"
167,1,"['table', 'set', 'observations', 'intervals', 'data set', 'data', 'case']", Histograms,seg_63,for the concrete compressive strength observations the application of eq. 3.8 seems to work well. equation 3.8 in this case gives a value of k = 5.29 and by rounding up 6 intervals should be used for this data set. the data organized according to the result of eq. 3.8 are given in table 3.5.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  2005,  1996,  5509,  4012, 27484,  3997,  9420,  1996,  4646,
         1997,  1041,  4160,  1012,  1017,  1012,  1022,  3849,  2000,  2147,
         2092,  1012,  8522,  1017,  1012,  1022,  1999,  2023,  2553,  3957,
         1037,  3643,  1997,  1047,  1027,  1019,  1012,  2756,  1998,  2011,
        26939,  2039,  1020, 14025,  2323,  2022,  2109,  2005,  2023,  2951,
         2275,  1012,  1996,  2951,  4114,  2429,  2000,  1996,  2765,  1997,
         1041,  4160,  1012,  1017,  1012,  1022,  2024,  2445,  1999,  2795,
         1017,  1012,  1019,  1012,   102])"
168,1,"['graphical', 'table', 'frequency distribution', 'observations', 'frequency', 'distribution', 'data']", Histograms,seg_63,figures 3.6 and 3.7 show the graphical representation of the processed data of table 3.5. it is seen from fig. 3.6 that the rule implied from eq. 3.8 works fine and the resulting frequency distribution provides a good resolution of the observations.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  4481,  1017,  1012,  1020,  1998,  1017,  1012,  1021,  2265,
         1996, 20477,  6630,  1997,  1996, 13995,  2951,  1997,  2795,  1017,
         1012,  1019,  1012,  2009,  2003,  2464,  2013, 20965,  1012,  1017,
         1012,  1020,  2008,  1996,  3627, 13339,  2013,  1041,  4160,  1012,
         1017,  1012,  1022,  2573,  2986,  1998,  1996,  4525,  6075,  4353,
         3640,  1037,  2204,  5813,  1997,  1996,  9420,  1012,   102])"
169,1,"['quantile', 'graphical representations', 'data set', 'observation', 'graphical', 'representations', 'set', 'observations', 'percentage', 'frequency', 'information', 'cumulative frequency', 'plots', 'data']", Quantile Plots,seg_65,"quantile plots are graphical representations containing information that is similar to the cumulative frequency plots introduced above. a quantile is related to a given percentage, e.g. the 0.65 quantile of a given data set of observations is the observation for which 65% of all observations in the data set have smaller values. the",tensor(1),"tensor([0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([24110, 15286, 14811])","tensor([  101, 24110, 15286, 14811,  2024, 20477, 15066,  4820,  2592,  2008,
         2003,  2714,  2000,  1996, 23260,  6075, 14811,  3107,  2682,  1012,
         1037, 24110, 15286,  2003,  3141,  2000,  1037,  2445,  7017,  1010,
         1041,  1012,  1043,  1012,  1996,  1014,  1012,  3515, 24110, 15286,
         1997,  1037,  2445,  2951,  2275,  1997,  9420,  2003,  1996,  8089,
         2005,  2029,  3515,  1003,  1997,  2035,  9420,  1999,  1996,  2951,
         2275,  2031,  3760,  5300,  1012,  1996,   102])"
170,1,"['quantile', 'box plots', 'tukey box plots', 'plots', 'upper quartile', 'quartile', 'median']", Quantile Plots,seg_65,0.75 quantile is also called the upper quartile (see also the tukey box plots in the next section) while the 0.25 quantile is called the lower quartile. the median is thus equal to 0.5 quantile.,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0.])","tensor([24110, 15286, 14811])","tensor([  101,  1014,  1012,  4293, 24110, 15286,  2003,  2036,  2170,  1996,
         3356, 24209,  8445,  9463,  1006,  2156,  2036,  1996, 10722, 14839,
         3482, 14811,  1999,  1996,  2279,  2930,  1007,  2096,  1996,  1014,
         1012,  2423, 24110, 15286,  2003,  2170,  1996,  2896, 24209,  8445,
         9463,  1012,  1996,  3991,  2003,  2947,  5020,  2000,  1014,  1012,
         1019, 24110, 15286,  1012,   102])"
171,1,"['quantile', 'observation', 'set', 'observations', 'quantile plot', 'plot', 'data set', 'data']", Quantile Plots,seg_65,"in order to construct a quantile plot the observations in the data set are arranged in ascending order. the quantile qυ is equal to a given observation x̂io in the ordered data set where the index of the quantile, υ , is given by:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([24110, 15286, 14811])","tensor([  101,  1999,  2344,  2000,  9570,  1037, 24110, 15286,  5436,  1996,
         9420,  1999,  1996,  2951,  2275,  2024,  5412,  1999, 22316,  2344,
         1012,  1996, 24110, 15286,  1053, 29735,  2003,  5020,  2000,  1037,
         2445,  8089,  8418,  2080,  1999,  1996,  3641,  2951,  2275,  2073,
         1996,  5950,  1997,  1996, 24110, 15286,  1010,  1175,  1010,  2003,
         2445,  2011,  1024,   102])"
172,1,"['quantile', 'data', 'median', 'table']", Quantile Plots,seg_65,as an example consider the traffic flow data from table 3.2. in table 3.6 the data are ordered in ascending order and the corresponding quantile indices are shown. the median (i.e. the 0.5 quantile) has been highlighted.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0.])","tensor([24110, 15286, 14811])","tensor([  101,  2004,  2019,  2742,  5136,  1996,  4026,  4834,  2951,  2013,
         2795,  1017,  1012,  1016,  1012,  1999,  2795,  1017,  1012,  1020,
         1996,  2951,  2024,  3641,  1999, 22316,  2344,  1998,  1996,  7978,
        24110, 15286, 29299,  2024,  3491,  1012,  1996,  3991,  1006,  1045,
         1012,  1041,  1012,  1996,  1014,  1012,  1019, 24110, 15286,  1007,
         2038,  2042, 11548,  1012,   102])"
173,1,"['quantile', 'table', 'interval', 'observations', 'frequency', 'distribution', 'quantiles', 'data', 'case']", Quantile Plots,seg_65,"as mentioned in sect. 3.3.2, when the observations are known it is preferable to use their quantiles to represent the cumulative distribution instead of the frequency of observations within interval. so for example, in the case of the traffic flow data for direction 2 (table 3.4) the cumulative distribution can be plotted the data values and the respective quantile indices. similarly, the cumulative distribution of the concrete cube compressive strength data can be plotted using the respective quantile indices shown in table 3.7. figure 3.8 illustrates the two cumulative distribution",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0.])","tensor([24110, 15286, 14811])","tensor([  101,  2004,  3855,  1999, 17831,  1012,  1017,  1012,  1017,  1012,
         1016,  1010,  2043,  1996,  9420,  2024,  2124,  2009,  2003,  9544,
         3085,  2000,  2224,  2037, 24110, 15286,  2015,  2000,  5050,  1996,
        23260,  4353,  2612,  1997,  1996,  6075,  1997,  9420,  2306, 13483,
         1012,  2061,  2005,  2742,  1010,  1999,  1996,  2553,  1997,  1996,
         4026,  4834,  2951,  2005,  3257,  1016,  1006,  2795,  1017,  1012,
         1018,  1007,  1996, 23260,  4353,  2064,  2022, 27347,  1996,  2951,
         5300,  1998,  1996,  7972, 24110, 15286, 29299,  1012,  6660,  1010,
         1996, 23260,  4353,  1997,  1996,  5509, 14291,  4012, 27484,  3997,
         2951,  2064,  2022, 27347,  2478,  1996,  7972, 24110, 15286, 29299,
         3491,  1999,  2795,  1017,  1012,  1021,  1012,  3275,  1017,  1012,
         1022, 24899,  1996,  2048, 23260,  4353,   102])"
174,1,"['quantile', 'set', 'data set', 'data', 'median']", Quantile Plots,seg_65,plots mentioned above. the median of the data set can be directly read from such a representation by finding the value that corresponds to the 0.5 quantile.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([24110, 15286, 14811])","tensor([  101, 14811,  3855,  2682,  1012,  1996,  3991,  1997,  1996,  2951,
         2275,  2064,  2022,  3495,  3191,  2013,  2107,  1037,  6630,  2011,
         4531,  1996,  3643,  2008, 14788,  2000,  1996,  1014,  1012,  1019,
        24110, 15286,  1012,   102])"
175,1,"['quantile', 'contrast', 'frequency', 'quantile plot', 'cumulative frequency', 'plots', 'plot', 'quantile plots']", Quantile Plots,seg_65,"the cumulative frequency plots can be distinguished from the quantile plots by the different axes titles. the x-axis of the quantile plot contains the quantile index, which in contrast can be found on the y-axis of the cumulative frequency plot. the latter connects the particular points of cumulative frequency values by drawing steps",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([24110, 15286, 14811])","tensor([  101,  1996, 23260,  6075, 14811,  2064,  2022,  5182,  2013,  1996,
        24110, 15286, 14811,  2011,  1996,  2367, 19589,  4486,  1012,  1996,
         1060,  1011,  8123,  1997,  1996, 24110, 15286,  5436,  3397,  1996,
        24110, 15286,  5950,  1010,  2029,  1999,  5688,  2064,  2022,  2179,
         2006,  1996,  1061,  1011,  8123,  1997,  1996, 23260,  6075,  5436,
         1012,  1996,  3732,  8539,  1996,  3327,  2685,  1997, 23260,  6075,
         5300,  2011,  5059,  4084,   102])"
176,1,"['quantile', 'plot', 'quantile plot']", Quantile Plots,seg_65,in between (fig. 3.8). the quantile plot simply illustrates the values by unconnected points (fig. 3.9).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([24110, 15286, 14811])","tensor([  101,  1999,  2090,  1006, 20965,  1012,  1017,  1012,  1022,  1007,
         1012,  1996, 24110, 15286,  5436,  3432, 24899,  1996,  5300,  2011,
         4895, 24230,  2685,  1006, 20965,  1012,  1017,  1012,  1023,  1007,
         1012,   102])"
177,1,"['quantile', 'plots', 'data', 'quantile plots']", Quantile Plots,seg_65,in fig. 3.9 the quantile plots for the traffic flow data for both directions are illustrated. in order to facilitate the comparison of the data these have been plotted,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([24110, 15286, 14811])","tensor([  101,  1999, 20965,  1012,  1017,  1012,  1023,  1996, 24110, 15286,
        14811,  2005,  1996,  4026,  4834,  2951,  2005,  2119,  7826,  2024,
         7203,  1012,  1999,  2344,  2000, 10956,  1996,  7831,  1997,  1996,
         2951,  2122,  2031,  2042, 27347,   102])"
178,1,"['quantile', 'table', 'plots', 'upper quartile', 'quartile', 'quantiles', 'data', 'quantile plots']", Quantile Plots,seg_65,on the same scale. it can be seen that the quantiles for the data in direction 2 are slightly higher than the corresponding ones for direction 1. for direction 1 the median (the 0.5 quantile) is close to 4500 (the real value read from table 3.6 is 4419). the corresponding value in direction 2 is slightly higher than 5000 (5100 read from table 3.6). the approximate values for the upper and lower quantiles may also be observed from the quantile plots. thus for example the lower quartile (0.25 quantile) in direction 1 is approximately equal to 4000 while the upper quartile (0.75 quantile) is close to 5000.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([24110, 15286, 14811])","tensor([  101,  2006,  1996,  2168,  4094,  1012,  2009,  2064,  2022,  2464,
         2008,  1996, 24110, 15286,  2015,  2005,  1996,  2951,  1999,  3257,
         1016,  2024,  3621,  3020,  2084,  1996,  7978,  3924,  2005,  3257,
         1015,  1012,  2005,  3257,  1015,  1996,  3991,  1006,  1996,  1014,
         1012,  1019, 24110, 15286,  1007,  2003,  2485,  2000, 10332,  2692,
         1006,  1996,  2613,  3643,  3191,  2013,  2795,  1017,  1012,  1020,
         2003, 28015,  2683,  1007,  1012,  1996,  7978,  3643,  1999,  3257,
         1016,  2003,  3621,  3020,  2084, 13509,  1006, 23475,  2692,  3191,
         2013,  2795,  1017,  1012,  1020,  1007,  1012,  1996, 15796,  5300,
         2005,  1996,  3356,  1998,  2896, 24110, 15286,  2015,  2089,  2036,
         2022,  5159,  2013,  1996, 24110, 15286, 14811,  1012,  2947,  2005,
         2742,  1996,  2896, 24209,  8445,  9463,  1006,  1014,  1012,  2423,
        24110, 15286,  1007,  1999,  3257,  1015,  2003,  3155,  5020,  2000,
        20143,  2096,  1996,  3356, 24209,  8445,  9463,  1006,  1014,  1012,
         4293, 24110, 15286,  1007,  2003,  2485,  2000, 13509,  1012,   102])"
179,1,"['quantile', 'slope', 'observations', 'information', 'quantile plot', 'scatter plot', 'plot', 'data', 'concentration']", Quantile Plots,seg_65,the slope of the quantile plot indicates the concentration of the data; a steep slope corresponds to a low concentration and a flat slope to a high concentration. the highest local concentration occurs when there are many observations with exactly the same value and this appears on the quantile plot by a horizontal series of points. for direction 1 the slope is quite flat up to about the 0.7 quantile. thereafter the slope increases and thus the concentration of the data is smaller. this matches the information provided by the one-dimensional scatter plot in fig. 3.2. the 0.7 quantile corresponds to a value close to 5000. it can be seen from fig. 3.2 that for larger observed traffic flow values the concentration of the observations decreases.,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0.])","tensor([24110, 15286, 14811])","tensor([  101,  1996,  9663,  1997,  1996, 24110, 15286,  5436,  7127,  1996,
         6693,  1997,  1996,  2951,  1025,  1037,  9561,  9663, 14788,  2000,
         1037,  2659,  6693,  1998,  1037,  4257,  9663,  2000,  1037,  2152,
         6693,  1012,  1996,  3284,  2334,  6693,  5158,  2043,  2045,  2024,
         2116,  9420,  2007,  3599,  1996,  2168,  3643,  1998,  2023,  3544,
         2006,  1996, 24110, 15286,  5436,  2011,  1037,  9876,  2186,  1997,
         2685,  1012,  2005,  3257,  1015,  1996,  9663,  2003,  3243,  4257,
         2039,  2000,  2055,  1996,  1014,  1012,  1021, 24110, 15286,  1012,
         6920,  1996,  9663,  7457,  1998,  2947,  1996,  6693,  1997,  1996,
         2951,  2003,  3760,  1012,  2023,  3503,  1996,  2592,  3024,  2011,
         1996,  2028,  1011,  8789,  8040, 20097,  5436,  1999, 20965,  1012,
         1017,  1012,  1016,  1012,  1996,  1014,  1012,  1021, 24110, 15286,
        14788,  2000,  1037,  3643,  2485,  2000, 13509,  1012,  2009,  2064,
         2022,  2464,  2013, 20965,  1012,  1017,  1012,  1016,  2008,  2005,
         3469,  5159,  4026,  4834,  5300,  1996,  6693,  1997,  1996,  9420,
        17913,  1012,   102])"
180,1,"['quantile', 'symmetric', 'set', 'symmetry', 'information', 'observations', 'quantile plot', 'plots', 'plot', 'data set', 'data', 'median']", Quantile Plots,seg_65,quantile plots may also provide information regarding the symmetry of data. if the observations in the data set are symmetrically dispersed around the median then the shape of the quantile plot in the upper half is a double mirrored image of the shape from the lower half. from fig. 3.9 it can be seen that for both directions the data are not symmetric and that for direction 2 the asymmetry is more pronounced.,tensor(1),"tensor([0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([24110, 15286, 14811])","tensor([  101, 24110, 15286, 14811,  2089,  2036,  3073,  2592,  4953,  1996,
        14991,  1997,  2951,  1012,  2065,  1996,  9420,  1999,  1996,  2951,
         2275,  2024, 23476,  2135, 15484,  2105,  1996,  3991,  2059,  1996,
         4338,  1997,  1996, 24110, 15286,  5436,  1999,  1996,  3356,  2431,
         2003,  1037,  3313, 22243,  3746,  1997,  1996,  4338,  2013,  1996,
         2896,  2431,  1012,  2013, 20965,  1012,  1017,  1012,  1023,  2009,
         2064,  2022,  2464,  2008,  2005,  2119,  7826,  1996,  2951,  2024,
         2025, 19490,  1998,  2008,  2005,  3257,  1016,  1996,  2004, 24335,
        24327,  2003,  2062,  8793,  1012,   102])"
181,1,"['quantile', 'range', 'table', 'slope', 'observations', 'quantile plot', 'plot', 'data']", Quantile Plots,seg_65,"following the same procedure as described above, the concrete cube compressive strength data are plotted in fig. 3.10, against the respective quantile values given in table 3.7. it can be seen that the quantile plot has an almost constant slope over the whole range of observations.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 0., 0.])","tensor([24110, 15286, 14811])","tensor([  101,  2206,  1996,  2168,  7709,  2004,  2649,  2682,  1010,  1996,
         5509, 14291,  4012, 27484,  3997,  2951,  2024, 27347,  1999, 20965,
         1012,  1017,  1012,  2184,  1010,  2114,  1996,  7972, 24110, 15286,
         5300,  2445,  1999,  2795,  1017,  1012,  1021,  1012,  2009,  2064,
         2022,  2464,  2008,  1996, 24110, 15286,  5436,  2038,  2019,  2471,
         5377,  9663,  2058,  1996,  2878,  2846,  1997,  9420,  1012,   102])"
182,1,"['quantile', 'observation', 'table', 'set', 'data set', 'data']", Quantile Plots,seg_65,from table 3.7 it can be seen that no observation corresponds directly to the median of the data set. in general the evaluation of a quantile which does not correspond to a given observation must be based on an interpolation. this may be performed by first calculating the hypothetical ith observation x̂io equal to a given quantile qυ with the index υ:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([24110, 15286, 14811])","tensor([  101,  2013,  2795,  1017,  1012,  1021,  2009,  2064,  2022,  2464,
         2008,  2053,  8089, 14788,  3495,  2000,  1996,  3991,  1997,  1996,
         2951,  2275,  1012,  1999,  2236,  1996,  9312,  1997,  1037, 24110,
        15286,  2029,  2515,  2025, 17254,  2000,  1037,  2445,  8089,  2442,
         2022,  2241,  2006,  2019,  6970, 18155,  3370,  1012,  2023,  2089,
         2022,  2864,  2011,  2034, 20177,  1996, 25613,  2009,  2232,  8089,
         8418,  2080,  5020,  2000,  1037,  2445, 24110, 15286,  1053, 29735,
         2007,  1996,  5950,  1175,  1024,   102])"
183,1,"['quantile', 'observation', 'integer part']", Quantile Plots,seg_65,"if i is an integer, the ith observation x̂io exists and is equal to qυ . if i is not an integer it will have a value consisting of an integer part, say k, and a fractional part, say p. the quantile qυ = x̂io using interpolation is given as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([24110, 15286, 14811])","tensor([  101,  2065,  1045,  2003,  2019, 16109,  1010,  1996,  2009,  2232,
         8089,  8418,  2080,  6526,  1998,  2003,  5020,  2000,  1053, 29735,
         1012,  2065,  1045,  2003,  2025,  2019, 16109,  2009,  2097,  2031,
         1037,  3643,  5398,  1997,  2019, 16109,  2112,  1010,  2360,  1047,
         1010,  1998,  1037, 12884,  2389,  2112,  1010,  2360,  1052,  1012,
         1996, 24110, 15286,  1053, 29735,  1027,  8418,  2080,  2478,  6970,
        18155,  3370,  2003,  2445,  2004,  1024,   102])"
184,1,"['quantile', 'upper quartile', 'quartile', 'data', 'case']", Quantile Plots,seg_65,"for example, in the case of the concrete cube compressive strength data (table 3.7) looking for the upper quartile (0.75 quantile) gives a value of i equal to:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([24110, 15286, 14811])","tensor([  101,  2005,  2742,  1010,  1999,  1996,  2553,  1997,  1996,  5509,
        14291,  4012, 27484,  3997,  2951,  1006,  2795,  1017,  1012,  1021,
         1007,  2559,  2005,  1996,  3356, 24209,  8445,  9463,  1006,  1014,
         1012,  4293, 24110, 15286,  1007,  3957,  1037,  3643,  1997,  1045,
         5020,  2000,  1024,   102])"
185,1,['quantile'], Quantile Plots,seg_65,"therefore, based on eq. 3.12 the 0.75 quantile is:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0.])","tensor([24110, 15286, 14811])","tensor([  101,  3568,  1010,  2241,  2006,  1041,  4160,  1012,  1017,  1012,
         2260,  1996,  1014,  1012,  4293, 24110, 15286,  2003,  1024,   102])"
186,1,"['box plots', 'set', 'information', 'sample', 'plots', 'data set', 'data']", Tukey Box Plots,seg_67,"tukey box plots provide information about several sample characteristics of the observations contained in a data set, see fig. 3.11.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([10722, 14839,  3482, 14811])","tensor([  101, 10722, 14839,  3482, 14811,  3073,  2592,  2055,  2195,  7099,
         6459,  1997,  1996,  9420,  4838,  1999,  1037,  2951,  2275,  1010,
         2156, 20965,  1012,  1017,  1012,  2340,  1012,   102])"
187,1,"['range', 'observations', 'interquartile range', 'quartiles', 'data', 'median']", Tukey Box Plots,seg_67,"the median is typically represented by a circle or a horizontal line within the box. the upper and lower sides of the box indicate the values of the upper and the lower quartiles, respectively. the distance between these quartiles is called the interquartile range, r ; 50% of the data are located within this range. a large interquartile range indicates that the observations are widely dispersed around the median and vice versa.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0.])","tensor([10722, 14839,  3482, 14811])","tensor([  101,  1996,  3991,  2003,  4050,  3421,  2011,  1037,  4418,  2030,
         1037,  9876,  2240,  2306,  1996,  3482,  1012,  1996,  3356,  1998,
         2896,  3903,  1997,  1996,  3482,  5769,  1996,  5300,  1997,  1996,
         3356,  1998,  1996,  2896, 24209,  8445,  9463,  2015,  1010,  4414,
         1012,  1996,  3292,  2090,  2122, 24209,  8445,  9463,  2015,  2003,
         2170,  1996,  6970, 16211, 28228,  2571,  2846,  1010,  1054,  1025,
         2753,  1003,  1997,  1996,  2951,  2024,  2284,  2306,  2023,  2846,
         1012,  1037,  2312,  6970, 16211, 28228,  2571,  2846,  7127,  2008,
         1996,  9420,  2024,  4235, 15484,  2105,  1996,  3991,  1998,  3580,
        18601,  1012,   102])"
188,1,"['outside value', 'upper quartile', 'observation', 'box plot', 'adjacent value', 'adjacent values', 'quartile', 'plot', 'tukey box plot']", Tukey Box Plots,seg_67,"another feature of the tukey box plot is the adjacent values. the upper adjacent value is defined as the largest observation less than or equal to the upper quartile plus 1.5r . the lower adjacent value is defined as the smallest observation greater than or equal to the lower quartile minus 1.5r . if an observation has a value outside the adjacent values, the observation is called an outside value and is shown in the box plot by a single point.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0.])","tensor([10722, 14839,  3482, 14811])","tensor([  101,  2178,  3444,  1997,  1996, 10722, 14839,  3482,  5436,  2003,
         1996,  5516,  5300,  1012,  1996,  3356,  5516,  3643,  2003,  4225,
         2004,  1996,  2922,  8089,  2625,  2084,  2030,  5020,  2000,  1996,
         3356, 24209,  8445,  9463,  4606,  1015,  1012,  1019,  2099,  1012,
         1996,  2896,  5516,  3643,  2003,  4225,  2004,  1996, 10479,  8089,
         3618,  2084,  2030,  5020,  2000,  1996,  2896, 24209,  8445,  9463,
        15718,  1015,  1012,  1019,  2099,  1012,  2065,  2019,  8089,  2038,
         1037,  3643,  2648,  1996,  5516,  5300,  1010,  1996,  8089,  2003,
         2170,  2019,  2648,  3643,  1998,  2003,  3491,  1999,  1996,  3482,
         5436,  2011,  1037,  2309,  2391,  1012,   102])"
189,1,"['box plots', 'table', 'sample', 'tukey box plots', 'box plot', 'plots', 'plot', 'tukey box plot', 'data']", Tukey Box Plots,seg_67,in table 3.8 the sample characteristics needed to construct the tukey box plots for the traffic flow data are given. the tukey box plot for this data is shown in fig. 3.12.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([10722, 14839,  3482, 14811])","tensor([  101,  1999,  2795,  1017,  1012,  1022,  1996,  7099,  6459,  2734,
         2000,  9570,  1996, 10722, 14839,  3482, 14811,  2005,  1996,  4026,
         4834,  2951,  2024,  2445,  1012,  1996, 10722, 14839,  3482,  5436,
         2005,  2023,  2951,  2003,  3491,  1999, 20965,  1012,  1017,  1012,
         2260,  1012,   102])"
190,1,"['symmetry', 'observations', 'box plot', 'plot', 'tukey box plot']", Tukey Box Plots,seg_67,the symmetry of the observations represented in a tukey box plot may be partially assessed. from fig. 3.12 can be seen that in direction 1 the observations with,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([10722, 14839,  3482, 14811])","tensor([  101,  1996, 14991,  1997,  1996,  9420,  3421,  1999,  1037, 10722,
        14839,  3482,  5436,  2089,  2022,  6822, 14155,  1012,  2013, 20965,
         1012,  1017,  1012,  2260,  2064,  2022,  2464,  2008,  1999,  3257,
         1015,  1996,  9420,  2007,   102])"
191,1,"['symmetric', 'range']", Tukey Box Plots,seg_67,values in the lower and the upper range are more symmetric than the ones in direction 2. it is seen that the observed values of traffic flow in direction 2 are systematically larger than for direction 1.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([10722, 14839,  3482, 14811])","tensor([  101,  5300,  1999,  1996,  2896,  1998,  1996,  3356,  2846,  2024,
         2062, 19490,  2084,  1996,  3924,  1999,  3257,  1016,  1012,  2009,
         2003,  2464,  2008,  1996,  5159,  5300,  1997,  4026,  4834,  1999,
         3257,  1016,  2024, 23087,  3469,  2084,  2005,  3257,  1015,  1012,
          102])"
192,1,"['data', 'set', 'sample statistics', 'outside values', 'table', 'sample', 'box plot', 'adjacent value', 'statistics', 'plot', 'tukey box plot']", Tukey Box Plots,seg_67,in fig. 3.13 the tukey box plot for the concrete cube compressive strength data is given based on the evaluation of the respective sample statistics given in table 3.9. for this set of data there are no outside values as the upper adjacent value is the maximum value of the data and the lower adjacent value corresponds to the lower value of the data.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0.])","tensor([10722, 14839,  3482, 14811])","tensor([  101,  1999, 20965,  1012,  1017,  1012,  2410,  1996, 10722, 14839,
         3482,  5436,  2005,  1996,  5509, 14291,  4012, 27484,  3997,  2951,
         2003,  2445,  2241,  2006,  1996,  9312,  1997,  1996,  7972,  7099,
         6747,  2445,  1999,  2795,  1017,  1012,  1023,  1012,  2005,  2023,
         2275,  1997,  2951,  2045,  2024,  2053,  2648,  5300,  2004,  1996,
         3356,  5516,  3643,  2003,  1996,  4555,  3643,  1997,  1996,  2951,
         1998,  1996,  2896,  5516,  3643, 14788,  2000,  1996,  2896,  3643,
         1997,  1996,  2951,  1012,   102])"
193,1,"['quantile', 'sets', 'observations', 'efficient', 'plots', 'data sets', 'plot', 'quantiles', 'data', 'case']", QQ Plots and Tukey MeanDifference Plot,seg_69,"quantile-quantile plots (or in short q-q plots) provide an efficient means of comparing observations from different data sets. for example in the case of the traffic flow data a comparison may be made between the number of cars in one direction and the number of cars in another direction. to do so, the corresponding quantiles may be compared, i.e. the 0.25 quantile of the observations for direction 1 with the 0.25 quantile of the observations for direction 2 etc. for this purpose the corresponding quantiles are plotted against each other in a q-q plot.",tensor(1),"tensor([0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 1053,  4160, 14811,  1998, 10722, 14839,  2812,  4305, 12494, 10127,
         5436])","tensor([  101, 24110, 15286,  1011, 24110, 15286, 14811,  1006,  2030,  1999,
         2460,  1053,  1011,  1053, 14811,  1007,  3073,  2019,  8114,  2965,
         1997, 13599,  9420,  2013,  2367,  2951,  4520,  1012,  2005,  2742,
         1999,  1996,  2553,  1997,  1996,  4026,  4834,  2951,  1037,  7831,
         2089,  2022,  2081,  2090,  1996,  2193,  1997,  3765,  1999,  2028,
         3257,  1998,  1996,  2193,  1997,  3765,  1999,  2178,  3257,  1012,
         2000,  2079,  2061,  1010,  1996,  7978, 24110, 15286,  2015,  2089,
         2022,  4102,  1010,  1045,  1012,  1041,  1012,  1996,  1014,  1012,
         2423, 24110, 15286,  1997,  1996,  9420,  2005,  3257,  1015,  2007,
         1996,  1014,  1012,  2423, 24110, 15286,  1997,  1996,  9420,  2005,
         3257,  1016,  4385,  1012,  2005,  2023,  3800,  1996,  7978, 24110,
        15286,  2015,  2024, 27347,  2114,  2169,  2060,  1999,  1037,  1053,
         1011,  1053,  5436,  1012,   102])"
194,1,"['sets', 'table', 'plotting', 'observations', 'number of observations', 'data sets', 'plot', 'quantiles', 'data']", QQ Plots and Tukey MeanDifference Plot,seg_69,"the data sets as given in table 3.6 contain the same number of observations and therefore their q-q plot may be made by plotting the evaluated quantiles against each other, see fig. 3.14.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1053,  4160, 14811,  1998, 10722, 14839,  2812,  4305, 12494, 10127,
         5436])","tensor([  101,  1996,  2951,  4520,  2004,  2445,  1999,  2795,  1017,  1012,
         1020,  5383,  1996,  2168,  2193,  1997,  9420,  1998,  3568,  2037,
         1053,  1011,  1053,  5436,  2089,  2022,  2081,  2011, 20699,  1996,
        16330, 24110, 15286,  2015,  2114,  2169,  2060,  1010,  2156, 20965,
         1012,  1017,  1012,  2403,  1012,   102])"
195,1,"['sets', 'range', 'set', 'observations', 'number of observations', 'distributions', 'data sets', 'plot', 'data set', 'quantiles', 'data']", QQ Plots and Tukey MeanDifference Plot,seg_69,"if the data sets compared do not have the same number of observations then the quantiles for the observations of one data set are evaluated first and subsequently the corresponding quantiles for the other data set are established by interpolation. from fig. 3.14 it is seen that the traffic flow is higher over the full range of observations for direction 2 as compared to direction 1. if the q-q plot would result in a line close to the y = x line, then the data would have nearly identical distributions.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 1053,  4160, 14811,  1998, 10722, 14839,  2812,  4305, 12494, 10127,
         5436])","tensor([  101,  2065,  1996,  2951,  4520,  4102,  2079,  2025,  2031,  1996,
         2168,  2193,  1997,  9420,  2059,  1996, 24110, 15286,  2015,  2005,
         1996,  9420,  1997,  2028,  2951,  2275,  2024, 16330,  2034,  1998,
         3525,  1996,  7978, 24110, 15286,  2015,  2005,  1996,  2060,  2951,
         2275,  2024,  2511,  2011,  6970, 18155,  3370,  1012,  2013, 20965,
         1012,  1017,  1012,  2403,  2009,  2003,  2464,  2008,  1996,  4026,
         4834,  2003,  3020,  2058,  1996,  2440,  2846,  1997,  9420,  2005,
         3257,  1016,  2004,  4102,  2000,  3257,  1015,  1012,  2065,  1996,
         1053,  1011,  1053,  5436,  2052,  2765,  1999,  1037,  2240,  2485,
         2000,  1996,  1061,  1027,  1060,  2240,  1010,  2059,  1996,  2951,
         2052,  2031,  3053,  7235, 20611,  1012,   102])"
196,1,"['sets', 'graphical', 'observations', 'data sets', 'plot', 'data']", QQ Plots and Tukey MeanDifference Plot,seg_69,"another graphical representation that facilitates the comparison of the observations contained in two different data sets is the tukey mean-difference plot. here ŷio − x̂io is plotted against (ŷio + x̂io)/2, where ŷio and x̂io are the observations of the",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0.])","tensor([ 1053,  4160, 14811,  1998, 10722, 14839,  2812,  4305, 12494, 10127,
         5436])","tensor([  101,  2178, 20477,  6630,  2008, 27777,  1996,  7831,  1997,  1996,
         9420,  4838,  1999,  2048,  2367,  2951,  4520,  2003,  1996, 10722,
        14839,  2812,  1011,  4489,  5436,  1012,  2182, 12316,  2080,  1597,
         8418,  2080,  2003, 27347,  2114,  1006, 12316,  2080,  1009,  8418,
         2080,  1007,  1013,  1016,  1010,  2073, 12316,  2080,  1998,  8418,
         2080,  2024,  1996,  9420,  1997,  1996,   102])"
197,1,"['sets', 'table', 'data sets', 'data']", QQ Plots and Tukey MeanDifference Plot,seg_69,ordered data sets being compared. the evaluation of the means and differences for the traffic flow data is provided in table 3.10.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 1053,  4160, 14811,  1998, 10722, 14839,  2812,  4305, 12494, 10127,
         5436])","tensor([ 101, 3641, 2951, 4520, 2108, 4102, 1012, 1996, 9312, 1997, 1996, 2965,
        1998, 5966, 2005, 1996, 4026, 4834, 2951, 2003, 3024, 1999, 2795, 1017,
        1012, 2184, 1012,  102])"
198,1,"['mean', 'data', 'plot']", QQ Plots and Tukey MeanDifference Plot,seg_69,"in fig. 3.15 the tukey mean-difference plot is given for the traffic flow data. the tukey mean-difference plot indicates that there is a systematic difference of 600 cars per day for traffic situations corresponding to a mean traffic flow of up to 6000 cars per day. thereafter, the differences in the two directions seem to be proportional in the mean traffic flow.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0.])","tensor([ 1053,  4160, 14811,  1998, 10722, 14839,  2812,  4305, 12494, 10127,
         5436])","tensor([  101,  1999, 20965,  1012,  1017,  1012,  2321,  1996, 10722, 14839,
         2812,  1011,  4489,  5436,  2003,  2445,  2005,  1996,  4026,  4834,
         2951,  1012,  1996, 10722, 14839,  2812,  1011,  4489,  5436,  7127,
         2008,  2045,  2003,  1037, 11778,  4489,  1997,  5174,  3765,  2566,
         2154,  2005,  4026,  8146,  7978,  2000,  1037,  2812,  4026,  4834,
         1997,  2039,  2000, 25961,  3765,  2566,  2154,  1012,  6920,  1010,
         1996,  5966,  1999,  1996,  2048,  7826,  4025,  2000,  2022, 14267,
         1999,  1996,  2812,  4026,  4834,  1012,   102])"
199,1,"['coefficient of correlation', 'sets', 'data', 'interval', 'descriptive statistics', 'coefficient', 'correlation', 'data sets', 'statistics']", Self Assessment QuestionsExercises,seg_71,1. what is the purpose of descriptive statistics? 2. within which interval can the coefficient of correlation of two data sets lie? what,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1015,  1012,  2054,  2003,  1996,  3800,  1997, 22726,  6747,
         1029,  1016,  1012,  2306,  2029, 13483,  2064,  1996, 19064,  1997,
        16902,  1997,  2048,  2951,  4520,  4682,  1029,  2054,   102])"
200,1,"['interval', 'extreme values', 'histogram']", Self Assessment QuestionsExercises,seg_71,do the extreme values of the interval express? 3. what is the role of the interval width chosen for building up a histogram for the,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2079,  1996,  6034,  5300,  1997,  1996, 13483,  4671,  1029,
         1017,  1012,  2054,  2003,  1996,  2535,  1997,  1996, 13483,  9381,
         4217,  2005,  2311,  2039,  1037,  2010,  3406, 13113,  2005,  1996,
          102])"
201,1,"['data set', 'sets', 'set', 'estimate', 'coefficient', 'correlation coefficient', 'correlation', 'box plot', 'plots', 'data sets', 'plot', 'tukey box plot', 'data']", Self Assessment QuestionsExercises,seg_71,representation of a data set? 4. which characteristics of a data set can be represented with a tukey box plot? 5. how can q-q plots be used? 6. provide a rough estimate of the correlation coefficient of the data sets plotted in,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  6630,  1997,  1037,  2951,  2275,  1029,  1018,  1012,  2029,
         6459,  1997,  1037,  2951,  2275,  2064,  2022,  3421,  2007,  1037,
        10722, 14839,  3482,  5436,  1029,  1019,  1012,  2129,  2064,  1053,
         1011,  1053, 14811,  2022,  2109,  1029,  1020,  1012,  3073,  1037,
         5931, 10197,  1997,  1996, 16902, 19064,  1997,  1996,  2951,  4520,
        27347,  1999,   102])"
202,1,"['statistical', 'table']", Self Assessment QuestionsExercises,seg_71,7. a number of statistical terms are shown in table 3.11. check if the terms have,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([ 101, 1021, 1012, 1037, 2193, 1997, 7778, 3408, 2024, 3491, 1999, 2795,
        1017, 1012, 2340, 1012, 4638, 2065, 1996, 3408, 2031,  102])"
203,1,"['location', 'parameter', 'location parameter', 'dispersion', 'measurements']", Self Assessment QuestionsExercises,seg_71,"something to do with (a) location parameter, (b) dispersion parameter or (c) none of the above. 8. measurements were taken of the concrete cover depth of a bridge column. the",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2242,  2000,  2079,  2007,  1006,  1037,  1007,  3295, 16381,
         1010,  1006,  1038,  1007,  4487, 17668, 10992, 16381,  2030,  1006,
         1039,  1007,  3904,  1997,  1996,  2682,  1012,  1022,  1012, 11702,
         2020,  2579,  1997,  1996,  5509,  3104,  5995,  1997,  1037,  2958,
         5930,  1012,  1996,   102])"
204,1,"['mean', 'set', 'frequency', 'sample', 'data set', 'data', 'sample mean']", Self Assessment QuestionsExercises,seg_71,histogram of the measured values has been plotted in fig. 3.17. the sample mean is equal to 0.16 mm. the sample mean is equal to 15 mm. the mode of the data set is equal to 15 mm. 9. which of the following are features of a symmetrical frequency diagram?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2010,  3406, 13113,  1997,  1996,  7594,  5300,  2038,  2042,
        27347,  1999, 20965,  1012,  1017,  1012,  2459,  1012,  1996,  7099,
         2812,  2003,  5020,  2000,  1014,  1012,  2385,  3461,  1012,  1996,
         7099,  2812,  2003,  5020,  2000,  2321,  3461,  1012,  1996,  5549,
         1997,  1996,  2951,  2275,  2003,  5020,  2000,  2321,  3461,  1012,
         1023,  1012,  2029,  1997,  1996,  2206,  2024,  2838,  1997,  1037,
        23476,  6075, 16403,  1029,   102])"
205,1,"['variance', 'variation', 'coefficient', 'skewness', 'median', 'coefficient of variation']", Self Assessment QuestionsExercises,seg_71,the variance is equal to the coefficient of variation. the mode is equal to the median. the skewness is equal to zero. none of the above.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1996, 23284,  2003,  5020,  2000,  1996, 19064,  1997,  8386,
         1012,  1996,  5549,  2003,  5020,  2000,  1996,  3991,  1012,  1996,
        15315,  7974,  2791,  2003,  5020,  2000,  5717,  1012,  3904,  1997,
         1996,  2682,  1012,   102])"
206,1,"['uncertainty', 'random', 'random variables', 'variables']",Chapter  Uncertainty Modeling,seg_73,"lecture 4 (aim of the present lecture) the aim of the present lecture is to provide a fundamental understanding of uncertainty and how this affects engineering decision making. furthermore, random variables are introduced and it is explained how they may be characterized depending on the given situation. on the basis of the lecture it is expected that the reader will acquire knowledge and skills with regard to:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 3127, 12503, 11643])","tensor([  101,  8835,  1018,  1006,  6614,  1997,  1996,  2556,  8835,  1007,
         1996,  6614,  1997,  1996,  2556,  8835,  2003,  2000,  3073,  1037,
         8050,  4824,  1997, 12503,  1998,  2129,  2023, 13531,  3330,  3247,
         2437,  1012,  7297,  1010,  6721, 10857,  2024,  3107,  1998,  2009,
         2003,  4541,  2129,  2027,  2089,  2022,  7356,  5834,  2006,  1996,
         2445,  3663,  1012,  2006,  1996,  3978,  1997,  1996,  8835,  2009,
         2003,  3517,  2008,  1996,  8068,  2097,  9878,  3716,  1998,  4813,
         2007,  7634,  2000,  1024,   102])"
207,1,"['functions', 'uncertainties', 'random variable', 'discrete', 'probability distribution', 'continuous', 'probability', 'expectation operation', 'expectation', 'probability density functions', 'moments', 'random', 'density functions', 'distribution', 'continuous probability distribution', 'variable']",Chapter  Uncertainty Modeling,seg_73,• why do uncertainties influence engineering problems and decision making? • which are the principally different types of uncertainties? • why is it useful to differentiate between different types of uncertainties? • which types of uncertainties can be reduced? • in what way can uncertainties depend on time? • in what way can scale influence uncertainties? • what is a random variable and how can it be characterized? • how are cumulative distribution and probability density functions related? • what is a discrete and what is a continuous probability distribution? • how are the moments of a random variable defined? • how is the expectation operation defined?,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 3127, 12503, 11643])","tensor([  101,  1528,  2339,  2079,  9662,  7368,  3747,  3330,  3471,  1998,
         3247,  2437,  1029,  1528,  2029,  2024,  1996, 16552,  2367,  4127,
         1997,  9662,  7368,  1029,  1528,  2339,  2003,  2009,  6179,  2000,
        21032,  2090,  2367,  4127,  1997,  9662,  7368,  1029,  1528,  2029,
         4127,  1997,  9662,  7368,  2064,  2022,  4359,  1029,  1528,  1999,
         2054,  2126,  2064,  9662,  7368, 12530,  2006,  2051,  1029,  1528,
         1999,  2054,  2126,  2064,  4094,  3747,  9662,  7368,  1029,  1528,
         2054,  2003,  1037,  6721,  8023,  1998,  2129,  2064,  2009,  2022,
         7356,  1029,  1528,  2129,  2024, 23260,  4353,  1998,  9723,  4304,
         4972,  3141,  1029,  1528,  2054,  2003,  1037, 16246,  1998,  2054,
         2003,  1037,  7142,  9723,  4353,  1029,  1528,  2129,  2024,  1996,
         5312,  1997,  1037,  6721,  8023,  4225,  1029,  1528,  2129,  2003,
         1996, 17626,  3169,  4225,  1029,   102])"
208,1,"['variability', 'risk', 'decision problem', 'uncertainties', 'level', 'efficient']", Introduction,seg_75,"a central role for engineers is to provide basis for decision making with regard to the cost efficient safeguarding of personnel, environment and assets in situations where uncertainties are at hand. a classical example is the decision problem of choosing the height of a dike. the risk of dike flooding can be reduced by increasing the height of the dike; however, due to the inherent natural variability in the water level",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.])",tensor([4955]),"tensor([  101,  1037,  2430,  2535,  2005,  6145,  2003,  2000,  3073,  3978,
         2005,  3247,  2437,  2007,  7634,  2000,  1996,  3465,  8114, 28805,
         2075,  1997,  5073,  1010,  4044,  1998,  7045,  1999,  8146,  2073,
         9662,  7368,  2024,  2012,  2192,  1012,  1037,  4556,  2742,  2003,
         1996,  3247,  3291,  1997, 10549,  1996,  4578,  1997,  1037,  4487,
         3489,  1012,  1996,  3891,  1997,  4487,  3489,  9451,  2064,  2022,
         4359,  2011,  4852,  1996,  4578,  1997,  1996,  4487,  3489,  1025,
         2174,  1010,  2349,  2000,  1996, 16112,  3019, 28436,  1999,  1996,
         2300,  2504,   102])"
209,1,"['risk', 'probability', 'level', 'risks', 'model', 'reference period']", Introduction,seg_75,"a certain probability of dike flooding in a given reference period will always remain. risk assessment within the theoretical framework of decision analysis (introduced in chap. 7) can help us in deciding on the optimal dike height by weighing the benefits of reduced dike flooding risks with the costs of increasing the dike height. however, a prerequisite for the risk assessment is that the means for assessing the probability of dike flooding are established, and this in turn requires that a probabilistic model for the future water level is available.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0.])",tensor([4955]),"tensor([  101,  1037,  3056,  9723,  1997,  4487,  3489,  9451,  1999,  1037,
         2445,  4431,  2558,  2097,  2467,  3961,  1012,  3891,  7667,  2306,
         1996,  9373,  7705,  1997,  3247,  4106,  1006,  3107,  1999, 15775,
         2361,  1012,  1021,  1007,  2064,  2393,  2149,  1999, 10561,  2006,
         1996, 15502,  4487,  3489,  4578,  2011, 15243,  1996,  6666,  1997,
         4359,  4487,  3489,  9451, 10831,  2007,  1996,  5366,  1997,  4852,
         1996,  4487,  3489,  4578,  1012,  2174,  1010,  1037,  3653,  2890,
        24871,  2005,  1996,  3891,  7667,  2003,  2008,  1996,  2965,  2005,
        20077,  1996,  9723,  1997,  4487,  3489,  9451,  2024,  2511,  1010,
         1998,  2023,  1999,  2735,  5942,  2008,  1037,  4013,  3676, 27965,
         4588,  2944,  2005,  1996,  2925,  2300,  2504,  2003,  2800,  1012,
          102])"
210,1,"['uncertainty', 'set', 'cost benefit analysis', 'frequency', 'reference period']", Uncertainties in Engineering Problems,seg_77,"for the purpose of discussing the phenomenon uncertainty in more detail, let us initially assume that the universe is deterministic and that our knowledge about the universe is perfect. this implies that it is possible by means of e.g. a set of exact equation systems and known boundary conditions by means of analysis to achieve perfect knowledge about any state, quantity or characteristic which otherwise cannot be directly observed or has yet not taken place. in principle, following this line of reasoning, the future as well as the past would be known or assessable with certainty. considering the dike flooding problem it would thus be possible to assess the exact number of floods which would occur in a given reference period (the frequency of floods) for a given dike height and an optimal decision can be achieved by cost benefit analysis.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([9662, 7368, 1999, 3330, 3471])","tensor([  101,  2005,  1996,  3800,  1997, 10537,  1996,  9575, 12503,  1999,
         2062,  6987,  1010,  2292,  2149,  3322,  7868,  2008,  1996,  5304,
         2003, 28283, 25300, 10074,  1998,  2008,  2256,  3716,  2055,  1996,
         5304,  2003,  3819,  1012,  2023, 12748,  2008,  2009,  2003,  2825,
         2011,  2965,  1997,  1041,  1012,  1043,  1012,  1037,  2275,  1997,
         6635,  8522,  3001,  1998,  2124,  6192,  3785,  2011,  2965,  1997,
         4106,  2000,  6162,  3819,  3716,  2055,  2151,  2110,  1010, 11712,
         2030,  8281,  2029,  4728,  3685,  2022,  3495,  5159,  2030,  2038,
         2664,  2025,  2579,  2173,  1012,  1999,  6958,  1010,  2206,  2023,
         2240,  1997, 13384,  1010,  1996,  2925,  2004,  2092,  2004,  1996,
         2627,  2052,  2022,  2124,  2030, 14358,  3085,  2007, 15855,  1012,
         6195,  1996,  4487,  3489,  9451,  3291,  2009,  2052,  2947,  2022,
         2825,  2000, 14358,  1996,  6635,  2193,  1997, 14295,  2029,  2052,
         5258,  1999,  1037,  2445,  4431,  2558,  1006,  1996,  6075,  1997,
        14295,  1007,  2005,  1037,  2445,  4487,  3489,  4578,  1998,  2019,
        15502,  3247,  2064,  2022,  4719,  2011,  3465,  5770,  4106,  1012,
          102])"
211,0,[], Uncertainties in Engineering Problems,seg_77,"whether the universe is deterministic or not is a rather deep philosophical question. despite the obviously challenging aspects of this question its answer is, however, not a prerequisite for purposes of engineering decision making because even if the universe is deterministic our knowledge about it is still in part incomplete and/or uncertain.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([9662, 7368, 1999, 3330, 3471])","tensor([  101,  3251,  1996,  5304,  2003, 28283, 25300, 10074,  2030,  2025,
         2003,  1037,  2738,  2784,  9569,  3160,  1012,  2750,  1996,  5525,
        10368,  5919,  1997,  2023,  3160,  2049,  3437,  2003,  1010,  2174,
         1010,  2025,  1037,  3653,  2890, 24871,  2005,  5682,  1997,  3330,
         3247,  2437,  2138,  2130,  2065,  1996,  5304,  2003, 28283, 25300,
        10074,  2256,  3716,  2055,  2009,  2003,  2145,  1999,  2112, 12958,
         1998,  1013,  2030,  9662,  1012,   102])"
212,1,"['variability', 'uncertainty', 'risk', 'probability theory', 'probability', 'uncertainties', 'bayesian', 'quantitative risk analysis', 'standard', 'bayesian probability theory', 'quantitative', 'model', 'statistical', 'statistical uncertainties', 'structural reliability analysis', 'bayesian probability']", Uncertainties in Engineering Problems,seg_77,"in engineering decision analysis subject to uncertainties such as quantitative risk analysis (qra) and structural reliability analysis (sra) a commonly accepted view point is that uncertainties should be interpreted and differentiated with regard to their type and origin. in this way it has become standard to differentiate between uncertainties due to inherent natural variability, model uncertainties and statistical uncertainties. whereas the first mentioned type of uncertainty is often denoted aleatory (or type 1) uncertainty, the two latter types are referred to as epistemic (or type 2) uncertainties. without further discussion here, it is just stated that, in principle, all prevailing types of uncertainties should be taken into account in engineering decision analysis within the framework of bayesian probability theory.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 1., 0., 0.])","tensor([9662, 7368, 1999, 3330, 3471])","tensor([  101,  1999,  3330,  3247,  4106,  3395,  2000,  9662,  7368,  2107,
         2004, 20155,  3891,  4106,  1006,  1053,  2527,  1007,  1998,  8332,
        15258,  4106,  1006,  5034,  2050,  1007,  1037,  4141,  3970,  3193,
         2391,  2003,  2008,  9662,  7368,  2323,  2022, 10009,  1998, 24374,
         2007,  7634,  2000,  2037,  2828,  1998,  4761,  1012,  1999,  2023,
         2126,  2009,  2038,  2468,  3115,  2000, 21032,  2090,  9662,  7368,
         2349,  2000, 16112,  3019, 28436,  1010,  2944,  9662,  7368,  1998,
         7778,  9662,  7368,  1012,  6168,  1996,  2034,  3855,  2828,  1997,
        12503,  2003,  2411, 19537, 15669, 14049,  1006,  2030,  2828,  1015,
         1007, 12503,  1010,  1996,  2048,  3732,  4127,  2024,  3615,  2000,
         2004,  4958, 27870,  7712,  1006,  2030,  2828,  1016,  1007,  9662,
         7368,  1012,  2302,  2582,  6594,  2182,  1010,  2009,  2003,  2074,
         3090,  2008,  1010,  1999,  6958,  1010,  2035, 19283,  4127,  1997,
         9662,  7368,  2323,  2022,  2579,  2046,  4070,  1999,  3330,  3247,
         4106,  2306,  1996,  7705,  1997,  3016, 25253,  9723,  3399,  1012,
          102])"
213,1,"['variability', 'regression', 'estimation', 'associated', 'uncertainties', 'events', 'predicted', 'statistical', 'model', 'case', 'uncertainty', 'level', 'parameters', 'levels', 'statistical uncertainties']", Uncertainties in Engineering Problems,seg_77,considering again the dike example it can be imagined that an engineering model might be formulated where future extreme water levels are predicted in terms of a regression of previously observed annual extremes. in this case the uncertainty due to inherent natural variability would be the uncertainty associated with the annual extreme water level. the model chosen for the annual extreme water level events would by itself introduce model uncertainties and the parameters of the model would introduce statistical uncertainties as their estimation would be based on a limited,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([9662, 7368, 1999, 3330, 3471])","tensor([  101,  6195,  2153,  1996,  4487,  3489,  2742,  2009,  2064,  2022,
         8078,  2008,  2019,  3330,  2944,  2453,  2022, 19788,  2073,  2925,
         6034,  2300,  3798,  2024, 10173,  1999,  3408,  1997,  1037, 26237,
         1997,  3130,  5159,  3296, 28800,  1012,  1999,  2023,  2553,  1996,
        12503,  2349,  2000, 16112,  3019, 28436,  2052,  2022,  1996, 12503,
         3378,  2007,  1996,  3296,  6034,  2300,  2504,  1012,  1996,  2944,
         4217,  2005,  1996,  3296,  6034,  2300,  2504,  2824,  2052,  2011,
         2993,  8970,  2944,  9662,  7368,  1998,  1996, 11709,  1997,  1996,
         2944,  2052,  8970,  7778,  9662,  7368,  2004,  2037, 24155,  2052,
         2022,  2241,  2006,  1037,  3132,   102])"
214,1,"['variability', 'uncertainty', 'associated', 'uncertainties', 'information', 'level', 'dependent', 'model', 'extrapolation']", Uncertainties in Engineering Problems,seg_77,"number of observed annual extremes. finally, the extrapolation of the annual extreme model to extremes over longer periods of time would introduce additional model uncertainties. the uncertainty associated with the future extreme water level is thus composed as illustrated in fig. 4.1. whereas the inherent natural variability is often understood as the uncertainty caused by the fact that the universe is not deterministic, it may also be interpreted simply as the uncertainty which cannot be reduced by means of collection of additional information. it is seen that this definition implies that the amount of uncertainty due to inherent natural variability depends on the models applied in the formulation of the engineering problem. presuming that a refinement of models corresponds to looking in a more detailed manner at the problem at hand, one could say that the uncertainty structure influencing a problem is scale dependent.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0.])","tensor([9662, 7368, 1999, 3330, 3471])","tensor([  101,  2193,  1997,  5159,  3296, 28800,  1012,  2633,  1010,  1996,
         4469, 18155,  3370,  1997,  1996,  3296,  6034,  2944,  2000, 28800,
         2058,  2936,  6993,  1997,  2051,  2052,  8970,  3176,  2944,  9662,
         7368,  1012,  1996, 12503,  3378,  2007,  1996,  2925,  6034,  2300,
         2504,  2003,  2947,  3605,  2004,  7203,  1999, 20965,  1012,  1018,
         1012,  1015,  1012,  6168,  1996, 16112,  3019, 28436,  2003,  2411,
         5319,  2004,  1996, 12503,  3303,  2011,  1996,  2755,  2008,  1996,
         5304,  2003,  2025, 28283, 25300, 10074,  1010,  2009,  2089,  2036,
         2022, 10009,  3432,  2004,  1996, 12503,  2029,  3685,  2022,  4359,
         2011,  2965,  1997,  3074,  1997,  3176,  2592,  1012,  2009,  2003,
         2464,  2008,  2023,  6210, 12748,  2008,  1996,  3815,  1997, 12503,
         2349,  2000, 16112,  3019, 28436,  9041,  2006,  1996,  4275,  4162,
         1999,  1996, 20219,  1997,  1996,  3330,  3291,  1012,  3653, 17421,
         2075,  2008,  1037, 25416,  3170,  3672,  1997,  4275, 14788,  2000,
         2559,  1999,  1037,  2062,  6851,  5450,  2012,  1996,  3291,  2012,
         2192,  1010,  2028,  2071,  2360,  2008,  1996, 12503,  3252, 25870,
         1037,  3291,  2003,  4094,  7790,  1012,   102])"
215,1,"['probability', 'uncertainties', 'prediction', 'levels', 'model', 'reference period', 'case']", Uncertainties in Engineering Problems,seg_77,"having formulated a model for the prediction of future extreme water levels and taking into account the various prevailing types of uncertainties, the probability of flooding within a given reference period can be assessed and, just as in the case of a deterministic and perfectly known universe, the optimum dike height can be decided, based on a cost-benefit assessment.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([9662, 7368, 1999, 3330, 3471])","tensor([  101,  2383, 19788,  1037,  2944,  2005,  1996, 17547,  1997,  2925,
         6034,  2300,  3798,  1998,  2635,  2046,  4070,  1996,  2536, 19283,
         4127,  1997,  9662,  7368,  1010,  1996,  9723,  1997,  9451,  2306,
         1037,  2445,  4431,  2558,  2064,  2022, 14155,  1998,  1010,  2074,
         2004,  1999,  1996,  2553,  1997,  1037, 28283, 25300, 10074,  1998,
         6669,  2124,  5304,  1010,  1996, 23569, 28591,  4487,  3489,  4578,
         2064,  2022,  2787,  1010,  2241,  2006,  1037,  3465,  1011,  5770,
         7667,  1012,   102])"
216,1,"['variability', 'precision', 'observation', 'uncertainty', 'associated', 'statistical uncertainty', 'prediction', 'errors', 'predicted', 'statistical', 'model']", Uncertainties in Engineering Problems,seg_77,"it is interesting to notice that the type of uncertainty associated with the state of knowledge has a time dependency. following fig. 4.2 it is possible to observe an uncertain phenomenon when it has occurred. in principle, if the observation is perfect without any errors the knowledge about the phenomenon is perfect. the modeling of the same phenomenon in the future, however, is uncertain as this involves models subject to natural variability, model uncertainty and statistical uncertainty. often, but not always, the models available tend to lose their precision rather fast so that phenomena lying just a few days or weeks ahead can be predicted only with significant uncertainty. an extreme example of this concerns the prediction of the weather.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([9662, 7368, 1999, 3330, 3471])","tensor([  101,  2009,  2003,  5875,  2000,  5060,  2008,  1996,  2828,  1997,
        12503,  3378,  2007,  1996,  2110,  1997,  3716,  2038,  1037,  2051,
        24394,  1012,  2206, 20965,  1012,  1018,  1012,  1016,  2009,  2003,
         2825,  2000, 11949,  2019,  9662,  9575,  2043,  2009,  2038,  4158,
         1012,  1999,  6958,  1010,  2065,  1996,  8089,  2003,  3819,  2302,
         2151, 10697,  1996,  3716,  2055,  1996,  9575,  2003,  3819,  1012,
         1996, 11643,  1997,  1996,  2168,  9575,  1999,  1996,  2925,  1010,
         2174,  1010,  2003,  9662,  2004,  2023,  7336,  4275,  3395,  2000,
         3019, 28436,  1010,  2944, 12503,  1998,  7778, 12503,  1012,  2411,
         1010,  2021,  2025,  2467,  1010,  1996,  4275,  2800,  7166,  2000,
         4558,  2037, 11718,  2738,  3435,  2061,  2008, 13352,  4688,  2074,
         1037,  2261,  2420,  2030,  3134,  3805,  2064,  2022, 10173,  2069,
         2007,  3278, 12503,  1012,  2019,  6034,  2742,  1997,  2023,  5936,
         1996, 17547,  1997,  1996,  4633,  1012,   102])"
217,1,"['model', 'associated']", Uncertainties in Engineering Problems,seg_77,"the above discussion shows another interesting effect, namely that the uncertainty associated with a model concerning the future, transforms from a mixture of",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([9662, 7368, 1999, 3330, 3471])","tensor([  101,  1996,  2682,  6594,  3065,  2178,  5875,  3466,  1010,  8419,
         2008,  1996, 12503,  3378,  2007,  1037,  2944,  7175,  1996,  2925,
         1010, 21743,  2013,  1037,  8150,  1997,   102])"
218,1,"['observations', 'uncertainty']", Uncertainties in Engineering Problems,seg_77,aleatory and epistemic uncertainty to a purely epistemic uncertainty when the modeled phenomenon is observed. this transition of the type of uncertainty is significant because it facilitates that the uncertainty is reduced by utilization of updating based on observations.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([9662, 7368, 1999, 3330, 3471])","tensor([  101, 15669, 14049,  1998,  4958, 27870,  7712, 12503,  2000,  1037,
        11850,  4958, 27870,  7712, 12503,  2043,  1996, 14440,  9575,  2003,
         5159,  1012,  2023,  6653,  1997,  1996,  2828,  1997, 12503,  2003,
         3278,  2138,  2009, 27777,  2008,  1996, 12503,  2003,  4359,  2011,
        27891,  1997,  2039, 16616,  2241,  2006,  9420,  1012,   102])"
219,0,[], Random Variables,seg_79,"the performance of an engineered system, facility or installation (in the following referred to as a system) may usually be modeled in mathematical and physical terms in conjunction with empirical relations.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 6721, 10857])","tensor([  101,  1996,  2836,  1997,  2019, 13685,  2291,  1010,  4322,  2030,
         8272,  1006,  1999,  1996,  2206,  3615,  2000,  2004,  1037,  2291,
         1007,  2089,  2788,  2022, 14440,  1999,  8045,  1998,  3558,  3408,
         1999,  9595,  2007, 17537,  4262,  1012,   102])"
220,1,"['uncertainty', 'set', 'parameters', 'random', 'random variables', 'model', 'basic random variables', 'variables']", Random Variables,seg_79,for a given set of model parameters the performance of the considered system can be determined on the basis of this model. the basic random variables are defined as the parameters that represents the available knowledge as well as the associated uncertainty in the considered model.,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.])","tensor([ 6721, 10857])","tensor([  101,  2005,  1037,  2445,  2275,  1997,  2944, 11709,  1996,  2836,
         1997,  1996,  2641,  2291,  2064,  2022,  4340,  2006,  1996,  3978,
         1997,  2023,  2944,  1012,  1996,  3937,  6721, 10857,  2024,  4225,
         2004,  1996, 11709,  2008,  5836,  1996,  2800,  3716,  2004,  2092,
         2004,  1996,  3378, 12503,  1999,  1996,  2641,  2944,  1012,   102])"
221,1,"['uncertainty', 'associated', 'statistical uncertainty', 'uncertainties', 'information', 'random', 'tests', 'random variables', 'statistical', 'model', 'basic random variables', 'variables', 'statistical uncertainties']", Random Variables,seg_79,"the basic random variables must be able to represent all types of uncertainties that are included in the analysis. the uncertainties, which must be considered are, as previously mentioned, the physical uncertainty, the statistical uncertainty and the model uncertainty. the physical uncertainties are typically uncertainties associated with the loading environment, the geometry of the structure, the material properties and the repair qualities. the statistical uncertainties arise due to incomplete statistical information e.g. due to a small number of material tests. finally, the model uncertainties must be considered to take into account the uncertainty associated with the idealized mathematical descriptions used to approximate the actual physical behavior of the structure.",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10857])","tensor([  101,  1996,  3937,  6721, 10857,  2442,  2022,  2583,  2000,  5050,
         2035,  4127,  1997,  9662,  7368,  2008,  2024,  2443,  1999,  1996,
         4106,  1012,  1996,  9662,  7368,  1010,  2029,  2442,  2022,  2641,
         2024,  1010,  2004,  3130,  3855,  1010,  1996,  3558, 12503,  1010,
         1996,  7778, 12503,  1998,  1996,  2944, 12503,  1012,  1996,  3558,
         9662,  7368,  2024,  4050,  9662,  7368,  3378,  2007,  1996, 10578,
         4044,  1010,  1996, 10988,  1997,  1996,  3252,  1010,  1996,  3430,
         5144,  1998,  1996,  7192, 11647,  1012,  1996,  7778,  9662,  7368,
        13368,  2349,  2000, 12958,  7778,  2592,  1041,  1012,  1043,  1012,
         2349,  2000,  1037,  2235,  2193,  1997,  3430,  5852,  1012,  2633,
         1010,  1996,  2944,  9662,  7368,  2442,  2022,  2641,  2000,  2202,
         2046,  4070,  1996, 12503,  3378,  2007,  1996,  7812,  3550,  8045,
        13271,  2109,  2000, 15796,  1996,  5025,  3558,  5248,  1997,  1996,
         3252,  1012,   102])"
222,1,"['cases', 'functions', 'estimated', 'risk', 'uncertainties', 'stochastic processes', 'model', 'statistical', 'information', 'random variables', 'parameters', 'processes', 'random', 'distribution', 'subjective', 'variables']", Random Variables,seg_79,"modern methods of reliability and risk analysis allow for a very general representation of these uncertainties ranging from non-stationary stochastic processes and fields to time invariant random variables, see e.g. melchers [10]. in most cases it is sufficient to model the uncertain quantities by random variables with given cumulative distribution functions and distribution parameters estimated on basis of statistical and/or subjective information. therefore the following is concerned with a basic description of the characteristics of random variables.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0.])","tensor([ 6721, 10857])","tensor([  101,  2715,  4725,  1997, 15258,  1998,  3891,  4106,  3499,  2005,
         1037,  2200,  2236,  6630,  1997,  2122,  9662,  7368,  7478,  2013,
         2512,  1011, 17337,  2358, 11663, 20875,  6194,  1998,  4249,  2000,
         2051, 23915,  6721, 10857,  1010,  2156,  1041,  1012,  1043,  1012,
        11463, 21844,  1031,  2184,  1033,  1012,  1999,  2087,  3572,  2009,
         2003,  7182,  2000,  2944,  1996,  9662, 12450,  2011,  6721, 10857,
         2007,  2445, 23260,  4353,  4972,  1998,  4353, 11709,  4358,  2006,
         3978,  1997,  7778,  1998,  1013,  2030, 20714,  2592,  1012,  3568,
         1996,  2206,  2003,  4986,  2007,  1037,  3937,  6412,  1997,  1996,
         6459,  1997,  6721, 10857,  1012,   102])"
223,1,"['continuous', 'continuous random variable', 'function', 'cumulative distribution function', 'probability', 'random variable', 'random', 'distribution', 'distribution function', 'variable']", Cumulative Distribution and Probability Density Functions,seg_81,"a random variable, which can take on any value, is called a continuous random variable. the probability that such a random variable takes on a specific value is zero. the probability that a continuous random variable, x, is less than or equal to a value, x, is given by the cumulative distribution function:",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([23260,  4353,  1998,  9723,  4304,  4972])","tensor([  101,  1037,  6721,  8023,  1010,  2029,  2064,  2202,  2006,  2151,
         3643,  1010,  2003,  2170,  1037,  7142,  6721,  8023,  1012,  1996,
         9723,  2008,  2107,  1037,  6721,  8023,  3138,  2006,  1037,  3563,
         3643,  2003,  5717,  1012,  1996,  9723,  2008,  1037,  7142,  6721,
         8023,  1010,  1060,  1010,  2003,  2625,  2084,  2030,  5020,  2000,
         1037,  3643,  1010,  1060,  1010,  2003,  2445,  2011,  1996, 23260,
         4353,  3853,  1024,   102])"
224,1,"['continuous', 'function', 'cumulative distribution function', 'realization', 'random variable', 'outcome', 'random', 'distribution', 'distribution function', 'variable']", Cumulative Distribution and Probability Density Functions,seg_81,"in general, capital letters denote a random variable and small letters denote an outcome or realization of a random variable. an example of a continuous cumulative distribution function is illustrated in fig. 4.3a).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([23260,  4353,  1998,  9723,  4304,  4972])","tensor([  101,  1999,  2236,  1010,  3007,  4144, 19090,  1037,  6721,  8023,
         1998,  2235,  4144, 19090,  2019,  9560,  2030, 12393,  1997,  1037,
         6721,  8023,  1012,  2019,  2742,  1997,  1037,  7142, 23260,  4353,
         3853,  2003,  7203,  1999, 20965,  1012,  1018,  1012, 23842,  1007,
         1012,   102])"
225,1,"['continuous', 'function', 'density function', 'probability', 'probability density function', 'continuous random variables', 'random', 'random variables', 'variables']", Cumulative Distribution and Probability Density Functions,seg_81,for continuous random variables the probability density function is given by:,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.])","tensor([23260,  4353,  1998,  9723,  4304,  4972])","tensor([  101,  2005,  7142,  6721, 10857,  1996,  9723,  4304,  3853,  2003,
         2445,  2011,  1024,   102])"
226,1,"['continuous', 'probability']", Cumulative Distribution and Probability Density Functions,seg_81,an example of a continuous probability density junction is illustrated in fig. 4.3b).,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([23260,  4353,  1998,  9723,  4304,  4972])","tensor([  101,  2019,  2742,  1997,  1037,  7142,  9723,  4304,  5098,  2003,
         7203,  1999, 20965,  1012,  1018,  1012,  1017,  2497,  1007,  1012,
          102])"
227,1,"['probability', 'outcome', 'interval']", Cumulative Distribution and Probability Density Functions,seg_81,"the probability of an outcome in the interval [x;x + dx] where dx is small, is given by p(x ∈ [x;x + dx]) = fx(x)dx.",tensor(1),"tensor([0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([23260,  4353,  1998,  9723,  4304,  4972])","tensor([  101,  1996,  9723,  1997,  2019,  9560,  1999,  1996, 13483,  1031,
         1060,  1025,  1060,  1009,  1040,  2595,  1033,  2073,  1040,  2595,
         2003,  2235,  1010,  2003,  2445,  2011,  1052,  1006,  1060,  1596,
         1031,  1060,  1025,  1060,  1009,  1040,  2595,  1033,  1007,  1027,
        23292,  1006,  1060,  1007,  1040,  2595,  1012,   102])"
228,1,"['function', 'sample space', 'discrete', 'cumulative distribution function', 'discrete random variables', 'sample', 'random', 'random variables', 'distribution', 'distribution function', 'variables']", Cumulative Distribution and Probability Density Functions,seg_81,random variables with a finite or infinite countable sample space are called discrete random variables. for discrete random variables the cumulative distribution function is given as:,tensor(1),"tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.])","tensor([23260,  4353,  1998,  9723,  4304,  4972])","tensor([  101,  6721, 10857,  2007,  1037, 10713,  2030, 10709,  4175,  3085,
         7099,  2686,  2024,  2170, 16246,  6721, 10857,  1012,  2005, 16246,
         6721, 10857,  1996, 23260,  4353,  3853,  2003,  2445,  2004,  1024,
          102])"
229,1,"['function', 'density function', 'probability', 'probability density function']", Cumulative Distribution and Probability Density Functions,seg_81,where px(xi) is the probability density function given as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])","tensor([23260,  4353,  1998,  9723,  4304,  4972])","tensor([ 101, 2073, 1052, 2595, 1006, 8418, 1007, 2003, 1996, 9723, 4304, 3853,
        2445, 2004, 1024,  102])"
230,1,"['function', 'density function', 'cumulative distribution function', 'probability', 'probability density function', 'distribution', 'distribution function', 'discrete']", Cumulative Distribution and Probability Density Functions,seg_81,a discrete cumulative distribution function and probability density function is illustrated in fig. 4.4.,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([23260,  4353,  1998,  9723,  4304,  4972])","tensor([  101,  1037, 16246, 23260,  4353,  3853,  1998,  9723,  4304,  3853,
         2003,  7203,  1999, 20965,  1012,  1018,  1012,  1018,  1012,   102])"
231,1,"['function', 'density function', 'probability density functions', 'probability', 'functions', 'distributions', 'moments', 'density functions', 'distribution', 'parameters']", Moments of Random Variables and the Expectation Operator,seg_83,probability distributions may be defined in terms of their parameters or moments. often cumulative distribution functions and probability density functions are written as fx(x;p) and fx(x;p) respectively to indicate the parameters p (or moments) defining the functions. whether the cumulative distribution and density function are defined by their moments or by their parameters is a matter of convenience and it is generally possible to establish one from the other.,tensor(1),"tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 5312,  1997,  6721, 10857,  1998,  1996, 17626,  6872])","tensor([  101,  9723, 20611,  2089,  2022,  4225,  1999,  3408,  1997,  2037,
        11709,  2030,  5312,  1012,  2411, 23260,  4353,  4972,  1998,  9723,
         4304,  4972,  2024,  2517,  2004, 23292,  1006,  1060,  1025,  1052,
         1007,  1998, 23292,  1006,  1060,  1025,  1052,  1007,  4414,  2000,
         5769,  1996, 11709,  1052,  1006,  2030,  5312,  1007, 12854,  1996,
         4972,  1012,  3251,  1996, 23260,  4353,  1998,  4304,  3853,  2024,
         4225,  2011,  2037,  5312,  2030,  2011,  2037, 11709,  2003,  1037,
         3043,  1997, 15106,  1998,  2009,  2003,  3227,  2825,  2000,  5323,
         2028,  2013,  1996,  2060,  1012,   102])"
232,1,"['continuous', 'continuous random variable', 'moment', 'random variable', 'random', 'variable']", Moments of Random Variables and the Expectation Operator,seg_83,the ith moment λi of a continuous random variable is defined by:,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 5312,  1997,  6721, 10857,  1998,  1996, 17626,  6872])","tensor([ 101, 1996, 2009, 2232, 2617, 1165, 2072, 1997, 1037, 7142, 6721, 8023,
        2003, 4225, 2011, 1024,  102])"
233,1,"['discrete random variable', 'discrete', 'random variable', 'random', 'variable']", Moments of Random Variables and the Expectation Operator,seg_83,and for a discrete random variable by:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0.])","tensor([ 5312,  1997,  6721, 10857,  1998,  1996, 17626,  6872])","tensor([  101,  1998,  2005,  1037, 16246,  6721,  8023,  2011,  1024,   102])"
234,1,"['continuous', 'mean', 'moment', 'continuous random variables', 'random', 'random variables', 'variables', 'expected value']", Moments of Random Variables and the Expectation Operator,seg_83,the mean (or expected value) of a continuous random variables x is defined as the first moment:,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.])","tensor([ 5312,  1997,  6721, 10857,  1998,  1996, 17626,  6872])","tensor([  101,  1996,  2812,  1006,  2030,  3517,  3643,  1007,  1997,  1037,
         7142,  6721, 10857,  1060,  2003,  4225,  2004,  1996,  2034,  2617,
         1024,   102])"
235,1,"['mean', 'discrete random variable', 'discrete', 'random variable', 'random', 'variable']", Moments of Random Variables and the Expectation Operator,seg_83,"for a discrete random variable s, the mean is given as:",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 5312,  1997,  6721, 10857,  1998,  1996, 17626,  6872])","tensor([  101,  2005,  1037, 16246,  6721,  8023,  1055,  1010,  1996,  2812,
         2003,  2445,  2004,  1024,   102])"
236,1,"['expectation operator', 'expectation']", Moments of Random Variables and the Expectation Operator,seg_83,where e[·] denotes the expectation operator.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 5312,  1997,  6721, 10857,  1998,  1996, 17626,  6872])","tensor([  101,  2073,  1041,  1031,  1087,  1033, 14796,  1996, 17626,  6872,
         1012,   102])"
237,1,"['continuous', 'moment', 'variance', 'continuous random variables', 'random', 'random variables', 'variables']", Moments of Random Variables and the Expectation Operator,seg_83,"2 similarly the variance, σx , is described by the second central moment, i.e. for continuous random variables it is:",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])","tensor([ 5312,  1997,  6721, 10857,  1998,  1996, 17626,  6872])","tensor([  101,  1016,  6660,  1996, 23284,  1010,  1173,  2595,  1010,  2003,
         2649,  2011,  1996,  2117,  2430,  2617,  1010,  1045,  1012,  1041,
         1012,  2005,  7142,  6721, 10857,  2009,  2003,  1024,   102])"
238,1,"['discrete', 'discrete random variables', 'random', 'random variables', 'variables']", Moments of Random Variables and the Expectation Operator,seg_83,and for discrete random variables it is:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])","tensor([ 5312,  1997,  6721, 10857,  1998,  1996, 17626,  6872])","tensor([  101,  1998,  2005, 16246,  6721, 10857,  2009,  2003,  1024,   102])"
239,1,['variance'], Moments of Random Variables and the Expectation Operator,seg_83,where var[x] denotes the variance of x.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 5312,  1997,  6721, 10857,  1998,  1996, 17626,  6872])","tensor([  101,  2073, 13075,  1031,  1060,  1033, 14796,  1996, 23284,  1997,
         1060,  1012,   102])"
240,1,"['coefficient of variation', 'deviation', 'variation', 'coefficient', 'random variable', 'random', 'standard deviation', 'standard', 'expected value', 'variable']", Moments of Random Variables and the Expectation Operator,seg_83,the ratio between the standard deviation σx and the expected value μx of a random variable x is denoted the coefficient of variation vx and is given by:,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 5312,  1997,  6721, 10857,  1998,  1996, 17626,  6872])","tensor([  101,  1996,  6463,  2090,  1996,  3115, 24353,  1173,  2595,  1998,
         1996,  3517,  3643,  1166,  2595,  1997,  1037,  6721,  8023,  1060,
         2003, 19537,  1996, 19064,  1997,  8386,  1058,  2595,  1998,  2003,
         2445,  2011,  1024,   102])"
241,1,"['variability', 'coefficient of variation', 'variation', 'coefficient', 'random variable', 'random', 'expected value', 'variable']", Moments of Random Variables and the Expectation Operator,seg_83,the coefficient of variation provides a useful descriptor for the variability of a random variable around its expected value.,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0.])","tensor([ 5312,  1997,  6721, 10857,  1998,  1996, 17626,  6872])","tensor([  101,  1996, 19064,  1997,  8386,  3640,  1037,  6179,  4078, 23235,
         2953,  2005,  1996, 28436,  1997,  1037,  6721,  8023,  2105,  2049,
         3517,  3643,  1012,   102])"
242,1,"['continuous', 'continuous random variable', 'function', 'density function', 'probability', 'probability density function', 'interval', 'random variable', 'random', 'variable']", Example Uniform Distribution,seg_85,as an example consider a continuous random variable with a uniform (constant) probability density function in the interval [a;b] as illustrated in fig. 4.5.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 6375, 4353])","tensor([  101,  2004,  2019,  2742,  5136,  1037,  7142,  6721,  8023,  2007,
         1037,  6375,  1006,  5377,  1007,  9723,  4304,  3853,  1999,  1996,
        13483,  1031,  1037,  1025,  1038,  1033,  2004,  7203,  1999, 20965,
         1012,  1018,  1012,  1019,  1012,   102])"
243,1,"['function', 'density function', 'probability', 'probability density function', 'random variable', 'random', 'uniformly distributed', 'variable']", Example Uniform Distribution,seg_85,the probability density function for a uniformly distributed random variable x is easily seen to be:,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([2742, 6375, 4353])","tensor([  101,  1996,  9723,  4304,  3853,  2005,  1037, 27423,  5500,  6721,
         8023,  1060,  2003,  4089,  2464,  2000,  2022,  1024,   102])"
244,1,"['function', 'density function', 'probability', 'probability density function', 'parameters']", Example Uniform Distribution,seg_85,"remembering that the area under the probability density function must integrate to 1. in eq. 4.12, a and b are the parameters of the probability density function.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0.])","tensor([2742, 6375, 4353])","tensor([  101, 10397,  2008,  1996,  2181,  2104,  1996,  9723,  4304,  3853,
         2442, 17409,  2000,  1015,  1012,  1999,  1041,  4160,  1012,  1018,
         1012,  2260,  1010,  1037,  1998,  1038,  2024,  1996, 11709,  1997,
         1996,  9723,  4304,  3853,  1012,   102])"
245,1,"['function', 'cumulative distribution function', 'random variable', 'random', 'uniformly distributed', 'distribution', 'distribution function', 'variable']", Example Uniform Distribution,seg_85,the cumulative distribution function for a uniformly distributed random variable x is thus:,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.])","tensor([2742, 6375, 4353])","tensor([  101,  1996, 23260,  4353,  3853,  2005,  1037, 27423,  5500,  6721,
         8023,  1060,  2003,  2947,  1024,   102])"
246,1,"['continuous', 'mean', 'moment', 'random']", Example Uniform Distribution,seg_85,the first moment i.e. the mean value (see eq. 4.7) of a continuous random vari-,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([2742, 6375, 4353])","tensor([  101,  1996,  2034,  2617,  1045,  1012,  1041,  1012,  1996,  2812,
         3643,  1006,  2156,  1041,  4160,  1012,  1018,  1012,  1021,  1007,
         1997,  1037,  7142,  6721, 13075,  2072,  1011,   102])"
247,1,"['uniform distribution', 'distribution']", Example Uniform Distribution,seg_85,able x with uniform distribution is thus:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([2742, 6375, 4353])","tensor([ 101, 2583, 1060, 2007, 6375, 4353, 2003, 2947, 1024,  102])"
248,1,['variance'], Example Uniform Distribution,seg_85,and the variance σx,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0.])","tensor([2742, 6375, 4353])","tensor([  101,  1998,  1996, 23284,  1173,  2595,   102])"
249,1,['moment'], Example Uniform Distribution,seg_85,2 (see eq. 4.9) is given through the second central moment:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.])","tensor([2742, 6375, 4353])","tensor([ 101, 1016, 1006, 2156, 1041, 4160, 1012, 1018, 1012, 1023, 1007, 2003,
        2445, 2083, 1996, 2117, 2430, 2617, 1024,  102])"
250,1,"['associated', 'functions', 'probabilistic', 'random', 'random variables', 'variables']", Example Uniform Distribution,seg_85,"lecture 5 (aim of the present lecture) the aim of the present lecture is to introduce the properties associated with the main characteristics of vectors of random variables and how to assess these characteristics. furthermore, it is described how probabilistic characterizations of functions of random variables can be established. on the basis of the lecture it is expected that the reader will acquire knowledge and skills with regard to:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 6375, 4353])","tensor([  101,  8835,  1019,  1006,  6614,  1997,  1996,  2556,  8835,  1007,
         1996,  6614,  1997,  1996,  2556,  8835,  2003,  2000,  8970,  1996,
         5144,  3378,  2007,  1996,  2364,  6459,  1997, 19019,  1997,  6721,
        10857,  1998,  2129,  2000, 14358,  2122,  6459,  1012,  7297,  1010,
         2009,  2003,  2649,  2129,  4013,  3676, 27965,  4588, 23191,  2015,
         1997,  4972,  1997,  6721, 10857,  2064,  2022,  2511,  1012,  2006,
         1996,  3978,  1997,  1996,  8835,  2009,  2003,  3517,  2008,  1996,
         8068,  2097,  9878,  3716,  1998,  4813,  2007,  7634,  2000,  1024,
          102])"
251,1,"['linear combination', 'linear', 'functions', 'random', 'random variables', 'expectation', 'expectation operation', 'variables', 'combination']", Example Uniform Distribution,seg_85,• how can the expectation operation be performed on a linear combination of random variables? • how can the expectation operation be performed on a linear combination of functions of random variables? • which rule applies for the expectation operation of functions of random variables?,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.])","tensor([2742, 6375, 4353])","tensor([  101,  1528,  2129,  2064,  1996, 17626,  3169,  2022,  2864,  2006,
         1037,  7399,  5257,  1997,  6721, 10857,  1029,  1528,  2129,  2064,
         1996, 17626,  3169,  2022,  2864,  2006,  1037,  7399,  5257,  1997,
         4972,  1997,  6721, 10857,  1029,  1528,  2029,  3627, 12033,  2005,
         1996, 17626,  3169,  1997,  4972,  1997,  6721, 10857,  1029,   102])"
252,1,"['coefficient', 'correlation coefficient', 'correlation', 'marginal probability distribution', 'conditional', 'covariance', 'conditional probability', 'marginal probability', 'probability distribution', 'variance operator', 'function', 'moment', 'marginal', 'probability', 'information', 'random variables', 'expectation', 'sum of two random variables', 'expectation operation', 'conditional probability distribution', 'variance', 'random', 'distribution', 'variables', 'joint']", Example Uniform Distribution,seg_85,• what is the relation between the expectation operation and the variance operation? • which are the properties of the expectation and the variance operator? • what is a random vector and what is a joint moment? • how is the covariance between two random variables defined? • how is the correlation coefficient defined and what information does it contain? • what is a marginal probability distribution? • what is a conditional probability distribution? • how can the probability distribution for the sum of two random variables be established? • how can the probability distribution for a function of random variables be established?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        1., 0., 1., 1., 0., 0., 0., 0.])","tensor([2742, 6375, 4353])","tensor([  101,  1528,  2054,  2003,  1996,  7189,  2090,  1996, 17626,  3169,
         1998,  1996, 23284,  3169,  1029,  1528,  2029,  2024,  1996,  5144,
         1997,  1996, 17626,  1998,  1996, 23284,  6872,  1029,  1528,  2054,
         2003,  1037,  6721,  9207,  1998,  2054,  2003,  1037,  4101,  2617,
         1029,  1528,  2129,  2003,  1996,  2522, 10755, 28335,  2090,  2048,
         6721, 10857,  4225,  1029,  1528,  2129,  2003,  1996, 16902, 19064,
         4225,  1998,  2054,  2592,  2515,  2009,  5383,  1029,  1528,  2054,
         2003,  1037, 14785,  9723,  4353,  1029,  1528,  2054,  2003,  1037,
        18462,  9723,  4353,  1029,  1528,  2129,  2064,  1996,  9723,  4353,
         2005,  1996,  7680,  1997,  2048,  6721, 10857,  2022,  2511,  1029,
         1528,  2129,  2064,  1996,  9723,  4353,  2005,  1037,  3853,  1997,
         6721, 10857,  2022,  2511,  1029,   102])"
253,1,"['random variable', 'random', 'expectation operation', 'expectation', 'variable']", Properties of the Expectation Operator,seg_87,"it is useful to note that the expectation operation possesses the following properties, where a, b and c are constants and x is a random variable:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 5144,  1997,  1996, 17626,  6872])","tensor([  101,  2009,  2003,  6179,  2000,  3602,  2008,  1996, 17626,  3169,
        14882,  1996,  2206,  5144,  1010,  2073,  1037,  1010,  1038,  1998,
         1039,  2024,  5377,  2015,  1998,  1060,  2003,  1037,  6721,  8023,
         1024,   102])"
254,1,"['linear', 'variance', 'random variable', 'random', 'expectation', 'variable']", Properties of the Expectation Operator,seg_87,"the implication of the last equation is that expectation, like differentiation or integration, is a linear operation. this linearity property is useful since it can be used, for example, to find the following formula for the variance of a random variable x in terms of more easily calculated quantities:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 5144,  1997,  1996, 17626,  6872])","tensor([  101,  1996, 25323,  1997,  1996,  2197,  8522,  2003,  2008, 17626,
         1010,  2066, 20582,  2030,  8346,  1010,  2003,  1037,  7399,  3169,
         1012,  2023,  7399,  3012,  3200,  2003,  6179,  2144,  2009,  2064,
         2022,  2109,  1010,  2005,  2742,  1010,  2000,  2424,  1996,  2206,
         5675,  2005,  1996, 23284,  1997,  1037,  6721,  8023,  1060,  1999,
         3408,  1997,  2062,  4089, 10174, 12450,  1024,   102])"
255,1,"['variance operator', 'variance']", Properties of the Expectation Operator,seg_87,by application of eq. 4.17 the following properties of the variance operator var [·] can easily be derived:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 5144,  1997,  1996, 17626,  6872])","tensor([  101,  2011,  4646,  1997,  1041,  4160,  1012,  1018,  1012,  2459,
         1996,  2206,  5144,  1997,  1996, 23284,  6872, 13075,  1031,  1087,
         1033,  2064,  4089,  2022,  5173,  1024,   102])"
256,1,"['random', 'random variable', 'variable']", Properties of the Expectation Operator,seg_87,"where a, b and c are constants and x is a random variable.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 5144,  1997,  1996, 17626,  6872])","tensor([ 101, 2073, 1037, 1010, 1038, 1998, 1039, 2024, 5377, 2015, 1998, 1060,
        2003, 1037, 6721, 8023, 1012,  102])"
257,1,"['functions', 'convex functions', 'inequality']", Properties of the Expectation Operator,seg_87,from eq. 4.17 it is furthermore seen that in general it is e[g(x)] = g(e[x]). in fact for convex functions g(x) it can be shown that the following inequality is valid (jensen’s inequality):,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0.])","tensor([ 5144,  1997,  1996, 17626,  6872])","tensor([  101,  2013,  1041,  4160,  1012,  1018,  1012,  2459,  2009,  2003,
         7297,  2464,  2008,  1999,  2236,  2009,  2003,  1041,  1031,  1043,
         1006,  1060,  1007,  1033,  1027,  1043,  1006,  1041,  1031,  1060,
         1033,  1007,  1012,  1999,  2755,  2005, 18309,  4972,  1043,  1006,
         1060,  1007,  2009,  2064,  2022,  3491,  2008,  1996,  2206, 16440,
         2003,  9398,  1006, 14857,  1521,  1055, 16440,  1007,  1024,   102])"
258,1,['linear'], Properties of the Expectation Operator,seg_87,where the equality holds if g(x) is linear.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 5144,  1997,  1996, 17626,  6872])","tensor([ 101, 2073, 1996, 9945, 4324, 2065, 1043, 1006, 1060, 1007, 2003, 7399,
        1012,  102])"
259,1,"['continuous', 'function', 'cumulative distribution function', 'continuous random variables', 'random', 'random variables', 'distribution', 'distribution function', 'variables', 'joint']", Random Vectors and Joint Moments,seg_89,"if a n-dimensional vector of continuous random variables x = (x1,x2, . . . ,xn)t , is considered the joint cumulative distribution function is given by:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 0., 0.])","tensor([ 6721, 19019,  1998,  4101,  5312])","tensor([  101,  2065,  1037,  1050,  1011,  8789,  9207,  1997,  7142,  6721,
        10857,  1060,  1027,  1006,  1060,  2487,  1010,  1060,  2475,  1010,
         1012,  1012,  1012,  1010,  1060,  2078,  1007,  1056,  1010,  2003,
         2641,  1996,  4101, 23260,  4353,  3853,  2003,  2445,  2011,  1024,
          102])"
260,1,"['joint probability', 'function', 'density function', 'probability', 'probability density function', 'joint probability density function', 'joint']", Random Vectors and Joint Moments,seg_89,and the joint probability density function is:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 0., 0., 0.])","tensor([ 6721, 19019,  1998,  4101,  5312])","tensor([ 101, 1998, 1996, 4101, 9723, 4304, 3853, 2003, 1024,  102])"
261,1,"['joint probability', 'function', 'density function', 'probability', 'probability density function', 'joint probability density function', 'distribution', 'joint']", Random Vectors and Joint Moments,seg_89,an example of a joint probability density function is illustrated in fig. 4.6 in the form of a binormal distribution.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 6721, 19019,  1998,  4101,  5312])","tensor([  101,  2019,  2742,  1997,  1037,  4101,  9723,  4304,  3853,  2003,
         7203,  1999, 20965,  1012,  1018,  1012,  1020,  1999,  1996,  2433,
         1997,  1037,  8026,  2953,  9067,  4353,  1012,   102])"
262,1,['covariance'], Random Vectors and Joint Moments,seg_89,the covariance cxixj between xi and xj is defined by:,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 6721, 19019,  1998,  4101,  5312])","tensor([  101,  1996,  2522, 10755, 28335,  1039,  9048,  2595,  3501,  2090,
         8418,  1998,  1060,  3501,  2003,  4225,  2011,  1024,   102])"
263,1,"['moment', 'variables', 'joint']", Random Vectors and Joint Moments,seg_89,and is also called the joint central moment between the variables xi and xj .,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 19019,  1998,  4101,  5312])","tensor([  101,  1998,  2003,  2036,  2170,  1996,  4101,  2430,  2617,  2090,
         1996, 10857,  8418,  1998,  1060,  3501,  1012,   102])"
264,1,"['dependence', 'linear', 'coefficient', 'correlation coefficient', 'correlation', 'variables', 'covariance']", Random Vectors and Joint Moments,seg_89,the covariance expresses the linear dependence between two variables. it is evident that cxixi = var[xi]. on the basis of the covariance the correlation coefficient is defined by:,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 0., 0., 0., 0.])","tensor([ 6721, 19019,  1998,  4101,  5312])","tensor([  101,  1996,  2522, 10755, 28335, 16783,  1996,  7399, 18642,  2090,
         2048, 10857,  1012,  2009,  2003, 10358,  2008,  1039,  9048,  9048,
         1027, 13075,  1031,  8418,  1033,  1012,  2006,  1996,  3978,  1997,
         1996,  2522, 10755, 28335,  1996, 16902, 19064,  2003,  4225,  2011,
         1024,   102])"
265,1,"['interval', 'coefficients', 'correlation', 'correlation coefficients']", Random Vectors and Joint Moments,seg_89,it is seen that ρx x = 1. the correlation coefficients can only take values in the i i interval [−1;1].,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 19019,  1998,  4101,  5312])","tensor([  101,  2009,  2003,  2464,  2008,  1171,  2595,  1060,  1027,  1015,
         1012,  1996, 16902, 21374,  2064,  2069,  2202,  5300,  1999,  1996,
         1045,  1045, 13483,  1031,  1597,  2487,  1025,  1015,  1033,  1012,
          102])"
266,1,"['cases', 'density function', 'functions', 'coefficient', 'outcome', 'correlation coefficient', 'correlation', 'independent', 'mean', 'function', 'random variables', 'random', 'density functions', 'variables', 'joint', 'variable']", Random Vectors and Joint Moments,seg_89,"a negative correlation coefficient between two random variables implies that if the outcome of one variable is large compared to its mean value, the outcome of the other variable is likely to be small compared to its mean value. a positive correlation coefficient between two variables implies that if the outcome of one variable is large compared to its mean value, the outcome of the other variable is also likely to be large compared to its mean value. if two variables are independent their correlation coefficient is zero and the joint density function is the product of the onedimensional density functions. in many cases it is possible to obtain a sufficiently",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 19019,  1998,  4101,  5312])","tensor([  101,  1037,  4997, 16902, 19064,  2090,  2048,  6721, 10857, 12748,
         2008,  2065,  1996,  9560,  1997,  2028,  8023,  2003,  2312,  4102,
         2000,  2049,  2812,  3643,  1010,  1996,  9560,  1997,  1996,  2060,
         8023,  2003,  3497,  2000,  2022,  2235,  4102,  2000,  2049,  2812,
         3643,  1012,  1037,  3893, 16902, 19064,  2090,  2048, 10857, 12748,
         2008,  2065,  1996,  9560,  1997,  2028,  8023,  2003,  2312,  4102,
         2000,  2049,  2812,  3643,  1010,  1996,  9560,  1997,  1996,  2060,
         8023,  2003,  2036,  3497,  2000,  2022,  2312,  4102,  2000,  2049,
         2812,  3643,  1012,  2065,  2048, 10857,  2024,  2981,  2037, 16902,
        19064,  2003,  5717,  1998,  1996,  4101,  4304,  3853,  2003,  1996,
         4031,  1997,  1996,  2028, 22172,  6132, 19301,  4304,  4972,  1012,
         1999,  2116,  3572,  2009,  2003,  2825,  2000,  6855,  1037, 12949,
          102])"
267,1,"['function', 'cumulative distribution function', 'functions', 'coefficients', 'approximation', 'correlation coefficients', 'parameters', 'correlation', 'distribution', 'distribution function', 'variables']", Random Vectors and Joint Moments,seg_89,"accurate approximation to the n-dimensional cumulative distribution function from the one-dimensional distribution functions of the n variables and their parameters, and the correlation coefficients.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 6721, 19019,  1998,  4101,  5312])","tensor([  101,  8321, 20167,  2000,  1996,  1050,  1011,  8789, 23260,  4353,
         3853,  2013,  1996,  2028,  1011,  8789,  4353,  4972,  1997,  1996,
         1050, 10857,  1998,  2037, 11709,  1010,  1998,  1996, 16902, 21374,
         1012,   102])"
268,1,"['function', 'linear', 'variance', 'random', 'expected value']", Random Vectors and Joint Moments,seg_89,"finally using eqs. 4.17, 4.18 and 4.22 it can be shown that the expected value e[y ] and the variance var[y ], where y is a linear function of the random vector x = (x1,x2, . . . ,xn)t i.e.:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 19019,  1998,  4101,  5312])","tensor([  101,  2633,  2478,  1041,  4160,  2015,  1012,  1018,  1012,  2459,
         1010,  1018,  1012,  2324,  1998,  1018,  1012,  2570,  2009,  2064,
         2022,  3491,  2008,  1996,  3517,  3643,  1041,  1031,  1061,  1033,
         1998,  1996, 23284, 13075,  1031,  1061,  1033,  1010,  2073,  1061,
         2003,  1037,  7399,  3853,  1997,  1996,  6721,  9207,  1060,  1027,
         1006,  1060,  2487,  1010,  1060,  2475,  1010,  1012,  1012,  1012,
         1010,  1060,  2078,  1007,  1056,  1045,  1012,  1041,  1012,  1024,
          102])"
269,1,"['function', 'linear', 'variances', 'expected values', 'variables']", Example Linear Combinations and Random Variables,seg_91,consider the linear function y = 5 + 2x1 + 3x2. the expected values of the random variables x1 and x2 are e[x1] = 7 as well as e[x2] = 4. the variances are,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 2742,  7399, 14930,  1998,  6721, 10857])","tensor([  101,  5136,  1996,  7399,  3853,  1061,  1027,  1019,  1009,  1016,
         2595,  2487,  1009,  1017,  2595,  2475,  1012,  1996,  3517,  5300,
         1997,  1996,  6721, 10857,  1060,  2487,  1998,  1060,  2475,  2024,
         1041,  1031,  1060,  2487,  1033,  1027,  1021,  2004,  2092,  2004,
         1041,  1031,  1060,  2475,  1033,  1027,  1018,  1012,  1996, 23284,
         2015,  2024,   102])"
270,1,"['random', 'random variables', 'variables', 'covariance', 'independent']", Example Linear Combinations and Random Variables,seg_91,"var[x1] = 1 and var[x2] = 1.5. both random variables x1 and x2 are assumed to be independent, hence the covariance is cx1x2 = 0.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  7399, 14930,  1998,  6721, 10857])","tensor([  101, 13075,  1031,  1060,  2487,  1033,  1027,  1015,  1998, 13075,
         1031,  1060,  2475,  1033,  1027,  1015,  1012,  1019,  1012,  2119,
         6721, 10857,  1060,  2487,  1998,  1060,  2475,  2024,  5071,  2000,
         2022,  2981,  1010,  6516,  1996,  2522, 10755, 28335,  2003,  1039,
         2595,  2487,  2595,  2475,  1027,  1014,  1012,   102])"
271,1,['expected value'], Example Linear Combinations and Random Variables,seg_91,the expected value e[y ] is determined as follows:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  7399, 14930,  1998,  6721, 10857])","tensor([ 101, 1996, 3517, 3643, 1041, 1031, 1061, 1033, 2003, 4340, 2004, 4076,
        1024,  102])"
272,1,['variance'], Example Linear Combinations and Random Variables,seg_91,the variance var (y ) is determined as follows:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  7399, 14930,  1998,  6721, 10857])","tensor([  101,  1996, 23284, 13075,  1006,  1061,  1007,  2003,  4340,  2004,
         4076,  1024,   102])"
273,0,[], Example Linear Combinations and Random Variables,seg_91,aiajcxixj⎞= a12var [x1] + a22var [x2],tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2742,  7399, 14930,  1998,  6721, 10857])","tensor([  101,   100,  1027, 17350,  2475, 10755,  1031,  1060,  2487,  1033,
         1009, 22441,  2475, 10755,  1031,  1060,  2475,  1033,   102])"
274,1,"['conditional probability', 'function', 'density function', 'probability', 'probability density function', 'conditional probability density', 'conditional probability density function', 'random variable', 'outcome', 'random', 'conditional', 'variable']", Conditional Distributions and Conditional Moments,seg_93,"the conditional probability density function for the random variable x1, conditional on the outcome of the random variable x2 is denoted fx1|x2(x1|x2) and defined by:",tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([18462, 20611,  1998, 18462,  5312])","tensor([  101,  1996, 18462,  9723,  4304,  3853,  2005,  1996,  6721,  8023,
         1060,  2487,  1010, 18462,  2006,  1996,  9560,  1997,  1996,  6721,
         8023,  1060,  2475,  2003, 19537, 23292,  2487,  1064,  1060,  2475,
         1006,  1060,  2487,  1064,  1060,  2475,  1007,  1998,  4225,  2011,
         1024,   102])"
275,1,"['conditional probability', 'function', 'density function', 'probability', 'probability density function', 'conditional probability density', 'conditional probability density function', 'distribution', 'conditional']", Conditional Distributions and Conditional Moments,seg_93,in accordance with the definition of conditional probability given previously. figure 4.7 illustrates the conditional probability density function fx1|x2(x1|x2 = 0.5t) of the binormal distribution illustrated in fig. 4.6.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([18462, 20611,  1998, 18462,  5312])","tensor([  101,  1999, 10388,  2007,  1996,  6210,  1997, 18462,  9723,  2445,
         3130,  1012,  3275,  1018,  1012,  1021, 24899,  1996, 18462,  9723,
         4304,  3853, 23292,  2487,  1064,  1060,  2475,  1006,  1060,  2487,
         1064,  1060,  2475,  1027,  1014,  1012,  1019,  2102,  1007,  1997,
         1996,  8026,  2953,  9067,  4353,  7203,  1999, 20965,  1012,  1018,
         1012,  1020,  1012,   102])"
276,1,"['events', 'probabilities', 'random', 'independent', 'case']", Conditional Distributions and Conditional Moments,seg_93,as for the case when probabilities of events were considered two random variables x1 and x2 are said to be independent when there is:,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([18462, 20611,  1998, 18462,  5312])","tensor([  101,  2004,  2005,  1996,  2553,  2043,  4013,  3676, 14680,  1997,
         2824,  2020,  2641,  2048,  6721, 10857,  1060,  2487,  1998,  1060,
         2475,  2024,  2056,  2000,  2022,  2981,  2043,  2045,  2003,  1024,
          102])"
277,1,"['conditional', 'distribution']", Conditional Distributions and Conditional Moments,seg_93,by integration of eq. 4.26 the conditional cumulative distribution fx1|x2(x1|x2) is obtained as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([18462, 20611,  1998, 18462,  5312])","tensor([  101,  2011,  8346,  1997,  1041,  4160,  1012,  1018,  1012,  2656,
         1996, 18462, 23260,  4353, 23292,  2487,  1064,  1060,  2475,  1006,
         1060,  2487,  1064,  1060,  2475,  1007,  2003,  4663,  2004,  1024,
          102])"
278,1,"['total probability theorem', 'function', 'density function', 'probability', 'probability density function', 'distribution', 'total probability']", Conditional Distributions and Conditional Moments,seg_93,"and finally by integration of eq. 4.28 weighed with the probability density function of x2, i.e. fx2(x2) the unconditional cumulative distribution fx1(x1) is achieved by the total probability theorem:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([18462, 20611,  1998, 18462,  5312])","tensor([  101,  1998,  2633,  2011,  8346,  1997,  1041,  4160,  1012,  1018,
         1012,  2654, 12781,  2007,  1996,  9723,  4304,  3853,  1997,  1060,
         2475,  1010,  1045,  1012,  1041,  1012, 23292,  2475,  1006,  1060,
         2475,  1007,  1996,  4895,  8663, 27064, 23260,  4353, 23292,  2487,
         1006,  1060,  2487,  1007,  2003,  4719,  2011,  1996,  2561,  9723,
         9872,  1024,   102])"
279,1,"['continuous', 'variance', 'continuous random variables', 'moments', 'random', 'random variables', 'conditional variance', 'jointly', 'variables', 'conditional', 'expected value']", Conditional Distributions and Conditional Moments,seg_93,the conditional moments of jointly distributed continuous random variables follow straightforwardly from eq. 4.7 by use of eq. 4.27 and the conditional expected value μx1|x2 and the conditional variance of e.g. the jointly distributed random variables x1 given x2 are evaluated by:,tensor(1),"tensor([0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([18462, 20611,  1998, 18462,  5312])","tensor([  101,  1996, 18462,  5312,  1997, 10776,  5500,  7142,  6721, 10857,
         3582, 19647,  2135,  2013,  1041,  4160,  1012,  1018,  1012,  1021,
         2011,  2224,  1997,  1041,  4160,  1012,  1018,  1012,  2676,  1998,
         1996, 18462,  3517,  3643,  1166,  2595,  2487,  1064,  1060,  2475,
         1998,  1996, 18462, 23284,  1997,  1041,  1012,  1043,  1012,  1996,
        10776,  5500,  6721, 10857,  1060,  2487,  2445,  1060,  2475,  2024,
        16330,  2011,  1024,   102])"
280,1,"['joint probability', 'conditional probability', 'function', 'density function', 'probability', 'probability density function', 'conditional probability density', 'conditional probability density function', 'joint probability density function', 'random', 'joint', 'conditional']", The Probability Distribution for the Sum of Two Random Variables,seg_95,"based on the result in eq. 4.26 the probability density function for the random variable y = x1 + x2 may be derived for a given joint probability density function fx1,x2(x1, x2). first the conditional probability density function of y given x1 = x1 is considered i.e.:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9723,  4353,  2005,  1996,  7680,  1997,  2048,  6721, 10857])","tensor([  101,  2241,  2006,  1996,  2765,  1999,  1041,  4160,  1012,  1018,
         1012,  2656,  1996,  9723,  4304,  3853,  2005,  1996,  6721,  8023,
         1061,  1027,  1060,  2487,  1009,  1060,  2475,  2089,  2022,  5173,
         2005,  1037,  2445,  4101,  9723,  4304,  3853, 23292,  2487,  1010,
         1060,  2475,  1006,  1060,  2487,  1010,  1060,  2475,  1007,  1012,
         2034,  1996, 18462,  9723,  4304,  3853,  1997,  1061,  2445,  1060,
         2487,  1027,  1060,  2487,  2003,  2641,  1045,  1012,  1041,  1012,
         1024,   102])"
281,1,"['conditional probability', 'function', 'density function', 'probability', 'probability density function', 'conditional probability density', 'conditional probability density function', 'conditional']", The Probability Distribution for the Sum of Two Random Variables,seg_95,where the conditional probability density function of x2 given x1 = x1 is:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 1996,  9723,  4353,  2005,  1996,  7680,  1997,  2048,  6721, 10857])","tensor([  101,  2073,  1996, 18462,  9723,  4304,  3853,  1997,  1060,  2475,
         2445,  1060,  2487,  1027,  1060,  2487,  2003,  1024,   102])"
282,1,"['function', 'density function', 'probability', 'probability density function']", The Probability Distribution for the Sum of Two Random Variables,seg_95,thus the probability density function for y given x1 = x1 can be written as:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 1996,  9723,  4353,  2005,  1996,  7680,  1997,  2048,  6721, 10857])","tensor([ 101, 2947, 1996, 9723, 4304, 3853, 2005, 1061, 2445, 1060, 2487, 1027,
        1060, 2487, 2064, 2022, 2517, 2004, 1024,  102])"
283,1,"['joint probability', 'function', 'density function', 'probability', 'probability density function', 'joint probability density function', 'joint']", The Probability Distribution for the Sum of Two Random Variables,seg_95,"and the joint probability density function for fy,x1(y, x1):",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 1996,  9723,  4353,  2005,  1996,  7680,  1997,  2048,  6721, 10857])","tensor([ 101, 1998, 1996, 4101, 9723, 4304, 3853, 2005, 1042, 2100, 1010, 1060,
        2487, 1006, 1061, 1010, 1060, 2487, 1007, 1024,  102])"
284,1,"['function', 'density function', 'marginal probability density', 'marginal', 'probability', 'probability density function', 'marginal probability']", The Probability Distribution for the Sum of Two Random Variables,seg_95,from which one can get the marginal probability density function of y by integrating out over the definition domain of x1 i.e.:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9723,  4353,  2005,  1996,  7680,  1997,  2048,  6721, 10857])","tensor([  101,  2013,  2029,  2028,  2064,  2131,  1996, 14785,  9723,  4304,
         3853,  1997,  1061,  2011, 22380,  2041,  2058,  1996,  6210,  5884,
         1997,  1060,  2487,  1045,  1012,  1041,  1012,  1024,   102])"
285,1,"['probability density functions', 'marginal probability density', 'marginal', 'probability', 'functions', 'marginal probability density functions', 'density functions', 'marginal probability', 'distribution']", The Probability Distribution for the Sum of Two Random Variables,seg_95,figure 4.8 illustrates both the marginal probability density functions of a binormal distribution.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0.])","tensor([ 1996,  9723,  4353,  2005,  1996,  7680,  1997,  2048,  6721, 10857])","tensor([  101,  3275,  1018,  1012,  1022, 24899,  2119,  1996, 14785,  9723,
         4304,  4972,  1997,  1037,  8026,  2953,  9067,  4353,  1012,   102])"
286,1,"['convolution integral', 'convolution', 'variables', 'case', 'independent']", The Probability Distribution for the Sum of Two Random Variables,seg_95,"for the special case where the variables x1 and x2 are independent, eq. 4.35 can be written in the form of a convolution integral:",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,
        0.])","tensor([ 1996,  9723,  4353,  2005,  1996,  7680,  1997,  2048,  6721, 10857])","tensor([  101,  2005,  1996,  2569,  2553,  2073,  1996, 10857,  1060,  2487,
         1998,  1060,  2475,  2024,  2981,  1010,  1041,  4160,  1012,  1018,
         1012,  3486,  2064,  2022,  2517,  1999,  1996,  2433,  1997,  1037,
         9530,  6767,  7630,  3508,  9897,  1024,   102])"
287,0,['n'], Example Density Function for the Sum of Two Random VariablesSpecial Case Normal Distribution,seg_97,"the total load l on a structural element, consisting of a live load n and self-weight",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2742,  4304,  3853,  2005,  1996,  7680,  1997,  2048,  6721, 10857,
        13102,  8586,  4818,  2553,  3671,  4353])","tensor([ 101, 1996, 2561, 7170, 1048, 2006, 1037, 8332, 5783, 1010, 5398, 1997,
        1037, 2444, 7170, 1050, 1998, 2969, 1011, 3635,  102])"
288,1,['random'], Example Density Function for the Sum of Two Random VariablesSpecial Case Normal Distribution,seg_97,g and expressed as l = n + g is to be determined. both the random variables—,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0.])","tensor([ 2742,  4304,  3853,  2005,  1996,  7680,  1997,  2048,  6721, 10857,
        13102,  8586,  4818,  2553,  3671,  4353])","tensor([  101,  1043,  1998,  5228,  2004,  1048,  1027,  1050,  1009,  1043,
         2003,  2000,  2022,  4340,  1012,  2119,  1996,  6721, 10857,  1517,
          102])"
289,1,"['normal', 'normal distribution', 'distribution']", Example Density Function for the Sum of Two Random VariablesSpecial Case Normal Distribution,seg_97,live load n and self weight g are assumed to follow the normal distribution and to,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 2742,  4304,  3853,  2005,  1996,  7680,  1997,  2048,  6721, 10857,
        13102,  8586,  4818,  2553,  3671,  4353])","tensor([ 101, 2444, 7170, 1050, 1998, 2969, 3635, 1043, 2024, 5071, 2000, 3582,
        1996, 3671, 4353, 1998, 2000,  102])"
290,1,['covariance'], Example Density Function for the Sum of Two Random VariablesSpecial Case Normal Distribution,seg_97,be independent—hence the covariance is cgn = 0.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  4304,  3853,  2005,  1996,  7680,  1997,  2048,  6721, 10857,
        13102,  8586,  4818,  2553,  3671,  4353])","tensor([  101,  2022,  2981,  1517,  6516,  1996,  2522, 10755, 28335,  2003,
         1039, 16206,  1027,  1014,  1012,   102])"
291,1,"['convolution', 'convolution integral']", Example Density Function for the Sum of Two Random VariablesSpecial Case Normal Distribution,seg_97,with the aid of the convolution integral it is possible to show that a sum of two,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 2742,  4304,  3853,  2005,  1996,  7680,  1997,  2048,  6721, 10857,
        13102,  8586,  4818,  2553,  3671,  4353])","tensor([ 101, 2007, 1996, 4681, 1997, 1996, 9530, 6767, 7630, 3508, 9897, 2009,
        2003, 2825, 2000, 2265, 2008, 1037, 7680, 1997, 2048,  102])"
292,1,"['distribution', 'normal distribution', 'normal', 'distributions']", Example Density Function for the Sum of Two Random VariablesSpecial Case Normal Distribution,seg_97,normal distributions follows a normal distribution (see also the section on central,tensor(1),"tensor([0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  4304,  3853,  2005,  1996,  7680,  1997,  2048,  6721, 10857,
        13102,  8586,  4818,  2553,  3671,  4353])","tensor([  101,  3671, 20611,  4076,  1037,  3671,  4353,  1006,  2156,  2036,
         1996,  2930,  2006,  2430,   102])"
293,0,[], Example Density Function for the Sum of Two Random VariablesSpecial Case Normal Distribution,seg_97,limit theorem on page 62):,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2742,  4304,  3853,  2005,  1996,  7680,  1997,  2048,  6721, 10857,
        13102,  8586,  4818,  2553,  3671,  4353])","tensor([ 101, 5787, 9872, 2006, 3931, 5786, 1007, 1024,  102])"
294,1,"['normal distributions', 'distributions', 'convolution', 'normal']", Example Density Function for the Sum of Two Random VariablesSpecial Case Normal Distribution,seg_97,the convolution of two normal distributions can be assessed by eq. 4.25. the,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 2742,  4304,  3853,  2005,  1996,  7680,  1997,  2048,  6721, 10857,
        13102,  8586,  4818,  2553,  3671,  4353])","tensor([  101,  1996,  9530,  6767,  7630,  3508,  1997,  2048,  3671, 20611,
         2064,  2022, 14155,  2011,  1041,  4160,  1012,  1018,  1012,  2423,
         1012,  1996,   102])"
295,1,['variances'], Example Density Function for the Sum of Two Random VariablesSpecial Case Normal Distribution,seg_97,expected values and variances of workload n and self-weight g are:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  4304,  3853,  2005,  1996,  7680,  1997,  2048,  6721, 10857,
        13102,  8586,  4818,  2553,  3671,  4353])","tensor([  101,  3517,  5300,  1998, 23284,  2015,  1997,  2147, 11066,  1050,
         1998,  2969,  1011,  3635,  1043,  2024,  1024,   102])"
296,1,['expected value'], Example Density Function for the Sum of Two Random VariablesSpecial Case Normal Distribution,seg_97,the expected value of the total load l is determined by:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  4304,  3853,  2005,  1996,  7680,  1997,  2048,  6721, 10857,
        13102,  8586,  4818,  2553,  3671,  4353])","tensor([ 101, 1996, 3517, 3643, 1997, 1996, 2561, 7170, 1048, 2003, 4340, 2011,
        1024,  102])"
297,1,['variance'], Example Density Function for the Sum of Two Random VariablesSpecial Case Normal Distribution,seg_97,the variance of the total load l is determined by:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  4304,  3853,  2005,  1996,  7680,  1997,  2048,  6721, 10857,
        13102,  8586,  4818,  2553,  3671,  4353])","tensor([  101,  1996, 23284,  1997,  1996,  2561,  7170,  1048,  2003,  4340,
         2011,  1024,   102])"
298,1,"['functions', 'random', 'density functions', 'random variables', 'variables']", Example Density Function for the Sum of Two Random VariablesSpecial Case Normal Distribution,seg_97,"the density functions of the three random variables n , g and l are illustrated",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  4304,  3853,  2005,  1996,  7680,  1997,  2048,  6721, 10857,
        13102,  8586,  4818,  2553,  3671,  4353])","tensor([  101,  1996,  4304,  4972,  1997,  1996,  2093,  6721, 10857,  1050,
         1010,  1043,  1998,  1048,  2024,  7203,   102])"
299,1,"['cases', 'function', 'cumulative distribution function', 'distribution function', 'realization', 'random variable', 'random', 'condition', 'distribution', 'variable', 'case']", The Probability Distribution for Functions of Random Variables,seg_99,"in some cases it is interesting to be able to derive the cumulative distribution function fy (y) for a random variable y which is given as a function of another random variable x i.e. y = g(x), with given cumulative distribution function fx(x). under the condition that the function g(x) is monotonically increasing and furthermore, represents a one-to-one mapping of x into y, a realization of y is only smaller than y0 if correspondingly the realization of x is smaller than x0 which in turn is given by x0 = g−1(y0). in this case the cumulative distribution function fy (y) can be readily determined by:",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 1996,  9723,  4353,  2005,  4972,  1997,  6721, 10857])","tensor([  101,  1999,  2070,  3572,  2009,  2003,  5875,  2000,  2022,  2583,
         2000, 18547,  1996, 23260,  4353,  3853,  1042,  2100,  1006,  1061,
         1007,  2005,  1037,  6721,  8023,  1061,  2029,  2003,  2445,  2004,
         1037,  3853,  1997,  2178,  6721,  8023,  1060,  1045,  1012,  1041,
         1012,  1061,  1027,  1043,  1006,  1060,  1007,  1010,  2007,  2445,
        23260,  4353,  3853, 23292,  1006,  1060,  1007,  1012,  2104,  1996,
         4650,  2008,  1996,  3853,  1043,  1006,  1060,  1007,  2003, 18847,
        25009,  3973,  4852,  1998,  7297,  1010,  5836,  1037,  2028,  1011,
         2000,  1011,  2028, 12375,  1997,  1060,  2046,  1061,  1010,  1037,
        12393,  1997,  1061,  2003,  2069,  3760,  2084,  1061,  2692,  2065,
         7978,  2135,  1996, 12393,  1997,  1060,  2003,  3760,  2084,  1060,
         2692,  2029,  1999,  2735,  2003,  2445,  2011,  1060,  2692,  1027,
         1043, 27944,  1006,  1061,  2692,  1007,  1012,  1999,  2023,  2553,
         1996, 23260,  4353,  3853,  1042,  2100,  1006,  1061,  1007,  2064,
         2022, 12192,  4340,  2011,  1024,   102])"
300,0,[], The Probability Distribution for Functions of Random Variables,seg_99,which is also written as:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 1996,  9723,  4353,  2005,  4972,  1997,  6721, 10857])","tensor([ 101, 2029, 2003, 2036, 2517, 2004, 1024,  102])"
301,1,"['function', 'density function', 'probability', 'probability density function']", The Probability Distribution for Functions of Random Variables,seg_99,in accordance with eq. 4.2 the probability density function fy (y) is simply given by:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9723,  4353,  2005,  4972,  1997,  6721, 10857])","tensor([  101,  1999, 10388,  2007,  1041,  4160,  1012,  1018,  1012,  1016,
         1996,  9723,  4304,  3853,  1042,  2100,  1006,  1061,  1007,  2003,
         3432,  2445,  2011,  1024,   102])"
302,0,[], The Probability Distribution for Functions of Random Variables,seg_99,which immediately leads to:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0])","tensor([ 1996,  9723,  4353,  2005,  4972,  1997,  6721, 10857])","tensor([ 101, 2029, 3202, 5260, 2000, 1024,  102])"
303,0,[], The Probability Distribution for Functions of Random Variables,seg_99,it is noticed that the application of eqs. 4.41 and 4.42 necessitates that g(x) is at least one time differentiable in regard to x.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 1996,  9723,  4353,  2005,  4972,  1997,  6721, 10857])","tensor([  101,  2009,  2003,  4384,  2008,  1996,  4646,  1997,  1041,  4160,
         2015,  1012,  1018,  1012,  4601,  1998,  1018,  1012,  4413, 26785,
         7971, 17570,  2015,  2008,  1043,  1006,  1060,  1007,  2003,  2012,
         2560,  2028,  2051,  2367, 19210,  1999,  7634,  2000,  1060,  1012,
          102])"
304,1,"['function', 'functions', 'realization', 'case']", The Probability Distribution for Functions of Random Variables,seg_99,"now if the function g(x) instead of being monotonically increasing is monotonically decreasing, a realization of y smaller that y0 corresponds to a realization of x larger than x0, in which case it is necessary to change the sign of the derivative dx/dy in eq. 4.42. generally, for monotonically increasing or decreasing one-to- one functions g(x) there is:",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9723,  4353,  2005,  4972,  1997,  6721, 10857])","tensor([  101,  2085,  2065,  1996,  3853,  1043,  1006,  1060,  1007,  2612,
         1997,  2108, 18847, 25009,  3973,  4852,  2003, 18847, 25009,  3973,
        16922,  1010,  1037, 12393,  1997,  1061,  3760,  2008,  1061,  2692,
        14788,  2000,  1037, 12393,  1997,  1060,  3469,  2084,  1060,  2692,
         1010,  1999,  2029,  2553,  2009,  2003,  4072,  2000,  2689,  1996,
         3696,  1997,  1996, 13819,  1040,  2595,  1013,  1040,  2100,  1999,
         1041,  4160,  1012,  1018,  1012,  4413,  1012,  3227,  1010,  2005,
        18847, 25009,  3973,  4852,  2030, 16922,  2028,  1011,  2000,  1011,
         2028,  4972,  1043,  1006,  1060,  1007,  2045,  2003,  1024,   102])"
305,1,"['jointly', 'random', 'case']", The Probability Distribution for Functions of Random Variables,seg_99,as shown in e.g. thoft-christensen and baker [13] the relationship given in eq. 4.34 can be generalized to consider the case of jointly distributed random variables.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 1., 0., 0., 0.])","tensor([ 1996,  9723,  4353,  2005,  4972,  1997,  6721, 10857])","tensor([  101,  2004,  3491,  1999,  1041,  1012,  1043,  1012, 27793,  6199,
         1011, 24189,  1998,  6243,  1031,  2410,  1033,  1996,  3276,  2445,
         1999,  1041,  4160,  1012,  1018,  1012,  4090,  2064,  2022, 18960,
         2000,  5136,  1996,  2553,  1997, 10776,  5500,  6721, 10857,  1012,
          102])"
306,1,"['random', 'functions']", The Probability Distribution for Functions of Random Variables,seg_99,"consider the random vector y = (y1, y2, . . . , yn)t with individual components given as one-to-one mapping monotonically increasing or decreasing functions gi , i = 1,2, . . . , n of the components of the random vector x = (x1,x2, . . . ,xn)t as:",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9723,  4353,  2005,  4972,  1997,  6721, 10857])","tensor([  101,  5136,  1996,  6721,  9207,  1061,  1027,  1006,  1061,  2487,
         1010,  1061,  2475,  1010,  1012,  1012,  1012,  1010,  1061,  2078,
         1007,  1056,  2007,  3265,  6177,  2445,  2004,  2028,  1011,  2000,
         1011,  2028, 12375, 18847, 25009,  3973,  4852,  2030, 16922,  4972,
        21025,  1010,  1045,  1027,  1015,  1010,  1016,  1010,  1012,  1012,
         1012,  1010,  1050,  1997,  1996,  6177,  1997,  1996,  6721,  9207,
         1060,  1027,  1006,  1060,  2487,  1010,  1060,  2475,  1010,  1012,
         1012,  1012,  1010,  1060,  2078,  1007,  1056,  2004,  1024,   102])"
307,0,[], The Probability Distribution for Functions of Random Variables,seg_99,then there is:,tensor(0),"tensor([0, 0, 0, 0, 0, 0])","tensor([ 1996,  9723,  4353,  2005,  4972,  1997,  6721, 10857])","tensor([ 101, 2059, 2045, 2003, 1024,  102])"
308,1,"['numerical', 'determinant']", The Probability Distribution for Functions of Random Variables,seg_99,where |j| is the numerical value of the determinant of j given by:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0.])","tensor([ 1996,  9723,  4353,  2005,  4972,  1997,  6721, 10857])","tensor([  101,  2073,  1064,  1046,  1064,  2003,  1996, 15973,  3643,  1997,
         1996, 28283, 22311,  3372,  1997,  1046,  2445,  2011,  1024,   102])"
309,1,"['random', 'function', 'expected value']", The Probability Distribution for Functions of Random Variables,seg_99,"finally, the expected value e[y ] of a function g(x) of the random vector x = (x1,x2, . . . ,xn)t is given by:",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9723,  4353,  2005,  4972,  1997,  6721, 10857])","tensor([ 101, 2633, 1010, 1996, 3517, 3643, 1041, 1031, 1061, 1033, 1997, 1037,
        3853, 1043, 1006, 1060, 1007, 1997, 1996, 6721, 9207, 1060, 1027, 1006,
        1060, 2487, 1010, 1060, 2475, 1010, 1012, 1012, 1012, 1010, 1060, 2078,
        1007, 1056, 2003, 2445, 2011, 1024,  102])"
310,1,"['random', 'variable']", Example Probability Distribution for a Function of Random Variables,seg_101,"the costs c of a project consist of fixed costs of 10,000 chf and variable costs of 100 chf for each working hour h , that is c = 10,000 + 100h . for the random",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0.])","tensor([ 2742,  9723,  4353,  2005,  1037,  3853,  1997,  6721, 10857])","tensor([  101,  1996,  5366,  1039,  1997,  1037,  2622,  8676,  1997,  4964,
         5366,  1997,  2184,  1010,  2199, 10381,  2546,  1998,  8023,  5366,
         1997,  2531, 10381,  2546,  2005,  2169,  2551,  3178,  1044,  1010,
         2008,  2003,  1039,  1027,  2184,  1010,  2199,  1009,  2531,  2232,
         1012,  2005,  1996,  6721,   102])"
311,1,"['probability', 'probability distribution', 'distribution']", Example Probability Distribution for a Function of Random Variables,seg_101,"variable h , the probability distribution is given. how can the probability distribution for c be derived?",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0.])","tensor([ 2742,  9723,  4353,  2005,  1037,  3853,  1997,  6721, 10857])","tensor([ 101, 8023, 1044, 1010, 1996, 9723, 4353, 2003, 2445, 1012, 2129, 2064,
        1996, 9723, 4353, 2005, 1039, 2022, 5173, 1029,  102])"
312,1,"['random', 'random variables', 'variables']", Example Probability Distribution for a Function of Random Variables,seg_101,"the relationship of the two random variables is monotonically increasing and one-to-one. the costs c are less than any particular value c only if the number of hours h is less than a particular value h, that is fc(c) = p [c ≤ c] = p [h ≤ g−1(c)] = fh (g−1(c)) (fig. 4.10).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  9723,  4353,  2005,  1037,  3853,  1997,  6721, 10857])","tensor([  101,  1996,  3276,  1997,  1996,  2048,  6721, 10857,  2003, 18847,
        25009,  3973,  4852,  1998,  2028,  1011,  2000,  1011,  2028,  1012,
         1996,  5366,  1039,  2024,  2625,  2084,  2151,  3327,  3643,  1039,
         2069,  2065,  1996,  2193,  1997,  2847,  1044,  2003,  2625,  2084,
         1037,  3327,  3643,  1044,  1010,  2008,  2003,  4429,  1006,  1039,
         1007,  1027,  1052,  1031,  1039,  1608,  1039,  1033,  1027,  1052,
         1031,  1044,  1608,  1043, 27944,  1006,  1039,  1007,  1033,  1027,
         1042,  2232,  1006,  1043, 27944,  1006,  1039,  1007,  1007,  1006,
        20965,  1012,  1018,  1012,  2184,  1007,  1012,   102])"
313,1,"['function', 'density function', 'probability', 'probability density function']", Example Probability Distribution for a Function of Random Variables,seg_101,"by applying eq. 4.43, the probability density function can be established. by",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2742,  9723,  4353,  2005,  1037,  3853,  1997,  6721, 10857])","tensor([  101,  2011, 11243,  1041,  4160,  1012,  1018,  1012,  4724,  1010,
         1996,  9723,  4304,  3853,  2064,  2022,  2511,  1012,  2011,   102])"
314,1,"['function', 'density function', 'range', 'probability', 'probability density function']", Example Probability Distribution for a Function of Random Variables,seg_101,"for a probability density function fh (h) = 2.2 − 0.02h in the range of 100 ≤ h ≤ 110, the following probability density function for fc(c) is obtained:",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  9723,  4353,  2005,  1037,  3853,  1997,  6721, 10857])","tensor([ 101, 2005, 1037, 9723, 4304, 3853, 1042, 2232, 1006, 1044, 1007, 1027,
        1016, 1012, 1016, 1597, 1014, 1012, 6185, 2232, 1999, 1996, 2846, 1997,
        2531, 1608, 1044, 1608, 7287, 1010, 1996, 2206, 9723, 4304, 3853, 2005,
        4429, 1006, 1039, 1007, 2003, 4663, 1024,  102])"
315,1,"['function', 'range']", Example Probability Distribution for a Function of Random Variables,seg_101,"the range values for c is obtained from the function c = 10,000 + 100h.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 2742,  9723,  4353,  2005,  1037,  3853,  1997,  6721, 10857])","tensor([ 101, 1996, 2846, 5300, 2005, 1039, 2003, 4663, 2013, 1996, 3853, 1039,
        1027, 2184, 1010, 2199, 1009, 2531, 2232, 1012,  102])"
316,1,"['function', 'density function', 'linear', 'rate', 'probability', 'probability density function', 'random', 'population', 'case']", Example Probability Distribution for a Function of Random Variables,seg_101,"in this simple case with a linear function, the shape of the density function does not change, as can be seen in fig. 4.10. as an example of a case where the shape of the probability density function changes, consider the function q = keλt . q, the number of bacteria in a tank, can increase starting from an initial population size k with a growth rate λ, until the tank is emptied after a random time period t .",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0.])","tensor([ 2742,  9723,  4353,  2005,  1037,  3853,  1997,  6721, 10857])","tensor([  101,  1999,  2023,  3722,  2553,  2007,  1037,  7399,  3853,  1010,
         1996,  4338,  1997,  1996,  4304,  3853,  2515,  2025,  2689,  1010,
         2004,  2064,  2022,  2464,  1999, 20965,  1012,  1018,  1012,  2184,
         1012,  2004,  2019,  2742,  1997,  1037,  2553,  2073,  1996,  4338,
         1997,  1996,  9723,  4304,  3853,  3431,  1010,  5136,  1996,  3853,
         1053,  1027, 17710, 29727,  2102,  1012,  1053,  1010,  1996,  2193,
         1997, 10327,  1999,  1037,  4951,  1010,  2064,  3623,  3225,  2013,
         2019,  3988,  2313,  2946,  1047,  2007,  1037,  3930,  3446,  1165,
         1010,  2127,  1996,  4951,  2003, 21764,  2044,  1037,  6721,  2051,
         2558,  1056,  1012,   102])"
317,1,"['variability', 'functions', 'lognormal', 'normal', 'model', 'probability distribution', 'random sequences', 'probability distributions', 'uncertainty', 'probability', 'central limit theorem', 'distributions', 'random', 'limit', 'distribution']", Example Probability Distribution for a Function of Random Variables,seg_101,"lecture 6 (aim of the present lecture) the aim of the present lecture is to first summarize typical probability distribution functions applied in engineering uncertainty modeling. thereafter, it is outlined how the normal and the lognormal probability distributions may be derived on the basis of the central limit theorem. furthermore, as an introduction on how to model uncertain phenomena with random variability over time, random sequences and their characterization are introduced. on the basis of the lecture it is expected that the reader will acquire knowledge and skills with regard to:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  9723,  4353,  2005,  1037,  3853,  1997,  6721, 10857])","tensor([  101,  8835,  1020,  1006,  6614,  1997,  1996,  2556,  8835,  1007,
         1996,  6614,  1997,  1996,  2556,  8835,  2003,  2000,  2034,  7680,
         7849,  4697,  5171,  9723,  4353,  4972,  4162,  1999,  3330, 12503,
        11643,  1012,  6920,  1010,  2009,  2003, 14801,  2129,  1996,  3671,
         1998,  1996,  8833, 12131,  9067,  9723, 20611,  2089,  2022,  5173,
         2006,  1996,  3978,  1997,  1996,  2430,  5787,  9872,  1012,  7297,
         1010,  2004,  2019,  4955,  2006,  2129,  2000,  2944,  9662, 13352,
         2007,  6721, 28436,  2058,  2051,  1010,  6721, 10071,  1998,  2037,
        23191,  2024,  3107,  1012,  2006,  1996,  3978,  1997,  1996,  8835,
         2009,  2003,  3517,  2008,  1996,  8068,  2097,  9878,  3716,  1998,
         4813,  2007,  7634,  2000,  1024,   102])"
318,1,"['lognormal distribution', 'random sequence', 'binomial distribution', 'correlated', 'linear combination', 'geometric distribution', 'bernoulli', 'lognormal', 'random variable', 'normal', 'standardized', 'geometric', 'binomial', 'trial', 'linear', 'random variables', 'central limit theorem', 'combination', 'variance', 'random', 'limit', 'distribution', 'variables', 'bernoulli trial', 'variable']", Example Probability Distribution for a Function of Random Variables,seg_101,• what does the central limit theorem say? • what is a standardized random variable? • how can the variance of a linear combination of correlated normal distributed random variables be calculated? • how can the lognormal distribution be derived? • in what way can uncertain phenomena depend on “time”? • what is a random sequence? • what is a bernoulli trial and what does it describe? • for what can the binomial distribution be used? • what is a geometric distribution and where can it be applied?,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  9723,  4353,  2005,  1037,  3853,  1997,  6721, 10857])","tensor([  101,  1528,  2054,  2515,  1996,  2430,  5787,  9872,  2360,  1029,
         1528,  2054,  2003,  1037, 16367,  6721,  8023,  1029,  1528,  2129,
         2064,  1996, 23284,  1997,  1037,  7399,  5257,  1997, 23900,  3671,
         5500,  6721, 10857,  2022, 10174,  1029,  1528,  2129,  2064,  1996,
         8833, 12131,  9067,  4353,  2022,  5173,  1029,  1528,  1999,  2054,
         2126,  2064,  9662, 13352, 12530,  2006,  1523,  2051,  1524,  1029,
         1528,  2054,  2003,  1037,  6721,  5537,  1029,  1528,  2054,  2003,
         1037, 16595,  7140,  6894,  3979,  1998,  2054,  2515,  2009,  6235,
         1029,  1528,  2005,  2054,  2064,  1996,  8026, 20936,  2389,  4353,
         2022,  2109,  1029,  1528,  2054,  2003,  1037, 14965,  4353,  1998,
         2073,  2064,  2009,  2022,  4162,  1029,   102])"
319,1,"['table', 'functions', 'exponential distribution', 'lognormal', 'standard deviation', 'expectation operator', 'case', 'deviation', 'probability', 'standard', 'expectation', 'parameters', 'distributions', 'moments', 'distribution', 'exponential']", Probability Density and Distribution Functions,seg_103,"in table 4.1 a selection of probability density and cumulative distribution functions is given with the definition of their distribution parameters and moments. with a given expectation operator and standard deviation, the parameters of the distributions can be assessed and the corresponding distributions can be drawn. in case of a non-shifted lognormal or exponential distribution, ε is equal to zero.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([9723, 4304, 1998, 4353, 4972])","tensor([  101,  1999,  2795,  1018,  1012,  1015,  1037,  4989,  1997,  9723,
         4304,  1998, 23260,  4353,  4972,  2003,  2445,  2007,  1996,  6210,
         1997,  2037,  4353, 11709,  1998,  5312,  1012,  2007,  1037,  2445,
        17626,  6872,  1998,  3115, 24353,  1010,  1996, 11709,  1997,  1996,
        20611,  2064,  2022, 14155,  1998,  1996,  7978, 20611,  2064,  2022,
         4567,  1012,  1999,  2553,  1997,  1037,  2512,  1011,  5429,  8833,
        12131,  9067,  2030, 27258,  4353,  1010,  1159,  2003,  5020,  2000,
         5717,  1012,   102])"
320,1,"['risk', 'table', 'functions', 'uncertainties', 'probabilistic modeling', 'probabilistic', 'dependent', 'distribution', 'case']", Probability Density and Distribution Functions,seg_103,"illustrations for the distribution types given in table 4.1 are shown in fig. 4.11. the relevance of the different distribution functions given in table 4.1 in connection with the probabilistic modeling of uncertainties in engineering risk and reliability analysis is strongly case dependent and the reader is suggested to consult the application specific literature for specific guidance. in the following, however,",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([9723, 4304, 1998, 4353, 4972])","tensor([  101, 11249,  2005,  1996,  4353,  4127,  2445,  1999,  2795,  1018,
         1012,  1015,  2024,  3491,  1999, 20965,  1012,  1018,  1012,  2340,
         1012,  1996, 21923,  1997,  1996,  2367,  4353,  4972,  2445,  1999,
         2795,  1018,  1012,  1015,  1999,  4434,  2007,  1996,  4013,  3676,
        27965,  4588, 11643,  1997,  9662,  7368,  1999,  3330,  3891,  1998,
        15258,  4106,  2003,  6118,  2553,  7790,  1998,  1996,  8068,  2003,
         4081,  2000, 23363,  1996,  4646,  3563,  3906,  2005,  3563,  8606,
         1012,  1999,  1996,  2206,  1010,  2174,  1010,   102])"
321,1,"['distributions', 'normal', 'limit', 'central limit theorem']", Probability Density and Distribution Functions,seg_103,a brief introduction to the central limit theorem and the derived normal and lognormal distributions is given.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0.])","tensor([9723, 4304, 1998, 4353, 4972])","tensor([  101,  1037,  4766,  4955,  2000,  1996,  2430,  5787,  9872,  1998,
         1996,  5173,  3671,  1998,  8833, 12131,  9067, 20611,  2003,  2445,
         1012,   102])"
322,1,"['central limit theorem', 'states', 'limit']", The Central Limit Theorem and Derived Distributions,seg_105,the central limit theorem states:,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0.])","tensor([ 1996,  2430,  5787,  9872,  1998,  5173, 20611])","tensor([ 101, 1996, 2430, 5787, 9872, 2163, 1024,  102])"
323,1,"['normal distribution', 'probability', 'random', 'normal', 'random variables', 'distribution', 'variables', 'probability distribution']", The Central Limit Theorem and Derived Distributions,seg_105,the probability distribution for the sum of a number of random variables approaches the normal distribution as the number becomes large.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 1996,  2430,  5787,  9872,  1998,  5173, 20611])","tensor([  101,  1996,  9723,  4353,  2005,  1996,  7680,  1997,  1037,  2193,
         1997,  6721, 10857,  8107,  1996,  3671,  4353,  2004,  1996,  2193,
         4150,  2312,  1012,   102])"
324,1,"['probability', 'results']", The Central Limit Theorem and Derived Distributions,seg_105,"this result, which indeed is one of the most important results in probability theory, will not be derived here but instead the general conditions for the validity of the theorem will be outlined.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 1996,  2430,  5787,  9872,  1998,  5173, 20611])","tensor([  101,  2023,  2765,  1010,  2029,  5262,  2003,  2028,  1997,  1996,
         2087,  2590,  3463,  1999,  9723,  3399,  1010,  2097,  2025,  2022,
         5173,  2182,  2021,  2612,  1996,  2236,  3785,  2005,  1996, 16406,
         1997,  1996,  9872,  2097,  2022, 14801,  1012,   102])"
325,1,"['skewed', 'distributions', 'random', 'random variables', 'variables', 'independent']", The Central Limit Theorem and Derived Distributions,seg_105,"in principle, the theorem is valid as long as the number of independent contributions to the sum is “large”. this implies that the sum may not be dominated by one or just a few random variables and furthermore, that the dependency between the random variables in the sum is not too strong. there is no requirement on the type of distributions of the random variables entering the sum, but if the distributions are skewed the number of variables in the sum which is required for the validity of the theorem increases.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  2430,  5787,  9872,  1998,  5173, 20611])","tensor([  101,  1999,  6958,  1010,  1996,  9872,  2003,  9398,  2004,  2146,
         2004,  1996,  2193,  1997,  2981,  5857,  2000,  1996,  7680,  2003,
         1523,  2312,  1524,  1012,  2023, 12748,  2008,  1996,  7680,  2089,
         2025,  2022,  6817,  2011,  2028,  2030,  2074,  1037,  2261,  6721,
        10857,  1998,  7297,  1010,  2008,  1996, 24394,  2090,  1996,  6721,
        10857,  1999,  1996,  7680,  2003,  2025,  2205,  2844,  1012,  2045,
         2003,  2053,  9095,  2006,  1996,  2828,  1997, 20611,  1997,  1996,
         6721, 10857,  5738,  1996,  7680,  1010,  2021,  2065,  1996, 20611,
         2024, 15315,  7974,  2098,  1996,  2193,  1997, 10857,  1999,  1996,
         7680,  2029,  2003,  3223,  2005,  1996, 16406,  1997,  1996,  9872,
         7457,  1012,   102])"
326,1,"['range', 'uncertainty', 'histograms', 'uniform distribution', 'measuring', 'errors', 'uniformly distributed', 'measurement', 'distribution', 'measurements', 'error', 'case']", Example Central Limit Theorem,seg_107,"for the purpose of illustration, consider the problem of assessing the accumulated error in repeated measurements. the length of a structural member is being measured using a ruler of length 2 m with the smallest measuring unit equal to 1 mm. it is assumed that all measurements are rounded off to the closest unit on the ruler and thus it is assumed that each measurement is subject to a measurement uncertainty which is uniformly distributed in the range ±0.5 mm. if the length of a considered structural member is smaller or equal to 2 m, the length can be measured by one measurement. it is clear that in this case the measurement uncertainty follows the uniform distribution as outlined in the above. however, if the member length is between 2 m and 4 m two measurements are required, if the member length is between 4 m and 8 m three measurements are required and so on. in fig. 4.12, the histograms of the corresponding resulting measurement errors are illustrated under the assumption that consecutive errors are independent.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.])","tensor([2742, 2430, 5787, 9872])","tensor([  101,  2005,  1996,  3800,  1997, 14614,  1010,  5136,  1996,  3291,
         1997, 20077,  1996, 14830,  7561,  1999,  5567, 11702,  1012,  1996,
         3091,  1997,  1037,  8332,  2266,  2003,  2108,  7594,  2478,  1037,
         7786,  1997,  3091,  1016,  1049,  2007,  1996, 10479,  9854,  3131,
         5020,  2000,  1015,  3461,  1012,  2009,  2003,  5071,  2008,  2035,
        11702,  2024,  8352,  2125,  2000,  1996,  7541,  3131,  2006,  1996,
         7786,  1998,  2947,  2009,  2003,  5071,  2008,  2169, 10903,  2003,
         3395,  2000,  1037, 10903, 12503,  2029,  2003, 27423,  5500,  1999,
         1996,  2846,  1081,  2692,  1012,  1019,  3461,  1012,  2065,  1996,
         3091,  1997,  1037,  2641,  8332,  2266,  2003,  3760,  2030,  5020,
         2000,  1016,  1049,  1010,  1996,  3091,  2064,  2022,  7594,  2011,
         2028, 10903,  1012,  2009,  2003,  3154,  2008,  1999,  2023,  2553,
         1996, 10903, 12503,  4076,  1996,  6375,  4353,  2004, 14801,  1999,
         1996,  2682,  1012,  2174,  1010,  2065,  1996,  2266,  3091,  2003,
         2090,  1016,  1049,  1998,  1018,  1049,  2048, 11702,  2024,  3223,
         1010,  2065,  1996,  2266,  3091,  2003,  2090,  1018,  1049,  1998,
         1022,  1049,  2093, 11702,  2024,  3223,  1998,  2061,  2006,  1012,
         1999, 20965,  1012,  1018,  1012,  2260,  1010,  1996,  2010,  3406,
        13113,  2015,  1997,  1996,  7978,  4525, 10903, 10697,  2024,  7203,
         2104,  1996, 11213,  2008,  5486, 10697,  2024,  2981,  1012,   102])"
327,1,"['density function', 'functions', 'errors', 'normal distribution', 'probability density function', 'histogram', 'normal', 'function', 'probability', 'random variables', 'probability density functions', 'sample', 'random', 'density functions', 'measurement', 'distribution', 'measurements', 'variables']", Example Central Limit Theorem,seg_107,"from fig. 4.12, it is seen that whereas the sample histogram for one measurement is clearly uniform, the histogram approaches a bell shape already for four repeated measurements and for most practical purposes may be considered to be normal distributed already for eight repeated measurements. the analytical form of the probability density function for the accumulated errors may be derived by repeated use of the result concerning the probability density function for the sum of random variables given in eq. 4.34. in benjamin and cornell [4], it is heuristically shown that the analytical probability density functions has the form of a normal distribution.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0.])","tensor([2742, 2430, 5787, 9872])","tensor([  101,  2013, 20965,  1012,  1018,  1012,  2260,  1010,  2009,  2003,
         2464,  2008,  6168,  1996,  7099,  2010,  3406, 13113,  2005,  2028,
        10903,  2003,  4415,  6375,  1010,  1996,  2010,  3406, 13113,  8107,
         1037,  4330,  4338,  2525,  2005,  2176,  5567, 11702,  1998,  2005,
         2087,  6742,  5682,  2089,  2022,  2641,  2000,  2022,  3671,  5500,
         2525,  2005,  2809,  5567, 11702,  1012,  1996, 17826,  2433,  1997,
         1996,  9723,  4304,  3853,  2005,  1996, 14830, 10697,  2089,  2022,
         5173,  2011,  5567,  2224,  1997,  1996,  2765,  7175,  1996,  9723,
         4304,  3853,  2005,  1996,  7680,  1997,  6721, 10857,  2445,  1999,
         1041,  4160,  1012,  1018,  1012,  4090,  1012,  1999,  6425,  1998,
        10921,  1031,  1018,  1033,  1010,  2009,  2003,  2002,  9496, 10074,
         3973,  3491,  2008,  1996, 17826,  9723,  4304,  4972,  2038,  1996,
         2433,  1997,  1037,  3671,  4353,  1012,   102])"
328,1,"['joint probability', 'function', 'density function', 'probability', 'probability density function', 'information', 'joint probability density function', 'limit', 'distribution', 'central limit theorem', 'joint']", The Normal Distribution,seg_109,the significant practical importance of the central limit theorem lies in the fact that even though only weak information is available regarding the number of contributions and their joint probability density function rather strong information is achieved for the distribution of sum of the contributions.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([1996, 3671, 4353])","tensor([ 101, 1996, 3278, 6742, 5197, 1997, 1996, 2430, 5787, 9872, 3658, 1999,
        1996, 2755, 2008, 2130, 2295, 2069, 5410, 2592, 2003, 2800, 4953, 1996,
        2193, 1997, 5857, 1998, 2037, 4101, 9723, 4304, 3853, 2738, 2844, 2592,
        2003, 4719, 2005, 1996, 4353, 1997, 7680, 1997, 1996, 5857, 1012,  102])"
329,1,"['distribution', 'probability', 'normal probability distribution', 'probabilistic', 'normal', 'independent', 'probabilistic modeling', 'probability distribution']", The Normal Distribution,seg_109,the normal probability distribution is thus applied very frequently in practical problems for the probabilistic modeling of uncertain phenomena which may be considered to originate from a cumulative effect of several independent uncertain contributions.,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0.])","tensor([1996, 3671, 4353])","tensor([  101,  1996,  3671,  9723,  4353,  2003,  2947,  4162,  2200,  4703,
         1999,  6742,  3471,  2005,  1996,  4013,  3676, 27965,  4588, 11643,
         1997,  9662, 13352,  2029,  2089,  2022,  2641,  2000, 21754,  2013,
         1037, 23260,  3466,  1997,  2195,  2981,  9662,  5857,  1012,   102])"
330,1,"['linear combination', 'normal distribution', 'linear', 'random', 'normal', 'random variables', 'distribution', 'variables', 'combination']", The Normal Distribution,seg_109,"the normal distribution has the property that the linear combination s of n normal distributed random variables xi , i = 1,2, . . . , n:",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1996, 3671, 4353])","tensor([  101,  1996,  3671,  4353,  2038,  1996,  3200,  2008,  1996,  7399,
         5257,  1055,  1997,  1050,  3671,  5500,  6721, 10857,  8418,  1010,
         1045,  1027,  1015,  1010,  1016,  1010,  1012,  1012,  1012,  1010,
         1050,  1024,   102])"
331,1,"['normal', 'distribution']", The Normal Distribution,seg_109,is also normal distributed. the distribution is said to be closed in respect to summation.,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([1996, 3671, 4353])","tensor([  101,  2003,  2036,  3671,  5500,  1012,  1996,  4353,  2003,  2056,
         2000,  2022,  2701,  1999,  4847,  2000,  7680, 28649,  1012,   102])"
332,1,"['function', 'density function', 'probability', 'interval', 'random variable', 'random', 'normal', 'distribution', 'distribution function', 'variable']", The Normal Distribution,seg_109,figure 4.13 shows the probability density and distribution function of a normal distributed random variable. the area between the interval of μ−σ ≤ x ≤ μ+σ for the density function corresponds to 68.3% of the total area of the density function which always has the value 1.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([1996, 3671, 4353])","tensor([  101,  3275,  1018,  1012,  2410,  3065,  1996,  9723,  4304,  1998,
         4353,  3853,  1997,  1037,  3671,  5500,  6721,  8023,  1012,  1996,
         2181,  2090,  1996, 13483,  1997,  1166, 22543, 29733,  1608,  1060,
         1608,  1166,  1009,  1173,  2005,  1996,  4304,  3853, 14788,  2000,
         6273,  1012,  1017,  1003,  1997,  1996,  2561,  2181,  1997,  1996,
         4304,  3853,  2029,  2467,  2038,  1996,  3643,  1015,  1012,   102])"
333,1,"['standardized', 'normal distribution', 'variance', 'standard normal distribution', 'standard normal', 'random variable', 'transformed', 'random', 'normal', 'standard', 'distribution', 'expected value', 'variable']", The Normal Distribution,seg_109,"one special version of the normal distribution should be mentioned, namely the standard normal distribution. in general a standardized (sometimes referred to as a reduced) random variable is a random variable which has been transformed such that it has an expected value e[x] = 0 and a variance v [x] = 1, i.e. the random",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])","tensor([1996, 3671, 4353])","tensor([  101,  2028,  2569,  2544,  1997,  1996,  3671,  4353,  2323,  2022,
         3855,  1010,  8419,  1996,  3115,  3671,  4353,  1012,  1999,  2236,
         1037, 16367,  1006,  2823,  3615,  2000,  2004,  1037,  4359,  1007,
         6721,  8023,  2003,  1037,  6721,  8023,  2029,  2038,  2042,  8590,
         2107,  2008,  2009,  2038,  2019,  3517,  3643,  1041,  1031,  1060,
         1033,  1027,  1014,  1998,  1037, 23284,  1058,  1031,  1060,  1033,
         1027,  1015,  1010,  1045,  1012,  1041,  1012,  1996,  6721,   102])"
334,0,[], The Normal Distribution,seg_109,variable y defined by:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0])","tensor([1996, 3671, 4353])","tensor([ 101, 8023, 1061, 4225, 2011, 1024,  102])"
335,1,"['standardized', 'normal distribution', 'standardization', 'standard normal', 'random variable', 'random', 'normal', 'standard', 'distribution', 'process', 'variable']", The Normal Distribution,seg_109,is a standardized random variable. if the random variable x follows the normal distribution the random variable y is standard normal distributed. in fig. 4.14 the process of standardization is illustrated.,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0.])","tensor([1996, 3671, 4353])","tensor([  101,  2003,  1037, 16367,  6721,  8023,  1012,  2065,  1996,  6721,
         8023,  1060,  4076,  1996,  3671,  4353,  1996,  6721,  8023,  1061,
         2003,  3115,  3671,  5500,  1012,  1999, 20965,  1012,  1018,  1012,
         2403,  1996,  2832,  1997, 28648,  2003,  7203,  1012,   102])"
336,1,"['function', 'normal distribution', 'density function', 'cumulative distribution function', 'functions', 'normal', 'distribution', 'distribution function']", The Normal Distribution,seg_109,it is common practice to denote the cumulative distribution function for the standard normal distribution by φ(x) and the corresponding density function by ϕ(x). these functions are broadly available in software packages such as ms excel and matlab.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1996, 3671, 4353])","tensor([  101,  2009,  2003,  2691,  3218,  2000, 19090,  1996, 23260,  4353,
         3853,  2005,  1996,  3115,  3671,  4353,  2011,  1176,  1006,  1060,
         1007,  1998,  1996,  7978,  4304,  3853,  2011,   100,  1006,  1060,
         1007,  1012,  2122,  4972,  2024, 13644,  2800,  1999,  4007, 14555,
         2107,  2004,  5796, 24970,  1998, 13523, 20470,  1012,   102])"
337,1,"['probability', 'lognormal', 'random variable', 'random', 'normal', 'distribution', 'lognormal distributed', 'variable', 'probability distribution']", The Lognormal Distribution,seg_111,"a random variable y is said to be lognormal distributed if the variable z = ln(y ) is normal distributed. it thus follows that if an uncertain phenomenon can be assumed to originate from a multiplicative effect of several uncertain contributions, then the probability distribution for the phenomenon can be assumed to be lognormal distributed.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.])","tensor([ 1996,  8833, 12131,  9067,  4353])","tensor([  101,  1037,  6721,  8023,  1061,  2003,  2056,  2000,  2022,  8833,
        12131,  9067,  5500,  2065,  1996,  8023,  1062,  1027,  1048,  2078,
         1006,  1061,  1007,  2003,  3671,  5500,  1012,  2009,  2947,  4076,
         2008,  2065,  2019,  9662,  9575,  2064,  2022,  5071,  2000, 21754,
         2013,  1037,  4800, 24759, 25184,  3466,  1997,  2195,  9662,  5857,
         1010,  2059,  1996,  9723,  4353,  2005,  1996,  9575,  2064,  2022,
         5071,  2000,  2022,  8833, 12131,  9067,  5500,  1012,   102])"
338,1,"['lognormal', 'lognormal distribution', 'distribution']", The Lognormal Distribution,seg_111,the lognormal distribution has the property that if:,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  8833, 12131,  9067,  4353])","tensor([  101,  1996,  8833, 12131,  9067,  4353,  2038,  1996,  3200,  2008,
         2065,  1024,   102])"
339,1,"['table', 'lognormal', 'parameters', 'random', 'random variables', 'variables', 'independent']", The Lognormal Distribution,seg_111,"and all yi are independent lognormal random variables with parameters λi , ζi and εi = 0 as given in table 4.2 then also p is lognormal with parameters:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 0., 0.])","tensor([ 1996,  8833, 12131,  9067,  4353])","tensor([  101,  1998,  2035, 12316,  2024,  2981,  8833, 12131,  9067,  6721,
        10857,  2007, 11709,  1165,  2072,  1010,  1160,  2072,  1998,  1159,
         2072,  1027,  1014,  2004,  2445,  1999,  2795,  1018,  1012,  1016,
         2059,  2036,  1052,  2003,  8833, 12131,  9067,  2007, 11709,  1024,
          102])"
340,1,"['continuous', 'events', 'random process', 'trials', 'random', 'levels', 'time variant', 'process', 'random sequence', 'stochastic process', 'discrete']", Stochastic Processes and Extremes,seg_113,"random quantities may be “time variant” in the sense that they take on new realizations at new trials or at new times. if the new realizations of the time variant random quantity occur at discrete times and take on discrete realizations, the random quantity is usually denoted a random sequence. well known examples are series of throws of dice; more engineering relevant examples are flooding events. if the realizations of the time variant quantity occur continuously in time and take on continuous realizations the random quantity is usually denoted a random process or stochastic process. examples include wind velocity, wave heights, snowfall and water levels.",tensor(1),"tensor([0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 2358, 11663, 20875,  6194,  1998, 28800])","tensor([  101,  6721, 12450,  2089,  2022,  1523,  2051,  8349,  1524,  1999,
         1996,  3168,  2008,  2027,  2202,  2006,  2047, 12393,  2015,  2012,
         2047,  7012,  2030,  2012,  2047,  2335,  1012,  2065,  1996,  2047,
        12393,  2015,  1997,  1996,  2051,  8349,  6721, 11712,  5258,  2012,
        16246,  2335,  1998,  2202,  2006, 16246, 12393,  2015,  1010,  1996,
         6721, 11712,  2003,  2788, 19537,  1037,  6721,  5537,  1012,  2092,
         2124,  4973,  2024,  2186,  1997, 11618,  1997, 18740,  1025,  2062,
         3330,  7882,  4973,  2024,  9451,  2824,  1012,  2065,  1996, 12393,
         2015,  1997,  1996,  2051,  8349, 11712,  5258, 10843,  1999,  2051,
         1998,  2202,  2006,  7142, 12393,  2015,  1996,  6721, 11712,  2003,
         2788, 19537,  1037,  6721,  2832,  2030,  2358, 11663, 20875,  2832,
         1012,  4973,  2421,  3612, 10146,  1010,  4400,  7535,  1010, 26043,
         1998,  2300,  3798,  1012,   102])"
341,1,"['cases', 'random sequence', 'binomial distribution', 'bernoulli', 'poisson', 'normal', 'bernoulli trials', 'intensity', 'model', 'binomial', 'continuous', 'random sequences', 'random processes', 'counting process', 'trials', 'random variables', 'process', 'gaussian processes', 'processes', 'poisson counting process', 'random process', 'random', 'distribution', 'variables']", Stochastic Processes and Extremes,seg_113,"in some cases random sequences and random processes may be represented in a given problem context in terms of random variables e.g. for the modeling of the “point in time” value of the intensity of the wind velocity, or the maximum (extreme) wind velocity during one year. however, in many cases this is not possible and then it is necessary to model the uncertain phenomena by a random process. in the following, first an important type of random sequence will be introduced, namely the sequence of bernoulli trials from which the binomial distribution has been derived. thereafter, a description of the poisson counting process is given and finally the continuous normal or gaussian processes are described. it should be noted that numerous other types of random processes have been suggested in the literature of which most have been derived from the mentioned.",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 2358, 11663, 20875,  6194,  1998, 28800])","tensor([  101,  1999,  2070,  3572,  6721, 10071,  1998,  6721,  6194,  2089,
         2022,  3421,  1999,  1037,  2445,  3291,  6123,  1999,  3408,  1997,
         6721, 10857,  1041,  1012,  1043,  1012,  2005,  1996, 11643,  1997,
         1996,  1523,  2391,  1999,  2051,  1524,  3643,  1997,  1996,  8015,
         1997,  1996,  3612, 10146,  1010,  2030,  1996,  4555,  1006,  6034,
         1007,  3612, 10146,  2076,  2028,  2095,  1012,  2174,  1010,  1999,
         2116,  3572,  2023,  2003,  2025,  2825,  1998,  2059,  2009,  2003,
         4072,  2000,  2944,  1996,  9662, 13352,  2011,  1037,  6721,  2832,
         1012,  1999,  1996,  2206,  1010,  2034,  2019,  2590,  2828,  1997,
         6721,  5537,  2097,  2022,  3107,  1010,  8419,  1996,  5537,  1997,
        16595,  7140,  6894,  7012,  2013,  2029,  1996,  8026, 20936,  2389,
         4353,  2038,  2042,  5173,  1012,  6920,  1010,  1037,  6412,  1997,
         1996, 13433, 24077, 10320,  2832,  2003,  2445,  1998,  2633,  1996,
         7142,  3671,  2030, 11721, 17854,  2937,  6194,  2024,  2649,  1012,
         2009,  2323,  2022,  3264,  2008,  3365,  2060,  4127,  1997,  6721,
         6194,  2031,  2042,  4081,  1999,  1996,  3906,  1997,  2029,  2087,
         2031,  2042,  5173,  2013,  1996,  3855,  1012,   102])"
342,1,"['outcomes', 'experiments', 'successes', 'binomial distribution', 'bernoulli', 'events', 'failure', 'bernoulli trials', 'mutually exclusive', 'binomial', 'trial', 'probability', 'probability of success', 'trials', 'success', 'distribution', 'bernoulli trial']", Random SequencesBernoulli Trials,seg_115,"a sequence of experiments with only two possible mutually exclusive outcomes is called a sequence of bernoulli trials. typically the two possible events of a bernoulli trial are referred to as a success or a failure. if it is assumed that the probability of success of a bernoulli trial is constant equal to p then the probability density of y successes in n trials py (y) i.e. the binomial distribution (or sometimes denoted b(n,p)) can be shown to be equal to:",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10071,  5677,  3630, 18083,  2072,  7012])","tensor([  101,  1037,  5537,  1997,  7885,  2007,  2069,  2048,  2825, 20271,
         7262, 13105,  2003,  2170,  1037,  5537,  1997, 16595,  7140,  6894,
         7012,  1012,  4050,  1996,  2048,  2825,  2824,  1997,  1037, 16595,
         7140,  6894,  3979,  2024,  3615,  2000,  2004,  1037,  3112,  2030,
         1037,  4945,  1012,  2065,  2009,  2003,  5071,  2008,  1996,  9723,
         1997,  3112,  1997,  1037, 16595,  7140,  6894,  3979,  2003,  5377,
         5020,  2000,  1052,  2059,  1996,  9723,  4304,  1997,  1061, 14152,
         1999,  1050,  7012,  1052,  2100,  1006,  1061,  1007,  1045,  1012,
         1041,  1012,  1996,  8026, 20936,  2389,  4353,  1006,  2030,  2823,
        19537,  1038,  1006,  1050,  1010,  1052,  1007,  1007,  2064,  2022,
         3491,  2000,  2022,  5020,  2000,  1024,   102])"
343,1,"['binomial', 'binomial operator']", Random SequencesBernoulli Trials,seg_115,n) is the binomial operator defined as:,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.])","tensor([ 6721, 10071,  5677,  3630, 18083,  2072,  7012])","tensor([  101,  1050,  1007,  2003,  1996,  8026, 20936,  2389,  6872,  4225,
         2004,  1024,   102])"
344,1,"['function', 'cumulative distribution function', 'distribution', 'distribution function']", Random SequencesBernoulli Trials,seg_115,the cumulative distribution function for y is thus given as:,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10071,  5677,  3630, 18083,  2072,  7012])","tensor([  101,  1996, 23260,  4353,  3853,  2005,  1061,  2003,  2947,  2445,
         2004,  1024,   102])"
345,1,"['expected value', 'variance', 'distribution', 'binomial', 'binomial distribution']", Random SequencesBernoulli Trials,seg_115,"in fig. 4.15 some examples of the binomial distribution are shown for n = 5. the expected value and the variance of y , i.e. e[y ]and var[y ] can be shown to be given as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10071,  5677,  3630, 18083,  2072,  7012])","tensor([  101,  1999, 20965,  1012,  1018,  1012,  2321,  2070,  4973,  1997,
         1996,  8026, 20936,  2389,  4353,  2024,  3491,  2005,  1050,  1027,
         1019,  1012,  1996,  3517,  3643,  1998,  1996, 23284,  1997,  1061,
         1010,  1045,  1012,  1041,  1012,  1041,  1031,  1061,  1033,  1998,
        13075,  1031,  1061,  1033,  2064,  2022,  3491,  2000,  2022,  2445,
         2004,  1024,   102])"
346,1,"['geometric', 'geometric distribution', 'event', 'trials', 'random number', 'random', 'distribution', 'statistical', 'success', 'independent']", Random SequencesBernoulli Trials,seg_115,"it is often of significant interest to assess the statistical characteristics of the random “time” or random number of trials n until the first success occurs. the probability density of this event, provided that the trials are independent, is given by the so-called geometric distribution:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0.])","tensor([ 6721, 10071,  5677,  3630, 18083,  2072,  7012])","tensor([  101,  2009,  2003,  2411,  1997,  3278,  3037,  2000, 14358,  1996,
         7778,  6459,  1997,  1996,  6721,  1523,  2051,  1524,  2030,  6721,
         2193,  1997,  7012,  1050,  2127,  1996,  2034,  3112,  5158,  1012,
         1996,  9723,  4304,  1997,  2023,  2724,  1010,  3024,  2008,  1996,
         7012,  2024,  2981,  1010,  2003,  2445,  2011,  1996,  2061,  1011,
         2170, 14965,  4353,  1024,   102])"
347,1,"['function', 'cumulative distribution function', 'distribution', 'distribution function']", Random SequencesBernoulli Trials,seg_115,and the corresponding cumulative distribution function by:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0.])","tensor([ 6721, 10071,  5677,  3630, 18083,  2072,  7012])","tensor([  101,  1998,  1996,  7978, 23260,  4353,  3853,  2011,  1024,   102])"
348,1,"['mean', 'geometric distribution', 'variance', 'distribution', 'geometric']", Random SequencesBernoulli Trials,seg_115,the mean value and the variance of the geometric distribution are given by:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 6721, 10071,  5677,  3630, 18083,  2072,  7012])","tensor([  101,  1996,  2812,  3643,  1998,  1996, 23284,  1997,  1996, 14965,
         4353,  2024,  2445,  2011,  1024,   102])"
349,1,"['average', 'success']", Random SequencesBernoulli Trials,seg_115,especially the result given in eq. 4.60 is of practical value as it gives the average “time” until success.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 6721, 10071,  5677,  3630, 18083,  2072,  7012])","tensor([ 101, 2926, 1996, 2765, 2445, 1999, 1041, 4160, 1012, 1018, 1012, 3438,
        2003, 1997, 6742, 3643, 2004, 2009, 3957, 1996, 2779, 1523, 2051, 1524,
        2127, 3112, 1012,  102])"
350,1,"['estimation', 'probability', 'return period', 'control', 'tests', 'measurements', 'average']", Example Quality Control of Concrete,seg_117,during the control procedures for the concrete quality on a building site not all of the tested specimens fulfilled the requirements to be used for structural purposes. the “rate of success” is the probability p that one specimen does not fulfil the requirements. according to the recent measurements p = 0.2. if we want to know after how many tests one specimen may fail the requirements we could assess the average return period which corresponds to the estimation operator described in eq. 4.60.,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 3737, 2491, 1997, 5509])","tensor([  101,  2076,  1996,  2491,  8853,  2005,  1996,  5509,  3737,  2006,
         1037,  2311,  2609,  2025,  2035,  1997,  1996,  7718,  9908, 16829,
         1996,  5918,  2000,  2022,  2109,  2005,  8332,  5682,  1012,  1996,
         1523,  3446,  1997,  3112,  1524,  2003,  1996,  9723,  1052,  2008,
         2028, 11375,  2515,  2025, 11865, 10270,  4014,  1996,  5918,  1012,
         2429,  2000,  1996,  3522, 11702,  1052,  1027,  1014,  1012,  1016,
         1012,  2065,  2057,  2215,  2000,  2113,  2044,  2129,  2116,  5852,
         2028, 11375,  2089,  8246,  1996,  5918,  2057,  2071, 14358,  1996,
         2779,  2709,  2558,  2029, 14788,  2000,  1996, 24155,  6872,  2649,
         1999,  1041,  4160,  1012,  1018,  1012,  3438,  1012,   102])"
351,1,"['continuous', 'events', 'random', 'model', 'discrete']", Example Quality Control of Concrete,seg_117,"lecture 7 (aim of the present lecture) the aim of the present lecture is to provide an understanding on how to model and probabilistically describe events that occur at discrete time points. in addition, continuous random processes are introduced and their main characteristics are provided. finally, extreme events and their modeling are introduced and the concept of return period is explained. on the basis of the lecture it is expected that the reader will acquire knowledge and skills with regard to:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([2742, 3737, 2491, 1997, 5509])","tensor([  101,  8835,  1021,  1006,  6614,  1997,  1996,  2556,  8835,  1007,
         1996,  6614,  1997,  1996,  2556,  8835,  2003,  2000,  3073,  2019,
         4824,  2006,  2129,  2000,  2944,  1998,  4013,  3676, 27965, 25084,
         6235,  2824,  2008,  5258,  2012, 16246,  2051,  2685,  1012,  1999,
         2804,  1010,  7142,  6721,  6194,  2024,  3107,  1998,  2037,  2364,
         6459,  2024,  3024,  1012,  2633,  1010,  6034,  2824,  1998,  2037,
        11643,  2024,  3107,  1998,  1996,  4145,  1997,  2709,  2558,  2003,
         4541,  1012,  2006,  1996,  3978,  1997,  1996,  8835,  2009,  2003,
         3517,  2008,  1996,  8068,  2097,  9878,  3716,  1998,  4813,  2007,
         7634,  2000,  1024,   102])"
352,1,"['poisson', 'poisson process', 'process']", Example Quality Control of Concrete,seg_117,• what is a simple poisson process and where can it be applied? • what are the properties which must be fulfilled before we can assume a poisson process?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([2742, 3737, 2491, 1997, 5509])","tensor([  101,  1528,  2054,  2003,  1037,  3722, 13433, 24077,  2832,  1998,
         2073,  2064,  2009,  2022,  4162,  1029,  1528,  2054,  2024,  1996,
         5144,  2029,  2442,  2022, 16829,  2077,  2057,  2064,  7868,  1037,
        13433, 24077,  2832,  1029,   102])"
353,1,"['homogeneity', 'independent', 'mean', 'stationarity', 'exponentially distributed', 'poisson', 'normal', 'poisson process', 'model', 'extreme value', 'continuous', 'process', 'exponentially', 'extreme value distributions', 'random process', 'distributions', 'random', 'distribution', 'variables']", Example Quality Control of Concrete,seg_117,• what does homogeneity refer to for a poisson process? • according to what distribution can the time between realizations of a poisson process be modeled? • which distribution does the sum of independent exponentially distributed variables follow and for what can this distribution be applied? • what is a continuous random process? • how can a normal process be characterized? • what does stationarity mean and how is it defined? • what is an extreme value and what is required to model it probabilistically? • which are the different types of extreme value distributions? • how are extreme value models and return periods related?,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([2742, 3737, 2491, 1997, 5509])","tensor([  101,  1528,  2054,  2515, 24004,  6914, 20175,  2100,  6523,  2000,
         2005,  1037, 13433, 24077,  2832,  1029,  1528,  2429,  2000,  2054,
         4353,  2064,  1996,  2051,  2090, 12393,  2015,  1997,  1037, 13433,
        24077,  2832,  2022, 14440,  1029,  1528,  2029,  4353,  2515,  1996,
         7680,  1997,  2981, 27258,  2135,  5500, 10857,  3582,  1998,  2005,
         2054,  2064,  2023,  4353,  2022,  4162,  1029,  1528,  2054,  2003,
         1037,  7142,  6721,  2832,  1029,  1528,  2129,  2064,  1037,  3671,
         2832,  2022,  7356,  1029,  1528,  2054,  2515,  2276,  8486,  3723,
         2812,  1998,  2129,  2003,  2009,  4225,  1029,  1528,  2054,  2003,
         2019,  6034,  3643,  1998,  2054,  2003,  3223,  2000,  2944,  2009,
         4013,  3676, 27965, 25084,  1029,  1528,  2029,  2024,  1996,  2367,
         4127,  1997,  6034,  3643, 20611,  1029,  1528,  2129,  2024,  6034,
         3643,  4275,  1998,  2709,  6993,  3141,  1029,   102])"
354,1,"['interval', 'processes', 'poisson', 'poisson process', 'process', 'discrete']", The Poisson Counting Process,seg_119,"the most commonly applied family of discrete processes in structural reliability are the poisson processes. due to the fact that poisson processes have found applications in many different types of engineering problems, a large number of different variants of poisson processes has evolved. in general, the process n(t) denoting the number of points in the interval [0; t[is called a simple poisson process if it satisfies the following conditions:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996, 13433, 24077, 10320,  2832])","tensor([  101,  1996,  2087,  4141,  4162,  2155,  1997, 16246,  6194,  1999,
         8332, 15258,  2024,  1996, 13433, 24077,  6194,  1012,  2349,  2000,
         1996,  2755,  2008, 13433, 24077,  6194,  2031,  2179,  5097,  1999,
         2116,  2367,  4127,  1997,  3330,  3471,  1010,  1037,  2312,  2193,
         1997,  2367, 10176,  1997, 13433, 24077,  6194,  2038,  7964,  1012,
         1999,  2236,  1010,  1996,  2832,  1050,  1006,  1056,  1007,  7939,
        20656,  1996,  2193,  1997,  2685,  1999,  1996, 13483,  1031,  1014,
         1025,  1056,  1031,  2003,  2170,  1037,  3722, 13433, 24077,  2832,
         2065,  2009,  2938,  2483, 14213,  1996,  2206,  3785,  1024,   102])"
355,1,"['function', 'mutually independent', 'probability', 'disjoint', 'events', 'interval', 'event', 'intervals', 'independent']", The Poisson Counting Process,seg_119,• the probability of one event in the interval [t; t + δt[is asymptotically proportional to the length of the interval δt . • the probability of more than one event in the interval [t; t + δt[ is a function of a higher order term of δt for δt → 0. • events in disjoint intervals are mutually independent.,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0.])","tensor([ 1996, 13433, 24077, 10320,  2832])","tensor([  101,  1528,  1996,  9723,  1997,  2028,  2724,  1999,  1996, 13483,
         1031,  1056,  1025,  1056,  1009,  1158,  2102,  1031,  2003,  2004,
        24335, 13876, 20214,  3973, 14267,  2000,  1996,  3091,  1997,  1996,
        13483,  1158,  2102,  1012,  1528,  1996,  9723,  1997,  2062,  2084,
         2028,  2724,  1999,  1996, 13483,  1031,  1056,  1025,  1056,  1009,
         1158,  2102,  1031,  2003,  1037,  3853,  1997,  1037,  3020,  2344,
         2744,  1997,  1158,  2102,  2005,  1158,  2102,  1585,  1014,  1012,
         1528,  2824,  1999,  4487,  2015,  5558, 18447, 14025,  2024, 20271,
         2981,  1012,   102])"
356,1,"['poisson', 'intensity', 'process', 'poisson process']", The Poisson Counting Process,seg_119,the poisson process may be defined completely by its intensity ν(t):,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 1996, 13433, 24077, 10320,  2832])","tensor([  101,  1996, 13433, 24077,  2832,  2089,  2022,  4225,  3294,  2011,
         2049,  8015,  1167,  1006,  1056,  1007,  1024,   102])"
357,1,['event'], The Poisson Counting Process,seg_119,1 ν(t) = lim p (one event in [t; t + δt[ ) (4.62) δt→0 δt,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996, 13433, 24077, 10320,  2832])","tensor([  101,  1015,  1167,  1006,  1056,  1007,  1027, 18525,  1052,  1006,
         2028,  2724,  1999,  1031,  1056,  1025,  1056,  1009,  1158,  2102,
         1031,  1007,  1006,  1018,  1012,  5786,  1007,  1158,  2102, 30113,
         2692,  1158,  2102,   102])"
358,1,"['poisson', 'homogeneous', 'poisson process', 'process']", The Poisson Counting Process,seg_119,"if ν(t) is constant in time the poisson process is said to be homogeneous, otherwise it is non-homogeneous.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 1996, 13433, 24077, 10320,  2832])","tensor([  101,  2065,  1167,  1006,  1056,  1007,  2003,  5377,  1999,  2051,
         1996, 13433, 24077,  2832,  2003,  2056,  2000,  2022, 24854,  1010,
         4728,  2009,  2003,  2512,  1011, 24854,  1012,   102])"
359,1,"['probability', 'interval', 'events', 'poisson', 'intensity', 'process', 'poisson process']", The Poisson Counting Process,seg_119,"in general, the probability of n events in the interval [0; t[ of a poisson process with intensity ν(t) can be given as:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996, 13433, 24077, 10320,  2832])","tensor([  101,  1999,  2236,  1010,  1996,  9723,  1997,  1050,  2824,  1999,
         1996, 13483,  1031,  1014,  1025,  1056,  1031,  1997,  1037, 13433,
        24077,  2832,  2007,  8015,  1167,  1006,  1056,  1007,  2064,  2022,
         2445,  2004,  1024,   102])"
360,1,"['mean', 'variance']", The Poisson Counting Process,seg_119,with mean value e[n(t)] and variance var[n(t)]:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 1996, 13433, 24077, 10320,  2832])","tensor([  101,  2007,  2812,  3643,  1041,  1031,  1050,  1006,  1056,  1007,
         1033,  1998, 23284, 13075,  1031,  1050,  1006,  1056,  1007,  1033,
         1024,   102])"
361,1,"['probability', 'events', 'interval']", The Poisson Counting Process,seg_119,the probability of no events in the interval [0; t[ i.e. p0(t) is especially interesting considering reliability problems. this probability may be determined directly from eq. 4.63 as:,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996, 13433, 24077, 10320,  2832])","tensor([  101,  1996,  9723,  1997,  2053,  2824,  1999,  1996, 13483,  1031,
         1014,  1025,  1056,  1031,  1045,  1012,  1041,  1012,  1052,  2692,
         1006,  1056,  1007,  2003,  2926,  5875,  6195, 15258,  3471,  1012,
         2023,  9723,  2089,  2022,  4340,  3495,  2013,  1041,  4160,  1012,
         1018,  1012,  6191,  2004,  1024,   102])"
362,1,"['exponential', 'events', 'exponential distributed']", The Poisson Counting Process,seg_119,implying that the time until and between events is exponential distributed.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.])","tensor([ 1996, 13433, 24077, 10320,  2832])","tensor([  101, 20242,  2008,  1996,  2051,  2127,  1998,  2090,  2824,  2003,
        27258,  5500,  1012,   102])"
363,1,"['function', 'cumulative distribution function', 'probability', 'event', 'distribution', 'distribution function']", The Poisson Counting Process,seg_119,"from eq. 4.65 the cumulative distribution function of the waiting time until the first event t1, i.e. ft1(t1) may be straightforwardly derived. recognizing that the probability of t1 > t is p0(t) there is:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 1996, 13433, 24077, 10320,  2832])","tensor([  101,  2013,  1041,  4160,  1012,  1018,  1012,  3515,  1996, 23260,
         4353,  3853,  1997,  1996,  3403,  2051,  2127,  1996,  2034,  2724,
         1056,  2487,  1010,  1045,  1012,  1041,  1012,  3027,  2487,  1006,
         1056,  2487,  1007,  2089,  2022, 19647,  2135,  5173,  1012, 14622,
         2008,  1996,  9723,  1997,  1056,  2487,  1028,  1056,  2003,  1052,
         2692,  1006,  1056,  1007,  2045,  2003,  1024,   102])"
364,1,"['exponential', 'exponential distributed', 'independent']", The Poisson Counting Process,seg_119,consider now the sum of n independent and exponential distributed waiting times t given as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996, 13433, 24077, 10320,  2832])","tensor([  101,  5136,  2085,  1996,  7680,  1997,  1050,  2981,  1998, 27258,
         5500,  3403,  2335,  1056,  2445,  2004,  1024,   102])"
365,1,"['probability', 'gamma', 'random', 'random variables', 'sum of two random variables', 'variables']", The Poisson Counting Process,seg_119,it can be shown by repeated application of the result on the probability distribution for the sum of two random variables (see eq. 4.34) that t is gamma distributed:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.])","tensor([ 1996, 13433, 24077, 10320,  2832])","tensor([  101,  2009,  2064,  2022,  3491,  2011,  5567,  4646,  1997,  1996,
         2765,  2006,  1996,  9723,  4353,  2005,  1996,  7680,  1997,  2048,
         6721, 10857,  1006,  2156,  1041,  4160,  1012,  1018,  1012,  4090,
         1007,  2008,  1056,  2003, 13091,  5500,  1024,   102])"
366,1,"['function', 'variation', 'realization', 'random variable', 'random process', 'level', 'random', 'process', 'variable']", Continuous Random Processes,seg_121,"a random process x(t) is a random function of time, meaning that for any point in time, the value of x(t) is a random variable. a realization of a random process (e.g. water level variation) is illustrated in fig. 4.16.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([7142, 6721, 6194])","tensor([  101,  1037,  6721,  2832,  1060,  1006,  1056,  1007,  2003,  1037,
         6721,  3853,  1997,  2051,  1010,  3574,  2008,  2005,  2151,  2391,
         1999,  2051,  1010,  1996,  3643,  1997,  1060,  1006,  1056,  1007,
         2003,  1037,  6721,  8023,  1012,  1037, 12393,  1997,  1037,  6721,
         2832,  1006,  1041,  1012,  1043,  1012,  2300,  2504,  8386,  1007,
         2003,  7203,  1999, 20965,  1012,  1018,  1012,  2385,  1012,   102])"
367,1,"['set', 'random', 'normal', 'random variables', 'process', 'stochastic process', 'jointly', 'variables']", Continuous Random Processes,seg_121,"a stochastic process x(t) is said to be normal or equivalently gaussian if any set of random variables x(ti), i = 1,2, . . . , n is jointly normal distributed.",tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([7142, 6721, 6194])","tensor([  101,  1037,  2358, 11663, 20875,  2832,  1060,  1006,  1056,  1007,
         2003,  2056,  2000,  2022,  3671,  2030,  5662,  2135, 11721, 17854,
         2937,  2065,  2151,  2275,  1997,  6721, 10857,  1060,  1006, 14841,
         1007,  1010,  1045,  1027,  1015,  1010,  1016,  1010,  1012,  1012,
         1012,  1010,  1050,  2003, 10776,  3671,  5500,  1012,   102])"
368,1,"['mean', 'random variable', 'random', 'process', 'stochastic process', 'variable']", Continuous Random Processes,seg_121,in accordance with the definition of the mean value of a random variable the mean value of all the possible realizations of the stochastic process at time t is given by:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([7142, 6721, 6194])","tensor([  101,  1999, 10388,  2007,  1996,  6210,  1997,  1996,  2812,  3643,
         1997,  1037,  6721,  8023,  1996,  2812,  3643,  1997,  2035,  1996,
         2825, 12393,  2015,  1997,  1996,  2358, 11663, 20875,  2832,  2012,
         2051,  1056,  2003,  2445,  2011,  1024,   102])"
369,1,"['function', 'realization', 'correlation']", Continuous Random Processes,seg_121,"the correlation between all the possible realizations at two points in time t1 and t2 as illustrated in fig. 4.17 is described through the autocorrelation function rxx(t1, t2). auto means that the function refers to only one realization.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0.])","tensor([7142, 6721, 6194])","tensor([  101,  1996, 16902,  2090,  2035,  1996,  2825, 12393,  2015,  2012,
         2048,  2685,  1999,  2051,  1056,  2487,  1998,  1056,  2475,  2004,
         7203,  1999, 20965,  1012,  1018,  1012,  2459,  2003,  2649,  2083,
         1996,  8285, 27108, 16570,  3370,  3853,  1054, 20348,  1006,  1056,
         2487,  1010,  1056,  2475,  1007,  1012,  8285,  2965,  2008,  1996,
         3853,  5218,  2000,  2069,  2028, 12393,  1012,   102])"
370,1,['function'], Continuous Random Processes,seg_121,the autocorrelation function is defined by:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([7142, 6721, 6194])","tensor([  101,  1996,  8285, 27108, 16570,  3370,  3853,  2003,  4225,  2011,
         1024,   102])"
371,1,['function'], Continuous Random Processes,seg_121,the auto-covariance function is defined as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([7142, 6721, 6194])","tensor([  101,  1996,  8285,  1011,  2522, 10755, 28335,  3853,  2003,  4225,
         2004,  1024,   102])"
372,1,"['covariance', 'function']", Continuous Random Processes,seg_121,for t1 = t2 = t the autocovariance function becomes the covariance function:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 0.])","tensor([7142, 6721, 6194])","tensor([  101,  2005,  1056,  2487,  1027,  1056,  2475,  1027,  1056,  1996,
         8285,  3597, 10755, 28335,  3853,  4150,  1996,  2522, 10755, 28335,
         3853,  1024,   102])"
373,1,"['function', 'deviation', 'standard deviation', 'standard']", Continuous Random Processes,seg_121,where σx(t) is the standard deviation function. the autocorrelation function can hence be expressed as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0.])","tensor([7142, 6721, 6194])","tensor([  101,  2073,  1173,  2595,  1006,  1056,  1007,  2003,  1996,  3115,
        24353,  3853,  1012,  1996,  8285, 27108, 16570,  3370,  3853,  2064,
         6516,  2022,  5228,  2004,  1024,   102])"
374,1,"['functions', 'processes', 'vector valued processes', 'stochastic processes', 'density functions', 'process', 'covariance']", Continuous Random Processes,seg_121,"the above definitions for the scalar process x(t) may be extended to cover also vector valued processes x(t) = (x1(t),x2(t), . . . ,xn(t))t having covariance functions cxixj = cov[xi(t1),xj (t2)]. for i = j these become the autocovariance functions and when i = j these are termed as cross-covariance functions. an illustration of realizations of two stochastic processes with density functions of all possible realizations at two different points in time is shown in fig. 4.18.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7142, 6721, 6194])","tensor([  101,  1996,  2682, 15182,  2005,  1996, 26743,  2099,  2832,  1060,
         1006,  1056,  1007,  2089,  2022,  3668,  2000,  3104,  2036,  9207,
        11126,  6194,  1060,  1006,  1056,  1007,  1027,  1006,  1060,  2487,
         1006,  1056,  1007,  1010,  1060,  2475,  1006,  1056,  1007,  1010,
         1012,  1012,  1012,  1010,  1060,  2078,  1006,  1056,  1007,  1007,
         1056,  2383,  2522, 10755, 28335,  4972,  1039,  9048,  2595,  3501,
         1027,  2522,  2615,  1031,  8418,  1006,  1056,  2487,  1007,  1010,
         1060,  3501,  1006,  1056,  2475,  1007,  1033,  1012,  2005,  1045,
         1027,  1046,  2122,  2468,  1996,  8285,  3597, 10755, 28335,  4972,
         1998,  2043,  1045,  1027,  1046,  2122,  2024, 12061,  2004,  2892,
         1011,  2522, 10755, 28335,  4972,  1012,  2019, 14614,  1997, 12393,
         2015,  1997,  2048,  2358, 11663, 20875,  6194,  2007,  4304,  4972,
         1997,  2035,  2825, 12393,  2015,  2012,  2048,  2367,  2685,  1999,
         2051,  2003,  3491,  1999, 20965,  1012,  1018,  1012,  2324,  1012,
          102])"
375,1,"['function', 'correlation']", Continuous Random Processes,seg_121,finally the correlation function may be defined as:,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([7142, 6721, 6194])","tensor([  101,  2633,  1996, 16902,  3853,  2089,  2022,  4225,  2004,  1024,
          102])"
376,1,"['function', 'correlation', 'exponentially']", Continuous Random Processes,seg_121,typically the correlation function is an exponentially decaying function in time.,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.])","tensor([7142, 6721, 6194])","tensor([  101,  4050,  1996, 16902,  3853,  2003,  2019, 27258,  2135, 13121,
         2075,  3853,  1999,  2051,  1012,   102])"
377,1,"['mean', 'function', 'probability', 'interval', 'safe domain', 'process', 'stochastic process']", Continuous Random Processes,seg_121,having defined the mean value function and the autocovariance function for the stochastic process x(t) the probability that the process remains within a certain safe domain d in the time interval [0; t] may be evaluated by:,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7142, 6721, 6194])","tensor([  101,  2383,  4225,  1996,  2812,  3643,  3853,  1998,  1996,  8285,
         3597, 10755, 28335,  3853,  2005,  1996,  2358, 11663, 20875,  2832,
         1060,  1006,  1056,  1007,  1996,  9723,  2008,  1996,  2832,  3464,
         2306,  1037,  3056,  3647,  5884,  1040,  1999,  1996,  2051, 13483,
         1031,  1014,  1025,  1056,  1033,  2089,  2022, 16330,  2011,  1024,
          102])"
378,1,"['interval', 'random', 'process', 'random process']", Continuous Random Processes,seg_121,"where n(t) is the number of out-crossings of the random process out of the domain d in the time interval [0, t]. this is illustrated in fig. 4.19.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([7142, 6721, 6194])","tensor([  101,  2073,  1050,  1006,  1056,  1007,  2003,  1996,  2193,  1997,
         2041,  1011, 20975,  1997,  1996,  6721,  2832,  2041,  1997,  1996,
         5884,  1040,  1999,  1996,  2051, 13483,  1031,  1014,  1010,  1056,
         1033,  1012,  2023,  2003,  7203,  1999, 20965,  1012,  1018,  1012,
         2539,  1012,   102])"
379,1,"['mean', 'function', 'stationary', 'random process', 'moments', 'random', 'process', 'stochastic process', 'independent']", Stationarity and Ergodicity,seg_123,"when the mean value function μx(t) and the autocorrelation function rxx(t) of a stochastic process x(t) do not depend on time, the process is said to be weakly stationary. only if all the moments of a random process are independent of time, is the random process said to be strictly stationary. illustrations of a stationary and non-stationary stochastic process are shown in fig. 4.20.",tensor(1),"tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 2276,  8486,  3723,  1998,  9413,  3995, 14808,  3012])","tensor([  101,  2043,  1996,  2812,  3643,  3853,  1166,  2595,  1006,  1056,
         1007,  1998,  1996,  8285, 27108, 16570,  3370,  3853,  1054, 20348,
         1006,  1056,  1007,  1997,  1037,  2358, 11663, 20875,  2832,  1060,
         1006,  1056,  1007,  2079,  2025, 12530,  2006,  2051,  1010,  1996,
         2832,  2003,  2056,  2000,  2022, 17541, 17337,  1012,  2069,  2065,
         2035,  1996,  5312,  1997,  1037,  6721,  2832,  2024,  2981,  1997,
         2051,  1010,  2003,  1996,  6721,  2832,  2056,  2000,  2022,  9975,
        17337,  1012, 11249,  1997,  1037, 17337,  1998,  2512,  1011, 17337,
         2358, 11663, 20875,  2832,  2024,  3491,  1999, 20965,  1012,  1018,
         1012,  2322,  1012,   102])"
380,1,"['function', 'stationarity', 'functions', 'case']", Stationarity and Ergodicity,seg_123,a consequence of stationarity is that the autocovariance functions and autocorrelation function only depend on the time difference τ = t1 − t2. in this case eq. 4.70 may be written as:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2276,  8486,  3723,  1998,  9413,  3995, 14808,  3012])","tensor([  101,  1037,  9509,  1997,  2276,  8486,  3723,  2003,  2008,  1996,
         8285,  3597, 10755, 28335,  4972,  1998,  8285, 27108, 16570,  3370,
         3853,  2069, 12530,  2006,  1996,  2051,  4489,  1174,  1027,  1056,
         2487,  1597,  1056,  2475,  1012,  1999,  2023,  2553,  1041,  4160,
         1012,  1018,  1012,  3963,  2089,  2022,  2517,  2004,  1024,   102])"
381,1,"['function', 'stationarity', 'stationary', 'processes', 'moments', 'stochastic processes', 'normal']", Stationarity and Ergodicity,seg_123,"it should be noted that for weakly stationary normal stochastic processes, the requirements for strict stationarity are automatically fulfilled as the normal distribution function is completely defined by the first two moments.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.])","tensor([ 2276,  8486,  3723,  1998,  9413,  3995, 14808,  3012])","tensor([  101,  2009,  2323,  2022,  3264,  2008,  2005, 17541, 17337,  3671,
         2358, 11663, 20875,  6194,  1010,  1996,  5918,  2005,  9384,  2276,
         8486,  3723,  2024,  8073, 16829,  2004,  1996,  3671,  4353,  3853,
         2003,  3294,  4225,  2011,  1996,  2034,  2048,  5312,  1012,   102])"
382,1,"['stationarity', 'processes', 'stochastic processes', 'varying', 'intervals', 'process']", Stationarity and Ergodicity,seg_123,"stationarity, in principle, implies that the process cannot start or stop. however, for practical purposes, this requirement may be relaxed if the process is considered at a sufficient time after its start and/or before its end. further, stationarity may be assumed even for slowly varying stochastic processes if sufficiently short time intervals are considered.",tensor(1),"tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 2276,  8486,  3723,  1998,  9413,  3995, 14808,  3012])","tensor([  101,  2276,  8486,  3723,  1010,  1999,  6958,  1010, 12748,  2008,
         1996,  2832,  3685,  2707,  2030,  2644,  1012,  2174,  1010,  2005,
         6742,  5682,  1010,  2023,  9095,  2089,  2022,  8363,  2065,  1996,
         2832,  2003,  2641,  2012,  1037,  7182,  2051,  2044,  2049,  2707,
         1998,  1013,  2030,  2077,  2049,  2203,  1012,  2582,  1010,  2276,
         8486,  3723,  2089,  2022,  5071,  2130,  2005,  3254,  9671,  2358,
        11663, 20875,  6194,  2065, 12949,  2460,  2051, 14025,  2024,  2641,
         1012,   102])"
383,1,"['ergodic', 'mean', 'function', 'stationarity', 'moments', 'process', 'stochastic process', 'average']", Stationarity and Ergodicity,seg_123,"if, in addition to stationarity, the mean value function and the autocorrelation function of a stochastic process may be defined by a time average over one realization of the stochastic process, the process is said to be weakly ergodic. if all moments of the process may be defined in this way, the process is said to be strictly ergodic.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 2276,  8486,  3723,  1998,  9413,  3995, 14808,  3012])","tensor([  101,  2065,  1010,  1999,  2804,  2000,  2276,  8486,  3723,  1010,
         1996,  2812,  3643,  3853,  1998,  1996,  8285, 27108, 16570,  3370,
         3853,  1997,  1037,  2358, 11663, 20875,  2832,  2089,  2022,  4225,
         2011,  1037,  2051,  2779,  2058,  2028, 12393,  1997,  1996,  2358,
        11663, 20875,  2832,  1010,  1996,  2832,  2003,  2056,  2000,  2022,
        17541,  9413,  3995, 14808,  1012,  2065,  2035,  5312,  1997,  1996,
         2832,  2089,  2022,  4225,  1999,  2023,  2126,  1010,  1996,  2832,
         2003,  2056,  2000,  2022,  9975,  9413,  3995, 14808,  1012,   102])"
384,1,"['cases', 'estimation', 'estimate', 'random variable', 'stochastic processes', 'model', 'statistical', 'data', 'process', 'parameters', 'ergodicity', 'processes', 'random', 'measurement', 'distribution', 'variable']", Stationarity and Ergodicity,seg_123,"the assumption of ergodicity is especially important for the estimation of the statistical characteristics of stochastic processes when only one or a few realizations of the process are available, provided they are sufficiently long. if, for example, an engineer wants to model the water temperature at a coastline with a random variable, assuming ergodicity, she/he can use data from one single measurement station to estimate the distribution parameters as long as the data stream is long enough. in practice, ergodicity is, in such cases, often assumed unless, of course, evidence of physical understanding supports the contrary.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2276,  8486,  3723,  1998,  9413,  3995, 14808,  3012])","tensor([  101,  1996, 11213,  1997,  9413,  3995, 14808,  3012,  2003,  2926,
         2590,  2005,  1996, 24155,  1997,  1996,  7778,  6459,  1997,  2358,
        11663, 20875,  6194,  2043,  2069,  2028,  2030,  1037,  2261, 12393,
         2015,  1997,  1996,  2832,  2024,  2800,  1010,  3024,  2027,  2024,
        12949,  2146,  1012,  2065,  1010,  2005,  2742,  1010,  2019,  3992,
         4122,  2000,  2944,  1996,  2300,  4860,  2012,  1037, 15458,  2007,
         1037,  6721,  8023,  1010, 10262,  9413,  3995, 14808,  3012,  1010,
         2016,  1013,  2002,  2064,  2224,  2951,  2013,  2028,  2309, 10903,
         2276,  2000, 10197,  1996,  4353, 11709,  2004,  2146,  2004,  1996,
         2951,  5460,  2003,  2146,  2438,  1012,  1999,  3218,  1010,  9413,
         3995, 14808,  3012,  2003,  1010,  1999,  2107,  3572,  1010,  2411,
         5071,  4983,  1010,  1997,  2607,  1010,  3350,  1997,  3558,  4824,
         6753,  1996, 10043,  1012,   102])"
385,1,"['extreme values', 'risk', 'level', 'random', 'reference period', 'case']", Statistical Assessment of Extreme Values,seg_125,"in risk and reliability assessments, extreme values (small and large) of random processes in a specified reference period are often of special interest. this is e.g. the case when considering the maximum sea water level, maximum wave heights, minimum ground water reservoir level, maximum wind pressures, strength of weakest link systems, maximum snow loads, etc.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7778, 7667, 1997, 6034, 5300])","tensor([  101,  1999,  3891,  1998, 15258, 20794,  1010,  6034,  5300,  1006,
         2235,  1998,  2312,  1007,  1997,  6721,  6194,  1999,  1037,  9675,
         4431,  2558,  2024,  2411,  1997,  2569,  3037,  1012,  2023,  2003,
         1041,  1012,  1043,  1012,  1996,  2553,  2043,  6195,  1996,  4555,
         2712,  2300,  2504,  1010,  4555,  4400,  7535,  1010,  6263,  2598,
         2300,  8071,  2504,  1010,  4555,  3612, 15399,  1010,  3997,  1997,
         5410,  4355,  4957,  3001,  1010,  4555,  4586, 15665,  1010,  4385,
         1012,   102])"
386,1,"['continuous', 'probability distributions', 'probability', 'distributions', 'level', 'distribution']", Statistical Assessment of Extreme Values,seg_125,"for continuous time-varying loads, which can be described by a scalar, i.e. the water level or the wind pressure, one can define a number of related probability distributions. often the simplest, namely the “arbitrary point in time”, distribution is considered.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0.])","tensor([7778, 7667, 1997, 6034, 5300])","tensor([  101,  2005,  7142,  2051,  1011,  9671, 15665,  1010,  2029,  2064,
         2022,  2649,  2011,  1037, 26743,  2099,  1010,  1045,  1012,  1041,
         1012,  1996,  2300,  2504,  2030,  1996,  3612,  3778,  1010,  2028,
         2064,  9375,  1037,  2193,  1997,  3141,  9723, 20611,  1012,  2411,
         1996, 21304,  1010,  8419,  1996,  1523, 15275,  2391,  1999,  2051,
         1524,  1010,  4353,  2003,  2641,  1012,   102])"
387,1,"['function', 'cumulative distribution function', 'realization', 'distribution', 'distribution function', 'arbitrary point in time']", Statistical Assessment of Extreme Values,seg_125,if x(t∗) is a realization of a single time-varying load at time t∗ then fx(x) is the arbitrary point in time cumulative distribution function of x(t) defined by:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7778, 7667, 1997, 6034, 5300])","tensor([  101,  2065,  1060,  1006,  1056, 30125,  1007,  2003,  1037, 12393,
         1997,  1037,  2309,  2051,  1011,  9671,  7170,  2012,  2051,  1056,
        30125,  2059, 23292,  1006,  1060,  1007,  2003,  1996, 15275,  2391,
         1999,  2051, 23260,  4353,  3853,  1997,  1060,  1006,  1056,  1007,
         4225,  2011,  1024,   102])"
388,1,"['histograms', 'observations', 'frequency', 'distributions', 'sample']", Statistical Assessment of Extreme Values,seg_125,in fig. 4.21 first observations of half yearly maximum values of wind speeds are plotted together with histograms showing the corresponding sample frequency distributions. in the same figure the equivalent presentations are also provided for observations corresponding to maximums observed over periods of one and five years.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([7778, 7667, 1997, 6034, 5300])","tensor([  101,  1999, 20965,  1012,  1018,  1012,  2538,  2034,  9420,  1997,
         2431, 12142,  4555,  5300,  1997,  3612, 10898,  2024, 27347,  2362,
         2007,  2010,  3406, 13113,  2015,  4760,  1996,  7978,  7099,  6075,
        20611,  1012,  1999,  1996,  2168,  3275,  1996,  5662, 18216,  2024,
         2036,  3024,  2005,  9420,  7978,  2000,  4555,  2015,  5159,  2058,
         6993,  1997,  2028,  1998,  2274,  2086,  1012,   102])"
389,1,"['mean', 'deviation', 'histograms', 'frequency', 'sample', 'standard deviation', 'standard']", Statistical Assessment of Extreme Values,seg_125,"from fig. 4.21 it is seen that there is a clear tendency that the mean value of the sample frequency histograms increases with increasing length of the considered period. at the same time, the standard deviation is seen to be decreasing.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([7778, 7667, 1997, 6034, 5300])","tensor([  101,  2013, 20965,  1012,  1018,  1012,  2538,  2009,  2003,  2464,
         2008,  2045,  2003,  1037,  3154, 11765,  2008,  1996,  2812,  3643,
         1997,  1996,  7099,  6075,  2010,  3406, 13113,  2015,  7457,  2007,
         4852,  3091,  1997,  1996,  2641,  2558,  1012,  2012,  1996,  2168,
         2051,  1010,  1996,  3115, 24353,  2003,  2464,  2000,  2022, 16922,
         1012,   102])"
390,1,"['observations', 'random', 'statistical', 'model', 'extreme value', 'independent']", Statistical Assessment of Extreme Values,seg_125,"for practical purposes, the observations of half yearly maxima may be assumed to be statistically independent and provide the basis (random “half yearly” point in time model) for the further modeling of the statistical characteristics of extremes for longer periods by extreme value considerations.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([7778, 7667, 1997, 6034, 5300])","tensor([  101,  2005,  6742,  5682,  1010,  1996,  9420,  1997,  2431, 12142,
        20446,  2050,  2089,  2022,  5071,  2000,  2022,  7778,  2135,  2981,
         1998,  3073,  1996,  3978,  1006,  6721,  1523,  2431, 12142,  1524,
         2391,  1999,  2051,  2944,  1007,  2005,  1996,  2582, 11643,  1997,
         1996,  7778,  6459,  1997, 28800,  2005,  2936,  6993,  2011,  6034,
         3643, 16852,  1012,   102])"
391,1,"['extreme events', 'functions', 'asymptotic', 'events', 'extreme value', 'random processes', 'trials', 'results', 'random variables', 'tail', 'processes', 'random', 'variables']", Statistical Assessment of Extreme Values,seg_125,"in the following section some results are given concerning the extreme events of trials of random variables and random processes, see also madsen et al. [9] and benjamin and cornell [4]. taking basis in the tail behavior of cumulative distribution functions, asymptotic results are given leading to the extreme value distributions.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([7778, 7667, 1997, 6034, 5300])","tensor([  101,  1999,  1996,  2206,  2930,  2070,  3463,  2024,  2445,  7175,
         1996,  6034,  2824,  1997,  7012,  1997,  6721, 10857,  1998,  6721,
         6194,  1010,  2156,  2036,  5506,  5054,  3802,  2632,  1012,  1031,
         1023,  1033,  1998,  6425,  1998, 10921,  1031,  1018,  1033,  1012,
         2635,  3978,  1999,  1996,  5725,  5248,  1997, 23260,  4353,  4972,
         1010,  2004, 24335, 13876, 20214,  3463,  2024,  2445,  2877,  2000,
         1996,  6034,  3643, 20611,  1012,   102])"
392,1,"['extreme events', 'reference period', 'events', 'distribution', 'arbitrary point in time', 'variable']", Extreme Value Distributions,seg_127,"when extreme events are of interest, the arbitrary point in time distribution of the load variable is not of immediate relevance but rather the distribution of the maximal values of the considered quantity over a given reference period.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0.])","tensor([ 6034,  3643, 20611])","tensor([  101,  2043,  6034,  2824,  2024,  1997,  3037,  1010,  1996, 15275,
         2391,  1999,  2051,  4353,  1997,  1996,  7170,  8023,  2003,  2025,
         1997,  6234, 21923,  2021,  2738,  1996,  4353,  1997,  1996, 29160,
         5300,  1997,  1996,  2641, 11712,  2058,  1037,  2445,  4431,  2558,
         1012,   102])"
393,1,"['ergodic', 'distribution', 'realization', 'random process', 'random', 'sampling', 'process', 'reference period']", Extreme Value Distributions,seg_127,"if the random process x(t) may be assumed to be ergodic, the distribution of the largest extreme in a reference period t , fxm,atx(x) can be thought of as being generated by sampling values of the maximal realization xmax from successive reference periods t .",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 6034,  3643, 20611])","tensor([  101,  2065,  1996,  6721,  2832,  1060,  1006,  1056,  1007,  2089,
         2022,  5071,  2000,  2022,  9413,  3995, 14808,  1010,  1996,  4353,
         1997,  1996,  2922,  6034,  1999,  1037,  4431,  2558,  1056,  1010,
        23292,  2213,  1010,  2012,  2595,  1006,  1060,  1007,  2064,  2022,
         2245,  1997,  2004,  2108,  7013,  2011, 16227,  5300,  1997,  1996,
        29160, 12393,  1060, 17848,  2013, 11165,  4431,  6993,  1056,  1012,
          102])"
394,1,"['function', 'cumulative distribution function', 'distribution', 'distribution function']", Extreme Value Distributions,seg_127,"the cumulative distribution function of the largest extreme in a period of nt , fxm,anxt (x), (with n being an integer) may be determined from the cumulative distri-",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 6034,  3643, 20611])","tensor([  101,  1996, 23260,  4353,  3853,  1997,  1996,  2922,  6034,  1999,
         1037,  2558,  1997, 23961,  1010, 23292,  2213,  1010,  2019, 18413,
         1006,  1060,  1007,  1010,  1006,  2007,  1050,  2108,  2019, 16109,
         1007,  2089,  2022,  4340,  2013,  1996, 23260,  4487,  3367,  3089,
         1011,   102])"
395,1,['function'], Extreme Value Distributions,seg_127,"bution function of the largest extreme in the period t , fxm,atx(x), by:",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 6034,  3643, 20611])","tensor([  101,  2021,  3258,  3853,  1997,  1996,  2922,  6034,  1999,  1996,
         2558,  1056,  1010, 23292,  2213,  1010,  2012,  2595,  1006,  1060,
         1007,  1010,  2011,  1024,   102])"
396,1,"['function', 'density function', 'probability', 'probability density function', 'events', 'independent events', 'independent']", Extreme Value Distributions,seg_127,which follows from the multiplication law for independent events. the corresponding probability density function may be established by differentiation of eq. 4.78 yielding:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6034,  3643, 20611])","tensor([  101,  2029,  4076,  2013,  1996, 24856,  2375,  2005,  2981,  2824,
         1012,  1996,  7978,  9723,  4304,  3853,  2089,  2022,  2511,  2011,
        20582,  1997,  1041,  4160,  1012,  1018,  1012,  6275, 21336,  1024,
          102])"
397,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution', 'case']", Extreme Value Distributions,seg_127,in fig. 4.22 the case of a normal distribution with mean value equal to 10 and standard deviation equal to 3 is illustrated for increasing n.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6034,  3643, 20611])","tensor([  101,  1999, 20965,  1012,  1018,  1012,  2570,  1996,  2553,  1997,
         1037,  3671,  4353,  2007,  2812,  3643,  5020,  2000,  2184,  1998,
         3115, 24353,  5020,  2000,  1017,  2003,  7203,  2005,  4852,  1050,
         1012,   102])"
398,1,"['function', 'cumulative distribution function', 'distribution', 'distribution function', 'reference period']", Extreme Value Distributions,seg_127,"similar to the derivation of eq. 4.78 the cumulative distribution function for the extreme minimum value in a considered reference period t , fxm,innt (x) may be found as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 6034,  3643, 20611])","tensor([  101,  2714,  2000,  1996, 29280,  1997,  1041,  4160,  1012,  1018,
         1012,  6275,  1996, 23260,  4353,  3853,  2005,  1996,  6034,  6263,
         3643,  1999,  1037,  2641,  4431,  2558,  1056,  1010, 23292,  2213,
         1010,  7601,  2102,  1006,  1060,  1007,  2089,  2022,  2179,  2004,
         1024,   102])"
399,1,"['table', 'type ii', 'random variable', 'extreme value', 'probability distributions', 'function', 'probability', 'process', 'parameters', 'ergodic', 'tail', 'extreme value distributions', 'extreme event', 'event', 'distributions', 'moments', 'random', 'distribution', 'reference period', 'variable']", Extreme Value Distributions,seg_127,"subject to the assumption that the considered process is ergodic, it can be shown that the cumulative function for an extreme event fxm,anxt (x) converges asymptotically (as the reference period nt increases) to one of three types of extreme value distributions, type i, type ii, or type iii. to which type the distribution converges depends only on the tail behavior (upper or lower) of the considered random variable generating the extremes, i.e. fxm,atx(x). in the following sections the three types or extreme value distributions will be introduced and it will be discussed under what conditions they may be assumed. in table 4.2 the definition of the extreme value probability distributions and their parameters and moments is summarized.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.])","tensor([ 6034,  3643, 20611])","tensor([  101,  3395,  2000,  1996, 11213,  2008,  1996,  2641,  2832,  2003,
         9413,  3995, 14808,  1010,  2009,  2064,  2022,  3491,  2008,  1996,
        23260,  3853,  2005,  2019,  6034,  2724, 23292,  2213,  1010,  2019,
        18413,  1006,  1060,  1007, 28314,  2015,  2004, 24335, 13876, 20214,
         3973,  1006,  2004,  1996,  4431,  2558, 23961,  7457,  1007,  2000,
         2028,  1997,  2093,  4127,  1997,  6034,  3643, 20611,  1010,  2828,
         1045,  1010,  2828,  2462,  1010,  2030,  2828,  3523,  1012,  2000,
         2029,  2828,  1996,  4353, 28314,  2015,  9041,  2069,  2006,  1996,
         5725,  5248,  1006,  3356,  2030,  2896,  1007,  1997,  1996,  2641,
         6721,  8023, 11717,  1996, 28800,  1010,  1045,  1012,  1041,  1012,
        23292,  2213,  1010,  2012,  2595,  1006,  1060,  1007,  1012,  1999,
         1996,  2206,  5433,  1996,  2093,  4127,  2030,  6034,  3643, 20611,
         2097,  2022,  3107,  1998,  2009,  2097,  2022,  6936,  2104,  2054,
         3785,  2027,  2089,  2022,  5071,  1012,  1999,  2795,  1018,  1012,
         1016,  1996,  6210,  1997,  1996,  6034,  3643,  9723, 20611,  1998,
         2037, 11709,  1998,  5312,  2003, 22539,  1012,   102])"
400,1,"['tail', 'function', 'functions', 'normal', 'distribution', 'exponential', 'case']", Type I Extreme Maximum Value DistributionGumbel Max,seg_129,"for upwards unbounded distribution functions fx(x) where the upper tail falls off in an exponential manner, such as the case of the exponential function, the normal",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.])","tensor([ 2828,  1045,  6034,  4555,  3643,  4353, 22850,  8671,  4098])","tensor([  101,  2005, 14873,  4895, 15494,  2098,  4353,  4972, 23292,  1006,
         1060,  1007,  2073,  1996,  3356,  5725,  4212,  2125,  1999,  2019,
        27258,  5450,  1010,  2107,  2004,  1996,  2553,  1997,  1996, 27258,
         3853,  1010,  1996,  3671,   102])"
401,1,"['gamma', 'reference period', 'gamma distribution', 'distribution']", Type I Extreme Maximum Value DistributionGumbel Max,seg_129,"distribution and the gamma distribution, the cumulative distribution of extremes in the reference period t i.e. fxm,atx(x) has the following form:",tensor(1),"tensor([0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2828,  1045,  6034,  4555,  3643,  4353, 22850,  8671,  4098])","tensor([  101,  4353,  1998,  1996, 13091,  4353,  1010,  1996, 23260,  4353,
         1997, 28800,  1999,  1996,  4431,  2558,  1056,  1045,  1012,  1041,
         1012, 23292,  2213,  1010,  2012,  2595,  1006,  1060,  1007,  2038,
         1996,  2206,  2433,  1024,   102])"
402,1,"['function', 'density function', 'probability', 'probability density function']", Type I Extreme Maximum Value DistributionGumbel Max,seg_129,with corresponding probability density function:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 2828,  1045,  6034,  4555,  3643,  4353, 22850,  8671,  4098])","tensor([ 101, 2007, 7978, 9723, 4304, 3853, 1024,  102])"
403,1,"['mean', 'distribution', 'deviation', 'gumbel distribution', 'standard deviation', 'standard', 'gumbel']", Type I Extreme Maximum Value DistributionGumbel Max,seg_129,which is also called the gumbel distribution for extreme maxima. the mean value and the standard deviation of the gumbel distribution may be related to the parameters u and α as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2828,  1045,  6034,  4555,  3643,  4353, 22850,  8671,  4098])","tensor([  101,  2029,  2003,  2036,  2170,  1996, 16031,  8671,  4353,  2005,
         6034, 20446,  2050,  1012,  1996,  2812,  3643,  1998,  1996,  3115,
        24353,  1997,  1996, 16031,  8671,  4353,  2089,  2022,  3141,  2000,
         1996, 11709,  1057,  1998,  1155,  2004,  1024,   102])"
404,1,"['mean', 'distribution', 'deviation', 'gumbel distribution', 'standard deviation', 'standard', 'gumbel', 'reference period', 'independent']", Type I Extreme Maximum Value DistributionGumbel Max,seg_129,"the gumbel distribution has the useful property that the standard deviation is independent of the considered reference period, i.e. σxmax = σxmax and that the mean",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0.])","tensor([ 2828,  1045,  6034,  4555,  3643,  4353, 22850,  8671,  4098])","tensor([  101,  1996, 16031,  8671,  4353,  2038,  1996,  6179,  3200,  2008,
         1996,  3115, 24353,  2003,  2981,  1997,  1996,  2641,  4431,  2558,
         1010,  1045,  1012,  1041,  1012,  1173,  2595, 17848,  1027,  1173,
         2595, 17848,  1998,  2008,  1996,  2812,   102])"
405,0,['n'], Type I Extreme Maximum Value DistributionGumbel Max,seg_129,nt t value μxmax depends on n in the following simple way:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2828,  1045,  6034,  4555,  3643,  4353, 22850,  8671,  4098])","tensor([  101, 23961,  1056,  3643,  1166,  2595, 17848,  9041,  2006,  1050,
         1999,  1996,  2206,  3722,  2126,  1024,   102])"
406,1,"['tail', 'function', 'cumulative distribution function', 'symmetry', 'distribution', 'exponential', 'distribution function', 'reference period', 'case']", Type I Extreme Minimum Value DistributionGumbel Min,seg_131,"in case that the cumulative distribution function fx(x) is downwards unbounded and the lower tail falls off in an exponential manner, symmetry considerations lead to a cumulative distribution function for the extreme minimum fxm,itn(x) within the reference period t of the following form:",tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 2828,  1045,  6034,  6263,  3643,  4353, 22850,  8671,  8117])","tensor([  101,  1999,  2553,  2008,  1996, 23260,  4353,  3853, 23292,  1006,
         1060,  1007,  2003, 28457,  4895, 15494,  2098,  1998,  1996,  2896,
         5725,  4212,  2125,  1999,  2019, 27258,  5450,  1010, 14991, 16852,
         2599,  2000,  1037, 23260,  4353,  3853,  2005,  1996,  6034,  6263,
        23292,  2213,  1010,  2009,  2078,  1006,  1060,  1007,  2306,  1996,
         4431,  2558,  1056,  1997,  1996,  2206,  2433,  1024,   102])"
407,1,"['function', 'density function', 'probability', 'probability density function']", Type I Extreme Minimum Value DistributionGumbel Min,seg_131,with corresponding probability density function:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 2828,  1045,  6034,  6263,  3643,  4353, 22850,  8671,  8117])","tensor([ 101, 2007, 7978, 9723, 4304, 3853, 1024,  102])"
408,1,"['mean', 'distribution', 'deviation', 'gumbel distribution', 'standard deviation', 'standard', 'gumbel']", Type I Extreme Minimum Value DistributionGumbel Min,seg_131,which is also called the gumbel distribution for extreme minima. the mean value and the standard deviation of the gumbel distribution can be related to the parameters u and α as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2828,  1045,  6034,  6263,  3643,  4353, 22850,  8671,  8117])","tensor([  101,  2029,  2003,  2036,  2170,  1996, 16031,  8671,  4353,  2005,
         6034,  7163,  2863,  1012,  1996,  2812,  3643,  1998,  1996,  3115,
        24353,  1997,  1996, 16031,  8671,  4353,  2064,  2022,  3141,  2000,
         1996, 11709,  1057,  1998,  1155,  2004,  1024,   102])"
409,1,"['tail', 'functions', 'distribution']", Type II Extreme Maximum Value DistributionFréchet Max,seg_133,for cumulative distribution functions downwards limited at zero and upwards unlimited with a tail falling off in the form:,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 2828,  2462,  6034,  4555,  3643,  4353, 19699, 27635,  2102,  4098])","tensor([  101,  2005, 23260,  4353,  4972, 28457,  3132,  2012,  5717,  1998,
        14873, 14668,  2007,  1037,  5725,  4634,  2125,  1999,  1996,  2433,
         1024,   102])"
410,1,"['function', 'cumulative distribution function', 'distribution', 'distribution function', 'reference period']", Type II Extreme Maximum Value DistributionFréchet Max,seg_133,"the cumulative distribution function of extreme maxima in the reference period t i.e. fxm,atx(x) has the following form:",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2828,  2462,  6034,  4555,  3643,  4353, 19699, 27635,  2102,  4098])","tensor([  101,  1996, 23260,  4353,  3853,  1997,  6034, 20446,  2050,  1999,
         1996,  4431,  2558,  1056,  1045,  1012,  1041,  1012, 23292,  2213,
         1010,  2012,  2595,  1006,  1060,  1007,  2038,  1996,  2206,  2433,
         1024,   102])"
411,1,"['function', 'density function', 'probability', 'probability density function']", Type II Extreme Maximum Value DistributionFréchet Max,seg_133,with corresponding probability density function:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 2828,  2462,  6034,  4555,  3643,  4353, 19699, 27635,  2102,  4098])","tensor([ 101, 2007, 7978, 9723, 4304, 3853, 1024,  102])"
412,1,"['mean', 'variance', 'distribution', 'parameters']", Type II Extreme Maximum Value DistributionFréchet Max,seg_133,which is also called the fréchet distribution for extreme maxima. the mean value and the variance of the fréchet distribution can be related to the parameters u and k as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0.])","tensor([ 2828,  2462,  6034,  4555,  3643,  4353, 19699, 27635,  2102,  4098])","tensor([  101,  2029,  2003,  2036,  2170,  1996, 10424, 27635,  2102,  4353,
         2005,  6034, 20446,  2050,  1012,  1996,  2812,  3643,  1998,  1996,
        23284,  1997,  1996, 10424, 27635,  2102,  4353,  2064,  2022,  3141,
         2000,  1996, 11709,  1057,  1998,  1047,  2004,  1024,   102])"
413,1,"['mean', 'moment', 'standard', 'distribution']", Type II Extreme Maximum Value DistributionFréchet Max,seg_133,where it is noticed that the mean value only exists for k > 1 and the standard deviation only exist for k > 2. in general it can be shown that the ith moment of the fréchet distribution exists only when k > i.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2828,  2462,  6034,  4555,  3643,  4353, 19699, 27635,  2102,  4098])","tensor([  101,  2073,  2009,  2003,  4384,  2008,  1996,  2812,  3643,  2069,
         6526,  2005,  1047,  1028,  1015,  1998,  1996,  3115, 24353,  2069,
         4839,  2005,  1047,  1028,  1016,  1012,  1999,  2236,  2009,  2064,
         2022,  3491,  2008,  1996,  2009,  2232,  2617,  1997,  1996, 10424,
        27635,  2102,  4353,  6526,  2069,  2043,  1047,  1028,  1045,  1012,
          102])"
414,1,"['tail', 'function', 'cumulative distribution function', 'distribution', 'distribution function', 'case']", Type III Extreme Minimum Value DistributionWeibull Min,seg_135,"finally, in the case where the cumulative distribution function fx(x) is downwards limited at ε and the lower tail falls off towards ε in the form:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2828,  3523,  6034,  6263,  3643,  4353, 19845,  8569,  3363,  8117])","tensor([  101,  2633,  1010,  1999,  1996,  2553,  2073,  1996, 23260,  4353,
         3853, 23292,  1006,  1060,  1007,  2003, 28457,  3132,  2012,  1159,
         1998,  1996,  2896,  5725,  4212,  2125,  2875,  1159,  1999,  1996,
         2433,  1024,   102])"
415,1,"['function', 'cumulative distribution function', 'distribution', 'distribution function', 'reference period']", Type III Extreme Minimum Value DistributionWeibull Min,seg_135,"leads to a cumulative distribution function for the extreme minimum fxm,itn(x) within the reference period t of the following form:",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2828,  3523,  6034,  6263,  3643,  4353, 19845,  8569,  3363,  8117])","tensor([  101,  5260,  2000,  1037, 23260,  4353,  3853,  2005,  1996,  6034,
         6263, 23292,  2213,  1010,  2009,  2078,  1006,  1060,  1007,  2306,
         1996,  4431,  2558,  1056,  1997,  1996,  2206,  2433,  1024,   102])"
416,1,"['function', 'density function', 'probability', 'probability density function']", Type III Extreme Minimum Value DistributionWeibull Min,seg_135,with corresponding probability density function:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 2828,  3523,  6034,  6263,  3643,  4353, 19845,  8569,  3363,  8117])","tensor([ 101, 2007, 7978, 9723, 4304, 3853, 1024,  102])"
417,1,"['mean', 'variance', 'weibull', 'distribution', 'weibull distribution', 'parameters']", Type III Extreme Minimum Value DistributionWeibull Min,seg_135,"which is also called the weibull distribution for extreme minima. the mean value and the variance of the weibull distribution can be related to the parameters u, k and ε as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 2828,  3523,  6034,  6263,  3643,  4353, 19845,  8569,  3363,  8117])","tensor([  101,  2029,  2003,  2036,  2170,  1996, 11417,  8569,  3363,  4353,
         2005,  6034,  7163,  2863,  1012,  1996,  2812,  3643,  1998,  1996,
        23284,  1997,  1996, 11417,  8569,  3363,  4353,  2064,  2022,  3141,
         2000,  1996, 11709,  1057,  1010,  1047,  1998,  1159,  2004,  1024,
          102])"
418,1,"['return period', 'extreme event', 'event']", Return Period for Extreme Events,seg_137,the return period tr for an extreme event corresponding to x may be defined by:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2709, 2558, 2005, 6034, 2824])","tensor([  101,  1996,  2709,  2558, 19817,  2005,  2019,  6034,  2724,  7978,
         2000,  1060,  2089,  2022,  4225,  2011,  1024,   102])"
419,1,"['function', 'cumulative distribution function', 'probability', 'events', 'event', 'distribution', 'distribution function', 'reference period', 'return period']", Return Period for Extreme Events,seg_137,"where t is the reference period for the cumulative distribution function of the extreme events fxm,atx(x). if, for example, the annual probability of an extreme load event is 0.02, the return period for this load event is 50 years.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0.])","tensor([2709, 2558, 2005, 6034, 2824])","tensor([  101,  2073,  1056,  2003,  1996,  4431,  2558,  2005,  1996, 23260,
         4353,  3853,  1997,  1996,  6034,  2824, 23292,  2213,  1010,  2012,
         2595,  1006,  1060,  1007,  1012,  2065,  1010,  2005,  2742,  1010,
         1996,  3296,  9723,  1997,  2019,  6034,  7170,  2724,  2003,  1014,
         1012,  6185,  1010,  1996,  2709,  2558,  2005,  2023,  7170,  2724,
         2003,  2753,  2086,  1012,   102])"
420,1,"['events', 'event', 'level', 'average', 'return period']", Example A Flood with a Year Return Period,seg_139,"the concept of the return period is often applied in the area of flood protection. in fig. 4.23, the 100-year return period flood event is defined by the exceedance of a water level of 8 m. the yearly maximum water level is higher than this threshold on average every 100 years, although waiting times between two consecutive events can differ from this average.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0.])","tensor([2742, 1037, 7186, 2007, 1037, 2095, 2709, 2558])","tensor([  101,  1996,  4145,  1997,  1996,  2709,  2558,  2003,  2411,  4162,
         1999,  1996,  2181,  1997,  7186,  3860,  1012,  1999, 20965,  1012,
         1018,  1012,  2603,  1010,  1996,  2531,  1011,  2095,  2709,  2558,
         7186,  2724,  2003,  4225,  2011,  1996, 13467,  6651,  1997,  1037,
         2300,  2504,  1997,  1022,  1049,  1012,  1996, 12142,  4555,  2300,
         2504,  2003,  3020,  2084,  2023, 11207,  2006,  2779,  2296,  2531,
         2086,  1010,  2348,  3403,  2335,  2090,  2048,  5486,  2824,  2064,
        11234,  2013,  2023,  2779,  1012,   102])"
421,1,"['quantile', 'probability', 'event', 'extreme value', 'return period']", Example A Flood with a Year Return Period,seg_139,the threshold defining an event with return period tr is called the characteristic value xc . this value has a probability of exceedance of p = 1/tr and can be determined as a quantile value of the corresponding extreme value distribution.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([2742, 1037, 7186, 2007, 1037, 2095, 2709, 2558])","tensor([  101,  1996, 11207, 12854,  2019,  2724,  2007,  2709,  2558, 19817,
         2003,  2170,  1996,  8281,  3643,  1060,  2278,  1012,  2023,  3643,
         2038,  1037,  9723,  1997, 13467,  6651,  1997,  1052,  1027,  1015,
         1013, 19817,  1998,  2064,  2022,  4340,  2004,  1037, 24110, 15286,
         3643,  1997,  1996,  7978,  6034,  3643,  4353,  1012,   102])"
422,1,"['distribution', 'probability', 'characteristic value', 'gumbel', 'return period']", Example A Flood with a Year Return Period,seg_139,"for the gumbel max distribution, it is possible to calculate the characteristic value approximately. by manipulation of eq. 4.81 it can be shown, by utilizing a taylor expansion to the first order of ln(p) in p = 1, that the characteristic value xc corresponding to an annual exceedance probability of p and corresponding return period tr = 1/p for a gumbel max distribution for large return periods can be",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 1037, 7186, 2007, 1037, 2095, 2709, 2558])","tensor([  101,  2005,  1996, 16031,  8671,  4098,  4353,  1010,  2009,  2003,
         2825,  2000, 18422,  1996,  8281,  3643,  3155,  1012,  2011, 16924,
         1997,  1041,  4160,  1012,  1018,  1012,  6282,  2009,  2064,  2022,
         3491,  1010,  2011, 16911,  1037,  4202,  4935,  2000,  1996,  2034,
         2344,  1997,  1048,  2078,  1006,  1052,  1007,  1999,  1052,  1027,
         1015,  1010,  2008,  1996,  8281,  3643,  1060,  2278,  7978,  2000,
         2019,  3296, 13467,  6651,  9723,  1997,  1052,  1998,  7978,  2709,
         2558, 19817,  1027,  1015,  1013,  1052,  2005,  1037, 16031,  8671,
         4098,  4353,  2005,  2312,  2709,  6993,  2064,  2022,   102])"
423,1,"['characteristic value', 'return period']", Example A Flood with a Year Return Period,seg_139,which shows that the characteristic value increases with the logarithm of the considered return period.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0.])","tensor([2742, 1037, 7186, 2007, 1037, 2095, 2709, 2558])","tensor([ 101, 2029, 3065, 2008, 1996, 8281, 3643, 7457, 2007, 1996, 8833, 8486,
        2705, 2213, 1997, 1996, 2641, 2709, 2558, 1012,  102])"
424,1,"['function', 'density function', 'simulation', 'event', 'level', 'gumbel', 'characteristic value', 'distribution', 'parameters', 'return period']", Example A Flood with a Year Return Period,seg_139,"for this example, using a gumbel max distribution with parameters u = 6 and α = 2.3 and a return period tr = 100, a characteristic value xc = 8 is obtained. the threshold value of 8 m defining the 100-year return period flood event is shown in figs. 4.23 and 4.24. while fig. 4.24 shows the density function of the yearly maximum water level using a gumbel max distribution with parameters as specified above, fig. 4.23 has been obtained based on a simulation of the gumbel max distribution with the same parameters.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0.])","tensor([2742, 1037, 7186, 2007, 1037, 2095, 2709, 2558])","tensor([  101,  2005,  2023,  2742,  1010,  2478,  1037, 16031,  8671,  4098,
         4353,  2007, 11709,  1057,  1027,  1020,  1998,  1155,  1027,  1016,
         1012,  1017,  1998,  1037,  2709,  2558, 19817,  1027,  2531,  1010,
         1037,  8281,  3643,  1060,  2278,  1027,  1022,  2003,  4663,  1012,
         1996, 11207,  3643,  1997,  1022,  1049, 12854,  1996,  2531,  1011,
         2095,  2709,  2558,  7186,  2724,  2003,  3491,  1999, 20965,  2015,
         1012,  1018,  1012,  2603,  1998,  1018,  1012,  2484,  1012,  2096,
        20965,  1012,  1018,  1012,  2484,  3065,  1996,  4304,  3853,  1997,
         1996, 12142,  4555,  2300,  2504,  2478,  1037, 16031,  8671,  4098,
         4353,  2007, 11709,  2004,  9675,  2682,  1010, 20965,  1012,  1018,
         1012,  2603,  2038,  2042,  4663,  2241,  2006,  1037, 12504,  1997,
         1996, 16031,  8671,  4098,  4353,  2007,  1996,  2168, 11709,  1012,
          102])"
425,1,['uncertainties'], Self Assessment QuestionsExercises,seg_141,1. what types of uncertainties can be distinguished and how do these depend on,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1015,  1012,  2054,  4127,  1997,  9662,  7368,  2064,  2022,
         5182,  1998,  2129,  2079,  2122, 12530,  2006,   102])"
426,1,"['uncertainties', 'random', 'expectation', 'expectation operator']", Self Assessment QuestionsExercises,seg_141,"the time and scale of modeling? 2. what is understood by the terms aleatory and epistemic uncertainties? 3. what is meant by the term “continuous random variable”? 4. using the properties of the expectation operator, how may the following nota-",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1996,  2051,  1998,  4094,  1997, 11643,  1029,  1016,  1012,
         2054,  2003,  5319,  2011,  1996,  3408, 15669, 14049,  1998,  4958,
        27870,  7712,  9662,  7368,  1029,  1017,  1012,  2054,  2003,  3214,
         2011,  1996,  2744,  1523,  7142,  6721,  8023,  1524,  1029,  1018,
         1012,  2478,  1996,  5144,  1997,  1996, 17626,  6872,  1010,  2129,
         2089,  1996,  2206,  2025,  2050,  1011,   102])"
427,1,"['probability', 'random', 'random variable', 'variable']", Self Assessment QuestionsExercises,seg_141,tions be rewritten? (note that a and b are constants and x is a random variable) a. e[a + bx] b. var[a + bx] 5. write down the names of the axes of the probability density and the cumula-,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101, 14841,  5644,  2022,  2128, 15773,  1029,  1006,  3602,  2008,
         1037,  1998,  1038,  2024,  5377,  2015,  1998,  1060,  2003,  1037,
         6721,  8023,  1007,  1037,  1012,  1041,  1031,  1037,  1009,  1038,
         2595,  1033,  1038,  1012, 13075,  1031,  1037,  1009,  1038,  2595,
         1033,  1019,  1012,  4339,  2091,  1996,  3415,  1997,  1996, 19589,
         1997,  1996,  9723,  4304,  1998,  1996, 13988,  7068,  1011,   102])"
428,1,"['functions', 'random variable', 'random', 'distribution', 'variable']", Self Assessment QuestionsExercises,seg_141,tive distribution functions of the random variable x illustrated in figs. 4.25,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101, 14841,  3726,  4353,  4972,  1997,  1996,  6721,  8023,  1060,
         7203,  1999, 20965,  2015,  1012,  1018,  1012,  2423,   102])"
429,1,"['density function', 'cumulative distribution function', 'median', 'mean', 'bernoulli', 'probability density function', 'locations', 'random variable', 'poisson', 'poisson process', 'standardized', 'continuous', 'trial', 'function', 'probability', 'process', 'central limit theorem', 'continuous random variable', 'random', 'limit', 'distribution', 'distribution function', 'bernoulli trial', 'variable']", Self Assessment QuestionsExercises,seg_141,"and 4.26. identify the locations of the mean, the mode and the median in the illustration of the probability density function. show also the value of the median in the illustration of the cumulative distribution function shown in fig. 4.26. 6. state the central limit theorem. 7. what is a standardized random variable and how is it defined? 8. what is a bernoulli trial and what does it describe? 9. what is a poisson process and where can it be applied? 10. the probability density function of a continuous random variable x, defined in",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1998,  1018,  1012,  2656,  1012,  6709,  1996,  5269,  1997,
         1996,  2812,  1010,  1996,  5549,  1998,  1996,  3991,  1999,  1996,
        14614,  1997,  1996,  9723,  4304,  3853,  1012,  2265,  2036,  1996,
         3643,  1997,  1996,  3991,  1999,  1996, 14614,  1997,  1996, 23260,
         4353,  3853,  3491,  1999, 20965,  1012,  1018,  1012,  2656,  1012,
         1020,  1012,  2110,  1996,  2430,  5787,  9872,  1012,  1021,  1012,
         2054,  2003,  1037, 16367,  6721,  8023,  1998,  2129,  2003,  2009,
         4225,  1029,  1022,  1012,  2054,  2003,  1037, 16595,  7140,  6894,
         3979,  1998,  2054,  2515,  2009,  6235,  1029,  1023,  1012,  2054,
         2003,  1037, 13433, 24077,  2832,  1998,  2073,  2064,  2009,  2022,
         4162,  1029,  2184,  1012,  1996,  9723,  4304,  3853,  1997,  1037,
         7142,  6721,  8023,  1060,  1010,  4225,  1999,   102])"
430,1,"['probability', 'interval']", Self Assessment QuestionsExercises,seg_141,"the interval [0,10], is illustrated in fig. 4.27. calculate the probability that x may exceed the value of 5. 11. it is given that the operational life (until breakdown) t of a diesel en-",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1996, 13483,  1031,  1014,  1010,  2184,  1033,  1010,  2003,
         7203,  1999, 20965,  1012,  1018,  1012,  2676,  1012, 18422,  1996,
         9723,  2008,  1060,  2089, 13467,  1996,  3643,  1997,  1019,  1012,
         2340,  1012,  2009,  2003,  2445,  2008,  1996,  6515,  2166,  1006,
         2127, 12554,  1007,  1056,  1997,  1037,  7937,  4372,  1011,   102])"
431,1,"['exponential', 'mean', 'exponential distribution', 'distribution']", Self Assessment QuestionsExercises,seg_141,"−λt gine follows an exponential distribution, ft (t) = 1 − e , with parameter λ and mean value, μt = 1/λ, equal to 10 years. calculate the probability that the engine breaks down within 2 years after placed in operation.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1597, 29727,  2102, 18353,  2063,  4076,  2019, 27258,  4353,
         1010,  3027,  1006,  1056,  1007,  1027,  1015,  1597,  1041,  1010,
         2007, 16381,  1165,  1998,  2812,  3643,  1010,  1166,  2102,  1027,
         1015,  1013,  1165,  1010,  5020,  2000,  2184,  2086,  1012, 18422,
         1996,  9723,  2008,  1996,  3194,  7807,  2091,  2306,  1016,  2086,
         2044,  2872,  1999,  3169,  1012,   102])"
432,1,"['events', 'average']", Self Assessment QuestionsExercises,seg_141,12. in a city there are on average 5 snowfall events in a year. assume that the,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2260,  1012,  1999,  1037,  2103,  2045,  2024,  2006,  2779,
         1019, 26043,  2824,  1999,  1037,  2095,  1012,  7868,  2008,  1996,
          102])"
433,1,"['mean', 'function', 'rate', 'probability', 'events', 'poisson', 'poisson process', 'process', 'distribution', 'discrete']", Self Assessment QuestionsExercises,seg_141,"occurrence of snowfall events follows a poisson process. the number of snowfall events in t years, x, is described by the discrete cumulative distribution −νt function p(x = k) = (νt)k e with annual mean rate ν. how large is the probk! ability of no snowfall in the next year? how large is the probability of exactly 5 snowfall events in the next year?",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101, 14404,  1997, 26043,  2824,  4076,  1037, 13433, 24077,  2832,
         1012,  1996,  2193,  1997, 26043,  2824,  1999,  1056,  2086,  1010,
         1060,  1010,  2003,  2649,  2011,  1996, 16246, 23260,  4353,  1597,
        16177,  2102,  3853,  1052,  1006,  1060,  1027,  1047,  1007,  1027,
         1006,  1167,  2102,  1007,  1047,  1041,  2007,  3296,  2812,  3446,
         1167,  1012,  2129,  2312,  2003,  1996,  4013,  2497,  2243,   999,
         3754,  1997,  2053, 26043,  1999,  1996,  2279,  2095,  1029,  2129,
         2312,  2003,  1996,  9723,  1997,  3599,  1019, 26043,  2824,  1999,
         1996,  2279,  2095,  1029,   102])"
434,1,"['estimation', 'functions', 'maximum likelihood', 'method', 'uncertainties', 'observations', 'probabilistic', 'model', 'data', 'probability distribution', 'probability distributions', 'method of moments', 'probability', 'probabilistic models', 'parameters', 'distributions', 'moments', 'likelihood', 'distribution']",Chapter  Estimation and Model Building,seg_143,"lecture 8 (aim of the present lecture) the aim of the present lecture is to provide an overview of how to establish probabilistic models and to introduce the basic tools for assessing the validity of model assumptions. first, the problem of selecting appropriate probability distribution functions for the purpose of modeling uncertainties with basis in observations of uncertain phenomena is treated, and it is shown how the concept of probability paper can provide a pragmatic basis for this purpose. then, the required theory and methodology for the estimation of parameters of probability distributions based on data is introduced. to this end, the method of moments and the maximum likelihood method as well as their limitations and applications are illustrated and discussed. on the basis of the lecture it is expected that the reader will acquire knowledge and skills with regard to:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3127, 24155,  1998,  2944,  2311])","tensor([  101,  8835,  1022,  1006,  6614,  1997,  1996,  2556,  8835,  1007,
         1996,  6614,  1997,  1996,  2556,  8835,  2003,  2000,  3073,  2019,
        19184,  1997,  2129,  2000,  5323,  4013,  3676, 27965,  4588,  4275,
         1998,  2000,  8970,  1996,  3937,  5906,  2005, 20077,  1996, 16406,
         1997,  2944, 17568,  1012,  2034,  1010,  1996,  3291,  1997, 17739,
         6413,  9723,  4353,  4972,  2005,  1996,  3800,  1997, 11643,  9662,
         7368,  2007,  3978,  1999,  9420,  1997,  9662, 13352,  2003,  5845,
         1010,  1998,  2009,  2003,  3491,  2129,  1996,  4145,  1997,  9723,
         3259,  2064,  3073,  1037, 10975,  8490, 12644,  3978,  2005,  2023,
         3800,  1012,  2059,  1010,  1996,  3223,  3399,  1998, 16134,  2005,
         1996, 24155,  1997, 11709,  1997,  9723, 20611,  2241,  2006,  2951,
         2003,  3107,  1012,  2000,  2023,  2203,  1010,  1996,  4118,  1997,
         5312,  1998,  1996,  4555, 16593,  4118,  2004,  2092,  2004,  2037,
        12546,  1998,  5097,  2024,  7203,  1998,  6936,  1012,  2006,  1996,
         3978,  1997,  1996,  8835,  2009,  2003,  3517,  2008,  1996,  8068,
         2097,  9878,  3716,  1998,  4813,  2007,  7634,  2000,  1024,   102])"
435,1,"['statistical uncertainty', 'estimated', 'maximum likelihood', 'method', 'information matrix', 'probabilistic model', 'covariance matrix', 'covariance', 'probability paper', 'associated', 'sample likelihood', 'estimate', 'probabilistic', 'quantile plot', 'model', 'statistical', 'plot', 'probability distribution', 'quantile', 'method of moments', 'uncertainty', 'probability', 'information', 'parameters', 'quantiles', 'sample', 'moments', 'likelihood', 'distribution', 'variable']",Chapter  Estimation and Model Building,seg_143,• what are the steps involved in establishing a probabilistic model for a random variable? • what is a probability paper and how is it constructed? how is it related to a quantile plot? • in what regions of the probability paper is it especially important that the plotted quantiles fit a straight line? • what is the principle behind the method of moments and how can it be applied to estimate the parameters of a probability distribution? • what is the principle behind the maximum likelihood method and how can it be applied to estimate the parameters of a probability distribution? • what is sample likelihood and how is it quantified? • how can the statistical uncertainty associated with estimated distribution parameters be quantified? • what is the information matrix and how does this relate to the covariance matrix of the estimated parameters?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 0.])","tensor([ 3127, 24155,  1998,  2944,  2311])","tensor([  101,  1528,  2054,  2024,  1996,  4084,  2920,  1999,  7411,  1037,
         4013,  3676, 27965,  4588,  2944,  2005,  1037,  6721,  8023,  1029,
         1528,  2054,  2003,  1037,  9723,  3259,  1998,  2129,  2003,  2009,
         3833,  1029,  2129,  2003,  2009,  3141,  2000,  1037, 24110, 15286,
         5436,  1029,  1528,  1999,  2054,  4655,  1997,  1996,  9723,  3259,
         2003,  2009,  2926,  2590,  2008,  1996, 27347, 24110, 15286,  2015,
         4906,  1037,  3442,  2240,  1029,  1528,  2054,  2003,  1996,  6958,
         2369,  1996,  4118,  1997,  5312,  1998,  2129,  2064,  2009,  2022,
         4162,  2000, 10197,  1996, 11709,  1997,  1037,  9723,  4353,  1029,
         1528,  2054,  2003,  1996,  6958,  2369,  1996,  4555, 16593,  4118,
         1998,  2129,  2064,  2009,  2022,  4162,  2000, 10197,  1996, 11709,
         1997,  1037,  9723,  4353,  1029,  1528,  2054,  2003,  7099, 16593,
         1998,  2129,  2003,  2009, 24110,  3775, 10451,  1029,  1528,  2129,
         2064,  1996,  7778, 12503,  3378,  2007,  4358,  4353, 11709,  2022,
        24110,  3775, 10451,  1029,  1528,  2054,  2003,  1996,  2592,  8185,
         1998,  2129,  2515,  2023, 14396,  2000,  1996,  2522, 10755, 28335,
         8185,  1997,  1996,  4358, 11709,  1029,   102])"
436,1,"['risk', 'treatment', 'probabilistic', 'probabilistic models', 'statistical', 'variables']", Introduction,seg_145,an important task in risk and reliability analysis is to establish probabilistic models for the further statistical treatment of uncertain variables.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 1., 0., 0., 1., 0., 0.])",tensor([4955]),"tensor([  101,  2019,  2590,  4708,  1999,  3891,  1998, 15258,  4106,  2003,
         2000,  5323,  4013,  3676, 27965,  4588,  4275,  2005,  1996,  2582,
         7778,  3949,  1997,  9662, 10857,  1012,   102])"
437,1,"['probabilistic model', 'probabilistic modeling', 'test results', 'observations', 'probabilistic', 'model', 'statistical', 'case', 'test', 'information', 'probabilistic models', 'results', 'standardization', 'variables', 'joint']", Introduction,seg_145,"in the literature, a large number of probabilistic models for load and resistance variables may be found. for example, in the probabilistic model code developed by the joint committee on structural safety (jcss [8]) where probabilistic models may be found for the description of the strength and stiffness characteristics of steel and concrete materials, soil characteristics and for the description of load and load effects covering many engineering application areas. however, it is not always the case that an appropriate probabilistic model for the considered problem is available. moreover, in engineering fields, such as in environmental engineering and hydrology standardization of the probabilistic modeling is relatively less developed. in such situations it is necessary that methodologies and tools are readily available for the statistical assessment of frequentistic information (e.g. observations and test results) and the formulation of probabilistic models of uncertain variables.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 1., 0., 0.])",tensor([4955]),"tensor([  101,  1999,  1996,  3906,  1010,  1037,  2312,  2193,  1997,  4013,
         3676, 27965,  4588,  4275,  2005,  7170,  1998,  5012, 10857,  2089,
         2022,  2179,  1012,  2005,  2742,  1010,  1999,  1996,  4013,  3676,
        27965,  4588,  2944,  3642,  2764,  2011,  1996,  4101,  2837,  2006,
         8332,  3808,  1006, 29175,  4757,  1031,  1022,  1033,  1007,  2073,
         4013,  3676, 27965,  4588,  4275,  2089,  2022,  2179,  2005,  1996,
         6412,  1997,  1996,  3997,  1998, 10551,  2791,  6459,  1997,  3886,
         1998,  5509,  4475,  1010,  5800,  6459,  1998,  2005,  1996,  6412,
         1997,  7170,  1998,  7170,  3896,  5266,  2116,  3330,  4646,  2752,
         1012,  2174,  1010,  2009,  2003,  2025,  2467,  1996,  2553,  2008,
         2019,  6413,  4013,  3676, 27965,  4588,  2944,  2005,  1996,  2641,
         3291,  2003,  2800,  1012,  9308,  1010,  1999,  3330,  4249,  1010,
         2107,  2004,  1999,  4483,  3330,  1998, 18479,  6483, 28648,  1997,
         1996,  4013,  3676, 27965,  4588, 11643,  2003,  4659,  2625,  2764,
         1012,  1999,  2107,  8146,  2009,  2003,  4072,  2008,  4118, 20792,
         1998,  5906,  2024, 12192,  2800,  2005,  1996,  7778,  7667,  1997,
         6976,  6553,  2592,  1006,  1041,  1012,  1043,  1012,  9420,  1998,
         3231,  3463,  1007,  1998,  1996, 20219,  1997,  4013,  3676, 27965,
         4588,  4275,  1997,  9662, 10857,  1012,   102])"
438,1,"['cases', 'model building', 'bayesian', 'information', 'observations', 'probabilistic', 'probabilistic models', 'results', 'experimental', 'probabilistic model', 'model', 'subjective', 'data']", Introduction,seg_145,"in practice two situations may thus be distinguished, namely the situation where a new probabilistic model is formulated from the very beginning and the situation where an already existing probabilistic model is updated on the basis of new information, e.g. observations or experimental results. the formulation of probabilistic models may be based on data (frequentistic information) alone, but most often data is not available to the extent where this is possible. in such cases, it is usually possible to base the model building on physical arguments, experience and judgment (subjective information). if also some data are available, the subjective information may be combined with the frequentistic information and the resulting probabilistic model is in effect of a bayesian nature.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0.])",tensor([4955]),"tensor([  101,  1999,  3218,  2048,  8146,  2089,  2947,  2022,  5182,  1010,
         8419,  1996,  3663,  2073,  1037,  2047,  4013,  3676, 27965,  4588,
         2944,  2003, 19788,  2013,  1996,  2200,  2927,  1998,  1996,  3663,
         2073,  2019,  2525,  4493,  4013,  3676, 27965,  4588,  2944,  2003,
         7172,  2006,  1996,  3978,  1997,  2047,  2592,  1010,  1041,  1012,
         1043,  1012,  9420,  2030,  6388,  3463,  1012,  1996, 20219,  1997,
         4013,  3676, 27965,  4588,  4275,  2089,  2022,  2241,  2006,  2951,
         1006,  6976,  6553,  2592,  1007,  2894,  1010,  2021,  2087,  2411,
         2951,  2003,  2025,  2800,  2000,  1996,  6698,  2073,  2023,  2003,
         2825,  1012,  1999,  2107,  3572,  1010,  2009,  2003,  2788,  2825,
         2000,  2918,  1996,  2944,  2311,  2006,  3558,  9918,  1010,  3325,
         1998,  8689,  1006, 20714,  2592,  1007,  1012,  2065,  2036,  2070,
         2951,  2024,  2800,  1010,  1996, 20714,  2592,  2089,  2022,  4117,
         2007,  1996,  6976,  6553,  2592,  1998,  1996,  4525,  4013,  3676,
        27965,  4588,  2944,  2003,  1999,  3466,  1997,  1037,  3016, 25253,
         3267,  1012,   102])"
439,1,"['uncertainty', 'information', 'probabilistic', 'probabilistic model', 'model', 'subjective']", Introduction,seg_145,"it should be emphasized that on the one hand the probabilistic model should aim for simplicity. on the other hand, the model should be accurate enough to allow for including important information collected during the lifetime of the considered technical system thereby facilitating the updating of the probabilistic model. in this way, uncertainty models, which initially are based entirely on subjective information will, as new information is collected, eventually be based on objective information.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0.])",tensor([4955]),"tensor([  101,  2009,  2323,  2022, 13155,  2008,  2006,  1996,  2028,  2192,
         1996,  4013,  3676, 27965,  4588,  2944,  2323,  6614,  2005, 17839,
         1012,  2006,  1996,  2060,  2192,  1010,  1996,  2944,  2323,  2022,
         8321,  2438,  2000,  3499,  2005,  2164,  2590,  2592,  5067,  2076,
         1996,  6480,  1997,  1996,  2641,  4087,  2291,  8558, 25505,  1996,
         2039, 16616,  1997,  1996,  4013,  3676, 27965,  4588,  2944,  1012,
         1999,  2023,  2126,  1010, 12503,  4275,  1010,  2029,  3322,  2024,
         2241,  4498,  2006, 20714,  2592,  2097,  1010,  2004,  2047,  2592,
         2003,  5067,  1010,  2776,  2022,  2241,  2006,  7863,  2592,  1012,
          102])"
440,1,"['model', 'process', 'model building']", Introduction,seg_145,"in essence, the model building process consists of five steps:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])",tensor([4955]),"tensor([  101,  1999, 11305,  1010,  1996,  2944,  2311,  2832,  3774,  1997,
         2274,  4084,  1024,   102])"
441,1,"['function', 'estimation', 'data', 'statistical', 'distribution', 'model', 'distribution function', 'parameters']", Introduction,seg_145,• assessment and statistical quantification of the available data • selection of distribution function • estimation of distribution parameters • model verification • model updating,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,
        1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0.])",tensor([4955]),"tensor([  101,  1528,  7667,  1998,  7778, 24110,  3775, 10803,  1997,  1996,
         2800,  2951,  1528,  4989,  1997,  4353,  3853,  1528, 24155,  1997,
         4353, 11709,  1528,  2944, 22616,  1528,  2944,  2039, 16616,   102])"
442,1,"['function', 'distribution function', 'information', 'probabilistic', 'distributions', 'probabilistic model', 'distribution', 'model', 'subjective', 'parameters', 'data']", Introduction,seg_145,"typically, the initial choice of the model, i.e. underlying assumptions regarding distributions and parameters may be based mainly on subjective information, whereas the assessment of the parameters of the distribution function and the verification of the models is performed on the basis of the available data. the principle for establishing a probabilistic model is illustrated in fig. 5.1.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])",tensor([4955]),"tensor([  101,  4050,  1010,  1996,  3988,  3601,  1997,  1996,  2944,  1010,
         1045,  1012,  1041,  1012, 10318, 17568,  4953, 20611,  1998, 11709,
         2089,  2022,  2241,  3701,  2006, 20714,  2592,  1010,  6168,  1996,
         7667,  1997,  1996, 11709,  1997,  1996,  4353,  3853,  1998,  1996,
        22616,  1997,  1996,  4275,  2003,  2864,  2006,  1996,  3978,  1997,
         1996,  2800,  2951,  1012,  1996,  6958,  2005,  7411,  1037,  4013,
         3676, 27965,  4588,  2944,  2003,  7203,  1999, 20965,  1012,  1019,
         1012,  1015,  1012,   102])"
443,1,"['bayesian', 'information', 'probabilistic', 'probabilistic models']", Introduction,seg_145,"as the probabilistic models are based on both frequentistic information and subjective information, these are bayesian in nature.",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 1., 1., 0., 0., 0., 0.])",tensor([4955]),"tensor([  101,  2004,  1996,  4013,  3676, 27965,  4588,  4275,  2024,  2241,
         2006,  2119,  6976,  6553,  2592,  1998, 20714,  2592,  1010,  2122,
         2024,  3016, 25253,  1999,  3267,  1012,   102])"
444,1,"['random processes', 'processes', 'probabilistic', 'random', 'random variables', 'probabilistic modeling', 'variables']", Introduction,seg_145,"in the following sections only the probabilistic modeling of random variables will be considered, but the described approach applies with some extensions also to the probabilistic modeling of random processes and random fields.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,
        1., 1., 0., 1., 0., 0., 0.])",tensor([4955]),"tensor([  101,  1999,  1996,  2206,  5433,  2069,  1996,  4013,  3676, 27965,
         4588, 11643,  1997,  6721, 10857,  2097,  2022,  2641,  1010,  2021,
         1996,  2649,  3921, 12033,  2007,  2070, 14305,  2036,  2000,  1996,
         4013,  3676, 27965,  4588, 11643,  1997,  6721,  6194,  1998,  6721,
         4249,  1012,   102])"
445,1,"['function', 'information', 'random variable', 'random', 'process', 'distribution', 'stochastic process', 'distribution function', 'combination', 'variable']", Selection of Probability Distributions,seg_147,"in general the distribution function for a given random variable or stochastic process is not known and must thus be chosen on the basis of frequentistic information, physical arguments or a combination of both.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0.])","tensor([ 4989,  1997,  9723, 20611])","tensor([  101,  1999,  2236,  1996,  4353,  3853,  2005,  1037,  2445,  6721,
         8023,  2030,  2358, 11663, 20875,  2832,  2003,  2025,  2124,  1998,
         2442,  2947,  2022,  4217,  2006,  1996,  3978,  1997,  6976,  6553,
         2592,  1010,  3558,  9918,  2030,  1037,  5257,  1997,  2119,  1012,
          102])"
446,1,"['function', 'distribution', 'statistical', 'distribution function']", Selection of Probability Distributions,seg_147,a formal classical approach (described in detail in sect. 5.9) for the identification of an appropriate distribution function on the basis of statistical evidence is to:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 4989,  1997,  9723, 20611])","tensor([  101,  1037,  5337,  4556,  3921,  1006,  2649,  1999,  6987,  1999,
        17831,  1012,  1019,  1012,  1023,  1007,  2005,  1996,  8720,  1997,
         2019,  6413,  4353,  3853,  2006,  1996,  3978,  1997,  7778,  3350,
         2003,  2000,  1024,   102])"
447,1,"['data', 'estimate', 'hypothesis', 'statistical test', 'distribution', 'statistical', 'parameters', 'test']", Selection of Probability Distributions,seg_147,• postulate a hypothesis for the distribution family. • estimate the parameters for the selected distribution on the basis of statistical data. • perform a statistical test to attempt to reject the hypothesis.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        1., 0., 0.])","tensor([ 4989,  1997,  9723, 20611])","tensor([  101,  1528,  2695,  9869,  1037, 10744,  2005,  1996,  4353,  2155,
         1012,  1528, 10197,  1996, 11709,  2005,  1996,  3479,  4353,  2006,
         1996,  3978,  1997,  7778,  2951,  1012,  1528,  4685,  1037,  7778,
         3231,  2000,  3535,  2000, 15454,  1996, 10744,  1012,   102])"
448,1,"['function', 'random variable', 'hypothesis', 'random', 'process', 'distribution', 'distribution function', 'variable']", Selection of Probability Distributions,seg_147,"if it is not possible to reject the hypothesis, the selected distribution function may be considered to be appropriate for the modeling of the considered random variable. if the hypothesis is rejected, a new hypothesis must be postulated and the process repeated.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 4989,  1997,  9723, 20611])","tensor([  101,  2065,  2009,  2003,  2025,  2825,  2000, 15454,  1996, 10744,
         1010,  1996,  3479,  4353,  3853,  2089,  2022,  2641,  2000,  2022,
         6413,  2005,  1996, 11643,  1997,  1996,  2641,  6721,  8023,  1012,
         2065,  1996, 10744,  2003,  5837,  1010,  1037,  2047, 10744,  2442,
         2022,  2695,  8898,  1998,  1996,  2832,  5567,  1012,   102])"
449,1,"['information', 'statistical test', 'tests', 'statistical', 'data', 'test']", Selection of Probability Distributions,seg_147,"this procedure follows closely the classical frequentistic approach to statistical analysis. however, in many practical engineering applications this procedure has limited value. this is not only due to the fact that the amount of available data most often is too limited to form a solid basis for a statistical test, but also because the available tests applied in situations with little frequentistic information may lead to false conclusions.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 4989,  1997,  9723, 20611])","tensor([  101,  2023,  7709,  4076,  4876,  1996,  4556,  6976,  6553,  3921,
         2000,  7778,  4106,  1012,  2174,  1010,  1999,  2116,  6742,  3330,
         5097,  2023,  7709,  2038,  3132,  3643,  1012,  2023,  2003,  2025,
         2069,  2349,  2000,  1996,  2755,  2008,  1996,  3815,  1997,  2800,
         2951,  2087,  2411,  2003,  2205,  3132,  2000,  2433,  1037,  5024,
         3978,  2005,  1037,  7778,  3231,  1010,  2021,  2036,  2138,  1996,
         2800,  5852,  4162,  1999,  8146,  2007,  2210,  6976,  6553,  2592,
         2089,  2599,  2000,  6270, 15306,  1012,   102])"
450,1,"['function', 'data', 'functions', 'distribution', 'statistical', 'distribution function', 'case']", Selection of Probability Distributions,seg_147,"in practice, however, it is often the case that physical arguments can be formulated for the choice of distribution functions and statistical data are therefore merely used for the purpose of checking whether the postulated distribution function is plausible.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([ 4989,  1997,  9723, 20611])","tensor([  101,  1999,  3218,  1010,  2174,  1010,  2009,  2003,  2411,  1996,
         2553,  2008,  3558,  9918,  2064,  2022, 19788,  2005,  1996,  3601,
         1997,  4353,  4972,  1998,  7778,  2951,  2024,  3568,  6414,  2109,
         2005,  1996,  3800,  1997,  9361,  3251,  1996,  2695,  8898,  4353,
         3853,  2003, 24286,  1012,   102])"
451,1,"['function', 'random variable', 'random', 'distribution', 'distribution function', 'variable']", Selection of Probability Distributions,seg_147,a practically applicable approach for the selection of the distribution function for the modeling of a random variable is thus:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0.])","tensor([ 4989,  1997,  9723, 20611])","tensor([  101,  1037,  8134, 12711,  3921,  2005,  1996,  4989,  1997,  1996,
         4353,  3853,  2005,  1996, 11643,  1997,  1037,  6721,  8023,  2003,
         2947,  1024,   102])"
452,1,"['probability paper', 'probability', 'distribution', 'statistical']", Selection of Probability Distributions,seg_147,"• first, to consider the physical reasons why the quantity at hand may belong to one or the other distribution family; • second, to check whether the statistical evidence is in gross contradiction with the assumed distribution by using e.g. probability paper as explained in the subsequent, or if relevant, the more formal approaches given in sect. 5.9.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4989,  1997,  9723, 20611])","tensor([  101,  1528,  2034,  1010,  2000,  5136,  1996,  3558,  4436,  2339,
         1996, 11712,  2012,  2192,  2089,  7141,  2000,  2028,  2030,  1996,
         2060,  4353,  2155,  1025,  1528,  2117,  1010,  2000,  4638,  3251,
         1996,  7778,  3350,  2003,  1999,  7977, 26917,  2007,  1996,  5071,
         4353,  2011,  2478,  1041,  1012,  1043,  1012,  9723,  3259,  2004,
         4541,  1999,  1996,  4745,  1010,  2030,  2065,  7882,  1010,  1996,
         2062,  5337,  8107,  2445,  1999, 17831,  1012,  1019,  1012,  1023,
         1012,   102])"
453,1,"['probability paper', 'probability', 'probabilistic modeling', 'random variable', 'probabilistic', 'random', 'distribution', 'variable', 'probability distribution']", Model Selection by Use of Probability Paper,seg_149,"having selected a probability distribution family for the probabilistic modeling of a random variable, the probability paper is an extremely useful tool for the purpose of checking the plausibility of the selected distribution family.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  2383,  3479,  1037,  9723,  4353,  2155,  2005,  1996,  4013,
         3676, 27965,  4588, 11643,  1997,  1037,  6721,  8023,  1010,  1996,
         9723,  3259,  2003,  2019,  5186,  6179,  6994,  2005,  1996,  3800,
         1997,  9361,  1996, 20228, 20559, 13464,  1997,  1996,  3479,  4353,
         2155,  1012,   102])"
454,1,"['complement', 'function', 'probability paper', 'probability distribution function', 'probability', 'distribution', 'distribution function', 'transformation', 'probability distribution']", Model Selection by Use of Probability Paper,seg_149,a probability paper for a given distribution family is constructed such that the cumulative probability distribution function (or the complement) for that distribution family will have the shape of a straight line when plotted on the paper. a probability paper is thus constructed by a non-linear transformation of the vertical axis (y-axis).,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  1037,  9723,  3259,  2005,  1037,  2445,  4353,  2155,  2003,
         3833,  2107,  2008,  1996, 23260,  9723,  4353,  3853,  1006,  2030,
         1996, 13711,  1007,  2005,  2008,  4353,  2155,  2097,  2031,  1996,
         4338,  1997,  1037,  3442,  2240,  2043, 27347,  2006,  1996,  3259,
         1012,  1037,  9723,  3259,  2003,  2947,  3833,  2011,  1037,  2512,
         1011,  7399,  8651,  1997,  1996,  7471,  8123,  1006,  1061,  1011,
         8123,  1007,  1012,   102])"
455,1,"['function', 'cumulative distribution function', 'random variable', 'random', 'normal', 'distribution', 'distribution function', 'variable']", Model Selection by Use of Probability Paper,seg_149,for a normal distributed random variable the cumulative distribution function is given as:,tensor(1),"tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  2005,  1037,  3671,  5500,  6721,  8023,  1996, 23260,  4353,
         3853,  2003,  2445,  2004,  1024,   102])"
456,1,"['mean', 'function', 'deviation', 'probability', 'standard normal', 'random variable', 'random', 'normal', 'standard deviation', 'standard', 'variable']", Model Selection by Use of Probability Paper,seg_149,where μx and σx are the mean value and the standard deviation of the normal distributed random variable and where φ(·) is the standard normal probability distribution function. equation 5.1 can be re-written as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  2073,  1166,  2595,  1998,  1173,  2595,  2024,  1996,  2812,
         3643,  1998,  1996,  3115, 24353,  1997,  1996,  3671,  5500,  6721,
         8023,  1998,  2073,  1176,  1006,  1087,  1007,  2003,  1996,  3115,
         3671,  9723,  4353,  3853,  1012,  8522,  1019,  1012,  1015,  2064,
         2022,  2128,  1011,  2517,  2004,  1024,   102])"
457,1,"['quantile', 'mean', 'deviation', 'slope', 'plotting', 'intercept', 'random variable', 'random', 'standard deviation', 'quantile plot', 'standard', 'plot', 'variable']", Model Selection by Use of Probability Paper,seg_149,"now by plotting x against φ−1(fx(x)), see also fig. 5.2, it is seen that a straight line is obtained with the slope depending on the standard deviation of the random variable x and the intercept on with the y-axis depending on the mean value of the random variable. such a plot is sometimes called a quantile plot, see also sect. 3.3.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  2085,  2011, 20699,  1060,  2114,  1176, 27944,  1006, 23292,
         1006,  1060,  1007,  1007,  1010,  2156,  2036, 20965,  1012,  1019,
         1012,  1016,  1010,  2009,  2003,  2464,  2008,  1037,  3442,  2240,
         2003,  4663,  2007,  1996,  9663,  5834,  2006,  1996,  3115, 24353,
         1997,  1996,  6721,  8023,  1060,  1998,  1996, 19115,  2006,  2007,
         1996,  1061,  1011,  8123,  5834,  2006,  1996,  2812,  3643,  1997,
         1996,  6721,  8023,  1012,  2107,  1037,  5436,  2003,  2823,  2170,
         1037, 24110, 15286,  5436,  1010,  2156,  2036, 17831,  1012,  1017,
         1012,  1017,  1012,   102])"
458,1,"['densities', 'probability', 'linear']", Model Selection by Use of Probability Paper,seg_149,"also in fig. 5.2 the scale of the non-linear y-axis is given corresponding to the linear mapping of the observed cumulative probability densities. in probability papers, typically, only this non-linear scale is given.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  2036,  1999, 20965,  1012,  1019,  1012,  1016,  1996,  4094,
         1997,  1996,  2512,  1011,  7399,  1061,  1011,  8123,  2003,  2445,
         7978,  2000,  1996,  7399, 12375,  1997,  1996,  5159, 23260,  9723,
         7939, 24279,  1012,  1999,  9723,  4981,  1010,  4050,  1010,  2069,
         2023,  2512,  1011,  7399,  4094,  2003,  2445,  1012,   102])"
459,1,"['probability', 'normal', 'probability paper', 'graphical']", Model Selection by Use of Probability Paper,seg_149,probability papers may also be constructed graphically. in fig. 5.3 the graphical construction of a normal probability paper is illustrated.,tensor(1),"tensor([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  9723,  4981,  2089,  2036,  2022,  3833, 20477,  2135,  1012,
         1999, 20965,  1012,  1019,  1012,  1017,  1996, 20477,  2810,  1997,
         1037,  3671,  9723,  3259,  2003,  7203,  1012,   102])"
460,1,"['set', 'probability', 'probability paper']", Model Selection by Use of Probability Paper,seg_149,"various types of probability paper are readily available for use. given an ordered set of observed values x̂io = (x̂io, x̂io, . . . , x̂n",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  2536,  4127,  1997,  9723,  3259,  2024, 12192,  2800,  2005,
         2224,  1012,  2445,  2019,  3641,  2275,  1997,  5159,  5300,  8418,
         2080,  1027,  1006,  8418,  2080,  1010,  8418,  2080,  1010,  1012,
         1012,  1012,  1010,  1060,  2078,   102])"
461,1,"['function', 'cumulative distribution function', 'random variable', 'random', 'distribution', 'distribution function', 'variable']", Model Selection by Use of Probability Paper,seg_149,"o )t of a random variable, the cumulative distribution function may be evaluated as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  1051,  1007,  1056,  1997,  1037,  6721,  8023,  1010,  1996,
        23260,  4353,  3853,  2089,  2022, 16330,  2004,  1024,   102])"
462,1,"['function', 'normal distribution', 'probability paper', 'cumulative distribution function', 'table', 'set', 'probability', 'normal', 'distribution', 'distribution function', 'distribution probability']", Model Selection by Use of Probability Paper,seg_149,in table 5.1 an example is given for a set of observed concrete cube compressive strength values together with the cumulative distribution function values as calculated using eq. 5.3. in fig. 5.4 the cumulative distribution values are plotted on a normal distribution probability paper.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  1999,  2795,  1019,  1012,  1015,  2019,  2742,  2003,  2445,
         2005,  1037,  2275,  1997,  5159,  5509, 14291,  4012, 27484,  3997,
         5300,  2362,  2007,  1996, 23260,  4353,  3853,  5300,  2004, 10174,
         2478,  1041,  4160,  1012,  1019,  1012,  1017,  1012,  1999, 20965,
         1012,  1019,  1012,  1018,  1996, 23260,  4353,  5300,  2024, 27347,
         2006,  1037,  3671,  4353,  9723,  3259,  1012,   102])"
463,1,"['parameter', 'estimation', 'slope', 'estimate', 'parameters', 'distribution']", Model Selection by Use of Probability Paper,seg_149,a first estimate of the distribution parameters may be readily determined from the slope and the position of the best straight line through the plotted cumulative distribution values. in sect. 5.3 the problem of parameter estimation is considered in more detail.,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  1037,  2034, 10197,  1997,  1996,  4353, 11709,  2089,  2022,
        12192,  4340,  2013,  1996,  9663,  1998,  1996,  2597,  1997,  1996,
         2190,  3442,  2240,  2083,  1996, 27347, 23260,  4353,  5300,  1012,
         1999, 17831,  1012,  1019,  1012,  1017,  1996,  3291,  1997, 16381,
        24155,  2003,  2641,  1999,  2062,  6987,  1012,   102])"
464,1,"['tail', 'function', 'distribution', 'cumulative distribution function', 'representative', 'distribution function']", Model Selection by Use of Probability Paper,seg_149,"from fig. 5.4 it is seen that the observed cumulative distribution function fits relatively well with a straight line. this might also be expected considering that the observed values of the concrete compressive strength are not really representative for the lower tail of the distribution, where due to the non-negativity of the com-",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  2013, 20965,  1012,  1019,  1012,  1018,  2009,  2003,  2464,
         2008,  1996,  5159, 23260,  4353,  3853, 16142,  4659,  2092,  2007,
         1037,  3442,  2240,  1012,  2023,  2453,  2036,  2022,  3517,  6195,
         2008,  1996,  5159,  5300,  1997,  1996,  5509,  4012, 27484,  3997,
         2024,  2025,  2428,  4387,  2005,  1996,  2896,  5725,  1997,  1996,
         4353,  1010,  2073,  2349,  2000,  1996,  2512,  1011, 11265, 20697,
         7730,  1997,  1996,  4012,  1011,   102])"
465,1,"['lognormal', 'lognormal distribution', 'distribution']", Model Selection by Use of Probability Paper,seg_149,pressive strength it might be assumed that a lognormal distribution would be more suitable.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  2811,  3512,  3997,  2009,  2453,  2022,  5071,  2008,  1037,
         8833, 12131,  9067,  4353,  2052,  2022,  2062,  7218,  1012,   102])"
466,1,"['probability paper', 'estimation', 'range', 'probability', 'probabilities', 'extrapolation']", Model Selection by Use of Probability Paper,seg_149,the estimation of the probabilities of values which are outside the measured range can be done by extrapolation (see fig. 5.4). the probability paper may also,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  1996, 24155,  1997,  1996,  4013,  3676, 14680,  1997,  5300,
         2029,  2024,  2648,  1996,  7594,  2846,  2064,  2022,  2589,  2011,
         4469, 18155,  3370,  1006,  2156, 20965,  1012,  1019,  1012,  1018,
         1007,  1012,  1996,  9723,  3259,  2089,  2036,   102])"
467,1,"['estimate', 'level']", Model Selection by Use of Probability Paper,seg_149,"be used for extreme phenomena such as the maximum water level, to estimate the",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  2022,  2109,  2005,  6034, 13352,  2107,  2004,  1996,  4555,
         2300,  2504,  1010,  2000, 10197,  1996,   102])"
468,1,"['return period', 'level']", Model Selection by Use of Probability Paper,seg_149,values of the water level with a certain return period (see e.g. schneider [11]). how-,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  5300,  1997,  1996,  2300,  2504,  2007,  1037,  3056,  2709,
         2558,  1006,  2156,  1041,  1012,  1043,  1012, 15159,  1031,  2340,
         1033,  1007,  1012,  2129,  1011,   102])"
469,0,[], Model Selection by Use of Probability Paper,seg_149,"ever, as always when extrapolating, extreme care must be exercised.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2944, 4989, 2011, 2224, 1997, 9723, 3259])","tensor([  101,  2412,  1010,  2004,  2467,  2043,  4469, 18155,  5844,  1010,
         6034,  2729,  2442,  2022, 17747,  1012,   102])"
470,1,"['estimate', 'distribution', 'parameters']", Estimation of Distribution Parameters,seg_151,"there are, in principle, two different methods to estimate the distribution parameters",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.])","tensor([24155,  1997,  4353, 11709])","tensor([  101,  2045,  2024,  1010,  1999,  6958,  1010,  2048,  2367,  4725,
         2000, 10197,  1996,  4353, 11709,   102])"
471,1,"['estimates', 'point estimates', 'data']", Estimation of Distribution Parameters,seg_151,"on the basis of data, namely the methods of point estimates and the methods of",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([24155,  1997,  4353, 11709])","tensor([  101,  2006,  1996,  3978,  1997,  2951,  1010,  8419,  1996,  4725,
         1997,  2391, 10035,  1998,  1996,  4725,  1997,   102])"
472,1,['estimates'], Estimation of Distribution Parameters,seg_151,"interval estimates. in the following section, however, only two of the methods of",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([24155,  1997,  4353, 11709])","tensor([  101, 13483, 10035,  1012,  1999,  1996,  2206,  2930,  1010,  2174,
         1010,  2069,  2048,  1997,  1996,  4725,  1997,   102])"
473,1,"['method of moments', 'estimates', 'moments', 'method']", Estimation of Distribution Parameters,seg_151,"point estimates will be explained, namely the method of moments and the method",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.])","tensor([24155,  1997,  4353, 11709])","tensor([  101,  2391, 10035,  2097,  2022,  4541,  1010,  8419,  1996,  4118,
         1997,  5312,  1998,  1996,  4118,   102])"
474,1,"['maximum likelihood', 'likelihood', 'risk']", Estimation of Distribution Parameters,seg_151,of maximum likelihood as these have proven especially useful in practical risk and,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([24155,  1997,  4353, 11709])","tensor([  101,  1997,  4555, 16593,  2004,  2122,  2031, 10003,  2926,  6179,
         1999,  6742,  3891,  1998,   102])"
475,0,[], Estimation of Distribution Parameters,seg_151,reliability engineering analysis.,tensor(0),"tensor([0, 0, 0, 0, 0, 0])","tensor([24155,  1997,  4353, 11709])","tensor([  101, 15258,  3330,  4106,  1012,   102])"
476,1,"['method of moments', 'estimated', 'moments', 'method', 'distribution', 'parameters']", The Method of Moments,seg_153,"using the method of moments, the parameters of the distribution can be estimated",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.])","tensor([1996, 4118, 1997, 5312])","tensor([  101,  2478,  1996,  4118,  1997,  5312,  1010,  1996, 11709,  1997,
         1996,  4353,  2064,  2022,  4358,   102])"
477,1,"['moments', 'sample']", The Method of Moments,seg_153,by equating the moments obtained from the sample and the moments obtained from,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.])","tensor([1996, 4118, 1997, 5312])","tensor([  101,  2011,  1041, 16211,  3436,  1996,  5312,  4663,  2013,  1996,
         7099,  1998,  1996,  5312,  4663,  2013,   102])"
478,1,['distribution'], The Method of Moments,seg_153,the distribution.,tensor(1),"tensor([0., 0., 1., 0., 0.])","tensor([1996, 4118, 1997, 5312])","tensor([ 101, 1996, 4353, 1012,  102])"
479,1,"['random variable', 'random', 'variable']", The Method of Moments,seg_153,assuming that the considered random variable x may be modeled by the prob-,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1996, 4118, 1997, 5312])","tensor([  101, 10262,  2008,  1996,  2641,  6721,  8023,  1060,  2089,  2022,
        14440,  2011,  1996,  4013,  2497,  1011,   102])"
480,1,"['function', 'density function', 'distribution']", The Method of Moments,seg_153,"ability density function fx(x;θ), where θ = (θ1, θ2, . . . , θk)t are the distribution",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])","tensor([1996, 4118, 1997, 5312])","tensor([  101,  3754,  4304,  3853, 23292,  1006,  1060,  1025,  1162,  1007,
         1010,  2073,  1162,  1027,  1006,  1162,  2487,  1010,  1162,  2475,
         1010,  1012,  1012,  1012,  1010,  1162,  2243,  1007,  1056,  2024,
         1996,  4353,   102])"
481,1,"['random variable', 'moments', 'random', 'variable']", The Method of Moments,seg_153,"parameters, the first k moments λ = (λ1, λ2, . . . , λk)t of the random variable x",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([1996, 4118, 1997, 5312])","tensor([  101, 11709,  1010,  1996,  2034,  1047,  5312,  1165,  1027,  1006,
         1165,  2487,  1010,  1165,  2475,  1010,  1012,  1012,  1012,  1010,
         1165,  2243,  1007,  1056,  1997,  1996,  6721,  8023,  1060,   102])"
482,0,[], The Method of Moments,seg_153,may be written as:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0])","tensor([1996, 4118, 1997, 5312])","tensor([ 101, 2089, 2022, 2517, 2004, 1024,  102])"
483,1,"['random sample', 'sample', 'random', 'distribution', 'parameters']", The Method of Moments,seg_153,"if the random sample, from which the distribution parameters θ = (θ1, θ2, . . . ,",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([1996, 4118, 1997, 5312])","tensor([  101,  2065,  1996,  6721,  7099,  1010,  2013,  2029,  1996,  4353,
        11709,  1162,  1027,  1006,  1162,  2487,  1010,  1162,  2475,  1010,
         1012,  1012,  1012,  1010,   102])"
484,1,['estimated'], The Method of Moments,seg_153,"θk)t are to be estimated, is collected in the vector x̂ = (x̂1, x̂2,, . . . , x̂n)t , the corre-",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([1996, 4118, 1997, 5312])","tensor([  101,  1162,  2243,  1007,  1056,  2024,  2000,  2022,  4358,  1010,
         2003,  5067,  1999,  1996,  9207,  1060,  1027,  1006,  1060,  2487,
         1010,  1060,  2475,  1010,  1010,  1012,  1012,  1012,  1010,  1060,
         2078,  1007,  1056,  1010,  1996,  2522, 14343,  1011,   102])"
485,1,"['moments', 'sample', 'sample moments']", The Method of Moments,seg_153,sponding k sample moments may be calculated as:,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([1996, 4118, 1997, 5312])","tensor([  101, 11867, 15422,  2075,  1047,  7099,  5312,  2089,  2022, 10174,
         2004,  1024,   102])"
486,1,"['random variable', 'moments', 'sample', 'random', 'sample moments', 'variable']", The Method of Moments,seg_153,by equating the k sample moments to the k moments of the random variable,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.])","tensor([1996, 4118, 1997, 5312])","tensor([  101,  2011,  1041, 16211,  3436,  1996,  1047,  7099,  5312,  2000,
         1996,  1047,  5312,  1997,  1996,  6721,  8023,   102])"
487,1,"['set', 'distribution', 'parameters']", The Method of Moments,seg_153,"x, a set of k equations with the k unknown distribution parameters is obtained, the",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0.])","tensor([1996, 4118, 1997, 5312])","tensor([  101,  1060,  1010,  1037,  2275,  1997,  1047, 11380,  2007,  1996,
         1047,  4242,  4353, 11709,  2003,  4663,  1010,  1996,   102])"
488,1,"['estimates', 'distribution', 'point estimates', 'parameters']", The Method of Moments,seg_153,solution of which gives the point estimates of the distribution parameters.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.])","tensor([1996, 4118, 1997, 5312])","tensor([  101,  5576,  1997,  2029,  3957,  1996,  2391, 10035,  1997,  1996,
         4353, 11709,  1012,   102])"
489,1,"['method of moments', 'risk', 'moments', 'method']", The Method of Maximum Likelihood,seg_155,"this method may be somewhat more difficult to use than the method of moments but has a number of very attractive properties, which makes it especially applicable in engineering risk and reliability analysis.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0.])","tensor([ 1996,  4118,  1997,  4555, 16593])","tensor([  101,  2023,  4118,  2089,  2022,  5399,  2062,  3697,  2000,  2224,
         2084,  1996,  4118,  1997,  5312,  2021,  2038,  1037,  2193,  1997,
         2200,  8702,  5144,  1010,  2029,  3084,  2009,  2926, 12711,  1999,
         3330,  3891,  1998, 15258,  4106,  1012,   102])"
490,1,"['function', 'probability', 'random sample', 'sample', 'method', 'random', 'likelihood', 'distribution', 'distribution function', 'parameters']", The Method of Maximum Likelihood,seg_155,the principle of the method is that the parameters of the distribution function are fitted such that the probability (likelihood) of the observed random sample is maximized.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 1996,  4118,  1997,  4555, 16593])","tensor([  101,  1996,  6958,  1997,  1996,  4118,  2003,  2008,  1996, 11709,
         1997,  1996,  4353,  3853,  2024,  7130,  2107,  2008,  1996,  9723,
         1006, 16593,  1007,  1997,  1996,  5159,  6721,  7099,  2003, 25845,
         2094,  1012,   102])"
491,1,"['function', 'density function', 'probability', 'probability density function', 'random variable', 'random', 'distribution', 'parameters', 'variable']", The Method of Maximum Likelihood,seg_155,"let the random variable of interest x have a probability density function fx(x; θ) where θ = (θ1, θ2, . . . , θk)t are the distribution parameters to be estimated.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 1996,  4118,  1997,  4555, 16593])","tensor([  101,  2292,  1996,  6721,  8023,  1997,  3037,  1060,  2031,  1037,
         9723,  4304,  3853, 23292,  1006,  1060,  1025,  1162,  1007,  2073,
         1162,  1027,  1006,  1162,  2487,  1010,  1162,  2475,  1010,  1012,
         1012,  1012,  1010,  1162,  2243,  1007,  1056,  2024,  1996,  4353,
        11709,  2000,  2022,  4358,  1012,   102])"
492,1,"['estimated', 'random sample', 'sample', 'random', 'likelihood', 'distribution', 'parameters']", The Method of Maximum Likelihood,seg_155,"if the random sample from which the distribution parameters θ = (θ1, θ2, . . . , θk)t are to be estimated are collected in the vector x̂ = (x̂1, x̂2,, . . . , x̂n)t , the likelihood l(θ|x̂) of the observed random sample is defined as:",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0.])","tensor([ 1996,  4118,  1997,  4555, 16593])","tensor([  101,  2065,  1996,  6721,  7099,  2013,  2029,  1996,  4353, 11709,
         1162,  1027,  1006,  1162,  2487,  1010,  1162,  2475,  1010,  1012,
         1012,  1012,  1010,  1162,  2243,  1007,  1056,  2024,  2000,  2022,
         4358,  2024,  5067,  1999,  1996,  9207,  1060,  1027,  1006,  1060,
         2487,  1010,  1060,  2475,  1010,  1010,  1012,  1012,  1012,  1010,
         1060,  2078,  1007,  1056,  1010,  1996, 16593,  1048,  1006,  1162,
         1064,  1060,  1007,  1997,  1996,  5159,  6721,  7099,  2003,  4225,
         2004,  1024,   102])"
493,1,"['maximum likelihood', 'estimates', 'likelihood', 'point estimates', 'parameters']", The Method of Maximum Likelihood,seg_155,"the maximum likelihood point estimates of the parameters θ = (θ1, θ2, . . . , θk)t may now be obtained by solving the following optimization problem:",tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 1996,  4118,  1997,  4555, 16593])","tensor([  101,  1996,  4555, 16593,  2391, 10035,  1997,  1996, 11709,  1162,
         1027,  1006,  1162,  2487,  1010,  1162,  2475,  1010,  1012,  1012,
         1012,  1010,  1162,  2243,  1007,  1056,  2089,  2085,  2022,  4663,
         2011, 13729,  1996,  2206, 20600,  3291,  1024,   102])"
494,1,"['observations', 'likelihood', 'distribution', 'parameters']", The Method of Maximum Likelihood,seg_155,"the principle is illustrated in fig. 5.5, where it can be seen that the distribution function is shifted to maximize the likelihood of the observations by means of changing the distribution parameters.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 0.])","tensor([ 1996,  4118,  1997,  4555, 16593])","tensor([  101,  1996,  6958,  2003,  7203,  1999, 20965,  1012,  1019,  1012,
         1019,  1010,  2073,  2009,  2064,  2022,  2464,  2008,  1996,  4353,
         3853,  2003,  5429,  2000, 25845,  1996, 16593,  1997,  1996,  9420,
         2011,  2965,  1997,  5278,  1996,  4353, 11709,  1012,   102])"
495,1,"['likelihood function', 'function', 'likelihood']", The Method of Maximum Likelihood,seg_155,instead of the likelihood function it is advantageous to consider the loglikelihood l(θ |x̂) i.e.:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  4118,  1997,  4555, 16593])","tensor([  101,  2612,  1997,  1996, 16593,  3853,  2009,  2003,  5056,  3560,
         2000,  5136,  1996,  8833, 10359,  3669,  9021,  1048,  1006,  1162,
         1064,  1060,  1007,  1045,  1012,  1041,  1012,  1024,   102])"
496,1,"['mean', 'normal distribution', 'maximum likelihood', 'estimates', 'method', 'normal', 'samples', 'likelihood', 'distribution', 'point estimates']", The Method of Maximum Likelihood,seg_155,"one of the most attractive properties of the maximum likelihood method is that when the number of samples i.e. n is sufficiently large the distribution of the parameter estimates converges towards a normal distribution with mean values μθ equal to the point estimates, i.e.:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 1996,  4118,  1997,  4555, 16593])","tensor([  101,  2028,  1997,  1996,  2087,  8702,  5144,  1997,  1996,  4555,
        16593,  4118,  2003,  2008,  2043,  1996,  2193,  1997,  8168,  1045,
         1012,  1041,  1012,  1050,  2003, 12949,  2312,  1996,  4353,  1997,
         1996, 16381, 10035, 28314,  2015,  2875,  1037,  3671,  4353,  2007,
         2812,  5300,  1166, 29725,  5020,  2000,  1996,  2391, 10035,  1010,
         1045,  1012,  1041,  1012,  1024,   102])"
497,1,"['estimates', 'covariance matrix', 'point estimates', 'covariance']", The Method of Maximum Likelihood,seg_155,the covariance matrix cθθ for the point estimates may readily be obtained by:,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 1996,  4118,  1997,  4555, 16593])","tensor([  101,  1996,  2522, 10755, 28335,  8185,  1039, 29725, 29725,  2005,
         1996,  2391, 10035,  2089, 12192,  2022,  4663,  2011,  1024,   102])"
498,1,"['function', 'fisher', 'information', 'information matrix', 'fisher information matrix', 'fisher information']", The Method of Maximum Likelihood,seg_155,"where h is the fisher information matrix with components determined by the second order partial derivatives of the log-likelihood function taken in the maximum, i.e.:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  4118,  1997,  4555, 16593])","tensor([  101,  2073,  1044,  2003,  1996,  8731,  2592,  8185,  2007,  6177,
         4340,  2011,  1996,  2117,  2344,  7704, 16942,  1997,  1996,  8833,
         1011, 16593,  3853,  2579,  1999,  1996,  4555,  1010,  1045,  1012,
         1041,  1012,  1024,   102])"
499,1,"['associated', 'fisher', 'uncertainties', 'information', 'estimates', 'information matrix', 'samples', 'statistical', 'point estimates', 'fisher information matrix', 'statistical uncertainties', 'fisher information']", The Method of Maximum Likelihood,seg_155,"with the fisher information matrix the uncertainties associated with the point estimates and the statistical uncertainties are considered (with a large number of samples, the uncertainties decrease).",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([ 1996,  4118,  1997,  4555, 16593])","tensor([  101,  2007,  1996,  8731,  2592,  8185,  1996,  9662,  7368,  3378,
         2007,  1996,  2391, 10035,  1998,  1996,  7778,  9662,  7368,  2024,
         2641,  1006,  2007,  1037,  2312,  2193,  1997,  8168,  1010,  1996,
         9662,  7368,  9885,  1007,  1012,   102])"
500,1,"['table', 'estimate', 'experimental', 'results', 'normal', 'experiment', 'parameters']", Example Parameter Estimation,seg_157,"consider again the experimental results of the concrete cube compressive strength values given in table 5.1. assuming that the concrete cube compressive strength is normal distributed, it is required now to estimate the parameters on the basis of the experiment results.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  5136,  2153,  1996,  6388,  3463,  1997,  1996,  5509, 14291,
         4012, 27484,  3997,  5300,  2445,  1999,  2795,  1019,  1012,  1015,
         1012, 10262,  2008,  1996,  5509, 14291,  4012, 27484,  3997,  2003,
         3671,  5500,  1010,  2009,  2003,  3223,  2085,  2000, 10197,  1996,
        11709,  2006,  1996,  3978,  1997,  1996,  7551,  3463,  1012,   102])"
501,1,"['normal distribution', 'moments', 'normal', 'distribution']", Example Parameter Estimation,seg_157,"with eqs. 4.5, 4.7 and 4.17 it can be shown that the first two moments of the normal distribution are given as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([ 101, 2007, 1041, 4160, 2015, 1012, 1018, 1012, 1019, 1010, 1018, 1012,
        1021, 1998, 1018, 1012, 2459, 2009, 2064, 2022, 3491, 2008, 1996, 2034,
        2048, 5312, 1997, 1996, 3671, 4353, 2024, 2445, 2004, 1024,  102])"
502,1,"['moments', 'sample', 'sample moments', 'data']", Example Parameter Estimation,seg_157,"analyzing the sample data, the first two sample moments can be found using the following equation:",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101, 20253,  1996,  7099,  2951,  1010,  1996,  2034,  2048,  7099,
         5312,  2064,  2022,  2179,  2478,  1996,  2206,  8522,  1024,   102])"
503,1,"['estimates', 'point estimates', 'parameters']", Example Parameter Estimation,seg_157,the point estimates of the parameters μ and σ may now be determined by solving the equations:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  1996,  2391, 10035,  1997,  1996, 11709,  1166,  1998,  1173,
         2089,  2085,  2022,  4340,  2011, 13729,  1996, 11380,  1024,   102])"
504,1,"['method of maximum likelihood', 'function', 'maximum likelihood', 'likelihood function', 'method', 'likelihood']", Example Parameter Estimation,seg_157,"using the method of maximum likelihood, the maximum likelihood function is readily written as:",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  2478,  1996,  4118,  1997,  4555, 16593,  1010,  1996,  4555,
        16593,  3853,  2003, 12192,  2517,  2004,  1024,   102])"
505,1,['function'], Example Parameter Estimation,seg_157,and correspondingly the log-likelihood function as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  1998,  7978,  2135,  1996,  8833,  1011, 16593,  3853,  2004,
         1024,   102])"
506,1,"['observations', 'number of observations']", Example Parameter Estimation,seg_157,"where n is the number of observations of the concrete cube compressive strength x̂ = (x̂1, x̂2,, . . . , x̂n)t .",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  2073,  1050,  2003,  1996,  2193,  1997,  9420,  1997,  1996,
         5509, 14291,  4012, 27484,  3997,  1060,  1027,  1006,  1060,  2487,
         1010,  1060,  2475,  1010,  1010,  1012,  1012,  1012,  1010,  1060,
         2078,  1007,  1056,  1012,   102])"
507,1,"['mean', 'estimates']", Example Parameter Estimation,seg_157,the mean values of the estimates may be determined by solving the following equations:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  1996,  2812,  5300,  1997,  1996, 10035,  2089,  2022,  4340,
         2011, 13729,  1996,  2206, 11380,  1024,   102])"
508,1,"['sample', 'data']", Example Parameter Estimation,seg_157,"which, by using the sample data gives:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([ 101, 2029, 1010, 2011, 2478, 1996, 7099, 2951, 3957, 1024,  102])"
509,1,"['method of moments', 'moments', 'method']", Example Parameter Estimation,seg_157,"not surprisingly, the same result as that obtained using the method of moments is obtained here.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  2025, 10889,  1010,  1996,  2168,  2765,  2004,  2008,  4663,
         2478,  1996,  4118,  1997,  5312,  2003,  4663,  2182,  1012,   102])"
510,1,"['covariance matrix', 'parameters', 'covariance']", Example Parameter Estimation,seg_157,"as mentioned previously, the covariance matrix cθθ for the parameters esti-",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  2004,  3855,  3130,  1010,  1996,  2522, 10755, 28335,  8185,
         1039, 29725, 29725,  2005,  1996, 11709,  9765,  2072,  1011,   102])"
511,1,"['information', 'information matrix']", Example Parameter Estimation,seg_157,mates may be determined through the information matrix h containing the second-,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101, 14711,  2089,  2022,  4340,  2083,  1996,  2592,  8185,  1044,
         4820,  1996,  2117,  1011,   102])"
512,1,"['information', 'function']", Example Parameter Estimation,seg_157,"order partial derivatives of the log-likelihood function, see eq. 5.17. the information",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  2344,  7704, 16942,  1997,  1996,  8833,  1011, 16593,  3853,
         1010,  2156,  1041,  4160,  1012,  1019,  1012,  2459,  1012,  1996,
         2592,   102])"
513,0,[], Example Parameter Estimation,seg_157,matrix may be found to be:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2742, 16381, 24155])","tensor([ 101, 8185, 2089, 2022, 2179, 2000, 2022, 1024,  102])"
514,1,"['sample', 'covariance matrix', 'covariance', 'data']", Example Parameter Estimation,seg_157,whereby the covariance matrix is evaluated using the sample data as:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101, 13557,  1996,  2522, 10755, 28335,  8185,  2003, 16330,  2478,
         1996,  7099,  2951,  2004,  1024,   102])"
515,1,"['probabilistic', 'probabilistic modeling']", Example Parameter Estimation,seg_157,"in probabilistic modeling, where the concrete cube compressive strength enters",tensor(1),"tensor([0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  1999,  4013,  3676, 27965,  4588, 11643,  1010,  2073,  1996,
         5509, 14291,  4012, 27484,  3997,  8039,   102])"
516,1,"['random variable', 'random', 'statistical', 'variable']", Example Parameter Estimation,seg_157,"as a random variable, it is then possible to take into account the statistical uncer-",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  2004,  1037,  6721,  8023,  1010,  2009,  2003,  2059,  2825,
         2000,  2202,  2046,  4070,  1996,  7778,  4895, 17119,  1011,   102])"
517,1,"['associated', 'estimates', 'distribution', 'parameters']", Example Parameter Estimation,seg_157,tainty associated with the estimates of the distribution parameters for the distri-,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101, 13843, 29405,  3378,  2007,  1996, 10035,  1997,  1996,  4353,
        11709,  2005,  1996,  4487,  3367,  3089,  1011,   102])"
518,1,"['function', 'distribution', 'parameters']", Example Parameter Estimation,seg_157,"bution function, simply by including the distribution parameters in the reliability",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  2021,  3258,  3853,  1010,  3432,  2011,  2164,  1996,  4353,
        11709,  1999,  1996, 15258,   102])"
519,1,"['mean', 'variables', 'normal']", Example Parameter Estimation,seg_157,analysis as normal distributed variables with the evaluated mean values and covari-,tensor(1),"tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  4106,  2004,  3671,  5500, 10857,  2007,  1996, 16330,  2812,
         5300,  1998,  2522, 10755,  2072,  1011,   102])"
520,1,"['bayesian regression analysis', 'regression', 'estimation', 'regression line', 'regression analysis', 'bayesian', 'probabilistic', 'random', 'random variables', 'probabilistic model', 'model', 'variables', 'data']", Example Parameter Estimation,seg_157,lecture 9 (aim of the present lecture) this lecture introduces bayesian estimation methods. the use of these methods makes it possible to update an existing probabilistic model with newly available data. bayesian regression analysis makes use of this principle. the basic principle here is that the relation between two random variables is represented through a regression line which can be updated based on new data. on the basis of the lecture it is expected that the reader will acquire knowledge and skills with regard to:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  8835,  1023,  1006,  6614,  1997,  1996,  2556,  8835,  1007,
         2023,  8835, 13999,  3016, 25253, 24155,  4725,  1012,  1996,  2224,
         1997,  2122,  4725,  3084,  2009,  2825,  2000, 10651,  2019,  4493,
         4013,  3676, 27965,  4588,  2944,  2007,  4397,  2800,  2951,  1012,
         3016, 25253, 26237,  4106,  3084,  2224,  1997,  2023,  6958,  1012,
         1996,  3937,  6958,  2182,  2003,  2008,  1996,  7189,  2090,  2048,
         6721, 10857,  2003,  3421,  2083,  1037, 26237,  2240,  2029,  2064,
         2022,  7172,  2241,  2006,  2047,  2951,  1012,  2006,  1996,  3978,
         1997,  1996,  8835,  2009,  2003,  3517,  2008,  1996,  8068,  2097,
         9878,  3716,  1998,  4813,  2007,  7634,  2000,  1024,   102])"
521,1,"['bayesian regression analysis', 'regression', 'estimation', 'regression analysis', 'method', 'linear regression', 'regression model', 'prior distribution', 'uncertainties', 'bayesian', 'random variable', 'probabilistic', 'least squares method', 'model', 'data', 'linear', 'information', 'random', 'distribution', 'least squares', 'variable']", Example Parameter Estimation,seg_157,• which kind of information is typically contained in an a priori model of the probabilistic distribution of a random variable? • how is the information from new data and the a priori information weighted in bayesian estimation methods? • what is a conjugated prior distribution? • what are the basic assumptions made in linear regression? • what principle is behind the least squares method? • how are the uncertainties quantified in a regression model? • how can a regression model be updated in line with bayesian regression analysis?,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1.,
        0., 0.])","tensor([ 2742, 16381, 24155])","tensor([  101,  1528,  2029,  2785,  1997,  2592,  2003,  4050,  4838,  1999,
         2019,  1037,  3188,  2072,  2944,  1997,  1996,  4013,  3676, 27965,
         4588,  4353,  1997,  1037,  6721,  8023,  1029,  1528,  2129,  2003,
         1996,  2592,  2013,  2047,  2951,  1998,  1996,  1037,  3188,  2072,
         2592, 18215,  1999,  3016, 25253, 24155,  4725,  1029,  1528,  2054,
         2003,  1037,  9530,  9103, 11644,  3188,  4353,  1029,  1528,  2054,
         2024,  1996,  3937, 17568,  2081,  1999,  7399, 26237,  1029,  1528,
         2054,  6958,  2003,  2369,  1996,  2560, 14320,  4118,  1029,  1528,
         2129,  2024,  1996,  9662,  7368, 24110,  3775, 10451,  1999,  1037,
        26237,  2944,  1029,  1528,  2129,  2064,  1037, 26237,  2944,  2022,
         7172,  1999,  2240,  2007,  3016, 25253, 26237,  4106,  1029,   102])"
522,1,"['estimated', 'probabilistic model', 'confidence', 'risk', 'random variable', 'probabilistic', 'model', 'data', 'function', 'results', 'parameters', 'random', 'distribution', 'distribution function', 'variable']", Bayesian Estimation Methods,seg_159,"a typical situation in risk and reliability analysis is that a prior probabilistic model for a random variable is available, e.g. a distribution function and its corresponding distribution parameters have been chosen and estimated based on previous experimental results, experience and professional judgment. as soon as additional data becomes available, it is desired to update the distribution parameters of the prior probabilistic model on the basis of this data, weighing the confidence in the prior model consistently with the evidence provided by the new data.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101,  1037,  5171,  3663,  1999,  3891,  1998, 15258,  4106,  2003,
         2008,  1037,  3188,  4013,  3676, 27965,  4588,  2944,  2005,  1037,
         6721,  8023,  2003,  2800,  1010,  1041,  1012,  1043,  1012,  1037,
         4353,  3853,  1998,  2049,  7978,  4353, 11709,  2031,  2042,  4217,
         1998,  4358,  2241,  2006,  3025,  6388,  3463,  1010,  3325,  1998,
         2658,  8689,  1012,  2004,  2574,  2004,  3176,  2951,  4150,  2800,
         1010,  2009,  2003,  9059,  2000, 10651,  1996,  4353, 11709,  1997,
         1996,  3188,  4013,  3676, 27965,  4588,  2944,  2006,  1996,  3978,
         1997,  2023,  2951,  1010, 15243,  1996,  7023,  1999,  1996,  3188,
         2944, 10862,  2007,  1996,  3350,  3024,  2011,  1996,  2047,  2951,
         1012,   102])"
523,1,"['function', 'density function', 'random variable', 'random', 'distribution', 'parameters', 'variable']", Bayesian Estimation Methods,seg_159,"consider a random variable x with density function fx(x). if θ denotes a vector of parameters defining its distribution, the density function of the random variable x can be written as:",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101,  5136,  1037,  6721,  8023,  1060,  2007,  4304,  3853, 23292,
         1006,  1060,  1007,  1012,  2065,  1162, 14796,  1037,  9207,  1997,
        11709, 12854,  2049,  4353,  1010,  1996,  4304,  3853,  1997,  1996,
         6721,  8023,  1060,  2064,  2022,  2517,  2004,  1024,   102])"
524,1,"['mean', 'normal', 'standard']", Bayesian Estimation Methods,seg_159,"if x is normal distributed, then θ would contain the mean and the standard deviation of x.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101,  2065,  1060,  2003,  3671,  5500,  1010,  2059,  1162,  2052,
         5383,  1996,  2812,  1998,  1996,  3115, 24353,  1997,  1060,  1012,
          102])"
525,1,"['function', 'density function', 'probability', 'probability density function', 'realization', 'random variable', 'random', 'parameters', 'conditional', 'variable']", Bayesian Estimation Methods,seg_159,"if the parameters are uncertain, fx(x, θ) can be considered as a conditional density function: fx(x|θ). θ denotes a realization of the random variable θ . the initial probability density function for the parameters θ is fθ",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101,  2065,  1996, 11709,  2024,  9662,  1010, 23292,  1006,  1060,
         1010,  1162,  1007,  2064,  2022,  2641,  2004,  1037, 18462,  4304,
         3853,  1024, 23292,  1006,  1060,  1064,  1162,  1007,  1012,  1162,
        14796,  1037, 12393,  1997,  1996,  6721,  8023,  1162,  1012,  1996,
         3988,  9723,  4304,  3853,  2005,  1996, 11709,  1162,  2003,  1042,
        29725,   102])"
526,1,['function'], Bayesian Estimation Methods,seg_159,density function.,tensor(1),"tensor([0., 0., 1., 0., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([ 101, 4304, 3853, 1012,  102])"
527,1,"['observations', 'random variable', 'random', 'variable']", Bayesian Estimation Methods,seg_159,it is assumed that n observations (realizations) of the random variable x are,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101,  2009,  2003,  5071,  2008,  1050,  9420,  1006, 12393,  2015,
         1007,  1997,  1996,  6721,  8023,  1060,  2024,   102])"
528,1,"['function', 'density function', 'sample', 'independent']", Bayesian Estimation Methods,seg_159,"available, thereby building up the sample x̂ = (x̂1, x̂2, . . . , x̂n)t . the realizations are assumed to be independent. the updated density function fθ",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 0., 0., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101,  2800,  1010,  8558,  2311,  2039,  1996,  7099,  1060,  1027,
         1006,  1060,  2487,  1010,  1060,  2475,  1010,  1012,  1012,  1012,
         1010,  1060,  2078,  1007,  1056,  1012,  1996, 12393,  2015,  2024,
         5071,  2000,  2022,  2981,  1012,  1996,  7172,  4304,  3853,  1042,
        29725,   102])"
529,1,"['function', 'density function', 'posterior', 'posterior density function']", Bayesian Estimation Methods,seg_159,"parameters θ , given the realizations x̂, is denoted the posterior density function and is given by (see jcss [8]):",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101, 11709,  1162,  1010,  2445,  1996, 12393,  2015,  1060,  1010,
         2003, 19537,  1996, 15219,  4304,  3853,  1998,  2003,  2445,  2011,
         1006,  2156, 29175,  4757,  1031,  1022,  1033,  1007,  1024,   102])"
530,1,"['function', 'density function', 'predictive density function', 'realization', 'random variable', 'random', 'likelihood', 'distribution', 'parameters', 'variable']", Bayesian Estimation Methods,seg_159,where l(x̂|θ) =∏in=1 fx(x̂i | θ) is the likelihood corresponding to the given observations assuming that the distribution parameters are θ . the integration in eq. 5.23 is over all possible realizations of θ . the updated density function of the random variable x given the realization x̂ is denoted the predictive density function and is defined by:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101,  2073,  1048,  1006,  1060,  1064,  1162,  1007,  1027,   100,
         1027,  1015, 23292,  1006,  8418,  1064,  1162,  1007,  2003,  1996,
        16593,  7978,  2000,  1996,  2445,  9420, 10262,  2008,  1996,  4353,
        11709,  2024,  1162,  1012,  1996,  8346,  1999,  1041,  4160,  1012,
         1019,  1012,  2603,  2003,  2058,  2035,  2825, 12393,  2015,  1997,
         1162,  1012,  1996,  7172,  4304,  3853,  1997,  1996,  6721,  8023,
         1060,  2445,  1996, 12393,  1060,  2003, 19537,  1996, 16014,  3512,
         4304,  3853,  1998,  2003,  4225,  2011,  1024,   102])"
531,1,"['function', 'posterior', 'functions', 'random variable', 'posterior distribution', 'random', 'distribution', 'distribution function', 'prior distribution', 'variable']", Bayesian Estimation Methods,seg_159,"given the distribution function for the random variable x, the prior distribution is often chosen such that the posterior distribution will be of the same type as the prior distribution (conjugated prior). in the literature, a number of prior, posterior and predictive distribution functions can be found, see e.g. jcss [8]. analytical solutions concerned with the following problems can be found for:",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101,  2445,  1996,  4353,  3853,  2005,  1996,  6721,  8023,  1060,
         1010,  1996,  3188,  4353,  2003,  2411,  4217,  2107,  2008,  1996,
        15219,  4353,  2097,  2022,  1997,  1996,  2168,  2828,  2004,  1996,
         3188,  4353,  1006,  9530,  9103, 11644,  3188,  1007,  1012,  1999,
         1996,  3906,  1010,  1037,  2193,  1997,  3188,  1010, 15219,  1998,
        16014,  3512,  4353,  4972,  2064,  2022,  2179,  1010,  2156,  1041,
         1012,  1043,  1012, 29175,  4757,  1031,  1022,  1033,  1012, 17826,
         7300,  4986,  2007,  1996,  2206,  3471,  2064,  2022,  2179,  2005,
         1024,   102])"
532,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution']", Bayesian Estimation Methods,seg_159,• normal distribution with unknown mean • normal distribution with unknown standard deviation • normal distribution with unknown mean and standard deviation,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0.,
        0., 1., 0., 1., 1., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101,  1528,  3671,  4353,  2007,  4242,  2812,  1528,  3671,  4353,
         2007,  4242,  3115, 24353,  1528,  3671,  4353,  2007,  4242,  2812,
         1998,  3115, 24353,   102])"
533,1,"['exponential distribution', 'gumbel', 'normal distribution', 'bernoulli', 'poisson', 'weibull', 'normal', 'gumbel distribution', 'poisson distribution', 'weibull distribution', 'standard deviations', 'standard', 'bernoulli distribution', 'deviations', 'distribution', 'exponential']", Bayesian Estimation Methods,seg_159,• gumbel distribution • weibull distribution • exponential distribution • bernoulli distribution • poisson distribution • multidimensional normal distribution with unknown means • multidimensional normal distribution with unknown standard deviations • multidimensional normal distribution with unknown means and standard deviations,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 1., 1., 1., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101,  1528, 16031,  8671,  4353,  1528, 11417,  8569,  3363,  4353,
         1528, 27258,  4353,  1528, 16595,  7140,  6894,  4353,  1528, 13433,
        24077,  4353,  1528,  4800, 22172,  6132, 19301,  3671,  4353,  2007,
         4242,  2965,  1528,  4800, 22172,  6132, 19301,  3671,  4353,  2007,
         4242,  3115, 24353,  2015,  1528,  4800, 22172,  6132, 19301,  3671,
         4353,  2007,  4242,  2965,  1998,  3115, 24353,  2015,   102])"
534,1,"['parameters', 'prior distribution', 'distribution']", Bayesian Estimation Methods,seg_159,the parameters in the prior distribution can be chosen or calculated in such a way that the prior reflects:,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101,  1996, 11709,  1999,  1996,  3188,  4353,  2064,  2022,  4217,
         2030, 10174,  1999,  2107,  1037,  2126,  2008,  1996,  3188, 11138,
         1024,   102])"
535,1,"['prior distribution', 'observations', 'random variable', 'estimates', 'random', 'distribution', 'subjective', 'parameters', 'variable']", Bayesian Estimation Methods,seg_159,"• the known (initial) observations of the random variable x from which estimates of the parameters in the prior distribution can be calculated, and/or • the subjective knowledge on the distribution of the parameters θ in the distribution of x.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101,  1528,  1996,  2124,  1006,  3988,  1007,  9420,  1997,  1996,
         6721,  8023,  1060,  2013,  2029, 10035,  1997,  1996, 11709,  1999,
         1996,  3188,  4353,  2064,  2022, 10174,  1010,  1998,  1013,  2030,
         1528,  1996, 20714,  3716,  2006,  1996,  4353,  1997,  1996, 11709,
         1162,  1999,  1996,  4353,  1997,  1060,  1012,   102])"
536,1,"['range', 'prior distribution', 'deviation', 'standard deviation', 'standard', 'distribution', 'parameters']", Bayesian Estimation Methods,seg_159,"in this way, it is possible to choose a prior distribution reflecting a range of situations from a very good prior knowledge of the parameters (small standard deviation) to almost no knowledge of the parameters (large standard deviation).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 3016, 25253, 24155,  4725])","tensor([  101,  1999,  2023,  2126,  1010,  2009,  2003,  2825,  2000,  5454,
         1037,  3188,  4353, 10842,  1037,  2846,  1997,  8146,  2013,  1037,
         2200,  2204,  3188,  3716,  1997,  1996, 11709,  1006,  2235,  3115,
        24353,  1007,  2000,  2471,  2053,  3716,  1997,  1996, 11709,  1006,
         2312,  3115, 24353,  1007,  1012,   102])"
537,1,"['mean', 'deviation', 'probabilistic', 'random', 'normal', 'standard deviation', 'standard', 'random variables', 'probabilistic model', 'model', 'variables']", Example Yield Stress of a Steel Bar,seg_161,as an example on the updating of random variables consider the probabilistic modeling of the yield stress of steel. the prior probabilistic model for the yield stress is assumed to be normal distributed with known (deterministic) standard deviation σfy = 17.5 mpa and uncertain mean value μfy . the mean value μfy is assumed to be normal distributed with a mean value μ′ = 350 mpa and standard deviation σ ′ = 10 mpa.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 10750,  6911,  1997,  1037,  3886,  3347])","tensor([  101,  2004,  2019,  2742,  2006,  1996,  2039, 16616,  1997,  6721,
        10857,  5136,  1996,  4013,  3676, 27965,  4588, 11643,  1997,  1996,
        10750,  6911,  1997,  3886,  1012,  1996,  3188,  4013,  3676, 27965,
         4588,  2944,  2005,  1996, 10750,  6911,  2003,  5071,  2000,  2022,
         3671,  5500,  2007,  2124,  1006, 28283, 25300, 10074,  1007,  3115,
        24353,  1173, 12031,  1027,  2459,  1012,  1019,  6131,  2050,  1998,
         9662,  2812,  3643,  1166, 12031,  1012,  1996,  2812,  3643,  1166,
        12031,  2003,  5071,  2000,  2022,  3671,  5500,  2007,  1037,  2812,
         3643,  1166,  1531,  1027,  8698,  6131,  2050,  1998,  3115, 24353,
         1173,  1531,  1027,  2184,  6131,  2050,  1012,   102])"
538,1,"['tests', 'results', 'samples']", Example Yield Stress of a Steel Bar,seg_161,assume now that yield stress tests are performed on 5 steel samples taken from a batch of the same steel material. the results of the yield stress tests are f̂y =,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 2742, 10750,  6911,  1997,  1037,  3886,  3347])","tensor([  101,  7868,  2085,  2008, 10750,  6911,  5852,  2024,  2864,  2006,
         1019,  3886,  8168,  2579,  2013,  1037, 14108,  1997,  1996,  2168,
         3886,  3430,  1012,  1996,  3463,  1997,  1996, 10750,  6911,  5852,
         2024,  1042,  2100,  1027,   102])"
539,1,"['mean', 'probability distributions', 'probability', 'test', 'probabilistic', 'distributions', 'results', 'probabilistic model', 'model', 'test results']", Example Yield Stress of a Steel Bar,seg_161,"based on the test results, the prior probabilistic model for the mean value of the yield stress can be updated using natural conjugate probability distributions as mentioned earlier.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0.])","tensor([ 2742, 10750,  6911,  1997,  1037,  3886,  3347])","tensor([  101,  2241,  2006,  1996,  3231,  3463,  1010,  1996,  3188,  4013,
         3676, 27965,  4588,  2944,  2005,  1996,  2812,  3643,  1997,  1996,
        10750,  6911,  2064,  2022,  7172,  2478,  3019,  9530,  9103,  5867,
         9723, 20611,  2004,  3855,  3041,  1012,   102])"
540,1,"['mean', 'deviation', 'posterior', 'probability', 'random variable', 'random', 'normal', 'standard deviation', 'standard', 'posterior probability', 'variable', 'case']", Example Yield Stress of a Steel Bar,seg_161,"in the case considered with a normal distributed random variable with uncertain mean and known standard deviation, the posterior probability density as given in eq. 5.23 may be reduced to (jcss [8])",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 10750,  6911,  1997,  1037,  3886,  3347])","tensor([  101,  1999,  1996,  2553,  2641,  2007,  1037,  3671,  5500,  6721,
         8023,  2007,  9662,  2812,  1998,  2124,  3115, 24353,  1010,  1996,
        15219,  9723,  4304,  2004,  2445,  1999,  1041,  4160,  1012,  1019,
         1012,  2603,  2089,  2022,  4359,  2000,  1006, 29175,  4757,  1031,
         1022,  1033,  1007,   102])"
541,1,"['mean', 'observations', 'sample', 'sample size', 'distribution', 'prior distribution', 'sample mean']", Example Yield Stress of a Steel Bar,seg_161,"x̄ is the sample mean of the observations, n′ is the sample size assumed for the prior distribution of μf and n is the sample size for the new sample. in the present",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0.])","tensor([ 2742, 10750,  6911,  1997,  1037,  3886,  3347])","tensor([ 101, 1060, 2003, 1996, 7099, 2812, 1997, 1996, 9420, 1010, 1050, 1531,
        2003, 1996, 7099, 2946, 5071, 2005, 1996, 3188, 4353, 1997, 1166, 2546,
        1998, 1050, 2003, 1996, 7099, 2946, 2005, 1996, 2047, 7099, 1012, 1999,
        1996, 2556,  102])"
542,1,"['posterior', 'observations', 'parameters']", Example Yield Stress of a Steel Bar,seg_161,"y example n′ = 3.06. based on the new observations, the posterior parameters are μ′′ = 353.22 and σ ′′ = 6.16.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2742, 10750,  6911,  1997,  1037,  3886,  3347])","tensor([  101,  1061,  2742,  1050,  1531,  1027,  1017,  1012,  5757,  1012,
         2241,  2006,  1996,  2047,  9420,  1010,  1996, 15219, 11709,  2024,
         1166,  1531,  1531,  1027,  3486,  2509,  1012,  2570,  1998,  1173,
         1531,  1531,  1027,  1020,  1012,  2385,  1012,   102])"
543,1,"['probability density functions', 'posterior', 'probability', 'functions', 'density functions', 'posterior probability']", Example Yield Stress of a Steel Bar,seg_161,figure 5.6 illustrates the prior and the posterior probability density functions for μfy .,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        0.])","tensor([ 2742, 10750,  6911,  1997,  1037,  3886,  3347])","tensor([  101,  3275,  1019,  1012,  1020, 24899,  1996,  3188,  1998,  1996,
        15219,  9723,  4304,  4972,  2005,  1166, 12031,  1012,   102])"
544,1,"['likelihood', 'observation']", Example Yield Stress of a Steel Bar,seg_161,the likelihood of the observation can be established as:,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 10750,  6911,  1997,  1037,  3886,  3347])","tensor([  101,  1996, 16593,  1997,  1996,  8089,  2064,  2022,  2511,  2004,
         1024,   102])"
545,1,"['function', 'likelihood function', 'results', 'likelihood', 'test results', 'test']", Example Yield Stress of a Steel Bar,seg_161,the likelihood function is also shown in fig. 5.6. it is seen from fig. 5.6 that the effect of the test results can be quite significant. according to jcss [8] the predictive,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 10750,  6911,  1997,  1037,  3886,  3347])","tensor([  101,  1996, 16593,  3853,  2003,  2036,  3491,  1999, 20965,  1012,
         1019,  1012,  1020,  1012,  2009,  2003,  2464,  2013, 20965,  1012,
         1019,  1012,  1020,  2008,  1996,  3466,  1997,  1996,  3231,  3463,
         2064,  2022,  3243,  3278,  1012,  2429,  2000, 29175,  4757,  1031,
         1022,  1033,  1996, 16014,  3512,   102])"
546,1,"['function', 'density function']", Example Yield Stress of a Steel Bar,seg_161,probability density function for the steel yield stress may be determined as:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 10750,  6911,  1997,  1037,  3886,  3347])","tensor([  101,  9723,  4304,  3853,  2005,  1996,  3886, 10750,  6911,  2089,
         2022,  4340,  2004,  1024,   102])"
547,1,"['mean', 'function', 'probability distribution function', 'probability', 'information', 'distribution', 'distribution function', 'probability distribution']", Example Yield Stress of a Steel Bar,seg_161,in fig. 5.7 the predictive probability distribution and the probability distribution function for the steel yield stress based on the prior information of the mean value are shown.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 2742, 10750,  6911,  1997,  1037,  3886,  3347])","tensor([  101,  1999, 20965,  1012,  1019,  1012,  1021,  1996, 16014,  3512,
         9723,  4353,  1998,  1996,  9723,  4353,  3853,  2005,  1996,  3886,
        10750,  6911,  2241,  2006,  1996,  3188,  2592,  1997,  1996,  2812,
         3643,  2024,  3491,  1012,   102])"
548,1,"['quantile', 'bayesian', 'characteristic value', 'process']", Example Yield Stress of a Steel Bar,seg_161,"the 5% quantile, which is a typical characteristic value for the steel yield stress, has changed from 317 mpa to 322 mpa as a result of the bayesian updating process (see fig. 5.7).",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 10750,  6911,  1997,  1037,  3886,  3347])","tensor([  101,  1996,  1019,  1003, 24110, 15286,  1010,  2029,  2003,  1037,
         5171,  8281,  3643,  2005,  1996,  3886, 10750,  6911,  1010,  2038,
         2904,  2013, 26628,  6131,  2050,  2000, 23768,  6131,  2050,  2004,
         1037,  2765,  1997,  1996,  3016, 25253,  2039, 16616,  2832,  1006,
         2156, 20965,  1012,  1019,  1012,  1021,  1007,  1012,   102])"
549,1,"['function', 'regression', 'linear', 'regression analysis', 'random variable', 'probabilistic', 'experimental', 'random', 'probabilistic model', 'model', 'variable', 'case']", Bayesian Regression Analysis,seg_163,"bayesian regression analysis is a very useful modeling tool in engineering where (semi-)empirical relationships based on experimental evidence play a big role. in this section, only the special, but in practice useful, case of univariate linear models is considered. a probabilistic model for a random variable y is to be developed where y is a linear function of a random variable x. the relationship between x and y can be expressed as follows:",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 3016, 25253, 26237,  4106])","tensor([  101,  3016, 25253, 26237,  4106,  2003,  1037,  2200,  6179, 11643,
         6994,  1999,  3330,  2073,  1006,  4100,  1011,  1007, 17537,  6550,
         2241,  2006,  6388,  3350,  2377,  1037,  2502,  2535,  1012,  1999,
         2023,  2930,  1010,  2069,  1996,  2569,  1010,  2021,  1999,  3218,
         6179,  1010,  2553,  1997,  4895, 28739, 13143,  7399,  4275,  2003,
         2641,  1012,  1037,  4013,  3676, 27965,  4588,  2944,  2005,  1037,
         6721,  8023,  1061,  2003,  2000,  2022,  2764,  2073,  1061,  2003,
         1037,  7399,  3853,  1997,  1037,  6721,  8023,  1060,  1012,  1996,
         3276,  2090,  1060,  1998,  1061,  2064,  2022,  5228,  2004,  4076,
         1024,   102])"
550,1,"['prior distribution', 'test results', 'mean', 'normal', 'standard deviation', 'model', 'test', 'residual', 'deviation', 'realization', 'results', 'standard', 'parameters', 'experiment', 'distribution', 'measurements']", Bayesian Regression Analysis,seg_163,"where β0 and β1 are model parameters, i ∈ {1,2, . . . , n} is the index of test result, and εi is a realization of the residual ε. it is assumed that the residual ε is normal distributed with zero mean and standard deviation σε . hence it is seen that y for given x is also normal distributed. it is assumed that n experiment or test results are available (x̂, ŷ)t . the first step involves assessing the prior distribution of the model parameters. in the next step, the model is updated with new measurements.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 3016, 25253, 26237,  4106])","tensor([  101,  2073,  1156,  2692,  1998,  1156,  2487,  2024,  2944, 11709,
         1010,  1045,  1596,  1063,  1015,  1010,  1016,  1010,  1012,  1012,
         1012,  1010,  1050,  1065,  2003,  1996,  5950,  1997,  3231,  2765,
         1010,  1998,  1159,  2072,  2003,  1037, 12393,  1997,  1996, 21961,
         1159,  1012,  2009,  2003,  5071,  2008,  1996, 21961,  1159,  2003,
         3671,  5500,  2007,  5717,  2812,  1998,  3115, 24353,  1173, 29723,
         1012,  6516,  2009,  2003,  2464,  2008,  1061,  2005,  2445,  1060,
         2003,  2036,  3671,  5500,  1012,  2009,  2003,  5071,  2008,  1050,
         7551,  2030,  3231,  3463,  2024,  2800,  1006,  1060,  1010,  1061,
         1007,  1056,  1012,  1996,  2034,  3357,  7336, 20077,  1996,  3188,
         4353,  1997,  1996,  2944, 11709,  1012,  1999,  1996,  2279,  3357,
         1010,  1996,  2944,  2003,  7172,  2007,  2047, 11702,  1012,   102])"
551,1,"['method', 'errors', 'predicted', 'least squares method', 'model', 'least squares', 'parameters', 'error']", Linear Regression Prior Model,seg_165,"the model parameters β0 and β1 can be assessed by using the least squares method. for this, the model error εi is assessed for each point by determining the difference between the measured and predicted values of yi . the model parameters which minimize the sum of all squared error terms are then obtained. the squared error terms are used in order to account for positive and negative errors equally.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0.])","tensor([ 7399, 26237,  3188,  2944])","tensor([  101,  1996,  2944, 11709,  1156,  2692,  1998,  1156,  2487,  2064,
         2022, 14155,  2011,  2478,  1996,  2560, 14320,  4118,  1012,  2005,
         2023,  1010,  1996,  2944,  7561,  1159,  2072,  2003, 14155,  2005,
         2169,  2391,  2011, 12515,  1996,  4489,  2090,  1996,  7594,  1998,
        10173,  5300,  1997, 12316,  1012,  1996,  2944, 11709,  2029, 18478,
         1996,  7680,  1997,  2035, 19942,  7561,  3408,  2024,  2059,  4663,
         1012,  1996, 19942,  7561,  3408,  2024,  2109,  1999,  2344,  2000,
         4070,  2005,  3893,  1998,  4997, 10697,  8053,  1012,   102])"
552,1,"['function', 'set']", Linear Regression Prior Model,seg_165,to minimize this function we set its partial derivatives with respect to β0 and β1 to be equal to zero:,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  3188,  2944])","tensor([  101,  2000, 18478,  2023,  3853,  2057,  2275,  2049,  7704, 16942,
         2007,  4847,  2000,  1156,  2692,  1998,  1156,  2487,  2000,  2022,
         5020,  2000,  5717,  1024,   102])"
553,1,['set'], Linear Regression Prior Model,seg_165,the set of equations obtained from eq. 5.34 can be written as:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 7399, 26237,  3188,  2944])","tensor([  101,  1996,  2275,  1997, 11380,  4663,  2013,  1041,  4160,  1012,
         1019,  1012,  4090,  2064,  2022,  2517,  2004,  1024,   102])"
554,0,[], Linear Regression Prior Model,seg_165,this equation system can be described in matrix notation:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 7399, 26237,  3188,  2944])","tensor([  101,  2023,  8522,  2291,  2064,  2022,  2649,  1999,  8185, 14869,
         1024,   102])"
555,1,"['results', 'test results', 'test']", Linear Regression Prior Model,seg_165,the vector ŷ contains the test results ŷi while the matrix x contains ones in the first column and the test results x̂i in the second column. equation 5.38 can be reformulated as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 7399, 26237,  3188,  2944])","tensor([  101,  1996,  9207,  1061,  3397,  1996,  3231,  3463, 12316,  2096,
         1996,  8185,  1060,  3397,  3924,  1999,  1996,  2034,  5930,  1998,
         1996,  3231,  3463,  8418,  1999,  1996,  2117,  5930,  1012,  8522,
         1019,  1012,  4229,  2064,  2022,  5290,  8898,  2004,  1024,   102])"
556,1,"['regression', 'method of maximum likelihood', 'linear', 'maximum likelihood', 'simple linear regression', 'method', 'linear regression', 'likelihood']", Linear Regression Prior Model,seg_165,"equation 5.39 can also be derived using the method of maximum likelihood. in a simple linear regression, the following solutions for β0 and β1 exist:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  3188,  2944])","tensor([  101,  8522,  1019,  1012,  4464,  2064,  2036,  2022,  5173,  2478,
         1996,  4118,  1997,  4555, 16593,  1012,  1999,  1037,  3722,  7399,
        26237,  1010,  1996,  2206,  7300,  2005,  1156,  2692,  1998,  1156,
         2487,  4839,  1024,   102])"
557,1,"['deviation', 'standard deviation', 'standard', 'error']", Linear Regression Prior Model,seg_165,the standard deviation of the error term ε is assessed by:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  3188,  2944])","tensor([  101,  1996,  3115, 24353,  1997,  1996,  7561,  2744,  1159,  2003,
        14155,  2011,  1024,   102])"
558,1,"['variance', 'measurements', 'parameters', 'error']", Linear Regression Prior Model,seg_165,where n is the number of measurements and k is the number of parameters in β . the variance of the error term can be written in matrix form as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  3188,  2944])","tensor([  101,  2073,  1050,  2003,  1996,  2193,  1997, 11702,  1998,  1047,
         2003,  1996,  2193,  1997, 11709,  1999,  1156,  1012,  1996, 23284,
         1997,  1996,  7561,  2744,  2064,  2022,  2517,  1999,  8185,  2433,
         2004,  1024,   102])"
559,1,"['normal distribution', 'normal', 'conditional distribution', 'distribution', 'parameters', 'conditional']", Linear Regression Prior Model,seg_165,the conditional distribution of ŷ given β and x̂ follows a normal distribution with the following parameters:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 0.])","tensor([ 7399, 26237,  3188,  2944])","tensor([  101,  1996, 18462,  4353,  1997,  1061,  2445,  1156,  1998,  1060,
         4076,  1037,  3671,  4353,  2007,  1996,  2206, 11709,  1024,   102])"
560,1,"['parameter', 'uncertainty', 'variances', 'covariance matrix', 'parameters', 'covariance']", Linear Regression Prior Model,seg_165,"the uncertainty of the parameters β can be represented using the covariance matrix, where the parameter variances are contained in the diagonal:",tensor(1),"tensor([0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  3188,  2944])","tensor([  101,  1996, 12503,  1997,  1996, 11709,  1156,  2064,  2022,  3421,
         2478,  1996,  2522, 10755, 28335,  8185,  1010,  2073,  1996, 16381,
        23284,  2015,  2024,  4838,  1999,  1996, 19754,  1024,   102])"
561,1,"['random variable', 'random', 'model', 'variable', 'case']", Linear Regression Prior Model,seg_165,"in case the random variable ŷ needs to be modeled using not only one, but k components in x̂, the model can be generalized to:",tensor(1),"tensor([0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  3188,  2944])","tensor([  101,  1999,  2553,  1996,  6721,  8023,  1061,  3791,  2000,  2022,
        14440,  2478,  2025,  2069,  2028,  1010,  2021,  1047,  6177,  1999,
         1060,  1010,  1996,  2944,  2064,  2022, 18960,  2000,  1024,   102])"
562,1,['parameters'], Linear Regression Prior Model,seg_165,the parameters β can also be assessed through eq. 5.39 with the only difference being that the matrix x holds one column for each of r components.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  3188,  2944])","tensor([  101,  1996, 11709,  1156,  2064,  2036,  2022, 14155,  2083,  1041,
         4160,  1012,  1019,  1012,  4464,  2007,  1996,  2069,  4489,  2108,
         2008,  1996,  8185,  1060,  4324,  2028,  5930,  2005,  2169,  1997,
         1054,  6177,  1012,   102])"
563,1,"['correlation', 'normal']", Example Tensile Strength of Timber Prior Model,seg_167,in the grading of timber materials it is normal to classify the sawn timber tensile strength based on the tensile modulus of elasticity. to be able to do this it is necessary to determine the correlation between the tensile strength x and the tensile modulus of elasticity y on the basis of measured values.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 15295,  9463,  3997,  1997,  7227,  3188,  2944])","tensor([  101,  1999,  1996, 26886,  1997,  7227,  4475,  2009,  2003,  3671,
         2000, 26268,  1996,  2387,  2078,  7227, 15295,  9463,  3997,  2241,
         2006,  1996, 15295,  9463, 16913, 11627,  1997, 21274,  3012,  1012,
         2000,  2022,  2583,  2000,  2079,  2023,  2009,  2003,  4072,  2000,
         5646,  1996, 16902,  2090,  1996, 15295,  9463,  3997,  1060,  1998,
         1996, 15295,  9463, 16913, 11627,  1997, 21274,  3012,  1061,  2006,
         1996,  3978,  1997,  7594,  5300,  1012,   102])"
564,1,"['table', 'results', 'experiment', 'model', 'parameters']", Example Tensile Strength of Timber Prior Model,seg_167,using the experiment results from table 5.2 the parameters β0 and β1 can now be assessed for the model y = β0 + β1x + ε:,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 15295,  9463,  3997,  1997,  7227,  3188,  2944])","tensor([  101,  2478,  1996,  7551,  3463,  2013,  2795,  1019,  1012,  1016,
         1996, 11709,  1156,  2692,  1998,  1156,  2487,  2064,  2085,  2022,
        14155,  2005,  1996,  2944,  1061,  1027,  1156,  2692,  1009,  1156,
         2487,  2595,  1009,  1159,  1024,   102])"
565,1,"['deviation', 'standard deviation', 'standard', 'error']", Example Tensile Strength of Timber Prior Model,seg_167,the standard deviation of the error term can be determined as follows:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 15295,  9463,  3997,  1997,  7227,  3188,  2944])","tensor([  101,  1996,  3115, 24353,  1997,  1996,  7561,  2744,  2064,  2022,
         4340,  2004,  4076,  1024,   102])"
566,1,['regression'], Example Tensile Strength of Timber Prior Model,seg_167,"hence, the regression equation is (see fig. 5.8):",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 15295,  9463,  3997,  1997,  7227,  3188,  2944])","tensor([  101,  6516,  1010,  1996, 26237,  8522,  2003,  1006,  2156, 20965,
         1012,  1019,  1012,  1022,  1007,  1024,   102])"
567,1,"['uncertainties', 'parameters']", Example Tensile Strength of Timber Prior Model,seg_167,the uncertainties of the parameters β are determined in the following way:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 15295,  9463,  3997,  1997,  7227,  3188,  2944])","tensor([  101,  1996,  9662,  7368,  1997,  1996, 11709,  1156,  2024,  4340,
         1999,  1996,  2206,  2126,  1024,   102])"
568,1,"['regression', 'linear', 'coefficients', 'parameters', 'random', 'linear regression', 'random variables', 'regression coefficients', 'model', 'measurements', 'variables']", Updating Regression Coefficients Posterior Model,seg_169,"in the foregoing section, linear regression has been introduced as a useful tool to investigate the linear relationship between two random variables. this section shows how the prior model for the regression coefficients β can be updated based on new measurements. it is assumed that in addition to the parameters β ′ and vβ′ from the prior model, the new measurements y and x are given.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 2039, 16616, 26237, 21374, 15219,  2944])","tensor([  101,  1999,  1996, 18921, 26966,  2930,  1010,  7399, 26237,  2038,
         2042,  3107,  2004,  1037,  6179,  6994,  2000,  8556,  1996,  7399,
         3276,  2090,  2048,  6721, 10857,  1012,  2023,  2930,  3065,  2129,
         1996,  3188,  2944,  2005,  1996, 26237, 21374,  1156,  2064,  2022,
         7172,  2241,  2006,  2047, 11702,  1012,  2009,  2003,  5071,  2008,
         1999,  2804,  2000,  1996, 11709,  1156,  1531,  1998,  1058, 29720,
         1531,  2013,  1996,  3188,  2944,  1010,  1996,  2047, 11702,  1061,
         1998,  1060,  2024,  2445,  1012,   102])"
569,1,"['model', 'parameters', 'posterior']", Updating Regression Coefficients Posterior Model,seg_169,the parameters for the posterior model are then derived from the following equation:,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2039, 16616, 26237, 21374, 15219,  2944])","tensor([  101,  1996, 11709,  2005,  1996, 15219,  2944,  2024,  2059,  5173,
         2013,  1996,  2206,  8522,  1024,   102])"
570,1,"['model', 'correlation']", Example Updating Regression Coefficients Determined in Example ,seg_171,the prior model in example 5.3 describes the correlation between the tensile strength x and the tensile modulus of elasticity y of timber:,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  2039, 16616, 26237, 21374,  4340,  1999,  2742])","tensor([  101,  1996,  3188,  2944,  1999,  2742,  1019,  1012,  1017,  5577,
         1996, 16902,  2090,  1996, 15295,  9463,  3997,  1060,  1998,  1996,
        15295,  9463, 16913, 11627,  1997, 21274,  3012,  1061,  1997,  7227,
         1024,   102])"
571,1,"['model', 'results', 'test results', 'test']", Example Updating Regression Coefficients Determined in Example ,seg_171,"now, the prior model is updated with two new test results, as indicated in table 5.3.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 2742,  2039, 16616, 26237, 21374,  4340,  1999,  2742])","tensor([ 101, 2085, 1010, 1996, 3188, 2944, 2003, 7172, 2007, 2048, 2047, 3231,
        3463, 1010, 2004, 5393, 1999, 2795, 1019, 1012, 1017, 1012,  102])"
572,1,['regression'], Example Updating Regression Coefficients Determined in Example ,seg_171,"hence, the updated regression equation is (see fig. 5.9):",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  2039, 16616, 26237, 21374,  4340,  1999,  2742])","tensor([  101,  6516,  1010,  1996,  7172, 26237,  8522,  2003,  1006,  2156,
        20965,  1012,  1019,  1012,  1023,  1007,  1024,   102])"
573,1,"['hypotheses', 'uncertainty', 'data', 'statistical uncertainty', 'probabilistic', 'sample', 'probabilistic models', 'sample size', 'statistical significance', 'statistical', 'significance']", Example Updating Regression Coefficients Determined in Example ,seg_171,"lecture 10 (aim of the present lecture) the aim of the present lecture is to introduce the concept of testing for statistical significance. first, it is explained how the statistical properties of sample characteristics depend on the sample size, i.e. the amount of data available, and in this way, the quantification of statistical uncertainty is addressed. then it is shown how hypotheses related to the statistical properties of probabilistic models may be tested on the basis of data. on the basis of the lecture it is expected that the reader will acquire knowledge and skills with regard to:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  2039, 16616, 26237, 21374,  4340,  1999,  2742])","tensor([  101,  8835,  2184,  1006,  6614,  1997,  1996,  2556,  8835,  1007,
         1996,  6614,  1997,  1996,  2556,  8835,  2003,  2000,  8970,  1996,
         4145,  1997,  5604,  2005,  7778,  7784,  1012,  2034,  1010,  2009,
         2003,  4541,  2129,  1996,  7778,  5144,  1997,  7099,  6459, 12530,
         2006,  1996,  7099,  2946,  1010,  1045,  1012,  1041,  1012,  1996,
         3815,  1997,  2951,  2800,  1010,  1998,  1999,  2023,  2126,  1010,
         1996, 24110,  3775, 10803,  1997,  7778, 12503,  2003,  8280,  1012,
         2059,  2009,  2003,  3491,  2129,  1044, 22571, 14573, 23072,  3141,
         2000,  1996,  7778,  5144,  1997,  4013,  3676, 27965,  4588,  4275,
         2089,  2022,  7718,  2006,  1996,  3978,  1997,  2951,  1012,  2006,
         1996,  3978,  1997,  1996,  8835,  2009,  2003,  3517,  2008,  1996,
         8068,  2097,  9878,  3716,  1998,  4813,  2007,  7634,  2000,  1024,
          102])"
574,1,"['confidence interval', 'sample statistics', 'functions', 'type ii', 'tests', 'type i error', 'unbiased estimator', 'confidence', 'mean', 'error', 'type ii error', 'interval', 'random variable', 'statistics', 'samples', 'statistical', 'sample mean', 'alternate hypothesis', 'unbiased', 'hypothesis', 'estimator', 'sample variance', 'significance', 'variance', 'sample', 'random', 'statistical significance', 'distribution', 'expected value', 'variable']", Example Updating Regression Coefficients Determined in Example ,seg_171,• what are the distribution functions applied in sample statistics and what are the principles on the basis of which they are derived? • what does it mean that the chi-square distribution is regenerative? • how is the chi-distribution related to the law of pythagoras? • how does the expected value of the sample mean depend on the number of samples? • how does the variance of the sample mean depend on the number of samples? • how can an unbiased estimator for the sample variance be established? • what is a confidence interval and how can it be established? • what is a hypothesis and how can it be tested? • what is a null-hypothesis and what is an alternate hypothesis? • what is a type i error and what is a type ii error? • what is the meaning of statistical significance? • how can tests of the mean of a random variable be performed?,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0.])","tensor([ 2742,  2039, 16616, 26237, 21374,  4340,  1999,  2742])","tensor([  101,  1528,  2054,  2024,  1996,  4353,  4972,  4162,  1999,  7099,
         6747,  1998,  2054,  2024,  1996,  6481,  2006,  1996,  3978,  1997,
         2029,  2027,  2024,  5173,  1029,  1528,  2054,  2515,  2009,  2812,
         2008,  1996,  9610,  1011,  2675,  4353,  2003, 19723, 24454,  8082,
         1029,  1528,  2129,  2003,  1996,  9610,  1011,  4353,  3141,  2000,
         1996,  2375,  1997,  1052, 22123,  3270, 20255,  3022,  1029,  1528,
         2129,  2515,  1996,  3517,  3643,  1997,  1996,  7099,  2812, 12530,
         2006,  1996,  2193,  1997,  8168,  1029,  1528,  2129,  2515,  1996,
        23284,  1997,  1996,  7099,  2812, 12530,  2006,  1996,  2193,  1997,
         8168,  1029,  1528,  2129,  2064,  2019,  4895, 11607,  6924,  9765,
         9581,  4263,  2005,  1996,  7099, 23284,  2022,  2511,  1029,  1528,
         2054,  2003,  1037,  7023, 13483,  1998,  2129,  2064,  2009,  2022,
         2511,  1029,  1528,  2054,  2003,  1037, 10744,  1998,  2129,  2064,
         2009,  2022,  7718,  1029,  1528,  2054,  2003,  1037, 19701,  1011,
        10744,  1998,  2054,  2003,  2019,  6585, 10744,  1029,  1528,  2054,
         2003,  1037,  2828,  1045,  7561,  1998,  2054,  2003,  1037,  2828,
         2462,  7561,  1029,  1528,  2054,  2003,  1996,  3574,  1997,  7778,
         7784,  1029,  1528,  2129,  2064,  5852,  1997,  1996,  2812,  1997,
         1037,  6721,  8023,  2022,  2864,  1029,   102])"
575,1,"['normal distribution', 'functions', 'distributions', 'normal', 'standard', 'distribution', 'statistical', 'numerical']", Probability Distributions in Statistics,seg_173,"throughout the classical statistical theory some distribution functions are repeatedly used for assessment and testing purposes. these include the important chisquare distribution, the chi-distribution, the t-distribution and the f-distribution. here, only the first two distributions are briefly introduced in accordance with benjamin and cornell [4]. the distributions are all related and may be derived from the normal distribution as shown in e.g. benjamin and cornell [4]. the numerical evaluation of the distributions may be performed using standard commercial spread sheets such as e.g. microsoft excel or tabulations as given in appendix c.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 9723, 20611,  1999,  6747])","tensor([  101,  2802,  1996,  4556,  7778,  3399,  2070,  4353,  4972,  2024,
         8385,  2109,  2005,  7667,  1998,  5604,  5682,  1012,  2122,  2421,
         1996,  2590,  9610,  2015, 16211,  2890,  4353,  1010,  1996,  9610,
         1011,  4353,  1010,  1996,  1056,  1011,  4353,  1998,  1996,  1042,
         1011,  4353,  1012,  2182,  1010,  2069,  1996,  2034,  2048, 20611,
         2024,  4780,  3107,  1999, 10388,  2007,  6425,  1998, 10921,  1031,
         1018,  1033,  1012,  1996, 20611,  2024,  2035,  3141,  1998,  2089,
         2022,  5173,  2013,  1996,  3671,  4353,  2004,  3491,  1999,  1041,
         1012,  1043,  1012,  6425,  1998, 10921,  1031,  1018,  1033,  1012,
         1996, 15973,  9312,  1997,  1996, 20611,  2089,  2022,  2864,  2478,
         3115,  3293,  3659,  8697,  2107,  2004,  1041,  1012,  1043,  1012,
         7513, 24970,  2030, 21628,  9513,  2015,  2004,  2445,  1999, 22524,
         1039,  1012,   102])"
576,1,"['standard normal', 'random', 'normal', 'random variables', 'standard', 'variables', 'independent']", The ChiSquare χDistribution,seg_175,"when xi , i = 1,2, . . . , n are standard normal distributed independent random variables the sum of the squares of the random variables yn i.e.:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 10521, 18886, 29446])","tensor([  101,  2043,  8418,  1010,  1045,  1027,  1015,  1010,  1016,  1010,
         1012,  1012,  1012,  1010,  1050,  2024,  3115,  3671,  5500,  2981,
         6721, 10857,  1996,  7680,  1997,  1996, 14320,  1997,  1996,  6721,
        10857,  1061,  2078,  1045,  1012,  1041,  1012,  1024,   102])"
577,1,"['function', 'density function']", The ChiSquare χDistribution,seg_175,is said to be chi-square distributed (some times written as χ2-distributed) with probability density function:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 10521, 18886, 29446])","tensor([ 101, 2003, 2056, 2000, 2022, 9610, 1011, 2675, 5500, 1006, 2070, 2335,
        2517, 2004, 1177, 2475, 1011, 5500, 1007, 2007, 9723, 4304, 3853, 1024,
         102])"
578,1,"['mean', 'degrees of freedom', 'variance']", The ChiSquare χDistribution,seg_175,with mean value μy = n (also referred to as the degrees of freedom) and variance,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 1., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 10521, 18886, 29446])","tensor([  101,  2007,  2812,  3643,  1166,  2100,  1027,  1050,  1006,  2036,
         3615,  2000,  2004,  1996,  5445,  1997,  4071,  1007,  1998, 23284,
          102])"
579,1,"['normal', 'errors', 'distribution', 'statistical', 'variables']", The ChiSquare χDistribution,seg_175,"as it shall be seen later, the chi-square distribution is often applied for assessing the statistical characteristics of squared errors but can also be applied in various engineering assessments involving squares of normal distributed variables such as e.g. the drag component of wave and wind loads and kinetic energy components.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 10521, 18886, 29446])","tensor([  101,  2004,  2009,  4618,  2022,  2464,  2101,  1010,  1996,  9610,
         1011,  2675,  4353,  2003,  2411,  4162,  2005, 20077,  1996,  7778,
         6459,  1997, 19942, 10697,  2021,  2064,  2036,  2022,  4162,  1999,
         2536,  3330, 20794,  5994, 14320,  1997,  3671,  5500, 10857,  2107,
         2004,  1041,  1012,  1043,  1012,  1996,  8011,  6922,  1997,  4400,
         1998,  3612, 15665,  1998, 20504,  2943,  6177,  1012,   102])"
580,1,"['degrees of freedom', 'distribution', 'variables']", The ChiSquare χDistribution,seg_175,the chi-square distribution is regenerative in the sense that the sum of two chisquare distributed variables i.e. yn1 +yn2 is also chi-square distributed with n1 +n2 degrees of freedom.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 10521, 18886, 29446])","tensor([  101,  1996,  9610,  1011,  2675,  4353,  2003, 19723, 24454,  8082,
         1999,  1996,  3168,  2008,  1996,  7680,  1997,  2048,  9610,  2015,
        16211,  2890,  5500, 10857,  1045,  1012,  1041,  1012,  1061,  2078,
         2487,  1009,  1061,  2078,  2475,  2003,  2036,  9610,  1011,  2675,
         5500,  2007,  1050,  2487,  1009,  1050,  2475,  5445,  1997,  4071,
         1012,   102])"
581,1,"['mean', 'normal distribution', 'normal', 'limit', 'distribution', 'central limit theorem']", The ChiSquare χDistribution,seg_175,"from the additive character of the chi-square distribution (eq. 5.52), it is seen, from application of the central limit theorem, that for sufficiently large n the chisquare distribution converges towards a normal distribution with mean value μy =",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,
        0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 10521, 18886, 29446])","tensor([  101,  2013,  1996, 29167,  2839,  1997,  1996,  9610,  1011,  2675,
         4353,  1006,  1041,  4160,  1012,  1019,  1012,  4720,  1007,  1010,
         2009,  2003,  2464,  1010,  2013,  4646,  1997,  1996,  2430,  5787,
         9872,  1010,  2008,  2005, 12949,  2312,  1050,  1996,  9610,  2015,
        16211,  2890,  4353, 28314,  2015,  2875,  1037,  3671,  4353,  2007,
         2812,  3643,  1166,  2100,  1027,   102])"
582,1,['variance'], The ChiSquare χDistribution,seg_175,"n n and variance σ 2 = 2n, see also fig. 5.10.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 10521, 18886, 29446])","tensor([  101,  1050,  1050,  1998, 23284,  1173,  1016,  1027,  1016,  2078,
         1010,  2156,  2036, 20965,  1012,  1019,  1012,  2184,  1012,   102])"
583,1,"['function', 'density function', 'probability', 'probability density function', 'variable', 'random variable', 'random']", The Chi χDistribution,seg_177,"when a random variable z is given as the square root of a chi-square distributed random variable yn, the variable z is said to follow a chi-distribution (sometimes written as χ -distributed) with probability density function:",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 1996,  9610,  1177, 10521, 18886, 29446])","tensor([ 101, 2043, 1037, 6721, 8023, 1062, 2003, 2445, 2004, 1996, 2675, 7117,
        1997, 1037, 9610, 1011, 2675, 5500, 6721, 8023, 1061, 2078, 1010, 1996,
        8023, 1062, 2003, 2056, 2000, 3582, 1037, 9610, 1011, 4353, 1006, 2823,
        2517, 2004, 1177, 1011, 5500, 1007, 2007, 9723, 4304, 3853, 1024,  102])"
584,1,"['mean', 'variance']", The Chi χDistribution,seg_177,the mean value μz and the variance σz2 are given by:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9610,  1177, 10521, 18886, 29446])","tensor([  101,  1996,  2812,  3643,  1166,  2480,  1998,  1996, 23284,  1173,
         2480,  2475,  2024,  2445,  2011,  1024,   102])"
585,1,"['deviations', 'standard', 'statistical', 'standard deviations']", The Chi χDistribution,seg_177,"the chi-distribution is e.g. used for the assessment of the distances measured using the principles of pythagoras or euclidean norms, and for the assessment of the statistical characteristics of standard deviations.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 1996,  9610,  1177, 10521, 18886, 29446])","tensor([  101,  1996,  9610,  1011,  4353,  2003,  1041,  1012,  1043,  1012,
         2109,  2005,  1996,  7667,  1997,  1996, 12103,  7594,  2478,  1996,
         6481,  1997,  1052, 22123,  3270, 20255,  3022,  2030, 25826, 17606,
         1010,  1998,  2005,  1996,  7667,  1997,  1996,  7778,  6459,  1997,
         3115, 24353,  2015,  1012,   102])"
586,1,"['variability', 'sample statistics', 'sample moments', 'confidence', 'estimators', 'probabilistic', 'statistics', 'statistical', 'data', 'parameter', 'uncertainty', 'sample characteristic', 'information', 'results', 'evaluating', 'confidence intervals', 'significance', 'significance testing', 'moments', 'sample', 'intervals', 'numerical']", Estimators for Sample DescriptorsSample Statistics,seg_179,"when frequentistic information becomes available e.g. in the form of experimental results, a first step is often to try to assess the data simply as they are, without too many assumptions regarding the probabilistic characteristics of the mechanism/process which generated them. such an assessment typically concerns the numerical summaries as described in chap. 3, e.g. the sample moments, but could in principle be any sample characteristic of the observed data which is found to be of interest in a given situation. in statistical terms, such characteristics are called sample statistics, and in the following section the statistical characteristics of such sample statistics will be considered in some detail. to this end, the uncertainty associated with parameter estimators will be assessed and confidence intervals on the estimators will be introduced. finally, significance testing is introduced as a means of evaluating the significance of the variability of statistical data.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1.,
        0., 0.])","tensor([ 9765,  9581,  6591,  2005,  7099,  4078, 23235,  5668, 21559, 10814,
         6747])","tensor([  101,  2043,  6976,  6553,  2592,  4150,  2800,  1041,  1012,  1043,
         1012,  1999,  1996,  2433,  1997,  6388,  3463,  1010,  1037,  2034,
         3357,  2003,  2411,  2000,  3046,  2000, 14358,  1996,  2951,  3432,
         2004,  2027,  2024,  1010,  2302,  2205,  2116, 17568,  4953,  1996,
         4013,  3676, 27965,  4588,  6459,  1997,  1996,  7337,  1013,  2832,
         2029,  7013,  2068,  1012,  2107,  2019,  7667,  4050,  5936,  1996,
        15973,  7680,  7849,  3111,  2004,  2649,  1999, 15775,  2361,  1012,
         1017,  1010,  1041,  1012,  1043,  1012,  1996,  7099,  5312,  1010,
         2021,  2071,  1999,  6958,  2022,  2151,  7099,  8281,  1997,  1996,
         5159,  2951,  2029,  2003,  2179,  2000,  2022,  1997,  3037,  1999,
         1037,  2445,  3663,  1012,  1999,  7778,  3408,  1010,  2107,  6459,
         2024,  2170,  7099,  6747,  1010,  1998,  1999,  1996,  2206,  2930,
         1996,  7778,  6459,  1997,  2107,  7099,  6747,  2097,  2022,  2641,
         1999,  2070,  6987,  1012,  2000,  2023,  2203,  1010,  1996, 12503,
         3378,  2007, 16381,  9765,  9581,  6591,  2097,  2022, 14155,  1998,
         7023, 14025,  2006,  1996,  9765,  9581,  6591,  2097,  2022,  3107,
         1012,  2633,  1010,  7784,  5604,  2003,  3107,  2004,  1037,  2965,
         1997, 23208,  1996,  7784,  1997,  1996, 28436,  1997,  7778,  2951,
         1012,   102])"
587,1,"['variability', 'functions', 'maximum likelihood', 'method', 'associated', 'distribution function', 'estimate', 'random variable', 'model', 'case', 'method of moments', 'function', 'uncertainty', 'decision problem', 'results', 'moments', 'random', 'likelihood', 'experiment', 'distribution', 'variable']", Statistical Characteristics of the Sample Average,seg_181,"as an example, consider the case where the permeability of a particular soil is of interest in an engineering decision problem. due to various effects such as inherent natural variability in the soil composition and the consolidation, the permeability of the considered soil is associated with significant uncertainty. as an engineering model, it is assumed that this uncertainty can be taken into account in the formulation of the decision problem by modeling the permeability by a random variable x with distribution function fx(x;p). having selected the family of distribution functions, i.e. the distribution function fx(x), it is still needed to estimate the parameters p, and as will be seen in the following sections, this can be done by the method of moments or the maximum likelihood method, provided that experiment results x̂ = (x̂1, x̂2, . . . , x̂n)t are available.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([  101,  2004,  2019,  2742,  1010,  5136,  1996,  2553,  2073,  1996,
         2566,  4168,  8010,  1997,  1037,  3327,  5800,  2003,  1997,  3037,
         1999,  2019,  3330,  3247,  3291,  1012,  2349,  2000,  2536,  3896,
         2107,  2004, 16112,  3019, 28436,  1999,  1996,  5800,  5512,  1998,
         1996, 17439,  1010,  1996,  2566,  4168,  8010,  1997,  1996,  2641,
         5800,  2003,  3378,  2007,  3278, 12503,  1012,  2004,  2019,  3330,
         2944,  1010,  2009,  2003,  5071,  2008,  2023, 12503,  2064,  2022,
         2579,  2046,  4070,  1999,  1996, 20219,  1997,  1996,  3247,  3291,
         2011, 11643,  1996,  2566,  4168,  8010,  2011,  1037,  6721,  8023,
         1060,  2007,  4353,  3853, 23292,  1006,  1060,  1025,  1052,  1007,
         1012,  2383,  3479,  1996,  2155,  1997,  4353,  4972,  1010,  1045,
         1012,  1041,  1012,  1996,  4353,  3853, 23292,  1006,  1060,  1007,
         1010,  2009,  2003,  2145,  2734,  2000, 10197,  1996, 11709,  1052,
         1010,  1998,  2004,  2097,  2022,  2464,  1999,  1996,  2206,  5433,
         1010,  2023,  2064,  2022,  2589,  2011,  1996,  4118,  1997,  5312,
         2030,  1996,  4555, 16593,  4118,  1010,  3024,  2008,  7551,  3463,
         1060,  1027,  1006,  1060,  2487,  1010,  1060,  2475,  1010,  1012,
         1012,  1012,  1010,  1060,  2078,  1007,  1056,  2024,  2800,  1012,
          102])"
588,1,"['functions', 'probabilistic model', 'experiments', 'independent', 'mean', 'associated', 'probabilistic', 'sample mean', 'independent random variables', 'model', 'statistical', 'uncertainty', 'results', 'random variables', 'sample variance', 'parameters', 'variance', 'sample', 'random', 'experiment', 'distribution', 'variables']", Statistical Characteristics of the Sample Average,seg_181,"in order to better appreciate the uncertainty associated with statistical characteristics such as distribution parameters p, the statistical properties of these parameters are now considered. it is assumed that experiment results of the yet unknown values are collected in the vector x. if the experiments are conducted independently, the realizations can be modeled as independent random variables xi , i = 1,2, . . . , n with cumulative distribution functions fxi (xi;p) = fx(x;p), i = 1,2, . . . , n. based on the probabilistic model of the realizations xi , i = 1,2, . . . , n it is possible to assess the statistical characteristics of the unknown sample mean x̄ and the unknown sample variance s2 given by:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([  101,  1999,  2344,  2000,  2488,  9120,  1996, 12503,  3378,  2007,
         7778,  6459,  2107,  2004,  4353, 11709,  1052,  1010,  1996,  7778,
         5144,  1997,  2122, 11709,  2024,  2085,  2641,  1012,  2009,  2003,
         5071,  2008,  7551,  3463,  1997,  1996,  2664,  4242,  5300,  2024,
         5067,  1999,  1996,  9207,  1060,  1012,  2065,  1996,  7885,  2024,
         4146,  9174,  1010,  1996, 12393,  2015,  2064,  2022, 14440,  2004,
         2981,  6721, 10857,  8418,  1010,  1045,  1027,  1015,  1010,  1016,
         1010,  1012,  1012,  1012,  1010,  1050,  2007, 23260,  4353,  4972,
        23292,  2072,  1006,  8418,  1025,  1052,  1007,  1027, 23292,  1006,
         1060,  1025,  1052,  1007,  1010,  1045,  1027,  1015,  1010,  1016,
         1010,  1012,  1012,  1012,  1010,  1050,  1012,  2241,  2006,  1996,
         4013,  3676, 27965,  4588,  2944,  1997,  1996, 12393,  2015,  8418,
         1010,  1045,  1027,  1015,  1010,  1016,  1010,  1012,  1012,  1012,
         1010,  1050,  2009,  2003,  2825,  2000, 14358,  1996,  7778,  6459,
         1997,  1996,  4242,  7099,  2812,  1060,  1998,  1996,  4242,  7099,
        23284,  1055,  2475,  2445,  2011,  1024,   102])"
589,1,"['sample statistics', 'functions', 'outcomes', 'mean', 'statistics', 'random variables', 'sample variance', 'variance', 'sample', 'random', 'experiment', 'distribution', 'variables', 'sample mean']", Statistical Characteristics of the Sample Average,seg_181,"the sample mean x̄ and the sample variance s2 are random variables given in terms of functions of the experiment outcomes xi , i = 1,2, . . . , n. such functions are in general called sample statistics and include, as mentioned previously, any characteristic of the considered distribution of interest.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([  101,  1996,  7099,  2812,  1060,  1998,  1996,  7099, 23284,  1055,
         2475,  2024,  6721, 10857,  2445,  1999,  3408,  1997,  4972,  1997,
         1996,  7551, 13105,  8418,  1010,  1045,  1027,  1015,  1010,  1016,
         1010,  1012,  1012,  1012,  1010,  1050,  1012,  2107,  4972,  2024,
         1999,  2236,  2170,  7099,  6747,  1998,  2421,  1010,  2004,  3855,
         3130,  1010,  2151,  8281,  1997,  1996,  2641,  4353,  1997,  3037,
         1012,   102])"
590,1,"['mean', 'uncertainty', 'variance', 'associated', 'sample', 'expected value', 'sample mean']", Statistical Characteristics of the Sample Average,seg_181,"in order to assess the uncertainty by which the sample mean x̄ is associated, it is interesting to consider its expected value e[x̄] and variance var[x̄]. the expected value is given as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([  101,  1999,  2344,  2000, 14358,  1996, 12503,  2011,  2029,  1996,
         7099,  2812,  1060,  2003,  3378,  1010,  2009,  2003,  5875,  2000,
         5136,  2049,  3517,  3643,  1041,  1031,  1060,  1033,  1998, 23284,
        13075,  1031,  1060,  1033,  1012,  1996,  3517,  3643,  2003,  2445,
         2004,  1024,   102])"
591,1,"['mean', 'random variable', 'sample', 'random', 'estimator', 'sample mean', 'expected value', 'variable', 'case']", Statistical Characteristics of the Sample Average,seg_181,"which shows that the expected value of the sample mean is indeed equal to the expected value of the underlying random variable, in this case the soil permeability. it was expected that the sample mean is a good estimator for the expected value of",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([  101,  2029,  3065,  2008,  1996,  3517,  3643,  1997,  1996,  7099,
         2812,  2003,  5262,  5020,  2000,  1996,  3517,  3643,  1997,  1996,
        10318,  6721,  8023,  1010,  1999,  2023,  2553,  1996,  5800,  2566,
         4168,  8010,  1012,  2009,  2001,  3517,  2008,  1996,  7099,  2812,
         2003,  1037,  2204,  9765,  9581,  4263,  2005,  1996,  3517,  3643,
         1997,   102])"
592,1,"['mean', 'random variable', 'sample', 'random', 'sample mean', 'variable']", Statistical Characteristics of the Sample Average,seg_181,"a random variable. however, due to the fact that the sample mean would be a real-",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([ 101, 1037, 6721, 8023, 1012, 2174, 1010, 2349, 2000, 1996, 2755, 2008,
        1996, 7099, 2812, 2052, 2022, 1037, 2613, 1011,  102])"
593,1,"['mean', 'random variable', 'sample', 'random', 'sample mean', 'variable']", Statistical Characteristics of the Sample Average,seg_181,"ization of a random variable, it is clear that the sample mean will normally not turn",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([ 101, 1045, 9276, 1997, 1037, 6721, 8023, 1010, 2009, 2003, 3154, 2008,
        1996, 7099, 2812, 2097, 5373, 2025, 2735,  102])"
594,1,"['random variable', 'random', 'expected value', 'variable']", Statistical Characteristics of the Sample Average,seg_181,out to be exactly equal to the expected value of the underlying random variable.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([  101,  2041,  2000,  2022,  3599,  5020,  2000,  1996,  3517,  3643,
         1997,  1996, 10318,  6721,  8023,  1012,   102])"
595,1,"['variability', 'mean', 'sample', 'expected value', 'sample mean']", Statistical Characteristics of the Sample Average,seg_181,the variability of the sample mean around its expected value can be assessed,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([  101,  1996, 28436,  1997,  1996,  7099,  2812,  2105,  2049,  3517,
         3643,  2064,  2022, 14155,   102])"
596,1,"['mean', 'variance', 'sample', 'sample mean']", Statistical Characteristics of the Sample Average,seg_181,through the variance of the sample mean var[x̄] given by:,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([  101,  2083,  1996, 23284,  1997,  1996,  7099,  2812, 13075,  1031,
         1060,  1033,  2445,  2011,  1024,   102])"
597,0,[], Statistical Characteristics of the Sample Average,seg_181,equation 5.61 may be rewritten as:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([  101,  8522,  1019,  1012,  6079,  2089,  2022,  2128, 15773,  2004,
         1024,   102])"
598,1,"['mean', 'variance', 'sample', 'sample mean']", Statistical Characteristics of the Sample Average,seg_181,from which it is seen that the variance of the sample mean var[x̄] decreases linearly,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([  101,  2013,  2029,  2009,  2003,  2464,  2008,  1996, 23284,  1997,
         1996,  7099,  2812, 13075,  1031,  1060,  1033, 17913,  7399,  2135,
          102])"
599,1,"['function', 'probability', 'sample', 'samples']", Statistical Characteristics of the Sample Average,seg_181,as a function of the number of samples. considering the probability that the sample,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([ 101, 2004, 1037, 3853, 1997, 1996, 2193, 1997, 8168, 1012, 6195, 1996,
        9723, 2008, 1996, 7099,  102])"
600,1,"['expected value', 'range']", Statistical Characteristics of the Sample Average,seg_181,"mean x̄ will lie within a certain range around the expected value of x i.e. μx ±kσx ,",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([  101,  2812,  1060,  2097,  4682,  2306,  1037,  3056,  2846,  2105,
         1996,  3517,  3643,  1997,  1060,  1045,  1012,  1041,  1012,  1166,
         2595,  1081,  2243, 29733,  2595,  1010,   102])"
601,1,"['factor', 'band width factor']", Statistical Characteristics of the Sample Average,seg_181,it is seen from eq. 5.62 that the band width factor k may be reduced by a factor of,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([ 101, 2009, 2003, 2464, 2013, 1041, 4160, 1012, 1019, 1012, 5786, 2008,
        1996, 2316, 9381, 5387, 1047, 2089, 2022, 4359, 2011, 1037, 5387, 1997,
         102])"
602,1,"['factor', 'experiments']", Statistical Characteristics of the Sample Average,seg_181,2 by increasing the number of experiments by a factor of 4. to reduce k by a factor,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([ 101, 1016, 2011, 4852, 1996, 2193, 1997, 7885, 2011, 1037, 5387, 1997,
        1018, 1012, 2000, 5547, 1047, 2011, 1037, 5387,  102])"
603,1,"['factor', 'experiments']", Statistical Characteristics of the Sample Average,seg_181,"of 4, the number of experiments must be increased by a factor of 16. it is seen that",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([ 101, 1997, 1018, 1010, 1996, 2193, 1997, 7885, 2442, 2022, 3445, 2011,
        1037, 5387, 1997, 2385, 1012, 2009, 2003, 2464, 2008,  102])"
604,1,"['uncertainty', 'experiments']", Statistical Characteristics of the Sample Average,seg_181,it becomes increasingly expensive in terms of experiments to reduce the uncertainty,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([  101,  2009,  4150,  6233,  6450,  1999,  3408,  1997,  7885,  2000,
         5547,  1996, 12503,   102])"
605,1,"['mean', 'function', 'density function', 'probability', 'probability density function', 'sample', 'sample mean']", Statistical Characteristics of the Sample Average,seg_181,associated with the sample mean. in fig. 5.11 the probability density function of a,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([  101,  3378,  2007,  1996,  7099,  2812,  1012,  1999, 20965,  1012,
         1019,  1012,  2340,  1996,  9723,  4304,  3853,  1997,  1037,   102])"
606,1,"['sample', 'sample size', 'mean']", Statistical Characteristics of the Sample Average,seg_181,sample mean is illustrated for different values of the sample size n.,tensor(1),"tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([7778, 6459, 1997, 1996, 7099, 2779])","tensor([ 101, 7099, 2812, 2003, 7203, 2005, 2367, 5300, 1997, 1996, 7099, 2946,
        1050, 1012,  102])"
607,1,"['mean', 'variance', 'random variable', 'sample average', 'sample', 'random', 'estimator', 'sample variance', 'average', 'variable']", Statistical Characteristics of the Sample Variance,seg_183,"whereas the sample average is of interest as an estimator of the mean value μx of a random variable, the sample variance s2 is of interest as an estimator of the variance σx",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 1., 0., 0., 0.])","tensor([ 7778,  6459,  1997,  1996,  7099, 23284])","tensor([  101,  6168,  1996,  7099,  2779,  2003,  1997,  3037,  2004,  2019,
         9765,  9581,  4263,  1997,  1996,  2812,  3643,  1166,  2595,  1997,
         1037,  6721,  8023,  1010,  1996,  7099, 23284,  1055,  2475,  2003,
         1997,  3037,  2004,  2019,  9765,  9581,  4263,  1997,  1996, 23284,
         1173,  2595,   102])"
608,1,"['variance', 'sample', 'expectation', 'sample variance', 'expected value']", Statistical Characteristics of the Sample Variance,seg_183,"2 . the expected value of the sample variance is determined by taking the expectation of the sample variance as given by eq. 5.59, i.e.:",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7778,  6459,  1997,  1996,  7099, 23284])","tensor([  101,  1016,  1012,  1996,  3517,  3643,  1997,  1996,  7099, 23284,
         2003,  4340,  2011,  2635,  1996, 17626,  1997,  1996,  7099, 23284,
         2004,  2445,  2011,  1041,  4160,  1012,  1019,  1012,  5354,  1010,
         1045,  1012,  1041,  1012,  1024,   102])"
609,1,"['mean', 'variance', 'random variable', 'biased', 'independence', 'sample', 'random', 'estimator', 'sample variance', 'expected value', 'variable']", Statistical Characteristics of the Sample Variance,seg_183,"in the step going from the third line to the fourth line in eq. 5.63 the assumption of independence has been used, i.e. using that e[xixj ] = 0 for i = j . from eq. 5.63 it is noticed that the expected value of the sample variance is different from the variance of the underlying random variable. even though this difference is small for large sample sizes n, this is disturbing and essentially means that the estimator s2 is biased, i.e. its mean value is different from σx",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7778,  6459,  1997,  1996,  7099, 23284])","tensor([  101,  1999,  1996,  3357,  2183,  2013,  1996,  2353,  2240,  2000,
         1996,  2959,  2240,  1999,  1041,  4160,  1012,  1019,  1012,  6191,
         1996, 11213,  1997,  4336,  2038,  2042,  2109,  1010,  1045,  1012,
         1041,  1012,  2478,  2008,  1041,  1031,  8418,  2595,  3501,  1033,
         1027,  1014,  2005,  1045,  1027,  1046,  1012,  2013,  1041,  4160,
         1012,  1019,  1012,  6191,  2009,  2003,  4384,  2008,  1996,  3517,
         3643,  1997,  1996,  7099, 23284,  2003,  2367,  2013,  1996, 23284,
         1997,  1996, 10318,  6721,  8023,  1012,  2130,  2295,  2023,  4489,
         2003,  2235,  2005,  2312,  7099, 10826,  1050,  1010,  2023,  2003,
        14888,  1998,  7687,  2965,  2008,  1996,  9765,  9581,  4263,  1055,
         2475,  2003, 25352,  1010,  1045,  1012,  1041,  1012,  2049,  2812,
         3643,  2003,  2367,  2013,  1173,  2595,   102])"
610,1,"['estimator', 'variance']", Statistical Characteristics of the Sample Variance,seg_183,2 . an estimator of the variance σx,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0.])","tensor([ 7778,  6459,  1997,  1996,  7099, 23284])","tensor([  101,  1016,  1012,  2019,  9765,  9581,  4263,  1997,  1996, 23284,
         1173,  2595,   102])"
611,1,['unbiased'], Statistical Characteristics of the Sample Variance,seg_183,"which is unbiased su2nbiased may, however, easily be constructed from s2 as:",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 7778,  6459,  1997,  1996,  7099, 23284])","tensor([  101,  2029,  2003,  4895, 11607,  6924, 10514,  2475, 27698,  7951,
         2098,  2089,  1010,  2174,  1010,  4089,  2022,  3833,  2013,  1055,
         2475,  2004,  1024,   102])"
612,1,"['biased estimator', 'variance', 'maximum likelihood', 'biased', 'moments', 'method', 'estimator', 'likelihood']", Statistical Characteristics of the Sample Variance,seg_183,it is noted (refer to sect. 5.3) that the biased estimator s2 for the variance is applied in both the methods of moments and the maximum likelihood method.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        1., 0., 0.])","tensor([ 7778,  6459,  1997,  1996,  7099, 23284])","tensor([  101,  2009,  2003,  3264,  1006,  6523,  2000, 17831,  1012,  1019,
         1012,  1017,  1007,  2008,  1996, 25352,  9765,  9581,  4263,  1055,
         2475,  2005,  1996, 23284,  2003,  4162,  1999,  2119,  1996,  4725,
         1997,  5312,  1998,  1996,  4555, 16593,  4118,  1012,   102])"
613,1,"['mean', 'parameter', 'estimators', 'associated', 'mean square error', 'biased', 'mean square', 'estimator', 'efficiency', 'error']", Statistical Characteristics of the Sample Variance,seg_183,"the goodness of an estimator cannot, however, be judged alone on the basis of whether or not it is biased. another characteristic of estimators commonly used is the efficiency, i.e. the mean square error, associated with an estimator. if the estimator s2 of the parameter σx",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 7778,  6459,  1997,  1996,  7099, 23284])","tensor([  101,  1996, 15003,  1997,  2019,  9765,  9581,  4263,  3685,  1010,
         2174,  1010,  2022, 13224,  2894,  2006,  1996,  3978,  1997,  3251,
         2030,  2025,  2009,  2003, 25352,  1012,  2178,  8281,  1997,  9765,
         9581,  6591,  4141,  2109,  2003,  1996,  8122,  1010,  1045,  1012,
         1041,  1012,  1996,  2812,  2675,  7561,  1010,  3378,  2007,  2019,
         9765,  9581,  4263,  1012,  2065,  1996,  9765,  9581,  4263,  1055,
         2475,  1997,  1996, 16381,  1173,  2595,   102])"
614,1,"['mean', 'mean square', 'mean square error', 'error']", Statistical Characteristics of the Sample Variance,seg_183,"2 is considered, the mean square error is given by:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 7778,  6459,  1997,  1996,  7099, 23284])","tensor([ 101, 1016, 2003, 2641, 1010, 1996, 2812, 2675, 7561, 2003, 2445, 2011,
        1024,  102])"
615,1,"['sufficiency', 'estimators', 'biased estimator', 'maximum likelihood', 'biased', 'invariance', 'efficient', 'method', 'estimator', 'efficiency', 'consistency', 'likelihood', 'robustness']", Statistical Characteristics of the Sample Variance,seg_183,"the efficiency of the estimator s2 can be shown to be better than the efficiency of the estimator su2nbiased and then the choice stands between a less efficient but unbiased estimator or a more efficient but biased estimator. a number of other criteria such as invariance, consistency, sufficiency and robustness may be considered when comparing estimators. these characteristics will not be considered here but it is simply noted that the maximum likelihood method estimators in general have equally",tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 7778,  6459,  1997,  1996,  7099, 23284])","tensor([  101,  1996,  8122,  1997,  1996,  9765,  9581,  4263,  1055,  2475,
         2064,  2022,  3491,  2000,  2022,  2488,  2084,  1996,  8122,  1997,
         1996,  9765,  9581,  4263, 10514,  2475, 27698,  7951,  2098,  1998,
         2059,  1996,  3601,  4832,  2090,  1037,  2625,  8114,  2021,  4895,
        11607,  6924,  9765,  9581,  4263,  2030,  1037,  2062,  8114,  2021,
        25352,  9765,  9581,  4263,  1012,  1037,  2193,  1997,  2060,  9181,
         2107,  2004,  1999, 10755, 28335,  1010, 18700,  1010, 10514, 26989,
        29125,  1998, 15873,  2791,  2089,  2022,  2641,  2043, 13599,  9765,
         9581,  6591,  1012,  2122,  6459,  2097,  2025,  2022,  2641,  2182,
         2021,  2009,  2003,  3432,  3264,  2008,  1996,  4555, 16593,  4118,
         9765,  9581,  6591,  1999,  2236,  2031,  8053,   102])"
616,1,['estimators'], Statistical Characteristics of the Sample Variance,seg_183,good or better characteristics than any other estimators. for more details the reader is referred to benjamin and cornell [4] where further references to specialized literature are also provided.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 7778,  6459,  1997,  1996,  7099, 23284])","tensor([  101,  2204,  2030,  2488,  6459,  2084,  2151,  2060,  9765,  9581,
         6591,  1012,  2005,  2062,  4751,  1996,  8068,  2003,  3615,  2000,
         6425,  1998, 10921,  1031,  1018,  1033,  2073,  2582,  7604,  2000,
         7772,  3906,  2024,  2036,  3024,  1012,   102])"
617,1,"['confidence interval', 'statistical uncertainty', 'estimated', 'confidence', 'significance level', 'estimators', 'associated', 'interval', 'estimate', 'statistical', 'parameter', 'uncertainty', 'probability', 'level', 'confidence intervals', 'significance', 'intervals']", Confidence Intervals,seg_185,"as seen in the previous section, estimators are associated with statistical uncertainty and thus it is essential that this uncertainty is quantified and taken into account in the considered problem context. a classical approach for the quantification and the communication of this uncertainty is by the use of confidence intervals. the 1 − α confidence interval on an estimate defines an interval within which the estimated parameter will occur with a predefined probability, with α being the significance level (see fig. 5.12).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  2004,  2464,  1999,  1996,  3025,  2930,  1010,  9765,  9581,
         6591,  2024,  3378,  2007,  7778, 12503,  1998,  2947,  2009,  2003,
         6827,  2008,  2023, 12503,  2003, 24110,  3775, 10451,  1998,  2579,
         2046,  4070,  1999,  1996,  2641,  3291,  6123,  1012,  1037,  4556,
         3921,  2005,  1996, 24110,  3775, 10803,  1998,  1996,  4807,  1997,
         2023, 12503,  2003,  2011,  1996,  2224,  1997,  7023, 14025,  1012,
         1996,  1015,  1597,  1155,  7023, 13483,  2006,  2019, 10197, 11859,
         2019, 13483,  2306,  2029,  1996,  4358, 16381,  2097,  5258,  2007,
         1037,  3653,  3207, 23460,  2094,  9723,  1010,  2007,  1155,  2108,
         1996,  7784,  2504,  1006,  2156, 20965,  1012,  1019,  1012,  2260,
         1007,  1012,   102])"
618,1,"['mean', 'confidence interval', 'deviation', 'interval', 'standard deviation', 'standard', 'confidence', 'variable', 'case']", Confidence Intervals,seg_185,"if the case is considered where the standard deviation σx of an uncertain variable x is known with certainty and the mean value is unknown, then the double sided and symmetrical 1 − α confidence interval on the mean value is given by:",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  2065,  1996,  2553,  2003,  2641,  2073,  1996,  3115, 24353,
         1173,  2595,  1997,  2019,  9662,  8023,  1060,  2003,  2124,  2007,
        15855,  1998,  1996,  2812,  3643,  2003,  4242,  1010,  2059,  1996,
         3313, 11536,  1998, 23476,  1015,  1597,  1155,  7023, 13483,  2006,
         1996,  2812,  3643,  2003,  2445,  2011,  1024,   102])"
619,1,"['samples', 'estimation', 'mean']", Confidence Intervals,seg_185,where n is the number of samples planned for the estimation of the mean value.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  2073,  1050,  2003,  1996,  2193,  1997,  8168,  3740,  2005,
         1996, 24155,  1997,  1996,  2812,  3643,  1012,   102])"
620,1,"['mean', 'confidence interval', 'deviation', 'interval', 'normal', 'standard deviation', 'standard', 'confidence', 'case']", Confidence Intervals,seg_185,"considering the case of a normal distributed yield stress of mild construction steel, and assuming that the standard deviation of the yield stress is known to be 20 mpa and the mean value is unknown, the 0.95 confidence interval of the mean value is given by:",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  6195,  1996,  2553,  1997,  1037,  3671,  5500, 10750,  6911,
         1997, 10256,  2810,  3886,  1010,  1998, 10262,  2008,  1996,  3115,
        24353,  1997,  1996, 10750,  6911,  2003,  2124,  2000,  2022,  2322,
         6131,  2050,  1998,  1996,  2812,  3643,  2003,  4242,  1010,  1996,
         1014,  1012,  5345,  7023, 13483,  1997,  1996,  2812,  3643,  2003,
         2445,  2011,  1024,   102])"
621,1,"['function', 'cumulative distribution function', 'table', 'standard normal', 'normal', 'standard', 'distribution', 'distribution function', 'percentile']", Confidence Intervals,seg_185,where −1.96 and 1.96 are the simple lower 2.5 and upper 2.5 percentile values of the standard normal cumulative distribution function (see also table c.1 and appendix b.1) and determined by:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  2073,  1597,  2487,  1012,  5986,  1998,  1015,  1012,  5986,
         2024,  1996,  3722,  2896,  1016,  1012,  1019,  1998,  3356,  1016,
         1012,  1019,  3867,  9463,  5300,  1997,  1996,  3115,  3671, 23260,
         4353,  3853,  1006,  2156,  2036,  2795,  1039,  1012,  1015,  1998,
        22524,  1038,  1012,  1015,  1007,  1998,  4340,  2011,  1024,   102])"
622,1,"['function', 'cumulative distribution function', 'standard normal', 'normal', 'standard', 'distribution', 'distribution function']", Confidence Intervals,seg_185,where φ−1(·) is the inverse standard normal cumulative distribution function.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  2073,  1176, 27944,  1006,  1087,  1007,  2003,  1996, 19262,
         3115,  3671, 23260,  4353,  3853,  1012,   102])"
623,1,['experiments'], Confidence Intervals,seg_185,"assuming that 16 experiments are planned, eq. 5.67 yields:",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 14025])","tensor([  101, 10262,  2008,  2385,  7885,  2024,  3740,  1010,  1041,  4160,
         1012,  1019,  1012,  6163, 16189,  1024,   102])"
624,1,"['mean', 'confidence interval', 'probability', 'interval', 'states', 'hypothesis', 'sample average', 'sample', 'confidence', 'average', 'sample mean']", Confidence Intervals,seg_185,"a 95% confidence interval for the true mean μx can then be derived with a given sample mean. using eq. 5.69, a confidence interval for the sample mean can also be established based on a hypothesis for the true mean, which simply states that with a probability of 0.95, the sample average of the steel yield stress will lie within an interval of ±9.8 mpa of the true mean value.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  1037,  5345,  1003,  7023, 13483,  2005,  1996,  2995,  2812,
         1166,  2595,  2064,  2059,  2022,  5173,  2007,  1037,  2445,  7099,
         2812,  1012,  2478,  1041,  4160,  1012,  1019,  1012,  6353,  1010,
         1037,  7023, 13483,  2005,  1996,  7099,  2812,  2064,  2036,  2022,
         2511,  2241,  2006,  1037, 10744,  2005,  1996,  2995,  2812,  1010,
         2029,  3432,  2163,  2008,  2007,  1037,  9723,  1997,  1014,  1012,
         5345,  1010,  1996,  7099,  2779,  1997,  1996,  3886, 10750,  6911,
         2097,  4682,  2306,  2019, 13483,  1997,  1081,  2683,  1012,  1022,
         6131,  2050,  1997,  1996,  2995,  2812,  3643,  1012,   102])"
625,1,"['cases', 'confidence interval', 'interval', 'experiments', 'intervals', 'confidence intervals', 'confidence']", Confidence Intervals,seg_185,"from eq. 5.66 it is seen that the confidence interval limits depend on α, n and σx . typically α is chosen as 0.1, 0.05 and 0.01 in engineering applications. narrow confidence intervals may be achieved by increasing the number of experiments, which on the other hand, may be expensive to achieve and in some cases not even possible for practical reasons.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  2013,  1041,  4160,  1012,  1019,  1012,  5764,  2009,  2003,
         2464,  2008,  1996,  7023, 13483,  6537, 12530,  2006,  1155,  1010,
         1050,  1998,  1173,  2595,  1012,  4050,  1155,  2003,  4217,  2004,
         1014,  1012,  1015,  1010,  1014,  1012,  5709,  1998,  1014,  1012,
         5890,  1999,  3330,  5097,  1012,  4867,  7023, 14025,  2089,  2022,
         4719,  2011,  4852,  1996,  2193,  1997,  7885,  1010,  2029,  2006,
         1996,  2060,  2192,  1010,  2089,  2022,  6450,  2000,  6162,  1998,
         1999,  2070,  3572,  2025,  2130,  2825,  2005,  6742,  4436,  1012,
          102])"
626,1,"['variability', 'mean', 'concentration', 'set', 'design', 'unbiased', 'probabilistic', 'probabilistic models', 'tests', 'samples', 'model', 'data']", Testing for Statistical Significance,seg_187,"in practical engineering problems, the engineer is often confronted with the challenge of deriving simple operational conclusions based on an often small set of data exhibiting a high degree of variability. an example of such a situation is the geotechnical engineer attempting, by means of a limited number of “on-site” vane tests, to verify that an empirical soil strength model based on soil specimen laboratory tests is unbiased. another example is the materials expert pursuing to verify, by analysis of the chloride content of drilled concrete cylinders samples, that the mean value of the surface concentration of chlorides on a concrete structure can be assumed equal to the value assumed in the design basis for the structure. yet another kind of problem is the selection and/or verification of probabilistic models as shall be seen later in sect. 5.9.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([5604, 2005, 7778, 7784])","tensor([  101,  1999,  6742,  3330,  3471,  1010,  1996,  3992,  2003,  2411,
        12892,  2007,  1996,  4119,  1997,  4315, 14966,  3722,  6515, 15306,
         2241,  2006,  2019,  2411,  2235,  2275,  1997,  2951, 22922,  1037,
         2152,  3014,  1997, 28436,  1012,  2019,  2742,  1997,  2107,  1037,
         3663,  2003,  1996, 20248, 15007, 20913,  3992,  7161,  1010,  2011,
         2965,  1997,  1037,  3132,  2193,  1997,  1523,  2006,  1011,  2609,
         1524, 23334,  5852,  1010,  2000, 20410,  2008,  2019, 17537,  5800,
         3997,  2944,  2241,  2006,  5800, 11375,  5911,  5852,  2003,  4895,
        11607,  6924,  1012,  2178,  2742,  2003,  1996,  4475,  6739, 11828,
         2000, 20410,  1010,  2011,  4106,  1997,  1996, 19057,  4180,  1997,
        24311,  5509, 18729,  8168,  1010,  2008,  1996,  2812,  3643,  1997,
         1996,  3302,  6693,  1997, 19057,  2015,  2006,  1037,  5509,  3252,
         2064,  2022,  5071,  5020,  2000,  1996,  3643,  5071,  1999,  1996,
         2640,  3978,  2005,  1996,  3252,  1012,  2664,  2178,  2785,  1997,
         3291,  2003,  1996,  4989,  1998,  1013,  2030, 22616,  1997,  4013,
         3676, 27965,  4588,  4275,  2004,  4618,  2022,  2464,  2101,  1999,
        17831,  1012,  1019,  1012,  1023,  1012,   102])"
627,1,['case'], Testing for Statistical Significance,seg_187,"it is essential that the basis for conclusions in problems such as those outlined above is made consistently from case to case (and from engineer to engineer), and",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([5604, 2005, 7778, 7784])","tensor([  101,  2009,  2003,  6827,  2008,  1996,  3978,  2005, 15306,  1999,
         3471,  2107,  2004,  2216, 14801,  2682,  2003,  2081, 10862,  2013,
         2553,  2000,  2553,  1006,  1998,  2013,  3992,  2000,  3992,  1007,
         1010,  1998,   102])"
628,1,"['variability', 'data']", Testing for Statistical Significance,seg_187,"that the variability of the observed data and the amount of data is taken appropriately into account. one approach which facilitates the support of such conclusions is the formulation and testing of hypothesis—hypothesis testing, which will be introduced in the following section.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([5604, 2005, 7778, 7784])","tensor([  101,  2008,  1996, 28436,  1997,  1996,  5159,  2951,  1998,  1996,
         3815,  1997,  2951,  2003,  2579, 23263,  2046,  4070,  1012,  2028,
         3921,  2029, 27777,  1996,  2490,  1997,  2107, 15306,  2003,  1996,
        20219,  1998,  5604,  1997, 10744,  1517, 10744,  5604,  1010,  2029,
         2097,  2022,  3107,  1999,  1996,  2206,  2930,  1012,   102])"
629,1,"['cases', 'sample statistics', 'functions', 'design', 'operating rules', 'data set', 'mean', 'locations', 'statistics', 'data', 'statistic', 'percentage', 'hypothesis', 'sample average', 'hypothesis testing', 'average', 'set', 'sample', 'concentration']", Testing for Statistical Significance,seg_187,"consider the example concerning the surface concentration of chlorides on a concrete structure. in the design basis for the structure, it was assumed that the surface concentration of chlorides (measured in percentage of total concrete weight) would be 0.3%. suppose now that the materials expert has studied the chloride contents of concrete cylinders taken from 10 different locations of the considered structure. even though the materials expert has collected a data set of 10 surface concentration values, the observed mean value, also called the sample average x̄ (in general terms one of several possible sample statistics, i.e. functions of the tested or otherwise observed data), will in some cases be below and in some cases above the true mean μx of the surface chloride concentration. the question is if it can be concluded, on the basis of the observed statistic, that the sample average deviates statistically significantly from the assumed mean value. to arrive at the solution of such problems hypothesis testing includes operating rules that describe how to reach a conclusion, which provides a means for assessing the percentage α of times where the reached conclusions are wrong in one way or the other.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([5604, 2005, 7778, 7784])","tensor([  101,  5136,  1996,  2742,  7175,  1996,  3302,  6693,  1997, 19057,
         2015,  2006,  1037,  5509,  3252,  1012,  1999,  1996,  2640,  3978,
         2005,  1996,  3252,  1010,  2009,  2001,  5071,  2008,  1996,  3302,
         6693,  1997, 19057,  2015,  1006,  7594,  1999,  7017,  1997,  2561,
         5509,  3635,  1007,  2052,  2022,  1014,  1012,  1017,  1003,  1012,
         6814,  2085,  2008,  1996,  4475,  6739,  2038,  3273,  1996, 19057,
         8417,  1997,  5509, 18729,  2579,  2013,  2184,  2367,  5269,  1997,
         1996,  2641,  3252,  1012,  2130,  2295,  1996,  4475,  6739,  2038,
         5067,  1037,  2951,  2275,  1997,  2184,  3302,  6693,  5300,  1010,
         1996,  5159,  2812,  3643,  1010,  2036,  2170,  1996,  7099,  2779,
         1060,  1006,  1999,  2236,  3408,  2028,  1997,  2195,  2825,  7099,
         6747,  1010,  1045,  1012,  1041,  1012,  4972,  1997,  1996,  7718,
         2030,  4728,  5159,  2951,  1007,  1010,  2097,  1999,  2070,  3572,
         2022,  2917,  1998,  1999,  2070,  3572,  2682,  1996,  2995,  2812,
         1166,  2595,  1997,  1996,  3302, 19057,  6693,  1012,  1996,  3160,
         2003,  2065,  2009,  2064,  2022,  5531,  1010,  2006,  1996,  3978,
         1997,  1996,  5159, 28093,  6553,  1010,  2008,  1996,  7099,  2779,
        14386,  8520,  7778,  2135,  6022,  2013,  1996,  5071,  2812,  3643,
         1012,  2000,  7180,  2012,  1996,  5576,  1997,  2107,  3471, 10744,
         5604,  2950,  4082,  3513,  2008,  6235,  2129,  2000,  3362,  1037,
         7091,  1010,  2029,  3640,  1037,  2965,  2005, 20077,  1996,  7017,
         1155,  1997,  2335,  2073,  1996,  2584, 15306,  2024,  3308,  1999,
         2028,  2126,  2030,  1996,  2060,  1012,   102])"
630,1,"['mean', 'operating rule', 'concentration', 'probability', 'interval', 'hypothesis', 'sample average', 'sample', 'results', 'average', 'alternate hypothesis', 'test results', 'test']", The Hypothesis Testing Procedure,seg_189,"continuing with the example introduced in the foregoing, a first step is the formulation of the null-hypothesis h0, i.e. expressing that the true mean value μx of the surface chloride concentration is equal to the assumed value of 0.3%. the next step is to formulate an operating rule on the basis of which the null-hypothesis can be either accepted or rejected given the test results. an operating rule could be to accept the null-hypothesis h0 if the sample average x̄ of the surface chloride concentration is within the interval 0.3% ±δ or otherwise to reject it. rejecting the null-hypothesis implies accepting the alternate hypothesis h1 that the true mean value μx is different from the assumed value. typically, the value δ is selected such that the probability α if the sample average x̄ being outside the interval given by δ is small, say 0.1.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 1996, 10744,  5604,  7709])","tensor([  101,  5719,  2007,  1996,  2742,  3107,  1999,  1996, 18921, 26966,
         1010,  1037,  2034,  3357,  2003,  1996, 20219,  1997,  1996, 19701,
         1011, 10744,  1044,  2692,  1010,  1045,  1012,  1041,  1012, 14026,
         2008,  1996,  2995,  2812,  3643,  1166,  2595,  1997,  1996,  3302,
        19057,  6693,  2003,  5020,  2000,  1996,  5071,  3643,  1997,  1014,
         1012,  1017,  1003,  1012,  1996,  2279,  3357,  2003,  2000,  5675,
         2618,  2019,  4082,  3627,  2006,  1996,  3978,  1997,  2029,  1996,
        19701,  1011, 10744,  2064,  2022,  2593,  3970,  2030,  5837,  2445,
         1996,  3231,  3463,  1012,  2019,  4082,  3627,  2071,  2022,  2000,
         5138,  1996, 19701,  1011, 10744,  1044,  2692,  2065,  1996,  7099,
         2779,  1060,  1997,  1996,  3302, 19057,  6693,  2003,  2306,  1996,
        13483,  1014,  1012,  1017,  1003,  1081, 29722,  2030,  4728,  2000,
        15454,  2009,  1012, 21936,  1996, 19701,  1011, 10744, 12748, 10564,
         1996,  6585, 10744,  1044,  2487,  2008,  1996,  2995,  2812,  3643,
         1166,  2595,  2003,  2367,  2013,  1996,  5071,  3643,  1012,  4050,
         1010,  1996,  3643,  1158,  2003,  3479,  2107,  2008,  1996,  9723,
         1155,  2065,  1996,  7099,  2779,  1060,  2108,  2648,  1996, 13483,
         2445,  2011,  1158,  2003,  2235,  1010,  2360,  1014,  1012,  1015,
         1012,   102])"
631,1,"['type ii errors', 'consequences', 'associated', 'table', 'bayesian', 'type ii', 'errors', 'type i and type ii errors']", The Hypothesis Testing Procedure,seg_189,"two types of errors may occur, namely, rejecting the null-hypothesis h0 when it is true or accepting it when it is false. these two different types of errors are referred to as type i and type ii errors respectively (table 5.4). it is important to note that performing type i as well as type ii errors may be associated with severe consequences. the selection of an appropriate value for α should reflect this. a possible approach for the selection of α is using bayesian decision analysis. the general principles of bayesian decision analysis are introduced in chap. 7.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996, 10744,  5604,  7709])","tensor([  101,  2048,  4127,  1997, 10697,  2089,  5258,  1010,  8419,  1010,
        21936,  1996, 19701,  1011, 10744,  1044,  2692,  2043,  2009,  2003,
         2995,  2030, 10564,  2009,  2043,  2009,  2003,  6270,  1012,  2122,
         2048,  2367,  4127,  1997, 10697,  2024,  3615,  2000,  2004,  2828,
         1045,  1998,  2828,  2462, 10697,  4414,  1006,  2795,  1019,  1012,
         1018,  1007,  1012,  2009,  2003,  2590,  2000,  3602,  2008,  4488,
         2828,  1045,  2004,  2092,  2004,  2828,  2462, 10697,  2089,  2022,
         3378,  2007,  5729,  8465,  1012,  1996,  4989,  1997,  2019,  6413,
         3643,  2005,  1155,  2323,  8339,  2023,  1012,  1037,  2825,  3921,
         2005,  1996,  4989,  1997,  1155,  2003,  2478,  3016, 25253,  3247,
         4106,  1012,  1996,  2236,  6481,  1997,  3016, 25253,  3247,  4106,
         2024,  3107,  1999, 15775,  2361,  1012,  1021,  1012,   102])"
632,0,[], The Hypothesis Testing Procedure,seg_189,in summary the procedure is:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 1996, 10744,  5604,  7709])","tensor([  101,  1999, 12654,  1996,  7709,  2003,  1024,   102])"
633,1,"['condition', 'type i error', 'operating rules', 'significance level', 'observation', 'statistical', 'alternate hypothesis', 'statistic', 'test', 'hypotheses', 'consequences', 'operating rule', 'probability', 'hypothesis', 'level', 'significance', 'sample', 'sample statistic', 'experiment', 'error']", The Hypothesis Testing Procedure,seg_189,"• formulate a null-hypothesis h0 expressing that the desired condition is fulfilled and formulate the alternate hypothesis h1. both hypotheses should be formulated in terms of a sample statistic. • formulate an operating rule such that the formulated null-hypothesis h0 may easily be either accepted or rejected on the basis of the observation of the sample statistic. operating rules are typically formulated by means of a constant δ. • select a significance level α for conducting the test (i.e. the probability of occurrence of a type i error). this should be done with due consideration of the consequences of performing this type of error. • by statistical analysis of the sample statistic, identify the value of δ resulting in a probability α of performing a type i error. • perform the planned testing, evaluate the corresponding sample statistic and check which hypothesis is supported by the experiment. • provided that the null-hypothesis h0 is not supported by the experiment, it is classified as significant at the α -significance level and rejected; otherwise it is accepted.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996, 10744,  5604,  7709])","tensor([  101,  1528,  5675,  2618,  1037, 19701,  1011, 10744,  1044,  2692,
        14026,  2008,  1996,  9059,  4650,  2003, 16829,  1998,  5675,  2618,
         1996,  6585, 10744,  1044,  2487,  1012,  2119,  1044, 22571, 14573,
        23072,  2323,  2022, 19788,  1999,  3408,  1997,  1037,  7099, 28093,
         6553,  1012,  1528,  5675,  2618,  2019,  4082,  3627,  2107,  2008,
         1996, 19788, 19701,  1011, 10744,  1044,  2692,  2089,  4089,  2022,
         2593,  3970,  2030,  5837,  2006,  1996,  3978,  1997,  1996,  8089,
         1997,  1996,  7099, 28093,  6553,  1012,  4082,  3513,  2024,  4050,
        19788,  2011,  2965,  1997,  1037,  5377,  1158,  1012,  1528,  7276,
         1037,  7784,  2504,  1155,  2005,  9283,  1996,  3231,  1006,  1045,
         1012,  1041,  1012,  1996,  9723,  1997, 14404,  1997,  1037,  2828,
         1045,  7561,  1007,  1012,  2023,  2323,  2022,  2589,  2007,  2349,
         9584,  1997,  1996,  8465,  1997,  4488,  2023,  2828,  1997,  7561,
         1012,  1528,  2011,  7778,  4106,  1997,  1996,  7099, 28093,  6553,
         1010,  6709,  1996,  3643,  1997,  1158,  4525,  1999,  1037,  9723,
         1155,  1997,  4488,  1037,  2828,  1045,  7561,  1012,  1528,  4685,
         1996,  3740,  5604,  1010, 16157,  1996,  7978,  7099, 28093,  6553,
         1998,  4638,  2029, 10744,  2003,  3569,  2011,  1996,  7551,  1012,
         1528,  3024,  2008,  1996, 19701,  1011, 10744,  1044,  2692,  2003,
         2025,  3569,  2011,  1996,  7551,  1010,  2009,  2003,  6219,  2004,
         3278,  2012,  1996,  1155,  1011,  7784,  2504,  1998,  5837,  1025,
         4728,  2009,  2003,  3970,  1012,   102])"
634,1,"['mean', 'range', 'confidence interval', 'probability', 'interval', 'hypothesis', 'sample', 'random', 'hypothesis testing', 'population', 'confidence', 'expected value', 'sample mean']", The Hypothesis Testing Procedure,seg_189,"figure 5.13 shows the difference between the confidence interval and the interval for hypothesis testing. if the interest concerns the expected value of a random variable, the confidence interval indicates a range deduced by the sample where the true mean lies with a certain probability. in hypothesis testing, an interval is specified where the sample mean has to be located so that the hypothesis for the true mean of the population can be accepted.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 1996, 10744,  5604,  7709])","tensor([  101,  3275,  1019,  1012,  2410,  3065,  1996,  4489,  2090,  1996,
         7023, 13483,  1998,  1996, 13483,  2005, 10744,  5604,  1012,  2065,
         1996,  3037,  5936,  1996,  3517,  3643,  1997,  1037,  6721,  8023,
         1010,  1996,  7023, 13483,  7127,  1037,  2846,  2139, 28901,  2011,
         1996,  7099,  2073,  1996,  2995,  2812,  3658,  2007,  1037,  3056,
         9723,  1012,  1999, 10744,  5604,  1010,  2019, 13483,  2003,  9675,
         2073,  1996,  7099,  2812,  2038,  2000,  2022,  2284,  2061,  2008,
         1996, 10744,  2005,  1996,  2995,  2812,  1997,  1996,  2313,  2064,
         2022,  3970,  1012,   102])"
635,1,"['cases', 'significance testing', 'significance']", The Hypothesis Testing Procedure,seg_189,in the following section a selection of typical cases for significance testing will be presented.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 1996, 10744,  5604,  7709])","tensor([ 101, 1999, 1996, 2206, 2930, 1037, 4989, 1997, 5171, 3572, 2005, 7784,
        5604, 2097, 2022, 3591, 1012,  102])"
636,1,"['variance', 'concentration']", Testing of the Mean with Known Variance,seg_191,"the example of the surface concentration of chlorides on a concrete structure is considered again. based on extensive experience obtained from the assessment of many structures, the variance of the chloride surface concentration is assumed to be equal to σx2 = (0.04)2 = 0.0016.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 5604,  1997,  1996,  2812,  2007,  2124, 23284])","tensor([  101,  1996,  2742,  1997,  1996,  3302,  6693,  1997, 19057,  2015,
         2006,  1037,  5509,  3252,  2003,  2641,  2153,  1012,  2241,  2006,
         4866,  3325,  4663,  2013,  1996,  7667,  1997,  2116,  5090,  1010,
         1996, 23284,  1997,  1996, 19057,  3302,  6693,  2003,  5071,  2000,
         2022,  5020,  2000,  1173,  2595,  2475,  1027,  1006,  1014,  1012,
         5840,  1007,  1016,  1027,  1014,  1012, 25604,  2575,  1012,   102])"
637,1,"['mean', 'operating rule', 'hypothesis', 'level', 'sample average', 'sample', 'normal', 'null hypothesis', 'average', 'alternate hypothesis', 'statistic']", Testing of the Mean with Known Variance,seg_191,let the null hypothesis h0 be formulated as the true mean μx being equal to 0.3%. the alternate hypothesis h1 is then simply given by μx = 0.3%. the considered statistic is the sample average x̄ which may be assumed to be normal distributed. the operating rule specifies that the null hypothesis h0 should be accepted at the α-significance level if:,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 5604,  1997,  1996,  2812,  2007,  2124, 23284])","tensor([  101,  2292,  1996, 19701, 10744,  1044,  2692,  2022, 19788,  2004,
         1996,  2995,  2812,  1166,  2595,  2108,  5020,  2000,  1014,  1012,
         1017,  1003,  1012,  1996,  6585, 10744,  1044,  2487,  2003,  2059,
         3432,  2445,  2011,  1166,  2595,  1027,  1014,  1012,  1017,  1003,
         1012,  1996,  2641, 28093,  6553,  2003,  1996,  7099,  2779,  1060,
         2029,  2089,  2022,  5071,  2000,  2022,  3671,  5500,  1012,  1996,
         4082,  3627, 27171,  2008,  1996, 19701, 10744,  1044,  2692,  2323,
         2022,  3970,  2012,  1996,  1155,  1011,  7784,  2504,  2065,  1024,
          102])"
638,1,"['probability', 'interval']", Testing of the Mean with Known Variance,seg_191,where δ is determined such that the probability of x̄ being outside the interval is equal to α i.e.:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 5604,  1997,  1996,  2812,  2007,  2124, 23284])","tensor([  101,  2073,  1158,  2003,  4340,  2107,  2008,  1996,  9723,  1997,
         1060,  2108,  2648,  1996, 13483,  2003,  5020,  2000,  1155,  1045,
         1012,  1041,  1012,  1024,   102])"
639,1,"['significance level', 'interval', 'hypothesis', 'level', 'sample average', 'sample', 'samples', 'null hypothesis', 'significance', 'average']", Testing of the Mean with Known Variance,seg_191,"yielding δ = 0.0208. if the observed sample average over 10 samples lies within the interval [0.28;0.32], the null hypothesis μx = 0.3% cannot be rejected at the α = 0.1 significance level.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0.])","tensor([ 5604,  1997,  1996,  2812,  2007,  2124, 23284])","tensor([  101, 21336,  1158,  1027,  1014,  1012,  6185,  2692,  2620,  1012,
         2065,  1996,  5159,  7099,  2779,  2058,  2184,  8168,  3658,  2306,
         1996, 13483,  1031,  1014,  1012,  2654,  1025,  1014,  1012,  3590,
         1033,  1010,  1996, 19701, 10744,  1166,  2595,  1027,  1014,  1012,
         1017,  1003,  3685,  2022,  5837,  2012,  1996,  1155,  1027,  1014,
         1012,  1015,  7784,  2504,  1012,   102])"
640,1,"['set', 'interval', 'hypothesis', 'sample', 'null hypothesis', 'data set', 'data']", Testing of the Mean with Known Variance,seg_191,"assuming that investigations of the material expert resulted in the following data set x̂ = (0.33,0.32,0.25,0.31,0.28,0.27,0.29,0.3,0.27,0.28)t the sample average is equal to x̄ = 0.29. this is seen to be within the boundaries of the interval given by δ and the null hypothesis h0 cannot be rejected.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 5604,  1997,  1996,  2812,  2007,  2124, 23284])","tensor([  101, 10262,  2008,  9751,  1997,  1996,  3430,  6739,  4504,  1999,
         1996,  2206,  2951,  2275,  1060,  1027,  1006,  1014,  1012,  3943,
         1010,  1014,  1012,  3590,  1010,  1014,  1012,  2423,  1010,  1014,
         1012,  2861,  1010,  1014,  1012,  2654,  1010,  1014,  1012,  2676,
         1010,  1014,  1012,  2756,  1010,  1014,  1012,  1017,  1010,  1014,
         1012,  2676,  1010,  1014,  1012,  2654,  1007,  1056,  1996,  7099,
         2779,  2003,  5020,  2000,  1060,  1027,  1014,  1012,  2756,  1012,
         2023,  2003,  2464,  2000,  2022,  2306,  1996,  7372,  1997,  1996,
        13483,  2445,  2011,  1158,  1998,  1996, 19701, 10744,  1044,  2692,
         3685,  2022,  5837,  1012,   102])"
641,1,"['mean', 'sets', 'case', 'variance', 'tests', 'data sets', 'data']", Some Remarks on Testing,seg_193,"in the foregoing, a simple case of testing of the mean with known variance has been introduced for the assessment of observed data. in the literature, several other tests such as the testing of the mean with unknown variance, the testing of the variance and the testing for comparison of two or more data sets can be found.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 2070, 12629,  2006,  5604])","tensor([  101,  1999,  1996, 18921, 26966,  1010,  1037,  3722,  2553,  1997,
         5604,  1997,  1996,  2812,  2007,  2124, 23284,  2038,  2042,  3107,
         2005,  1996,  7667,  1997,  5159,  2951,  1012,  1999,  1996,  3906,
         1010,  2195,  2060,  5852,  2107,  2004,  1996,  5604,  1997,  1996,
         2812,  2007,  4242, 23284,  1010,  1996,  5604,  1997,  1996, 23284,
         1998,  1996,  5604,  2005,  7831,  1997,  2048,  2030,  2062,  2951,
         4520,  2064,  2022,  2179,  1012,   102])"
642,1,"['type ii errors', 'type ii', 'tests', 'errors', 'null hypothesis', 'significance level', 'decision problem', 'probability', 'hypothesis', 'level', 'significance', 'type i and type ii errors', 'variance']", Some Remarks on Testing,seg_193,"for all these tests, it should be noted that the different ways of formulating the null hypothesis h0 and the different choices of the significance level α have an impact on the probability of the type i and type ii errors, respectively. the optimal choice is a decision problem which can be solved by considering a proper weighing of costs and benefits. this is also reflected in the way different organizations formulate their null hypothesis h0. an organization buying goods from a producing organization tends to postulate that the quality of the goods is below a given criterion, unless it can be shown by testing to be statistically significantly above the specified criterion. this encourages the producing organization to attempt to reduce the variance of the quality of the produced goods.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 2070, 12629,  2006,  5604])","tensor([  101,  2005,  2035,  2122,  5852,  1010,  2009,  2323,  2022,  3264,
         2008,  1996,  2367,  3971,  1997,  5675,  3436,  1996, 19701, 10744,
         1044,  2692,  1998,  1996,  2367,  9804,  1997,  1996,  7784,  2504,
         1155,  2031,  2019,  4254,  2006,  1996,  9723,  1997,  1996,  2828,
         1045,  1998,  2828,  2462, 10697,  1010,  4414,  1012,  1996, 15502,
         3601,  2003,  1037,  3247,  3291,  2029,  2064,  2022, 13332,  2011,
         6195,  1037,  5372, 15243,  1997,  5366,  1998,  6666,  1012,  2023,
         2003,  2036,  7686,  1999,  1996,  2126,  2367,  4411,  5675,  2618,
         2037, 19701, 10744,  1044,  2692,  1012,  2019,  3029,  9343,  5350,
         2013,  1037,  5155,  3029, 12102,  2000,  2695,  9869,  2008,  1996,
         3737,  1997,  1996,  5350,  2003,  2917,  1037,  2445, 19229,  1010,
         4983,  2009,  2064,  2022,  3491,  2011,  5604,  2000,  2022,  7778,
         2135,  6022,  2682,  1996,  9675, 19229,  1012,  2023, 16171,  1996,
         5155,  3029,  2000,  3535,  2000,  5547,  1996, 23284,  1997,  1996,
         3737,  1997,  1996,  2550,  5350,  1012,   102])"
643,1,"['probability', 'probabilistic', 'probabilistic model', 'distribution', 'model', 'parameters', 'data', 'probability distribution']", Some Remarks on Testing,seg_193,"lecture 11 (aim of the present lecture) the aim of the present lecture is to address the problem of model verification and comparison. having established a probabilistic model in terms of a probability distribution and estimated probability distribution parameters, the issue here is how to evaluate the appropriateness of the established model by means of data. furthermore, in order to provide guidance on the comparison of the goodness of equally acceptable models, a basis for the comparison of two or more models is provided. on the basis of the lecture it is expected that the reader will acquire knowledge and skills with regard to:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2070, 12629,  2006,  5604])","tensor([  101,  8835,  2340,  1006,  6614,  1997,  1996,  2556,  8835,  1007,
         1996,  6614,  1997,  1996,  2556,  8835,  2003,  2000,  4769,  1996,
         3291,  1997,  2944, 22616,  1998,  7831,  1012,  2383,  2511,  1037,
         4013,  3676, 27965,  4588,  2944,  1999,  3408,  1997,  1037,  9723,
         4353,  1998,  4358,  9723,  4353, 11709,  1010,  1996,  3277,  2182,
         2003,  2129,  2000, 16157,  1996,  6413,  2791,  1997,  1996,  2511,
         2944,  2011,  2965,  1997,  2951,  1012,  7297,  1010,  1999,  2344,
         2000,  3073,  8606,  2006,  1996,  7831,  1997,  1996, 15003,  1997,
         8053, 11701,  4275,  1010,  1037,  3978,  2005,  1996,  7831,  1997,
         2048,  2030,  2062,  4275,  2003,  3024,  1012,  2006,  1996,  3978,
         1997,  1996,  8835,  2009,  2003,  3517,  2008,  1996,  8068,  2097,
         9878,  3716,  1998,  4813,  2007,  7634,  2000,  1024,   102])"
644,1,"['tests', 'goodness of fit', 'fit test', 'random variable', 'probabilistic', 'statistical', 'model', 'test', 'continuous', 'probabilistic models', 'continuous random variable', 'statistical tests', 'random', 'variable']", Some Remarks on Testing,seg_193,• how can probabilistic models be evaluated and validated on the basis of statistical tests? • what is the idea behind the χ2-goodness of fit test and how is it performed? • how is the χ2-goodness of fit test applied for the testing of the validity of continuous random variable models? • what is the idea behind the kolmogorov-smirnov goodness of fit test and how is it performed? • how is the quantile-plot related to the kolmogorov-smirnov goodness of fit test? • how conclusive are statistical tests for the purpose of model verification and what must be kept in mind? • how can probabilistic models be compared with regard to appropriateness?,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2070, 12629,  2006,  5604])","tensor([  101,  1528,  2129,  2064,  4013,  3676, 27965,  4588,  4275,  2022,
        16330,  1998,  9398,  4383,  2006,  1996,  3978,  1997,  7778,  5852,
         1029,  1528,  2054,  2003,  1996,  2801,  2369,  1996,  1177,  2475,
         1011, 15003,  1997,  4906,  3231,  1998,  2129,  2003,  2009,  2864,
         1029,  1528,  2129,  2003,  1996,  1177,  2475,  1011, 15003,  1997,
         4906,  3231,  4162,  2005,  1996,  5604,  1997,  1996, 16406,  1997,
         7142,  6721,  8023,  4275,  1029,  1528,  2054,  2003,  1996,  2801,
         2369,  1996, 12849, 13728, 22844, 12298,  1011, 15488,  4313, 16693,
        15003,  1997,  4906,  3231,  1998,  2129,  2003,  2009,  2864,  1029,
         1528,  2129,  2003,  1996, 24110, 15286,  1011,  5436,  3141,  2000,
         1996, 12849, 13728, 22844, 12298,  1011, 15488,  4313, 16693, 15003,
         1997,  4906,  3231,  1029,  1528,  2129,  9530, 23633,  2024,  7778,
         5852,  2005,  1996,  3800,  1997,  2944, 22616,  1998,  2054,  2442,
         2022,  2921,  1999,  2568,  1029,  1528,  2129,  2064,  4013,  3676,
        27965,  4588,  4275,  2022,  4102,  2007,  7634,  2000,  6413,  2791,
         1029,   102])"
645,1,"['probability distributions', 'probability', 'observations', 'distributions', 'experimental', 'method', 'results', 'data']", Model Evaluation by Statistical Testing,seg_195,"in sect. 5.2.1, a highly qualitative method—the probability paper—was introduced for the identification of a family or type of probability distributions representing data obtained from observations or experimental results. this method, in conjunction with a physical understanding of the mechanism generating the observed data,",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,
        0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0.])","tensor([2944, 9312, 2011, 7778, 5604])","tensor([  101,  1999, 17831,  1012,  1019,  1012,  1016,  1012,  1015,  1010,
         1037,  3811, 24209, 11475, 27453,  4118,  1517,  1996,  9723,  3259,
         1517,  2001,  3107,  2005,  1996,  8720,  1997,  1037,  2155,  2030,
         2828,  1997,  9723, 20611,  5052,  2951,  4663,  2013,  9420,  2030,
         6388,  3463,  1012,  2023,  4118,  1010,  1999,  9595,  2007,  1037,
         3558,  4824,  1997,  1996,  7337, 11717,  1996,  5159,  2951,  1010,
          102])"
646,1,"['estimated', 'maximum likelihood', 'method', 'tests', 'quantitative', 'goodness of fit', 'model building', 'distribution function', 'model', 'statistical', 'probability distribution', 'method of moments', 'function', 'probability', 'parameters', 'range', 'moments', 'likelihood', 'distribution']", Model Evaluation by Statistical Testing,seg_195,"represents a very pragmatic approach for establishing at least a preliminary model assumption. the next step in model building typically concerns the assessment of the parameters of the assumed distribution function and to this end the method of moments and the maximum likelihood method were introduced. from the foregoing sections it is obvious that the quality of the established model is a combination of the appropriateness of the selected probability distribution and the estimated parameters, i.e. the goodness of fit. it would thus be of significant interest to be able to assess the goodness of fit in a quantitative way thereby enabling for a systematic and consistent way of justifying or rejecting model assumptions. for this purpose, the classical statistical distribution tests, i.e. the goodness of fit tests have been developed. a number of different types of tests have been developed in the past, in part with very specialized and, consequently, limited applicability over different application areas. in the following sections, two such tests—the χ2- and the kolmogorov-smirnov goodness of fit tests—will be explained as these have gained some importance in a broader range of engineering application areas.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([2944, 9312, 2011, 7778, 5604])","tensor([  101,  5836,  1037,  2200, 10975,  8490, 12644,  3921,  2005,  7411,
         2012,  2560,  1037,  8824,  2944, 11213,  1012,  1996,  2279,  3357,
         1999,  2944,  2311,  4050,  5936,  1996,  7667,  1997,  1996, 11709,
         1997,  1996,  5071,  4353,  3853,  1998,  2000,  2023,  2203,  1996,
         4118,  1997,  5312,  1998,  1996,  4555, 16593,  4118,  2020,  3107,
         1012,  2013,  1996, 18921, 26966,  5433,  2009,  2003,  5793,  2008,
         1996,  3737,  1997,  1996,  2511,  2944,  2003,  1037,  5257,  1997,
         1996,  6413,  2791,  1997,  1996,  3479,  9723,  4353,  1998,  1996,
         4358, 11709,  1010,  1045,  1012,  1041,  1012,  1996, 15003,  1997,
         4906,  1012,  2009,  2052,  2947,  2022,  1997,  3278,  3037,  2000,
         2022,  2583,  2000, 14358,  1996, 15003,  1997,  4906,  1999,  1037,
        20155,  2126,  8558, 12067,  2005,  1037, 11778,  1998,  8335,  2126,
         1997, 16114,  2075,  2030, 21936,  2944, 17568,  1012,  2005,  2023,
         3800,  1010,  1996,  4556,  7778,  4353,  5852,  1010,  1045,  1012,
         1041,  1012,  1996, 15003,  1997,  4906,  5852,  2031,  2042,  2764,
         1012,  1037,  2193,  1997,  2367,  4127,  1997,  5852,  2031,  2042,
         2764,  1999,  1996,  2627,  1010,  1999,  2112,  2007,  2200,  7772,
         1998,  1010,  8821,  1010,  3132, 10439, 19341,  8553,  2058,  2367,
         4646,  2752,  1012,  1999,  1996,  2206,  5433,  1010,  2048,  2107,
         5852,  1517,  1996,  1177,  2475,  1011,  1998,  1996, 12849, 13728,
        22844, 12298,  1011, 15488,  4313, 16693, 15003,  1997,  4906,  5852,
         1517,  2097,  2022,  4541,  2004,  2122,  2031,  4227,  2070,  5197,
         1999,  1037, 12368,  2846,  1997,  3330,  4646,  2752,  1012,   102])"
647,1,"['continuous', 'fit test', 'probability distributions', 'probability', 'test', 'distributions', 'continuous probability distributions', 'discrete probability distributions', 'goodness of fit', 'discrete']", Model Evaluation by Statistical Testing,seg_195,"in principle the χ2-goodness of fit test is applicable only for discrete probability distributions. however, it may easily be adapted to continuous probability distributions as shall be seen in the following. the kolmogorov-smirnov goodness of fit test, on the other hand, is only applicable for continuous probability distributions.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([2944, 9312, 2011, 7778, 5604])","tensor([  101,  1999,  6958,  1996,  1177,  2475,  1011, 15003,  1997,  4906,
         3231,  2003, 12711,  2069,  2005, 16246,  9723, 20611,  1012,  2174,
         1010,  2009,  2089,  4089,  2022,  5967,  2000,  7142,  9723, 20611,
         2004,  4618,  2022,  2464,  1999,  1996,  2206,  1012,  1996, 12849,
        13728, 22844, 12298,  1011, 15488,  4313, 16693, 15003,  1997,  4906,
         3231,  1010,  2006,  1996,  2060,  2192,  1010,  2003,  2069, 12711,
         2005,  7142,  9723, 20611,  1012,   102])"
648,1,"['fit test', 'distribution', 'discrete', 'test']", The ChiSquare χGoodness of Fit Test,seg_197,the χ2-goodness of fit test is applicable for discrete cumulative distribution functions p(xi) e.g. defined by:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  1996,  1177,  2475,  1011, 15003,  1997,  4906,  3231,  2003,
        12711,  2005, 16246, 23260,  4353,  4972,  1052,  1006,  8418,  1007,
         1041,  1012,  1043,  1012,  4225,  2011,  1024,   102])"
649,1,"['function', 'frequencies', 'cumulative distribution function', 'predicted', 'distribution', 'model', 'distribution function']", The ChiSquare χGoodness of Fit Test,seg_197,"intuitively, postulating a cumulative distribution function of the type as given in eq. 5.73, the differences between predicted frequencies np,i (using the assumed model) and the observed frequencies no,i , should indicate the quality of the postulated cumulative distribution function; this is the idea behind the χ2-test.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101, 29202,  2135,  1010,  2695, 10924,  1037, 23260,  4353,  3853,
         1997,  1996,  2828,  2004,  2445,  1999,  1041,  4160,  1012,  1019,
         1012,  6421,  1010,  1996,  5966,  2090, 10173, 13139, 27937,  1010,
         1045,  1006,  2478,  1996,  5071,  2944,  1007,  1998,  1996,  5159,
        13139,  2053,  1010,  1045,  1010,  2323,  5769,  1996,  3737,  1997,
         1996,  2695,  8898, 23260,  4353,  3853,  1025,  2023,  2003,  1996,
         2801,  2369,  1996,  1177,  2475,  1011,  3231,  1012,   102])"
650,1,"['variance', 'random variable', 'random', 'expected value', 'variable']", The ChiSquare χGoodness of Fit Test,seg_197,assume that the random variable xj is sampled n times. then the expected value and the variance of xj i.e. e[xj ]and var[xj ] are given by (see also sect. 4.3):,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  7868,  2008,  1996,  6721,  8023,  1060,  3501,  2003, 18925,
         1050,  2335,  1012,  2059,  1996,  3517,  3643,  1998,  1996, 23284,
         1997,  1060,  3501,  1045,  1012,  1041,  1012,  1041,  1031,  1060,
         3501,  1033,  1998, 13075,  1031,  1060,  3501,  1033,  2024,  2445,
         2011,  1006,  2156,  2036, 17831,  1012,  1018,  1012,  1017,  1007,
         1024,   102])"
651,1,"['frequency', 'histogram', 'sample', 'random', 'limit', 'standardized', 'model', 'central limit theorem', 'deviations']", The ChiSquare χGoodness of Fit Test,seg_197,"in accordance with the central limit theorem and provided that the postulated model is correct, it is reasonable to assume that the standardized random deviations of the sample frequency histogram from the postulated frequency histogram εj i.e.:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  1999, 10388,  2007,  1996,  2430,  5787,  9872,  1998,  3024,
         2008,  1996,  2695,  8898,  2944,  2003,  6149,  1010,  2009,  2003,
         9608,  2000,  7868,  2008,  1996, 16367,  6721, 24353,  2015,  1997,
         1996,  7099,  6075,  2010,  3406, 13113,  2013,  1996,  2695,  8898,
         6075,  2010,  3406, 13113,  1159,  3501,  1045,  1012,  1041,  1012,
         1024,   102])"
652,1,"['discrete random variable', 'random variable', 'normal', 'samples', 'statistic', 'discrete', 'standard', 'central limit theorem', 'standard normal', 'sample', 'random', 'sample statistic', 'limit', 'deviations', 'variable']", The ChiSquare χGoodness of Fit Test,seg_197,"are standard normal distributed. this however assumes that the number of samples of each of the xj values is large enough for the central limit theorem to be valid. if, however, not just the absolute values of the deviations but rather the squared deviations εj2, summed up over all possible values of the discrete random variable i.e. for j = 1,2, . . . , k, are considered, it is known from sect. 5.6.1 that this sample statistic is chi-square distributed:",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  2024,  3115,  3671,  5500,  1012,  2023,  2174, 15980,  2008,
         1996,  2193,  1997,  8168,  1997,  2169,  1997,  1996,  1060,  3501,
         5300,  2003,  2312,  2438,  2005,  1996,  2430,  5787,  9872,  2000,
         2022,  9398,  1012,  2065,  1010,  2174,  1010,  2025,  2074,  1996,
         7619,  5300,  1997,  1996, 24353,  2015,  2021,  2738,  1996, 19942,
        24353,  2015,  1159,  3501,  2475,  1010,  7680,  7583,  2039,  2058,
         2035,  2825,  5300,  1997,  1996, 16246,  6721,  8023,  1045,  1012,
         1041,  1012,  2005,  1046,  1027,  1015,  1010,  1016,  1010,  1012,
         1012,  1012,  1010,  1047,  1010,  2024,  2641,  1010,  2009,  2003,
         2124,  2013, 17831,  1012,  1019,  1012,  1020,  1012,  1015,  2008,
         2023,  7099, 28093,  6553,  2003,  9610,  1011,  2675,  5500,  1024,
          102])"
653,1,"['discrete', 'discrete random variables', 'degrees of freedom', 'factor', 'random', 'dependent', 'random variables', 'variables', 'statistic']", The ChiSquare χGoodness of Fit Test,seg_197,"due to the fact that the numbers of realizations of the discrete random variables are dependent, the statistic given by eq. 5.76 does not in fact have k degrees of freedom but only k − 1. furthermore, for the same reason, each term in eq. 5.76 shall be reduced with the factor (1 − p(xj )) whereby finally the modified statistic εm",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  2349,  2000,  1996,  2755,  2008,  1996,  3616,  1997, 12393,
         2015,  1997,  1996, 16246,  6721, 10857,  2024,  7790,  1010,  1996,
        28093,  6553,  2445,  2011,  1041,  4160,  1012,  1019,  1012,  6146,
         2515,  2025,  1999,  2755,  2031,  1047,  5445,  1997,  4071,  2021,
         2069,  1047,  1597,  1015,  1012,  7297,  1010,  2005,  1996,  2168,
         3114,  1010,  2169,  2744,  1999,  1041,  4160,  1012,  1019,  1012,
         6146,  4618,  2022,  4359,  2007,  1996,  5387,  1006,  1015,  1597,
         1052,  1006,  1060,  3501,  1007,  1007, 13557,  2633,  1996,  6310,
        28093,  6553,  1159,  2213,   102])"
654,0,[], The ChiSquare χGoodness of Fit Test,seg_197,2 is obtained:,tensor(0),"tensor([0, 0, 0, 0, 0, 0])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([ 101, 1016, 2003, 4663, 1024,  102])"
655,1,['degrees of freedom'], The ChiSquare χGoodness of Fit Test,seg_197,which is chi-square distributed with k − 1 degrees of freedom.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([ 101, 2029, 2003, 9610, 1011, 2675, 5500, 2007, 1047, 1597, 1015, 5445,
        1997, 4071, 1012,  102])"
656,1,"['function', 'operating rule', 'states', 'hypothesis', 'level', 'null hypothesis', 'data', 'test']", The ChiSquare χGoodness of Fit Test,seg_197,"following the principles given in sect. 5.8, it is thus possible to formulate and test, at the α-significance level, the null-hypothesis h0 that the postulated distribution function is not in contradiction with the observed data. the operating rule states that the null hypothesis cannot be accepted if εm",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  2206,  1996,  6481,  2445,  1999, 17831,  1012,  1019,  1012,
         1022,  1010,  2009,  2003,  2947,  2825,  2000,  5675,  2618,  1998,
         3231,  1010,  2012,  1996,  1155,  1011,  7784,  2504,  1010,  1996,
        19701,  1011, 10744,  1044,  2692,  2008,  1996,  2695,  8898,  4353,
         3853,  2003,  2025,  1999, 26917,  2007,  1996,  5159,  2951,  1012,
         1996,  4082,  3627,  2163,  2008,  1996, 19701, 10744,  3685,  2022,
         3970,  2065,  1159,  2213,   102])"
657,1,['critical value'], The ChiSquare χGoodness of Fit Test,seg_197,2 ≥ δ where the critical value δ with,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([ 101, 1016, 1609, 1158, 2073, 1996, 4187, 3643, 1158, 2007,  102])"
658,1,"['table', 'sample', 'sample statistic', 'statistic']", The ChiSquare χGoodness of Fit Test,seg_197,which the sample statistic shall be compared can be calculated from table c.3 such as:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  2029,  1996,  7099, 28093,  6553,  4618,  2022,  4102,  2064,
         2022, 10174,  2013,  2795,  1039,  1012,  1017,  2107,  2004,  1024,
          102])"
659,1,"['probability', 'hypothesis', 'distributions', 'distribution', 'parameters', 'alternate hypothesis', 'probability distribution']", The ChiSquare χGoodness of Fit Test,seg_197,it should be underlined that the alternate hypothesis h1 is less informative in the sense that this hypothesis in principle envelopes all possible distributions and distribution parameters except those of the postulated probability distribution.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  2009,  2323,  2022,  2104, 18194,  2008,  1996,  6585, 10744,
         1044,  2487,  2003,  2625, 12367,  8082,  1999,  1996,  3168,  2008,
         2023, 10744,  1999,  6958, 11255,  2015,  2035,  2825, 20611,  1998,
         4353, 11709,  3272,  2216,  1997,  1996,  2695,  8898,  9723,  4353,
         1012,   102])"
660,1,"['table', 'outcome', 'null hypothesis', 'mean', 'normal distribution', 'interval', 'random variable', 'normal', 'standard deviation', 'predicted', 'plot', 'data', 'discrete', 'continuous', 'deviation', 'histograms', 'probability', 'realization', 'hypothesis', 'random variables', 'standard', 'representative', 'continuous random variable', 'sample space', 'discrete random variables', 'sample', 'random', 'intervals', 'distribution', 'variables', 'variable']", The ChiSquare χGoodness of Fit Test,seg_197,"as an example, consider that a normal distribution with mean value μ = 33 and standard deviation σ = 5 is postulated as being representative for the data of the observed concrete compressive strengths presented in table 5.1—this postulate is the null hypothesis h0. it is clear that the concrete compressive strength is a continuous variable but this can be discretized by dividing the continuous sample space into intervals. the probability of a realization of the continuous random variable in each of the intervals is given as the probability that the outcome of the random variable is smaller that the upper boundary of the interval minus the probability that the outcome of the random variable is smaller than the lower boundary of the interval. it is then possible, adopting the same procedure as explained in the above, for discrete random variables to plot the histograms with the observed and predicted frequencies, no,i and np,i , respectively, for the different data ranges i = 1,2, . . . , k in one figure, see fig. 5.14.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  2004,  2019,  2742,  1010,  5136,  2008,  1037,  3671,  4353,
         2007,  2812,  3643,  1166,  1027,  3943,  1998,  3115, 24353,  1173,
         1027,  1019,  2003,  2695,  8898,  2004,  2108,  4387,  2005,  1996,
         2951,  1997,  1996,  5159,  5509,  4012, 27484, 20828,  3591,  1999,
         2795,  1019,  1012,  1015,  1517,  2023,  2695,  9869,  2003,  1996,
        19701, 10744,  1044,  2692,  1012,  2009,  2003,  3154,  2008,  1996,
         5509,  4012, 27484,  3997,  2003,  1037,  7142,  8023,  2021,  2023,
         2064,  2022,  5860, 13465,  3550,  2011, 16023,  1996,  7142,  7099,
         2686,  2046, 14025,  1012,  1996,  9723,  1997,  1037, 12393,  1997,
         1996,  7142,  6721,  8023,  1999,  2169,  1997,  1996, 14025,  2003,
         2445,  2004,  1996,  9723,  2008,  1996,  9560,  1997,  1996,  6721,
         8023,  2003,  3760,  2008,  1996,  3356,  6192,  1997,  1996, 13483,
        15718,  1996,  9723,  2008,  1996,  9560,  1997,  1996,  6721,  8023,
         2003,  3760,  2084,  1996,  2896,  6192,  1997,  1996, 13483,  1012,
         2009,  2003,  2059,  2825,  1010, 16151,  1996,  2168,  7709,  2004,
         4541,  1999,  1996,  2682,  1010,  2005, 16246,  6721, 10857,  2000,
         5436,  1996,  2010,  3406, 13113,  2015,  2007,  1996,  5159,  1998,
        10173, 13139,  1010,  2053,  1010,  1045,  1998, 27937,  1010,  1045,
         1010,  4414,  1010,  2005,  1996,  2367,  2951,  8483,  1045,  1027,
         1015,  1010,  1016,  1010,  1012,  1012,  1012,  1010,  1047,  1999,
         2028,  3275,  1010,  2156, 20965,  1012,  1019,  1012,  2403,  1012,
          102])"
661,1,"['normal distribution', 'frequencies', 'histograms', 'interval', 'observations', 'number of observations', 'normal', 'intervals', 'predicted', 'distribution', 'data']", The ChiSquare χGoodness of Fit Test,seg_197,"from fig. 5.14 it is seen that the chosen discretized implies that the number of different data ranges k is equal to 4. however, it is noted that the observed and the predicted frequencies in the lower interval is relatively small and it is doubtful if the conditions prevailing the normal distribution assumption are fulfilled. to overcome this problem, it is recommended in the literature, see e.g. benjamin and cornell [4] to lump the data in adjacent intervals such that the number of observations in each interval is about 5 or larger. lumping the frequencies in the two lower intervals yields the histograms shown in fig. 5.15.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  2013, 20965,  1012,  1019,  1012,  2403,  2009,  2003,  2464,
         2008,  1996,  4217,  5860, 13465,  3550, 12748,  2008,  1996,  2193,
         1997,  2367,  2951,  8483,  1047,  2003,  5020,  2000,  1018,  1012,
         2174,  1010,  2009,  2003,  3264,  2008,  1996,  5159,  1998,  1996,
        10173, 13139,  1999,  1996,  2896, 13483,  2003,  4659,  2235,  1998,
         2009,  2003, 21888,  2065,  1996,  3785, 19283,  1996,  3671,  4353,
        11213,  2024, 16829,  1012,  2000,  9462,  2023,  3291,  1010,  2009,
         2003,  6749,  1999,  1996,  3906,  1010,  2156,  1041,  1012,  1043,
         1012,  6425,  1998, 10921,  1031,  1018,  1033,  2000, 15116,  1996,
         2951,  1999,  5516, 14025,  2107,  2008,  1996,  2193,  1997,  9420,
         1999,  2169, 13483,  2003,  2055,  1019,  2030,  3469,  1012, 15116,
         2075,  1996, 13139,  1999,  1996,  2048,  2896, 14025, 16189,  1996,
         2010,  3406, 13113,  2015,  3491,  1999, 20965,  1012,  1019,  1012,
         2321,  1012,   102])"
662,1,"['statistic', 'table']", The ChiSquare χGoodness of Fit Test,seg_197,"following the approach outlined in the foregoing, the statistic given in eq. 5.77 is now evaluated as summarized in table 5.5.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  2206,  1996,  3921, 14801,  1999,  1996, 18921, 26966,  1010,
         1996, 28093,  6553,  2445,  1999,  1041,  4160,  1012,  1019,  1012,
         6255,  2003,  2085, 16330,  2004, 22539,  1999,  2795,  1019,  1012,
         1019,  1012,   102])"
663,1,"['significance level', 'case', 'table', 'degrees of freedom', 'critical value', 'level', 'sample', 'sample statistic', 'distribution', 'significance', 'statistic', 'test']", The ChiSquare χGoodness of Fit Test,seg_197,"finally, in order to either reject or accept the null-hypothesis h0, the sample statistic, as calculated in table 5.5, must be compared with a critical value δ given by eq. 5.78. in the present case, if the test is performed at a 5% significance level, the value δ = 5.9915 can be calculated from a chi-square distribution with 2 degrees of freedom. by comparison of the sample statistic in table 5.5, i.e. 0.406829 with",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  2633,  1010,  1999,  2344,  2000,  2593, 15454,  2030,  5138,
         1996, 19701,  1011, 10744,  1044,  2692,  1010,  1996,  7099, 28093,
         6553,  1010,  2004, 10174,  1999,  2795,  1019,  1012,  1019,  1010,
         2442,  2022,  4102,  2007,  1037,  4187,  3643,  1158,  2445,  2011,
         1041,  4160,  1012,  1019,  1012,  6275,  1012,  1999,  1996,  2556,
         2553,  1010,  2065,  1996,  3231,  2003,  2864,  2012,  1037,  1019,
         1003,  7784,  2504,  1010,  1996,  3643,  1158,  1027,  1019,  1012,
         5585, 16068,  2064,  2022, 10174,  2013,  1037,  9610,  1011,  2675,
         4353,  2007,  1016,  5445,  1997,  4071,  1012,  2011,  7831,  1997,
         1996,  7099, 28093,  6553,  1999,  2795,  1019,  1012,  1019,  1010,
         1045,  1012,  1041,  1012,  1014,  1012, 27433,  2620, 24594,  2007,
          102])"
664,1,"['significance level', 'hypothesis', 'level', 'critical value', 'null hypothesis', 'significance']", The ChiSquare χGoodness of Fit Test,seg_197,"the critical value δ = 5.9915, it is seen that the null hypothesis cannot be rejected at the 5% significance level.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  1996,  4187,  3643,  1158,  1027,  1019,  1012,  5585, 16068,
         1010,  2009,  2003,  2464,  2008,  1996, 19701, 10744,  3685,  2022,
         5837,  2012,  1996,  1019,  1003,  7784,  2504,  1012,   102])"
665,1,"['deviation', 'case', 'estimated', 'degrees of freedom', 'standard deviation', 'standard', 'experiment', 'distribution', 'parameters', 'data', 'test']", The ChiSquare χGoodness of Fit Test,seg_197,"in the foregoing example not only the type of distribution, but also the parameters of the distribution were postulated. in practice, it is often the case that the parameters of the distribution are estimated first and thereafter the test for distribution type is performed. this is, in principle, possible following exactly the same approach as outlined in the above, with the modification that the number of degrees of freedom is reduced with the number of parameters estimated from the available data. if, as in the example concerning the concrete cube compressive strength, it is assumed that first the experiment data are used to assess the standard deviation of the distribution as shown in sect. 5.6, the number of degrees of freedom is reduced to 1.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  1999,  1996, 18921, 26966,  2742,  2025,  2069,  1996,  2828,
         1997,  4353,  1010,  2021,  2036,  1996, 11709,  1997,  1996,  4353,
         2020,  2695,  8898,  1012,  1999,  3218,  1010,  2009,  2003,  2411,
         1996,  2553,  2008,  1996, 11709,  1997,  1996,  4353,  2024,  4358,
         2034,  1998,  6920,  1996,  3231,  2005,  4353,  2828,  2003,  2864,
         1012,  2023,  2003,  1010,  1999,  6958,  1010,  2825,  2206,  3599,
         1996,  2168,  3921,  2004, 14801,  1999,  1996,  2682,  1010,  2007,
         1996, 14080,  2008,  1996,  2193,  1997,  5445,  1997,  4071,  2003,
         4359,  2007,  1996,  2193,  1997, 11709,  4358,  2013,  1996,  2800,
         2951,  1012,  2065,  1010,  2004,  1999,  1996,  2742,  7175,  1996,
         5509, 14291,  4012, 27484,  3997,  1010,  2009,  2003,  5071,  2008,
         2034,  1996,  7551,  2951,  2024,  2109,  2000, 14358,  1996,  3115,
        24353,  1997,  1996,  4353,  2004,  3491,  1999, 17831,  1012,  1019,
         1012,  1020,  1010,  1996,  2193,  1997,  5445,  1997,  4071,  2003,
         4359,  2000,  1015,  1012,   102])"
666,1,"['mean', 'deviation', 'normal distribution', 'table', 'normal', 'standard deviation', 'standard', 'distribution']", The ChiSquare χGoodness of Fit Test,seg_197,"postulating as before, a normal distribution with mean equal to μ = 33 but now with the standard deviation found in sect. 5.3, i.e. σ = 4.05, the calculations are modified as shown in table 5.6.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  2695, 10924,  2004,  2077,  1010,  1037,  3671,  4353,  2007,
         2812,  5020,  2000,  1166,  1027,  3943,  2021,  2085,  2007,  1996,
         3115, 24353,  2179,  1999, 17831,  1012,  1019,  1012,  1017,  1010,
         1045,  1012,  1041,  1012,  1173,  1027,  1018,  1012,  5709,  1010,
         1996, 16268,  2024,  6310,  2004,  3491,  1999,  2795,  1019,  1012,
         1020,  1012,   102])"
667,1,"['significance level', 'degree of freedom', 'level', 'significance']", The ChiSquare χGoodness of Fit Test,seg_197,with only 1 degree of freedom the critical level δ is reduced to δ = 3.84 but the null-hypothesis still cannot be rejected at the 5% significance level.,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  2007,  2069,  1015,  3014,  1997,  4071,  1996,  4187,  2504,
         1158,  2003,  4359,  2000,  1158,  1027,  1017,  1012,  6391,  2021,
         1996, 19701,  1011, 10744,  2145,  3685,  2022,  5837,  2012,  1996,
         1019,  1003,  7784,  2504,  1012,   102])"
668,1,"['estimated', 'degrees of freedom', 'distribution', 'parameters', 'data']", The ChiSquare χGoodness of Fit Test,seg_197,"from the above, it is seen that the available data simply do not permit that both of the distribution parameters are first estimated and thereafter the distribution and parameters postulates tested. the number of degrees of freedom is not sufficient. in engineering applications this problem is not unusual as the available data are gen-",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  2013,  1996,  2682,  1010,  2009,  2003,  2464,  2008,  1996,
         2800,  2951,  3432,  2079,  2025,  9146,  2008,  2119,  1997,  1996,
         4353, 11709,  2024,  2034,  4358,  1998,  6920,  1996,  4353,  1998,
        11709,  2695, 18969,  7718,  1012,  1996,  2193,  1997,  5445,  1997,
         4071,  2003,  2025,  7182,  1012,  1999,  3330,  5097,  2023,  3291,
         2003,  2025,  5866,  2004,  1996,  2800,  2951,  2024,  8991,  1011,
          102])"
669,1,"['statistical', 'data']", The ChiSquare χGoodness of Fit Test,seg_197,"erally sparse. in other areas, such as in production industries, the available amount of data is generally very substantial and the merits of statistical testing are more obvious.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  9610,  2015, 16211,  2890,  1177, 24146,  2791,  1997,  4906,
         3231])","tensor([  101,  3690,  9215, 20288,  1012,  1999,  2060,  2752,  1010,  2107,
         2004,  1999,  2537,  6088,  1010,  1996,  2800,  3815,  1997,  2951,
         2003,  3227,  2200,  6937,  1998,  1996, 22617,  1997,  7778,  5604,
         2024,  2062,  5793,  1012,   102])"
670,1,"['cases', 'errors', 'goodness of fit', 'fit test', 'histogram', 'predicted', 'data', 'statistic', 'case', 'test', 'parameters', 'distributions', 'sample', 'distribution']", The KolmogorovSmirnov Goodness of Fit Test,seg_199,"whereas the χ2-goodness of fit test takes basis in a statistic quantifying the squared errors between the observed sample histogram and the predicted postulated histogram, the goodness of fit test due to kolmogorov-smirnov utilizes a sample statistic formulated in terms of the cumulative distributions. the χ2-goodness of fit test can be applied in cases where both distribution and parameters are postulated, as well as in cases where only the distribution is postulated and the parameters estimated, using the same data utilized for the testing. this is not the case for the kolmogorov-smirnov test. here both the distribution family and parameters must be postulated.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 1996, 12849, 13728, 22844, 12298,  6491,  4313, 16693, 15003,  1997,
         4906,  3231])","tensor([  101,  6168,  1996,  1177,  2475,  1011, 15003,  1997,  4906,  3231,
         3138,  3978,  1999,  1037, 28093,  6553, 24110, 27351,  2075,  1996,
        19942, 10697,  2090,  1996,  5159,  7099,  2010,  3406, 13113,  1998,
         1996, 10173,  2695,  8898,  2010,  3406, 13113,  1010,  1996, 15003,
         1997,  4906,  3231,  2349,  2000, 12849, 13728, 22844, 12298,  1011,
        15488,  4313, 16693, 21852,  1037,  7099, 28093,  6553, 19788,  1999,
         3408,  1997,  1996, 23260, 20611,  1012,  1996,  1177,  2475,  1011,
        15003,  1997,  4906,  3231,  2064,  2022,  4162,  1999,  3572,  2073,
         2119,  4353,  1998, 11709,  2024,  2695,  8898,  1010,  2004,  2092,
         2004,  1999,  3572,  2073,  2069,  1996,  4353,  2003,  2695,  8898,
         1998,  1996, 11709,  4358,  1010,  2478,  1996,  2168,  2951, 12550,
         2005,  1996,  5604,  1012,  2023,  2003,  2025,  1996,  2553,  2005,
         1996, 12849, 13728, 22844, 12298,  1011, 15488,  4313, 16693,  3231,
         1012,  2182,  2119,  1996,  4353,  2155,  1998, 11709,  2442,  2022,
         2695,  8898,  1012,   102])"
671,1,"['function', 'cumulative distribution function', 'distribution', 'distribution function']", The KolmogorovSmirnov Goodness of Fit Test,seg_199,if the observed cumulative distribution function fo(x̂io) is written as:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996, 12849, 13728, 22844, 12298,  6491,  4313, 16693, 15003,  1997,
         4906,  3231])","tensor([  101,  2065,  1996,  5159, 23260,  4353,  3853,  1042,  2080,  1006,
         8418,  2080,  1007,  2003,  2517,  2004,  1024,   102])"
672,1,"['function', 'cumulative distribution function', 'observation', 'data', 'distribution function', 'hypothesis', 'significance testing', 'sample', 'distribution', 'null hypothesis', 'significance', 'statistic']", The KolmogorovSmirnov Goodness of Fit Test,seg_199,"where x̂io is the ith smallest observation in the sample of size n and the postulated cumulative distribution function is fp(x), then the following statistic may be utilized for significance testing of the null hypothesis that the observed data do not deviate statistically significantly from the postulated distribution function:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 1996, 12849, 13728, 22844, 12298,  6491,  4313, 16693, 15003,  1997,
         4906,  3231])","tensor([  101,  2073,  8418,  2080,  2003,  1996,  2009,  2232, 10479,  8089,
         1999,  1996,  7099,  1997,  2946,  1050,  1998,  1996,  2695,  8898,
        23260,  4353,  3853,  2003,  1042,  2361,  1006,  1060,  1007,  1010,
         2059,  1996,  2206, 28093,  6553,  2089,  2022, 12550,  2005,  7784,
         5604,  1997,  1996, 19701, 10744,  2008,  1996,  5159,  2951,  2079,
         2025, 14386,  3686,  7778,  2135,  6022,  2013,  1996,  2695,  8898,
         4353,  3853,  1024,   102])"
673,1,"['table', 'statistic', 'distribution']", The KolmogorovSmirnov Goodness of Fit Test,seg_199,the distribution of the statistic εmax is tabulated for relevant values of α and n in table 5.7.,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 1996, 12849, 13728, 22844, 12298,  6491,  4313, 16693, 15003,  1997,
         4906,  3231])","tensor([  101,  1996,  4353,  1997,  1996, 28093,  6553,  1159, 17848,  2003,
        21628,  8898,  2005,  7882,  5300,  1997,  1155,  1998,  1050,  1999,
         2795,  1019,  1012,  1021,  1012,   102])"
674,1,"['function', 'cumulative distribution function', 'distribution function', 'hypothesis', 'distribution', 'null hypothesis', 'data']", The KolmogorovSmirnov Goodness of Fit Test,seg_199,the null hypothesis h0 may be formulated expressing that the observed data follows the postulated cumulative distribution function.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 0.])","tensor([ 1996, 12849, 13728, 22844, 12298,  6491,  4313, 16693, 15003,  1997,
         4906,  3231])","tensor([  101,  1996, 19701, 10744,  1044,  2692,  2089,  2022, 19788, 14026,
         2008,  1996,  5159,  2951,  4076,  1996,  2695,  8898, 23260,  4353,
         3853,  1012,   102])"
675,1,"['operating rule', 'hypothesis', 'level', 'null hypothesis', 'critical value']", The KolmogorovSmirnov Goodness of Fit Test,seg_199,the operating rule specifies that the null hypothesis h0 cannot be accepted at the α-significance level if: εmax ≥ δ where the critical value δ can be calculated such that:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 1996, 12849, 13728, 22844, 12298,  6491,  4313, 16693, 15003,  1997,
         4906,  3231])","tensor([  101,  1996,  4082,  3627, 27171,  2008,  1996, 19701, 10744,  1044,
         2692,  3685,  2022,  3970,  2012,  1996,  1155,  1011,  7784,  2504,
         2065,  1024,  1159, 17848,  1609,  1158,  2073,  1996,  4187,  3643,
         1158,  2064,  2022, 10174,  2107,  2008,  1024,   102])"
676,1,['table'], The KolmogorovSmirnov Goodness of Fit Test,seg_199,where δ is determined from table 5.7 (or table c.4).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0.])","tensor([ 1996, 12849, 13728, 22844, 12298,  6491,  4313, 16693, 15003,  1997,
         4906,  3231])","tensor([ 101, 2073, 1158, 2003, 4340, 2013, 2795, 1019, 1012, 1021, 1006, 2030,
        2795, 1039, 1012, 1018, 1007, 1012,  102])"
677,1,"['mean', 'function', 'deviation', 'cumulative distribution function', 'probability', 'normal', 'standard deviation', 'standard', 'predicted', 'distribution', 'distribution function', 'data', 'representative']", The KolmogorovSmirnov Goodness of Fit Test,seg_199,"consider again the example concerning the compressive strength of concrete. as before, it is postulated that the data is representative for a normal distribution with mean value μ = 33 and a standard deviation equal to σ = 5. by inspection of fig. 5.16 it is seen that the largest deviation between the observed cumulative distribution function and the predicted postulated probability distribution occurs for the 18th data point corresponding to a concrete cube compressive",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996, 12849, 13728, 22844, 12298,  6491,  4313, 16693, 15003,  1997,
         4906,  3231])","tensor([  101,  5136,  2153,  1996,  2742,  7175,  1996,  4012, 27484,  3997,
         1997,  5509,  1012,  2004,  2077,  1010,  2009,  2003,  2695,  8898,
         2008,  1996,  2951,  2003,  4387,  2005,  1037,  3671,  4353,  2007,
         2812,  3643,  1166,  1027,  3943,  1998,  1037,  3115, 24353,  5020,
         2000,  1173,  1027,  1019,  1012,  2011, 10569,  1997, 20965,  1012,
         1019,  1012,  2385,  2009,  2003,  2464,  2008,  1996,  2922, 24353,
         2090,  1996,  5159, 23260,  4353,  3853,  1998,  1996, 10173,  2695,
         8898,  9723,  4353,  5158,  2005,  1996,  4985,  2951,  2391,  7978,
         2000,  1037,  5509, 14291,  4012, 27484,   102])"
678,1,"['function', 'cumulative distribution function', 'distribution', 'distribution function']", The KolmogorovSmirnov Goodness of Fit Test,seg_199,strength of 37.1 mpa. for this value the postulated cumulative distribution function yields:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0.])","tensor([ 1996, 12849, 13728, 22844, 12298,  6491,  4313, 16693, 15003,  1997,
         4906,  3231])","tensor([  101,  3997,  1997,  4261,  1012,  1015,  6131,  2050,  1012,  2005,
         2023,  3643,  1996,  2695,  8898, 23260,  4353,  3853, 16189,  1024,
          102])"
679,1,"['function', 'cumulative distribution function', 'distribution', 'distribution function']", The KolmogorovSmirnov Goodness of Fit Test,seg_199,and the observed cumulative distribution function yields:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0.])","tensor([ 1996, 12849, 13728, 22844, 12298,  6491,  4313, 16693, 15003,  1997,
         4906,  3231])","tensor([  101,  1998,  1996,  5159, 23260,  4353,  3853, 16189,  1024,   102])"
680,1,"['significance level', 'table', 'hypothesis', 'critical value', 'level', 'sample', 'sample statistic', 'null hypothesis', 'significance', 'statistic']", The KolmogorovSmirnov Goodness of Fit Test,seg_199,"whereby the sample statistic becomes εmax = 0.9 − 0.794 = 0.106. from table 5.7 the critical value δ for n = 20 and a 5% significance level is 0.29. since the sample statistic 0.106 is smaller than the critical value 0.29, the null hypothesis cannot be rejected.",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0.])","tensor([ 1996, 12849, 13728, 22844, 12298,  6491,  4313, 16693, 15003,  1997,
         4906,  3231])","tensor([  101, 13557,  1996,  7099, 28093,  6553,  4150,  1159, 17848,  1027,
         1014,  1012,  1023,  1597,  1014,  1012,  6535,  2549,  1027,  1014,
         1012, 10114,  1012,  2013,  2795,  1019,  1012,  1021,  1996,  4187,
         3643,  1158,  2005,  1050,  1027,  2322,  1998,  1037,  1019,  1003,
         7784,  2504,  2003,  1014,  1012,  2756,  1012,  2144,  1996,  7099,
        28093,  6553,  1014,  1012, 10114,  2003,  3760,  2084,  1996,  4187,
         3643,  1014,  1012,  2756,  1010,  1996, 19701, 10744,  3685,  2022,
         5837,  1012,   102])"
681,1,"['statistical tests', 'tests', 'statistical', 'evaluating']", Model Comparison,seg_201,in the foregoing sections statistical tests were introduced as means for evaluating the,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([2944, 7831])","tensor([  101,  1999,  1996, 18921, 26966,  5433,  7778,  5852,  2020,  3107,
         2004,  2965,  2005, 23208,  1996,   102])"
682,1,"['function', 'distribution', 'distribution function', 'data']", Model Comparison,seg_201,goodness of fit of a given postulated distribution function to observed data. these,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.])","tensor([2944, 7831])","tensor([  101, 15003,  1997,  4906,  1997,  1037,  2445,  2695,  8898,  4353,
         3853,  2000,  5159,  2951,  1012,  2122,   102])"
683,1,['distribution'], Model Comparison,seg_201,"tests can, however, only be applied to assess the plausibility of a given distribution",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0.])","tensor([2944, 7831])","tensor([  101,  5852,  2064,  1010,  2174,  1010,  2069,  2022,  4162,  2000,
        14358,  1996, 20228, 20559, 13464,  1997,  1037,  2445,  4353,   102])"
684,1,"['distribution', 'functions', 'representative', 'data']", Model Comparison,seg_201,being representative for the observed data. other postulated distribution functions,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.])","tensor([2944, 7831])","tensor([ 101, 2108, 4387, 2005, 1996, 5159, 2951, 1012, 2060, 2695, 8898, 4353,
        4972,  102])"
685,1,"['data', 'representative']", Model Comparison,seg_201,could also be representative for the observed data. the question thus remains how to,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2944, 7831])","tensor([ 101, 2071, 2036, 2022, 4387, 2005, 1996, 5159, 2951, 1012, 1996, 3160,
        2947, 3464, 2129, 2000,  102])"
686,1,['distributions'], Model Comparison,seg_201,select between two postulated distributions which both cannot be rejected by testing,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2944, 7831])","tensor([  101,  7276,  2090,  2048,  2695,  8898, 20611,  2029,  2119,  3685,
         2022,  5837,  2011,  5604,   102])"
687,0,[], Model Comparison,seg_201,"as possible candidates. to this end two possibilities might be considered, namely",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2944, 7831])","tensor([  101,  2004,  2825,  5347,  1012,  2000,  2023,  2203,  2048, 12020,
         2453,  2022,  2641,  1010,  8419,   102])"
688,1,"['sample', 'sample likelihood', 'likelihood']", Model Comparison,seg_201,by comparison of the sample likelihood defined by eq. 5.6 or by comparison of the,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([2944, 7831])","tensor([  101,  2011,  7831,  1997,  1996,  7099, 16593,  4225,  2011,  1041,
         4160,  1012,  1019,  1012,  1020,  2030,  2011,  7831,  1997,  1996,
          102])"
689,1,"['sample statistics', 'sample', 'statistics']", Model Comparison,seg_201,likelihood of the sample statistics given by eq. 5.77 or eq. 5.80. direct comparison,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([2944, 7831])","tensor([  101, 16593,  1997,  1996,  7099,  6747,  2445,  2011,  1041,  4160,
         1012,  1019,  1012,  6255,  2030,  1041,  4160,  1012,  1019,  1012,
         3770,  1012,  3622,  7831,   102])"
690,1,"['fit test', 'sample', 'sample statistic', 'statistic', 'test']", Model Comparison,seg_201,of the sample statistic for the χ2-goodness of fit test is not a consistent means for,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([2944, 7831])","tensor([  101,  1997,  1996,  7099, 28093,  6553,  2005,  1996,  1177,  2475,
         1011, 15003,  1997,  4906,  3231,  2003,  2025,  1037,  8335,  2965,
         2005,   102])"
691,1,"['cases', 'degrees of freedom']", Model Comparison,seg_201,comparison as the number of degrees of freedom may be different for the cases,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.])","tensor([2944, 7831])","tensor([ 101, 7831, 2004, 1996, 2193, 1997, 5445, 1997, 4071, 2089, 2022, 2367,
        2005, 1996, 3572,  102])"
692,1,"['fit test', 'cases', 'test']", Model Comparison,seg_201,as an example consider the two cases where the χ2-goodness of fit test was,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0.])","tensor([2944, 7831])","tensor([  101,  2004,  2019,  2742,  5136,  1996,  2048,  3572,  2073,  1996,
         1177,  2475,  1011, 15003,  1997,  4906,  3231,  2001,   102])"
693,1,"['normal distribution', 'normal', 'distribution', 'goodness of fit']", Model Comparison,seg_201,applied first for testing the goodness of fit for a postulated normal distribution with,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([2944, 7831])","tensor([  101,  4162,  2034,  2005,  5604,  1996, 15003,  1997,  4906,  2005,
         1037,  2695,  8898,  3671,  4353,  2007,   102])"
694,1,"['normal distribution', 'normal', 'distribution', 'parameters']", Model Comparison,seg_201,"postulated parameters μ = 33, σ = 5 and thereafter a postulated normal distribution",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0.])","tensor([2944, 7831])","tensor([  101,  2695,  8898, 11709,  1166,  1027,  3943,  1010,  1173,  1027,
         1019,  1998,  6920,  1037,  2695,  8898,  3671,  4353,   102])"
695,1,"['set', 'estimated', 'parameters', 'data set', 'data']", Model Comparison,seg_201,"for which the parameters were estimated from the data set, i.e. μ = 33, σ = 4.05.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2944, 7831])","tensor([  101,  2005,  2029,  1996, 11709,  2020,  4358,  2013,  1996,  2951,
         2275,  1010,  1045,  1012,  1041,  1012,  1166,  1027,  3943,  1010,
         1173,  1027,  1018,  1012,  5709,  1012,   102])"
696,1,"['sample', 'sample statistic', 'statistic', 'case']", Model Comparison,seg_201,for the first case the sample statistic is equal to 0.4068 and the number of degrees,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([2944, 7831])","tensor([  101,  2005,  1996,  2034,  2553,  1996,  7099, 28093,  6553,  2003,
         5020,  2000,  1014,  1012, 27433,  2620,  1998,  1996,  2193,  1997,
         5445,   102])"
697,1,"['sample', 'sample statistic', 'statistic', 'case']", Model Comparison,seg_201,of freedom is 2. for the second case the sample statistic is equal to 0.44852 and the,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([2944, 7831])","tensor([  101,  1997,  4071,  2003,  1016,  1012,  2005,  1996,  2117,  2553,
         1996,  7099, 28093,  6553,  2003,  5020,  2000,  1014,  1012,  4008,
        27531,  2475,  1998,  1996,   102])"
698,1,['degrees of freedom'], Model Comparison,seg_201,number of degrees of freedom is equal to 1.,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([2944, 7831])","tensor([ 101, 2193, 1997, 5445, 1997, 4071, 2003, 5020, 2000, 1015, 1012,  102])"
699,1,"['sample likelihood', 'sample', 'likelihood', 'case']", Model Comparison,seg_201,the sample likelihood (see eq. 5.6) of the first case (σ = 5) is l(θ | x̂) = 2.74 ·,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2944, 7831])","tensor([  101,  1996,  7099, 16593,  1006,  2156,  1041,  4160,  1012,  1019,
         1012,  1020,  1007,  1997,  1996,  2034,  2553,  1006,  1173,  1027,
         1019,  1007,  2003,  1048,  1006,  1162,  1064,  1060,  1007,  1027,
         1016,  1012,  6356,  1087,   102])"
700,1,['case'], Model Comparison,seg_201,the postulate made in the second case is more probable than the postulate made in,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([2944, 7831])","tensor([  101,  1996,  2695,  9869,  2081,  1999,  1996,  2117,  2553,  2003,
         2062, 15596,  2084,  1996,  2695,  9869,  2081,  1999,   102])"
701,1,['case'], Model Comparison,seg_201,the first case.,tensor(1),"tensor([0., 0., 0., 1., 0., 0.])","tensor([2944, 7831])","tensor([ 101, 1996, 2034, 2553, 1012,  102])"
702,1,"['function', 'likelihood', 'density function']", Model Comparison,seg_201,"alternatively, the corresponding likelihood values of the density function can be",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([2944, 7831])","tensor([  101, 14084,  1010,  1996,  7978, 16593,  5300,  1997,  1996,  4304,
         3853,  2064,  2022,   102])"
703,0,[], Model Comparison,seg_201,compared. this can be calculated with eq. 5.53:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2944, 7831])","tensor([  101,  4102,  1012,  2023,  2064,  2022, 10174,  2007,  1041,  4160,
         1012,  1019,  1012,  5187,  1024,   102])"
704,1,['likelihood'], Model Comparison,seg_201,"based on the obtained likelihood values of 0.408 and 1.842, it can be said that",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([2944, 7831])","tensor([  101,  2241,  2006,  1996,  4663, 16593,  5300,  1997,  1014,  1012,
         2871,  2620,  1998,  1015,  1012,  6391,  2475,  1010,  2009,  2064,
         2022,  2056,  2008,   102])"
705,1,['case'], Model Comparison,seg_201,the postulation made in the second case is more probable than the postulate made in,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([2944, 7831])","tensor([  101,  1996,  2695,  9513,  2081,  1999,  1996,  2117,  2553,  2003,
         2062, 15596,  2084,  1996,  2695,  9869,  2081,  1999,   102])"
706,1,['case'], Model Comparison,seg_201,the first case.,tensor(1),"tensor([0., 0., 0., 1., 0., 0.])","tensor([2944, 7831])","tensor([ 101, 1996, 2034, 2553, 1012,  102])"
707,1,['data'], Self Assessment QuestionsExercises,seg_203,"1. in fig. 5.17, data of the annual observed maximum values of precipitation per",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1015,  1012,  1999, 20965,  1012,  1019,  1012,  2459,  1010,
         2951,  1997,  1996,  3296,  5159,  4555,  5300,  1997, 13511,  2566,
          102])"
708,1,"['distribution', 'probability paper', 'probability', 'maximum likelihood', 'gumbel', 'method', 'likelihood', 'gumbel distribution']", Self Assessment QuestionsExercises,seg_203,hour (rainfall) are plotted on a probability paper for the gumbel distribution. the “best-fit” line is also shown. can an engineer accept the gumbel distribution as being suitable for the modeling of the annual maximum precipitation per hour? 2. the maximum likelihood method (mlm) enables engineers to calculate the,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  3178,  1006, 10101,  1007,  2024, 27347,  2006,  1037,  9723,
         3259,  2005,  1996, 16031,  8671,  4353,  1012,  1996,  1523,  2190,
         1011,  4906,  1524,  2240,  2003,  2036,  3491,  1012,  2064,  2019,
         3992,  5138,  1996, 16031,  8671,  4353,  2004,  2108,  7218,  2005,
         1996, 11643,  1997,  1996,  3296,  4555, 13511,  2566,  3178,  1029,
         1016,  1012,  1996,  4555, 16593,  4118,  1006, 19875,  2213,  1007,
        12939,  6145,  2000, 18422,  1996,   102])"
709,1,"['uncertainty', 'associated', 'estimated', 'information', 'random variable', 'estimates', 'random', 'distribution', 'point estimates', 'parameters', 'data', 'variable']", Self Assessment QuestionsExercises,seg_203,distribution parameters of a random variable on the basis of data. which of the following statement(s) is (are) correct? the mlm provides point estimates of the distribution parameters. the mlm provides information about the uncertainty associated with the estimated parameters. the mlm provides no information about the uncertainty associated with the estimated parameters. 3. from past experience it is known that the shear strength of soil can be de-,tensor(1),"tensor([0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  4353, 11709,  1997,  1037,  6721,  8023,  2006,  1996,  3978,
         1997,  2951,  1012,  2029,  1997,  1996,  2206,  4861,  1006,  1055,
         1007,  2003,  1006,  2024,  1007,  6149,  1029,  1996, 19875,  2213,
         3640,  2391, 10035,  1997,  1996,  4353, 11709,  1012,  1996, 19875,
         2213,  3640,  2592,  2055,  1996, 12503,  3378,  2007,  1996,  4358,
        11709,  1012,  1996, 19875,  2213,  3640,  2053,  2592,  2055,  1996,
        12503,  3378,  2007,  1996,  4358, 11709,  1012,  1017,  1012,  2013,
         2627,  3325,  2009,  2003,  2124,  2008,  1996, 18330,  3997,  1997,
         5800,  2064,  2022,  2139,  1011,   102])"
710,1,"['distribution', 'probability paper', 'probability', 'lognormal', 'estimate', 'maximum likelihood', 'method', 'parameters', 'likelihood', 'samples', 'lognormal distribution', 'data']", Self Assessment QuestionsExercises,seg_203,scribed by a lognormal distribution. 15 samples of soil are taken from a site and an engineer wants to use the data in order to estimate the parameters of the lognormal distribution. the engineer: may use a probability paper to estimate the parameters of the lognormal distribution. may use the maximum likelihood method to estimate the parameters of the lognormal distribution.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101, 27789,  2094,  2011,  1037,  8833, 12131,  9067,  4353,  1012,
         2321,  8168,  1997,  5800,  2024,  2579,  2013,  1037,  2609,  1998,
         2019,  3992,  4122,  2000,  2224,  1996,  2951,  1999,  2344,  2000,
        10197,  1996, 11709,  1997,  1996,  8833, 12131,  9067,  4353,  1012,
         1996,  3992,  1024,  2089,  2224,  1037,  9723,  3259,  2000, 10197,
         1996, 11709,  1997,  1996,  8833, 12131,  9067,  4353,  1012,  2089,
         2224,  1996,  4555, 16593,  4118,  2000, 10197,  1996, 11709,  1997,
         1996,  8833, 12131,  9067,  4353,  1012,   102])"
711,1,"['method of moments', 'lognormal', 'estimate', 'probabilistic', 'moments', 'method', 'parameters', 'probabilistic model', 'distribution', 'model', 'lognormal distribution']", Self Assessment QuestionsExercises,seg_203,"may use the method of moments to estimate the parameters of the lognormal distribution. none of the above. 4. which are the steps involved in establishing a probabilistic model? 5. express in words the following mathematical expression, where x̄ is the sam-",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2089,  2224,  1996,  4118,  1997,  5312,  2000, 10197,  1996,
        11709,  1997,  1996,  8833, 12131,  9067,  4353,  1012,  3904,  1997,
         1996,  2682,  1012,  1018,  1012,  2029,  2024,  1996,  4084,  2920,
         1999,  7411,  1037,  4013,  3676, 27965,  4588,  2944,  1029,  1019,
         1012,  4671,  1999,  2616,  1996,  2206,  8045,  3670,  1010,  2073,
         1060,  2003,  1996,  3520,  1011,   102])"
712,1,"['mean', 'random variable', 'random', 'average', 'variable']", Self Assessment QuestionsExercises,seg_203,ple average of a random variable x and μx is the true mean of the random variable:,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 1., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101, 20228,  2063,  2779,  1997,  1037,  6721,  8023,  1060,  1998,
         1166,  2595,  2003,  1996,  2995,  2812,  1997,  1996,  6721,  8023,
         1024,   102])"
713,1,"['mean', 'hypothesis', 'hypothesis testing', 'null hypothesis', 'test']", Self Assessment QuestionsExercises,seg_203,6. what are the main steps of hypothesis testing? 7. an engineer wants to test the null hypothesis that the mean value of the con-,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1020,  1012,  2054,  2024,  1996,  2364,  4084,  1997, 10744,
         5604,  1029,  1021,  1012,  2019,  3992,  4122,  2000,  3231,  1996,
        19701, 10744,  2008,  1996,  2812,  3643,  1997,  1996,  9530,  1011,
          102])"
714,1,"['mean', 'type ii error', 'probability paper', 'null hypothesis', 'probability', 'design', 'type ii', 'hypothesis', 'type i error', 'hypothesis test', 'measurements', 'significance', 'error', 'test']", Self Assessment QuestionsExercises,seg_203,"crete cover depth of a concrete structure corresponds to design assumptions. in a preliminary assessment, a limited number of measurements of the concrete cover depth are made, and after performing the hypothesis test the engineer accepts the null hypothesis. after a few years, a comprehensive survey of the concrete cover depth is carried out and several measurements are made. the survey shows that the mean value of the concrete cover depth does not fulfill the design assumptions. which of the following statement(s) is(are) correct? in the preliminary survey the engineer has performed a type i error. in the preliminary survey the engineer has performed a type ii error. in the preliminary survey the engineer has performed a type i and a type ii error. 8. describe in a few words the significance of the use of probability paper in",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101, 19111,  3104,  5995,  1997,  1037,  5509,  3252, 14788,  2000,
         2640, 17568,  1012,  1999,  1037,  8824,  7667,  1010,  1037,  3132,
         2193,  1997, 11702,  1997,  1996,  5509,  3104,  5995,  2024,  2081,
         1010,  1998,  2044,  4488,  1996, 10744,  3231,  1996,  3992, 13385,
         1996, 19701, 10744,  1012,  2044,  1037,  2261,  2086,  1010,  1037,
         7721,  5002,  1997,  1996,  5509,  3104,  5995,  2003,  3344,  2041,
         1998,  2195, 11702,  2024,  2081,  1012,  1996,  5002,  3065,  2008,
         1996,  2812,  3643,  1997,  1996,  5509,  3104,  5995,  2515,  2025,
        13883,  1996,  2640, 17568,  1012,  2029,  1997,  1996,  2206,  4861,
         1006,  1055,  1007,  2003,  1006,  2024,  1007,  6149,  1029,  1999,
         1996,  8824,  5002,  1996,  3992,  2038,  2864,  1037,  2828,  1045,
         7561,  1012,  1999,  1996,  8824,  5002,  1996,  3992,  2038,  2864,
         1037,  2828,  2462,  7561,  1012,  1999,  1996,  8824,  5002,  1996,
         3992,  2038,  2864,  1037,  2828,  1045,  1998,  1037,  2828,  2462,
         7561,  1012,  1022,  1012,  6235,  1999,  1037,  2261,  2616,  1996,
         7784,  1997,  1996,  2224,  1997,  9723,  3259,  1999,   102])"
715,1,"['data', 'χ2 test', 'test']", Self Assessment QuestionsExercises,seg_203,"model selection and how it can be constructed. 9. in order to perform a χ2 test, how does the data need to be divided? 10. what are the main differences between the χ2 and the kolmogorov-smirnov",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2944,  4989,  1998,  2129,  2009,  2064,  2022,  3833,  1012,
         1023,  1012,  1999,  2344,  2000,  4685,  1037,  1177,  2475,  3231,
         1010,  2129,  2515,  1996,  2951,  2342,  2000,  2022,  4055,  1029,
         2184,  1012,  2054,  2024,  1996,  2364,  5966,  2090,  1996,  1177,
         2475,  1998,  1996, 12849, 13728, 22844, 12298,  1011, 15488,  4313,
        16693,   102])"
716,1,['tests'], Self Assessment QuestionsExercises,seg_203,"goodness of fit tests? 11. based on experience, it is known that the concrete compressive strength may",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101, 15003,  1997,  4906,  5852,  1029,  2340,  1012,  2241,  2006,
         3325,  1010,  2009,  2003,  2124,  2008,  1996,  5509,  4012, 27484,
         3997,  2089,   102])"
717,1,"['null hypothesis', 'goodness of fit', 'fit test', 'mean', 'significance level', 'normal distribution', 'random variable', 'normal', 'standard deviation', 'statistic', 'test', 'deviation', 'hypothesis', 'level', 'standard', 'significance', 'sample', 'random', 'sample statistic', 'intervals', 'distribution', 'variable']", Self Assessment QuestionsExercises,seg_203,be modeled by a normal distributed random variable x with mean value μx = 30 mpa and standard deviation σx = 5 mpa. the compressive strengths of 20 concrete cylinders are measured. an engineer wants to test the null hypothesis h0 that x follows a normal distribution with the above given parameters. he/she carries out a χ2 goodness of fit test by dividing the samples into 3 intervals. he/she calculates a chi-square sample statistic equal to εm 2 = 0.41. can the engineer accept the null hypothesis at the 5% significance level? 12. an engineer wants to examine and compare the suitability of two distribution,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2022, 14440,  2011,  1037,  3671,  5500,  6721,  8023,  1060,
         2007,  2812,  3643,  1166,  2595,  1027,  2382,  6131,  2050,  1998,
         3115, 24353,  1173,  2595,  1027,  1019,  6131,  2050,  1012,  1996,
         4012, 27484, 20828,  1997,  2322,  5509, 18729,  2024,  7594,  1012,
         2019,  3992,  4122,  2000,  3231,  1996, 19701, 10744,  1044,  2692,
         2008,  1060,  4076,  1037,  3671,  4353,  2007,  1996,  2682,  2445,
        11709,  1012,  2002,  1013,  2016,  7883,  2041,  1037,  1177,  2475,
        15003,  1997,  4906,  3231,  2011, 16023,  1996,  8168,  2046,  1017,
        14025,  1012,  2002,  1013,  2016, 18422,  2015,  1037,  9610,  1011,
         2675,  7099, 28093,  6553,  5020,  2000,  1159,  2213,  1016,  1027,
         1014,  1012,  4601,  1012,  2064,  1996,  3992,  5138,  1996, 19701,
        10744,  2012,  1996,  1019,  1003,  7784,  2504,  1029,  2260,  1012,
         2019,  3992,  4122,  2000, 11628,  1998, 12826,  1996,  4848,  8010,
         1997,  2048,  4353,   102])"
718,1,"['significance level', 'sample statistics', 'table', 'level', 'sample', 'random', 'results', 'statistics', 'model', 'measurements', 'significance']", Self Assessment QuestionsExercises,seg_203,function model alternatives for a random material property. measurements of the material property are taken. the engineer uses the two model alternatives to calculate the chi-square sample statistics and the corresponding sample likelihoods. the results are given in table 5.8. which of the following statement(s) is(are) correct? the engineer may accept model 1 at the 5% significance level.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  3853,  2944, 15955,  2005,  1037,  6721,  3430,  3200,  1012,
        11702,  1997,  1996,  3430,  3200,  2024,  2579,  1012,  1996,  3992,
         3594,  1996,  2048,  2944, 15955,  2000, 18422,  1996,  9610,  1011,
         2675,  7099,  6747,  1998,  1996,  7978,  7099, 16593,  2015,  1012,
         1996,  3463,  2024,  2445,  1999,  2795,  1019,  1012,  1022,  1012,
         2029,  1997,  1996,  2206,  4861,  1006,  1055,  1007,  2003,  1006,
         2024,  1007,  6149,  1029,  1996,  3992,  2089,  5138,  2944,  1015,
         2012,  1996,  1019,  1003,  7784,  2504,  1012,   102])"
719,1,"['model', 'significance', 'significance level', 'level']", Self Assessment QuestionsExercises,seg_203,the engineer may accept model 2 at the 5% significance level. model 1 is more suitable than model 2. none of the above.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([ 101, 1996, 3992, 2089, 5138, 2944, 1016, 2012, 1996, 1019, 1003, 7784,
        2504, 1012, 2944, 1015, 2003, 2062, 7218, 2084, 2944, 1016, 1012, 3904,
        1997, 1996, 2682, 1012,  102])"
720,1,"['error', 'set', 'probability', 'events', 'probabilities', 'random', 'random variables', 'variables']",Chapter  Methods of Structural Reliability,seg_205,lecture 12 (aim of the present lecture) the aim of the present lecture is to introduce the basic theory and tools facilitating the representation of events in terms of random variables and to calculate the probability of occurrence of such events. the classical error accumulation law is introduced first and this is then set in perspective to more modern and more general tools to assess probabilities. on the basis of the lecture it is expected that the reader will acquire knowledge on the following issues:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 3127,  4725,  1997,  8332, 15258])","tensor([  101,  8835,  2260,  1006,  6614,  1997,  1996,  2556,  8835,  1007,
         1996,  6614,  1997,  1996,  2556,  8835,  2003,  2000,  8970,  1996,
         3937,  3399,  1998,  5906, 25505,  1996,  6630,  1997,  2824,  1999,
         3408,  1997,  6721, 10857,  1998,  2000, 18422,  1996,  9723,  1997,
        14404,  1997,  2107,  2824,  1012,  1996,  4556,  7561, 20299,  2375,
         2003,  3107,  2034,  1998,  2023,  2003,  2059,  2275,  1999,  7339,
         2000,  2062,  2715,  1998,  2062,  2236,  5906,  2000, 14358,  4013,
         3676, 14680,  1012,  2006,  1996,  3978,  1997,  1996,  8835,  2009,
         2003,  3517,  2008,  1996,  8068,  2097,  9878,  3716,  2006,  1996,
         2206,  3314,  1024,   102])"
721,1,"['method', 'monte carlo method', 'events', 'failure', 'limit state function', 'normal', 'safety margin', 'case', 'function', 'linear', 'probability', 'nonlinear', 'random variables', 'basic random variables', 'random', 'limit', 'variables']",Chapter  Methods of Structural Reliability,seg_205,• how can events be represented in terms of basic random variables? • what is a limit state function and what is a safety margin? • what is the meaning of a reliability index and how does it relate to a failure probability? • how to calculate the reliability index for a linear safety margin when all basic random variables are normal distributed? • how to calculate the reliability index in the case of nonlinear safety margins? • what is the idea behind the monte carlo method? • what are the steps in the monte carlo method and how are they performed?,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 3127,  4725,  1997,  8332, 15258])","tensor([  101,  1528,  2129,  2064,  2824,  2022,  3421,  1999,  3408,  1997,
         3937,  6721, 10857,  1029,  1528,  2054,  2003,  1037,  5787,  2110,
         3853,  1998,  2054,  2003,  1037,  3808,  7785,  1029,  1528,  2054,
         2003,  1996,  3574,  1997,  1037, 15258,  5950,  1998,  2129,  2515,
         2009, 14396,  2000,  1037,  4945,  9723,  1029,  1528,  2129,  2000,
        18422,  1996, 15258,  5950,  2005,  1037,  7399,  3808,  7785,  2043,
         2035,  3937,  6721, 10857,  2024,  3671,  5500,  1029,  1528,  2129,
         2000, 18422,  1996, 15258,  5950,  1999,  1996,  2553,  1997, 27400,
         3808, 17034,  1029,  1528,  2054,  2003,  1996,  2801,  2369,  1996,
        10125,  9758,  4118,  1029,  1528,  2054,  2024,  1996,  4084,  1999,
         1996, 10125,  9758,  4118,  1998,  2129,  2024,  2027,  2864,  1029,
          102])"
722,0,[], Introduction,seg_207,"the first developments of first order reliability methods, also known as form methods, took place in the 1960s. since then the methods have been refined and extended significantly and by now they form one of the most important methods for reliability evaluations in structural reliability theory. several commercial computer codes have been developed for form analysis and the methods are widely used in practical engineering problems and for code calibration purposes.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0])",tensor([4955]),"tensor([  101,  1996,  2034,  8973,  1997,  2034,  2344, 15258,  4725,  1010,
         2036,  2124,  2004,  2433,  4725,  1010,  2165,  2173,  1999,  1996,
         4120,  1012,  2144,  2059,  1996,  4725,  2031,  2042, 15514,  1998,
         3668,  6022,  1998,  2011,  2085,  2027,  2433,  2028,  1997,  1996,
         2087,  2590,  4725,  2005, 15258,  9312,  2015,  1999,  8332, 15258,
         3399,  1012,  2195,  3293,  3274,  9537,  2031,  2042,  2764,  2005,
         2433,  4106,  1998,  1996,  4725,  2024,  4235,  2109,  1999,  6742,
         3330,  3471,  1998,  2005,  3642, 10250, 12322,  8156,  5682,  1012,
          102])"
723,0,[], Introduction,seg_207,in the present chapter first the basic idea behind form methods is highlighted and thereafter the individual steps of the methods are explained in detail.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])",tensor([4955]),"tensor([  101,  1999,  1996,  2556,  3127,  2034,  1996,  3937,  2801,  2369,
         2433,  4725,  2003, 11548,  1998,  6920,  1996,  3265,  4084,  1997,
         1996,  4725,  2024,  4541,  1999,  6987,  1012,   102])"
724,1,['monte carlo methods'], Introduction,seg_207,"finally, the basic concepts of monte carlo methods are outlined.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])",tensor([4955]),"tensor([  101,  2633,  1010,  1996,  3937,  8474,  1997, 10125,  9758,  4725,
         2024, 14801,  1012,   102])"
725,1,"['probability of failure', 'probability', 'failure', 'states', 'reference period']", Failure Events and Basic Random Variables,seg_209,"in reliability analysis of technical systems and components the main problem is to evaluate the probability of failure corresponding to a specified reference period. however, other non-failure states of the considered component or system may also be of interest, such as excessive damage, unavailability, etc.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 4945,  2824,  1998,  3937,  6721, 10857])","tensor([  101,  1999, 15258,  4106,  1997,  4087,  3001,  1998,  6177,  1996,
         2364,  3291,  2003,  2000, 16157,  1996,  9723,  1997,  4945,  7978,
         2000,  1037,  9675,  4431,  2558,  1012,  2174,  1010,  2060,  2512,
         1011,  4945,  2163,  1997,  1996,  2641,  6922,  2030,  2291,  2089,
         2036,  2022,  1997,  3037,  1010,  2107,  2004, 11664,  4053,  1010,
        14477,  3567, 11733,  8553,  1010,  4385,  1012,   102])"
726,1,"['loss', 'consequences', 'associated', 'events', 'failure', 'states', 'failure events']", Failure Events and Basic Random Variables,seg_209,"in general, any state which may be associated with consequences in terms of costs, loss of lives and impact to the environment is of interest. in the following section no differentiation will be made between these different types of states. for simplicity, all these events will be referred to as being failure events, bearing in mind, however, that also non-failure states may be considered in the same manner.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4945,  2824,  1998,  3937,  6721, 10857])","tensor([  101,  1999,  2236,  1010,  2151,  2110,  2029,  2089,  2022,  3378,
         2007,  8465,  1999,  3408,  1997,  5366,  1010,  3279,  1997,  3268,
         1998,  4254,  2000,  1996,  4044,  2003,  1997,  3037,  1012,  1999,
         1996,  2206,  2930,  2053, 20582,  2097,  2022,  2081,  2090,  2122,
         2367,  4127,  1997,  2163,  1012,  2005, 17839,  1010,  2035,  2122,
         2824,  2097,  2022,  3615,  2000,  2004,  2108,  4945,  2824,  1010,
         7682,  1999,  2568,  1010,  2174,  1010,  2008,  2036,  2512,  1011,
         4945,  2163,  2089,  2022,  2641,  1999,  1996,  2168,  5450,  1012,
          102])"
727,1,"['function', 'events', 'failure', 'event', 'limit state function', 'failure events', 'limit', 'failure event']", Failure Events and Basic Random Variables,seg_209,"it is convenient to describe failure events in terms of functional relations. if they are fulfilled, the considered event will occur. a failure event may be described by a functional relation, the limit state function g(x), in the following way:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4945,  2824,  1998,  3937,  6721, 10857])","tensor([  101,  2009,  2003, 14057,  2000,  6235,  4945,  2824,  1999,  3408,
         1997,  8360,  4262,  1012,  2065,  2027,  2024, 16829,  1010,  1996,
         2641,  2724,  2097,  5258,  1012,  1037,  4945,  2724,  2089,  2022,
         2649,  2011,  1037,  8360,  7189,  1010,  1996,  5787,  2110,  3853,
         1043,  1006,  1060,  1007,  1010,  1999,  1996,  2206,  2126,  1024,
          102])"
728,1,"['probability of failure', 'function', 'set', 'uncertainties', 'probability', 'failure', 'event', 'random', 'random variables', 'basic random variables', 'variables', 'failure event']", Failure Events and Basic Random Variables,seg_209,"where the components of the vector x are realizations of the basic random variables x representing all the relevant uncertainties influencing the probability of failure. in eq. 6.1 the failure event f is simply defined as the set of realizations of the function g(x), which is zero or negative.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4945,  2824,  1998,  3937,  6721, 10857])","tensor([  101,  2073,  1996,  6177,  1997,  1996,  9207,  1060,  2024, 12393,
         2015,  1997,  1996,  3937,  6721, 10857,  1060,  5052,  2035,  1996,
         7882,  9662,  7368, 25870,  1996,  9723,  1997,  4945,  1012,  1999,
         1041,  4160,  1012,  1020,  1012,  1015,  1996,  4945,  2724,  1042,
         2003,  3432,  4225,  2004,  1996,  2275,  1997, 12393,  2015,  1997,
         1996,  3853,  1043,  1006,  1060,  1007,  1010,  2029,  2003,  5717,
         2030,  4997,  1012,   102])"
729,1,"['events', 'failure']", Failure Events and Basic Random Variables,seg_209,"as already mentioned, events other than failure may also be of interest. in e.g. reliability updating problems, events of the following form are highly relevant:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4945,  2824,  1998,  3937,  6721, 10857])","tensor([  101,  2004,  2525,  3855,  1010,  2824,  2060,  2084,  4945,  2089,
         2036,  2022,  1997,  3037,  1012,  1999,  1041,  1012,  1043,  1012,
        15258,  2039, 16616,  3471,  1010,  2824,  1997,  1996,  2206,  2433,
         2024,  3811,  7882,  1024,   102])"
730,1,"['variables', 'event']", Failure Events and Basic Random Variables,seg_209,where i is an inspection event and h(x) refers to the basic uncertain variables representing the inspection event.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0.])","tensor([ 4945,  2824,  1998,  3937,  6721, 10857])","tensor([  101,  2073,  1045,  2003,  2019, 10569,  2724,  1998,  1044,  1006,
         1060,  1007,  5218,  2000,  1996,  3937,  9662, 10857,  5052,  1996,
        10569,  2724,  1012,   102])"
731,1,"['probability of failure', 'probability', 'failure', 'event', 'failure event']", Failure Events and Basic Random Variables,seg_209,"having defined the failure event, the probability of failure pf may be determined by the following integral:",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 4945,  2824,  1998,  3937,  6721, 10857])","tensor([ 101, 2383, 4225, 1996, 4945, 2724, 1010, 1996, 9723, 1997, 4945, 1052,
        2546, 2089, 2022, 4340, 2011, 1996, 2206, 9897, 1024,  102])"
732,1,"['density function', 'simulation', 'efficient', 'asymptotic', 'joint probability', 'probability density function', 'laplace integral expansions', 'function', 'probability', 'monte carlo simulation', 'random variables', 'laplace expansions', 'joint probability density function', 'random', 'variables', 'joint', 'numerical']", Failure Events and Basic Random Variables,seg_209,"where fx(x) is the joint probability density function of the random variables x. this integral is, however, non-trivial to solve and numerical approximations are expedient. various methods for the solution of the integral in eq. 6.3 have been proposed including numerical integration techniques, monte carlo simulation and asymptotic laplace expansions. numerical integration techniques very rapidly became inefficient for increasing size of the vector x and are in general irrelevant. in the following section the focus is directed toward the widely applied and quite efficient form methods, which furthermore are consistent with the solutions obtained by asymptotic laplace integral expansions.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 0., 0.])","tensor([ 4945,  2824,  1998,  3937,  6721, 10857])","tensor([  101,  2073, 23292,  1006,  1060,  1007,  2003,  1996,  4101,  9723,
         4304,  3853,  1997,  1996,  6721, 10857,  1060,  1012,  2023,  9897,
         2003,  1010,  2174,  1010,  2512,  1011, 20610,  2000,  9611,  1998,
        15973, 20167,  2015,  2024,  4654,  5669, 11638,  1012,  2536,  4725,
         2005,  1996,  5576,  1997,  1996,  9897,  1999,  1041,  4160,  1012,
         1020,  1012,  1017,  2031,  2042,  3818,  2164, 15973,  8346,  5461,
         1010, 10125,  9758, 12504,  1998,  2004, 24335, 13876, 20214,  5001,
        19217,  4935,  2015,  1012, 15973,  8346,  5461,  2200,  5901,  2150,
         1999, 12879,  8873, 23402,  3372,  2005,  4852,  2946,  1997,  1996,
         9207,  1060,  1998,  2024,  1999,  2236, 22537,  1012,  1999,  1996,
         2206,  2930,  1996,  3579,  2003,  2856,  2646,  1996,  4235,  4162,
         1998,  3243,  8114,  2433,  4725,  1010,  2029,  7297,  2024,  8335,
         2007,  1996,  7300,  4663,  2011,  2004, 24335, 13876, 20214,  5001,
        19217,  9897,  4935,  2015,  1012,   102])"
733,1,"['function', 'linear', 'limit state function', 'random', 'random variables', 'limit', 'basic random variables', 'variables', 'case']", Linear Limit State Functions and Normal Distributed Variables,seg_211,for illustrative purposes first the case where the limit state function g(x) is a linear function of the basic random variables x is considered. then the limit state function may be written as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0.])","tensor([ 7399,  5787,  2110,  4972,  1998,  3671,  5500, 10857])","tensor([  101,  2005,  5665, 19966, 18514,  5682,  2034,  1996,  2553,  2073,
         1996,  5787,  2110,  3853,  1043,  1006,  1060,  1007,  2003,  1037,
         7399,  3853,  1997,  1996,  3937,  6721, 10857,  1060,  2003,  2641,
         1012,  2059,  1996,  5787,  2110,  3853,  2089,  2022,  2517,  2004,
         1024,   102])"
734,1,"['failure', 'safety margin']", Linear Limit State Functions and Normal Distributed Variables,seg_211,the safety margin is then defined as m = g(x). failure can be defined by:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.])","tensor([ 7399,  5787,  2110,  4972,  1998,  3671,  5500, 10857])","tensor([ 101, 1996, 3808, 7785, 2003, 2059, 4225, 2004, 1049, 1027, 1043, 1006,
        1060, 1007, 1012, 4945, 2064, 2022, 4225, 2011, 1024,  102])"
735,1,"['linear', 'random', 'normal', 'random variables', 'safety margin', 'basic random variables', 'variables']", Linear Limit State Functions and Normal Distributed Variables,seg_211,"if the basic random variables are normal distributed, the linear safety margin m defined through",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])","tensor([ 7399,  5787,  2110,  4972,  1998,  3671,  5500, 10857])","tensor([  101,  2065,  1996,  3937,  6721, 10857,  2024,  3671,  5500,  1010,
         1996,  7399,  3808,  7785,  1049,  4225,  2083,   102])"
736,1,"['mean', 'normal', 'variance']", Linear Limit State Functions and Normal Distributed Variables,seg_211,is also normal distributed with mean value and variance:,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.])","tensor([ 7399,  5787,  2110,  4972,  1998,  3671,  5500, 10857])","tensor([  101,  2003,  2036,  3671,  5500,  2007,  2812,  3643,  1998, 23284,
         1024,   102])"
737,1,"['coefficients', 'correlation', 'variables', 'correlation coefficients']", Linear Limit State Functions and Normal Distributed Variables,seg_211,ρij are the correlation coefficients between the variables xi and xj .,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 7399,  5787,  2110,  4972,  1998,  3671,  5500, 10857])","tensor([  101,  1171, 28418,  2024,  1996, 16902, 21374,  2090,  1996, 10857,
         8418,  1998,  1060,  3501,  1012,   102])"
738,1,"['probability of failure', 'probability', 'failure', 'event', 'failure event']", Linear Limit State Functions and Normal Distributed Variables,seg_211,"defining the failure event by eq. 6.1, the probability of failure can be written as:",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0.])","tensor([ 7399,  5787,  2110,  4972,  1998,  3671,  5500, 10857])","tensor([  101, 12854,  1996,  4945,  2724,  2011,  1041,  4160,  1012,  1020,
         1012,  1015,  1010,  1996,  9723,  1997,  4945,  2064,  2022,  2517,
         2004,  1024,   102])"
739,1,"['function', 'standard normal', 'normal', 'standard', 'case']", Linear Limit State Functions and Normal Distributed Variables,seg_211,which in this simple case reduces to the evaluation of the standard normal distribution function:,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.])","tensor([ 7399,  5787,  2110,  4972,  1998,  3671,  5500, 10857])","tensor([  101,  2029,  1999,  2023,  3722,  2553, 13416,  2000,  1996,  9312,
         1997,  1996,  3115,  3671,  4353,  3853,  1024,   102])"
740,0,[], Linear Limit State Functions and Normal Distributed Variables,seg_211,where the reliability index β (cornell [6] and basler [3]) is given as:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 7399,  5787,  2110,  4972,  1998,  3671,  5500, 10857])","tensor([  101,  2073,  1996, 15258,  5950,  1156,  1006, 10921,  1031,  1020,
         1033,  1998, 19021,  3917,  1031,  1017,  1033,  1007,  2003,  2445,
         2004,  1024,   102])"
741,1,"['random', 'random variables', 'basic random variables', 'variables', 'case']", Linear Limit State Functions and Normal Distributed Variables,seg_211,"in the case where the basic random variables are uncorrelated, the reliability index β as defined in eq. 6.10 has a geometrical interpretation as illustrated in fig. 6.1 where a two dimensional case is considered.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 7399,  5787,  2110,  4972,  1998,  3671,  5500, 10857])","tensor([  101,  1999,  1996,  2553,  2073,  1996,  3937,  6721, 10857,  2024,
         4895, 27108, 16570,  4383,  1010,  1996, 15258,  5950,  1156,  2004,
         4225,  1999,  1041,  4160,  1012,  1020,  1012,  2184,  2038,  1037,
        14965,  2389,  7613,  2004,  7203,  1999, 20965,  1012,  1020,  1012,
         1015,  2073,  1037,  2048,  8789,  2553,  2003,  2641,  1012,   102])"
742,1,"['function', 'standardization', 'limit state function', 'transformed', 'random', 'random variables', 'limit', 'variables']", Linear Limit State Functions and Normal Distributed Variables,seg_211,in fig. 6.1 the limit state function g(x) has been transformed into the limit state function g(u) by standardization of the random variables as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 7399,  5787,  2110,  4972,  1998,  3671,  5500, 10857])","tensor([  101,  1999, 20965,  1012,  1020,  1012,  1015,  1996,  5787,  2110,
         3853,  1043,  1006,  1060,  1007,  2038,  2042,  8590,  2046,  1996,
         5787,  2110,  3853,  1043,  1006,  1057,  1007,  2011, 28648,  1997,
         1996,  6721, 10857,  2004,  1024,   102])"
743,1,"['mean', 'deviation', 'random', 'standard deviation', 'random variables', 'standard', 'variables']", Linear Limit State Functions and Normal Distributed Variables,seg_211,such that the random variables ui have mean values equal to zero and standard deviation values equal to one.,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0.])","tensor([ 7399,  5787,  2110,  4972,  1998,  3671,  5500, 10857])","tensor([  101,  2107,  2008,  1996,  6721, 10857, 21318,  2031,  2812,  5300,
         5020,  2000,  5717,  1998,  3115, 24353,  5300,  5020,  2000,  2028,
         1012,   102])"
744,1,"['failure domain', 'function', 'design', 'failure', 'design point', 'event', 'limit state function', 'limit', 'safe domain', 'failure event']", Linear Limit State Functions and Normal Distributed Variables,seg_211,"the reliability index β has the simple geometrical interpretation as the smallest distance from the line (or generally the hyper-plane) forming the boundary between the safe domain and the failure domain, i.e. the domain defined by the failure event. it should be noted that this definition of the reliability index due to hasofer and lind [7] does not depend on the limit state function but rather the boundary between the safe domain and the failure domain. the point on the failure surface with the smallest distance to the origin is commonly called the design point or the most likely failure point.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 7399,  5787,  2110,  4972,  1998,  3671,  5500, 10857])","tensor([  101,  1996, 15258,  5950,  1156,  2038,  1996,  3722, 14965,  2389,
         7613,  2004,  1996, 10479,  3292,  2013,  1996,  2240,  1006,  2030,
         3227,  1996, 23760,  1011,  4946,  1007,  5716,  1996,  6192,  2090,
         1996,  3647,  5884,  1998,  1996,  4945,  5884,  1010,  1045,  1012,
         1041,  1012,  1996,  5884,  4225,  2011,  1996,  4945,  2724,  1012,
         2009,  2323,  2022,  3264,  2008,  2023,  6210,  1997,  1996, 15258,
         5950,  2349,  2000,  2038, 11253,  2121,  1998, 11409,  2094,  1031,
         1021,  1033,  2515,  2025, 12530,  2006,  1996,  5787,  2110,  3853,
         2021,  2738,  1996,  6192,  2090,  1996,  3647,  5884,  1998,  1996,
         4945,  5884,  1012,  1996,  2391,  2006,  1996,  4945,  3302,  2007,
         1996, 10479,  3292,  2000,  1996,  4761,  2003,  4141,  2170,  1996,
         2640,  2391,  2030,  1996,  2087,  3497,  4945,  2391,  1012,   102])"
745,1,"['mean', 'probability of failure', 'probability', 'failure', 'information', 'random', 'random variables', 'standard', 'deviations', 'standard deviations', 'basic random variables', 'variables', 'case']", Linear Limit State Functions and Normal Distributed Variables,seg_211,"it is seen that the evaluation of the probability of failure in this simple case reduces to a simple evaluation in terms of mean values and standard deviations of the basic random variables, i.e. the first and second order information.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 7399,  5787,  2110,  4972,  1998,  3671,  5500, 10857])","tensor([  101,  2009,  2003,  2464,  2008,  1996,  9312,  1997,  1996,  9723,
         1997,  4945,  1999,  2023,  3722,  2553, 13416,  2000,  1037,  3722,
         9312,  1999,  3408,  1997,  2812,  5300,  1998,  3115, 24353,  2015,
         1997,  1996,  3937,  6721, 10857,  1010,  1045,  1012,  1041,  1012,
         1996,  2034,  1998,  2117,  2344,  2592,  1012,   102])"
746,0,[], Example Reliability of a Steel RodLinear Safety Margin,seg_213,consider a steel rod under pure tension loading. the rod will fail if the applied stresses on the rod cross-sectional area (a = 10 mm2) exceed the steel yield strength. the yield strength r of the rod and the annual maximum stress in the rod s are as-,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2742, 15258,  1997,  1037,  3886,  8473,  4179,  2906,  3808,  7785])","tensor([  101,  5136,  1037,  3886,  8473,  2104,  5760,  6980, 10578,  1012,
         1996,  8473,  2097,  8246,  2065,  1996,  4162, 23253,  2006,  1996,
         8473,  2892,  1011, 27197,  2181,  1006,  1037,  1027,  2184,  3461,
         2475,  1007, 13467,  1996,  3886, 10750,  3997,  1012,  1996, 10750,
         3997,  1054,  1997,  1996,  8473,  1998,  1996,  3296,  4555,  6911,
         1999,  1996,  8473,  1055,  2024,  2004,  1011,   102])"
747,1,"['mean', 'standard deviations', 'normal', 'standard', 'deviations', 'variables']", Example Reliability of a Steel RodLinear Safety Margin,seg_213,"sumed to be uncertain, modeled by uncorrelated normal distributed variables r and s, respectively. the mean values and the standard deviations of the yield strength and the loading force are given as μr = 350 mpa, σr = 35 mpa and μs = 1500 n, σs = 300 n respectively.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 15258,  1997,  1037,  3886,  8473,  4179,  2906,  3808,  7785])","tensor([  101,  7680,  2098,  2000,  2022,  9662,  1010, 14440,  2011,  4895,
        27108, 16570,  4383,  3671,  5500, 10857,  1054,  1998,  1055,  1010,
         4414,  1012,  1996,  2812,  5300,  1998,  1996,  3115, 24353,  2015,
         1997,  1996, 10750,  3997,  1998,  1996, 10578,  2486,  2024,  2445,
         2004,  1166,  2099,  1027,  8698,  6131,  2050,  1010,  1173,  2099,
         1027,  3486,  6131,  2050,  1998,  1166,  2015,  1027, 10347,  1050,
         1010,  1173,  2015,  1027,  3998,  1050,  4414,  1012,   102])"
748,1,"['function', 'failure', 'event', 'limit state function', 'limit']", Example Reliability of a Steel RodLinear Safety Margin,seg_213,the limit state function describing the event of failure may be written as:,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 15258,  1997,  1037,  3886,  8473,  4179,  2906,  3808,  7785])","tensor([ 101, 1996, 5787, 2110, 3853, 7851, 1996, 2724, 1997, 4945, 2089, 2022,
        2517, 2004, 1024,  102])"
749,1,['safety margin'], Example Reliability of a Steel RodLinear Safety Margin,seg_213,whereby the safety margin m may be written as:,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 15258,  1997,  1037,  3886,  8473,  4179,  2906,  3808,  7785])","tensor([  101, 13557,  1996,  3808,  7785,  1049,  2089,  2022,  2517,  2004,
         1024,   102])"
750,1,"['mean', 'deviation', 'standard deviation', 'standard', 'safety margin']", Example Reliability of a Steel RodLinear Safety Margin,seg_213,the mean value and standard deviation of the safety margin m are thus:,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 2742, 15258,  1997,  1037,  3886,  8473,  4179,  2906,  3808,  7785])","tensor([  101,  1996,  2812,  3643,  1998,  3115, 24353,  1997,  1996,  3808,
         7785,  1049,  2024,  2947,  1024,   102])"
751,0,[], Example Reliability of a Steel RodLinear Safety Margin,seg_213,whereby the reliability index may be calculated as:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2742, 15258,  1997,  1037,  3886,  8473,  4179,  2906,  3808,  7785])","tensor([  101, 13557,  1996, 15258,  5950,  2089,  2022, 10174,  2004,  1024,
          102])"
752,1,"['probability', 'failure']", Example Reliability of a Steel RodLinear Safety Margin,seg_213,finally the annual failure probability is determined as:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 2742, 15258,  1997,  1037,  3886,  8473,  4179,  2906,  3808,  7785])","tensor([ 101, 2633, 1996, 3296, 4945, 9723, 2003, 4340, 2004, 1024,  102])"
753,1,"['function', 'results', 'errors', 'statistical']", The Error Propagation Law,seg_215,"the results given in eq. 6.7 have been applied to study the statistical characteristics of errors ε accumulating in accordance with some differentiable function h(x), i.e.:",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  7561, 20594,  2375])","tensor([  101,  1996,  3463,  2445,  1999,  1041,  4160,  1012,  1020,  1012,
         1021,  2031,  2042,  4162,  2000,  2817,  1996,  7778,  6459,  1997,
        10697,  1159, 16222,  2819, 10924,  1999, 10388,  2007,  2070,  2367,
        19210,  3853,  1044,  1006,  1060,  1007,  1010,  1045,  1012,  1041,
         1012,  1024,   102])"
754,1,"['correlation', 'mean', 'uncertainties', 'coefficients', 'function', 'linear', 'random variables', 'basic random variables', 'correlation coefficients', 'random', 'deviations', 'measurement', 'variables']", The Error Propagation Law,seg_215,"where x = (x1, x2, . . . , xn)t is a vector of realizations of the basic random variables x representing measurement uncertainties with mean values μx = (μx1,μx2, . . . ,μxn)t and covariances cxixj = ρij σxi σxj where σxi are the standard deviations and ρij the correlation coefficients. the idea is to approximate the function h(x) by its taylor expansion including only the linear terms, i.e.:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  7561, 20594,  2375])","tensor([  101,  2073,  1060,  1027,  1006,  1060,  2487,  1010,  1060,  2475,
         1010,  1012,  1012,  1012,  1010,  1060,  2078,  1007,  1056,  2003,
         1037,  9207,  1997, 12393,  2015,  1997,  1996,  3937,  6721, 10857,
         1060,  5052, 10903,  9662,  7368,  2007,  2812,  5300,  1166,  2595,
         1027,  1006,  1166,  2595,  2487,  1010,  1166,  2595,  2475,  1010,
         1012,  1012,  1012,  1010,  1166,  2595,  2078,  1007,  1056,  1998,
         2522, 10755, 28335,  2015,  1039,  9048,  2595,  3501,  1027,  1171,
        28418,  1173,  9048,  1173,  2595,  3501,  2073,  1173,  9048,  2024,
         1996,  3115, 24353,  2015,  1998,  1171, 28418,  1996, 16902, 21374,
         1012,  1996,  2801,  2003,  2000, 15796,  1996,  3853,  1044,  1006,
         1060,  1007,  2011,  2049,  4202,  4935,  2164,  2069,  1996,  7399,
         3408,  1010,  1045,  1012,  1041,  1012,  1024,   102])"
755,1,"['mean', 'linearization']", The Error Propagation Law,seg_215,"where x0 = (x1,0, x2,0, . . . , xn,0)t is the point in which the linearization is performed, normally chosen as the mean value point.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 1996,  7561, 20594,  2375])","tensor([ 101, 2073, 1060, 2692, 1027, 1006, 1060, 2487, 1010, 1014, 1010, 1060,
        2475, 1010, 1014, 1010, 1012, 1012, 1012, 1010, 1060, 2078, 1010, 1014,
        1007, 1056, 2003, 1996, 2391, 1999, 2029, 1996, 7399, 3989, 2003, 2864,
        1010, 5373, 4217, 2004, 1996, 2812, 3643, 2391, 1012,  102])"
756,1,"['expected value', 'error']", The Error Propagation Law,seg_215,x = x0. from eqs. 6.13 and 6.7 it is seen that the expected value of the error e[ε],tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 1996,  7561, 20594,  2375])","tensor([ 101, 1060, 1027, 1060, 2692, 1012, 2013, 1041, 4160, 2015, 1012, 1020,
        1012, 2410, 1998, 1020, 1012, 1021, 2009, 2003, 2464, 2008, 1996, 3517,
        3643, 1997, 1996, 7561, 1041, 1031, 1159, 1033,  102])"
757,0,[], The Error Propagation Law,seg_215,can be assessed by:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0])","tensor([ 1996,  7561, 20594,  2375])","tensor([  101,  2064,  2022, 14155,  2011,  1024,   102])"
758,1,['variance'], The Error Propagation Law,seg_215,and its variance var[ε] can be determined by:,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  7561, 20594,  2375])","tensor([  101,  1998,  2049, 23284, 13075,  1031,  1159,  1033,  2064,  2022,
         4340,  2011,  1024,   102])"
759,1,"['error', 'variance']", The Error Propagation Law,seg_215,it is important to notice that the variance of the error as given by eq. 6.15 depends,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 1996,  7561, 20594,  2375])","tensor([  101,  2009,  2003,  2590,  2000,  5060,  2008,  1996, 23284,  1997,
         1996,  7561,  2004,  2445,  2011,  1041,  4160,  1012,  1020,  1012,
         2321,  9041,   102])"
760,1,['linearization'], The Error Propagation Law,seg_215,"on the linearization point, i.e. x0 = (x1,0, x2,0, . . . , xn,0)t .",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 1996,  7561, 20594,  2375])","tensor([ 101, 2006, 1996, 7399, 3989, 2391, 1010, 1045, 1012, 1041, 1012, 1060,
        2692, 1027, 1006, 1060, 2487, 1010, 1014, 1010, 1060, 2475, 1010, 1014,
        1010, 1012, 1012, 1012, 1010, 1060, 2078, 1010, 1014, 1007, 1056, 1012,
         102])"
761,1,"['error propagation law', 'error']", The Error Propagation Law,seg_215,in fig. 6.2 the basic principle of the error propagation law is represented for,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0.])","tensor([ 1996,  7561, 20594,  2375])","tensor([  101,  1999, 20965,  1012,  1020,  1012,  1016,  1996,  3937,  6958,
         1997,  1996,  7561, 20594,  2375,  2003,  3421,  2005,   102])"
762,1,"['random', 'random variable', 'variable', 'case']", The Error Propagation Law,seg_215,the (two-dimensional and therefore still representable) case that a random variable,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.])","tensor([ 1996,  7561, 20594,  2375])","tensor([ 101, 1996, 1006, 2048, 1011, 8789, 1998, 3568, 2145, 5050, 3085, 1007,
        2553, 2008, 1037, 6721, 8023,  102])"
763,1,"['mean', 'function', 'random variable', 'random', 'linearized', 'variable']", The Error Propagation Law,seg_215,y depends on only one random variable x: y = g(x). the function y = g(x) is linearized in the mean value point and the relationship between y and x is approx-,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  7561, 20594,  2375])","tensor([  101,  1061,  9041,  2006,  2069,  2028,  6721,  8023,  1060,  1024,
         1061,  1027,  1043,  1006,  1060,  1007,  1012,  1996,  3853,  1061,
         1027,  1043,  1006,  1060,  1007,  2003,  7399,  3550,  1999,  1996,
         2812,  3643,  2391,  1998,  1996,  3276,  2090,  1061,  1998,  1060,
         2003, 22480,  1011,   102])"
764,1,"['error', 'transformation', 'random variable', 'true distribution', 'random', 'distribution', 'error propagation law', 'variable']", The Error Propagation Law,seg_215,"∂x the distribution of the derived random variable y represented on the vertical axis in fig. 6.2 illustrates the approximated distribution resulting from the error propagation law. depending on the form of the transformation function h(x), the distribution can differ from the true distribution for y significantly.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0.])","tensor([ 1996,  7561, 20594,  2375])","tensor([  101,  1592,  2595,  1996,  4353,  1997,  1996,  5173,  6721,  8023,
         1061,  3421,  2006,  1996,  7471,  8123,  1999, 20965,  1012,  1020,
         1012,  1016, 24899,  1996, 15796,  2094,  4353,  4525,  2013,  1996,
         7561, 20594,  2375,  1012,  5834,  2006,  1996,  2433,  1997,  1996,
         8651,  3853,  1044,  1006,  1060,  1007,  1010,  1996,  4353,  2064,
        11234,  2013,  1996,  2995,  4353,  2005,  1061,  6022,  1012,   102])"
765,1,"['condition', 'independent', 'normal', 'expected values', 'error propagation law', 'standard deviations', 'uncertainty', 'probability', 'random variables', 'standard', 'random', 'deviations', 'measurement', 'variables', 'error']", Example Error Propagation Law,seg_217,"as an example of the use of the error propagation law consider a right angle triangle abc, where c is the right angle. the lengths of the 2 catheti a and b are measured. due to measurement uncertainty a and b are modeled as independent normal distributed random variables with expected values μa = 12.2, μb = 5.1 and standard deviations σa = 0.4 and σb = 0.3, respectively. it is assumed that a critical condition will occur if the hypotenuse c is larger than 13.5 and the probability that this condition should happen is to be assessed.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,
        1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2742,  7561, 20594,  2375])","tensor([  101,  2004,  2019,  2742,  1997,  1996,  2224,  1997,  1996,  7561,
        20594,  2375,  5136,  1037,  2157,  6466,  9546,  5925,  1010,  2073,
         1039,  2003,  1996,  2157,  6466,  1012,  1996, 10742,  1997,  1996,
         1016,  4937, 27065,  2072,  1037,  1998,  1038,  2024,  7594,  1012,
         2349,  2000, 10903, 12503,  1037,  1998,  1038,  2024, 14440,  2004,
         2981,  3671,  5500,  6721, 10857,  2007,  3517,  5300,  1166,  2050,
         1027,  2260,  1012,  1016,  1010,  1166,  2497,  1027,  1019,  1012,
         1015,  1998,  3115, 24353,  2015,  1173,  2050,  1027,  1014,  1012,
         1018,  1998,  1173,  2497,  1027,  1014,  1012,  1017,  1010,  4414,
         1012,  2009,  2003,  5071,  2008,  1037,  4187,  4650,  2097,  5258,
         2065,  1996,  1044, 22571, 12184, 10182,  2063,  1039,  2003,  3469,
         2084,  2410,  1012,  1019,  1998,  1996,  9723,  2008,  2023,  4650,
         2323,  4148,  2003,  2000,  2022, 14155,  1012,   102])"
766,1,"['probabilistic', 'probabilistic model', 'statistical', 'model']", Example Error Propagation Law,seg_217,"based on the probabilistic model of a and b, the statistical characteristics of the hypotenuse c given by:",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  7561, 20594,  2375])","tensor([  101,  2241,  2006,  1996,  4013,  3676, 27965,  4588,  2944,  1997,
         1037,  1998,  1038,  1010,  1996,  7778,  6459,  1997,  1996,  1044,
        22571, 12184, 10182,  2063,  1039,  2445,  2011,  1024,   102])"
767,1,"['model', 'error']", Example Error Propagation Law,seg_217,"may be assessed through the error propagation model given by eqs. 6.14–6.15, yielding (see also appendix b):",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  7561, 20594,  2375])","tensor([  101,  2089,  2022, 14155,  2083,  1996,  7561, 20594,  2944,  2445,
         2011,  1041,  4160,  2015,  1012,  1020,  1012,  2403,  1516,  1020,
         1012,  2321,  1010, 21336,  1006,  2156,  2036, 22524,  1038,  1007,
         1024,   102])"
768,1,['expected values'], Example Error Propagation Law,seg_217,which by inserting for a and b their expected values yield:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 2742,  7561, 20594,  2375])","tensor([  101,  2029,  2011, 19274,  2075,  2005,  1037,  1998,  1038,  2037,
         3517,  5300, 10750,  1024,   102])"
769,1,"['mean', 'variance', 'linearization', 'deviations', 'standard', 'standard deviations']", Example Error Propagation Law,seg_217,"as seen from the above, the variance of the hypotenuse c depends on the chosen linearization point. if, instead of the mean value point, a value corresponding to the mean value plus two standard deviations was chosen, the variance of c would have been:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 2742,  7561, 20594,  2375])","tensor([  101,  2004,  2464,  2013,  1996,  2682,  1010,  1996, 23284,  1997,
         1996,  1044, 22571, 12184, 10182,  2063,  1039,  9041,  2006,  1996,
         4217,  7399,  3989,  2391,  1012,  2065,  1010,  2612,  1997,  1996,
         2812,  3643,  2391,  1010,  1037,  3643,  7978,  2000,  1996,  2812,
         3643,  4606,  2048,  3115, 24353,  2015,  2001,  4217,  1010,  1996,
        23284,  1997,  1039,  2052,  2031,  2042,  1024,   102])"
770,1,"['probability', 'errors', 'consequences']", Example Error Propagation Law,seg_217,"which can be shown to imply a 0.3% reduction of the probability that the hypotenuse is larger than 13.5. even though such a change seems small, it could be of importance in a practical situation where the consequences of errors can be significant.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0.])","tensor([ 2742,  7561, 20594,  2375])","tensor([  101,  2029,  2064,  2022,  3491,  2000, 19515,  1037,  1014,  1012,
         1017,  1003,  7312,  1997,  1996,  9723,  2008,  1996,  1044, 22571,
        12184, 10182,  2063,  2003,  3469,  2084,  2410,  1012,  1019,  1012,
         2130,  2295,  2107,  1037,  2689,  3849,  2235,  1010,  2009,  2071,
         2022,  1997,  5197,  1999,  1037,  6742,  3663,  2073,  1996,  8465,
         1997, 10697,  2064,  2022,  3278,  1012,   102])"
771,1,"['failure domain', 'error', 'function', 'failure', 'limit state function', 'linearization', 'random', 'random variables', 'limit', 'safe domain', 'error propagation law', 'variables', 'basic random variables']", Nonlinear Limit State Functions,seg_219,"when the limit state function is non-linear in the basic random variables x, the situation is not as simple as outlined in the previous section. an obvious approach is, however, considering the error propagation law explained in the foregoing, to represent the failure domain in terms of a linearization of the boundary between the safe domain and the failure domain, i.e. the failure surface. still, the question remains how to do this appropriately.",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([27400,  5787,  2110,  4972])","tensor([  101,  2043,  1996,  5787,  2110,  3853,  2003,  2512,  1011,  7399,
         1999,  1996,  3937,  6721, 10857,  1060,  1010,  1996,  3663,  2003,
         2025,  2004,  3722,  2004, 14801,  1999,  1996,  3025,  2930,  1012,
         2019,  5793,  3921,  2003,  1010,  2174,  1010,  6195,  1996,  7561,
        20594,  2375,  4541,  1999,  1996, 18921, 26966,  1010,  2000,  5050,
         1996,  4945,  5884,  1999,  3408,  1997,  1037,  7399,  3989,  1997,
         1996,  6192,  2090,  1996,  3647,  5884,  1998,  1996,  4945,  5884,
         1010,  1045,  1012,  1041,  1012,  1996,  4945,  3302,  1012,  2145,
         1010,  1996,  3160,  3464,  2129,  2000,  2079,  2023, 23263,  1012,
          102])"
772,1,"['normalized', 'design', 'failure', 'design point', 'linearization']", Nonlinear Limit State Functions,seg_219,hasofer and lind [7] suggest performing this linearization in the design point of the failure surface represented in normalized space. the situation is illustrated in the 2-dimensional space in fig. 6.3.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([27400,  5787,  2110,  4972])","tensor([  101,  2038, 11253,  2121,  1998, 11409,  2094,  1031,  1021,  1033,
         6592,  4488,  2023,  7399,  3989,  1999,  1996,  2640,  2391,  1997,
         1996,  4945,  3302,  3421,  1999,  3671,  3550,  2686,  1012,  1996,
         3663,  2003,  7203,  1999,  1996,  1016,  1011,  8789,  2686,  1999,
        20965,  1012,  1020,  1012,  1017,  1012,   102])"
773,1,"['design', 'failure', 'design point', 'linearized', 'normal']", Nonlinear Limit State Functions,seg_219,"in fig. 6.3 a principal sketch is given, illustrating that the failure surface is linearized in the design point u∗ by the line g′(u) = 0. the α-vector is the outward directed normal vector to the failure surface in the design point u∗ i.e. the point on the linearized failure surface with the shortest distance β to the origin.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([27400,  5787,  2110,  4972])","tensor([  101,  1999, 20965,  1012,  1020,  1012,  1017,  1037,  4054, 11080,
         2003,  2445,  1010, 28252,  2008,  1996,  4945,  3302,  2003,  7399,
         3550,  1999,  1996,  2640,  2391,  1057, 30125,  2011,  1996,  2240,
         1043,  1531,  1006,  1057,  1007,  1027,  1014,  1012,  1996,  1155,
         1011,  9207,  2003,  1996, 15436,  2856,  3671,  9207,  2000,  1996,
         4945,  3302,  1999,  1996,  2640,  2391,  1057, 30125,  1045,  1012,
         1041,  1012,  1996,  2391,  2006,  1996,  7399,  3550,  4945,  3302,
         2007,  1996, 20047,  3292,  1156,  2000,  1996,  4761,  1012,   102])"
774,1,"['function', 'design', 'design point', 'limit state function', 'limit']", Nonlinear Limit State Functions,seg_219,"as the limit state function is in general non-linear, one does not know the design point in advance. this has to be found e.g. by solving the following optimization problem:",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([27400,  5787,  2110,  4972])","tensor([  101,  2004,  1996,  5787,  2110,  3853,  2003,  1999,  2236,  2512,
         1011,  7399,  1010,  2028,  2515,  2025,  2113,  1996,  2640,  2391,
         1999,  5083,  1012,  2023,  2038,  2000,  2022,  2179,  1041,  1012,
         1043,  1012,  2011, 13729,  1996,  2206, 20600,  3291,  1024,   102])"
775,1,"['function', 'limit', 'limit state function']", Nonlinear Limit State Functions,seg_219,"this problem may be solved in a number of different ways. provided that the limit state function is differentiable, the following iteration scheme may be followed:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([27400,  5787,  2110,  4972])","tensor([  101,  2023,  3291,  2089,  2022, 13332,  1999,  1037,  2193,  1997,
         2367,  3971,  1012,  3024,  2008,  1996,  5787,  2110,  3853,  2003,
         2367, 19210,  1010,  1996,  2206, 27758,  5679,  2089,  2022,  2628,
         1024,   102])"
776,1,"['design', 'normal', 'failure', 'design point']", Nonlinear Limit State Functions,seg_219,"first, a design point is guessed u∗ = βα and inserted into eq. 6.17 whereby a new normal vector α to the failure surface is achieved. then this α-vector is inserted into eq. 6.18, from which a new β-value is calculated.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([27400,  5787,  2110,  4972])","tensor([  101,  2034,  1010,  1037,  2640,  2391,  2003, 11445,  1057, 30125,
         1027,  1156, 14608,  1998, 12889,  2046,  1041,  4160,  1012,  1020,
         1012,  2459, 13557,  1037,  2047,  3671,  9207,  1155,  2000,  1996,
         4945,  3302,  2003,  4719,  1012,  2059,  2023,  1155,  1011,  9207,
         2003, 12889,  2046,  1041,  4160,  1012,  1020,  1012,  2324,  1010,
         2013,  2029,  1037,  2047,  1156,  1011,  3643,  2003, 10174,  1012,
          102])"
777,1,"['probability of failure', 'probability', 'design', 'failure', 'sensitivity', 'design point', 'factors', 'random', 'normal', 'random variables', 'variables']", Nonlinear Limit State Functions,seg_219,"the iteration scheme will converge in a few (say normally 6–10) iterations and provides the design point u∗ as well as the reliability index β and the outward normal to the failure surface in the design point α. as already mentioned, the reliability index β may be related directly to the probability of failure. the components of the α-vector may be interpreted as sensitivity factors giving the relative importance of the individual random variables for the reliability index β .",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([27400,  5787,  2110,  4972])","tensor([  101,  1996, 27758,  5679,  2097, 28314,  1999,  1037,  2261,  1006,
         2360,  5373,  1020,  1516,  2184,  1007, 27758,  2015,  1998,  3640,
         1996,  2640,  2391,  1057, 30125,  2004,  2092,  2004,  1996, 15258,
         5950,  1156,  1998,  1996, 15436,  3671,  2000,  1996,  4945,  3302,
         1999,  1996,  2640,  2391,  1155,  1012,  2004,  2525,  3855,  1010,
         1996, 15258,  5950,  1156,  2089,  2022,  3141,  3495,  2000,  1996,
         9723,  1997,  4945,  1012,  1996,  6177,  1997,  1996,  1155,  1011,
         9207,  2089,  2022, 10009,  2004, 14639,  5876,  3228,  1996,  5816,
         5197,  1997,  1996,  3265,  6721, 10857,  2005,  1996, 15258,  5950,
         1156,  1012,   102])"
778,1,"['design', 'failure', 'design point', 'factor', 'random', 'random variables', 'correction factor', 'basic random variables', 'variables']", Nonlinear Limit State Functions,seg_219,"second order reliability methods (sorm) follow the same principles as form. however, as a logical extension of form, the failure surface is expanded to the second order in the design point. the result of a sorm analysis may be given as the form β multiplied with a correction factor evaluated on the basis of the second order partial derivatives of the failure surface in the design point. the sorm analysis becomes exact for failure surfaces given as a second order polynomial of the basic random variables. in general, however, the result of a sorm analysis can be shown to be asymptotically exact for any shape of the failure surface as β approaches infinity. the interested reader is referred to the literature for the details of sorm analysis, e.g. madsen et al. [9].",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([27400,  5787,  2110,  4972])","tensor([  101,  2117,  2344, 15258,  4725,  1006,  2061, 10867,  1007,  3582,
         1996,  2168,  6481,  2004,  2433,  1012,  2174,  1010,  2004,  1037,
        11177,  5331,  1997,  2433,  1010,  1996,  4945,  3302,  2003,  4423,
         2000,  1996,  2117,  2344,  1999,  1996,  2640,  2391,  1012,  1996,
         2765,  1997,  1037,  2061, 10867,  4106,  2089,  2022,  2445,  2004,
         1996,  2433,  1156, 28608,  2007,  1037, 18140,  5387, 16330,  2006,
         1996,  3978,  1997,  1996,  2117,  2344,  7704, 16942,  1997,  1996,
         4945,  3302,  1999,  1996,  2640,  2391,  1012,  1996,  2061, 10867,
         4106,  4150,  6635,  2005,  4945,  9972,  2445,  2004,  1037,  2117,
         2344, 17505,  1997,  1996,  3937,  6721, 10857,  1012,  1999,  2236,
         1010,  2174,  1010,  1996,  2765,  1997,  1037,  2061, 10867,  4106,
         2064,  2022,  3491,  2000,  2022,  2004, 24335, 13876, 20214,  3973,
         6635,  2005,  2151,  4338,  1997,  1996,  4945,  3302,  2004,  1156,
         8107, 15579,  1012,  1996,  4699,  8068,  2003,  3615,  2000,  1996,
         3906,  2005,  1996,  4751,  1997,  2061, 10867,  4106,  1010,  1041,
         1012,  1043,  1012,  5506,  5054,  3802,  2632,  1012,  1031,  1023,
         1033,  1012,   102])"
779,0,[], Example FORMNonlinear Limit State Function,seg_221,"consider again the steel rod from example 6.1. however, now it is assumed that the cross sectional area of the steel rod a is also uncertain.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2742, 2433, 8540, 4179, 2906, 5787, 2110, 3853])","tensor([  101,  5136,  2153,  1996,  3886,  8473,  2013,  2742,  1020,  1012,
         1015,  1012,  2174,  1010,  2085,  2009,  2003,  5071,  2008,  1996,
         2892, 27197,  2181,  1997,  1996,  3886,  8473,  1037,  2003,  2036,
         9662,  1012,   102])"
780,1,"['mean', 'deviation', 'normal', 'standard deviation', 'standard']", Example FORMNonlinear Limit State Function,seg_221,"the steel yield strength r is normal distributed with mean values and standard deviation μr = 350 mpa, σr = 3 mpa5 and the loading s is normal distributed with mean value and standard deviation μs = 1500 n, σs = 300 n. finally, the cross sectional area a is assumed normal distributed with mean value and standard deviation μa = 10 mm2 σa = 1 mm2.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 2433, 8540, 4179, 2906, 5787, 2110, 3853])","tensor([  101,  1996,  3886, 10750,  3997,  1054,  2003,  3671,  5500,  2007,
         2812,  5300,  1998,  3115, 24353,  1166,  2099,  1027,  8698,  6131,
         2050,  1010,  1173,  2099,  1027,  1017,  6131,  2050,  2629,  1998,
         1996, 10578,  1055,  2003,  3671,  5500,  2007,  2812,  3643,  1998,
         3115, 24353,  1166,  2015,  1027, 10347,  1050,  1010,  1173,  2015,
         1027,  3998,  1050,  1012,  2633,  1010,  1996,  2892, 27197,  2181,
         1037,  2003,  5071,  3671,  5500,  2007,  2812,  3643,  1998,  3115,
        24353,  1166,  2050,  1027,  2184,  3461,  2475,  1173,  2050,  1027,
         1015,  3461,  2475,  1012,   102])"
781,1,"['function', 'limit', 'limit state function']", Example FORMNonlinear Limit State Function,seg_221,the limit state function may be written as:,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([2742, 2433, 8540, 4179, 2906, 5787, 2110, 3853])","tensor([ 101, 1996, 5787, 2110, 3853, 2089, 2022, 2517, 2004, 1024,  102])"
782,1,"['transform', 'random', 'normal', 'random variables', 'standardized', 'variables']", Example FORMNonlinear Limit State Function,seg_221,"now the first step is to transform the normal distributed random variables r, a and s into standardized normal distributed random variables, i.e.:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 2433, 8540, 4179, 2906, 5787, 2110, 3853])","tensor([  101,  2085,  1996,  2034,  3357,  2003,  2000, 10938,  1996,  3671,
         5500,  6721, 10857,  1054,  1010,  1037,  1998,  1055,  2046, 16367,
         3671,  5500,  6721, 10857,  1010,  1045,  1012,  1041,  1012,  1024,
          102])"
783,1,"['function', 'limit state function', 'random', 'normal', 'random variables', 'limit', 'standardized', 'variables']", Example FORMNonlinear Limit State Function,seg_221,the limit state function may now be written in the space of the standardized normal distributed random variables as:,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,
        1., 0., 0., 0.])","tensor([2742, 2433, 8540, 4179, 2906, 5787, 2110, 3853])","tensor([  101,  1996,  5787,  2110,  3853,  2089,  2085,  2022,  2517,  1999,
         1996,  2686,  1997,  1996, 16367,  3671,  5500,  6721, 10857,  2004,
         1024,   102])"
784,0,[], Example FORMNonlinear Limit State Function,seg_221,= 350ur + 350ua − 300us + 35urua + 2000,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2742, 2433, 8540, 4179, 2906, 5787, 2110, 3853])","tensor([  101,  1027,  8698,  3126,  1009,  8698,  6692,  1597,  3998,  2271,
         1009,  3486, 14129,  2050,  1009,  2456,   102])"
785,1,"['design', 'design point']", Example FORMNonlinear Limit State Function,seg_221,the reliability index and the design point may be determined in accordance with eqs. 6.17 and 6.18 as (see also appendix b):,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 2433, 8540, 4179, 2906, 5787, 2110, 3853])","tensor([  101,  1996, 15258,  5950,  1998,  1996,  2640,  2391,  2089,  2022,
         4340,  1999, 10388,  2007,  1041,  4160,  2015,  1012,  1020,  1012,
         2459,  1998,  1020,  1012,  2324,  2004,  1006,  2156,  2036, 22524,
         1038,  1007,  1024,   102])"
786,1,['table'], Example FORMNonlinear Limit State Function,seg_221,which by calculation gives the iteration history shown in table 6.1.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([2742, 2433, 8540, 4179, 2906, 5787, 2110, 3853])","tensor([  101,  2029,  2011, 17208,  3957,  1996, 27758,  2381,  3491,  1999,
         2795,  1020,  1012,  1015,  1012,   102])"
787,1,"['table', 'random variable', 'random', 'variable']", Example FORMNonlinear Limit State Function,seg_221,from table 6.1 it is seen that the basic random variable s modeling the load on the steel rod is slightly dominating with an α-value equal to 0.6087. furthermore it,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([2742, 2433, 8540, 4179, 2906, 5787, 2110, 3853])","tensor([  101,  2013,  2795,  1020,  1012,  1015,  2009,  2003,  2464,  2008,
         1996,  3937,  6721,  8023,  1055, 11643,  1996,  7170,  2006,  1996,
         3886,  8473,  2003,  3621, 21949,  2007,  2019,  1155,  1011,  3643,
         5020,  2000,  1014,  1012,  3438,  2620,  2581,  1012,  7297,  2009,
          102])"
788,1,"['probability', 'variables', 'failure']", Example FORMNonlinear Limit State Function,seg_221,is seen that both the variables r and a are acting as resistance variables as their α- values are negative. the annual failure probability for the steel rod is determined as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 2433, 8540, 4179, 2906, 5787, 2110, 3853])","tensor([  101,  2003,  2464,  2008,  2119,  1996, 10857,  1054,  1998,  1037,
         2024,  3772,  2004,  5012, 10857,  2004,  2037,  1155,  1011,  5300,
         2024,  4997,  1012,  1996,  3296,  4945,  9723,  2005,  1996,  3886,
         8473,  2003,  4340,  2004,  1024,   102])"
789,1,['probability'], Simulation Methods,seg_223,consider again the probability integral defined in eq. 6.3:,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([12504,  4725])","tensor([ 101, 5136, 2153, 1996, 9723, 9897, 4225, 1999, 1041, 4160, 1012, 1020,
        1012, 1017, 1024,  102])"
790,1,"['cases', 'simulation', 'estimated', 'design', 'treatment', 'associated', 'failure', 'limit state function', 'function', 'probability', 'simulation techniques', 'limit']", Simulation Methods,seg_223,"it has been seen that form methods may successfully be applied for the evaluation of this integral. the integral may also be estimated by so-called simulation techniques. in the literature, a large variety of simulation techniques may be found and a treatment of these will not be given in the present text. here it is just noted that simulation techniques have proven their value especially for problems where the representation of the limit state function is associated with difficulties. such cases are e.g. when the limit state function is not differentiable or when several design points contribute to the failure probability.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 0., 0.])","tensor([12504,  4725])","tensor([  101,  2009,  2038,  2042,  2464,  2008,  2433,  4725,  2089,  5147,
         2022,  4162,  2005,  1996,  9312,  1997,  2023,  9897,  1012,  1996,
         9897,  2089,  2036,  2022,  4358,  2011,  2061,  1011,  2170, 12504,
         5461,  1012,  1999,  1996,  3906,  1010,  1037,  2312,  3528,  1997,
        12504,  5461,  2089,  2022,  2179,  1998,  1037,  3949,  1997,  2122,
         2097,  2025,  2022,  2445,  1999,  1996,  2556,  3793,  1012,  2182,
         2009,  2003,  2074,  3264,  2008, 12504,  5461,  2031, 10003,  2037,
         3643,  2926,  2005,  3471,  2073,  1996,  6630,  1997,  1996,  5787,
         2110,  3853,  2003,  3378,  2007,  8190,  1012,  2107,  3572,  2024,
         1041,  1012,  1043,  1012,  2043,  1996,  5787,  2110,  3853,  2003,
         2025,  2367, 19210,  2030,  2043,  2195,  2640,  2685,  9002,  2000,
         1996,  4945,  9723,  1012,   102])"
791,1,"['simulation', 'simulation techniques', 'method', 'simulation technique', 'monte carlo method']", Simulation Methods,seg_223,"however, as all simulation techniques have their origin in the so-called crude monte carlo method, the principles of this simulation technique will be briefly outlined below.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([12504,  4725])","tensor([  101,  2174,  1010,  2004,  2035, 12504,  5461,  2031,  2037,  4761,
         1999,  1996,  2061,  1011,  2170, 13587, 10125,  9758,  4118,  1010,
         1996,  6481,  1997,  2023, 12504,  6028,  2097,  2022,  4780, 14801,
         2917,  1012,   102])"
792,1,"['indicator function', 'function', 'probability', 'simulation', 'simulation techniques', 'indicator']", Simulation Methods,seg_223,the basis for simulation techniques is well illustrated by rewriting the probability integral in eq. 6.19 by means of an indicator function:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([12504,  4725])","tensor([  101,  1996,  3978,  2005, 12504,  5461,  2003,  2092,  7203,  2011,
         2128, 18560,  1996,  9723,  9897,  1999,  1041,  4160,  1012,  1020,
         1012,  2539,  2011,  2965,  1997,  2019, 17245,  3853,  1024,   102])"
793,1,"['indicator function', 'function', 'sample space', 'sample statistics', 'sample', 'statistics', 'expected value', 'indicator']", Simulation Methods,seg_223,"where the integration domain is changed from the part of the sample space of the vector x = (x1,x2, . . . ,xn)t for which g(x) ≤ 0 to the entire sample space of x and where i[g(x) ≤ 0] is an indicator function equal to 1 if g(x) ≤ 0 and otherwise equal to zero. equation 6.20 is in this way seen to yield the expected value of the indicator function i [g(x) ≤ 0]. therefore, if now n realizations of the vector x, i.e. xj , j = 1,2 . . . ,n are sampled it follows from sample statistics that:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0.])","tensor([12504,  4725])","tensor([  101,  2073,  1996,  8346,  5884,  2003,  2904,  2013,  1996,  2112,
         1997,  1996,  7099,  2686,  1997,  1996,  9207,  1060,  1027,  1006,
         1060,  2487,  1010,  1060,  2475,  1010,  1012,  1012,  1012,  1010,
         1060,  2078,  1007,  1056,  2005,  2029,  1043,  1006,  1060,  1007,
         1608,  1014,  2000,  1996,  2972,  7099,  2686,  1997,  1060,  1998,
         2073,  1045,  1031,  1043,  1006,  1060,  1007,  1608,  1014,  1033,
         2003,  2019, 17245,  3853,  5020,  2000,  1015,  2065,  1043,  1006,
         1060,  1007,  1608,  1014,  1998,  4728,  5020,  2000,  5717,  1012,
         8522,  1020,  1012,  2322,  2003,  1999,  2023,  2126,  2464,  2000,
        10750,  1996,  3517,  3643,  1997,  1996, 17245,  3853,  1045,  1031,
         1043,  1006,  1060,  1007,  1608,  1014,  1033,  1012,  3568,  1010,
         2065,  2085,  1050, 12393,  2015,  1997,  1996,  9207,  1060,  1010,
         1045,  1012,  1041,  1012,  1060,  3501,  1010,  1046,  1027,  1015,
         1010,  1016,  1012,  1012,  1012,  1010,  1050,  2024, 18925,  2009,
         4076,  2013,  7099,  6747,  2008,  1024,   102])"
794,1,"['probability', 'failure', 'unbiased', 'estimator', 'unbiased estimator']", Simulation Methods,seg_223,is an unbiased estimator of the failure probability pf .,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([12504,  4725])","tensor([  101,  2003,  2019,  4895, 11607,  6924,  9765,  9581,  4263,  1997,
         1996,  4945,  9723,  1052,  2546,  1012,   102])"
795,1,"['simulation', 'estimated', 'outcomes', 'simulated', 'failure', 'limit state function', 'simulations', 'case', 'function', 'probability', 'monte carlo simulation', 'random variables', 'simulation technique', 'basic random variables', 'random', 'limit', 'variables']", Simulation Methods,seg_223,"the principle of the crude monte carlo simulation technique rests directly on the application of eq. 6.21. a large number of realizations of the basic random variables x, i.e. xj , j = 1,2 . . . ,n are generated (or simulated) and for each of the outcomes xj it is checked whether or not the limit state function taken in xj is positive. all the simulations for which this is not the case are counted (nf ) and after n simulations the failure probability pf may be estimated through:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,
        0., 0., 0., 1., 0., 0., 0.])","tensor([12504,  4725])","tensor([  101,  1996,  6958,  1997,  1996, 13587, 10125,  9758, 12504,  6028,
        16626,  3495,  2006,  1996,  4646,  1997,  1041,  4160,  1012,  1020,
         1012,  2538,  1012,  1037,  2312,  2193,  1997, 12393,  2015,  1997,
         1996,  3937,  6721, 10857,  1060,  1010,  1045,  1012,  1041,  1012,
         1060,  3501,  1010,  1046,  1027,  1015,  1010,  1016,  1012,  1012,
         1012,  1010,  1050,  2024,  7013,  1006,  2030, 23599,  1007,  1998,
         2005,  2169,  1997,  1996, 13105,  1060,  3501,  2009,  2003,  7039,
         3251,  2030,  2025,  1996,  5787,  2110,  3853,  2579,  1999,  1060,
         3501,  2003,  3893,  1012,  2035,  1996, 24710,  2005,  2029,  2023,
         2003,  2025,  1996,  2553,  2024,  8897,  1006,  1050,  2546,  1007,
         1998,  2044,  1050, 24710,  1996,  4945,  9723,  1052,  2546,  2089,
         2022,  4358,  2083,  1024,   102])"
796,1,"['probability of failure', 'uncertainty', 'probability', 'variation', 'failure', 'estimate', 'coefficient', 'simulations', 'sample', 'expected value', 'coefficient of variation']", Simulation Methods,seg_223,"which then may be considered a sample expected value of the probability of failure. in fact, for n → ∞ the estimate of the failure probability becomes exact. however, simulations are often costly in computation time and the uncertainty of the estimate is thus of interest. it is easily realized that the coefficient of variation of the estimate is proportional to 1/√nf .",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([12504,  4725])","tensor([  101,  2029,  2059,  2089,  2022,  2641,  1037,  7099,  3517,  3643,
         1997,  1996,  9723,  1997,  4945,  1012,  1999,  2755,  1010,  2005,
         1050,  1585,  1601,  1996, 10197,  1997,  1996,  4945,  9723,  4150,
         6635,  1012,  2174,  1010, 24710,  2024,  2411, 17047,  1999, 22334,
         2051,  1998,  1996, 12503,  1997,  1996, 10197,  2003,  2947,  1997,
         3037,  1012,  2009,  2003,  4089,  3651,  2008,  1996, 19064,  1997,
         8386,  1997,  1996, 10197,  2003, 14267,  2000,  1015,  1013,  1600,
         2078,  2546,  1012,   102])"
797,1,"['coefficient', 'mean', 'uncertainties', 'lognormal', 'random variable', 'limit state function', 'normal', 'standard deviation', 'lognormal distributed', 'coefficient of variation', 'function', 'deviation', 'variation', 'standard', 'random', 'limit', 'variable']", Example  Monte Carlo Simulation,seg_225,"consider the limit state function g(x) ≤ 0 = b − k with k being the expected costs of a construction project, and b the budget. k is lognormal distributed with mean value μk = 50000 [chf]. due to the uncertainties in the performance of the project, the coefficient of variation of the costs is 0.2. the budget is modeled as a normal distributed random variable with mean value μb = 100000 [chf] and standard deviation σb = 5000 [chf].",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2742, 10125,  9758, 12504])","tensor([  101,  5136,  1996,  5787,  2110,  3853,  1043,  1006,  1060,  1007,
         1608,  1014,  1027,  1038,  1597,  1047,  2007,  1047,  2108,  1996,
         3517,  5366,  1997,  1037,  2810,  2622,  1010,  1998,  1038,  1996,
         5166,  1012,  1047,  2003,  8833, 12131,  9067,  5500,  2007,  2812,
         3643,  1166,  2243,  1027, 13509,  2692,  1031, 10381,  2546,  1033,
         1012,  2349,  2000,  1996,  9662,  7368,  1999,  1996,  2836,  1997,
         1996,  2622,  1010,  1996, 19064,  1997,  8386,  1997,  1996,  5366,
         2003,  1014,  1012,  1016,  1012,  1996,  5166,  2003, 14440,  2004,
         1037,  3671,  5500,  6721,  8023,  2007,  2812,  3643,  1166,  2497,
         1027,  6694,  8889,  1031, 10381,  2546,  1033,  1998,  3115, 24353,
         1173,  2497,  1027, 13509,  1031, 10381,  2546,  1033,  1012,   102])"
798,1,"['probability of failure', 'function', 'range', 'linear', 'probability', 'monte carlo simulation', 'simulation', 'failure', 'limit state function', 'simulations', 'limit']", Example  Monte Carlo Simulation,seg_225,"performing a monte carlo simulation with n = 100000, 19 simulations are found to lie outside of the positive range; in fig. 6.4 these simulations lie under the straight line, which represents the linear limit state function. the probability of failure is calculated as pf = n",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 2742, 10125,  9758, 12504])","tensor([  101,  4488,  1037, 10125,  9758, 12504,  2007,  1050,  1027,  6694,
         8889,  1010,  2539, 24710,  2024,  2179,  2000,  4682,  2648,  1997,
         1996,  3893,  2846,  1025,  1999, 20965,  1012,  1020,  1012,  1018,
         2122, 24710,  4682,  2104,  1996,  3442,  2240,  1010,  2029,  5836,
         1996,  7399,  5787,  2110,  3853,  1012,  1996,  9723,  1997,  4945,
         2003, 10174,  2004,  1052,  2546,  1027,  1050,   102])"
799,1,['vary'], Example  Monte Carlo Simulation,seg_225,nf = 10010900 = 1.9 · 10−4. this value will vary due to the rather,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 2742, 10125,  9758, 12504])","tensor([  101,  1050,  2546,  1027,  2531, 10790, 21057,  2692,  1027,  1015,
         1012,  1023,  1087,  2184, 22543,  2549,  1012,  2023,  3643,  2097,
         8137,  2349,  2000,  1996,  2738,   102])"
800,1,"['simulation', 'monte carlo simulation', 'simulations']", Example  Monte Carlo Simulation,seg_225,small number of simulations (105) in every new monte carlo simulation performed.,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.])","tensor([ 2742, 10125,  9758, 12504])","tensor([  101,  2235,  2193,  1997, 24710,  1006,  8746,  1007,  1999,  2296,
         2047, 10125,  9758, 12504,  2864,  1012,   102])"
801,1,"['variance reduction methods', 'variance', 'probability', 'monte carlo simulation', 'simulation', 'estimate', 'coefficient', 'simulations']", Example  Monte Carlo Simulation,seg_225,if monte carlo simulation is pursued to estimate a probability in the order of 10−6 it must be expected that approximately 108 simulations are necessary to achieve an estimate with a coefficient of variance in the order of 10%. a large number of simulations are thus required using crude monte carlo simulation and all refinements of this crude technique have the purpose of reducing the variance of the estimate. such methods are for this reason often referred to as variance reduction methods.,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0.])","tensor([ 2742, 10125,  9758, 12504])","tensor([  101,  2065, 10125,  9758, 12504,  2003,  9505,  2000, 10197,  1037,
         9723,  1999,  1996,  2344,  1997,  2184, 22543,  2575,  2009,  2442,
         2022,  3517,  2008,  3155, 10715, 24710,  2024,  4072,  2000,  6162,
         2019, 10197,  2007,  1037, 19064,  1997, 23284,  1999,  1996,  2344,
         1997,  2184,  1003,  1012,  1037,  2312,  2193,  1997, 24710,  2024,
         2947,  3223,  2478, 13587, 10125,  9758, 12504,  1998,  2035, 25416,
         3170,  8163,  1997,  2023, 13587,  6028,  2031,  1996,  3800,  1997,
         8161,  1996, 23284,  1997,  1996, 10197,  1012,  2107,  4725,  2024,
         2005,  2023,  3114,  2411,  3615,  2000,  2004, 23284,  7312,  4725,
         1012,   102])"
802,1,"['function', 'density function', 'simulation', 'outcomes', 'random', 'joint', 'independent']", Example  Monte Carlo Simulation,seg_225,the simulation of the n outcomes of the joint density function in eq. 6.21 is in principle simple and may be seen as consisting of two steps. here the steps will be illustrated assuming that the n components of the random vector x are independent.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.])","tensor([ 2742, 10125,  9758, 12504])","tensor([  101,  1996, 12504,  1997,  1996,  1050, 13105,  1997,  1996,  4101,
         4304,  3853,  1999,  1041,  4160,  1012,  1020,  1012,  2538,  2003,
         1999,  6958,  3722,  1998,  2089,  2022,  2464,  2004,  5398,  1997,
         2048,  4084,  1012,  2182,  1996,  4084,  2097,  2022,  7203, 10262,
         2008,  1996,  1050,  6177,  1997,  1996,  6721,  9207,  1060,  2024,
         2981,  1012,   102])"
803,1,"['uniform distribution', 'distribution']", Example  Monte Carlo Simulation,seg_225,"in the first step, a “pseudo random” number with a uniform distribution between 0 and 1 is generated for each of the components in xj i.e. xji , i = 1,2,3, . . . , n. the",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 10125,  9758, 12504])","tensor([  101,  1999,  1996,  2034,  3357,  1010,  1037,  1523, 18404,  6721,
         1524,  2193,  2007,  1037,  6375,  4353,  2090,  1014,  1998,  1015,
         2003,  7013,  2005,  2169,  1997,  1996,  6177,  1999,  1060,  3501,
         1045,  1012,  1041,  1012,  1060,  4478,  1010,  1045,  1027,  1015,
         1010,  1016,  1010,  1017,  1010,  1012,  1012,  1012,  1010,  1050,
         1012,  1996,   102])"
804,1,['functions'], Example  Monte Carlo Simulation,seg_225,generation of such numbers may be facilitated by built-in functions of basically all programming languages and spreadsheet software.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 10125,  9758, 12504])","tensor([  101,  4245,  1997,  2107,  3616,  2089,  2022, 19601,  2011,  2328,
         1011,  1999,  4972,  1997, 10468,  2035,  4730,  4155,  1998, 20861,
        21030,  2102,  4007,  1012,   102])"
805,1,"['outcomes', 'random” numbers']", Example  Monte Carlo Simulation,seg_225,"in the second step, the outcomes of the “pseudo random” numbers zji are transformed to outcomes of xji by:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 10125,  9758, 12504])","tensor([  101,  1999,  1996,  2117,  3357,  1010,  1996, 13105,  1997,  1996,
         1523, 18404,  6721,  1524,  3616,  1062,  4478,  2024,  8590,  2000,
        13105,  1997,  1060,  4478,  2011,  1024,   102])"
806,1,"['function', 'cumulative distribution function', 'random variable', 'random', 'distribution', 'distribution function', 'variable']", Example  Monte Carlo Simulation,seg_225,where fxi (·) is the cumulative distribution function for the random variable xi .,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        0.])","tensor([ 2742, 10125,  9758, 12504])","tensor([  101,  2073, 23292,  2072,  1006,  1087,  1007,  2003,  1996, 23260,
         4353,  3853,  2005,  1996,  6721,  8023,  8418,  1012,   102])"
807,1,['process'], Example  Monte Carlo Simulation,seg_225,the principle is also illustrated in fig. 6.5. this process is continued until all components of the vector xj have been generated.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742, 10125,  9758, 12504])","tensor([  101,  1996,  6958,  2003,  2036,  7203,  1999, 20965,  1012,  1020,
         1012,  1019,  1012,  2023,  2832,  2003,  2506,  2127,  2035,  6177,
         1997,  1996,  9207,  1060,  3501,  2031,  2042,  7013,  1012,   102])"
808,1,"['events', 'failure', 'random', 'failure events', 'random variables', 'basic random variables', 'variables']", Self Assessment QuestionsExercises,seg_227,1. how may failure events be represented in terms of basic random variables in the,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1015,  1012,  2129,  2089,  4945,  2824,  2022,  3421,  1999,
         3408,  1997,  3937,  6721, 10857,  1999,  1996,   102])"
809,0,[], Self Assessment QuestionsExercises,seg_227,context of structural reliability theory? 2. what is the geometrical interpretation of the reliability index and how does it,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  6123,  1997,  8332, 15258,  3399,  1029,  1016,  1012,  2054,
         2003,  1996, 14965,  2389,  7613,  1997,  1996, 15258,  5950,  1998,
         2129,  2515,  2009,   102])"
810,1,"['probability', 'monte carlo simulation', 'simulation', 'failure', 'sample', 'method', 'expected value']", Self Assessment QuestionsExercises,seg_227,"relate to the failure probability? 3. using the monte carlo simulation method, a sample expected value of the prob-",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        1., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101, 14396,  2000,  1996,  4945,  9723,  1029,  1017,  1012,  2478,
         1996, 10125,  9758, 12504,  4118,  1010,  1037,  7099,  3517,  3643,
         1997,  1996,  4013,  2497,  1011,   102])"
811,1,"['probability of failure', 'moment', 'estimation', 'probability', 'estimated', 'failure']", Self Assessment QuestionsExercises,seg_227,ability of failure is estimated. how may the accuracy in the estimation of the probability of failure be increased? 4. consider a timber beam subjected to an annual maximum bending moment l.,tensor(1),"tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  3754,  1997,  4945,  2003,  4358,  1012,  2129,  2089,  1996,
        10640,  1999,  1996, 24155,  1997,  1996,  9723,  1997,  4945,  2022,
         3445,  1029,  1018,  1012,  5136,  1037,  7227,  7504, 13532,  2000,
         2019,  3296,  4555, 14457,  2617,  1048,  1012,   102])"
812,1,"['mean', 'probability of failure', 'moment', 'deviation', 'probability', 'failure', 'random variable', 'random', 'normal', 'standard deviation', 'standard', 'variable', 'independent']", Self Assessment QuestionsExercises,seg_227,"the bending strength of the beam r is modeled by a normal distributed random variable with mean μr = 30 kn m and standard deviation σr = 5 kn m and the annual maximum bending moment is modeled by a normal distributed random variable with mean μl = 9 kn m and standard deviation σl = 2 kn m. it is assumed that r and l are independent. the timber beam fails when the applied moment exceeds the bending strength. calculate the reliability index β and the probability of failure of the timber beam. 5. consider a steel rod that carries a deterministic load, s = 35 kn m. the resis-",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1996, 14457,  3997,  1997,  1996,  7504,  1054,  2003, 14440,
         2011,  1037,  3671,  5500,  6721,  8023,  2007,  2812,  1166,  2099,
         1027,  2382, 14161,  1049,  1998,  3115, 24353,  1173,  2099,  1027,
         1019, 14161,  1049,  1998,  1996,  3296,  4555, 14457,  2617,  2003,
        14440,  2011,  1037,  3671,  5500,  6721,  8023,  2007,  2812,  1166,
         2140,  1027,  1023, 14161,  1049,  1998,  3115, 24353,  1173,  2140,
         1027,  1016, 14161,  1049,  1012,  2009,  2003,  5071,  2008,  1054,
         1998,  1048,  2024,  2981,  1012,  1996,  7227,  7504, 11896,  2043,
         1996,  4162,  2617, 23651,  1996, 14457,  3997,  1012, 18422,  1996,
        15258,  5950,  1156,  1998,  1996,  9723,  1997,  4945,  1997,  1996,
         7227,  7504,  1012,  1019,  1012,  5136,  1037,  3886,  8473,  2008,
         7883,  1037, 28283, 25300, 10074,  7170,  1010,  1055,  1027,  3486,
        14161,  1049,  1012,  1996, 24501,  2483,  1011,   102])"
813,1,['normal'], Self Assessment QuestionsExercises,seg_227,"tance, r, of the rod is given by the following product: r = a ·fy , where a is the area of the rod, equal to 100 mm2 and fy is the yield stress modeled as a normal",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  9092,  3401,  1010,  1054,  1010,  1997,  1996,  8473,  2003,
         2445,  2011,  1996,  2206,  4031,  1024,  1054,  1027,  1037,  1087,
         1042,  2100,  1010,  2073,  1037,  2003,  1996,  2181,  1997,  1996,
         8473,  1010,  5020,  2000,  2531,  3461,  2475,  1998,  1042,  2100,
         2003,  1996, 10750,  6911, 14440,  2004,  1037,  3671,   102])"
814,1,"['mean', 'function', 'density function', 'deviation', 'probability', 'probability density function', 'failure', 'estimate', 'random variable', 'random', 'standard', 'safety margin', 'variable']", Self Assessment QuestionsExercises,seg_227,"distributed random variable with mean μf = 425 · 10−3 kn/mm2 and standard y deviation σf = 25 · 10−3 kn/mm2. formulate a safety margin, m , for the steel y rod and estimate the reliability of the rod. draw the probability density function of the safety margin and indicate the safe and failure regions. 6. the position of a ship is measured by two fixed points a and b located at the",tensor(1),"tensor([0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  5500,  6721,  8023,  2007,  2812,  1166,  2546,  1027, 23285,
         1087,  2184, 22543,  2509, 14161,  1013,  3461,  2475,  1998,  3115,
         1061, 24353,  1173,  2546,  1027,  2423,  1087,  2184, 22543,  2509,
        14161,  1013,  3461,  2475,  1012,  5675,  2618,  1037,  3808,  7785,
         1010,  1049,  1010,  2005,  1996,  3886,  1061,  8473,  1998, 10197,
         1996, 15258,  1997,  1996,  8473,  1012,  4009,  1996,  9723,  4304,
         3853,  1997,  1996,  3808,  7785,  1998,  5769,  1996,  3647,  1998,
         4945,  4655,  1012,  1020,  1012,  1996,  2597,  1997,  1037,  2911,
         2003,  7594,  2011,  2048,  4964,  2685,  1037,  1998,  1038,  2284,
         2012,  1996,   102])"
815,1,"['information', 'error']", Self Assessment QuestionsExercises,seg_227,"coast, see fig. 6.6. angles α and β have been measured from the basis line ab at the same time. determine the error in b if the following information is provided:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  3023,  1010,  2156, 20965,  1012,  1020,  1012,  1020,  1012,
        12113,  1155,  1998,  1156,  2031,  2042,  7594,  2013,  1996,  3978,
         2240, 11113,  2012,  1996,  2168,  2051,  1012,  5646,  1996,  7561,
         1999,  1038,  2065,  1996,  2206,  2592,  2003,  3024,  1024,   102])"
816,1,"['mean', 'deviation', 'standard deviation', 'standard']", Self Assessment QuestionsExercises,seg_227,"where, for instance, c = 6 km±0.005 km means that the mean value of c is 6 km and the standard deviation of c is 0.005 km.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2073,  1010,  2005,  6013,  1010,  1039,  1027,  1020,  2463,
        29657,  2692,  1012,  4002,  2629,  2463,  2965,  2008,  1996,  2812,
         3643,  1997,  1039,  2003,  1020,  2463,  1998,  1996,  3115, 24353,
         1997,  1039,  2003,  1014,  1012,  4002,  2629,  2463,  1012,   102])"
817,1,"['consequences', 'risk', 'posterior', 'information', 'experiments']",Chapter  Bayesian Decision Analysis,seg_229,"lecture 13 (aim of the present lecture) the aim of the present lecture is to illustrate how the basic knowledge acquired through the present course provides a strong basis for engineering decision making. by establishing probabilistic engineering models that are consistent with the available knowledge, it is shown how risk or expected consequences can be utilized to identify and rank different engineering decision alternatives. to this end on the basis of a simple example, the three principally different types of decision analysis are introduced, namely the prior-, posteriorand the pre-posterior decision analysis. whereas the prior and the posterior decision analyses only differ in the available information at hand at the time of decision making and may serve as a direct basis for the planning of engineering activities involving changes of the state of nature, the pre-posterior analysis forms a strong basis for the planning of collection of information through e.g. experiments in the laboratory or in the field. on the basis of the lecture, it is expected that the reader should acquire knowledge and skills with regard to:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 3127,  3016, 25253,  3247,  4106])","tensor([  101,  8835,  2410,  1006,  6614,  1997,  1996,  2556,  8835,  1007,
         1996,  6614,  1997,  1996,  2556,  8835,  2003,  2000, 19141,  2129,
         1996,  3937,  3716,  3734,  2083,  1996,  2556,  2607,  3640,  1037,
         2844,  3978,  2005,  3330,  3247,  2437,  1012,  2011,  7411,  4013,
         3676, 27965,  4588,  3330,  4275,  2008,  2024,  8335,  2007,  1996,
         2800,  3716,  1010,  2009,  2003,  3491,  2129,  3891,  2030,  3517,
         8465,  2064,  2022, 12550,  2000,  6709,  1998,  4635,  2367,  3330,
         3247, 15955,  1012,  2000,  2023,  2203,  2006,  1996,  3978,  1997,
         1037,  3722,  2742,  1010,  1996,  2093, 16552,  2367,  4127,  1997,
         3247,  4106,  2024,  3107,  1010,  8419,  1996,  3188,  1011,  1010,
        15219,  5685,  1996,  3653,  1011, 15219,  3247,  4106,  1012,  6168,
         1996,  3188,  1998,  1996, 15219,  3247, 16478,  2069, 11234,  1999,
         1996,  2800,  2592,  2012,  2192,  2012,  1996,  2051,  1997,  3247,
         2437,  1998,  2089,  3710,  2004,  1037,  3622,  3978,  2005,  1996,
         4041,  1997,  3330,  3450,  5994,  3431,  1997,  1996,  2110,  1997,
         3267,  1010,  1996,  3653,  1011, 15219,  4106,  3596,  1037,  2844,
         3978,  2005,  1996,  4041,  1997,  3074,  1997,  2592,  2083,  1041,
         1012,  1043,  1012,  7885,  1999,  1996,  5911,  2030,  1999,  1996,
         2492,  1012,  2006,  1996,  3978,  1997,  1996,  8835,  1010,  2009,
         2003,  3517,  2008,  1996,  8068,  2323,  9878,  3716,  1998,  4813,
         2007,  7634,  2000,  1024,   102])"
818,1,"['function', 'consequences', 'posterior decision analysis', 'uncertainty', 'risk', 'prior and posterior decision analysis', 'posterior', 'utility', 'information', 'associated', 'probabilities', 'event', 'value of information']",Chapter  Bayesian Decision Analysis,seg_229,• what must be identified before a decision analysis can be performed? • what is a utility function and what role does it play in decision making? • how are risk and utility related? • how is a decision event tree constructed? • how can expected utility be calculated based on branching probabilities and consequences? • how can the uncertainty associated with information be accounted for in decision analysis? • what is the difference between prior and posterior decision analysis? • what is the idea behind the pre-posterior decision analysis? • how can the value of information be assessed?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 0.])","tensor([ 3127,  3016, 25253,  3247,  4106])","tensor([  101,  1528,  2054,  2442,  2022,  4453,  2077,  1037,  3247,  4106,
         2064,  2022,  2864,  1029,  1528,  2054,  2003,  1037,  9710,  3853,
         1998,  2054,  2535,  2515,  2009,  2377,  1999,  3247,  2437,  1029,
         1528,  2129,  2024,  3891,  1998,  9710,  3141,  1029,  1528,  2129,
         2003,  1037,  3247,  2724,  3392,  3833,  1029,  1528,  2129,  2064,
         3517,  9710,  2022, 10174,  2241,  2006, 23346,  4013,  3676, 14680,
         1998,  8465,  1029,  1528,  2129,  2064,  1996, 12503,  3378,  2007,
         2592,  2022, 14729,  2005,  1999,  3247,  4106,  1029,  1528,  2054,
         2003,  1996,  4489,  2090,  3188,  1998, 15219,  3247,  4106,  1029,
         1528,  2054,  2003,  1996,  2801,  2369,  1996,  3653,  1011, 15219,
         3247,  4106,  1029,  1528,  2129,  2064,  1996,  3643,  1997,  2592,
         2022, 14155,  1029,   102])"
819,1,['design'], Introduction,seg_231,"the ultimate task for an engineer is to establish a consistent decision basis for the planning, design, manufacturing, construction, operation and management of engineering facilities such that the overall life cycle benefit of the facilities is maximized and such that the given requirements to the safety of personnel and environment specified by legislation or society are fulfilled.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([4955]),"tensor([  101,  1996,  7209,  4708,  2005,  2019,  3992,  2003,  2000,  5323,
         1037,  8335,  3247,  3978,  2005,  1996,  4041,  1010,  2640,  1010,
         5814,  1010,  2810,  1010,  3169,  1998,  2968,  1997,  3330,  4128,
         2107,  2008,  1996,  3452,  2166,  5402,  5770,  1997,  1996,  4128,
         2003, 25845,  2094,  1998,  2107,  2008,  1996,  2445,  5918,  2000,
         1996,  3808,  1997,  5073,  1998,  4044,  9675,  2011,  6094,  2030,
         2554,  2024, 16829,  1012,   102])"
820,1,"['decision problem', 'processes', 'information']", Introduction,seg_231,"as the available information (regarding, e.g., soil properties, loading, material properties, future operational conditions and deterioration processes in general) is incomplete or uncertain, the decision problem is subject to uncertain information.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 0.])",tensor([4955]),"tensor([  101,  2004,  1996,  2800,  2592,  1006,  4953,  1010,  1041,  1012,
         1043,  1012,  1010,  5800,  5144,  1010, 10578,  1010,  3430,  5144,
         1010,  2925,  6515,  3785,  1998, 26118,  6194,  1999,  2236,  1007,
         2003, 12958,  2030,  9662,  1010,  1996,  3247,  3291,  2003,  3395,
         2000,  9662,  2592,  1012,   102])"
821,1,"['information', 'decision theory', 'risk', 'decision problem']", Introduction,seg_231,"the present chapter introduces some fundamental issues of decision making subject to uncertain information. general aspects of decision theory are considered and illustrated using a simple example. finally, the risk analysis decision problem is defined in general terms within the context of decision theory.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])",tensor([4955]),"tensor([  101,  1996,  2556,  3127, 13999,  2070,  8050,  3314,  1997,  3247,
         2437,  3395,  2000,  9662,  2592,  1012,  2236,  5919,  1997,  3247,
         3399,  2024,  2641,  1998,  7203,  2478,  1037,  3722,  2742,  1012,
         2633,  1010,  1996,  3891,  4106,  3247,  3291,  2003,  4225,  1999,
         2236,  3408,  2306,  1996,  6123,  1997,  3247,  3399,  1012,   102])"
822,1,"['decision problems', 'consequences']", The DecisionEvent Tree,seg_233,"in practical decision problems such as feasibility studies, reassessment of existing structures or decommissioning of facilities that have become obsolete, the number of alternative actions can be extremely large and a framework for the systematic analysis of the corresponding consequences is therefore expedient.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 1996,  3247, 18697,  3372,  3392])","tensor([  101,  1999,  6742,  3247,  3471,  2107,  2004, 24010,  2913,  1010,
         2128, 27241,  4757,  3672,  1997,  4493,  5090,  2030, 21933,  7382,
        14643,  3258,  2075,  1997,  4128,  2008,  2031,  2468, 15832,  1010,
         1996,  2193,  1997,  4522,  4506,  2064,  2022,  5186,  2312,  1998,
         1037,  7705,  2005,  1996, 11778,  4106,  1997,  1996,  7978,  8465,
         2003,  3568,  4654,  5669, 11638,  1012,   102])"
823,1,['decision problems'], The DecisionEvent Tree,seg_233,a decision/event tree as illustrated in fig. 7.1 may conveniently represent the decision problems.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0.])","tensor([ 1996,  3247, 18697,  3372,  3392])","tensor([  101,  1037,  3247,  1013,  2724,  3392,  2004,  7203,  1999, 20965,
         1012,  1021,  1012,  1015,  2089, 14057,  2135,  5050,  1996,  3247,
         3471,  1012,   102])"
824,1,"['location', 'decision problem']", The DecisionEvent Tree,seg_233,"for the purpose of illustration, the decision/event tree in fig. 7.1 considers the following very simple decision problem. in the specifications for the construction of a production facility using large amounts of fresh water, it is specified that a water source capable of producing at least 100 units of water per day must be available. as it is known that the underground at the location of the planned production facility actually contains a water reservoir, one option is to develop a well locally at the site of the production facility. however, as the capacity of the local water reservoir is not known with certainty another option is to get the water from an alternative location where a suitable well already exists.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  3247, 18697,  3372,  3392])","tensor([  101,  2005,  1996,  3800,  1997, 14614,  1010,  1996,  3247,  1013,
         2724,  3392,  1999, 20965,  1012,  1021,  1012,  1015, 10592,  1996,
         2206,  2200,  3722,  3247,  3291,  1012,  1999,  1996, 15480,  2005,
         1996,  2810,  1997,  1037,  2537,  4322,  2478,  2312,  8310,  1997,
         4840,  2300,  1010,  2009,  2003,  9675,  2008,  1037,  2300,  3120,
         5214,  1997,  5155,  2012,  2560,  2531,  3197,  1997,  2300,  2566,
         2154,  2442,  2022,  2800,  1012,  2004,  2009,  2003,  2124,  2008,
         1996,  5230,  2012,  1996,  3295,  1997,  1996,  3740,  2537,  4322,
         2941,  3397,  1037,  2300,  8071,  1010,  2028,  5724,  2003,  2000,
         4503,  1037,  2092,  7246,  2012,  1996,  2609,  1997,  1996,  2537,
         4322,  1012,  2174,  1010,  2004,  1996,  3977,  1997,  1996,  2334,
         2300,  8071,  2003,  2025,  2124,  2007, 15855,  2178,  5724,  2003,
         2000,  2131,  1996,  2300,  2013,  2019,  4522,  3295,  2073,  1037,
         7218,  2092,  2525,  6526,  1012,   102])"
825,1,"['consequences', 'associated']", The DecisionEvent Tree,seg_233,"the different options are associated with different costs and different potential consequences. the costs of establishing a well locally is assumed to be equal to 10 monetary units. if the already existing well is used, it is necessary to construct a pipeline. as the existing well is located far away from the planned production facility, the associated costs are assumed to be equal to 100 monetary units.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 1996,  3247, 18697,  3372,  3392])","tensor([  101,  1996,  2367,  7047,  2024,  3378,  2007,  2367,  5366,  1998,
         2367,  4022,  8465,  1012,  1996,  5366,  1997,  7411,  1037,  2092,
         7246,  2003,  5071,  2000,  2022,  5020,  2000,  2184, 12194,  3197,
         1012,  2065,  1996,  2525,  4493,  2092,  2003,  2109,  1010,  2009,
         2003,  4072,  2000,  9570,  1037, 13117,  1012,  2004,  1996,  4493,
         2092,  2003,  2284,  2521,  2185,  2013,  1996,  3740,  2537,  4322,
         1010,  1996,  3378,  5366,  2024,  5071,  2000,  2022,  5020,  2000,
         2531, 12194,  3197,  1012,   102])"
826,1,['probability'], The DecisionEvent Tree,seg_233,"based on experience from similar geological conditions, it is judged that the probability that a local well will be able to produce the required amount of water is 0.4. correspondingly, the probability that the well will not be able to fulfill the given requirements is 0.6.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 1996,  3247, 18697,  3372,  3392])","tensor([  101,  2241,  2006,  3325,  2013,  2714,  9843,  3785,  1010,  2009,
         2003, 13224,  2008,  1996,  9723,  2008,  1037,  2334,  2092,  2097,
         2022,  2583,  2000,  3965,  1996,  3223,  3815,  1997,  2300,  2003,
         1014,  1012,  1018,  1012,  7978,  2135,  1010,  1996,  9723,  2008,
         1996,  2092,  2097,  2025,  2022,  2583,  2000, 13883,  1996,  2445,
         5918,  2003,  1014,  1012,  1020,  1012,   102])"
827,0,[], The DecisionEvent Tree,seg_233,"the consequence of establishing locally a well, which turns out to be unable to produce the required amount of water, is that a pipeline to the existing—but",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 1996,  3247, 18697,  3372,  3392])","tensor([  101,  1996,  9509,  1997,  7411,  7246,  1037,  2092,  1010,  2029,
         4332,  2041,  2000,  2022,  4039,  2000,  3965,  1996,  3223,  3815,
         1997,  2300,  1010,  2003,  2008,  1037, 13117,  2000,  1996,  4493,
         1517,  2021,   102])"
828,1,['case'], The DecisionEvent Tree,seg_233,distant—well must be constructed. it is assumed that in this case all the water for the production facility is supplied from this well.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  3247, 18697,  3372,  3392])","tensor([ 101, 6802, 1517, 2092, 2442, 2022, 3833, 1012, 2009, 2003, 5071, 2008,
        1999, 2023, 2553, 2035, 1996, 2300, 2005, 1996, 2537, 4322, 2003, 8127,
        2013, 2023, 2092, 1012,  102])"
829,1,"['information', 'states', 'data', 'decision problem']", The DecisionEvent Tree,seg_233,"the task is now to analyze the decision problem making consistent use of all the information available to the engineer, including his/her degree of belief in the possible states, his/her subsequent observed data and his/her preferences among the various possible action/state pairs.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 1996,  3247, 18697,  3372,  3392])","tensor([  101,  1996,  4708,  2003,  2085,  2000, 17908,  1996,  3247,  3291,
         2437,  8335,  2224,  1997,  2035,  1996,  2592,  2800,  2000,  1996,
         3992,  1010,  2164,  2010,  1013,  2014,  3014,  1997,  6772,  1999,
         1996,  2825,  2163,  1010,  2010,  1013,  2014,  4745,  5159,  2951,
         1998,  2010,  1013,  2014, 18394,  2426,  1996,  2536,  2825,  2895,
         1013,  2110,  7689,  1012,   102])"
830,1,"['consequences', 'expected values']", The DecisionEvent Tree,seg_233,"to this end, use will be made of the fact that decisions shall be based on expected values of the corresponding consequences. this issue is addressed further in the following section.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  3247, 18697,  3372,  3392])","tensor([ 101, 2000, 2023, 2203, 1010, 2224, 2097, 2022, 2081, 1997, 1996, 2755,
        2008, 6567, 4618, 2022, 2241, 2006, 3517, 5300, 1997, 1996, 7978, 8465,
        1012, 2023, 3277, 2003, 8280, 2582, 1999, 1996, 2206, 2930, 1012,  102])"
831,1,['case'], Decisions Based on Expected Values,seg_235,"consider the simple case where the engineer must choose between actions a1 and a2. the consequence of action a2 is c with certainty whereas the consequence of action a1 is uncertain. the state of nature may be θ1, in which case the consequence is a and the state of nature may be θ2 in which case the consequence is b . the decision/event tree is illustrated in fig. 7.2.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6567, 2241, 2006, 3517, 5300])","tensor([  101,  5136,  1996,  3722,  2553,  2073,  1996,  3992,  2442,  5454,
         2090,  4506, 17350,  1998, 22441,  1012,  1996,  9509,  1997,  2895,
        22441,  2003,  1039,  2007, 15855,  6168,  1996,  9509,  1997,  2895,
        17350,  2003,  9662,  1012,  1996,  2110,  1997,  3267,  2089,  2022,
         1162,  2487,  1010,  1999,  2029,  2553,  1996,  9509,  2003,  1037,
         1998,  1996,  2110,  1997,  3267,  2089,  2022,  1162,  2475,  1999,
         2029,  2553,  1996,  9509,  2003,  1038,  1012,  1996,  3247,  1013,
         2724,  3392,  2003,  7203,  1999, 20965,  1012,  1021,  1012,  1016,
         1012,   102])"
832,1,"['states', 'likelihood', 'consequences']", Decisions Based on Expected Values,seg_235,"before the true state of nature is known, the optimal decision depends upon the likelihood of the various states of the nature θ and the consequences a, b and c.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([6567, 2241, 2006, 3517, 5300])","tensor([  101,  2077,  1996,  2995,  2110,  1997,  3267,  2003,  2124,  1010,
         1996, 15502,  3247,  9041,  2588,  1996, 16593,  1997,  1996,  2536,
         2163,  1997,  1996,  3267,  1162,  1998,  1996,  8465,  1037,  1010,
         1038,  1998,  1039,  1012,   102])"
833,1,"['function', 'numerical', 'decision problem']", Decisions Based on Expected Values,seg_235,"a further analysis of the decision problem requires the numerical assessment of the preferences of the decision maker. it is assumed that the decision maker prefers b to a, c to a, and b to c. this statement of preferences may be expressed by any function u such that:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0.])","tensor([6567, 2241, 2006, 3517, 5300])","tensor([  101,  1037,  2582,  4106,  1997,  1996,  3247,  3291,  5942,  1996,
        15973,  7667,  1997,  1996, 18394,  1997,  1996,  3247,  9338,  1012,
         2009,  2003,  5071,  2008,  1996,  3247,  9338, 19233,  1038,  2000,
         1037,  1010,  1039,  2000,  1037,  1010,  1998,  1038,  2000,  1039,
         1012,  2023,  4861,  1997, 18394,  2089,  2022,  5228,  2011,  2151,
         3853,  1057,  2107,  2008,  1024,   102])"
834,1,"['utility', 'function']", Decisions Based on Expected Values,seg_235,"the task is to find a particular function u, namely the utility function, such that it is logically consistent to decide between a1 and a2 by comparing u(c) with the",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([6567, 2241, 2006, 3517, 5300])","tensor([  101,  1996,  4708,  2003,  2000,  2424,  1037,  3327,  3853,  1057,
         1010,  8419,  1996,  9710,  3853,  1010,  2107,  2008,  2009,  2003,
        11177,  2135,  8335,  2000,  5630,  2090, 17350,  1998, 22441,  2011,
        13599,  1057,  1006,  1039,  1007,  2007,  1996,   102])"
835,1,['utility'], Decisions Based on Expected Values,seg_235,"expected value of the utility of the action a1, namely:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6567, 2241, 2006, 3517, 5300])","tensor([  101,  3517,  3643,  1997,  1996,  9710,  1997,  1996,  2895, 17350,
         1010,  8419,  1024,   102])"
836,1,['probability'], Decisions Based on Expected Values,seg_235,where p is the probability that the state of nature is θ1.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6567, 2241, 2006, 3517, 5300])","tensor([ 101, 2073, 1052, 2003, 1996, 9723, 2008, 1996, 2110, 1997, 3267, 2003,
        1162, 2487, 1012,  102])"
837,1,"['probability', 'expected value']", Decisions Based on Expected Values,seg_235,"assuming that u(a) and u(b) have been given appropriate values, the question is—what value should u(c) have in order to make the expected value a valid decision criterion? if the probability of θ1 being the state of nature p is equal to 0, the decision maker would choose a1 over a2 because she/he prefers b to c. on the other hand, if the probability of θ1 being the state of nature is equal to 1 she/he would choose a2 over a1. for a value of p somewhere between 0 and 1, the decision maker will be indifferent to choosing a1 over a2. this value p∗ may be determined and u(c) is assigned as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([6567, 2241, 2006, 3517, 5300])","tensor([  101, 10262,  2008,  1057,  1006,  1037,  1007,  1998,  1057,  1006,
         1038,  1007,  2031,  2042,  2445,  6413,  5300,  1010,  1996,  3160,
         2003,  1517,  2054,  3643,  2323,  1057,  1006,  1039,  1007,  2031,
         1999,  2344,  2000,  2191,  1996,  3517,  3643,  1037,  9398,  3247,
        19229,  1029,  2065,  1996,  9723,  1997,  1162,  2487,  2108,  1996,
         2110,  1997,  3267,  1052,  2003,  5020,  2000,  1014,  1010,  1996,
         3247,  9338,  2052,  5454, 17350,  2058, 22441,  2138,  2016,  1013,
         2002, 19233,  1038,  2000,  1039,  1012,  2006,  1996,  2060,  2192,
         1010,  2065,  1996,  9723,  1997,  1162,  2487,  2108,  1996,  2110,
         1997,  3267,  2003,  5020,  2000,  1015,  2016,  1013,  2002,  2052,
         5454, 22441,  2058, 17350,  1012,  2005,  1037,  3643,  1997,  1052,
         4873,  2090,  1014,  1998,  1015,  1010,  1996,  3247,  9338,  2097,
         2022, 24436,  2000, 10549, 17350,  2058, 22441,  1012,  2023,  3643,
         1052, 30125,  2089,  2022,  4340,  1998,  1057,  1006,  1039,  1007,
         2003,  4137,  2004,  1024,   102])"
838,1,"['utility', 'function']", Decisions Based on Expected Values,seg_235,"from eq. 7.3 it is seen that u(c) will lie between u(a) and u(b) for all choices of p∗ and therefore the utility function is consistent with the stated preferences. furthermore, it is seen that the decision maker should choose the action a1 to a2 only if the expected utility given this action e[u|a1] is greater than e[u|a2]. this is realized by noting that for all p greater than p∗ and with u(c) given by eq. 7.3. there is:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6567, 2241, 2006, 3517, 5300])","tensor([  101,  2013,  1041,  4160,  1012,  1021,  1012,  1017,  2009,  2003,
         2464,  2008,  1057,  1006,  1039,  1007,  2097,  4682,  2090,  1057,
         1006,  1037,  1007,  1998,  1057,  1006,  1038,  1007,  2005,  2035,
         9804,  1997,  1052, 30125,  1998,  3568,  1996,  9710,  3853,  2003,
         8335,  2007,  1996,  3090, 18394,  1012,  7297,  1010,  2009,  2003,
         2464,  2008,  1996,  3247,  9338,  2323,  5454,  1996,  2895, 17350,
         2000, 22441,  2069,  2065,  1996,  3517,  9710,  2445,  2023,  2895,
         1041,  1031,  1057,  1064, 17350,  1033,  2003,  3618,  2084,  1041,
         1031,  1057,  1064, 22441,  1033,  1012,  2023,  2003,  3651,  2011,
         9073,  2008,  2005,  2035,  1052,  3618,  2084,  1052, 30125,  1998,
         2007,  1057,  1006,  1039,  1007,  2445,  2011,  1041,  4160,  1012,
         1021,  1012,  1017,  1012,  2045,  2003,  1024,   102])"
839,1,"['probability', 'utility', 'consistency', 'expected values']", Decisions Based on Expected Values,seg_235,"this means that if u(c) is properly assigned in consistency with the decision makers stated preferences (i.e. b preferred to c preferred to a) and the indifference probability p∗, the ranking of the expected values of the utility determines the ranking of actions.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([6567, 2241, 2006, 3517, 5300])","tensor([  101,  2023,  2965,  2008,  2065,  1057,  1006,  1039,  1007,  2003,
         7919,  4137,  1999, 18700,  2007,  1996,  3247, 11153,  3090, 18394,
         1006,  1045,  1012,  1041,  1012,  1038,  6871,  2000,  1039,  6871,
         2000,  1037,  1007,  1998,  1996, 25920,  9723,  1052, 30125,  1010,
         1996,  5464,  1997,  1996,  3517,  5300,  1997,  1996,  9710, 16463,
         1996,  5464,  1997,  4506,  1012,   102])"
840,1,"['probability', 'utility', 'numerical', 'decision problem']", Decision Making Subject to Uncertainty,seg_237,"having formulated the decision problem in terms of a decision/event tree, with proper assignment of utility and probability structure, the numerical evaluation of decision alternatives may be performed.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  2437,  3395,  2000, 12503])","tensor([  101,  2383, 19788,  1996,  3247,  3291,  1999,  3408,  1997,  1037,
         3247,  1013,  2724,  3392,  1010,  2007,  5372,  8775,  1997,  9710,
         1998,  9723,  3252,  1010,  1996, 15973,  9312,  1997,  3247, 15955,
         2089,  2022,  2864,  1012,   102])"
841,1,"['posterior', 'information', 'posterior analysis', 'prior analysis']", Decision Making Subject to Uncertainty,seg_237,"depending on the state of information at the time of the decision analysis, three different analysis types are distinguished, namely prior analysis, posterior analysis and pre-posterior analysis. each of these are important in practical applications of decision analysis and are therefore discussed briefly in the following.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 3247,  2437,  3395,  2000, 12503])","tensor([  101,  5834,  2006,  1996,  2110,  1997,  2592,  2012,  1996,  2051,
         1997,  1996,  3247,  4106,  1010,  2093,  2367,  4106,  4127,  2024,
         5182,  1010,  8419,  3188,  4106,  1010, 15219,  4106,  1998,  3653,
         1011, 15219,  4106,  1012,  2169,  1997,  2122,  2024,  2590,  1999,
         6742,  5097,  1997,  3247,  4106,  1998,  2024,  3568,  6936,  4780,
         1999,  1996,  2206,  1012,   102])"
842,1,"['function', 'consequences', 'estimated', 'utility', 'probabilities']", Decision Analysis with Given InformationPrior Analysis,seg_239,"when the utility function has been defined and the probabilities of the various state of nature corresponding to different consequences have been estimated, the analysis is reduced to the calculation of the expected utilities corresponding to the different action alternatives. in the following examples the utility is represented in a simplified manner through the costs whereby the optimal decisions now should be identified as the decisions minimizing expected costs, which then is equivalent to maximizing expected utility.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 3247,  4106,  2007,  2445,  2592, 18098, 25346,  4106])","tensor([  101,  2043,  1996,  9710,  3853,  2038,  2042,  4225,  1998,  1996,
         4013,  3676, 14680,  1997,  1996,  2536,  2110,  1997,  3267,  7978,
         2000,  2367,  8465,  2031,  2042,  4358,  1010,  1996,  4106,  2003,
         4359,  2000,  1996, 17208,  1997,  1996,  3517, 16548,  7978,  2000,
         1996,  2367,  2895, 15955,  1012,  1999,  1996,  2206,  4973,  1996,
         9710,  2003,  3421,  1999,  1037, 11038,  5450,  2083,  1996,  5366,
        13557,  1996, 15502,  6567,  2085,  2323,  2022,  4453,  2004,  1996,
         6567,  7163,  4328,  6774,  3517,  5366,  1010,  2029,  2059,  2003,
         5662,  2000, 20446,  6026,  3517,  9710,  1012,   102])"
843,1,['probabilistic'], Decision Analysis with Given InformationPrior Analysis,seg_239,"at this stage, the probabilistic description p [θ ] of the state of nature θ is usually called a prior description and called p ′[θ ].",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  2445,  2592, 18098, 25346,  4106])","tensor([  101,  2012,  2023,  2754,  1010,  1996,  4013,  3676, 27965,  4588,
         6412,  1052,  1031,  1162,  1033,  1997,  1996,  2110,  1997,  3267,
         1162,  2003,  2788,  2170,  1037,  3188,  6412,  1998,  2170,  1052,
         1531,  1031,  1162,  1033,  1012,   102])"
844,1,"['prior decision analysis', 'decision problem']", Decision Analysis with Given InformationPrior Analysis,seg_239,to illustrate the prior decision analysis the decision problem from sect. 7.2 is considered again. the decision problem is stated as follows. the decision maker has a choice between two actions:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 3247,  4106,  2007,  2445,  2592, 18098, 25346,  4106])","tensor([  101,  2000, 19141,  1996,  3188,  3247,  4106,  1996,  3247,  3291,
         2013, 17831,  1012,  1021,  1012,  1016,  2003,  2641,  2153,  1012,
         1996,  3247,  3291,  2003,  3090,  2004,  4076,  1012,  1996,  3247,
         9338,  2038,  1037,  3601,  2090,  2048,  4506,  1024,   102])"
845,0,[], Decision Analysis with Given InformationPrior Analysis,seg_239,a1: establish a new well a2: establish a pipeline from an existing well,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 3247,  4106,  2007,  2445,  2592, 18098, 25346,  4106])","tensor([  101, 17350,  1024,  5323,  1037,  2047,  2092, 22441,  1024,  5323,
         1037, 13117,  2013,  2019,  4493,  2092,   102])"
846,1,['states'], Decision Analysis with Given InformationPrior Analysis,seg_239,the possible states of nature are the following two:,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  2445,  2592, 18098, 25346,  4106])","tensor([ 101, 1996, 2825, 2163, 1997, 3267, 2024, 1996, 2206, 2048, 1024,  102])"
847,0,[], Decision Analysis with Given InformationPrior Analysis,seg_239,θ1: capacity insufficient θ2: capacity sufficient,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 3247,  4106,  2007,  2445,  2592, 18098, 25346,  4106])","tensor([  101,  1162,  2487,  1024,  3977, 13990,  1162,  2475,  1024,  3977,
         7182,   102])"
848,1,"['prior probabilities', 'probabilities']", Decision Analysis with Given InformationPrior Analysis,seg_239,the prior probabilities are:,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  2445,  2592, 18098, 25346,  4106])","tensor([  101,  1996,  3188,  4013,  3676, 14680,  2024,  1024,   102])"
849,1,['information'], Decision Analysis with Given InformationPrior Analysis,seg_239,"based on the prior information alone, it is easily seen that the expected cost e′[c] amounts to:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  2445,  2592, 18098, 25346,  4106])","tensor([ 101, 2241, 2006, 1996, 3188, 2592, 2894, 1010, 2009, 2003, 4089, 2464,
        2008, 1996, 3517, 3465, 1041, 1531, 1031, 1039, 1033, 8310, 2000, 1024,
         102])"
850,0,[], Decision Analysis with Given InformationPrior Analysis,seg_239,the decision/event tree is illustrated in fig. 7.3 together with the expected costs (in boxes).,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0])","tensor([ 3247,  4106,  2007,  2445,  2592, 18098, 25346,  4106])","tensor([  101,  1996,  3247,  1013,  2724,  3392,  2003,  7203,  1999, 20965,
         1012,  1021,  1012,  1017,  2362,  2007,  1996,  3517,  5366,  1006,
         1999,  8378,  1007,  1012,   102])"
851,1,['utility'], Decision Analysis with Given InformationPrior Analysis,seg_239,it is seen that action alternative a1 yields the smallest expense (largest expected utility) so this action alternative is the optimal decision.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  2445,  2592, 18098, 25346,  4106])","tensor([  101,  2009,  2003,  2464,  2008,  2895,  4522, 17350, 16189,  1996,
        10479, 10961,  1006,  2922,  3517,  9710,  1007,  2061,  2023,  2895,
         4522,  2003,  1996, 15502,  3247,  1012,   102])"
852,1,"['probability', 'information']", Decision Analysis with Additional InformationPosterior Analysis,seg_241,"when additional information becomes available, the probability structure in the decision problem may be updated. having updated the probability structure the decision analysis is unchanged in comparison to the situation with given—prior information.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([  101,  2043,  3176,  2592,  4150,  2800,  1010,  1996,  9723,  3252,
         1999,  1996,  3247,  3291,  2089,  2022,  7172,  1012,  2383,  7172,
         1996,  9723,  3252,  1996,  3247,  4106,  2003, 15704,  1999,  7831,
         2000,  1996,  3663,  2007,  2445,  1517,  3188,  2592,  1012,   102])"
853,1,"['posterior', 'probability', 'experiment', 'posterior probability']", Decision Analysis with Additional InformationPosterior Analysis,seg_241,"given the result of an experiment zk , the updated probability structure (or just the posterior probability) is called p ′′[θ ] and may be evaluated by use of the bayes’ rule:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([  101,  2445,  1996,  2765,  1997,  2019,  7551,  1062,  2243,  1010,
         1996,  7172,  9723,  3252,  1006,  2030,  2074,  1996, 15219,  9723,
         1007,  2003,  2170,  1052,  1531,  1531,  1031,  1162,  1033,  1998,
         2089,  2022, 16330,  2011,  2224,  1997,  1996,  3016,  2229,  1521,
         3627,  1024,   102])"
854,0,[], Decision Analysis with Additional InformationPosterior Analysis,seg_241,which may be explained as:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([ 101, 2029, 2089, 2022, 4541, 2004, 1024,  102])"
855,1,"['observation', 'sample likelihood', 'probability', 'information', 'prior probability', 'sample', 'factor', 'likelihood']", Decision Analysis with Additional InformationPosterior Analysis,seg_241,the normalizing factor is to ensure that p ′′[θi] forms a proper probability. the mixing of new and old information appears through the sample likelihood p [zk| θi] and the prior probability p ′[θi]. the likelihood is the probability of obtaining the observation zk given the true state of nature θi .,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([  101,  1996,  3671,  6026,  5387,  2003,  2000,  5676,  2008,  1052,
         1531,  1531,  1031,  1162,  2072,  1033,  3596,  1037,  5372,  9723,
         1012,  1996,  6809,  1997,  2047,  1998,  2214,  2592,  3544,  2083,
         1996,  7099, 16593,  1052,  1031,  1062,  2243,  1064,  1162,  2072,
         1033,  1998,  1996,  3188,  9723,  1052,  1531,  1031,  1162,  2072,
         1033,  1012,  1996, 16593,  2003,  1996,  9723,  1997, 11381,  1996,
         8089,  1062,  2243,  2445,  1996,  2995,  2110,  1997,  3267,  1162,
         2072,  1012,   102])"
856,1,"['posterior', 'functions', 'information', 'sample', 'sample size', 'density functions', 'likelihood', 'case']", Decision Analysis with Additional InformationPosterior Analysis,seg_241,"in fig. 7.4 an illustration is given of corresponding prior and posterior probability density functions together with likelihood functions. in the first case, the prior information is strong and the likelihood is weak (e.g. due to a small sample size). in the second case, the prior information and the likelihood are of comparable strength. in the last case, the prior information is relatively weak in comparison to the likelihood.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([  101,  1999, 20965,  1012,  1021,  1012,  1018,  2019, 14614,  2003,
         2445,  1997,  7978,  3188,  1998, 15219,  9723,  4304,  4972,  2362,
         2007, 16593,  4972,  1012,  1999,  1996,  2034,  2553,  1010,  1996,
         3188,  2592,  2003,  2844,  1998,  1996, 16593,  2003,  5410,  1006,
         1041,  1012,  1043,  1012,  2349,  2000,  1037,  2235,  7099,  2946,
         1007,  1012,  1999,  1996,  2117,  2553,  1010,  1996,  3188,  2592,
         1998,  1996, 16593,  2024,  1997, 12435,  3997,  1012,  1999,  1996,
         2197,  2553,  1010,  1996,  3188,  2592,  2003,  4659,  5410,  1999,
         7831,  2000,  1996, 16593,  1012,   102])"
857,1,"['posterior decision analysis', 'posterior', 'decision problem']", Decision Analysis with Additional InformationPosterior Analysis,seg_241,"to illustrate the posterior decision analysis, the water supply decision problem is considered again.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([  101,  2000, 19141,  1996, 15219,  3247,  4106,  1010,  1996,  2300,
         4425,  3247,  3291,  2003,  2641,  2153,  1012,   102])"
858,1,"['estimated', 'information', 'test']", Decision Analysis with Additional InformationPosterior Analysis,seg_241,"it is assumed that information about the capacity of the local reservoir can be estimated by the implementation of a less expensive test well and a subsequent pump test. it is assumed that the cost of establishing a test well is equal to 1 monetary unit. however, the information obtained from the pump test is only indicative as the result of the difference in scale from the test well to the planned local well.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([  101,  2009,  2003,  5071,  2008,  2592,  2055,  1996,  3977,  1997,
         1996,  2334,  8071,  2064,  2022,  4358,  2011,  1996,  7375,  1997,
         1037,  2625,  6450,  3231,  2092,  1998,  1037,  4745, 10216,  3231,
         1012,  2009,  2003,  5071,  2008,  1996,  3465,  1997,  7411,  1037,
         3231,  2092,  2003,  5020,  2000,  1015, 12194,  3131,  1012,  2174,
         1010,  1996,  2592,  4663,  2013,  1996, 10216,  3231,  2003,  2069,
        24668,  2004,  1996,  2765,  1997,  1996,  4489,  1999,  4094,  2013,
         1996,  3231,  2092,  2000,  1996,  3740,  2334,  2092,  1012,   102])"
859,1,['test'], Decision Analysis with Additional InformationPosterior Analysis,seg_241,it is further assumed that the pump test can provide the following different information—i.e. indicators regarding the capacity of the local reservoir.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([  101,  2009,  2003,  2582,  5071,  2008,  1996, 10216,  3231,  2064,
         3073,  1996,  2206,  2367,  2592,  1517,  1045,  1012,  1041,  1012,
        20390,  4953,  1996,  3977,  1997,  1996,  2334,  8071,  1012,   102])"
860,0,[], Decision Analysis with Additional InformationPosterior Analysis,seg_241,the capacity of the reservoir is:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([ 101, 1996, 3977, 1997, 1996, 8071, 2003, 1024,  102])"
861,0,[], Decision Analysis with Additional InformationPosterior Analysis,seg_241,"• larger than the given production requirements by 5% i.e. larger than 105 water volume units per day. • less than 95% of the required water production, i.e. less than 95 water volume units. • between 95 and 105 water units.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([ 101, 1528, 3469, 2084, 1996, 2445, 2537, 5918, 2011, 1019, 1003, 1045,
        1012, 1041, 1012, 3469, 2084, 8746, 2300, 3872, 3197, 2566, 2154, 1012,
        1528, 2625, 2084, 5345, 1003, 1997, 1996, 3223, 2300, 2537, 1010, 1045,
        1012, 1041, 1012, 2625, 2084, 5345, 2300, 3872, 3197, 1012, 1528, 2090,
        5345, 1998, 8746, 2300, 3197, 1012,  102])"
862,1,"['uncertainty', 'table', 'information', 'likelihood', 'test']", Decision Analysis with Additional InformationPosterior Analysis,seg_241,the information from the pump test is subject to uncertainty and the likelihood of the actual capacity of the local reservoir given the three different indications described above are given in table 7.1.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0.])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([  101,  1996,  2592,  2013,  1996, 10216,  3231,  2003,  3395,  2000,
        12503,  1998,  1996, 16593,  1997,  1996,  5025,  3977,  1997,  1996,
         2334,  8071,  2445,  1996,  2093,  2367, 24936,  2649,  2682,  2024,
         2445,  1999,  2795,  1021,  1012,  1015,  1012,   102])"
863,1,"['trial', 'posterior', 'test']", Decision Analysis with Additional InformationPosterior Analysis,seg_241,"given that a test well is established and a trial pump test conducted with the result that a capacity smaller than 95 water volume units is indicated, a posterior",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([  101,  2445,  2008,  1037,  3231,  2092,  2003,  2511,  1998,  1037,
         3979, 10216,  3231,  4146,  2007,  1996,  2765,  2008,  1037,  3977,
         3760,  2084,  5345,  2300,  3872,  3197,  2003,  5393,  1010,  1037,
        15219,   102])"
864,0,[], Decision Analysis with Additional InformationPosterior Analysis,seg_241,decision analysis can be performed to identify whether the optimal decision is to establish a well locally or if it is more optimal to construct a pipeline to the existing well.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([  101,  3247,  4106,  2064,  2022,  2864,  2000,  6709,  3251,  1996,
        15502,  3247,  2003,  2000,  5323,  1037,  2092,  7246,  2030,  2065,
         2009,  2003,  2062, 15502,  2000,  9570,  1037, 13117,  2000,  1996,
         4493,  2092,  1012,   102])"
865,1,"['information', 'posterior probabilities', 'probabilities', 'posterior']", Decision Analysis with Additional InformationPosterior Analysis,seg_241,"therefore, the posterior probabilities given the new information p ′′[θ | z] can be given as:",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([  101,  3568,  1010,  1996, 15219,  4013,  3676, 14680,  2445,  1996,
         2047,  2592,  1052,  1531,  1531,  1031,  1162,  1064,  1062,  1033,
         2064,  2022,  2445,  2004,  1024,   102])"
866,1,"['posterior', 'utility', 'probabilities', 'expected values']", Decision Analysis with Additional InformationPosterior Analysis,seg_241,which are also shown in fig. 7.5. having determined the updated probabilities the posterior expected values e′′[c| i2] of the utility corresponding to the optimal action alternative is readily obtained as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([  101,  2029,  2024,  2036,  3491,  1999, 20965,  1012,  1021,  1012,
         1019,  1012,  2383,  4340,  1996,  7172,  4013,  3676, 14680,  1996,
        15219,  3517,  5300,  1041,  1531,  1531,  1031,  1039,  1064,  1045,
         2475,  1033,  1997,  1996,  9710,  7978,  2000,  1996, 15502,  2895,
         4522,  2003, 12192,  4663,  2004,  1024,   102])"
867,1,"['information', 'prior decision analysis']", Decision Analysis with Additional InformationPosterior Analysis,seg_241,"considering the additional information, it is seen that the optimal decision has been switched to action a2 as compared to the prior decision analysis.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 3247,  4106,  2007,  3176,  2592, 19894, 11124,  2953,  4106])","tensor([  101,  6195,  1996,  3176,  2592,  1010,  2009,  2003,  2464,  2008,
         1996, 15502,  3247,  2038,  2042,  7237,  2000,  2895, 22441,  2004,
         4102,  2000,  1996,  3188,  3247,  4106,  1012,   102])"
868,1,"['utility', 'information', 'experiment', 'expected value']", Decision Analysis with Unknown InformationPreposterior Analysis,seg_243,"often the decision maker has the possibility to ‘buy’ additional information through an experiment before actually making his/her choice of action. if the cost of this information is small in comparison to the potential value of the information, the decision maker should perform the experiment. if several different types of experiments are possible, the decision maker must choose the experiment yielding the overall largest expected value of utility.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 1., 0., 1., 0., 0.])","tensor([ 3247,  4106,  2007,  4242,  2592, 28139, 19894, 11124,  2953,  4106])","tensor([  101,  2411,  1996,  3247,  9338,  2038,  1996,  6061,  2000,  1520,
         4965,  1521,  3176,  2592,  2083,  2019,  7551,  2077,  2941,  2437,
         2010,  1013,  2014,  3601,  1997,  2895,  1012,  2065,  1996,  3465,
         1997,  2023,  2592,  2003,  2235,  1999,  7831,  2000,  1996,  4022,
         3643,  1997,  1996,  2592,  1010,  1996,  3247,  9338,  2323,  4685,
         1996,  7551,  1012,  2065,  2195,  2367,  4127,  1997,  7885,  2024,
         2825,  1010,  1996,  3247,  9338,  2442,  5454,  1996,  7551, 21336,
         1996,  3452,  2922,  3517,  3643,  1997,  9710,  1012,   102])"
869,1,"['trial', 'tests']", Decision Analysis with Unknown InformationPreposterior Analysis,seg_243,"if the example from the previous sections is considered again, the decision problem could be formulated as a decision to decide whether or not to perform the trial pump tests. the pre-posterior decision analysis facilitates this.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  4242,  2592, 28139, 19894, 11124,  2953,  4106])","tensor([  101,  2065,  1996,  2742,  2013,  1996,  3025,  5433,  2003,  2641,
         2153,  1010,  1996,  3247,  3291,  2071,  2022, 19788,  2004,  1037,
         3247,  2000,  5630,  3251,  2030,  2025,  2000,  4685,  1996,  3979,
        10216,  5852,  1012,  1996,  3653,  1011, 15219,  3247,  4106, 27777,
         2023,  1012,   102])"
870,1,"['information', 'experiment']", Decision Analysis with Unknown InformationPreposterior Analysis,seg_243,the situation prior to performing the experiment has already been considered in sect. 7.5. there it was found that the expected cost based entirely on the prior information e′[c] is 70 monetary units.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  4242,  2592, 28139, 19894, 11124,  2953,  4106])","tensor([  101,  1996,  3663,  3188,  2000,  4488,  1996,  7551,  2038,  2525,
         2042,  2641,  1999, 17831,  1012,  1021,  1012,  1019,  1012,  2045,
         2009,  2001,  2179,  2008,  1996,  3517,  3465,  2241,  4498,  2006,
         1996,  3188,  2592,  1041,  1531,  1031,  1039,  1033,  2003,  3963,
        12194,  3197,  1012,   102])"
871,1,['experiment'], Decision Analysis with Unknown InformationPreposterior Analysis,seg_243,"in this situation the experiment is planned but the result is still unknown. in this situation the expected cost, disregarding the experiment cost, can be found as:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  4242,  2592, 28139, 19894, 11124,  2953,  4106])","tensor([  101,  1999,  2023,  3663,  1996,  7551,  2003,  3740,  2021,  1996,
         2765,  2003,  2145,  4242,  1012,  1999,  2023,  3663,  1996,  3517,
         3465,  1010, 27770,  2075,  1996,  7551,  3465,  1010,  2064,  2022,
         2179,  2004,  1024,   102])"
872,1,['experiment'], Decision Analysis with Unknown InformationPreposterior Analysis,seg_243,where n is the number of different possible experiment findings and m is the number of different decision alternatives. in eq. 7.7 the only new term in comparison to the previous section is p ′[ii] which may be calculated by:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  4242,  2592, 28139, 19894, 11124,  2953,  4106])","tensor([  101,  2073,  1050,  2003,  1996,  2193,  1997,  2367,  2825,  7551,
         9556,  1998,  1049,  2003,  1996,  2193,  1997,  2367,  3247, 15955,
         1012,  1999,  1041,  4160,  1012,  1021,  1012,  1021,  1996,  2069,
         2047,  2744,  1999,  7831,  2000,  1996,  3025,  2930,  2003,  1052,
         1531,  1031,  2462,  1033,  2029,  2089,  2022, 10174,  2011,  1024,
          102])"
873,1,"['tests', 'probabilities', 'prior probabilities']", Decision Analysis with Unknown InformationPreposterior Analysis,seg_243,"with reference to sects. 7.5 and 7.6 the prior probabilities of obtaining the different indications by the tests are p ′ [i1], p ′ [i2] and p ′ [i3] and given by:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  4242,  2592, 28139, 19894, 11124,  2953,  4106])","tensor([  101,  2007,  4431,  2000, 17831,  2015,  1012,  1021,  1012,  1019,
         1998,  1021,  1012,  1020,  1996,  3188,  4013,  3676, 14680,  1997,
        11381,  1996,  2367, 24936,  2011,  1996,  5852,  2024,  1052,  1531,
         1031,  1045,  2487,  1033,  1010,  1052,  1531,  1031,  1045,  2475,
         1033,  1998,  1052,  1531,  1031,  1045,  2509,  1033,  1998,  2445,
         2011,  1024,   102])"
874,1,['posterior'], Decision Analysis with Unknown InformationPreposterior Analysis,seg_243,the posterior expected cost terms in eq. 7.7 are found to be:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 3247,  4106,  2007,  4242,  2592, 28139, 19894, 11124,  2953,  4106])","tensor([  101,  1996, 15219,  3517,  3465,  3408,  1999,  1041,  4160,  1012,
         1021,  1012,  1021,  2024,  2179,  2000,  2022,  1024,   102])"
875,1,"['posterior', 'probabilities', 'posterior probabilities', 'experiment']", Decision Analysis with Unknown InformationPreposterior Analysis,seg_243,where the posterior probabilities p ′′[θi | i1] and p ′′[θi | i2] are determined as already shown in sect. 7.6 for p ′′[θi | i3]. the expected cost corresponding to the situation where the experiment with the experiment costs cp is therefore:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  4242,  2592, 28139, 19894, 11124,  2953,  4106])","tensor([  101,  2073,  1996, 15219,  4013,  3676, 14680,  1052,  1531,  1531,
         1031,  1162,  2072,  1064,  1045,  2487,  1033,  1998,  1052,  1531,
         1531,  1031,  1162,  2072,  1064,  1045,  2475,  1033,  2024,  4340,
         2004,  2525,  3491,  1999, 17831,  1012,  1021,  1012,  1020,  2005,
         1052,  1531,  1531,  1031,  1162,  2072,  1064,  1045,  2509,  1033,
         1012,  1996,  3517,  3465,  7978,  2000,  1996,  3663,  2073,  1996,
         7551,  2007,  1996,  7551,  5366, 18133,  2003,  3568,  1024,   102])"
876,1,['experiment'], Decision Analysis with Unknown InformationPreposterior Analysis,seg_243,"by comparing this result with the expected cost corresponding to the prior information, it is seen that the experiment should be performed if the cost of the experiment is less than 0.6 monetary units:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 3247,  4106,  2007,  4242,  2592, 28139, 19894, 11124,  2953,  4106])","tensor([  101,  2011, 13599,  2023,  2765,  2007,  1996,  3517,  3465,  7978,
         2000,  1996,  3188,  2592,  1010,  2009,  2003,  2464,  2008,  1996,
         7551,  2323,  2022,  2864,  2065,  1996,  3465,  1997,  1996,  7551,
         2003,  2625,  2084,  1014,  1012,  1020, 12194,  3197,  1024,   102])"
877,1,"['value of information', 'information', 'case']", Decision Analysis with Unknown InformationPreposterior Analysis,seg_243,the 0.6 monetary units in this case correspond to the value of information.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 3247,  4106,  2007,  4242,  2592, 28139, 19894, 11124,  2953,  4106])","tensor([  101,  1996,  1014,  1012,  1020, 12194,  3197,  1999,  2023,  2553,
        17254,  2000,  1996,  3643,  1997,  2592,  1012,   102])"
878,1,"['decision theory', 'risk']", The Risk Treatment Decision Problem,seg_245,"having introduced the fundamental concepts of decision theory, it will now be considered how these carry over to the principally different types of risk analysis.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([1996, 3891, 3949, 3247, 3291])","tensor([  101,  2383,  3107,  1996,  8050,  8474,  1997,  3247,  3399,  1010,
         2009,  2097,  2085,  2022,  2641,  2129,  2122,  4287,  2058,  2000,
         1996, 16552,  2367,  4127,  1997,  3891,  4106,  1012,   102])"
879,1,"['risk', 'associated', 'information', 'probabilistic', 'prior decision analysis', 'statistical', 'probabilistic modeling', 'prior analysis', 'risks']", The Risk Treatment Decision Problem,seg_245,"the simplest form of risk analysis, i.e. a simple evaluation of the risks associated with a given activity and/or decision alternative may be related directly to the prior decision analysis. in the prior analysis the risk is evaluated on the basis of statistical information and probabilistic modeling available prior to any decision and/or activity. a simple decision/event tree in fig. 7.6 illustrates the prior analysis. in a prior analysis the risk for each possible activity/option may e.g. be evaluated as:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,
        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1996, 3891, 3949, 3247, 3291])","tensor([  101,  1996, 21304,  2433,  1997,  3891,  4106,  1010,  1045,  1012,
         1041,  1012,  1037,  3722,  9312,  1997,  1996, 10831,  3378,  2007,
         1037,  2445,  4023,  1998,  1013,  2030,  3247,  4522,  2089,  2022,
         3141,  3495,  2000,  1996,  3188,  3247,  4106,  1012,  1999,  1996,
         3188,  4106,  1996,  3891,  2003, 16330,  2006,  1996,  3978,  1997,
         7778,  2592,  1998,  4013,  3676, 27965,  4588, 11643,  2800,  3188,
         2000,  2151,  3247,  1998,  1013,  2030,  4023,  1012,  1037,  3722,
         3247,  1013,  2724,  3392,  1999, 20965,  1012,  1021,  1012,  1020,
        24899,  1996,  3188,  4106,  1012,  1999,  1037,  3188,  4106,  1996,
         3891,  2005,  2169,  2825,  4023,  1013,  5724,  2089,  1041,  1012,
         1043,  1012,  2022, 16330,  2004,  1024,   102])"
880,1,"['risk', 'probability', 'utility', 'event']", The Risk Treatment Decision Problem,seg_245,"where r is the risk, u the utility, pi is the ith branching probability and ci the consequence of the event of branch i.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([1996, 3891, 3949, 3247, 3291])","tensor([  101,  2073,  1054,  2003,  1996,  3891,  1010,  1057,  1996,  9710,
         1010, 14255,  2003,  1996,  2009,  2232, 23346,  9723,  1998, 25022,
         1996,  9509,  1997,  1996,  2724,  1997,  3589,  1045,  1012,   102])"
881,1,"['prior analysis', 'risks', 'risk']", The Risk Treatment Decision Problem,seg_245,"a prior analysis, in fact, corresponds closely to the assessment of the risk associated with a known activity. a prior analysis thus forms the basis for the comparison of risks between different activities.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0.])","tensor([1996, 3891, 3949, 3247, 3291])","tensor([  101,  1037,  3188,  4106,  1010,  1999,  2755,  1010, 14788,  4876,
         2000,  1996,  7667,  1997,  1996,  3891,  3378,  2007,  1037,  2124,
         4023,  1012,  1037,  3188,  4106,  2947,  3596,  1996,  3978,  2005,
         1996,  7831,  1997, 10831,  2090,  2367,  3450,  1012,   102])"
882,1,"['consequences', 'risk', 'posterior', 'information', 'probabilities', 'posterior analysis', 'prior analysis']", The Risk Treatment Decision Problem,seg_245,"a posterior analysis is, in principle, of the same form as the prior analysis; however, changes in the branching probabilities and/or the consequences in the decision/event tree reflect that the considered problem has been changed as an effect of risk reducing measures, risk mitigating measures and/or collection of additional information.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([1996, 3891, 3949, 3247, 3291])","tensor([  101,  1037, 15219,  4106,  2003,  1010,  1999,  6958,  1010,  1997,
         1996,  2168,  2433,  2004,  1996,  3188,  4106,  1025,  2174,  1010,
         3431,  1999,  1996, 23346,  4013,  3676, 14680,  1998,  1013,  2030,
         1996,  8465,  1999,  1996,  3247,  1013,  2724,  3392,  8339,  2008,
         1996,  2641,  3291,  2038,  2042,  2904,  2004,  2019,  3466,  1997,
         3891,  8161,  5761,  1010,  3891, 10210, 13340,  3436,  5761,  1998,
         1013,  2030,  3074,  1997,  3176,  2592,  1012,   102])"
883,1,"['posterior', 'design', 'posterior analysis', 'errors']", The Risk Treatment Decision Problem,seg_245,"a posterior analysis may thus be used to evaluate the effect of activities, which factually have been performed. for example, for assessment of existing facilities, the testing and inspection of the “as built” facility would be expected to reveal many gross design and construction errors, leading to a more accurate reliability analysis.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1996, 3891, 3949, 3247, 3291])","tensor([  101,  1037, 15219,  4106,  2089,  2947,  2022,  2109,  2000, 16157,
         1996,  3466,  1997,  3450,  1010,  2029, 25854,  2135,  2031,  2042,
         2864,  1012,  2005,  2742,  1010,  2005,  7667,  1997,  4493,  4128,
         1010,  1996,  5604,  1998, 10569,  1997,  1996,  1523,  2004,  2328,
         1524,  4322,  2052,  2022,  3517,  2000,  7487,  2116,  7977,  2640,
         1998,  2810, 10697,  1010,  2877,  2000,  1037,  2062,  8321, 15258,
         4106,  1012,   102])"
884,1,"['results', 'information', 'risk']", The Risk Treatment Decision Problem,seg_245,"a pre-posterior analysis may be illustrated by the decision/event tree shown in fig. 7.7. using pre-posterior analysis, optimal decisions with regard to activities that may be performed in the future, e.g. the planning of risk reducing activities and/or collection of information may be identified. an important prerequisite for pre-posterior analysis is that decision rules need to be formulated for specifying the future actions that will be taken on the basis of the results of the planned activities.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([1996, 3891, 3949, 3247, 3291])","tensor([  101,  1037,  3653,  1011, 15219,  4106,  2089,  2022,  7203,  2011,
         1996,  3247,  1013,  2724,  3392,  3491,  1999, 20965,  1012,  1021,
         1012,  1021,  1012,  2478,  3653,  1011, 15219,  4106,  1010, 15502,
         6567,  2007,  7634,  2000,  3450,  2008,  2089,  2022,  2864,  1999,
         1996,  2925,  1010,  1041,  1012,  1043,  1012,  1996,  4041,  1997,
         3891,  8161,  3450,  1998,  1013,  2030,  3074,  1997,  2592,  2089,
         2022,  4453,  1012,  2019,  2590,  3653,  2890, 24871,  2005,  3653,
         1011, 15219,  4106,  2003,  2008,  3247,  3513,  2342,  2000,  2022,
        19788,  2005, 20648,  2075,  1996,  2925,  4506,  2008,  2097,  2022,
         2579,  2006,  1996,  3978,  1997,  1996,  3463,  1997,  1996,  3740,
         3450,  1012,   102])"
885,0,[], The Risk Treatment Decision Problem,seg_245,"in a pre-posterior analysis, the optimal investigation a∗ is identified through:",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([1996, 3891, 3949, 3247, 3291])","tensor([  101,  1999,  1037,  3653,  1011, 15219,  4106,  1010,  1996, 15502,
         4812,  1037, 30125,  2003,  4453,  2083,  1024,   102])"
886,0,[], The Risk Treatment Decision Problem,seg_245,where a(z) are the different possible actions that can be taken on the basis of the,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([1996, 3891, 3949, 3247, 3291])","tensor([ 101, 2073, 1037, 1006, 1062, 1007, 2024, 1996, 2367, 2825, 4506, 2008,
        2064, 2022, 2579, 2006, 1996, 3978, 1997, 1996,  102])"
887,1,"['posterior', 'associated', 'events', 'information', 'utility', 'probabilistic', 'expected value']", The Risk Treatment Decision Problem,seg_245,"′ result of the considered investigation z and e[·] is the expected value operator. and ′′ refer to the probabilistic description of the events of interest based on prior and posterior information respectively. in eq. 7.10 the expected utility has been associated only with expected costs; hence the optimal decision is identified through a minimization. if utility, more generally, is associated with expected benefits, the optimization should be performed through maximization.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([1996, 3891, 3949, 3247, 3291])","tensor([  101,  1531,  2765,  1997,  1996,  2641,  4812,  1062,  1998,  1041,
         1031,  1087,  1033,  2003,  1996,  3517,  3643,  6872,  1012,  1998,
         1531,  1531,  6523,  2000,  1996,  4013,  3676, 27965,  4588,  6412,
         1997,  1996,  2824,  1997,  3037,  2241,  2006,  3188,  1998, 15219,
         2592,  4414,  1012,  1999,  1041,  4160,  1012,  1021,  1012,  2184,
         1996,  3517,  9710,  2038,  2042,  3378,  2069,  2007,  3517,  5366,
         1025,  6516,  1996, 15502,  3247,  2003,  4453,  2083,  1037,  7163,
         4328,  9276,  1012,  2065,  9710,  1010,  2062,  3227,  1010,  2003,
         3378,  2007,  3517,  6666,  1010,  1996, 20600,  2323,  2022,  2864,
         2083, 20446,  3989,  1012,   102])"
888,1,['risk'], The Risk Treatment Decision Problem,seg_245,"pre-posterior analyses form a strong decision support tool and have been intensively used for the purpose of risk based inspection planning. however, so far preposterior decision analysis has been grossly overlooked in risk assessments.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([1996, 3891, 3949, 3247, 3291])","tensor([  101,  3653,  1011, 15219, 16478,  2433,  1037,  2844,  3247,  2490,
         6994,  1998,  2031,  2042, 11806,  2135,  2109,  2005,  1996,  3800,
         1997,  3891,  2241, 10569,  4041,  1012,  2174,  1010,  2061,  2521,
        17463, 14122, 11124,  2953,  3247,  4106,  2038,  2042,  7977,  2135,
        17092,  1999,  3891, 20794,  1012,   102])"
889,1,"['posterior decision analysis', 'risk', 'posterior', 'events', 'structural reliability analysis', 'probabilities', 'prior or posterior decision analysis', 'combination']", The Risk Treatment Decision Problem,seg_245,"it is important to note that the probabilities for the different events represented in the prior or posterior decision analysis may be assessed by logic tree analysis, classical reliability analysis and structural reliability analysis or any combination of these. the risk analysis thus in effect includes all these aspects of systems and component modeling in addition to providing the framework for the decision making.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1996, 3891, 3949, 3247, 3291])","tensor([  101,  2009,  2003,  2590,  2000,  3602,  2008,  1996,  4013,  3676,
        14680,  2005,  1996,  2367,  2824,  3421,  1999,  1996,  3188,  2030,
        15219,  3247,  4106,  2089,  2022, 14155,  2011,  7961,  3392,  4106,
         1010,  4556, 15258,  4106,  1998,  8332, 15258,  4106,  2030,  2151,
         5257,  1997,  2122,  1012,  1996,  3891,  4106,  2947,  1999,  3466,
         2950,  2035,  2122,  5919,  1997,  3001,  1998,  6922, 11643,  1999,
         2804,  2000,  4346,  1996,  7705,  2005,  1996,  3247,  2437,  1012,
          102])"
890,1,"['function', 'posterior decision analysis', 'posterior', 'prior and posterior decision analysis', 'utility', 'event']", Self Assessment QuestionsExercises,seg_247,"1. what must be identified before a decision analysis can be performed? 2. what is a utility function? 3. what is the difference between prior and posterior decision analysis? 4. what is the idea behind the pre-posterior decision analysis? 5. after the occurrence of an event of heavy snowfall, you need to decide whether",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1015,  1012,  2054,  2442,  2022,  4453,  2077,  1037,  3247,
         4106,  2064,  2022,  2864,  1029,  1016,  1012,  2054,  2003,  1037,
         9710,  3853,  1029,  1017,  1012,  2054,  2003,  1996,  4489,  2090,
         3188,  1998, 15219,  3247,  4106,  1029,  1018,  1012,  2054,  2003,
         1996,  2801,  2369,  1996,  3653,  1011, 15219,  3247,  4106,  1029,
         1019,  1012,  2044,  1996, 14404,  1997,  2019,  2724,  1997,  3082,
        26043,  1010,  2017,  2342,  2000,  5630,  3251,   102])"
891,1,"['associated', 'probability', 'estimated', 'information', 'process', 'case']", Self Assessment QuestionsExercises,seg_247,"to clean up the snow from a roof or not. in the following some information is provided to assist you in the decision making process. the clean up of the roof can be made by the local fire department. this option is associated with a cost equal to 4000 sfr. in the case of collapse of the roof due to the snow load, the associated cost is equal to 1000000 sfr. the probability of collapse of the roof has been estimated using first order reliability methods (form). if the snow is dry (denoted assd), the probability of collapse is: pf (sd) = 10−3. if the snow is wet (denoted as sw ), the probability",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  2000,  4550,  2039,  1996,  4586,  2013,  1037,  4412,  2030,
         2025,  1012,  1999,  1996,  2206,  2070,  2592,  2003,  3024,  2000,
         6509,  2017,  1999,  1996,  3247,  2437,  2832,  1012,  1996,  4550,
         2039,  1997,  1996,  4412,  2064,  2022,  2081,  2011,  1996,  2334,
         2543,  2533,  1012,  2023,  5724,  2003,  3378,  2007,  1037,  3465,
         5020,  2000, 20143, 16420,  2099,  1012,  1999,  1996,  2553,  1997,
         7859,  1997,  1996,  4412,  2349,  2000,  1996,  4586,  7170,  1010,
         1996,  3378,  3465,  2003,  5020,  2000,  6694,  8889,  2692, 16420,
         2099,  1012,  1996,  9723,  1997,  7859,  1997,  1996,  4412,  2038,
         2042,  4358,  2478,  2034,  2344, 15258,  4725,  1006,  2433,  1007,
         1012,  2065,  1996,  4586,  2003,  4318,  1006, 19537,  4632,  2094,
         1007,  1010,  1996,  9723,  1997,  7859,  2003,  1024,  1052,  2546,
         1006, 17371,  1007,  1027,  2184, 22543,  2509,  1012,  2065,  1996,
         4586,  2003,  4954,  1006, 19537,  2004, 25430,  1007,  1010,  1996,
         9723,   102])"
892,1,"['cases', 'uncertainty', 'probability', 'event', 'case', 'test']", Self Assessment QuestionsExercises,seg_247,"of collapse is: pf (sw) = 6.2 · 10−3. based on experience, it can be said that tells that the probability of having wet snow on the roof is p(sw) = 0.6. in case where there is no snow (denoted as sn ), on the roof the probability of collapse is equal to: pf (sn) = 5 · 10−4. with the aid of a melting test, the characteristics of the snow can be ascertained. the test costs 1000 sfr. the test indication of the snow property, isd or isw , is correct only in 75% of the cases. as the uncertainty of the indication is independent of the snow properties, p(isd|sd) = p(isw |sw). build an appropriate event tree and use it to find out which decision alternative— “clean up”, “do nothing” or “carry out test” is the most beneficial one in terms of cost.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2969,  7667,  3980, 10288,  2121, 18380,  2015])","tensor([  101,  1997,  7859,  2003,  1024,  1052,  2546,  1006, 25430,  1007,
         1027,  1020,  1012,  1016,  1087,  2184, 22543,  2509,  1012,  2241,
         2006,  3325,  1010,  2009,  2064,  2022,  2056,  2008,  4136,  2008,
         1996,  9723,  1997,  2383,  4954,  4586,  2006,  1996,  4412,  2003,
         1052,  1006, 25430,  1007,  1027,  1014,  1012,  1020,  1012,  1999,
         2553,  2073,  2045,  2003,  2053,  4586,  1006, 19537,  2004,  1055,
         2078,  1007,  1010,  2006,  1996,  4412,  1996,  9723,  1997,  7859,
         2003,  5020,  2000,  1024,  1052,  2546,  1006,  1055,  2078,  1007,
         1027,  1019,  1087,  2184, 22543,  2549,  1012,  2007,  1996,  4681,
         1997,  1037, 13721,  3231,  1010,  1996,  6459,  1997,  1996,  4586,
         2064,  2022,  2004, 17119, 28055,  1012,  1996,  3231,  5366,  6694,
        16420,  2099,  1012,  1996,  3231, 12407,  1997,  1996,  4586,  3200,
         1010,  2003,  2094,  2030,  2003,  2860,  1010,  2003,  6149,  2069,
         1999,  4293,  1003,  1997,  1996,  3572,  1012,  2004,  1996, 12503,
         1997,  1996, 12407,  2003,  2981,  1997,  1996,  4586,  5144,  1010,
         1052,  1006,  2003,  2094,  1064, 17371,  1007,  1027,  1052,  1006,
         2003,  2860,  1064, 25430,  1007,  1012,  3857,  2019,  6413,  2724,
         3392,  1998,  2224,  2009,  2000,  2424,  2041,  2029,  3247,  4522,
         1517,  1523,  4550,  2039,  1524,  1010,  1523,  2079,  2498,  1524,
         2030,  1523,  4287,  2041,  3231,  1524,  2003,  1996,  2087, 15189,
         2028,  1999,  3408,  1997,  3465,  1012,   102])"
893,1,['sustainable'],A Chapter ,seg_251,"1.1 according to the so-called brundtland commission [5], sustainable develop-",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1015,  1012,  1015,  2429,  2000,  1996,  2061,  1011,  2170,
         7987,  8630, 19270,  3222,  1031,  1019,  1033,  1010,  9084,  4503,
         1011,   102])"
894,1,"['efficient', 'joint', 'sustainable']",A Chapter ,seg_251,"ment is defined as development “that meets the needs of the present without compromising the ability of future generations to meet their own needs”. consideration of sustainable development leads to sustainable decision making which may be understood as based on a joint consideration of society, economy and environment (for more see sect. 1.1). 1.2 a beneficial engineered facility is understood as: being economically efficient",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0.])","tensor([1037, 3127])","tensor([  101,  2273,  2102,  2003,  4225,  2004,  2458,  1523,  2008,  6010,
         1996,  3791,  1997,  1996,  2556,  2302,  4012, 25013,  1996,  3754,
         1997,  2925,  8213,  2000,  3113,  2037,  2219,  3791,  1524,  1012,
         9584,  1997,  9084,  2458,  5260,  2000,  9084,  3247,  2437,  2029,
         2089,  2022,  5319,  2004,  2241,  2006,  1037,  4101,  9584,  1997,
         2554,  1010,  4610,  1998,  4044,  1006,  2005,  2062,  2156, 17831,
         1012,  1015,  1012,  1015,  1007,  1012,  1015,  1012,  1016,  1037,
        15189, 13685,  4322,  2003,  5319,  2004,  1024,  2108, 15318,  8114,
          102])"
895,1,"['limit', 'event']",A Chapter ,seg_251,"in serving a specific purpose, fulfilling given requirements with regard to the safety of the personnel, and fulfilling given requirements to limit the adverse effects of the facility on the environment (for more, see sect. 1.2). 1.3 as discussed in sect. 1.3 when considering an activity with only one event with",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1999,  3529,  1037,  3563,  3800,  1010, 21570,  2445,  5918,
         2007,  7634,  2000,  1996,  3808,  1997,  1996,  5073,  1010,  1998,
        21570,  2445,  5918,  2000,  5787,  1996, 15316,  3896,  1997,  1996,
         4322,  2006,  1996,  4044,  1006,  2005,  2062,  1010,  2156, 17831,
         1012,  1015,  1012,  1016,  1007,  1012,  1015,  1012,  1017,  2004,
         6936,  1999, 17831,  1012,  1015,  1012,  1017,  2043,  6195,  2019,
         4023,  2007,  2069,  2028,  2724,  2007,   102])"
896,1,"['consequences', 'risk', 'probability', 'event']",A Chapter ,seg_251,"potential consequences c, the risk r is the probability p that this event will occur multiplied with the consequences given the event occurs i.e.:",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  4022,  8465,  1039,  1010,  1996,  3891,  1054,  2003,  1996,
         9723,  1052,  2008,  2023,  2724,  2097,  5258, 28608,  2007,  1996,
         8465,  2445,  1996,  2724,  5158,  1045,  1012,  1041,  1012,  1024,
          102])"
897,0,[],A Chapter ,seg_251,1.4 the term “acceptable risks” points out to “what is one prepared to invest and/or,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([1037, 3127])","tensor([  101,  1015,  1012,  1018,  1996,  2744,  1523, 11701, 10831,  1524,
         2685,  2041,  2000,  1523,  2054,  2003,  2028,  4810,  2000, 15697,
         1998,  1013,  2030,   102])"
898,1,"['risk', 'event']",A Chapter ,seg_251,"pay for the purpose of getting a potential benefit” (for more, see sect. 1.2). 1.5 as discussed in sect. 1.3, the risk of an event is calculated by eq. 1.1 such as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  3477,  2005,  1996,  3800,  1997,  2893,  1037,  4022,  5770,
         1524,  1006,  2005,  2062,  1010,  2156, 17831,  1012,  1015,  1012,
         1016,  1007,  1012,  1015,  1012,  1019,  2004,  6936,  1999, 17831,
         1012,  1015,  1012,  1017,  1010,  1996,  3891,  1997,  2019,  2724,
         2003, 10174,  2011,  1041,  4160,  1012,  1015,  1012,  1015,  2107,
         2004,  1024,   102])"
899,1,"['risk', 'associated', 'table', 'event']",A Chapter ,seg_251,r = pc. hence the given table can easily be completed and it can be seen that event 3 is associated with the higher risk.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.])","tensor([1037, 3127])","tensor([ 101, 1054, 1027, 7473, 1012, 6516, 1996, 2445, 2795, 2064, 4089, 2022,
        2949, 1998, 2009, 2064, 2022, 2464, 2008, 2724, 1017, 2003, 3378, 2007,
        1996, 3020, 3891, 1012,  102])"
900,1,"['probability', 'estimation']",A Chapter ,seg_253,2.1 the estimation is based on the frequentistic interpretation of probability. in,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1016,  1012,  1015,  1996, 24155,  2003,  2241,  2006,  1996,
         6976,  6553,  7613,  1997,  9723,  1012,  1999,   102])"
901,1,"['probability', 'event', 'trials', 'experiment']",A Chapter ,seg_253,"the frequentistic interpretation, the probability p(a) is simply the relative frequency of occurrence of the event a as observed in an experiment with n trials. it is mathematically defined as (for more, see sect. 2.2):",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1996,  6976,  6553,  7613,  1010,  1996,  9723,  1052,  1006,
         1037,  1007,  2003,  3432,  1996,  5816,  6075,  1997, 14404,  1997,
         1996,  2724,  1037,  2004,  5159,  1999,  2019,  7551,  2007,  1050,
         7012,  1012,  2009,  2003,  8045,  2135,  4225,  2004,  1006,  2005,
         2062,  1010,  2156, 17831,  1012,  1016,  1012,  1016,  1007,  1024,
          102])"
902,1,"['conditional probability', 'probability', 'conditional']",A Chapter ,seg_253,"2.2 following the bayes’ rule (see sect. 2.5), the conditional probability of the",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1016,  1012,  1016,  2206,  1996,  3016,  2229,  1521,  3627,
         1006,  2156, 17831,  1012,  1016,  1012,  1019,  1007,  1010,  1996,
        18462,  9723,  1997,  1996,   102])"
903,1,['event'],A Chapter ,seg_253,event e1 given that the event e2 has occurred is written as:,tensor(1),"tensor([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([ 101, 2724, 1041, 2487, 2445, 2008, 1996, 2724, 1041, 2475, 2038, 4158,
        2003, 2517, 2004, 1024,  102])"
904,1,"['probability', 'event', 'probability theory']",A Chapter ,seg_253,"2.3 in probability theory, the probability p(a) of an event a can take any value",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([ 101, 1016, 1012, 1017, 1999, 9723, 3399, 1010, 1996, 9723, 1052, 1006,
        1037, 1007, 1997, 2019, 2724, 1037, 2064, 2202, 2151, 3643,  102])"
905,1,"['set', 'events', 'intersection']",A Chapter ,seg_253,"within the following boundaries: 0 ≤ p(a) ≤ 1 −1 ≤ p(a) ≤ 1 −∞ ≤ p(a) ≤ ∞ 2.4 if the intersection of two events, a and b corresponds to the empty set ∅, i.e.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2306,  1996,  2206,  7372,  1024,  1014,  1608,  1052,  1006,
         1037,  1007,  1608,  1015,  1597,  2487,  1608,  1052,  1006,  1037,
         1007,  1608,  1015,  1597, 30128,  1608,  1052,  1006,  1037,  1007,
         1608,  1601,  1016,  1012,  1018,  2065,  1996,  6840,  1997,  2048,
         2824,  1010,  1037,  1998,  1038, 14788,  2000,  1996,  4064,  2275,
         1593,  1010,  1045,  1012,  1041,  1012,   102])"
906,1,"['events', 'mutually exclusive', 'independent']",A Chapter ,seg_253,"a ∩ b = ∅, the two events are: mutually exclusive. independent. empty events. 2.5 which of the following expressions is(are) correct?",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1037,  1604,  1038,  1027,  1593,  1010,  1996,  2048,  2824,
         2024,  1024, 20271,  7262,  1012,  2981,  1012,  4064,  2824,  1012,
         1016,  1012,  1019,  2029,  1997,  1996,  2206, 11423,  2003,  1006,
         2024,  1007,  6149,  1029,   102])"
907,1,"['probability', 'events', 'event', 'intersection', 'probability of the intersection of two events', 'union', 'mutually exclusive events', 'mutually exclusive', 'probability of event', 'independent']",A Chapter ,seg_253,"the probability of the union of two events a and b is equal to the sum of the probability of event a and the probability of event b , given that the two events are mutually exclusive. the probability of the union of two events a and b is equal to the probability of the sum of event a and event b , given that the two events are mutually exclusive. the probability of the intersection of two events a and b is equal to the product of the probability of event a and the probability of event b , given that the two events are mutually exclusive. the probability of the intersection of two events a and b is equal to the product of the probability of event a and the probability of event b , given that the two events are independent. 2.6 the probability of the intersection of two mutually exclusive events is equal",tensor(1),"tensor([0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1996,  9723,  1997,  1996,  2586,  1997,  2048,  2824,  1037,
         1998,  1038,  2003,  5020,  2000,  1996,  7680,  1997,  1996,  9723,
         1997,  2724,  1037,  1998,  1996,  9723,  1997,  2724,  1038,  1010,
         2445,  2008,  1996,  2048,  2824,  2024, 20271,  7262,  1012,  1996,
         9723,  1997,  1996,  2586,  1997,  2048,  2824,  1037,  1998,  1038,
         2003,  5020,  2000,  1996,  9723,  1997,  1996,  7680,  1997,  2724,
         1037,  1998,  2724,  1038,  1010,  2445,  2008,  1996,  2048,  2824,
         2024, 20271,  7262,  1012,  1996,  9723,  1997,  1996,  6840,  1997,
         2048,  2824,  1037,  1998,  1038,  2003,  5020,  2000,  1996,  4031,
         1997,  1996,  9723,  1997,  2724,  1037,  1998,  1996,  9723,  1997,
         2724,  1038,  1010,  2445,  2008,  1996,  2048,  2824,  2024, 20271,
         7262,  1012,  1996,  9723,  1997,  1996,  6840,  1997,  2048,  2824,
         1037,  1998,  1038,  2003,  5020,  2000,  1996,  4031,  1997,  1996,
         9723,  1997,  2724,  1037,  1998,  1996,  9723,  1997,  2724,  1038,
         1010,  2445,  2008,  1996,  2048,  2824,  2024,  2981,  1012,  1016,
         1012,  1020,  1996,  9723,  1997,  1996,  6840,  1997,  2048, 20271,
         7262,  2824,  2003,  5020,   102])"
908,1,"['events', 'probabilities']",A Chapter ,seg_253,to: the product of the probabilities of the individual events. the sum of the probabilities of the individual events. the difference between the probabilities of the individual events. one (1). zero (0).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2000,  1024,  1996,  4031,  1997,  1996,  4013,  3676, 14680,
         1997,  1996,  3265,  2824,  1012,  1996,  7680,  1997,  1996,  4013,
         3676, 14680,  1997,  1996,  3265,  2824,  1012,  1996,  4489,  2090,
         1996,  4013,  3676, 14680,  1997,  1996,  3265,  2824,  1012,  2028,
         1006,  1015,  1007,  1012,  5717,  1006,  1014,  1007,  1012,   102])"
909,1,"['events', 'sample spaces', 'sample']",A Chapter ,seg_253,"2.7 within the theory of sample spaces and events, which of the following state-",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([1037, 3127])","tensor([ 101, 1016, 1012, 1021, 2306, 1996, 3399, 1997, 7099, 7258, 1998, 2824,
        1010, 2029, 1997, 1996, 2206, 2110, 1011,  102])"
910,1,"['sample space', 'probability', 'events', 'event', 'union', 'sample', 'mutually exclusive events', 'mutually exclusive']",A Chapter ,seg_253,ments is(are) correct? an event a is defined as a subset of a sample space ω . a sample space ω is defined as a subset of an event a. 2.8 the probability of the union of two not mutually exclusive events a and b,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0.])","tensor([1037, 3127])","tensor([  101,  2273,  3215,  2003,  1006,  2024,  1007,  6149,  1029,  2019,
         2724,  1037,  2003,  4225,  2004,  1037, 16745,  1997,  1037,  7099,
         2686,  1179,  1012,  1037,  7099,  2686,  1179,  2003,  4225,  2004,
         1037, 16745,  1997,  2019,  2724,  1037,  1012,  1016,  1012,  1022,
         1996,  9723,  1997,  1996,  2586,  1997,  2048,  2025, 20271,  7262,
         2824,  1037,  1998,  1038,   102])"
911,1,"['sample space', 'probability', 'event', 'sample', 'probability of event']",A Chapter ,seg_253,"is given as: p(a ∪ b) = p(a) + p(b) − p(a ∩ b). it is provided that the probability of event a is equal to 0.1, the probability of event b is 0.1 and the probability of event b given event a, i.e. p(b|a) is 0.8. which result is correct? p(a ∪ b) = −0.6 p(a ∪ b) = 0.12 p(a ∪ b) = 0.04 2.9 for an event a in the sample space ω , event ā represents the complementary",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 1., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2003,  2445,  2004,  1024,  1052,  1006,  1037,  1605,  1038,
         1007,  1027,  1052,  1006,  1037,  1007,  1009,  1052,  1006,  1038,
         1007,  1597,  1052,  1006,  1037,  1604,  1038,  1007,  1012,  2009,
         2003,  3024,  2008,  1996,  9723,  1997,  2724,  1037,  2003,  5020,
         2000,  1014,  1012,  1015,  1010,  1996,  9723,  1997,  2724,  1038,
         2003,  1014,  1012,  1015,  1998,  1996,  9723,  1997,  2724,  1038,
         2445,  2724,  1037,  1010,  1045,  1012,  1041,  1012,  1052,  1006,
         1038,  1064,  1037,  1007,  2003,  1014,  1012,  1022,  1012,  2029,
         2765,  2003,  6149,  1029,  1052,  1006,  1037,  1605,  1038,  1007,
         1027,  1597,  2692,  1012,  1020,  1052,  1006,  1037,  1605,  1038,
         1007,  1027,  1014,  1012,  2260,  1052,  1006,  1037,  1605,  1038,
         1007,  1027,  1014,  1012,  5840,  1016,  1012,  1023,  2005,  2019,
         2724,  1037,  1999,  1996,  7099,  2686,  1179,  1010,  2724,  1037,
         5836,  1996, 21053,   102])"
912,1,"['event', 'distributive laws', 'associative and distributive laws']",A Chapter ,seg_253,"event of event a. which of the following hold? a ∪ ā = ω a ∩ ā = ω a ∪ ā = ∅ 2.10 the commutative, associative and distributive laws describe how to:",tensor(1),"tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2724,  1997,  2724,  1037,  1012,  2029,  1997,  1996,  2206,
         2907,  1029,  1037,  1605,  1037,  1027,  1179,  1037,  1604,  1037,
         1027,  1179,  1037,  1605,  1037,  1027,  1593,  1016,  1012,  2184,
         1996,  4012, 28120,  8082,  1010,  4632, 10085,  2401,  6024,  1998,
         4487,  3367,  3089,  8569,  6024,  4277,  6235,  2129,  2000,  1024,
          102])"
913,1,['sets'],A Chapter ,seg_253,operate with intersections of sets. operate with unions of sets. none of the above. 2.11 following the principles explained in sect. 2.5 we obtain:,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  5452,  2007, 26540,  1997,  4520,  1012,  5452,  2007,  9209,
         1997,  4520,  1012,  3904,  1997,  1996,  2682,  1012,  1016,  1012,
         2340,  2206,  1996,  6481,  4541,  1999, 17831,  1012,  1016,  1012,
         1019,  2057,  6855,  1024,   102])"
914,1,['table'],A Chapter ,seg_253,a) the table is completed as follows:,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([ 101, 1037, 1007, 1996, 2795, 2003, 2949, 2004, 4076, 1024,  102])"
915,1,['probability'],A Chapter ,seg_253,b) using the bayes’ rule the probability that the final decision made by snf is the same with the indicative assessment of dr. beispiel is:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1038,  1007,  2478,  1996,  3016,  2229,  1521,  3627,  1996,
         9723,  2008,  1996,  2345,  3247,  2081,  2011,  1055,  2078,  2546,
         2003,  1996,  2168,  2007,  1996, 24668,  7667,  1997,  2852,  1012,
        21388, 13102,  9257,  2003,  1024,   102])"
916,1,"['descriptive statistics', 'statistics']",A Chapter ,seg_255,3.1 the main purpose of the use of descriptive statistics is to assess the characteris-,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1017,  1012,  1015,  1996,  2364,  3800,  1997,  1996,  2224,
         1997, 22726,  6747,  2003,  2000, 14358,  1996,  2839,  2483,  1011,
          102])"
917,1,"['coefficient of correlation', 'uncertainty', 'interval', 'coefficient', 'level', 'sample', 'correlation', 'data']",A Chapter ,seg_255,"tics and the level of uncertainty of a given quantity of interest without assuming anything in terms of the degree or nature of the randomness underlying the data analyzed (see also sect. 3.1). 3.2 by definition, the sample coefficient of correlation may lie in the interval",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        0., 0., 0., 1., 0.])","tensor([1037, 3127])","tensor([  101, 14841,  6169,  1998,  1996,  2504,  1997, 12503,  1997,  1037,
         2445, 11712,  1997,  3037,  2302, 10262,  2505,  1999,  3408,  1997,
         1996,  3014,  2030,  3267,  1997,  1996,  6721,  2791, 10318,  1996,
         2951, 16578,  1006,  2156,  2036, 17831,  1012,  1017,  1012,  1015,
         1007,  1012,  1017,  1012,  1016,  2011,  6210,  1010,  1996,  7099,
        19064,  1997, 16902,  2089,  4682,  1999,  1996, 13483,   102])"
918,1,"['cases', 'sets', 'linear', 'interval', 'data sets', 'data']",A Chapter ,seg_255,"[−1;1]. in both extreme cases, there are linear relationships between two data sets (see also sect. 3.2). 3.3 the interval width plays a role for the resolution of the representation of the",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1031,  1597,  2487,  1025,  1015,  1033,  1012,  1999,  2119,
         6034,  3572,  1010,  2045,  2024,  7399,  6550,  2090,  2048,  2951,
         4520,  1006,  2156,  2036, 17831,  1012,  1017,  1012,  1016,  1007,
         1012,  1017,  1012,  1017,  1996, 13483,  9381,  3248,  1037,  2535,
         2005,  1996,  5813,  1997,  1996,  6630,  1997,  1996,   102])"
919,1,"['set', 'interval', 'histogram', 'random', 'data set', 'data']",A Chapter ,seg_255,"observations. if the interval width is too large, the histogram tells little about relative occurrences of individual phenomena. if the width is too small, the relative occurrences in each interval fluctuate due to the random nature of the phenomena (see also sect. 3.3). 3.4 as discussed in sect. 3.3, five characteristics of a data set are normally pre-",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  9420,  1012,  2065,  1996, 13483,  9381,  2003,  2205,  2312,
         1010,  1996,  2010,  3406, 13113,  4136,  2210,  2055,  5816, 27247,
         1997,  3265, 13352,  1012,  2065,  1996,  9381,  2003,  2205,  2235,
         1010,  1996,  5816, 27247,  1999,  2169, 13483, 19857,  6593, 20598,
         2349,  2000,  1996,  6721,  3267,  1997,  1996, 13352,  1006,  2156,
         2036, 17831,  1012,  1017,  1012,  1017,  1007,  1012,  1017,  1012,
         1018,  2004,  6936,  1999, 17831,  1012,  1017,  1012,  1017,  1010,
         2274,  6459,  1997,  1037,  2951,  2275,  2024,  5373,  3653,  1011,
          102])"
920,1,"['outside values', 'tukey box plot', 'observations', 'efficient', 'box plot', 'adjacent value', 'plots', 'upper quartile', 'plot', 'quartile', 'median']",A Chapter ,seg_255,"sented in a tukey box plot: the lower adjacent value, the lower quartile, the median, the upper quartile and the upper adjacent value. outside values can also be shown on a tukey box plot. 3.5 q-q plots provide an efficient means of comparison of observations of two",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2741,  2098,  1999,  1037, 10722, 14839,  3482,  5436,  1024,
         1996,  2896,  5516,  3643,  1010,  1996,  2896, 24209,  8445,  9463,
         1010,  1996,  3991,  1010,  1996,  3356, 24209,  8445,  9463,  1998,
         1996,  3356,  5516,  3643,  1012,  2648,  5300,  2064,  2036,  2022,
         3491,  2006,  1037, 10722, 14839,  3482,  5436,  1012,  1017,  1012,
         1019,  1053,  1011,  1053, 14811,  3073,  2019,  8114,  2965,  1997,
         7831,  1997,  9420,  1997,  2048,   102])"
921,1,"['sets', 'estimate', 'coefficient', 'correlation coefficient', 'correlation', 'data sets', 'data']",A Chapter ,seg_255,different data sets (see also sect. 3.3). 3.6 provide an estimate of the correlation coefficient of the data sets plotted in the,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2367,  2951,  4520,  1006,  2156,  2036, 17831,  1012,  1017,
         1012,  1017,  1007,  1012,  1017,  1012,  1020,  3073,  2019, 10197,
         1997,  1996, 16902, 19064,  1997,  1996,  2951,  4520, 27347,  1999,
         1996,   102])"
922,1,"['statistical', 'table']",A Chapter ,seg_255,3.7 a number of statistical terms are shown in the following table. check if the,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0.])","tensor([1037, 3127])","tensor([ 101, 1017, 1012, 1021, 1037, 2193, 1997, 7778, 3408, 2024, 3491, 1999,
        1996, 2206, 2795, 1012, 4638, 2065, 1996,  102])"
923,1,"['location', 'parameter', 'location parameter', 'dispersion']",A Chapter ,seg_255,"terms have something to do with (a) location parameter, (b) dispersion parameter or (c) none of the above.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  3408,  2031,  2242,  2000,  2079,  2007,  1006,  1037,  1007,
         3295, 16381,  1010,  1006,  1038,  1007,  4487, 17668, 10992, 16381,
         2030,  1006,  1039,  1007,  3904,  1997,  1996,  2682,  1012,   102])"
924,1,['measurements'],A Chapter ,seg_255,3.8 measurements were taken of the concrete cover depth of a bridge column. the,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([1037, 3127])","tensor([  101,  1017,  1012,  1022, 11702,  2020,  2579,  1997,  1996,  5509,
         3104,  5995,  1997,  1037,  2958,  5930,  1012,  1996,   102])"
925,0,[],A Chapter ,seg_255,histogram of the measured values has been plotted.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([1037, 3127])","tensor([  101,  2010,  3406, 13113,  1997,  1996,  7594,  5300,  2038,  2042,
        27347,  1012,   102])"
926,1,"['mean', 'set', 'probability', 'sample', 'data set', 'data', 'sample mean']",A Chapter ,seg_255,"the sample mean, x̄, is equal to 0.16 mm. the sample mean, x̄, is equal to 15 mm. the mode of the data set is equal to 15 mm. 3.9 which of the following are features of a symmetrical probability density func-",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1996,  7099,  2812,  1010,  1060,  1010,  2003,  5020,  2000,
         1014,  1012,  2385,  3461,  1012,  1996,  7099,  2812,  1010,  1060,
         1010,  2003,  5020,  2000,  2321,  3461,  1012,  1996,  5549,  1997,
         1996,  2951,  2275,  2003,  5020,  2000,  2321,  3461,  1012,  1017,
         1012,  1023,  2029,  1997,  1996,  2206,  2024,  2838,  1997,  1037,
        23476,  9723,  4304,  4569,  2278,  1011,   102])"
927,1,"['variance', 'variation', 'coefficient', 'skewness', 'median', 'coefficient of variation']",A Chapter ,seg_255,tion? the variance is equal to the coefficient of variation. the mode is equal to the median. the skewness is equal to zero. none of the above.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([1037, 3127])","tensor([  101, 14841,  2239,  1029,  1996, 23284,  2003,  5020,  2000,  1996,
        19064,  1997,  8386,  1012,  1996,  5549,  2003,  5020,  2000,  1996,
         3991,  1012,  1996, 15315,  7974,  2791,  2003,  5020,  2000,  5717,
         1012,  3904,  1997,  1996,  2682,  1012,   102])"
928,1,"['variability', 'uncertainty']",A Chapter ,seg_257,4.1 inherent natural variability may be interpreted simply as the uncertainty which,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1018,  1012,  1015, 16112,  3019, 28436,  2089,  2022, 10009,
         3432,  2004,  1996, 12503,  2029,   102])"
929,1,"['variability', 'uncertainty', 'associated', 'statistical uncertainty', 'information', 'dependent', 'errors', 'statistical', 'model']",A Chapter ,seg_257,"cannot be reduced by means of collection of additional information. this definition implies that the amount of uncertainty due to inherent natural variability depends on the models applied in the formulation of the engineering problem. presuming that a refinement of models corresponds to looking in more detail at the problem at hand, one could say that the uncertainty structure influencing a problem is scale dependent. the type of uncertainty associated with the state of knowledge has a time dependency. in principle, if the observation is perfect without any errors the knowledge about the phenomenon is perfect. the modeling of the same phenomenon in the future, however, is uncertain as this involves models subject to natural variability, model uncertainty and statistical uncertainty. the above discussion shows another interesting effect that the uncertainty associated with a model concerning the future transforms from a mixture of aleatory and epistemic uncertainty to a purely epistemic uncertainty when the modeled phenomenon is observed (see also sect. 4.2).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  3685,  2022,  4359,  2011,  2965,  1997,  3074,  1997,  3176,
         2592,  1012,  2023,  6210, 12748,  2008,  1996,  3815,  1997, 12503,
         2349,  2000, 16112,  3019, 28436,  9041,  2006,  1996,  4275,  4162,
         1999,  1996, 20219,  1997,  1996,  3330,  3291,  1012,  3653, 17421,
         2075,  2008,  1037, 25416,  3170,  3672,  1997,  4275, 14788,  2000,
         2559,  1999,  2062,  6987,  2012,  1996,  3291,  2012,  2192,  1010,
         2028,  2071,  2360,  2008,  1996, 12503,  3252, 25870,  1037,  3291,
         2003,  4094,  7790,  1012,  1996,  2828,  1997, 12503,  3378,  2007,
         1996,  2110,  1997,  3716,  2038,  1037,  2051, 24394,  1012,  1999,
         6958,  1010,  2065,  1996,  8089,  2003,  3819,  2302,  2151, 10697,
         1996,  3716,  2055,  1996,  9575,  2003,  3819,  1012,  1996, 11643,
         1997,  1996,  2168,  9575,  1999,  1996,  2925,  1010,  2174,  1010,
         2003,  9662,  2004,  2023,  7336,  4275,  3395,  2000,  3019, 28436,
         1010,  2944, 12503,  1998,  7778, 12503,  1012,  1996,  2682,  6594,
         3065,  2178,  5875,  3466,  2008,  1996, 12503,  3378,  2007,  1037,
         2944,  7175,  1996,  2925, 21743,  2013,  1037,  8150,  1997, 15669,
        14049,  1998,  4958, 27870,  7712, 12503,  2000,  1037, 11850,  4958,
        27870,  7712, 12503,  2043,  1996, 14440,  9575,  2003,  5159,  1006,
         2156,  2036, 17831,  1012,  1018,  1012,  1016,  1007,  1012,   102])"
930,1,"['uncertainty', 'statistical uncertainty', 'statistical', 'model']",A Chapter ,seg_257,4.2 epistemic uncertainty involves statistical uncertainty and model uncertainty.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1018,  1012,  1016,  4958, 27870,  7712, 12503,  7336,  7778,
        12503,  1998,  2944, 12503,  1012,   102])"
931,1,"['continuous', 'continuous random variable', 'uncertainty', 'information', 'random variable', 'random', 'variable']",A Chapter ,seg_257,"epistemic uncertainty may be reduced by e.g. collecting additional information. on the other hand, aleatory uncertainty is related to the random nature of phenomena, and thus cannot be reduced by collecting information (see also sect. 4.2). 4.3 a continuous random variable is a random variable which can take on any",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  4958, 27870,  7712, 12503,  2089,  2022,  4359,  2011,  1041,
         1012,  1043,  1012,  9334,  3176,  2592,  1012,  2006,  1996,  2060,
         2192,  1010, 15669, 14049, 12503,  2003,  3141,  2000,  1996,  6721,
         3267,  1997, 13352,  1010,  1998,  2947,  3685,  2022,  4359,  2011,
         9334,  2592,  1006,  2156,  2036, 17831,  1012,  1018,  1012,  1016,
         1007,  1012,  1018,  1012,  1017,  1037,  7142,  6721,  8023,  2003,
         1037,  6721,  8023,  2029,  2064,  2202,  2006,  2151,   102])"
932,1,"['random', 'random variable', 'variable']",A Chapter ,seg_257,a) e[a + bx] = a + be[x] b) var[a + bx] = b2 · var[x] 4.5 the required characteristics of the random variable are shown in the following,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1037,  1007,  1041,  1031,  1037,  1009,  1038,  2595,  1033,
         1027,  1037,  1009,  2022,  1031,  1060,  1033,  1038,  1007, 13075,
         1031,  1037,  1009,  1038,  2595,  1033,  1027,  1038,  2475,  1087,
        13075,  1031,  1060,  1033,  1018,  1012,  1019,  1996,  3223,  6459,
         1997,  1996,  6721,  8023,  2024,  3491,  1999,  1996,  2206,   102])"
933,1,"['probability', 'limit', 'distribution', 'central limit theorem', 'probability distribution']",A Chapter ,seg_257,"4.6 according to the central limit theorem, “the probability distribution for the",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([1037, 3127])","tensor([ 101, 1018, 1012, 1020, 2429, 2000, 1996, 2430, 5787, 9872, 1010, 1523,
        1996, 9723, 4353, 2005, 1996,  102])"
934,1,"['normal distribution', 'standard normal distribution', 'standard normal', 'random', 'normal', 'random variables', 'standard', 'distribution', 'variables', 'case']",A Chapter ,seg_257,sum of a number of random variables approaches the normal distribution as the number becomes large”. 4.7 the standard normal distribution is a special case of the normal distribution.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.])","tensor([1037, 3127])","tensor([  101,  7680,  1997,  1037,  2193,  1997,  6721, 10857,  8107,  1996,
         3671,  4353,  2004,  1996,  2193,  4150,  2312,  1524,  1012,  1018,
         1012,  1021,  1996,  3115,  3671,  4353,  2003,  1037,  2569,  2553,
         1997,  1996,  3671,  4353,  1012,   102])"
935,1,"['variance', 'random variable', 'transformed', 'random', 'standardized', 'expected value', 'variable']",A Chapter ,seg_257,a standardized random variable is a random variable that has been transformed such as its expected value is equal to zero and its variance is equal to one (see also eq. 4.48).,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1037, 16367,  6721,  8023,  2003,  1037,  6721,  8023,  2008,
         2038,  2042,  8590,  2107,  2004,  2049,  3517,  3643,  2003,  5020,
         2000,  5717,  1998,  2049, 23284,  2003,  5020,  2000,  2028,  1006,
         2156,  2036,  1041,  4160,  1012,  1018,  1012,  4466,  1007,  1012,
          102])"
936,1,"['mutually exclusive', 'experiments']",A Chapter ,seg_257,4.8 a sequence of experiments with only two possible mutually exclusive out-,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1018,  1012,  1022,  1037,  5537,  1997,  7885,  2007,  2069,
         2048,  2825, 20271,  7262,  2041,  1011,   102])"
937,1,"['trial', 'discrete', 'bernoulli', 'processes', 'events', 'failure', 'trials', 'poisson', 'bernoulli trials', 'poisson process', 'process', 'bernoulli trial', 'success']",A Chapter ,seg_257,"comes is called a sequence of bernoulli trials. typically, the two possible events of a bernoulli trial are referred to as a success or a failure (see also sect. 4.4). 4.9 poisson process is a family of discrete processes, which may be used for mod-",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  3310,  2003,  2170,  1037,  5537,  1997, 16595,  7140,  6894,
         7012,  1012,  4050,  1010,  1996,  2048,  2825,  2824,  1997,  1037,
        16595,  7140,  6894,  3979,  2024,  3615,  2000,  2004,  1037,  3112,
         2030,  1037,  4945,  1006,  2156,  2036, 17831,  1012,  1018,  1012,
         1018,  1007,  1012,  1018,  1012,  1023, 13433, 24077,  2832,  2003,
         1037,  2155,  1997, 16246,  6194,  1010,  2029,  2089,  2022,  2109,
         2005, 16913,  1011,   102])"
938,1,"['probability', 'events']",A Chapter ,seg_257,eling the number of occurrences of events (see also sect. 4.3). 4.10 the probability of exceeding the value of 5 is calculated as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101, 12005,  3070,  1996,  2193,  1997, 27247,  1997,  2824,  1006,
         2156,  2036, 17831,  1012,  1018,  1012,  1017,  1007,  1012,  1018,
         1012,  2184,  1996,  9723,  1997, 17003,  1996,  3643,  1997,  1019,
         2003, 10174,  2004,  1024,   102])"
939,1,['probability'],A Chapter ,seg_257,4.11 the probability that the engine breaks down within 2 years after placed in,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([ 101, 1018, 1012, 2340, 1996, 9723, 2008, 1996, 3194, 7807, 2091, 2306,
        1016, 2086, 2044, 2872, 1999,  102])"
940,0,[],A Chapter ,seg_257,operation is calculated as:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0])","tensor([1037, 3127])","tensor([  101,  3169,  2003, 10174,  2004,  1024,   102])"
941,1,['probability'],A Chapter ,seg_257,4.12 the probability of no snowfall in the next year is equal to 0.067. the probabil-,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1018,  1012,  2260,  1996,  9723,  1997,  2053, 26043,  1999,
         1996,  2279,  2095,  2003,  5020,  2000,  1014,  1012,  5757,  2581,
         1012,  1996,  4013,  3676, 14454,  1011,   102])"
942,1,['probabilities'],A Chapter ,seg_257,ity of exactly 5 snowfalls in the next year is equal to 0.176. the probabilities are calculated as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2009,  2100,  1997,  3599,  1019, 26043,  2015,  1999,  1996,
         2279,  2095,  2003,  5020,  2000,  1014,  1012, 18561,  1012,  1996,
         4013,  3676, 14680,  2024, 10174,  2004,  1024,   102])"
943,1,['data'],A Chapter ,seg_259,5.1 the data seem to fit well on a straight line and hence the assumption of a,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1019,  1012,  1015,  1996,  2951,  4025,  2000,  4906,  2092,
         2006,  1037,  3442,  2240,  1998,  6516,  1996, 11213,  1997,  1037,
          102])"
944,1,"['method', 'maximum likelihood', 'likelihood', 'distribution']",A Chapter ,seg_259,gumbel distribution can be accepted by the engineer. 5.2 the maximum likelihood method (mlm) enables engineers to calculate the,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101, 16031,  8671,  4353,  2064,  2022,  3970,  2011,  1996,  3992,
         1012,  1019,  1012,  1016,  1996,  4555, 16593,  4118,  1006, 19875,
         2213,  1007, 12939,  6145,  2000, 18422,  1996,   102])"
945,1,"['uncertainty', 'associated', 'estimated', 'information', 'random variable', 'estimates', 'random', 'distribution', 'point estimates', 'parameters', 'data', 'variable']",A Chapter ,seg_259,distribution parameters of a random variable on the basis of data. which of the following statement(s) is(are) correct? the mlm provides point estimates of the distribution parameters. the mlm provides information about the uncertainty associated with the estimated parameters. the mlm provides no information about the uncertainty associated with the estimated parameters. 5.3 from past experience it is known that the shear strength of soil can be de-,tensor(1),"tensor([0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  4353, 11709,  1997,  1037,  6721,  8023,  2006,  1996,  3978,
         1997,  2951,  1012,  2029,  1997,  1996,  2206,  4861,  1006,  1055,
         1007,  2003,  1006,  2024,  1007,  6149,  1029,  1996, 19875,  2213,
         3640,  2391, 10035,  1997,  1996,  4353, 11709,  1012,  1996, 19875,
         2213,  3640,  2592,  2055,  1996, 12503,  3378,  2007,  1996,  4358,
        11709,  1012,  1996, 19875,  2213,  3640,  2053,  2592,  2055,  1996,
        12503,  3378,  2007,  1996,  4358, 11709,  1012,  1019,  1012,  1017,
         2013,  2627,  3325,  2009,  2003,  2124,  2008,  1996, 18330,  3997,
         1997,  5800,  2064,  2022,  2139,  1011,   102])"
946,1,"['method of moments', 'probability paper', 'probability', 'lognormal', 'estimate', 'maximum likelihood', 'method', 'samples', 'moments', 'parameters', 'likelihood', 'distribution', 'lognormal distribution', 'data']",A Chapter ,seg_259,scribed by a lognormal distribution. 15 samples of soil are taken from a site and an engineer wants to use the data in order to estimate the parameters of the lognormal distribution. the engineer: may use a probability paper to estimate the parameters of the lognormal distribution. may use the maximum likelihood method to estimate the parameters of the lognormal distribution. may use the method of moments to estimate the parameters of the lognormal distribution. none of the above.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101, 27789,  2094,  2011,  1037,  8833, 12131,  9067,  4353,  1012,
         2321,  8168,  1997,  5800,  2024,  2579,  2013,  1037,  2609,  1998,
         2019,  3992,  4122,  2000,  2224,  1996,  2951,  1999,  2344,  2000,
        10197,  1996, 11709,  1997,  1996,  8833, 12131,  9067,  4353,  1012,
         1996,  3992,  1024,  2089,  2224,  1037,  9723,  3259,  2000, 10197,
         1996, 11709,  1997,  1996,  8833, 12131,  9067,  4353,  1012,  2089,
         2224,  1996,  4555, 16593,  4118,  2000, 10197,  1996, 11709,  1997,
         1996,  8833, 12131,  9067,  4353,  1012,  2089,  2224,  1996,  4118,
         1997,  5312,  2000, 10197,  1996, 11709,  1997,  1996,  8833, 12131,
         9067,  4353,  1012,  3904,  1997,  1996,  2682,  1012,   102])"
947,1,"['model', 'probabilistic model', 'probabilistic']",A Chapter ,seg_259,"5.4 the procedure of establishing a probabilistic model, as described in sect. 5.1,",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1019,  1012,  1018,  1996,  7709,  1997,  7411,  1037,  4013,
         3676, 27965,  4588,  2944,  1010,  2004,  2649,  1999, 17831,  1012,
         1019,  1012,  1015,  1010,   102])"
948,1,"['parameter', 'function', 'estimation', 'data', 'probability', 'distribution function', 'sample', 'sample average', 'statistical', 'distribution', 'model', 'average']",A Chapter ,seg_259,consists of five steps: 1) assessment and statistical quantification of the available data 2) selection of distribution function 3) estimation of distribution parameter 4) model verification and 5) model updating. 5.5 the probability that the sample average of the steel yield stress will lie within,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  3774,  1997,  2274,  4084,  1024,  1015,  1007,  7667,  1998,
         7778, 24110,  3775, 10803,  1997,  1996,  2800,  2951,  1016,  1007,
         4989,  1997,  4353,  3853,  1017,  1007, 24155,  1997,  4353, 16381,
         1018,  1007,  2944, 22616,  1998,  1019,  1007,  2944,  2039, 16616,
         1012,  1019,  1012,  1019,  1996,  9723,  2008,  1996,  7099,  2779,
         1997,  1996,  3886, 10750,  6911,  2097,  4682,  2306,   102])"
949,1,"['mean', 'interval', 'hypothesis', 'hypothesis testing']",A Chapter ,seg_259,"an interval of ±9.8 mpa of the true mean value μx is 0.95 (see also sect. 5.3, eq. 5.22). 5.6 the hypothesis testing procedure, as described also in sect. 5.4, consists of",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2019, 13483,  1997,  1081,  2683,  1012,  1022,  6131,  2050,
         1997,  1996,  2995,  2812,  3643,  1166,  2595,  2003,  1014,  1012,
         5345,  1006,  2156,  2036, 17831,  1012,  1019,  1012,  1017,  1010,
         1041,  4160,  1012,  1019,  1012,  2570,  1007,  1012,  1019,  1012,
         1020,  1996, 10744,  5604,  7709,  1010,  2004,  2649,  2036,  1999,
        17831,  1012,  1019,  1012,  1018,  1010,  3774,  1997,   102])"
950,1,"['mean', 'significance level', 'operating rule', 'probability', 'hypothesis', 'level', 'sample', 'tests', 'sample statistic', 'type i error', 'null hypothesis', 'significance', 'statistic', 'error']",A Chapter ,seg_259,"the following steps/actions: 1) formulate a null hypothesis, h0 2) formulate an operating rule, h1 3) select a significance level, α 4) identify the value resulting in a probability α of performing a type i error 5) perform the testing, obtain the sample statistic 6) judge the null hypothesis. 5.7 an engineer tests the null hypothesis that the mean value of the concrete cover",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1996,  2206,  4084,  1013,  4506,  1024,  1015,  1007,  5675,
         2618,  1037, 19701, 10744,  1010,  1044,  2692,  1016,  1007,  5675,
         2618,  2019,  4082,  3627,  1010,  1044,  2487,  1017,  1007,  7276,
         1037,  7784,  2504,  1010,  1155,  1018,  1007,  6709,  1996,  3643,
         4525,  1999,  1037,  9723,  1155,  1997,  4488,  1037,  2828,  1045,
         7561,  1019,  1007,  4685,  1996,  5604,  1010,  6855,  1996,  7099,
        28093,  6553,  1020,  1007,  3648,  1996, 19701, 10744,  1012,  1019,
         1012,  1021,  2019,  3992,  5852,  1996, 19701, 10744,  2008,  1996,
         2812,  3643,  1997,  1996,  5509,  3104,   102])"
951,1,"['mean', 'probability papers', 'type ii error', 'null hypothesis', 'probability', 'design', 'type ii', 'hypothesis', 'type i error', 'hypothesis test', 'measurements', 'error', 'test']",A Chapter ,seg_259,"depth of a concrete structure corresponds to design assumptions. in a preliminary assessment a limited number of measurements of the concrete cover depth are made, and after performing the hypothesis test the engineer accepts the null hypothesis. after a few years, a comprehensive survey of the concrete cover depth is carried out, i.e. many measurements are made. the survey shows that the mean value of the concrete cover depth does not fulfill the design assumptions. which of the following statement(s) is(are) correct? in the preliminary survey the engineer has performed a type i error. in the preliminary survey the engineer has performed a type ii error. in the preliminary survey the engineer has performed a type i and a type ii error. 5.8 probability papers are useful for checking the plausibility of a selected distri-",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([1037, 3127])","tensor([  101,  5995,  1997,  1037,  5509,  3252, 14788,  2000,  2640, 17568,
         1012,  1999,  1037,  8824,  7667,  1037,  3132,  2193,  1997, 11702,
         1997,  1996,  5509,  3104,  5995,  2024,  2081,  1010,  1998,  2044,
         4488,  1996, 10744,  3231,  1996,  3992, 13385,  1996, 19701, 10744,
         1012,  2044,  1037,  2261,  2086,  1010,  1037,  7721,  5002,  1997,
         1996,  5509,  3104,  5995,  2003,  3344,  2041,  1010,  1045,  1012,
         1041,  1012,  2116, 11702,  2024,  2081,  1012,  1996,  5002,  3065,
         2008,  1996,  2812,  3643,  1997,  1996,  5509,  3104,  5995,  2515,
         2025, 13883,  1996,  2640, 17568,  1012,  2029,  1997,  1996,  2206,
         4861,  1006,  1055,  1007,  2003,  1006,  2024,  1007,  6149,  1029,
         1999,  1996,  8824,  5002,  1996,  3992,  2038,  2864,  1037,  2828,
         1045,  7561,  1012,  1999,  1996,  8824,  5002,  1996,  3992,  2038,
         2864,  1037,  2828,  2462,  7561,  1012,  1999,  1996,  8824,  5002,
         1996,  3992,  2038,  2864,  1037,  2828,  1045,  1998,  1037,  2828,
         2462,  7561,  1012,  1019,  1012,  1022,  9723,  4981,  2024,  6179,
         2005,  9361,  1996, 20228, 20559, 13464,  1997,  1037,  3479,  4487,
         3367,  3089,  1011,   102])"
952,1,"['complement', 'function', 'probability paper', 'probability distribution function', 'probability', 'interval', 'distribution', 'distribution function', 'data', 'probability distribution']",A Chapter ,seg_259,bution family. a probability paper for a given distribution family is constructed such that the cumulative probability distribution function (or the complement) for that distribution family will have the shape of a straight line when plotted on the paper. a probability paper is thus constructed by a non-linear transformation of the y-axis (see also sect. 5.2). 5.9 it is suggested that the data are lumped in a way that each interval con-,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2021,  3258,  2155,  1012,  1037,  9723,  3259,  2005,  1037,
         2445,  4353,  2155,  2003,  3833,  2107,  2008,  1996, 23260,  9723,
         4353,  3853,  1006,  2030,  1996, 13711,  1007,  2005,  2008,  4353,
         2155,  2097,  2031,  1996,  4338,  1997,  1037,  3442,  2240,  2043,
        27347,  2006,  1996,  3259,  1012,  1037,  9723,  3259,  2003,  2947,
         3833,  2011,  1037,  2512,  1011,  7399,  8651,  1997,  1996,  1061,
         1011,  8123,  1006,  2156,  2036, 17831,  1012,  1019,  1012,  1016,
         1007,  1012,  1019,  1012,  1023,  2009,  2003,  4081,  2008,  1996,
         2951,  2024, 15116,  2098,  1999,  1037,  2126,  2008,  2169, 13483,
         9530,  1011,   102])"
953,1,"['continuous', 'function', 'observations', 'tests', 'continuous distribution', 'distribution', 'model', 'distribution function', 'goodness of fit', 'data']",A Chapter ,seg_259,"tains about 5 or more observations. if the data are realizations from a continuous distribution function, then they must be descritized (see also sect. 5.9). 5.10 both tests are used to assess the goodness of fit of the assumed model with",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 1., 0., 0.])","tensor([1037, 3127])","tensor([  101, 13843,  3619,  2055,  1019,  2030,  2062,  9420,  1012,  2065,
         1996,  2951,  2024, 12393,  2015,  2013,  1037,  7142,  4353,  3853,
         1010,  2059,  2027,  2442,  2022,  4078, 26775, 25090,  5422,  1006,
         2156,  2036, 17831,  1012,  1019,  1012,  1023,  1007,  1012,  1019,
         1012,  2184,  2119,  5852,  2024,  2109,  2000, 14358,  1996, 15003,
         1997,  4906,  1997,  1996,  5071,  2944,  2007,   102])"
954,1,"['continuous', 'function', 'functions', 'continuous distribution', 'distribution', 'distribution function', 'discrete distribution', 'discrete', 'test']",A Chapter ,seg_259,"data. the chi-square test is used basically for discrete distribution functions, while the kolmogorov-smirnov test is used for continuous distribution functions. however, by the descretization of a continuous distribution function, the chi-square test can be used also for the continuous distribution functions. another difference is that whereas the chi-square test can",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2951,  1012,  1996,  9610,  1011,  2675,  3231,  2003,  2109,
        10468,  2005, 16246,  4353,  4972,  1010,  2096,  1996, 12849, 13728,
        22844, 12298,  1011, 15488,  4313, 16693,  3231,  2003,  2109,  2005,
         7142,  4353,  4972,  1012,  2174,  1010,  2011,  1996,  4078, 16748,
         3775,  9276,  1997,  1037,  7142,  4353,  3853,  1010,  1996,  9610,
         1011,  2675,  3231,  2064,  2022,  2109,  2036,  2005,  1996,  7142,
         4353,  4972,  1012,  2178,  4489,  2003,  2008,  6168,  1996,  9610,
         1011,  2675,  3231,  2064,   102])"
955,1,"['fit test', 'cases', 'estimated', 'distribution', 'parameters', 'goodness of fit', 'data', 'test']",A Chapter ,seg_259,"be applied for the cases where the distribution parameters are already estimated from the same data, the kolmogorov-smirnov test cannot be applied when the distribution parameters are estimated from the same data (see also sect. 5.9). 5.11 following the introduction of the χ2 goodness of fit test in sect. 5.9, the",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([1037, 3127])","tensor([  101,  2022,  4162,  2005,  1996,  3572,  2073,  1996,  4353, 11709,
         2024,  2525,  4358,  2013,  1996,  2168,  2951,  1010,  1996, 12849,
        13728, 22844, 12298,  1011, 15488,  4313, 16693,  3231,  3685,  2022,
         4162,  2043,  1996,  4353, 11709,  2024,  4358,  2013,  1996,  2168,
         2951,  1006,  2156,  2036, 17831,  1012,  1019,  1012,  1023,  1007,
         1012,  1019,  1012,  2340,  2206,  1996,  4955,  1997,  1996,  1177,
         2475, 15003,  1997,  4906,  3231,  1999, 17831,  1012,  1019,  1012,
         1023,  1010,  1996,   102])"
956,1,"['degrees of freedom', 'sample', 'sample statistic', 'statistic']",A Chapter ,seg_259,number of degrees of freedom of the chi-square sample statistic εm,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2193,  1997,  5445,  1997,  4071,  1997,  1996,  9610,  1011,
         2675,  7099, 28093,  6553,  1159,  2213,   102])"
957,1,"['operating rule', 'hypothesis', 'normal', 'intervals', 'null hypothesis', 'parameters']",A Chapter ,seg_259,"k − 1 = 3 − 1 = 2, where k the number of intervals into which the samples were divided. the null hypothesis ho that x follows a normal distribution with the given parameters can be tested using the following operating rule:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1047,  1597,  1015,  1027,  1017,  1597,  1015,  1027,  1016,
         1010,  2073,  1047,  1996,  2193,  1997, 14025,  2046,  2029,  1996,
         8168,  2020,  4055,  1012,  1996, 19701, 10744,  7570,  2008,  1060,
         4076,  1037,  3671,  4353,  2007,  1996,  2445, 11709,  2064,  2022,
         7718,  2478,  1996,  2206,  4082,  3627,  1024,   102])"
958,1,"['significance level', 'table', 'hypothesis', 'critical value', 'level', 'sample', 'sample statistic', 'null hypothesis', 'significance', 'statistic']",A Chapter ,seg_259,"where δ is the critical value with the sample statistic shall be compared. using table c.3 and for a significance level of 5% it is observed that δ = 5.9915, a value that is larger than εm 2 = 0.41. hence the null hypothesis cannot be rejected at the 5% significance level. 5.12 an engineer wants to examine and compare the suitability of two distri-",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2073,  1158,  2003,  1996,  4187,  3643,  2007,  1996,  7099,
        28093,  6553,  4618,  2022,  4102,  1012,  2478,  2795,  1039,  1012,
         1017,  1998,  2005,  1037,  7784,  2504,  1997,  1019,  1003,  2009,
         2003,  5159,  2008,  1158,  1027,  1019,  1012,  5585, 16068,  1010,
         1037,  3643,  2008,  2003,  3469,  2084,  1159,  2213,  1016,  1027,
         1014,  1012,  4601,  1012,  6516,  1996, 19701, 10744,  3685,  2022,
         5837,  2012,  1996,  1019,  1003,  7784,  2504,  1012,  1019,  1012,
         2260,  2019,  3992,  4122,  2000, 11628,  1998, 12826,  1996,  4848,
         8010,  1997,  2048,  4487,  3367,  3089,  1011,   102])"
959,1,"['function', 'sample statistics', 'sample', 'random', 'results', 'statistics', 'model']",A Chapter ,seg_259,bution function model alternatives for a random material property. measurements are taken of the material property. the engineer uses the two model alternatives to calculate the chi-square sample statistics and the corresponding sample likelihoods. the results are given in the following table:,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2021,  3258,  3853,  2944, 15955,  2005,  1037,  6721,  3430,
         3200,  1012, 11702,  2024,  2579,  1997,  1996,  3430,  3200,  1012,
         1996,  3992,  3594,  1996,  2048,  2944, 15955,  2000, 18422,  1996,
         9610,  1011,  2675,  7099,  6747,  1998,  1996,  7978,  7099, 16593,
         2015,  1012,  1996,  3463,  2024,  2445,  1999,  1996,  2206,  2795,
         1024,   102])"
960,1,"['model', 'significance', 'significance level', 'level']",A Chapter ,seg_259,which of the following statement(s) is(are) correct? the engineer may accept model 1 at the 5% significance level. the engineer may accept model 2 at the 5% significance level. model 1 is more suitable than model 2. none of the above.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([1037, 3127])","tensor([ 101, 2029, 1997, 1996, 2206, 4861, 1006, 1055, 1007, 2003, 1006, 2024,
        1007, 6149, 1029, 1996, 3992, 2089, 5138, 2944, 1015, 2012, 1996, 1019,
        1003, 7784, 2504, 1012, 1996, 3992, 2089, 5138, 2944, 1016, 2012, 1996,
        1019, 1003, 7784, 2504, 1012, 2944, 1015, 2003, 2062, 7218, 2084, 2944,
        1016, 1012, 3904, 1997, 1996, 2682, 1012,  102])"
961,1,"['events', 'failure', 'failure events']",A Chapter ,seg_261,6.1 it is convenient to describe failure events in terms of functional relations. if,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([1037, 3127])","tensor([  101,  1020,  1012,  1015,  2009,  2003, 14057,  2000,  6235,  4945,
         2824,  1999,  3408,  1997,  8360,  4262,  1012,  2065,   102])"
962,1,"['probability of failure', 'function', 'probability', 'uncertainties', 'failure', 'event', 'limit state function', 'random', 'random variables', 'limit', 'basic random variables', 'variables', 'failure event']",A Chapter ,seg_261,"fulfilled, the considered event will occur. a failure event may be described by a functional relation, the limit state function g(x), such as: f = {g(x) ≤ 0}, where the components of the vector x are realizations of the so-called basic random variables x representing all the relevant uncertainties influencing the probability of failure (see also sect. 6.2).",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101, 16829,  1010,  1996,  2641,  2724,  2097,  5258,  1012,  1037,
         4945,  2724,  2089,  2022,  2649,  2011,  1037,  8360,  7189,  1010,
         1996,  5787,  2110,  3853,  1043,  1006,  1060,  1007,  1010,  2107,
         2004,  1024,  1042,  1027,  1063,  1043,  1006,  1060,  1007,  1608,
         1014,  1065,  1010,  2073,  1996,  6177,  1997,  1996,  9207,  1060,
         2024, 12393,  2015,  1997,  1996,  2061,  1011,  2170,  3937,  6721,
        10857,  1060,  5052,  2035,  1996,  7882,  9662,  7368, 25870,  1996,
         9723,  1997,  4945,  1006,  2156,  2036, 17831,  1012,  1020,  1012,
         1016,  1007,  1012,   102])"
963,0,[],A Chapter ,seg_261,6.2 the reliability index may be defined as the shortest distance between the,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([1037, 3127])","tensor([  101,  1020,  1012,  1016,  1996, 15258,  5950,  2089,  2022,  4225,
         2004,  1996, 20047,  3292,  2090,  1996,   102])"
964,1,"['probability of failure', 'function', 'probability', 'estimate', 'failure', 'limit state function', 'limit']",A Chapter ,seg_261,curve represented by the limit state function and the origin. the reliability index β is related to the probability of failure pf as: pf = φ(−β) (see also sect. 6.3). 6.3 the estimate of the failure probability becomes exact as the number of simula-,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  7774,  3421,  2011,  1996,  5787,  2110,  3853,  1998,  1996,
         4761,  1012,  1996, 15258,  5950,  1156,  2003,  3141,  2000,  1996,
         9723,  1997,  4945,  1052,  2546,  2004,  1024,  1052,  2546,  1027,
         1176,  1006,  1597, 29720,  1007,  1006,  2156,  2036, 17831,  1012,
         1020,  1012,  1017,  1007,  1012,  1020,  1012,  1017,  1996, 10197,
         1997,  1996,  4945,  9723,  4150,  6635,  2004,  1996,  2193,  1997,
        21934,  7068,  1011,   102])"
965,1,"['mean', 'deviation', 'standard deviation', 'expectation', 'standard', 'safety margin', 'expectation operator']",A Chapter ,seg_261,σm mean μm and standard deviation σm of the safety margin m = r − l can be calculated by applying the properties of the expectation operator (see sect. 4.3) on the safety margin expression. this gives:,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1173,  2213,  2812,  1166,  2213,  1998,  3115, 24353,  1173,
         2213,  1997,  1996,  3808,  7785,  1049,  1027,  1054,  1597,  1048,
         2064,  2022, 10174,  2011, 11243,  1996,  5144,  1997,  1996, 17626,
         6872,  1006,  2156, 17831,  1012,  1018,  1012,  1017,  1007,  2006,
         1996,  3808,  7785,  3670,  1012,  2023,  3957,  1024,   102])"
966,0,[],A Chapter ,seg_261,hence the reliability index is equal to:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([1037, 3127])","tensor([  101,  6516,  1996, 15258,  5950,  2003,  5020,  2000,  1024,   102])"
967,1,"['probability', 'probability of failure', 'failure']",A Chapter ,seg_261,the annual probability of failure of the timber beam is:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([ 101, 1996, 3296, 9723, 1997, 4945, 1997, 1996, 7227, 7504, 2003, 1024,
         102])"
968,1,"['safety margin', 'table']",A Chapter ,seg_261,"where φ(−3.9) = 4.8 · 10−5, can be found from table c.1. 6.5 the safety margin can be written as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2073,  1176,  1006,  1597,  2509,  1012,  1023,  1007,  1027,
         1018,  1012,  1022,  1087,  2184, 22543,  2629,  1010,  2064,  2022,
         2179,  2013,  2795,  1039,  1012,  1015,  1012,  1020,  1012,  1019,
         1996,  3808,  7785,  2064,  2022,  2517,  2004,  1024,   102])"
969,1,"['mean', 'deviation', 'normal', 'standard deviation', 'standard']",A Chapter ,seg_261,"since the yield stress fy is normal distributed, m is also normal distributed and its mean and standard deviation can be calculated as follows:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2144,  1996, 10750,  6911,  1042,  2100,  2003,  3671,  5500,
         1010,  1049,  2003,  2036,  3671,  5500,  1998,  2049,  2812,  1998,
         3115, 24353,  2064,  2022, 10174,  2004,  4076,  1024,   102])"
970,1,['variance'],A Chapter ,seg_261,the variance is calculated as:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1996, 23284,  2003, 10174,  2004,  1024,   102])"
971,1,"['deviation', 'standard deviation', 'standard']",A Chapter ,seg_261,the standard deviation is then:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1996,  3115, 24353,  2003,  2059,  1024,   102])"
972,1,"['probability', 'probability of failure', 'failure']",A Chapter ,seg_261,"the probability of failure of the rod is then (following eq. 6.8, sect. 6.3):",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1996,  9723,  1997,  4945,  1997,  1996,  8473,  2003,  2059,
         1006,  2206,  1041,  4160,  1012,  1020,  1012,  1022,  1010, 17831,
         1012,  1020,  1012,  1017,  1007,  1024,   102])"
973,0,[],A Chapter ,seg_261,whereas the reliability of the rod is simply:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([1037, 3127])","tensor([  101,  6168,  1996, 15258,  1997,  1996,  8473,  2003,  3432,  1024,
          102])"
974,1,"['function', 'normal distribution', 'density function', 'standard normal distribution', 'table', 'probability', 'probability density function', 'standard normal', 'normal', 'standard', 'safety margin', 'distribution']",A Chapter ,seg_261,(note: the standard normal distribution value corresponding to −3 is taken from table c.1. it is easier to draw the probability density function of the standardized safety margin i.e. of zm . the area under the density function to the right of −3 in the x-axis represents the safe region.),tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1006,  3602,  1024,  1996,  3115,  3671,  4353,  3643,  7978,
         2000,  1597,  2509,  2003,  2579,  2013,  2795,  1039,  1012,  1015,
         1012,  2009,  2003,  6082,  2000,  4009,  1996,  9723,  4304,  3853,
         1997,  1996, 16367,  3808,  7785,  1045,  1012,  1041,  1012,  1997,
         1062,  2213,  1012,  1996,  2181,  2104,  1996,  4304,  3853,  2000,
         1996,  2157,  1997,  1597,  2509,  1999,  1996,  1060,  1011,  8123,
         5836,  1996,  3647,  2555,  1012,  1007,   102])"
975,0,[],A Chapter ,seg_261,"6.6 using fig. 6.5 and basic principles of geometry,",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([1037, 3127])","tensor([  101,  1020,  1012,  1020,  2478, 20965,  1012,  1020,  1012,  1019,
         1998,  3937,  6481,  1997, 10988,  1010,   102])"
976,1,"['expectation operator', 'expectation']",A Chapter ,seg_261,"using the properties of the expectation operator (see sect. 4.3),",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2478,  1996,  5144,  1997,  1996, 17626,  6872,  1006,  2156,
        17831,  1012,  1018,  1012,  1017,  1007,  1010,   102])"
977,1,"['deviation', 'associated', 'estimated', 'measurement', 'error']",A Chapter ,seg_261,the error associated with the measurement of side is represented by the standard deviation σ [b] and is estimated as:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1996,  7561,  3378,  2007,  1996, 10903,  1997,  2217,  2003,
         3421,  2011,  1996,  3115, 24353,  1173,  1031,  1038,  1033,  1998,
         2003,  4358,  2004,  1024,   102])"
978,0,[],A Chapter ,seg_261,and eventually:,tensor(0),"tensor([0, 0, 0, 0, 0])","tensor([1037, 3127])","tensor([ 101, 1998, 2776, 1024,  102])"
979,1,['error'],A Chapter ,seg_261,the error in b is calculated by:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1996,  7561,  1999,  1038,  2003, 10174,  2011,  1024,   102])"
980,1,"['consequences', 'events', 'probabilistic', 'probabilistic models']",A Chapter ,seg_263,7.1 the probabilistic models concerning events of interest and the consequences,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.])","tensor([1037, 3127])","tensor([  101,  1021,  1012,  1015,  1996,  4013,  3676, 27965,  4588,  4275,
         7175,  2824,  1997,  3037,  1998,  1996,  8465,   102])"
981,1,"['function', 'utility', 'event', 'numerical']",A Chapter ,seg_263,for each event and action. 7.2 utility function is a numerical assessment of the preferences of the decision,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2005,  2169,  2724,  1998,  2895,  1012,  1021,  1012,  1016,
         9710,  3853,  2003,  1037, 15973,  7667,  1997,  1996, 18394,  1997,
         1996,  3247,   102])"
982,1,"['information', 'prior decision analysis']",A Chapter ,seg_263,maker (see also sect. 7.3). 7.3 prior decision analysis is based on existing information and experience for a,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  9338,  1006,  2156,  2036, 17831,  1012,  1021,  1012,  1017,
         1007,  1012,  1021,  1012,  1017,  3188,  3247,  4106,  2003,  2241,
         2006,  4493,  2592,  1998,  3325,  2005,  1037,   102])"
983,1,"['posterior decision analysis', 'posterior', 'decision problem', 'probability', 'events', 'estimate', 'information', 'probabilities']",A Chapter ,seg_263,first estimate of the probability of the considered events. in posterior decision analysis new information is used to update the above probabilities and carry out a reassessment of the decision problem (see also sects. 7.6 and 7.7). 7.4 in pre-posterior decision analysis the decision maker can evaluate whether it is,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  2034, 10197,  1997,  1996,  9723,  1997,  1996,  2641,  2824,
         1012,  1999, 15219,  3247,  4106,  2047,  2592,  2003,  2109,  2000,
        10651,  1996,  2682,  4013,  3676, 14680,  1998,  4287,  2041,  1037,
         2128, 27241,  4757,  3672,  1997,  1996,  3247,  3291,  1006,  2156,
         2036, 17831,  2015,  1012,  1021,  1012,  1020,  1998,  1021,  1012,
         1021,  1007,  1012,  1021,  1012,  1018,  1999,  3653,  1011, 15219,
         3247,  4106,  1996,  3247,  9338,  2064, 16157,  3251,  2009,  2003,
          102])"
984,1,['information'],A Chapter ,seg_263,"useful or not to “buy” new information that will enable to make her/his final decision (see also sect. 7.8). 7.5 using the information provided,",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  6179,  2030,  2025,  2000,  1523,  4965,  1524,  2047,  2592,
         2008,  2097,  9585,  2000,  2191,  2014,  1013,  2010,  2345,  3247,
         1006,  2156,  2036, 17831,  1012,  1021,  1012,  1022,  1007,  1012,
         1021,  1012,  1019,  2478,  1996,  2592,  3024,  1010,   102])"
985,0,[],A Chapter ,seg_263,"using the bayes’ rule,",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([1037, 3127])","tensor([ 101, 2478, 1996, 3016, 2229, 1521, 3627, 1010,  102])"
986,1,"['events', 'associated', 'event']",A Chapter ,seg_263,the event tree can now be completed. an example of calculation is provided in the following. consider the branch associated with the activity “clean up the roof”. if the roof is cleaned up there are two events that may occur according to the problem:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([  101,  1996,  2724,  3392,  2064,  2085,  2022,  2949,  1012,  2019,
         2742,  1997, 17208,  2003,  3024,  1999,  1996,  2206,  1012,  5136,
         1996,  3589,  3378,  2007,  1996,  4023,  1523,  4550,  2039,  1996,
         4412,  1524,  1012,  2065,  1996,  4412,  2003, 12176,  2039,  2045,
         2024,  2048,  2824,  2008,  2089,  5258,  2429,  2000,  1996,  3291,
         1024,   102])"
987,0,[],A Chapter ,seg_263,• the roof may collapse (due to various reasons) • the roof will not collapse (survival of the roof),tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0])","tensor([1037, 3127])","tensor([ 101, 1528, 1996, 4412, 2089, 7859, 1006, 2349, 2000, 2536, 4436, 1007,
        1528, 1996, 4412, 2097, 2025, 7859, 1006, 7691, 1997, 1996, 4412, 1007,
         102])"
988,1,"['associated', 'probability', 'events', 'event']",A Chapter ,seg_263,these events are associated with some probability as shown in the event tree branches:,tensor(1),"tensor([0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([ 101, 2122, 2824, 2024, 3378, 2007, 2070, 9723, 2004, 3491, 1999, 1996,
        2724, 3392, 5628, 1024,  102])"
989,0,[],A Chapter ,seg_263,hence the expected cost of this action is:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([1037, 3127])","tensor([ 101, 6516, 1996, 3517, 3465, 1997, 2023, 2895, 2003, 1024,  102])"
990,1,['event'],A Chapter ,seg_263,in a similar way the rest of the event tree may be completed.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([1037, 3127])","tensor([ 101, 1999, 1037, 2714, 2126, 1996, 2717, 1997, 1996, 2724, 3392, 2089,
        2022, 2949, 1012,  102])"
991,1,['associated'],A Chapter ,seg_263,it can be seen that the action associated with the smaller cost is not to clean up the roof.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([1037, 3127])","tensor([ 101, 2009, 2064, 2022, 2464, 2008, 1996, 2895, 3378, 2007, 1996, 3760,
        3465, 2003, 2025, 2000, 4550, 2039, 1996, 4412, 1012,  102])"
992,1,"['normal distribution', 'standard normal distribution', 'standard normal', 'normal', 'standard', 'distribution']",B Chapter  B Equation ,seg_269,calculation of the standard normal distribution φ(·):,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([1038, 3127, 1038, 8522])","tensor([  101, 17208,  1997,  1996,  3115,  3671,  4353,  1176,  1006,  1087,
         1007,  1024,   102])"
993,1,"['table', 'standard normal', 'random variable', 'random', 'normal', 'standard', 'variable']",B Chapter  B Equation ,seg_269,if z is a standard normal distributed random variable then: p = p(z ≤ z) = φ(z). if for example z = 0.2 from table c.1 it is: φ(0.2) = 0.5793.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([1038, 3127, 1038, 8522])","tensor([ 101, 2065, 1062, 2003, 1037, 3115, 3671, 5500, 6721, 8023, 2059, 1024,
        1052, 1027, 1052, 1006, 1062, 1608, 1062, 1007, 1027, 1176, 1006, 1062,
        1007, 1012, 2065, 2005, 2742, 1062, 1027, 1014, 1012, 1016, 2013, 2795,
        1039, 1012, 1015, 2009, 2003, 1024, 1176, 1006, 1014, 1012, 1016, 1007,
        1027, 1014, 1012, 5401, 2683, 2509, 1012,  102])"
994,1,"['normal distribution', 'standard normal distribution', 'table', 'standard normal', 'random variable', 'random', 'normal', 'standard', 'distribution', 'variable']",B Chapter  B Equation ,seg_269,calculation of the inverse standard normal distribution φ−1(·): if z is a standard normal distributed random variable then: φ−1(p) = z. if for example p = 0.5793 from table c.1 it is: φ−1(0.5793) = 0.2.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1038, 3127, 1038, 8522])","tensor([  101, 17208,  1997,  1996, 19262,  3115,  3671,  4353,  1176, 27944,
         1006,  1087,  1007,  1024,  2065,  1062,  2003,  1037,  3115,  3671,
         5500,  6721,  8023,  2059,  1024,  1176, 27944,  1006,  1052,  1007,
         1027,  1062,  1012,  2065,  2005,  2742,  1052,  1027,  1014,  1012,
         5401,  2683,  2509,  2013,  2795,  1039,  1012,  1015,  2009,  2003,
         1024,  1176, 27944,  1006,  1014,  1012,  5401,  2683,  2509,  1007,
         1027,  1014,  1012,  1016,  1012,   102])"
995,1,"['significance', 'significance level', 'level']",B Chapter  B Equation ,seg_269,so in eq. 5.68 if the significance level α is assumed equal to 10% then it is:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([1038, 3127, 1038, 8522])","tensor([ 101, 2061, 1999, 1041, 4160, 1012, 1019, 1012, 6273, 2065, 1996, 7784,
        2504, 1155, 2003, 5071, 5020, 2000, 2184, 1003, 2059, 2009, 2003, 1024,
         102])"
996,1,['table'],B Chapter  B Equation ,seg_269,"which from table c.1, as explained above, yields (approximately): φ−1 (0.95) = 1.65 (1.645 approximately if you carry out interpolation).",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1038, 3127, 1038, 8522])","tensor([  101,  2029,  2013,  2795,  1039,  1012,  1015,  1010,  2004,  4541,
         2682,  1010, 16189,  1006,  3155,  1007,  1024,  1176, 27944,  1006,
         1014,  1012,  5345,  1007,  1027,  1015,  1012,  3515,  1006,  1015,
         1012,  4185,  2629,  3155,  2065,  2017,  4287,  2041,  6970, 18155,
         3370,  1007,  1012,   102])"
997,1,"['random variable', 'transformed', 'random', 'normal', 'variable']",B Equation ,seg_271,from eq. 5.70 the normal distributed random variable x̄ is transformed to a standard normal distributed random variable such as:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 1., 1., 0., 0., 0., 0.])","tensor([1038, 8522])","tensor([ 101, 2013, 1041, 4160, 1012, 1019, 1012, 3963, 1996, 3671, 5500, 6721,
        8023, 1060, 2003, 8590, 2000, 1037, 3115, 3671, 5500, 6721, 8023, 2107,
        2004, 1024,  102])"
998,1,"['standardization', 'results', 'variable']",B Equation ,seg_271,following the standardization of x̄ a new variable z results,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0.])","tensor([1038, 8522])","tensor([  101,  2206,  1996, 28648,  1997,  1060,  1037,  2047,  8023,  1062,
         3463,   102])"
999,1,"['mean', 'deviation', 'standard normal', 'normal', 'standard deviation', 'standard']",B Equation ,seg_271,that is standard normal distributed with mean equal to 0 and standard deviation equal to 1. so form the above equation we can write:,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1038, 8522])","tensor([  101,  2008,  2003,  3115,  3671,  5500,  2007,  2812,  5020,  2000,
         1014,  1998,  3115, 24353,  5020,  2000,  1015,  1012,  2061,  2433,
         1996,  2682,  8522,  2057,  2064,  4339,  1024,   102])"
1000,1,['table'],B Equation ,seg_271,where the value of φ−1(0.95) = 1.645 is found with the help of table c.1.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([1038, 8522])","tensor([  101,  2073,  1996,  3643,  1997,  1176, 27944,  1006,  1014,  1012,
         5345,  1007,  1027,  1015,  1012,  4185,  2629,  2003,  2179,  2007,
         1996,  2393,  1997,  2795,  1039,  1012,  1015,  1012,   102])"
1001,1,"['test', 'degrees of freedom', 'significance']",B Examples on ChiSquare Significance Test,seg_273,the degrees of freedom in the chi-square significance test are defined as:,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 1038,  4973,  2006,  9610,  2015, 16211,  2890,  7784,  3231])","tensor([ 101, 1996, 5445, 1997, 4071, 1999, 1996, 9610, 1011, 2675, 7784, 3231,
        2024, 4225, 2004, 1024,  102])"
1002,1,"['intervals', 'distribution', 'parameters', 'data']",B Examples on ChiSquare Significance Test,seg_273,where k is the number of intervals to which the available data are arranged and l is the number of the parameters of the assumed distribution which are calculated from the data directly.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 1038,  4973,  2006,  9610,  2015, 16211,  2890,  7784,  3231])","tensor([  101,  2073,  1047,  2003,  1996,  2193,  1997, 14025,  2000,  2029,
         1996,  2800,  2951,  2024,  5412,  1998,  1048,  2003,  1996,  2193,
         1997,  1996, 11709,  1997,  1996,  5071,  4353,  2029,  2024, 10174,
         2013,  1996,  2951,  3495,  1012,   102])"
1003,1,"['mean', 'parameter', 'deviation', 'normal', 'standard deviation', 'standard', 'distribution', 'data']",B Examples on ChiSquare Significance Test,seg_273,"in the example in sect. 5.9.1, first both the distribution (normal) and the parameters (mean and standard deviation) are postulated i.e. assumed for the concrete compressive strength data. this means that l = 0 since no parameter is calculated from the data.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0.])","tensor([ 1038,  4973,  2006,  9610,  2015, 16211,  2890,  7784,  3231])","tensor([  101,  1999,  1996,  2742,  1999, 17831,  1012,  1019,  1012,  1023,
         1012,  1015,  1010,  2034,  2119,  1996,  4353,  1006,  3671,  1007,
         1998,  1996, 11709,  1006,  2812,  1998,  3115, 24353,  1007,  2024,
         2695,  8898,  1045,  1012,  1041,  1012,  5071,  2005,  1996,  5509,
         4012, 27484,  3997,  2951,  1012,  2023,  2965,  2008,  1048,  1027,
         1014,  2144,  2053, 16381,  2003, 10174,  2013,  1996,  2951,  1012,
          102])"
1004,1,"['data', 'degrees of freedom', 'sample', 'sample statistic', 'intervals', 'statistic']",B Examples on ChiSquare Significance Test,seg_273,the data are arranged into 3 intervals and so k = 3. following the above expression for the degrees of freedom the sample statistic (see eq. 5.77) has k − 1 − l = 3 − 1 − 0 = 2 degrees of freedom.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0.])","tensor([ 1038,  4973,  2006,  9610,  2015, 16211,  2890,  7784,  3231])","tensor([  101,  1996,  2951,  2024,  5412,  2046,  1017, 14025,  1998,  2061,
         1047,  1027,  1017,  1012,  2206,  1996,  2682,  3670,  2005,  1996,
         5445,  1997,  4071,  1996,  7099, 28093,  6553,  1006,  2156,  1041,
         4160,  1012,  1019,  1012,  6255,  1007,  2038,  1047,  1597,  1015,
         1597,  1048,  1027,  1017,  1597,  1015,  1597,  1014,  1027,  1016,
         5445,  1997,  4071,  1012,   102])"
1005,1,"['mean', 'parameter', 'deviation', 'data', 'degrees of freedom', 'sample', 'normal', 'standard deviation', 'standard', 'sample statistic', 'distribution', 'statistic', 'case']",B Examples on ChiSquare Significance Test,seg_273,"subsequently, for the same example, the distribution (normal) and the mean value are postulated i.e. assumed while the standard deviation (parameter of the assumed distribution) is calculated from the data and hence l = 1. so in that case, the degrees of freedom of the sample statistic are: k − 1 − l = 3 − 1 − 1 = 1.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 1038,  4973,  2006,  9610,  2015, 16211,  2890,  7784,  3231])","tensor([  101,  3525,  1010,  2005,  1996,  2168,  2742,  1010,  1996,  4353,
         1006,  3671,  1007,  1998,  1996,  2812,  3643,  2024,  2695,  8898,
         1045,  1012,  1041,  1012,  5071,  2096,  1996,  3115, 24353,  1006,
        16381,  1997,  1996,  5071,  4353,  1007,  2003, 10174,  2013,  1996,
         2951,  1998,  6516,  1048,  1027,  1015,  1012,  2061,  1999,  2008,
         2553,  1010,  1996,  5445,  1997,  4071,  1997,  1996,  7099, 28093,
         6553,  2024,  1024,  1047,  1597,  1015,  1597,  1048,  1027,  1017,
         1597,  1015,  1597,  1015,  1027,  1015,  1012,   102])"
1006,1,"['table', 'probability', 'degrees of freedom', 'critical value']",B Examples on ChiSquare Significance Test,seg_273,the critical value δ can be found from table c.3 using the degrees of freedom and the probability,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 0.])","tensor([ 1038,  4973,  2006,  9610,  2015, 16211,  2890,  7784,  3231])","tensor([ 101, 1996, 4187, 3643, 1158, 2064, 2022, 2179, 2013, 2795, 1039, 1012,
        1017, 2478, 1996, 5445, 1997, 4071, 1998, 1996, 9723,  102])"
1007,1,"['estimated', 'variance']",B Chapter  B Example ,seg_277,the variance of c is estimated as follows:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([1038, 3127, 1038, 2742])","tensor([  101,  1996, 23284,  1997,  1039,  2003,  4358,  2004,  4076,  1024,
          102])"
1008,0,[],B Example ,seg_279,the reliability index is obtained by solving g(u) = 0 to β . g(u) is already given in the example:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0])","tensor([1038, 2742])","tensor([  101,  1996, 15258,  5950,  2003,  4663,  2011, 13729,  1043,  1006,
         1057,  1007,  1027,  1014,  2000,  1156,  1012,  1043,  1006,  1057,
         1007,  2003,  2525,  2445,  1999,  1996,  2742,  1024,   102])"
1009,1,['table'],B Example ,seg_279,"the aim is to calculate β so that it represents the smallest distance to the origin. the starting values of β , αr , αa and αs are assumed (see table 6.1, column “start”). the next step (column for iteration 1) is calculated as follows:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1038, 2742])","tensor([  101,  1996,  6614,  2003,  2000, 18422,  1156,  2061,  2008,  2009,
         5836,  1996, 10479,  3292,  2000,  1996,  4761,  1012,  1996,  3225,
         5300,  1997,  1156,  1010,  1155,  2099,  1010,  1155,  2050,  1998,
         1155,  2015,  2024,  5071,  1006,  2156,  2795,  1020,  1012,  1015,
         1010,  5930,  1523,  2707,  1524,  1007,  1012,  1996,  2279,  3357,
         1006,  5930,  2005, 27758,  1015,  1007,  2003, 10174,  2004,  4076,
         1024,   102])"
1010,1,['convergence'],B Example ,seg_279,the iterations are continued until the value of β converges to one value (in this example convergence occurs after the 4th iteration).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([1038, 2742])","tensor([  101,  1996, 27758,  2015,  2024,  2506,  2127,  1996,  3643,  1997,
         1156, 28314,  2015,  2000,  2028,  3643,  1006,  1999,  2023,  2742,
        19143,  5158,  2044,  1996,  4343, 27758,  1007,  1012,   102])"
