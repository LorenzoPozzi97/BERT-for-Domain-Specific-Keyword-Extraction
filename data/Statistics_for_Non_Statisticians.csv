,Relevance,Tags,Heading,Seg,Sentence,Enc Relevance,Enc Tags,Enc Heading,Enc Sentence
0,1,"['statistics', 'data']",Chapter  Data Collection,seg_1,"this chapter explains some basic concepts within statistics. also, we look at the most important ways to collect data in surveys.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0.])","tensor([3127, 2951, 3074])","tensor([  101,  2023,  3127,  7607,  2070,  3937,  8474,  2306,  6747,  1012,
         2036,  1010,  2057,  2298,  2012,  1996,  2087,  2590,  3971,  2000,
         8145,  2951,  1999, 12265,  1012,   102])"
1,1,"['data collection', 'data']",Chapter  Data Collection,seg_1,"statistics can be defined as a collection of techniques used when planning a data collection, and when subsequently analyzing and presenting data.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.])","tensor([3127, 2951, 3074])","tensor([  101,  6747,  2064,  2022,  4225,  2004,  1037,  3074,  1997,  5461,
         2109,  2043,  4041,  1037,  2951,  3074,  1010,  1998,  2043,  3525,
        20253,  1998, 10886,  2951,  1012,   102])"
2,1,"['probability theory', 'probability', 'statistics', 'population', 'statistical']",Chapter  Data Collection,seg_1,"dating back to ancient times people have needed knowledge about population size, to carry out a census of the armies or calculate expected taxes. the word statistics is derived from the word “status” (originally coming from latin); and it was exactly the status of society, which was the subject of the first statistics! later emerged probability theory (in connection with games!), demographics and insurance science as areas, in which statistical thinking was essential.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([3127, 2951, 3074])","tensor([  101,  5306,  2067,  2000,  3418,  2335,  2111,  2031,  2734,  3716,
         2055,  2313,  2946,  1010,  2000,  4287,  2041,  1037,  2883,  1997,
         1996,  8749,  2030, 18422,  3517,  7773,  1012,  1996,  2773,  6747,
         2003,  5173,  2013,  1996,  2773,  1523,  3570,  1524,  1006,  2761,
         2746,  2013,  3763,  1007,  1025,  1998,  2009,  2001,  3599,  1996,
         3570,  1997,  2554,  1010,  2029,  2001,  1996,  3395,  1997,  1996,
         2034,  6747,   999,  2101,  6003,  9723,  3399,  1006,  1999,  4434,
         2007,  2399,   999,  1007,  1010, 28321,  1998,  5427,  2671,  2004,
         2752,  1010,  1999,  2029,  7778,  3241,  2001,  6827,  1012,   102])"
3,1,"['process', 'statistics', 'data']",Chapter  Data Collection,seg_1,"in today’s digital age it is easy to collect as well as process and disseminate data, and therefore statistics is used for a variety of surveys throughout society.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3127, 2951, 3074])","tensor([  101,  1999,  2651,  1521,  1055,  3617,  2287,  2009,  2003,  3733,
         2000,  8145,  2004,  2092,  2004,  2832,  1998,  4487, 11393, 19269,
         2951,  1010,  1998,  3568,  6747,  2003,  2109,  2005,  1037,  3528,
         1997, 12265,  2802,  2554,  1012,   102])"
4,1,['statistical'],Chapter  Data Collection,seg_1,most statistical surveys can be divided into the following phases:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3127, 2951, 3074])","tensor([  101,  2087,  7778, 12265,  2064,  2022,  4055,  2046,  1996,  2206,
        12335,  1024,   102])"
5,1,"['data collection', 'data']",Chapter  Data Collection,seg_1,1. clarification of concepts 2. planning of data collection 3. data collection 4. analysis and presentation of data 5. conclusion,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([3127, 2951, 3074])","tensor([  101,  1015,  1012, 18856,  8486, 10803,  1997,  8474,  1016,  1012,
         4041,  1997,  2951,  3074,  1017,  1012,  2951,  3074,  1018,  1012,
         4106,  1998,  8312,  1997,  2951,  1019,  1012,  7091,   102])"
6,1,['statisticians'],Chapter  Data Collection,seg_1,statistical methods (and statisticians!) are particularly useful in phases 2 and 4 of the survey.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([3127, 2951, 3074])","tensor([  101,  7778,  4725,  1006,  1998, 28093,  6553,  7066,   999,  1007,
         2024,  3391,  6179,  1999, 12335,  1016,  1998,  1018,  1997,  1996,
         5002,  1012,   102])"
7,1,['statistics'],Chapter  Data Collection,seg_1,there are two kinds of statistics:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([3127, 2951, 3074])","tensor([ 101, 2045, 2024, 2048, 7957, 1997, 6747, 1024,  102])"
8,1,"['descriptive statistics', 'statistics']",Chapter  Data Collection,seg_1,– descriptive statistics – analytical statistics,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 0.])","tensor([3127, 2951, 3074])","tensor([  101,  1516, 22726,  6747,  1516, 17826,  6747,   102])"
9,1,"['data', 'tables', 'statistics', 'charts', 'statistical', 'percentages']",Chapter  Data Collection,seg_1,"descriptive statistics means describing data using tables, charts and simple statistical calculations such as averages, percentages, etc. this is what many people understand by the word “statistics”. it was also the kind of statistics that was produced in ancient times.",tensor(1),"tensor([0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3127, 2951, 3074])","tensor([  101, 22726,  6747,  2965,  7851,  2951,  2478,  7251,  1010,  6093,
         1998,  3722,  7778, 16268,  2107,  2004, 20185,  1010,  7017,  2015,
         1010,  4385,  1012,  2023,  2003,  2054,  2116,  2111,  3305,  2011,
         1996,  2773,  1523,  6747,  1524,  1012,  2009,  2001,  2036,  1996,
         2785,  1997,  6747,  2008,  2001,  2550,  1999,  3418,  2335,  1012,
          102])"
10,1,"['data', 'probability', 'estimate', 'statistics']",Chapter  Data Collection,seg_1,"analytical statistics is used to assess differences and relationships in data. for example, we could examine whether there is a relation between height and weight of a group of persons; or whether there is a difference between height of boys and height of girls, as well as provide an estimate of how large this difference is. analytical statistics is a mathematical discipline, based on calculus of probability. it is a relatively new discipline that has been developed throughout the twentieth century.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([3127, 2951, 3074])","tensor([  101, 17826,  6747,  2003,  2109,  2000, 14358,  5966,  1998,  6550,
         1999,  2951,  1012,  2005,  2742,  1010,  2057,  2071, 11628,  3251,
         2045,  2003,  1037,  7189,  2090,  4578,  1998,  3635,  1997,  1037,
         2177,  1997,  5381,  1025,  2030,  3251,  2045,  2003,  1037,  4489,
         2090,  4578,  1997,  3337,  1998,  4578,  1997,  3057,  1010,  2004,
         2092,  2004,  3073,  2019, 10197,  1997,  2129,  2312,  2023,  4489,
         2003,  1012, 17826,  6747,  2003,  1037,  8045,  9009,  1010,  2241,
         2006, 19276,  1997,  9723,  1012,  2009,  2003,  1037,  4659,  2047,
         9009,  2008,  2038,  2042,  2764,  2802,  1996,  9086,  2301,  1012,
          102])"
11,1,"['descriptive statistics', 'statistics']",Chapter  Data Collection,seg_1,"this book is about descriptive statistics as well as analytical statistics. in practice, you need both. analytical statistics is a very large topic, and here we can only scratch the surface (see especially chaps. 5, 7, and 8). if you want to know more about analytical statistics, see some of the more advanced books in the literature list.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([3127, 2951, 3074])","tensor([  101,  2023,  2338,  2003,  2055, 22726,  6747,  2004,  2092,  2004,
        17826,  6747,  1012,  1999,  3218,  1010,  2017,  2342,  2119,  1012,
        17826,  6747,  2003,  1037,  2200,  2312,  8476,  1010,  1998,  2182,
         2057,  2064,  2069, 11969,  1996,  3302,  1006,  2156,  2926, 15775,
         4523,  1012,  1019,  1010,  1021,  1010,  1998,  1022,  1007,  1012,
         2065,  2017,  2215,  2000,  2113,  2062,  2055, 17826,  6747,  1010,
         2156,  2070,  1997,  1996,  2062,  3935,  2808,  1999,  1996,  3906,
         2862,  1012,   102])"
12,1,"['information', 'sample', 'sample ', 'population', 'data']", Sample Surveys,seg_3,"in any survey (*), we collect information on the individuals of either the entire population (*) (a total survey) or a relatively small number of individuals of a sample (*) (a sample survey), in order to analyze and present data.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0.])","tensor([ 7099, 12265])","tensor([  101,  1999,  2151,  5002,  1006,  1008,  1007,  1010,  2057,  8145,
         2592,  2006,  1996,  3633,  1997,  2593,  1996,  2972,  2313,  1006,
         1008,  1007,  1006,  1037,  2561,  5002,  1007,  2030,  1037,  4659,
         2235,  2193,  1997,  3633,  1997,  1037,  7099,  1006,  1008,  1007,
         1006,  1037,  7099,  5002,  1007,  1010,  1999,  2344,  2000, 17908,
         1998,  2556,  2951,  1012,   102])"
13,1,"['sample', 'results', 'population']", Sample Surveys,seg_3,"we are interested in the entire population of individuals. the advantage of investigating only a sample is that it is both faster and cheaper than investigating the whole population. in some situations, a carefully planned sample survey can even give more accurate results than a badly planned total survey!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 7099, 12265])","tensor([  101,  2057,  2024,  4699,  1999,  1996,  2972,  2313,  1997,  3633,
         1012,  1996,  5056,  1997, 11538,  2069,  1037,  7099,  2003,  2008,
         2009,  2003,  2119,  5514,  1998, 16269,  2084, 11538,  1996,  2878,
         2313,  1012,  1999,  2070,  8146,  1010,  1037,  5362,  3740,  7099,
         5002,  2064,  2130,  2507,  2062,  8321,  3463,  2084,  1037,  6649,
         3740,  2561,  5002,   999,   102])"
14,1,"['estimate', 'sample', 'population']", Sample Surveys,seg_3,we investigate the individuals in the sample in order to study the whole population! this means that the sample gives us an estimate (*) of the characteristics of the population (fig. 1.1).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 7099, 12265])","tensor([  101,  2057,  8556,  1996,  3633,  1999,  1996,  7099,  1999,  2344,
         2000,  2817,  1996,  2878,  2313,   999,  2023,  2965,  2008,  1996,
         7099,  3957,  2149,  2019, 10197,  1006,  1008,  1007,  1997,  1996,
         6459,  1997,  1996,  2313,  1006, 20965,  1012,  1015,  1012,  1015,
         1007,  1012,   102])"
15,0,[], Sample Surveys,seg_3,examples of characteristics:,tensor(0),"tensor([0, 0, 0, 0, 0, 0])","tensor([ 7099, 12265])","tensor([ 101, 4973, 1997, 6459, 1024,  102])"
16,1,"['average', 'percentage']", Sample Surveys,seg_3,"– average (*) of a measurable attribute of individuals, e.g., height – percentage of individuals who belong to a particular category (e.g., who have a",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 7099, 12265])","tensor([  101,  1516,  2779,  1006,  1008,  1007,  1997,  1037,  2033, 28329,
        17961,  1997,  3633,  1010,  1041,  1012,  1043,  1012,  1010,  4578,
         1516,  7017,  1997,  3633,  2040,  7141,  2000,  1037,  3327,  4696,
         1006,  1041,  1012,  1043,  1012,  1010,  2040,  2031,  1037,   102])"
17,1,"['estimate', 'population', 'sample', 'random', 'sampling', 'sampling ', 'representative']", Sample Surveys,seg_3,"the larger the sample, the better an estimate of the population! it is also important that the sample is representative of the population. in practice, this means that the individuals in the sample are selected at random, in order to cover the whole population. we are dealing much more with sampling (*) in chap. 6.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7099, 12265])","tensor([  101,  1996,  3469,  1996,  7099,  1010,  1996,  2488,  2019, 10197,
         1997,  1996,  2313,   999,  2009,  2003,  2036,  2590,  2008,  1996,
         7099,  2003,  4387,  1997,  1996,  2313,  1012,  1999,  3218,  1010,
         2023,  2965,  2008,  1996,  3633,  1999,  1996,  7099,  2024,  3479,
         2012,  6721,  1010,  1999,  2344,  2000,  3104,  1996,  2878,  2313,
         1012,  2057,  2024,  7149,  2172,  2062,  2007, 16227,  1006,  1008,
         1007,  1999, 15775,  2361,  1012,  1020,  1012,   102])"
18,1,"['sample', 'population']", Sample Surveys,seg_3,"the sample (and the population) may consist of different types of individuals, depending on the context.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 7099, 12265])","tensor([ 101, 1996, 7099, 1006, 1998, 1996, 2313, 1007, 2089, 8676, 1997, 2367,
        4127, 1997, 3633, 1010, 5834, 2006, 1996, 6123, 1012,  102])"
19,1,['samples'], Sample Surveys,seg_3,the concepts in this book can be applied to all types of samples. the examples are mainly samples consisting of people. but the principles can be applied to all types of samples.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0.])","tensor([ 7099, 12265])","tensor([ 101, 1996, 8474, 1999, 2023, 2338, 2064, 2022, 4162, 2000, 2035, 4127,
        1997, 8168, 1012, 1996, 4973, 2024, 3701, 8168, 5398, 1997, 2111, 1012,
        2021, 1996, 6481, 2064, 2022, 4162, 2000, 2035, 4127, 1997, 8168, 1012,
         102])"
20,1,['samples'], Sample Surveys,seg_3,"typical applications with samples consisting of people are: analysis of attitudes, consumption, durable goods, interests and hobbies, eating and drinking habits, transportation, traffic, vacation, media (tv, radio, newspapers) and certain sensitive topics, such as alcohol consumption.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7099, 12265])","tensor([  101,  5171,  5097,  2007,  8168,  5398,  1997,  2111,  2024,  1024,
         4106,  1997, 13818,  1010,  8381,  1010, 25634,  5350,  1010,  5426,
         1998,  7570, 27982,  1010,  5983,  1998,  5948, 14243,  1010,  5193,
         1010,  4026,  1010, 10885,  1010,  2865,  1006,  2694,  1010,  2557,
         1010,  6399,  1007,  1998,  3056,  7591,  7832,  1010,  2107,  2004,
         6544,  8381,  1012,   102])"
21,1,['sample'], Sample Surveys,seg_3,"sample surveys can provide a high degree of flexibility: one can in the same survey have questions on media consumption, traffic patterns and attitudes. sample surveys are also widely used in commercial surveys, e.g., in connection with telephone interviews.",tensor(1),"tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7099, 12265])","tensor([  101,  7099, 12265,  2064,  3073,  1037,  2152,  3014,  1997, 16991,
         1024,  2028,  2064,  1999,  1996,  2168,  5002,  2031,  3980,  2006,
         2865,  8381,  1010,  4026,  7060,  1998, 13818,  1012,  7099, 12265,
         2024,  2036,  4235,  2109,  1999,  3293, 12265,  1010,  1041,  1012,
         1043,  1012,  1010,  1999,  4434,  2007,  7026,  7636,  1012,   102])"
22,1,"['control', 'quality control', 'sampling', 'statistical']", Sample Surveys,seg_3,"one of the essential applications of sampling is sampling inspection in the field of statistical quality control. if you are working with statistical quality control, most of this book will be relevant to you. the issues that are specific to this discipline, however, will not be dealt with here. see the literature list for books on statistical quality control.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 7099, 12265])","tensor([  101,  2028,  1997,  1996,  6827,  5097,  1997, 16227,  2003, 16227,
        10569,  1999,  1996,  2492,  1997,  7778,  3737,  2491,  1012,  2065,
         2017,  2024,  2551,  2007,  7778,  3737,  2491,  1010,  2087,  1997,
         2023,  2338,  2097,  2022,  7882,  2000,  2017,  1012,  1996,  3314,
         2008,  2024,  3563,  2000,  2023,  9009,  1010,  2174,  1010,  2097,
         2025,  2022,  9411,  2007,  2182,  1012,  2156,  1996,  3906,  2862,
         2005,  2808,  2006,  7778,  3737,  2491,  1012,   102])"
23,0,[], Fitness Club Example of a Sample Survey,seg_5,this example is fictitious survey. it will be used as an example for the subsequent chapters.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([10516,  2252,  2742,  1997,  1037,  7099,  5002])","tensor([  101,  2023,  2742,  2003, 23577,  5002,  1012,  2009,  2097,  2022,
         2109,  2004,  2019,  2742,  2005,  1996,  4745,  9159,  1012,   102])"
24,1,['loss'], Fitness Club Example of a Sample Survey,seg_5,"fitness club has a number of sports facilities. this includes facilities for strength training, weight loss and cardiovascular workout.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0.])","tensor([10516,  2252,  2742,  1997,  1037,  7099,  5002])","tensor([  101, 10516,  2252,  2038,  1037,  2193,  1997,  2998,  4128,  1012,
         2023,  2950,  4128,  2005,  3997,  2731,  1010,  3635,  3279,  1998,
        22935, 27090,  1012,   102])"
25,1,['information'], Fitness Club Example of a Sample Survey,seg_5,"fitness club wants to understand the needs of their young customers, kids of age 12–17 years. the club wants to know, how satisfied these kids are with the sports facilities. they also want to obtain information about their health in order to better customize the sports facilities for the various types of training.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([10516,  2252,  2742,  1997,  1037,  7099,  5002])","tensor([  101, 10516,  2252,  4122,  2000,  3305,  1996,  3791,  1997,  2037,
         2402,  6304,  1010,  4268,  1997,  2287,  2260,  1516,  2459,  2086,
         1012,  1996,  2252,  4122,  2000,  2113,  1010,  2129,  8510,  2122,
         4268,  2024,  2007,  1996,  2998,  4128,  1012,  2027,  2036,  2215,
         2000,  6855,  2592,  2055,  2037,  2740,  1999,  2344,  2000,  2488,
         7661,  4697,  1996,  2998,  4128,  2005,  1996,  2536,  4127,  1997,
         2731,  1012,   102])"
26,1,['sample'], Fitness Club Example of a Sample Survey,seg_5,"therefore, a sample survey is carried out among the kids using the sports facilities. we will later discuss how the survey can be organized. moreover, we present some findings from the survey.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([10516,  2252,  2742,  1997,  1037,  7099,  5002])","tensor([ 101, 3568, 1010, 1037, 7099, 5002, 2003, 3344, 2041, 2426, 1996, 4268,
        2478, 1996, 2998, 4128, 1012, 2057, 2097, 2101, 6848, 2129, 1996, 5002,
        2064, 2022, 4114, 1012, 9308, 1010, 2057, 2556, 2070, 9556, 2013, 1996,
        5002, 1012,  102])"
27,1,"['sample', 'population']", Fitness Club Example of a Sample Survey,seg_5,the population consists of kids using the sports facilities in fitness club. the individual is one kid. the sample consists of 30 kids.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([10516,  2252,  2742,  1997,  1037,  7099,  5002])","tensor([  101,  1996,  2313,  3774,  1997,  4268,  2478,  1996,  2998,  4128,
         1999, 10516,  2252,  1012,  1996,  3265,  2003,  2028,  4845,  1012,
         1996,  7099,  3774,  1997,  2382,  4268,  1012,   102])"
28,1,['data'], Fitness Club Example of a Sample Survey,seg_5,some data related to this example are found in the appendices in chap. 9 of this book.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([10516,  2252,  2742,  1997,  1037,  7099,  5002])","tensor([  101,  2070,  2951,  3141,  2000,  2023,  2742,  2024,  2179,  1999,
         1996, 10439, 10497, 23522,  1999, 15775,  2361,  1012,  1023,  1997,
         2023,  2338,  1012,   102])"
29,1,"['case', 'design', 'information', 'factors', 'experiment ', 'experiment', 'data', 'test']", Experiments,seg_7,"in certain situations, the information needed simply does not exist at all! in this case, one can plan (or design) an experiment (*), with the aim to provide the relevant data. in the experiment, we test the influence of one or more factors on a measured result. this approach is widely used in technical and industrial contexts.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([7885]),"tensor([  101,  1999,  3056,  8146,  1010,  1996,  2592,  2734,  3432,  2515,
         2025,  4839,  2012,  2035,   999,  1999,  2023,  2553,  1010,  2028,
         2064,  2933,  1006,  2030,  2640,  1007,  2019,  7551,  1006,  1008,
         1007,  1010,  2007,  1996,  6614,  2000,  3073,  1996,  7882,  2951,
         1012,  1999,  1996,  7551,  1010,  2057,  3231,  1996,  3747,  1997,
         2028,  2030,  2062,  5876,  2006,  1037,  7594,  2765,  1012,  2023,
         3921,  2003,  4235,  2109,  1999,  4087,  1998,  3919, 18046,  1012,
          102])"
30,1,"['sample', 'results', 'experimental', 'experiment', 'experiments', 'population']", Experiments,seg_7,"in these situations the population is not well defined. the experiment can produce very different results, depending on how it is planned. nevertheless, it also makes sense to talk about a sample in this situation. we can perceive the experimental results as a sample of all the (infinitely many) possible results we could have obtained in all similar experiments.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])",tensor([7885]),"tensor([  101,  1999,  2122,  8146,  1996,  2313,  2003,  2025,  2092,  4225,
         1012,  1996,  7551,  2064,  3965,  2200,  2367,  3463,  1010,  5834,
         2006,  2129,  2009,  2003,  3740,  1012,  6600,  1010,  2009,  2036,
         3084,  3168,  2000,  2831,  2055,  1037,  7099,  1999,  2023,  3663,
         1012,  2057,  2064, 23084,  1996,  6388,  3463,  2004,  1037,  7099,
         1997,  2035,  1996,  1006, 25773,  2116,  1007,  2825,  3463,  2057,
         2071,  2031,  4663,  1999,  2035,  2714,  7885,  1012,   102])"
31,1,"['data', 'sample', 'experiment', 'statistical']", Experiments,seg_7,"the statistical techniques used to analyze and present data are by and large the same, regardless of whether data are collected in a sample survey or an experiment.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.])",tensor([7885]),"tensor([  101,  1996,  7778,  5461,  2109,  2000, 17908,  1998,  2556,  2951,
         2024,  2011,  1998,  2312,  1996,  2168,  1010,  7539,  1997,  3251,
         2951,  2024,  5067,  1999,  1037,  7099,  5002,  2030,  2019,  7551,
         1012,   102])"
32,1,['experiments'], Experiments,seg_7,"the special techniques used in the planning of experiments will not be dealt with in this book; the space does not allow this. there are numerous books on this topic, see the literature list.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])",tensor([7885]),"tensor([ 101, 1996, 2569, 5461, 2109, 1999, 1996, 4041, 1997, 7885, 2097, 2025,
        2022, 9411, 2007, 1999, 2023, 2338, 1025, 1996, 2686, 2515, 2025, 3499,
        2023, 1012, 2045, 2024, 3365, 2808, 2006, 2023, 8476, 1010, 2156, 1996,
        3906, 2862, 1012,  102])"
33,1,"['sample', 'experiment']", Experiments An Example,seg_9,"to illustrate the difference between a sample survey and an experiment, we present an example of a planned experiment.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0.])","tensor([7885, 2019, 2742])","tensor([  101,  2000, 19141,  1996,  4489,  2090,  1037,  7099,  5002,  1998,
         2019,  7551,  1010,  2057,  2556,  2019,  2742,  1997,  1037,  3740,
         7551,  1012,   102])"
34,0,[], Experiments An Example,seg_9,a producer of soft drinks wants to launch a new product with an entirely new blend of flavor ingredients. he is uncertain how to balance the flavors with,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0])","tensor([7885, 2019, 2742])","tensor([  101,  1037,  3135,  1997,  3730,  8974,  4122,  2000,  4888,  1037,
         2047,  4031,  2007,  2019,  4498,  2047, 12586,  1997, 14894, 12760,
         1012,  2002,  2003,  9662,  2129,  2000,  5703,  1996, 26389,  2007,
          102])"
35,1,"['experiment', 'combinations']", Experiments An Example,seg_9,"carbonate and sugar. he therefore performs an experiment, where he is testing various combinations of the ingredients.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0.])","tensor([7885, 2019, 2742])","tensor([  101, 26427,  1998,  5699,  1012,  2002,  3568, 10438,  2019,  7551,
         1010,  2073,  2002,  2003,  5604,  2536, 14930,  1997,  1996, 12760,
         1012,   102])"
36,0,[], Experiments An Example,seg_9,"the producer makes a list with the lowest respectively highest value added of the three ingredients, which he expects to use.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0])","tensor([7885, 2019, 2742])","tensor([  101,  1996,  3135,  3084,  1037,  2862,  2007,  1996,  7290,  4414,
         3284,  3643,  2794,  1997,  1996,  2093, 12760,  1010,  2029,  2002,
        24273,  2000,  2224,  1012,   102])"
37,1,"['sets', 'table', 'experimental', 'combinations']", Experiments An Example,seg_9,then he sets an experimental plan. he makes a few bottles of product of each of the eight possible combinations of the three ingredients (table 1.1).,tensor(1),"tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([7885, 2019, 2742])","tensor([  101,  2059,  2002,  4520,  2019,  6388,  2933,  1012,  2002,  3084,
         1037,  2261, 11015,  1997,  4031,  1997,  2169,  1997,  1996,  2809,
         2825, 14930,  1997,  1996,  2093, 12760,  1006,  2795,  1015,  1012,
         1015,  1007,  1012,   102])"
38,1,['rates'], Experiments An Example,seg_9,"here “1” means add “a little” of the ingredient, while “2” means add “a lot”. a taste panel of ten persons tastes all bottles and rates them. they use a scale of 1–7, where 4 is “neither good nor bad,” 1 is “unusually poor,” 7 is “unusually good”.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([7885, 2019, 2742])","tensor([  101,  2182,  1523,  1015,  1524,  2965,  5587,  1523,  1037,  2210,
         1524,  1997,  1996, 21774,  1010,  2096,  1523,  1016,  1524,  2965,
         5587,  1523,  1037,  2843,  1524,  1012,  1037,  5510,  5997,  1997,
         2702,  5381, 16958,  2035, 11015,  1998,  6165,  2068,  1012,  2027,
         2224,  1037,  4094,  1997,  1015,  1516,  1021,  1010,  2073,  1018,
         2003,  1523,  4445,  2204,  4496,  2919,  1010,  1524,  1015,  2003,
         1523, 12890,  3532,  1010,  1524,  1021,  2003,  1523, 12890,  2204,
         1524,  1012,   102])"
39,1,"['table', 'combinations', 'results', 'experiment', 'rates', 'average', 'combination']", Experiments An Example,seg_9,"the average rating for each combination is shown in the last column of the table. afterwards the manufacturer studies the results of the experiment. he observes that the four combinations with a “high” value of the flavor have better rates than the four combinations with a “low” value of the flavor. the average of the four combinations with a “high” value is 4.95, and the average of the four combinations with a “low” value is only 3.38. it would therefore appear that a lot of the flavor ingredient must be added.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([7885, 2019, 2742])","tensor([  101,  1996,  2779,  5790,  2005,  2169,  5257,  2003,  3491,  1999,
         1996,  2197,  5930,  1997,  1996,  2795,  1012,  5728,  1996,  7751,
         2913,  1996,  3463,  1997,  1996,  7551,  1012,  2002, 24451,  2008,
         1996,  2176, 14930,  2007,  1037,  1523,  2152,  1524,  3643,  1997,
         1996, 14894,  2031,  2488,  6165,  2084,  1996,  2176, 14930,  2007,
         1037,  1523,  2659,  1524,  3643,  1997,  1996, 14894,  1012,  1996,
         2779,  1997,  1996,  2176, 14930,  2007,  1037,  1523,  2152,  1524,
         3643,  2003,  1018,  1012,  5345,  1010,  1998,  1996,  2779,  1997,
         1996,  2176, 14930,  2007,  1037,  1523,  2659,  1524,  3643,  2003,
         2069,  1017,  1012,  4229,  1012,  2009,  2052,  3568,  3711,  2008,
         1037,  2843,  1997,  1996, 14894, 21774,  2442,  2022,  2794,  1012,
          102])"
40,0,[], Experiments An Example,seg_9,"by calculating the other averages, he can see that it is not so important, how much carbonate and sugar is added. he therefore chooses to add an intermediate dose of carbonate and sugar.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([7885, 2019, 2742])","tensor([  101,  2011, 20177,  1996,  2060, 20185,  1010,  2002,  2064,  2156,
         2008,  2009,  2003,  2025,  2061,  2590,  1010,  2129,  2172, 26427,
         1998,  5699,  2003,  2794,  1012,  2002,  3568, 15867,  2000,  5587,
         2019,  7783, 13004,  1997, 26427,  1998,  5699,  1012,   102])"
41,1,['case'], Experiments An Example,seg_9,"experiments of this type are widely used in industry, but they can also be used in marketing (as in this case) and social sciences.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([7885, 2019, 2742])","tensor([ 101, 7885, 1997, 2023, 2828, 2024, 4235, 2109, 1999, 3068, 1010, 2021,
        2027, 2064, 2036, 2022, 2109, 1999, 5821, 1006, 2004, 1999, 2023, 2553,
        1007, 1998, 2591, 4163, 1012,  102])"
42,1,"['sample', 'results', 'experiment', 'data']", Data Collection,seg_11,"we have now seen the difference between sample surveys and planned experiments. in the next chapter, we show how to present results from a sample survey or an experiment. first, however, we see how data are collected.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([2951, 3074])","tensor([  101,  2057,  2031,  2085,  2464,  1996,  4489,  2090,  7099, 12265,
         1998,  3740,  7885,  1012,  1999,  1996,  2279,  3127,  1010,  2057,
         2265,  2129,  2000,  2556,  3463,  2013,  1037,  7099,  5002,  2030,
         2019,  7551,  1012,  2034,  1010,  2174,  1010,  2057,  2156,  2129,
         2951,  2024,  5067,  1012,   102])"
43,1,"['populations', 'sources of error', 'data collection', 'data', 'error']", Data Collection,seg_11,"most of the book describes general techniques that can be applied to all types of populations. populations consisting of or involving humans are probably what will interest most readers of this book. that is why the rest of this chapter is on data collection methods, questionnaires and various sources of error for surveys of populations consisting of or involving people; for example, populations of enterprises, institutions or families.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([2951, 3074])","tensor([  101,  2087,  1997,  1996,  2338,  5577,  2236,  5461,  2008,  2064,
         2022,  4162,  2000,  2035,  4127,  1997,  7080,  1012,  7080,  5398,
         1997,  2030,  5994,  4286,  2024,  2763,  2054,  2097,  3037,  2087,
         8141,  1997,  2023,  2338,  1012,  2008,  2003,  2339,  1996,  2717,
         1997,  2023,  3127,  2003,  2006,  2951,  3074,  4725,  1010,  3160,
        20589,  2015,  1998,  2536,  4216,  1997,  7561,  2005, 12265,  1997,
         7080,  5398,  1997,  2030,  5994,  2111,  1025,  2005,  2742,  1010,
         7080,  1997,  9926,  1010,  4896,  2030,  2945,  1012,   102])"
44,1,['population'], Data Collection,seg_11,"if your population is not of this type, you can skip the rest of this chapter. the rest of the book will be useful, regardless of the type of population you are dealing with.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.])","tensor([2951, 3074])","tensor([  101,  2065,  2115,  2313,  2003,  2025,  1997,  2023,  2828,  1010,
         2017,  2064, 13558,  1996,  2717,  1997,  2023,  3127,  1012,  1996,
         2717,  1997,  1996,  2338,  2097,  2022,  6179,  1010,  7539,  1997,
         1996,  2828,  1997,  2313,  2017,  2024,  7149,  2007,  1012,   102])"
45,1,"['information', 'varying', 'sample', 'register']", Registers,seg_13,"before planning a sample survey, one should always check if the necessary information already exists in a database. a database consisting of people is often called a register. the vast majority of enterprises, institutions and organizations possess one or more databases with “business data”; for example member databases, customer databases, etc. it is varying from organization to organization, how much information on each individual these databases contain.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([18687]),"tensor([  101,  2077,  4041,  1037,  7099,  5002,  1010,  2028,  2323,  2467,
         4638,  2065,  1996,  4072,  2592,  2525,  6526,  1999,  1037,  7809,
         1012,  1037,  7809,  5398,  1997,  2111,  2003,  2411,  2170,  1037,
         4236,  1012,  1996,  6565,  3484,  1997,  9926,  1010,  4896,  1998,
         4411, 10295,  2028,  2030,  2062, 17881,  2007,  1523,  2449,  2951,
         1524,  1025,  2005,  2742,  2266, 17881,  1010,  8013, 17881,  1010,
         4385,  1012,  2009,  2003,  9671,  2013,  3029,  2000,  3029,  1010,
         2129,  2172,  2592,  2006,  2169,  3265,  2122, 17881,  5383,  1012,
          102])"
46,1,"['data', 'information', 'results', 'register', 'statistical']", Registers,seg_13,"if a register contains the information you need, it is often relatively simple to produce the desired statistical results. this is simply reuse of data already collected for administrative purposes!",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])",tensor([18687]),"tensor([ 101, 2065, 1037, 4236, 3397, 1996, 2592, 2017, 2342, 1010, 2009, 2003,
        2411, 4659, 3722, 2000, 3965, 1996, 9059, 7778, 3463, 1012, 2023, 2003,
        3432, 2128, 8557, 1997, 2951, 2525, 5067, 2005, 3831, 5682,  999,  102])"
47,1,"['sample', 'register', 'population']", Registers,seg_13,"this will most often be done on the whole population, as it rarely is more difficult than making an investigation on only a sample of the individuals in the register. therefore, register surveys are nearly always total surveys.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.])",tensor([18687]),"tensor([  101,  2023,  2097,  2087,  2411,  2022,  2589,  2006,  1996,  2878,
         2313,  1010,  2004,  2009,  6524,  2003,  2062,  3697,  2084,  2437,
         2019,  4812,  2006,  2069,  1037,  7099,  1997,  1996,  3633,  1999,
         1996,  4236,  1012,  3568,  1010,  4236, 12265,  2024,  3053,  2467,
         2561, 12265,  1012,   102])"
48,1,"['registers', 'information', 'sample']", Registers,seg_13,"briefly, you should use registers, when the information needed is available in an appropriate form, and you have access to it. on the other hand, you will use sample surveys, where registers do not exist, are inadequate, or you do not have access to them.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])",tensor([18687]),"tensor([  101,  4780,  1010,  2017,  2323,  2224, 18687,  1010,  2043,  1996,
         2592,  2734,  2003,  2800,  1999,  2019,  6413,  2433,  1010,  1998,
         2017,  2031,  3229,  2000,  2009,  1012,  2006,  1996,  2060,  2192,
         1010,  2017,  2097,  2224,  7099, 12265,  1010,  2073, 18687,  2079,
         2025,  4839,  1010,  2024, 14710,  1010,  2030,  2017,  2079,  2025,
         2031,  3229,  2000,  2068,  1012,   102])"
49,1,"['sample', 'information', 'register']", Registers,seg_13,"if the register does not contain the desired information, it can often be used as the basis for selecting a sample. sometimes, a register survey is combined with a sample survey. the register provides part of the requested information, the rest you get through the sample survey.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0.])",tensor([18687]),"tensor([  101,  2065,  1996,  4236,  2515,  2025,  5383,  1996,  9059,  2592,
         1010,  2009,  2064,  2411,  2022,  2109,  2004,  1996,  3978,  2005,
        17739,  1037,  7099,  1012,  2823,  1010,  1037,  4236,  5002,  2003,
         4117,  2007,  1037,  7099,  5002,  1012,  1996,  4236,  3640,  2112,
         1997,  1996,  7303,  2592,  1010,  1996,  2717,  2017,  2131,  2083,
         1996,  7099,  5002,  1012,   102])"
50,1,"['registers', 'information', 'register', 'population', 'statistical']", Registers,seg_13,"examples of register surveys are found in many research projects in the social sciences. one can use the information from the administrative registers for many purposes. the national statistical institutes in many countries have access to the registers, providing valuable information on the entire population of a nation.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])",tensor([18687]),"tensor([  101,  4973,  1997,  4236, 12265,  2024,  2179,  1999,  2116,  2470,
         3934,  1999,  1996,  2591,  4163,  1012,  2028,  2064,  2224,  1996,
         2592,  2013,  1996,  3831, 18687,  2005,  2116,  5682,  1012,  1996,
         2120,  7778, 12769,  1999,  2116,  3032,  2031,  3229,  2000,  1996,
        18687,  1010,  4346,  7070,  2592,  2006,  1996,  2972,  2313,  1997,
         1037,  3842,  1012,   102])"
51,0,[], Questionnaire Surveys,seg_15,a questionnaire survey has three parties:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 3160, 20589, 12265])","tensor([  101,  1037,  3160, 20589,  5002,  2038,  2093,  4243,  1024,   102])"
52,0,[], Questionnaire Surveys,seg_15,"– a researcher, who formulates the questions. – an interviewer, who asks the questions. – a respondent, who answers the questions.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 3160, 20589, 12265])","tensor([  101,  1516,  1037, 10753,  1010,  2040,  5675,  4570,  1996,  3980,
         1012,  1516,  2019,  4357,  2121,  1010,  2040,  5176,  1996,  3980,
         1012,  1516,  1037,  6869,  4765,  1010,  2040,  6998,  1996,  3980,
         1012,   102])"
53,1,['results'], Questionnaire Surveys,seg_15,"the usefulness of the results from a questionnaire survey depends on, whether the researcher formulates the questions in a way that ensures effective communication between the interviewer and the respondent!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 3160, 20589, 12265])","tensor([  101,  1996,  6179,  2791,  1997,  1996,  3463,  2013,  1037,  3160,
        20589,  5002,  9041,  2006,  1010,  3251,  1996, 10753,  5675,  4570,
         1996,  3980,  1999,  1037,  2126,  2008, 21312,  4621,  4807,  2090,
         1996,  4357,  2121,  1998,  1996,  6869,  4765,   999,   102])"
54,0,[], Questionnaire Surveys,seg_15,there are two main types of questions:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 3160, 20589, 12265])","tensor([ 101, 2045, 2024, 2048, 2364, 4127, 1997, 3980, 1024,  102])"
55,1,"['results', 'tables']", Background Questions,seg_17,"this could be sex, age, marital status, type of accommodation, residence, education, employment, annual income, etc. these questions are used to group results in tables and graphs.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0.])","tensor([4281, 3980])","tensor([  101,  2023,  2071,  2022,  3348,  1010,  2287,  1010, 23143,  3570,
         1010,  2828,  1997, 11366,  1010,  5039,  1010,  2495,  1010,  6107,
         1010,  3296,  3318,  1010,  4385,  1012,  2122,  3980,  2024,  2109,
         2000,  2177,  3463,  1999,  7251,  1998, 19287,  1012,   102])"
56,1,"['tables', 'distribution']", Background Questions,seg_17,"for example you create tables, where you study the distribution of the answer to a question across different groups (e.g., age groups). more can be found on this in chaps. 2 and 5.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([4281, 3980])","tensor([  101,  2005,  2742,  2017,  3443,  7251,  1010,  2073,  2017,  2817,
         1996,  4353,  1997,  1996,  3437,  2000,  1037,  3160,  2408,  2367,
         2967,  1006,  1041,  1012,  1043,  1012,  1010,  2287,  2967,  1007,
         1012,  2062,  2064,  2022,  2179,  2006,  2023,  1999, 15775,  4523,
         1012,  1016,  1998,  1019,  1012,   102])"
57,0,[], Study Questions,seg_19,they constitute the body of the questionnaire.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101,  2027, 12346,  1996,  2303,  1997,  1996,  3160, 20589,  1012,
          102])"
58,0,[], Study Questions,seg_19,a few examples:,tensor(0),"tensor([0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([ 101, 1037, 2261, 4973, 1024,  102])"
59,0,['e'], Study Questions,seg_19,(a) are you in favor of or against nuclear power? (b) were you employed last week? (c) how many nights in a hotel did you have last year? (d) what was your annual income last year? (e) do you think the prime minister is doing a good job?,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([ 101, 1006, 1037, 1007, 2024, 2017, 1999, 5684, 1997, 2030, 2114, 4517,
        2373, 1029, 1006, 1038, 1007, 2020, 2017, 4846, 2197, 2733, 1029, 1006,
        1039, 1007, 2129, 2116, 6385, 1999, 1037, 3309, 2106, 2017, 2031, 2197,
        2095, 1029, 1006, 1040, 1007, 2054, 2001, 2115, 3296, 3318, 2197, 2095,
        1029, 1006, 1041, 1007, 2079, 2017, 2228, 1996, 3539, 2704, 2003, 2725,
        1037, 2204, 3105, 1029,  102])"
60,0,[], Study Questions,seg_19,it is important to distinguish between closed and open questions:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101,  2009,  2003,  2590,  2000, 10782,  2090,  2701,  1998,  2330,
         3980,  1024,   102])"
61,0,[], Study Questions,seg_19,1. closed questions are of the type:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([ 101, 1015, 1012, 2701, 3980, 2024, 1997, 1996, 2828, 1024,  102])"
62,0,[], Study Questions,seg_19,what education do you have?,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([ 101, 2054, 2495, 2079, 2017, 2031, 1029,  102])"
63,0,[], Study Questions,seg_19,– elementary education – apprentice/technical education – secondary education – higher education,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101,  1516,  4732,  2495,  1516, 13357,  1013,  4087,  2495,  1516,
         3905,  2495,  1516,  3020,  2495,   102])"
64,1,"['categories', 'data']", Study Questions,seg_19,"there are, in other words, a limited number of categories (often only two, such as yes/no). the advantage is that it is easy to handle the data processing afterwards, because it is not necessary to do any “recoding” of the responses. the disadvantage is that the respondent may not find the right answer option. 2. open questions are of the type:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2817, 3980])","tensor([  101,  2045,  2024,  1010,  1999,  2060,  2616,  1010,  1037,  3132,
         2193,  1997,  7236,  1006,  2411,  2069,  2048,  1010,  2107,  2004,
         2748,  1013,  2053,  1007,  1012,  1996,  5056,  2003,  2008,  2009,
         2003,  3733,  2000,  5047,  1996,  2951,  6364,  5728,  1010,  2138,
         2009,  2003,  2025,  4072,  2000,  2079,  2151,  1523, 28667,  7716,
         2075,  1524,  1997,  1996, 10960,  1012,  1996, 20502,  2003,  2008,
         1996,  6869,  4765,  2089,  2025,  2424,  1996,  2157,  3437,  5724,
         1012,  1016,  1012,  2330,  3980,  2024,  1997,  1996,  2828,  1024,
          102])"
65,1,"['categories', 'data']", Study Questions,seg_19,what education do you have? respondents have the opportunity towrite any free text.the advantage is that the respondent always has an answer option. the disadvantage is that it is cumbersome and time consuming to handle the data processing afterwards: a personwith subject matter knowledge must “code” the free text into well-defined categories. 3. semi-closed questions are of the type:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2817, 3980])","tensor([  101,  2054,  2495,  2079,  2017,  2031,  1029, 25094,  2031,  1996,
         4495, 15805, 17625,  2151,  2489,  3793,  1012,  1996,  5056,  2003,
         2008,  1996,  6869,  4765,  2467,  2038,  2019,  3437,  5724,  1012,
         1996, 20502,  2003,  2008,  2009,  2003, 13988, 17198,  8462,  1998,
         2051, 15077,  2000,  5047,  1996,  2951,  6364,  5728,  1024,  1037,
         2711, 24415,  3395,  3043,  3716,  2442,  1523,  3642,  1524,  1996,
         2489,  3793,  2046,  2092,  1011,  4225,  7236,  1012,  1017,  1012,
         4100,  1011,  2701,  3980,  2024,  1997,  1996,  2828,  1024,   102])"
66,0,[], Study Questions,seg_19,what education do you have?,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([ 101, 2054, 2495, 2079, 2017, 2031, 1029,  102])"
67,0,[], Study Questions,seg_19,– elementary education – apprentice/technical education – secondary education,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101,  1516,  4732,  2495,  1516, 13357,  1013,  4087,  2495,  1516,
         3905,  2495,   102])"
68,0,[], Study Questions,seg_19,"this is just a closed question, which has an “other” category with the opportunity to write free text. this combines the best from the closed and open questions.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101,  2023,  2003,  2074,  1037,  2701,  3160,  1010,  2029,  2038,
         2019,  1523,  2060,  1524,  4696,  2007,  1996,  4495,  2000,  4339,
         2489,  3793,  1012,  2023, 13585,  1996,  2190,  2013,  1996,  2701,
         1998,  2330,  3980,  1012,   102])"
69,1,"['categories', 'response']", Study Questions,seg_19,many questions in questionnaires express an assessment. there are a number of ordered response categories.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0.])","tensor([2817, 3980])","tensor([  101,  2116,  3980,  1999,  3160, 20589,  2015,  4671,  2019,  7667,
         1012,  2045,  2024,  1037,  2193,  1997,  3641,  3433,  7236,  1012,
          102])"
70,0,[], Study Questions,seg_19,an example: what do you think about the course?,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([ 101, 2019, 2742, 1024, 2054, 2079, 2017, 2228, 2055, 1996, 2607, 1029,
         102])"
71,1,['categories'], Study Questions,seg_19,"the number of categories in this kind of question is a topic of much discussion. there is consensus that the number of categories should not be too big and not too small either! approximately three to seven categories is probably the best, five answer categories is perhaps most commonly used.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([2817, 3980])","tensor([  101,  1996,  2193,  1997,  7236,  1999,  2023,  2785,  1997,  3160,
         2003,  1037,  8476,  1997,  2172,  6594,  1012,  2045,  2003, 10465,
         2008,  1996,  2193,  1997,  7236,  2323,  2025,  2022,  2205,  2502,
         1998,  2025,  2205,  2235,  2593,   999,  3155,  2093,  2000,  2698,
         7236,  2003,  2763,  1996,  2190,  1010,  2274,  3437,  7236,  2003,
         3383,  2087,  4141,  2109,  1012,   102])"
72,1,['categories'], Study Questions,seg_19,"on the other hand, many people prefer an even number of categories. it is based on an idea that an odd number of categories (for example 5) encourage the respondent to choose the middle category, because it is convenient. therefore, many prefer four or maybe six answer categories.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0.])","tensor([2817, 3980])","tensor([  101,  2006,  1996,  2060,  2192,  1010,  2116,  2111,  9544,  2019,
         2130,  2193,  1997,  7236,  1012,  2009,  2003,  2241,  2006,  2019,
         2801,  2008,  2019,  5976,  2193,  1997,  7236,  1006,  2005,  2742,
         1019,  1007,  8627,  1996,  6869,  4765,  2000,  5454,  1996,  2690,
         4696,  1010,  2138,  2009,  2003, 14057,  1012,  3568,  1010,  2116,
         9544,  2176,  2030,  2672,  2416,  3437,  7236,  1012,   102])"
73,1,"['biased', 'risk']", Study Questions,seg_19,"introduction questions are used now and then, particularly with complex issues, where the respondent needs to be “guided” in the topic. it is very important to be careful with the wording of these questions, otherwise you run the risk of being biased (“leading questions”). the risk of influencing the respondents is simply too big. there exist several examples on this in connection with, e.g., polls.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2817, 3980])","tensor([  101,  4955,  3980,  2024,  2109,  2085,  1998,  2059,  1010,  3391,
         2007,  3375,  3314,  1010,  2073,  1996,  6869,  4765,  3791,  2000,
         2022,  1523,  8546,  1524,  1999,  1996,  8476,  1012,  2009,  2003,
         2200,  2590,  2000,  2022,  6176,  2007,  1996,  2773,  2075,  1997,
         2122,  3980,  1010,  4728,  2017,  2448,  1996,  3891,  1997,  2108,
        25352,  1006,  1523,  2877,  3980,  1524,  1007,  1012,  1996,  3891,
         1997, 25870,  1996, 25094,  2003,  3432,  2205,  2502,  1012,  2045,
         4839,  2195,  4973,  2006,  2023,  1999,  4434,  2007,  1010,  1041,
         1012,  1043,  1012,  1010, 14592,  1012,   102])"
74,0,[], Study Questions,seg_19,"deliberately influencing the respondent in order to provide a certain answer is (hopefully!) a rare exception, although it certainly occurs.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101,  9969, 25870,  1996,  6869,  4765,  1999,  2344,  2000,  3073,
         1037,  3056,  3437,  2003,  1006, 11504,   999,  1007,  1037,  4678,
         6453,  1010,  2348,  2009,  5121,  5158,  1012,   102])"
75,0,[], Study Questions,seg_19,"control questions are used to ensure that the respondent has understood the questions and answered honestly, e.g., within a very complicated and sensitive issue.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([ 101, 2491, 3980, 2024, 2109, 2000, 5676, 2008, 1996, 6869, 4765, 2038,
        5319, 1996, 3980, 1998, 4660, 9826, 1010, 1041, 1012, 1043, 1012, 1010,
        2306, 1037, 2200, 8552, 1998, 7591, 3277, 1012,  102])"
76,0,[], Study Questions,seg_19,"you therefore ask the “same” question in a new wording. or you ask a supplementary question, which may shed light on the responses to some of the key questions of the study.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101,  2017,  3568,  3198,  1996,  1523,  2168,  1524,  3160,  1999,
         1037,  2047,  2773,  2075,  1012,  2030,  2017,  3198,  1037, 26215,
         3160,  1010,  2029,  2089,  8328,  2422,  2006,  1996, 10960,  2000,
         2070,  1997,  1996,  3145,  3980,  1997,  1996,  2817,  1012,   102])"
77,1,['control'], Study Questions,seg_19,"an example from a survey, in which people were asked about their alcohol consumption the day before: the following control question was asked:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([2817, 3980])","tensor([ 101, 2019, 2742, 2013, 1037, 5002, 1010, 1999, 2029, 2111, 2020, 2356,
        2055, 2037, 6544, 8381, 1996, 2154, 2077, 1024, 1996, 2206, 2491, 3160,
        2001, 2356, 1024,  102])"
78,1,"['table', 'distribution']", Study Questions,seg_19,"do you normally drink more, the same or less? the distribution of the answers is given in table 1.2.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0.])","tensor([2817, 3980])","tensor([ 101, 2079, 2017, 5373, 4392, 2062, 1010, 1996, 2168, 2030, 2625, 1029,
        1996, 4353, 1997, 1996, 6998, 2003, 2445, 1999, 2795, 1015, 1012, 1016,
        1012,  102])"
79,1,['case'], Study Questions,seg_19,"the interviews were evenly spread over all weekdays and all months of 1 year! thus, there should be a roughly equal number of people, who normally drink more respectively less. this was not the case, because the respondents want to appear not to drink too much. this may indicate that the respondents did not answer honestly!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2817, 3980])","tensor([  101,  1996,  7636,  2020, 18030,  3659,  2058,  2035, 19759,  1998,
         2035,  2706,  1997,  1015,  2095,   999,  2947,  1010,  2045,  2323,
         2022,  1037,  5560,  5020,  2193,  1997,  2111,  1010,  2040,  5373,
         4392,  2062,  4414,  2625,  1012,  2023,  2001,  2025,  1996,  2553,
         1010,  2138,  1996, 25094,  2215,  2000,  3711,  2025,  2000,  4392,
         2205,  2172,  1012,  2023,  2089,  5769,  2008,  1996, 25094,  2106,
         2025,  3437,  9826,   999,   102])"
80,1,['risk'], Study Questions,seg_19,it is very important not to ask too many questions! there is a tendency to ask a lot of redundant questions just “out of curiosity”. this bores the respondent and increases the risk that the respondent will skip the last questions. maybe those questions were essential?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([2817, 3980])","tensor([  101,  2009,  2003,  2200,  2590,  2025,  2000,  3198,  2205,  2116,
         3980,   999,  2045,  2003,  1037, 11765,  2000,  3198,  1037,  2843,
         1997, 21707,  3980,  2074,  1523,  2041,  1997, 10628,  1524,  1012,
         2023,  8501,  2015,  1996,  6869,  4765,  1998,  7457,  1996,  3891,
         2008,  1996,  6869,  4765,  2097, 13558,  1996,  2197,  3980,  1012,
         2672,  2216,  3980,  2020,  6827,  1029,   102])"
81,0,[], Study Questions,seg_19,"a basic requirement for the questionnaire is to ensure a clear communication, i.e., the use of simple and clear formulations. keep it simple!",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101,  1037,  3937,  9095,  2005,  1996,  3160, 20589,  2003,  2000,
         5676,  1037,  3154,  4807,  1010,  1045,  1012,  1041,  1012,  1010,
         1996,  2224,  1997,  3722,  1998,  3154, 20219,  2015,  1012,  2562,
         2009,  3722,   999,   102])"
82,0,[], Study Questions,seg_19,it is important that the researcher when formulating the questions tries to put himself in the respondent’s place. keep in mind that the respondent is not an expert!,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101,  2009,  2003,  2590,  2008,  1996, 10753,  2043,  5675,  3436,
         1996,  3980,  5363,  2000,  2404,  2370,  1999,  1996,  6869,  4765,
         1521,  1055,  2173,  1012,  2562,  1999,  2568,  2008,  1996,  6869,
         4765,  2003,  2025,  2019,  6739,   999,   102])"
83,1,"['categories', 'results']", Study Questions,seg_19,"if the results of the questionnaire are to be compared with results from other questionnaires, care must be taken to ensure comparability. this means that you pose the same questions and use the same answer categories as in the other surveys.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([2817, 3980])","tensor([  101,  2065,  1996,  3463,  1997,  1996,  3160, 20589,  2024,  2000,
         2022,  4102,  2007,  3463,  2013,  2060,  3160, 20589,  2015,  1010,
         2729,  2442,  2022,  2579,  2000,  5676,  4012, 28689,  8553,  1012,
         2023,  2965,  2008,  2017, 13382,  1996,  2168,  3980,  1998,  2224,
         1996,  2168,  3437,  7236,  2004,  1999,  1996,  2060, 12265,  1012,
          102])"
84,0,[], Study Questions,seg_19,"remember to always ask about one thing at a time. it may be tempting to put two issues together in one question, but the result is rarely good.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101,  3342,  2000,  2467,  3198,  2055,  2028,  2518,  2012,  1037,
         2051,  1012,  2009,  2089,  2022, 23421,  2000,  2404,  2048,  3314,
         2362,  1999,  2028,  3160,  1010,  2021,  1996,  2765,  2003,  6524,
         2204,  1012,   102])"
85,0,[], Study Questions,seg_19,an example from real life is the following question: are you shopping,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([ 101, 2019, 2742, 2013, 2613, 2166, 2003, 1996, 2206, 3160, 1024, 2024,
        2017, 6023,  102])"
86,0,[], Study Questions,seg_19,"here, the researcher is dealing with two issues:",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101,  2182,  1010,  1996, 10753,  2003,  7149,  2007,  2048,  3314,
         1024,   102])"
87,0,[], Study Questions,seg_19,(a) whether the respondent is responsible for a large or small proportion of,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101,  1006,  1037,  1007,  3251,  1996,  6869,  4765,  2003,  3625,
         2005,  1037,  2312,  2030,  2235, 10817,  1997,   102])"
88,0,[], Study Questions,seg_19,purchases in the household. (b) whether the respondent is alone or together with others when shopping.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101, 17402,  1999,  1996,  4398,  1012,  1006,  1038,  1007,  3251,
         1996,  6869,  4765,  2003,  2894,  2030,  2362,  2007,  2500,  2043,
         6023,  1012,   102])"
89,0,[], Study Questions,seg_19,"the result was, that many men answered “alone” because they buy things for themselves, for example in a kiosk. on the other hand, many women answered “together with others” because they bring their children when shopping in the supermarket. the purpose of the question was to identify the person responsible for most of the shopping in the household.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2817, 3980])","tensor([  101,  1996,  2765,  2001,  1010,  2008,  2116,  2273,  4660,  1523,
         2894,  1524,  2138,  2027,  4965,  2477,  2005,  3209,  1010,  2005,
         2742,  1999,  1037, 11382,  2891,  2243,  1012,  2006,  1996,  2060,
         2192,  1010,  2116,  2308,  4660,  1523,  2362,  2007,  2500,  1524,
         2138,  2027,  3288,  2037,  2336,  2043,  6023,  1999,  1996, 17006,
         1012,  1996,  3800,  1997,  1996,  3160,  2001,  2000,  6709,  1996,
         2711,  3625,  2005,  2087,  1997,  1996,  6023,  1999,  1996,  4398,
         1012,   102])"
90,1,['test'], Study Questions,seg_19,it is always a good idea to test the questions on a few people in order to see if they are understood and are answered without problems. the respondents might be “friends and relatives”! such a study (known as a pilot study) can detect many problems before it is too late!,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([2817, 3980])","tensor([  101,  2009,  2003,  2467,  1037,  2204,  2801,  2000,  3231,  1996,
         3980,  2006,  1037,  2261,  2111,  1999,  2344,  2000,  2156,  2065,
         2027,  2024,  5319,  1998,  2024,  4660,  2302,  3471,  1012,  1996,
        25094,  2453,  2022,  1523,  2814,  1998,  9064,  1524,   999,  2107,
         1037,  2817,  1006,  2124,  2004,  1037,  4405,  2817,  1007,  2064,
        11487,  2116,  3471,  2077,  2009,  2003,  2205,  2397,   999,   102])"
91,1,"['results', 'errors', 'data collection', 'data', 'case']", Sources of Errors in Surveys,seg_21,"regardless of how data are collected in a questionnaire, problems may arise, which in the worst case may influence the results significantly. there may be errors that come from the question wording and errors arising from the data collection.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([  101,  7539,  1997,  2129,  2951,  2024,  5067,  1999,  1037,  3160,
        20589,  1010,  3471,  2089, 13368,  1010,  2029,  1999,  1996,  5409,
         2553,  2089,  3747,  1996,  3463,  6022,  1012,  2045,  2089,  2022,
        10697,  2008,  2272,  2013,  1996,  3160,  2773,  2075,  1998, 10697,
        17707,  2013,  1996,  2951,  3074,  1012,   102])"
92,1,['errors'], Sources of Errors in Surveys,seg_21,many errors in surveys come from problems with the wording of the questions. this can be,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([  101,  2116, 10697,  1999, 12265,  2272,  2013,  3471,  2007,  1996,
         2773,  2075,  1997,  1996,  3980,  1012,  2023,  2064,  2022,   102])"
93,0,[], Sources of Errors in Surveys,seg_21,"– unclear questions. – people cannot or will not respond, they do not know, etc. – unconscious influence on the respondent, e.g., in connection with “intro- duction questions.”",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([  101,  1516, 10599,  3980,  1012,  1516,  2111,  3685,  2030,  2097,
         2025,  6869,  1010,  2027,  2079,  2025,  2113,  1010,  4385,  1012,
         1516,  9787,  3747,  2006,  1996,  6869,  4765,  1010,  1041,  1012,
         1043,  1012,  1010,  1999,  4434,  2007,  1523, 17174,  1011, 23245,
         3258,  3980,  1012,  1524,   102])"
94,1,['errors'], Sources of Errors in Surveys,seg_21,these errors can only avoided by a careful wording of the questions!,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([  101,  2122, 10697,  2064,  2069,  9511,  2011,  1037,  6176,  2773,
         2075,  1997,  1996,  3980,   999,   102])"
95,1,"['data collection', 'data']", Sources of Errors in Surveys,seg_21,non-response (*) is another major problem. this means that some respondents do not participate in the survey. this may be due to problems in the data collection.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([  101,  2512,  1011,  3433,  1006,  1008,  1007,  2003,  2178,  2350,
         3291,  1012,  2023,  2965,  2008,  2070, 25094,  2079,  2025,  5589,
         1999,  1996,  5002,  1012,  2023,  2089,  2022,  2349,  2000,  3471,
         1999,  1996,  2951,  3074,  1012,   102])"
96,1,"['mean', 'results', 'average']", Sources of Errors in Surveys,seg_21,"non-response may mean that the results of the survey are misleading! for example, it is often difficult (both by telephone interview and by visit) to catch the “busy businessman,” who is seldom at home (working more hours per week than the average). if you are not working hard to reach this group of respondents, it is obvious that the survey results will be “unbalanced”.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([  101,  2512,  1011,  3433,  2089,  2812,  2008,  1996,  3463,  1997,
         1996,  5002,  2024, 22369,   999,  2005,  2742,  1010,  2009,  2003,
         2411,  3697,  1006,  2119,  2011,  7026,  4357,  1998,  2011,  3942,
         1007,  2000,  4608,  1996,  1523,  5697,  6883,  1010,  1524,  2040,
         2003, 15839,  2012,  2188,  1006,  2551,  2062,  2847,  2566,  2733,
         2084,  1996,  2779,  1007,  1012,  2065,  2017,  2024,  2025,  2551,
         2524,  2000,  3362,  2023,  2177,  1997, 25094,  1010,  2009,  2003,
         5793,  2008,  1996,  5002,  3463,  2097,  2022,  1523,  4895, 26657,
         2094,  1524,  1012,   102])"
97,0,[], Sources of Errors in Surveys,seg_21,the two most frequent causes of non-response are:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([ 101, 1996, 2048, 2087, 6976, 5320, 1997, 2512, 1011, 3433, 2024, 1024,
         102])"
98,0,[], Sources of Errors in Surveys,seg_21,"– people are not at home, or not reached (by telephone interview or visits). – refusal: people may simply ignore any attempt of contact.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([  101,  1516,  2111,  2024,  2025,  2012,  2188,  1010,  2030,  2025,
         2584,  1006,  2011,  7026,  4357,  2030,  7879,  1007,  1012,  1516,
        13948,  1024,  2111,  2089,  3432,  8568,  2151,  3535,  1997,  3967,
         1012,   102])"
99,1,"['rate', 'response']", Sources of Errors in Surveys,seg_21,"the only solution to not reaching people is tomake numerous attempts of contact (telephone calls, visits, etc.). typically you should make attempts both in the weekend and on weekdays, both in daytime and evening hours, if you want to increase the response rate. this means at least four contact attempts, which of course is costly...",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([  101,  1996,  2069,  5576,  2000,  2025,  4285,  2111,  2003,  3419,
        13808,  3365,  4740,  1997,  3967,  1006,  7026,  4455,  1010,  7879,
         1010,  4385,  1012,  1007,  1012,  4050,  2017,  2323,  2191,  4740,
         2119,  1999,  1996,  5353,  1998,  2006, 19759,  1010,  2119,  1999,
        12217,  1998,  3944,  2847,  1010,  2065,  2017,  2215,  2000,  3623,
         1996,  3433,  3446,  1012,  2023,  2965,  2012,  2560,  2176,  3967,
         4740,  1010,  2029,  1997,  2607,  2003, 17047,  1012,  1012,  1012,
          102])"
100,0,[], Sources of Errors in Surveys,seg_21,"the problem with refusal is growing as the number of more or less dubious marketing companies increases, which again increases the number of phone calls to each household. people have simply had enough!",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([  101,  1996,  3291,  2007, 13948,  2003,  3652,  2004,  1996,  2193,
         1997,  2062,  2030,  2625, 22917,  5821,  3316,  7457,  1010,  2029,
         2153,  7457,  1996,  2193,  1997,  3042,  4455,  2000,  2169,  4398,
         1012,  2111,  2031,  3432,  2018,  2438,   999,   102])"
101,0,[], Sources of Errors in Surveys,seg_21,"the problem is of course impossible to avoid completely. however, it is possible to reduce it:",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([ 101, 1996, 3291, 2003, 1997, 2607, 5263, 2000, 4468, 3294, 1012, 2174,
        1010, 2009, 2003, 2825, 2000, 5547, 2009, 1024,  102])"
102,0,[], Sources of Errors in Surveys,seg_21,"– first of all, proper training of the interviewers is important, so that they can",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([ 101, 1516, 2034, 1997, 2035, 1010, 5372, 2731, 1997, 1996, 4357, 2545,
        2003, 2590, 1010, 2061, 2008, 2027, 2064,  102])"
103,0,[], Sources of Errors in Surveys,seg_21,"better cope with “troublesome” respondents. – one can also offer the respondents a reward (e.g., a lottery ticket) for participat-",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([  101,  2488, 11997,  2007,  1523, 13460,  8462,  1524, 25094,  1012,
         1516,  2028,  2064,  2036,  3749,  1996, 25094,  1037, 10377,  1006,
         1041,  1012,  1043,  1012,  1010,  1037, 15213,  7281,  1007,  2005,
         2112, 28775,  4502,  2102,  1011,   102])"
104,1,"['data collection', 'data']", Sources of Errors in Surveys,seg_21,ing in the survey. – combining multiple data collection methods (for instance telephone interviews,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([  101, 13749,  1999,  1996,  5002,  1012,  1516, 11566,  3674,  2951,
         3074,  4725,  1006,  2005,  6013,  7026,  7636,   102])"
105,1,"['rate', 'response']", Sources of Errors in Surveys,seg_21,"and visits) can also provide a higher response rate. this again increases the costs, however.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 4216,  1997, 10697,  1999, 12265])","tensor([ 101, 1998, 7879, 1007, 2064, 2036, 3073, 1037, 3020, 3433, 3446, 1012,
        2023, 2153, 7457, 1996, 5366, 1010, 2174, 1012,  102])"
106,1,"['data collection', 'data']", Comparing Methods of Data Collection,seg_23,the main types of data collection in surveys are:,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  1996,  2364,  4127,  1997,  2951,  3074,  1999, 12265,  2024,
         1024,   102])"
107,1,"['data collection', 'data']", Comparing Methods of Data Collection,seg_23,"– internet, e-mail or other electronic data collection (e.g., text messages on mobile phones) – mail questionnaire – telephone interviews – personal interview (“visit” or “face-to-face” interview)",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  1516,  4274,  1010,  1041,  1011,  5653,  2030,  2060,  4816,
         2951,  3074,  1006,  1041,  1012,  1043,  1012,  1010,  3793,  7696,
         2006,  4684, 11640,  1007,  1516,  5653,  3160, 20589,  1516,  7026,
         7636,  1516,  3167,  4357,  1006,  1523,  3942,  1524,  2030,  1523,
         2227,  1011,  2000,  1011,  2227,  1524,  4357,  1007,   102])"
108,1,"['data collection', 'method', 'data']", Comparing Methods of Data Collection,seg_23,"when selecting a data collection method, you should obviously evaluate both cost and quality.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  2043, 17739,  1037,  2951,  3074,  4118,  1010,  2017,  2323,
         5525, 16157,  2119,  3465,  1998,  3737,  1012,   102])"
109,0,[], Comparing Methods of Data Collection,seg_23,"usually, there is a relationship between cost and quality! the above list is roughly arranged in order of increasing cost and quality. thus internet or e-mail questionnaires are cheap, while personal interviews are expensive.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  2788,  1010,  2045,  2003,  1037,  3276,  2090,  3465,  1998,
         3737,   999,  1996,  2682,  2862,  2003,  5560,  5412,  1999,  2344,
         1997,  4852,  3465,  1998,  3737,  1012,  2947,  4274,  2030,  1041,
         1011,  5653,  3160, 20589,  2015,  2024, 10036,  1010,  2096,  3167,
         7636,  2024,  6450,  1012,   102])"
110,1,['rate'], Comparing Methods of Data Collection,seg_23,"the quality depends largely on the non-response rate, i.e., proportion of respondents who do not participate. the non-response rate is considerable by internet or email; by personal interviews it is much smaller.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  1996,  3737,  9041,  4321,  2006,  1996,  2512,  1011,  3433,
         3446,  1010,  1045,  1012,  1041,  1012,  1010, 10817,  1997, 25094,
         2040,  2079,  2025,  5589,  1012,  1996,  2512,  1011,  3433,  3446,
         2003,  6196,  2011,  4274,  2030, 10373,  1025,  2011,  3167,  7636,
         2009,  2003,  2172,  3760,  1012,   102])"
111,1,"['errors', 'process', 'data collection', 'data']", Comparing Methods of Data Collection,seg_23,"errors in data are also a quality issue. the traditional mail questionnaire gives no opportunity to correct errors during the interviewing process, as do the other data collection types.",tensor(1),"tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101, 10697,  1999,  2951,  2024,  2036,  1037,  3737,  3277,  1012,
         1996,  3151,  5653,  3160, 20589,  3957,  2053,  4495,  2000,  6149,
        10697,  2076,  1996, 27805,  2832,  1010,  2004,  2079,  1996,  2060,
         2951,  3074,  4127,  1012,   102])"
112,1,"['interaction', 'data collection', 'data']", Comparing Methods of Data Collection,seg_23,"the first two methods of data collection are carried out without an interviewer, while there is an interviewer present with the last two methods. the interaction between interviewer and respondent in the last two data collection methods has both advantages and disadvantages that can affect quality:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  1996,  2034,  2048,  4725,  1997,  2951,  3074,  2024,  3344,
         2041,  2302,  2019,  4357,  2121,  1010,  2096,  2045,  2003,  2019,
         4357,  2121,  2556,  2007,  1996,  2197,  2048,  4725,  1012,  1996,
         8290,  2090,  4357,  2121,  1998,  6869,  4765,  1999,  1996,  2197,
         2048,  2951,  3074,  4725,  2038,  2119, 12637,  1998, 20502,  2015,
         2008,  2064,  7461,  3737,  1024,   102])"
113,0,[], Comparing Methods of Data Collection,seg_23,"– advantage: the interviewer can help the respondent, if there is any doubt about",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([ 101, 1516, 5056, 1024, 1996, 4357, 2121, 2064, 2393, 1996, 6869, 4765,
        1010, 2065, 2045, 2003, 2151, 4797, 2055,  102])"
114,0,[], Comparing Methods of Data Collection,seg_23,"what is meant by a question. this can reduce the number of questions that are not answered (or answered by “do not know”, etc.). – disadvantage: the interviewer may unconsciously affect the respondent’s",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  2054,  2003,  3214,  2011,  1037,  3160,  1012,  2023,  2064,
         5547,  1996,  2193,  1997,  3980,  2008,  2024,  2025,  4660,  1006,
         2030,  4660,  2011,  1523,  2079,  2025,  2113,  1524,  1010,  4385,
         1012,  1007,  1012,  1516, 20502,  1024,  1996,  4357,  2121,  2089,
         9787,  2135,  7461,  1996,  6869,  4765,  1521,  1055,   102])"
115,1,['risk'], Comparing Methods of Data Collection,seg_23,"answer in a certain direction. this risk is largest in personal interviews, but can be minimized by careful training of interviewers.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  3437,  1999,  1037,  3056,  3257,  1012,  2023,  3891,  2003,
         2922,  1999,  3167,  7636,  1010,  2021,  2064,  2022, 18478,  2094,
         2011,  6176,  2731,  1997,  4357,  2545,  1012,   102])"
116,1,"['data collection', 'method', 'factor', 'data']", Comparing Methods of Data Collection,seg_23,"another key factor in the choice of data collection method is the questionnaire’s length, i.e., number of questions.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  2178,  3145,  5387,  1999,  1996,  3601,  1997,  2951,  3074,
         4118,  2003,  1996,  3160, 20589,  1521,  1055,  3091,  1010,  1045,
         1012,  1041,  1012,  1010,  2193,  1997,  3980,  1012,   102])"
117,1,"['data collection', 'mean', 'data']", Comparing Methods of Data Collection,seg_23,"the first two data collection methods should not be used for very long questionnaires, where the respondent’s (lack of) patience will often mean “less serious” (or no!) replies.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  1996,  2034,  2048,  2951,  3074,  4725,  2323,  2025,  2022,
         2109,  2005,  2200,  2146,  3160, 20589,  2015,  1010,  2073,  1996,
         6869,  4765,  1521,  1055,  1006,  3768,  1997,  1007, 11752,  2097,
         2411,  2812,  1523,  2625,  3809,  1524,  1006,  2030,  2053,   999,
         1007, 14054,  1012,   102])"
118,1,"['data collection', 'data']", Comparing Methods of Data Collection,seg_23,"the last two data collection methods involve a direct contact between interviewer and respondent. the talented interviewer can exploit this, if a respondent is becoming impatient.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  1996,  2197,  2048,  2951,  3074,  4725,  9125,  1037,  3622,
         3967,  2090,  4357,  2121,  1998,  6869,  4765,  1012,  1996, 10904,
         4357,  2121,  2064, 18077,  2023,  1010,  2065,  1037,  6869,  4765,
         2003,  3352, 17380,  1012,   102])"
119,1,['factor'], Comparing Methods of Data Collection,seg_23,"the possibility of “previewing” is yet another factor to take into account. in personal interviews different visual effects, which can help the respondent to understand a difficult issue, can be shown. by telephone interview audio files can",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  1996,  6061,  1997,  1523, 19236,  2075,  1524,  2003,  2664,
         2178,  5387,  2000,  2202,  2046,  4070,  1012,  1999,  3167,  7636,
         2367,  5107,  3896,  1010,  2029,  2064,  2393,  1996,  6869,  4765,
         2000,  3305,  1037,  3697,  3277,  1010,  2064,  2022,  3491,  1012,
         2011,  7026,  4357,  5746,  6764,  2064,   102])"
120,0,[], Comparing Methods of Data Collection,seg_23,"be played during the interview. in mail questionnaires one can display pictures. finally, in internet interviews “almost everything” is possible by taking advantage of modern technology.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  2022,  2209,  2076,  1996,  4357,  1012,  1999,  5653,  3160,
        20589,  2015,  2028,  2064,  4653,  4620,  1012,  2633,  1010,  1999,
         4274,  7636,  1523,  2471,  2673,  1524,  2003,  2825,  2011,  2635,
         5056,  1997,  2715,  2974,  1012,   102])"
121,1,"['rate', 'response', 'method', 'data collection', 'combination', 'data']", Comparing Methods of Data Collection,seg_23,"often you will use a combination of two data collection methods. one collection method (often telephone) is used for most interviews. for the reminders is used a second data collection method, e.g., personal interview. the skilled interviewer can better persuade a respondent to participate in the survey. this can be used to achieve a higher response rate.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0.])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  2411,  2017,  2097,  2224,  1037,  5257,  1997,  2048,  2951,
         3074,  4725,  1012,  2028,  3074,  4118,  1006,  2411,  7026,  1007,
         2003,  2109,  2005,  2087,  7636,  1012,  2005,  1996, 14764,  2015,
         2003,  2109,  1037,  2117,  2951,  3074,  4118,  1010,  1041,  1012,
         1043,  1012,  1010,  3167,  4357,  1012,  1996, 10571,  4357,  2121,
         2064,  2488, 13984,  1037,  6869,  4765,  2000,  5589,  1999,  1996,
         5002,  1012,  2023,  2064,  2022,  2109,  2000,  6162,  1037,  3020,
         3433,  3446,  1012,   102])"
122,1,"['results', 'information']", Comparing Methods of Data Collection,seg_23,"also, this enables an assessment of whether there is a difference in the results among those who answered respectively did not answer in the first round. this provides information on the validity of the survey results.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0.])","tensor([13599,  4725,  1997,  2951,  3074])","tensor([  101,  2036,  1010,  2023, 12939,  2019,  7667,  1997,  3251,  2045,
         2003,  1037,  4489,  1999,  1996,  3463,  2426,  2216,  2040,  4660,
         4414,  2106,  2025,  3437,  1999,  1996,  2034,  2461,  1012,  2023,
         3640,  2592,  2006,  1996, 16406,  1997,  1996,  5002,  3463,  1012,
          102])"
123,1,"['information', 'sample', 'register', 'process']", Example Continued,seg_25,"fitness club has a register of all customers. this register contains contact information (name, address, etc.) as well as information on sex and age of all customers; this makes it easy to extract a sample of customers in the age group 12–17 for a survey. we will discuss the process of selecting a sample in chap. 6.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 2506])","tensor([  101, 10516,  2252,  2038,  1037,  4236,  1997,  2035,  6304,  1012,
         2023,  4236,  3397,  3967,  2592,  1006,  2171,  1010,  4769,  1010,
         4385,  1012,  1007,  2004,  2092,  2004,  2592,  2006,  3348,  1998,
         2287,  1997,  2035,  6304,  1025,  2023,  3084,  2009,  3733,  2000,
        14817,  1037,  7099,  1997,  6304,  1999,  1996,  2287,  2177,  2260,
         1516,  2459,  2005,  1037,  5002,  1012,  2057,  2097,  6848,  1996,
         2832,  1997, 17739,  1037,  7099,  1999, 15775,  2361,  1012,  1020,
         1012,   102])"
124,0,[], Example Continued,seg_25,"fitness club chooses to send a mail questionnaire to all the selected customers. it is a relatively inexpensive solution. a disadvantage is that for instance the questions about their health are answered subjectively. this is, indeed, a common feature of most questionnaire surveys!",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0])","tensor([2742, 2506])","tensor([  101, 10516,  2252, 15867,  2000,  4604,  1037,  5653,  3160, 20589,
         2000,  2035,  1996,  3479,  6304,  1012,  2009,  2003,  1037,  4659,
        23766,  5576,  1012,  1037, 20502,  2003,  2008,  2005,  6013,  1996,
         3980,  2055,  2037,  2740,  2024,  4660, 20714,  2135,  1012,  2023,
         2003,  1010,  5262,  1010,  1037,  2691,  3444,  1997,  2087,  3160,
        20589, 12265,   999,   102])"
125,1,['rates'], Example Continued,seg_25,"another drawback is that such surveys often lead to high non-response rates. to reduce this problem, fitness club encloses a stamped addressed envelope. also, the customers responding will participate in a lottery, where they can win smart mobile phones and other electronic gadgets, which are particularly attractive to kids of this age group.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2742, 2506])","tensor([  101,  2178,  4009,  5963,  2003,  2008,  2107, 12265,  2411,  2599,
         2000,  2152,  2512,  1011,  3433,  6165,  1012,  2000,  5547,  2023,
         3291,  1010, 10516,  2252,  4372, 20464, 27465,  1037, 20834,  8280,
        11255,  1012,  2036,  1010,  1996,  6304, 14120,  2097,  5589,  1999,
         1037, 15213,  1010,  2073,  2027,  2064,  2663,  6047,  4684, 11640,
         1998,  2060,  4816, 11721, 28682,  1010,  2029,  2024,  3391,  8702,
         2000,  4268,  1997,  2023,  2287,  2177,  1012,   102])"
126,1,"['results', 'tables']",Chapter  Presentation of Data,seg_27,"in this chapter we show how to present the results of a questionnaire survey, using graphs and tables.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0.])","tensor([3127, 8312, 1997, 2951])","tensor([  101,  1999,  2023,  3127,  2057,  2265,  2129,  2000,  2556,  1996,
         3463,  1997,  1037,  3160, 20589,  5002,  1010,  2478, 19287,  1998,
         7251,  1012,   102])"
127,1,"['combinations', 'errors', 'plots', 'statistical', 'charts', 'data']",Chapter  Presentation of Data,seg_27,"graphs (charts, plots, etc.) are suited to get a feel of patterns, structures, trends and relationships in data and thus are an invaluable supplement to a statistical analysis. they are also useful tools to find unlikely (e.g., extremely large or extremely small) data values or combinations of data values (such as a very high person who does not weigh much), which may be errors in the data.",tensor(1),"tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.])","tensor([3127, 8312, 1997, 2951])","tensor([  101, 19287,  1006,  6093,  1010, 14811,  1010,  4385,  1012,  1007,
         2024, 10897,  2000,  2131,  1037,  2514,  1997,  7060,  1010,  5090,
         1010, 12878,  1998,  6550,  1999,  2951,  1998,  2947,  2024,  2019,
         1999, 10175,  6692,  3468, 12448,  2000,  1037,  7778,  4106,  1012,
         2027,  2024,  2036,  6179,  5906,  2000,  2424,  9832,  1006,  1041,
         1012,  1043,  1012,  1010,  5186,  2312,  2030,  5186,  2235,  1007,
         2951,  5300,  2030, 14930,  1997,  2951,  5300,  1006,  2107,  2004,
         1037,  2200,  2152,  2711,  2040,  2515,  2025, 17042,  2172,  1007,
         1010,  2029,  2089,  2022, 10697,  1999,  1996,  2951,  1012,   102])"
128,0,[],Chapter  Presentation of Data,seg_27,"it is easy to create graphs with a spreadsheet, such as microsoft excel or open office calc. here, only the main types of graphs are covered. there are many other types of graphs than those shown here. see the “help” menu in your spreadsheet to see the possibilities!",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([3127, 8312, 1997, 2951])","tensor([  101,  2009,  2003,  3733,  2000,  3443, 19287,  2007,  1037, 20861,
        21030,  2102,  1010,  2107,  2004,  7513, 24970,  2030,  2330,  2436,
        10250,  2278,  1012,  2182,  1010,  2069,  1996,  2364,  4127,  1997,
        19287,  2024,  3139,  1012,  2045,  2024,  2116,  2060,  4127,  1997,
        19287,  2084,  2216,  3491,  2182,  1012,  2156,  1996,  1523,  2393,
         1524, 12183,  1999,  2115, 20861, 21030,  2102,  2000,  2156,  1996,
        12020,   999,   102])"
129,1,['data'],Chapter  Presentation of Data,seg_27,"tables are another way of presenting data, which is also discussed briefly here.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3127, 8312, 1997, 2951])","tensor([  101,  7251,  2024,  2178,  2126,  1997, 10886,  2951,  1010,  2029,
         2003,  2036,  6936,  4780,  2182,  1012,   102])"
130,1,"['charts', 'information', 'table']", Bar Charts,seg_29,bar charts are familiar to most people. they conveniently summarize information from a table in a clear and illustrative manner.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3347, 6093])","tensor([  101,  3347,  6093,  2024,  5220,  2000,  2087,  2111,  1012,  2027,
        14057,  2135,  7680,  7849,  4697,  2592,  2013,  1037,  2795,  1999,
         1037,  3154,  1998,  5665, 19966, 18514,  5450,  1012,   102])"
131,1,"['loss', 'average', 'table']", Bar Charts,seg_29,"let us consider an example from the “fitness club” survey. as one focus of the young customers is weight loss, a table of average height and weight by sex is interesting. it may, in tabular form look as shown in table 2.1.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([3347, 6093])","tensor([  101,  2292,  2149,  5136,  2019,  2742,  2013,  1996,  1523, 10516,
         2252,  1524,  5002,  1012,  2004,  2028,  3579,  1997,  1996,  2402,
         6304,  2003,  3635,  3279,  1010,  1037,  2795,  1997,  2779,  4578,
         1998,  3635,  2011,  3348,  2003,  5875,  1012,  2009,  2089,  1010,
         1999, 21628,  7934,  2433,  2298,  2004,  3491,  1999,  2795,  1016,
         1012,  1015,  1012,   102])"
132,1,"['bar chart', 'chart']", Bar Charts,seg_29,"a corresponding bar chart is shown in fig. 2.1. we see, at a glance, that the boys are both slightly taller and slightly heavier than the girls.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3347, 6093])","tensor([  101,  1037,  7978,  3347,  3673,  2003,  3491,  1999, 20965,  1012,
         1016,  1012,  1015,  1012,  2057,  2156,  1010,  2012,  1037,  6054,
         1010,  2008,  1996,  3337,  2024,  2119,  3621, 12283,  1998,  3621,
        11907,  2084,  1996,  3057,  1012,   102])"
133,1,"['moment', 'graphical', 'chart', 'average']", Bar Charts,seg_29,"it is also well known that this type of chart can be constructed to “cheat” with the axes. if we consider for a moment only the (average) height, it could in graphical form look like the one shown in fig. 2.2.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3347, 6093])","tensor([  101,  2009,  2003,  2036,  2092,  2124,  2008,  2023,  2828,  1997,
         3673,  2064,  2022,  3833,  2000,  1523, 21910,  1524,  2007,  1996,
        19589,  1012,  2065,  2057,  5136,  2005,  1037,  2617,  2069,  1996,
         1006,  2779,  1007,  4578,  1010,  2009,  2071,  1999, 20477,  2433,
         2298,  2066,  1996,  2028,  3491,  1999, 20965,  1012,  1016,  1012,
         1016,  1012,   102])"
134,1,"['information', 'chart']", Bar Charts,seg_29,"this chart contains the same information on the height of girls and boys, as the graph above. however, most people will get a wrong impression of the situation by considering this chart. the boys seem to be much taller than the girls, because the lower part of the bars is cut off!",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([3347, 6093])","tensor([  101,  2023,  3673,  3397,  1996,  2168,  2592,  2006,  1996,  4578,
         1997,  3057,  1998,  3337,  1010,  2004,  1996, 10629,  2682,  1012,
         2174,  1010,  2087,  2111,  2097,  2131,  1037,  3308,  8605,  1997,
         1996,  3663,  2011,  6195,  2023,  3673,  1012,  1996,  3337,  4025,
         2000,  2022,  2172, 12283,  2084,  1996,  3057,  1010,  2138,  1996,
         2896,  2112,  1997,  1996,  6963,  2003,  3013,  2125,   999,   102])"
135,1,"['charts', 'bar charts']", Bar Charts,seg_29,"it is therefore important to be aware of the axes when studying bar charts, as well as when constructing them.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([3347, 6093])","tensor([  101,  2009,  2003,  3568,  2590,  2000,  2022,  5204,  1997,  1996,
        19589,  2043,  5702,  3347,  6093,  1010,  2004,  2092,  2004,  2043,
        15696,  2068,  1012,   102])"
136,1,"['bar chart', 'frequency', 'histogram', 'distribution', 'frequency ', 'chart', 'data']", Histograms,seg_31,"a histogram (*) is a special bar chart. it shows the frequency (*) of the data values: you can visualize the distribution of data values, for example, where “the center” is located (i.e., where there are many data values), how large the “spread” is, etc.",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  1037,  2010,  3406, 13113,  1006,  1008,  1007,  2003,  1037,
         2569,  3347,  3673,  1012,  2009,  3065,  1996,  6075,  1006,  1008,
         1007,  1997,  1996,  2951,  5300,  1024,  2017,  2064,  5107,  4697,
         1996,  4353,  1997,  2951,  5300,  1010,  2005,  2742,  1010,  2073,
         1523,  1996,  2415,  1524,  2003,  2284,  1006,  1045,  1012,  1041,
         1012,  1010,  2073,  2045,  2024,  2116,  2951,  5300,  1007,  1010,
         2129,  2312,  1996,  1523,  3659,  1524,  2003,  1010,  4385,  1012,
          102])"
137,1,['histogram'], Histograms,seg_31,histogram of height fig. 2.3 histogram,tensor(1),"tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  2010,  3406, 13113,  1997,  4578, 20965,  1012,  1016,  1012,
         1017,  2010,  3406, 13113,   102])"
138,1,"['data', 'table']", Histograms,seg_31,"if you order and group the data values [see data in the (chap. 9)] of height, you get table 2.2.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  2065,  2017,  2344,  1998,  2177,  1996,  2951,  5300,  1031,
         2156,  2951,  1999,  1996,  1006, 15775,  2361,  1012,  1023,  1007,
         1033,  1997,  4578,  1010,  2017,  2131,  2795,  1016,  1012,  1016,
         1012,   102])"
139,1,"['interval', 'intervals']", Histograms,seg_31,"this means: in the interval from (just over) 100 to 120 cm there are in total three kids. the intervals must of course be constructed so that they do not overlap. therefore, only one endpoint belongs to the interval. the center of the first interval is 110 cm.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  2023,  2965,  1024,  1999,  1996, 13483,  2013,  1006,  2074,
         2058,  1007,  2531,  2000,  6036,  4642,  2045,  2024,  1999,  2561,
         2093,  4268,  1012,  1996, 14025,  2442,  1997,  2607,  2022,  3833,
         2061,  2008,  2027,  2079,  2025, 17702,  1012,  3568,  1010,  2069,
         2028,  2203,  8400,  7460,  2000,  1996, 13483,  1012,  1996,  2415,
         1997,  1996,  2034, 13483,  2003,  7287,  4642,  1012,   102])"
140,1,['frequencies'], Histograms,seg_31,"counting of the frequencies can be done manually. or you can let microsoft excel do it: use the add-in menu “data analysis,” which has a menu item “histogram.” this option is not available in open office calc.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101, 10320,  1997,  1996, 13139,  2064,  2022,  2589, 21118,  1012,
         2030,  2017,  2064,  2292,  7513, 24970,  2079,  2009,  1024,  2224,
         1996,  5587,  1011,  1999, 12183,  1523,  2951,  4106,  1010,  1524,
         2029,  2038,  1037, 12183,  8875,  1523,  2010,  3406, 13113,  1012,
         1524,  2023,  5724,  2003,  2025,  2800,  1999,  2330,  2436, 10250,
         2278,  1012,   102])"
141,1,"['table', 'bar chart', 'frequency', 'chart', 'data']", Histograms,seg_31,"when the frequency data from the table are plotted in a bar chart, it looks like this as shown in fig. 2.3.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  2043,  1996,  6075,  2951,  2013,  1996,  2795,  2024, 27347,
         1999,  1037,  3347,  3673,  1010,  2009,  3504,  2066,  2023,  2004,
         3491,  1999, 20965,  1012,  1016,  1012,  1017,  1012,   102])"
142,0,[], Histograms,seg_31,the general considerations you should have in connection with determining the number of bars and the width of the bars are:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  1996,  2236, 16852,  2017,  2323,  2031,  1999,  4434,  2007,
        12515,  1996,  2193,  1997,  6963,  1998,  1996,  9381,  1997,  1996,
         6963,  2024,  1024,   102])"
143,1,['observations'], Histograms,seg_31,– the graph should fit onto the paper (or screen). – the graph should be able to accommodate all observations. be aware of the,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  1516,  1996, 10629,  2323,  4906,  3031,  1996,  3259,  1006,
         2030,  3898,  1007,  1012,  1516,  1996, 10629,  2323,  2022,  2583,
         2000,  8752,  2035,  9420,  1012,  2022,  5204,  1997,  1996,   102])"
144,1,['data'], Histograms,seg_31,"minimum value and the maximum value. – the graph does not “violate” the data material. if, for example, there are two",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  6263,  3643,  1998,  1996,  4555,  3643,  1012,  1516,  1996,
        10629,  2515,  2025,  1523, 23640,  1524,  1996,  2951,  3430,  1012,
         2065,  1010,  2005,  2742,  1010,  2045,  2024,  2048,   102])"
145,1,"['information', 'intervals', 'distribution']", Histograms,seg_31,"obvious “bulges” in the distribution, do not make so few bars that this important information disappears! – the intervals must be defined clearly. there must be no doubt as to which",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  5793,  1523, 23708,  2015,  1524,  1999,  1996,  4353,  1010,
         2079,  2025,  2191,  2061,  2261,  6963,  2008,  2023,  2590,  2592,
        17144,   999,  1516,  1996, 14025,  2442,  2022,  4225,  4415,  1012,
         2045,  2442,  2022,  2053,  4797,  2004,  2000,  2029,   102])"
146,1,"['interval', 'observation']", Histograms,seg_31,"interval an observation should belong. you must be sure to which interval the endpoints belong. – you should be able to compare the graph with other graphs, e.g., from previous",tensor(1),"tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101, 13483,  2019,  8089,  2323,  7141,  1012,  2017,  2442,  2022,
         2469,  2000,  2029, 13483,  1996,  2203, 26521,  7141,  1012,  1516,
         2017,  2323,  2022,  2583,  2000, 12826,  1996, 10629,  2007,  2060,
        19287,  1010,  1041,  1012,  1043,  1012,  1010,  2013,  3025,   102])"
147,0,[], Histograms,seg_31,the first decision is: how many bars should you use?,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2010,  3406, 13113,  2015])","tensor([ 101, 1996, 2034, 3247, 2003, 1024, 2129, 2116, 6963, 2323, 2017, 2224,
        1029,  102])"
148,1,"['observations', 'histogram']", Histograms,seg_31,"– the histogram should normally have 3–13 bars. – the more the observations, the more the bars.",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  1516,  1996,  2010,  3406, 13113,  2323,  5373,  2031,  1017,
         1516,  2410,  6963,  1012,  1516,  1996,  2062,  1996,  9420,  1010,
         1996,  2062,  1996,  6963,  1012,   102])"
149,1,['table'], Histograms,seg_31,"as a rough guide, you can use table 2.3. in the “fitness club” example, the number of values is 30. the number of bars should be between 3 and 7, so 5 is probably a fairly good choice.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  2004,  1037,  5931,  5009,  1010,  2017,  2064,  2224,  2795,
         1016,  1012,  1017,  1012,  1999,  1996,  1523, 10516,  2252,  1524,
         2742,  1010,  1996,  2193,  1997,  5300,  2003,  2382,  1012,  1996,
         2193,  1997,  6963,  2323,  2022,  2090,  1017,  1998,  1021,  1010,
         2061,  1019,  2003,  2763,  1037,  7199,  2204,  3601,  1012,   102])"
150,1,['function'], Histograms,seg_31,"technical note to determine the number of bars, we can use the following formula: no. of bars ¼ log (n)/log (2) here n is the number of values and log is the logarithmic function (use calculator or spreadsheet for this calculation). you can use logarithms of base 10 or natural logarithms; the result of the formula will be the same. in the “fitness club” example, n ¼ 30. here we use logarithms of base 10: no. of bars¼ log (n)/log (2)¼ log (30)/log (2)¼ 1.48/0.30¼ 4.9¼ app. 5.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  4087,  3602,  2000,  5646,  1996,  2193,  1997,  6963,  1010,
         2057,  2064,  2224,  1996,  2206,  5675,  1024,  2053,  1012,  1997,
         6963,  1091,  8833,  1006,  1050,  1007,  1013,  8833,  1006,  1016,
         1007,  2182,  1050,  2003,  1996,  2193,  1997,  5300,  1998,  8833,
         2003,  1996,  8833,  8486,  2705,  7712,  3853,  1006,  2224, 10250,
        19879,  4263,  2030, 20861, 21030,  2102,  2005,  2023, 17208,  1007,
         1012,  2017,  2064,  2224,  8833,  8486,  2705,  5244,  1997,  2918,
         2184,  2030,  3019,  8833,  8486,  2705,  5244,  1025,  1996,  2765,
         1997,  1996,  5675,  2097,  2022,  1996,  2168,  1012,  1999,  1996,
         1523, 10516,  2252,  1524,  2742,  1010,  1050,  1091,  2382,  1012,
         2182,  2057,  2224,  8833,  8486,  2705,  5244,  1997,  2918,  2184,
         1024,  2053,  1012,  1997,  6963, 29664,  8833,  1006,  1050,  1007,
         1013,  8833,  1006,  1016,  1007,  1091,  8833,  1006,  2382,  1007,
         1013,  8833,  1006,  1016,  1007,  1091,  1015,  1012,  4466,  1013,
         1014,  1012,  2382, 29664,  1018,  1012,  1023, 29664, 10439,  1012,
         1019,  1012,   102])"
151,0,[], Histograms,seg_31,"the next question is: how wide should the bars be? having determined the number of bars, we can easily find out how wide each bar must be:",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2010,  3406, 13113,  2015])","tensor([ 101, 1996, 2279, 3160, 2003, 1024, 2129, 2898, 2323, 1996, 6963, 2022,
        1029, 2383, 4340, 1996, 2193, 1997, 6963, 1010, 2057, 2064, 4089, 2424,
        2041, 2129, 2898, 2169, 3347, 2442, 2022, 1024,  102])"
152,1,['interval'], Histograms,seg_31,"1. interval length ¼ (maximum value minimum value)/(number of bars). 2. round off the result to an appropriate number, if necessary.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  1015,  1012, 13483,  3091,  1091,  1006,  4555,  3643,  6263,
         3643,  1007,  1013,  1006,  2193,  1997,  6963,  1007,  1012,  1016,
         1012,  2461,  2125,  1996,  2765,  2000,  2019,  6413,  2193,  1010,
         2065,  4072,  1012,   102])"
153,1,['data'], Histograms,seg_31,"for the height data, maximum value ¼ 198 cm and minimum value ¼ 112 cm. this gives (198 112)/5 ¼ 17. this might be rounded to 20. the previously proposed classification seems to provide a reasonably good description of data.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  2005,  1996,  4578,  2951,  1010,  4555,  3643,  1091, 20003,
         4642,  1998,  6263,  3643,  1091, 11176,  4642,  1012,  2023,  3957,
         1006, 20003, 11176,  1007,  1013,  1019,  1091,  2459,  1012,  2023,
         2453,  2022,  8352,  2000,  2322,  1012,  1996,  3130,  3818,  5579,
         3849,  2000,  3073,  1037, 16286,  2204,  6412,  1997,  2951,  1012,
          102])"
154,1,['histograms'], Histograms,seg_31,"from time to time you see histograms, where the bars are not equally wide. you should absolutely avoid this, because it is difficult for the reader to interpret. it is, for example, not immediately clear how to scale the y-axis. should you take",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  2013,  2051,  2000,  2051,  2017,  2156,  2010,  3406, 13113,
         2015,  1010,  2073,  1996,  6963,  2024,  2025,  8053,  2898,  1012,
         2017,  2323,  7078,  4468,  2023,  1010,  2138,  2009,  2003,  3697,
         2005,  1996,  8068,  2000, 17841,  1012,  2009,  2003,  1010,  2005,
         2742,  1010,  2025,  3202,  3154,  2129,  2000,  4094,  1996,  1061,
         1011,  8123,  1012,  2323,  2017,  2202,   102])"
155,0,[], Histograms,seg_31,"into account the visual impression? then the bar height should be smaller, if a bar is wider. this means, however, that the readings on the y-axis cannot be readily interpreted.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  2046,  4070,  1996,  5107,  8605,  1029,  2059,  1996,  3347,
         4578,  2323,  2022,  3760,  1010,  2065,  1037,  3347,  2003,  7289,
         1012,  2023,  2965,  1010,  2174,  1010,  2008,  1996, 15324,  2006,
         1996,  1061,  1011,  8123,  3685,  2022, 12192, 10009,  1012,   102])"
156,1,['histograms'], Histograms,seg_31,you should only use histograms with equally wide bars!,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2010,  3406, 13113,  2015])","tensor([  101,  2017,  2323,  2069,  2224,  2010,  3406, 13113,  2015,  2007,
         8053,  2898,  6963,   999,   102])"
157,1,"['frequency', 'histogram', 'pie chart', 'chart', 'charts', 'data']", Pie Charts,seg_33,"pie charts are often used to show how large a part of the “pie” each group represents. this may be used with frequency data (equivalent to a histogram), but often the pie chart is used in connection with economic quantities, such as expenses or income.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([11345,  6093])","tensor([  101, 11345,  6093,  2024,  2411,  2109,  2000,  2265,  2129,  2312,
         1037,  2112,  1997,  1996,  1523, 11345,  1524,  2169,  2177,  5836,
         1012,  2023,  2089,  2022,  2109,  2007,  6075,  2951,  1006,  5662,
         2000,  1037,  2010,  3406, 13113,  1007,  1010,  2021,  2411,  1996,
        11345,  3673,  2003,  2109,  1999,  4434,  2007,  3171, 12450,  1010,
         2107,  2004, 11727,  2030,  3318,  1012,   102])"
158,1,"['frequency table', 'table', 'frequency', 'sample']", Pie Charts,seg_33,a frequency table of the number of girls and boys in our sample looks like the tabular form shown in table 2.4.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([11345,  6093])","tensor([  101,  1037,  6075,  2795,  1997,  1996,  2193,  1997,  3057,  1998,
         3337,  1999,  2256,  7099,  3504,  2066,  1996, 21628,  7934,  2433,
         3491,  1999,  2795,  1016,  1012,  1018,  1012,   102])"
159,1,"['bar chart', 'information', 'pie chart', 'chart']", Pie Charts,seg_33,this information can be illustrated graphically as either a bar chart (fig. 2.4) or a pie chart (fig. 2.5).,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([11345,  6093])","tensor([  101,  2023,  2592,  2064,  2022,  7203, 20477,  2135,  2004,  2593,
         1037,  3347,  3673,  1006, 20965,  1012,  1016,  1012,  1018,  1007,
         2030,  1037, 11345,  3673,  1006, 20965,  1012,  1016,  1012,  1019,
         1007,  1012,   102])"
160,1,"['pie chart', 'information', 'chart']", Pie Charts,seg_33,"the same information is given in the two graphs! if there are only a few groups like in this example, many feel that the pie chart is the most illustrative chart. the pie chart also has the advantage that you cannot “cheat” in the same manner as in",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([11345,  6093])","tensor([  101,  1996,  2168,  2592,  2003,  2445,  1999,  1996,  2048, 19287,
          999,  2065,  2045,  2024,  2069,  1037,  2261,  2967,  2066,  1999,
         2023,  2742,  1010,  2116,  2514,  2008,  1996, 11345,  3673,  2003,
         1996,  2087,  5665, 19966, 18514,  3673,  1012,  1996, 11345,  3673,
         2036,  2038,  1996,  5056,  2008,  2017,  3685,  1523, 21910,  1524,
         1999,  1996,  2168,  5450,  2004,  1999,   102])"
161,0,[], Pie Charts,seg_33,frequency of each sex,tensor(0),"tensor([0, 0, 0, 0, 0, 0])","tensor([11345,  6093])","tensor([ 101, 6075, 1997, 2169, 3348,  102])"
162,1,"['bar chart', 'bar charts', 'chart', 'charts', 'case']", Pie Charts,seg_33,"the bar chart, where you can cut part of the axes. in case of more than six to seven groups, bar charts are probably most appropriate.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([11345,  6093])","tensor([  101,  1996,  3347,  3673,  1010,  2073,  2017,  2064,  3013,  2112,
         1997,  1996, 19589,  1012,  1999,  2553,  1997,  2062,  2084,  2416,
         2000,  2698,  2967,  1010,  3347,  6093,  2024,  2763,  2087,  6413,
         1012,   102])"
163,1,"['variables', 'plots']", Scatter Plots,seg_35,scatter plots are well suited to show relationships between two variables.,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 8040, 20097, 14811])","tensor([  101,  8040, 20097, 14811,  2024,  2092, 10897,  2000,  2265,  6550,
         2090,  2048, 10857,  1012,   102])"
164,1,"['scatter plot', 'plot']", Scatter Plots,seg_35,"in the “fitness club” example, we assume that there is a relationship between height and weight: the taller a kid is, the heavier it is. a scatter plot is illustrated in fig. 2.6.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 8040, 20097, 14811])","tensor([  101,  1999,  1996,  1523, 10516,  2252,  1524,  2742,  1010,  2057,
         7868,  2008,  2045,  2003,  1037,  3276,  2090,  4578,  1998,  3635,
         1024,  1996, 12283,  1037,  4845,  2003,  1010,  1996, 11907,  2009,
         2003,  1012,  1037,  8040, 20097,  5436,  2003,  7203,  1999, 20965,
         1012,  1016,  1012,  1020,  1012,   102])"
165,1,"['dependent variable', 'independent variable', 'dependent', 'variable', 'independent']", Scatter Plots,seg_35,"weight is the y variable or the dependent variable. height is the x-variable or the independent variable. we imagine that weight depends on the height, i.e., there is a “cause” and an “effect.”",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 8040, 20097, 14811])","tensor([ 101, 3635, 2003, 1996, 1061, 8023, 2030, 1996, 7790, 8023, 1012, 4578,
        2003, 1996, 1060, 1011, 8023, 2030, 1996, 2981, 8023, 1012, 2057, 5674,
        2008, 3635, 9041, 2006, 1996, 4578, 1010, 1045, 1012, 1041, 1012, 1010,
        2045, 2003, 1037, 1523, 3426, 1524, 1998, 2019, 1523, 3466, 1012, 1524,
         102])"
166,1,"['cases', 'correlation', 'variable']", Scatter Plots,seg_35,"in other cases, it is more arbitrary, as to which variable we choose as x and y. we simply imagine that there must be a relationship (or correlation), without necessarily a “cause” and an “effect.”",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 8040, 20097, 14811])","tensor([  101,  1999,  2060,  3572,  1010,  2009,  2003,  2062, 15275,  1010,
         2004,  2000,  2029,  8023,  2057,  5454,  2004,  1060,  1998,  1061,
         1012,  2057,  3432,  5674,  2008,  2045,  2442,  2022,  1037,  3276,
         1006,  2030, 16902,  1007,  1010,  2302,  9352,  1037,  1523,  3426,
         1524,  1998,  2019,  1523,  3466,  1012,  1524,   102])"
167,1,"['statistical', 'variables']", Scatter Plots,seg_35,"chapter 7 gives tools to investigate whether there is indeed a statistical correlation between the two variables, height and weight.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0.])","tensor([ 8040, 20097, 14811])","tensor([  101,  3127,  1021,  3957,  5906,  2000,  8556,  3251,  2045,  2003,
         5262,  1037,  7778, 16902,  2090,  1996,  2048, 10857,  1010,  4578,
         1998,  3635,  1012,   102])"
168,1,"['statistical', 'plot']", Scatter Plots,seg_35,"on the other hand, one cannot by statistical methods or by studying graphs determine whether there is a “cause” and an “effect.” yet one can regularly in newspapers see examples of conclusions, where studying a plot leads to a conclusion, that x is the “cause” and y is the “effect.”",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 8040, 20097, 14811])","tensor([  101,  2006,  1996,  2060,  2192,  1010,  2028,  3685,  2011,  7778,
         4725,  2030,  2011,  5702, 19287,  5646,  3251,  2045,  2003,  1037,
         1523,  3426,  1524,  1998,  2019,  1523,  3466,  1012,  1524,  2664,
         2028,  2064,  5570,  1999,  6399,  2156,  4973,  1997, 15306,  1010,
         2073,  5702,  1037,  5436,  5260,  2000,  1037,  7091,  1010,  2008,
         1060,  2003,  1996,  1523,  3426,  1524,  1998,  1061,  2003,  1996,
         1523,  3466,  1012,  1524,   102])"
169,1,['charts'], Line Charts,seg_37,"line charts are often used to illustrate a trend, where the x-variable is, e.g., time, age or seniority.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2240, 6093])","tensor([  101,  2240,  6093,  2024,  2411,  2109,  2000, 19141,  1037,  9874,
         1010,  2073,  1996,  1060,  1011,  8023,  2003,  1010,  1041,  1012,
         1043,  1012,  1010,  2051,  1010,  2287,  2030,  3026,  3012,  1012,
          102])"
170,1,"['average', 'data', 'table']", Line Charts,seg_37,"in the “fitness club” example, we would like to show how the average weight of the kids increases with age. we have the data as shown in table 2.5.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0.])","tensor([2240, 6093])","tensor([  101,  1999,  1996,  1523, 10516,  2252,  1524,  2742,  1010,  2057,
         2052,  2066,  2000,  2265,  2129,  1996,  2779,  3635,  1997,  1996,
         4268,  7457,  2007,  2287,  1012,  2057,  2031,  1996,  2951,  2004,
         3491,  1999,  2795,  1016,  1012,  1019,  1012,   102])"
171,1,"['line chart', 'information', 'bar charts', 'chart', 'charts']", Line Charts,seg_37,"we can illustrate this with a line chart as shown in fig. 2.7. this chart suggests that there might be some increase in weight with increasing age. as for bar charts, it is important to be aware of the axes. the same information can also be visualized in fig. 2.8.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2240, 6093])","tensor([  101,  2057,  2064, 19141,  2023,  2007,  1037,  2240,  3673,  2004,
         3491,  1999, 20965,  1012,  1016,  1012,  1021,  1012,  2023,  3673,
         6083,  2008,  2045,  2453,  2022,  2070,  3623,  1999,  3635,  2007,
         4852,  2287,  1012,  2004,  2005,  3347,  6093,  1010,  2009,  2003,
         2590,  2000,  2022,  5204,  1997,  1996, 19589,  1012,  1996,  2168,
         2592,  2064,  2036,  2022,  5107,  3550,  1999, 20965,  1012,  1016,
         1012,  1022,  1012,   102])"
172,1,['mean'], Line Charts,seg_37,mean weight vs. age mean weight,tensor(1),"tensor([0., 1., 0., 0., 0., 0., 1., 0., 0.])","tensor([2240, 6093])","tensor([ 101, 2812, 3635, 5443, 1012, 2287, 2812, 3635,  102])"
173,1,['mean'], Line Charts,seg_37,mean weight vs. age mean weight,tensor(1),"tensor([0., 1., 0., 0., 0., 0., 1., 0., 0.])","tensor([2240, 6093])","tensor([ 101, 2812, 3635, 5443, 1012, 2287, 2812, 3635,  102])"
174,0,[], Line Charts,seg_37,the visual appearance is now quite different! it now seems as if there is a dramatic increase in weight with increasing age.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0])","tensor([2240, 6093])","tensor([ 101, 1996, 5107, 3311, 2003, 2085, 3243, 2367,  999, 2009, 2085, 3849,
        2004, 2065, 2045, 2003, 1037, 6918, 3623, 1999, 3635, 2007, 4852, 2287,
        1012,  102])"
175,1,"['bubble plot', 'scatter plot', 'plot', 'variable']", Bubble Plots,seg_39,"the bubble plot is a variant of the scatter plot. instead of points, bubbles are plotted.the size of each bubble (either area or diameter) represents the value of a third variable.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0.])","tensor([11957, 14811])","tensor([  101,  1996, 11957,  5436,  2003,  1037,  8349,  1997,  1996,  8040,
        20097,  5436,  1012,  2612,  1997,  2685,  1010, 17255,  2024, 27347,
         1012,  1996,  2946,  1997,  2169, 11957,  1006,  2593,  2181,  2030,
         6705,  1007,  5836,  1996,  3643,  1997,  1037,  2353,  8023,  1012,
          102])"
176,1,['variable'], Bubble Plots,seg_39,"it is probably most “fair” to let the area of the bubble be proportional to the value of the third variable, because the area is closely linked to the immediate visual impression.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([11957, 14811])","tensor([  101,  2009,  2003,  2763,  2087,  1523,  4189,  1524,  2000,  2292,
         1996,  2181,  1997,  1996, 11957,  2022, 14267,  2000,  1996,  3643,
         1997,  1996,  2353,  8023,  1010,  2138,  1996,  2181,  2003,  4876,
         5799,  2000,  1996,  6234,  5107,  8605,  1012,   102])"
177,1,"['vary', 'factor', 'variable']", Bubble Plots,seg_39,"however, if the third variable does not vary much (e.g., at the most by a factor of 2 between the minimum and maximum value), it is probably best to let the diameter of the bubble be proportional to this variable. otherwise, it will be simply too hard to see the difference in size of the bubbles.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([11957, 14811])","tensor([  101,  2174,  1010,  2065,  1996,  2353,  8023,  2515,  2025,  8137,
         2172,  1006,  1041,  1012,  1043,  1012,  1010,  2012,  1996,  2087,
         2011,  1037,  5387,  1997,  1016,  2090,  1996,  6263,  1998,  4555,
         3643,  1007,  1010,  2009,  2003,  2763,  2190,  2000,  2292,  1996,
         6705,  1997,  1996, 11957,  2022, 14267,  2000,  2023,  8023,  1012,
         4728,  1010,  2009,  2097,  2022,  3432,  2205,  2524,  2000,  2156,
         1996,  4489,  1999,  2946,  1997,  1996, 17255,  1012,   102])"
178,0,[], Bubble Plots,seg_39,in the “fitness club” example we let the diameter of the bubble show the age of the kids. large bubbles are the oldest kids and small bubbles are the youngest kids (fig. 2.9).,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([11957, 14811])","tensor([  101,  1999,  1996,  1523, 10516,  2252,  1524,  2742,  2057,  2292,
         1996,  6705,  1997,  1996, 11957,  2265,  1996,  2287,  1997,  1996,
         4268,  1012,  2312, 17255,  2024,  1996,  4587,  4268,  1998,  2235,
        17255,  2024,  1996,  6587,  4268,  1006, 20965,  1012,  1016,  1012,
         1023,  1007,  1012,   102])"
179,1,['plot'], Bubble Plots,seg_39,"in this plot, one can clearly see that the three kids at the left in the diagram, who are small in terms of both height and weight, are among the youngest, because the bubbles are relatively small.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([11957, 14811])","tensor([  101,  1999,  2023,  5436,  1010,  2028,  2064,  4415,  2156,  2008,
         1996,  2093,  4268,  2012,  1996,  2187,  1999,  1996, 16403,  1010,
         2040,  2024,  2235,  1999,  3408,  1997,  2119,  4578,  1998,  3635,
         1010,  2024,  2426,  1996,  6587,  1010,  2138,  1996, 17255,  2024,
         4659,  2235,  1012,   102])"
180,1,"['results', 'tables', 'statistical']", Tables,seg_41,"charts are an important way to present the results of an investigation. other methods are tables, which we discuss here, and various statistical “key figures,” which are the subject of the next chapter.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])",tensor([7251]),"tensor([ 101, 6093, 2024, 2019, 2590, 2126, 2000, 2556, 1996, 3463, 1997, 2019,
        4812, 1012, 2060, 4725, 2024, 7251, 1010, 2029, 2057, 6848, 2182, 1010,
        1998, 2536, 7778, 1523, 3145, 4481, 1010, 1524, 2029, 2024, 1996, 3395,
        1997, 1996, 2279, 3127, 1012,  102])"
181,1,['table'], The Ingredients of a Table,seg_43,"consider a typical table, such as table 2.6.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 1996, 12760,  1997,  1037,  2795])","tensor([ 101, 5136, 1037, 5171, 2795, 1010, 2107, 2004, 2795, 1016, 1012, 1020,
        1012,  102])"
182,1,['table'], The Ingredients of a Table,seg_43,the table ingredients are:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0.])","tensor([ 1996, 12760,  1997,  1037,  2795])","tensor([  101,  1996,  2795, 12760,  2024,  1024,   102])"
183,1,"['variable', 'table']", The Ingredients of a Table,seg_43,"– table title: “no. of kids by sex and age.” – column title: it is a grouping of the variable “age,” supplemented with a",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996, 12760,  1997,  1037,  2795])","tensor([  101,  1516,  2795,  2516,  1024,  1523,  2053,  1012,  1997,  4268,
         2011,  3348,  1998,  2287,  1012,  1524,  1516,  5930,  2516,  1024,
         2009,  2003,  1037, 19765,  1997,  1996,  8023,  1523,  2287,  1010,
         1524, 20585,  2007,  1037,   102])"
184,1,"['frequencies', 'table', 'percentages']", The Ingredients of a Table,seg_43,"“total” column. – row title: it is a grouping of “sex,” supplemented with a “total” row. – cells: this is the “core” of the table, for example, frequencies, percentages or an",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.])","tensor([ 1996, 12760,  1997,  1037,  2795])","tensor([  101,  1523,  2561,  1524,  5930,  1012,  1516,  5216,  2516,  1024,
         2009,  2003,  1037, 19765,  1997,  1523,  3348,  1010,  1524, 20585,
         2007,  1037,  1523,  2561,  1524,  5216,  1012,  1516,  4442,  1024,
         2023,  2003,  1996,  1523,  4563,  1524,  1997,  1996,  2795,  1010,
         2005,  2742,  1010, 13139,  1010,  7017,  2015,  2030,  2019,   102])"
185,1,"['table', 'information', 'frequency', 'data', 'variable']", The Ingredients of a Table,seg_43,average of a variable. in table 2.6 the cells contain a frequency. – footnotes: at the bottom of the table we find some information on the data,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.])","tensor([ 1996, 12760,  1997,  1037,  2795])","tensor([  101,  2779,  1997,  1037,  8023,  1012,  1999,  2795,  1016,  1012,
         1020,  1996,  4442,  5383,  1037,  6075,  1012,  1516,  3329, 20564,
         1024,  2012,  1996,  3953,  1997,  1996,  2795,  2057,  2424,  2070,
         2592,  2006,  1996,  2951,   102])"
186,0,[], The Ingredients of a Table,seg_43,"sources, possibly supplemented by additional notes and comments.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 1996, 12760,  1997,  1037,  2795])","tensor([  101,  4216,  1010,  4298, 20585,  2011,  3176,  3964,  1998,  7928,
         1012,   102])"
187,1,"['variables', 'data', 'table']", The Ingredients of a Table,seg_43,"so there are two dimensions of the table: rows and columns. each dimension will often show a grouping of data. or, one dimension can consist of several different variables, as in table 2.7.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0.])","tensor([ 1996, 12760,  1997,  1037,  2795])","tensor([  101,  2061,  2045,  2024,  2048,  9646,  1997,  1996,  2795,  1024,
        10281,  1998,  7753,  1012,  2169,  9812,  2097,  2411,  2265,  1037,
        19765,  1997,  2951,  1012,  2030,  1010,  2028,  9812,  2064,  8676,
         1997,  2195,  2367, 10857,  1010,  2004,  1999,  2795,  1016,  1012,
         1021,  1012,   102])"
188,1,"['variables', 'average']", The Ingredients of a Table,seg_43,"here, the column dimension consists of the average for three variables: height, weight and age.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 1996, 12760,  1997,  1037,  2795])","tensor([  101,  2182,  1010,  1996,  5930,  9812,  3774,  1997,  1996,  2779,
         2005,  2093, 10857,  1024,  4578,  1010,  3635,  1998,  2287,  1012,
          102])"
189,1,"['variable', 'table']", The Ingredients of a Table,seg_43,"on other occasions, the column dimension could be several calculations of the same variable, as shown in table 2.8.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 1996, 12760,  1997,  1037,  2795])","tensor([  101,  2006,  2060,  6642,  1010,  1996,  5930,  9812,  2071,  2022,
         2195, 16268,  1997,  1996,  2168,  8023,  1010,  2004,  3491,  1999,
         2795,  1016,  1012,  1022,  1012,   102])"
190,1,['average'], The Ingredients of a Table,seg_43,"here, the column dimension consists of minimum, average and maximum values of weight.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996, 12760,  1997,  1037,  2795])","tensor([ 101, 2182, 1010, 1996, 5930, 9812, 3774, 1997, 6263, 1010, 2779, 1998,
        4555, 5300, 1997, 3635, 1012,  102])"
191,1,"['frequencies', 'table', 'sample', 'percentages']", Percentages,seg_45,"the first table shown above gives the sample frequencies of kids in each combination of sex and age. often, you will prefer to display the sample percentages. the sample frequencies are less interesting themselves.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0.])","tensor([7017, 2015])","tensor([  101,  1996,  2034,  2795,  3491,  2682,  3957,  1996,  7099, 13139,
         1997,  4268,  1999,  2169,  5257,  1997,  3348,  1998,  2287,  1012,
         2411,  1010,  2017,  2097,  9544,  2000,  4653,  1996,  7099,  7017,
         2015,  1012,  1996,  7099, 13139,  2024,  2625,  5875,  3209,  1012,
          102])"
192,1,"['sample', 'register', 'population', 'percentages', 'representative']", Percentages,seg_45,"the sample percentages are directly comparable to the population percentages, which may be known from a register. thus, you can immediately assess whether the sample is representative; more about this in chap. 5.",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7017, 2015])","tensor([  101,  1996,  7099,  7017,  2015,  2024,  3495, 12435,  2000,  1996,
         2313,  7017,  2015,  1010,  2029,  2089,  2022,  2124,  2013,  1037,
         4236,  1012,  2947,  1010,  2017,  2064,  3202, 14358,  3251,  1996,
         7099,  2003,  4387,  1025,  2062,  2055,  2023,  1999, 15775,  2361,
         1012,  1019,  1012,   102])"
193,1,"['table', 'percentage', 'sample', 'combination', 'percentages']", Percentages,seg_45,"the percentage breakdown in the sample is shown in table 2.9. our sample is not very large. the use of percentages in a small sample is of questionable value. for instance, 10% in the combination of boys aged 16–17 years covers only three kids... use percentages with caution, when the sample is small!",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([7017, 2015])","tensor([  101,  1996,  7017, 12554,  1999,  1996,  7099,  2003,  3491,  1999,
         2795,  1016,  1012,  1023,  1012,  2256,  7099,  2003,  2025,  2200,
         2312,  1012,  1996,  2224,  1997,  7017,  2015,  1999,  1037,  2235,
         7099,  2003,  1997, 21068,  3643,  1012,  2005,  6013,  1010,  2184,
         1003,  1999,  1996,  5257,  1997,  3337,  4793,  2385,  1516,  2459,
         2086,  4472,  2069,  2093,  4268,  1012,  1012,  1012,  2224,  7017,
         2015,  2007, 14046,  1010,  2043,  1996,  7099,  2003,  2235,   999,
          102])"
194,1,"['table', 'percent', 'percentages']", Percentages,seg_45,"often percentages are given as row percent or column percent. this means that the percentages add up to 100% along rows, respectively, columns. this is shown as row percent (table 2.10) and column percent (table 2.11).",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([7017, 2015])","tensor([  101,  2411,  7017,  2015,  2024,  2445,  2004,  5216,  3867,  2030,
         5930,  3867,  1012,  2023,  2965,  2008,  1996,  7017,  2015,  5587,
         2039,  2000,  2531,  1003,  2247, 10281,  1010,  4414,  1010,  7753,
         1012,  2023,  2003,  3491,  2004,  5216,  3867,  1006,  2795,  1016,
         1012,  2184,  1007,  1998,  5930,  3867,  1006,  2795,  1016,  1012,
         2340,  1007,  1012,   102])"
195,1,"['percent', 'distribution']", Percentages,seg_45,"the reason for the use of row percent or column percent is often a particular interest in the distribution of one dimension, while the other dimension is only seen as a grouping.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7017, 2015])","tensor([  101,  1996,  3114,  2005,  1996,  2224,  1997,  5216,  3867,  2030,
         5930,  3867,  2003,  2411,  1037,  3327,  3037,  1999,  1996,  4353,
         1997,  2028,  9812,  1010,  2096,  1996,  2060,  9812,  2003,  2069,
         2464,  2004,  1037, 19765,  1012,   102])"
196,0,[], Percentages,seg_45,it may also be that one perceives one dimension as a “cause” and the other as an “effect.”,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0])","tensor([7017, 2015])","tensor([  101,  2009,  2089,  2036,  2022,  2008,  2028, 23084,  2015,  2028,
         9812,  2004,  1037,  1523,  3426,  1524,  1998,  1996,  2060,  2004,
         2019,  1523,  3466,  1012,  1524,   102])"
197,1,"['categories', 'subjective', 'test']", Percentages,seg_45,"in the “fitness club” example the kids were asked whether they do cardiovascular workouts (row dimension). they have also been asked how they assess their physical fitness; here we use three categories: bad, medium and good. this is obviously a subjective assessment. the alternative is to measure their fitness rating through a physical test, which would be expensive.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([7017, 2015])","tensor([  101,  1999,  1996,  1523, 10516,  2252,  1524,  2742,  1996,  4268,
         2020,  2356,  3251,  2027,  2079, 22935, 27090,  2015,  1006,  5216,
         9812,  1007,  1012,  2027,  2031,  2036,  2042,  2356,  2129,  2027,
        14358,  2037,  3558, 10516,  1025,  2182,  2057,  2224,  2093,  7236,
         1024,  2919,  1010,  5396,  1998,  2204,  1012,  2023,  2003,  5525,
         1037, 20714,  7667,  1012,  1996,  4522,  2003,  2000,  5468,  2037,
        10516,  5790,  2083,  1037,  3558,  3231,  1010,  2029,  2052,  2022,
         6450,  1012,   102])"
198,1,"['percent', 'table']", Percentages,seg_45,"in table 2.12 we use row percent, because we expect that cardiovascular workouts may affect the physical fitness, not vice versa.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7017, 2015])","tensor([  101,  1999,  2795,  1016,  1012,  2260,  2057,  2224,  5216,  3867,
         1010,  2138,  2057,  5987,  2008, 22935, 27090,  2015,  2089,  7461,
         1996,  3558, 10516,  1010,  2025,  3580, 18601,  1012,   102])"
199,0,[], Percentages,seg_45,"it is common, like here, to supplement with the column at the right, showing the number of individuals in each group.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0])","tensor([7017, 2015])","tensor([  101,  2009,  2003,  2691,  1010,  2066,  2182,  1010,  2000, 12448,
         2007,  1996,  5930,  2012,  1996,  2157,  1010,  4760,  1996,  2193,
         1997,  3633,  1999,  2169,  2177,  1012,   102])"
200,1,['table'], Percentages,seg_45,"in this table, one might suggest a trend that cardiovascular workouts do have an impact on the physical fitness. in chap. 5 we return to this example.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7017, 2015])","tensor([  101,  1999,  2023,  2795,  1010,  2028,  2453,  6592,  1037,  9874,
         2008, 22935, 27090,  2015,  2079,  2031,  2019,  4254,  2006,  1996,
         3558, 10516,  1012,  1999, 15775,  2361,  1012,  1019,  2057,  2709,
         2000,  2023,  2742,  1012,   102])"
201,1,"['data', 'quantitative']",Chapter  Description of Data,seg_47,"in chap. 2, we discussed two different types of data: quantitative data and qualitative data.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,
        0., 0., 0., 1., 0., 0.])","tensor([3127, 6412, 1997, 2951])","tensor([  101,  1999, 15775,  2361,  1012,  1016,  1010,  2057,  6936,  2048,
         2367,  4127,  1997,  2951,  1024, 20155,  2951,  1998, 24209, 11475,
        27453,  2951,  1012,   102])"
202,1,"['sample', 'tables', 'quantitative', 'data']",Chapter  Description of Data,seg_47,"– quantitative data: these data are used for calculations and for defining the axes in graphs. – qualitative data: these data correspond to groupings of the sample, in tables or graphs.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0.])","tensor([3127, 6412, 1997, 2951])","tensor([  101,  1516, 20155,  2951,  1024,  2122,  2951,  2024,  2109,  2005,
        16268,  1998,  2005, 12854,  1996, 19589,  1999, 19287,  1012,  1516,
        24209, 11475, 27453,  2951,  1024,  2122,  2951, 17254,  2000, 19765,
         2015,  1997,  1996,  7099,  1010,  1999,  7251,  2030, 19287,  1012,
          102])"
203,1,"['variance', 'data', 'sample statistics', 'sample', 'statistics', 'quantitative', 'average']",Chapter  Description of Data,seg_47,"in this chapter, we mainly discuss quantitative data. we present some important sample statistics (*); these are “key numbers” used to describe quantitative data from a sample, for example average and variance.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 1., 0., 0.])","tensor([3127, 6412, 1997, 2951])","tensor([  101,  1999,  2023,  3127,  1010,  2057,  3701,  6848, 20155,  2951,
         1012,  2057,  2556,  2070,  2590,  7099,  6747,  1006,  1008,  1007,
         1025,  2122,  2024,  1523,  3145,  3616,  1524,  2109,  2000,  6235,
        20155,  2951,  2013,  1037,  7099,  1010,  2005,  2742,  2779,  1998,
        23284,  1012,   102])"
204,1,"['statistics', 'data']",Chapter  Description of Data,seg_47,"at the end of the chapter, we discuss in more detail various types of data and the statistics relevant for each type.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0.])","tensor([3127, 6412, 1997, 2951])","tensor([ 101, 2012, 1996, 2203, 1997, 1996, 3127, 1010, 2057, 6848, 1999, 2062,
        6987, 2536, 4127, 1997, 2951, 1998, 1996, 6747, 7882, 2005, 2169, 2828,
        1012,  102])"
205,1,"['histogram', 'data', 'quantitative']",Chapter  Description of Data,seg_47,"an important tool to describe quantitative data is the histogram, which gives an immediate visual impression of what is characteristic for data.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([3127, 6412, 1997, 2951])","tensor([  101,  2019,  2590,  6994,  2000,  6235, 20155,  2951,  2003,  1996,
         2010,  3406, 13113,  1010,  2029,  3957,  2019,  6234,  5107,  8605,
         1997,  2054,  2003,  8281,  2005,  2951,  1012,   102])"
206,1,"['histogram', 'sample', 'sample size', 'distribution', 'data']",Chapter  Description of Data,seg_47,"a histogram should be based on a sample of adequate size. in fig. 3.1 we can see that a histogram based on a small sample is very “rough”. as the sample size grows, one can gradually visualize the distribution of the data values.",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0.])","tensor([3127, 6412, 1997, 2951])","tensor([  101,  1037,  2010,  3406, 13113,  2323,  2022,  2241,  2006,  1037,
         7099,  1997, 11706,  2946,  1012,  1999, 20965,  1012,  1017,  1012,
         1015,  2057,  2064,  2156,  2008,  1037,  2010,  3406, 13113,  2241,
         2006,  1037,  2235,  7099,  2003,  2200,  1523,  5931,  1524,  1012,
         2004,  1996,  7099,  2946,  7502,  1010,  2028,  2064,  6360,  5107,
         4697,  1996,  4353,  1997,  1996,  2951,  5300,  1012,   102])"
207,1,['distribution'],Chapter  Description of Data,seg_47,"however, it is often desirable to summarize the distribution by some “key numbers,” i.e., the numbers that describe various characteristics of the distribution. these key numbers are the main topic of this chapter.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3127, 6412, 1997, 2951])","tensor([  101,  2174,  1010,  2009,  2003,  2411, 16166,  2000,  7680,  7849,
         4697,  1996,  4353,  2011,  2070,  1523,  3145,  3616,  1010,  1524,
         1045,  1012,  1041,  1012,  1010,  1996,  3616,  2008,  6235,  2536,
         6459,  1997,  1996,  4353,  1012,  2122,  3145,  3616,  2024,  1996,
         2364,  8476,  1997,  2023,  3127,  1012,   102])"
208,1,"['variation', 'data']", Systematic and Random Variation,seg_49,statistics is about describing the variation in data. it is important to distinguish between two different sources of variation (fig. 3.2).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([11778,  1998,  6721,  8386])","tensor([  101,  6747,  2003,  2055,  7851,  1996,  8386,  1999,  2951,  1012,
         2009,  2003,  2590,  2000, 10782,  2090,  2048,  2367,  4216,  1997,
         8386,  1006, 20965,  1012,  1017,  1012,  1016,  1007,  1012,   102])"
209,1,"['variation', 'systematic variation', 'random variation', 'random', 'data']", Systematic and Random Variation,seg_49,– systematic variation: the center of data – random variation: the spread of data,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0.])","tensor([11778,  1998,  6721,  8386])","tensor([  101,  1516, 11778,  8386,  1024,  1996,  2415,  1997,  2951,  1516,
         6721,  8386,  1024,  1996,  3659,  1997,  2951,   102])"
210,1,"['histograms', 'locations', 'variation', 'measurements', 'average']", Systematic and Random Variation,seg_49,figure 3.2 illustrates the two different sources of variation. the figure contains four histograms with measurements of daily average temperatures ( c) at four different locations.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([11778,  1998,  6721,  8386])","tensor([  101,  3275,  1017,  1012,  1016, 24899,  1996,  2048,  2367,  4216,
         1997,  8386,  1012,  1996,  3275,  3397,  2176,  2010,  3406, 13113,
         2015,  2007, 11702,  1997,  3679,  2779,  7715,  1006,  1039,  1007,
         2012,  2176,  2367,  5269,  1012,   102])"
211,1,"['variable', 'distributions']", Systematic and Random Variation,seg_49,"the top two distributions are characterized by having a large spread, i.e., highly variable climate. the two lower distributions have a small spread, i.e., a significantly more stable climate.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([11778,  1998,  6721,  8386])","tensor([  101,  1996,  2327,  2048, 20611,  2024,  7356,  2011,  2383,  1037,
         2312,  3659,  1010,  1045,  1012,  1041,  1012,  1010,  3811,  8023,
         4785,  1012,  1996,  2048,  2896, 20611,  2031,  1037,  2235,  3659,
         1010,  1045,  1012,  1041,  1012,  1010,  1037,  6022,  2062,  6540,
         4785,  1012,   102])"
212,1,['distributions'], Systematic and Random Variation,seg_49,"on the other hand, the two distributions to the left have a center of approx. 10 , and the two distributions to the right have a center of approx. 20 . this means that the two distributions to the left represent a cold climate, whereas the two distributions to the right represent a warm climate.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([11778,  1998,  6721,  8386])","tensor([  101,  2006,  1996,  2060,  2192,  1010,  1996,  2048, 20611,  2000,
         1996,  2187,  2031,  1037,  2415,  1997, 22480,  1012,  2184,  1010,
         1998,  1996,  2048, 20611,  2000,  1996,  2157,  2031,  1037,  2415,
         1997, 22480,  1012,  2322,  1012,  2023,  2965,  2008,  1996,  2048,
        20611,  2000,  1996,  2187,  5050,  1037,  3147,  4785,  1010,  6168,
         1996,  2048, 20611,  2000,  1996,  2157,  5050,  1037,  4010,  4785,
         1012,   102])"
213,1,"['location', 'statistical', 'dispersion']", Systematic and Random Variation,seg_49,"in statistical terminology, we often use the term location (*) rather than center. also, we often use the term dispersion (*) rather than spread.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([11778,  1998,  6721,  8386])","tensor([  101,  1999,  7778, 18444,  1010,  2057,  2411,  2224,  1996,  2744,
         3295,  1006,  1008,  1007,  2738,  2084,  2415,  1012,  2036,  1010,
         2057,  2411,  2224,  1996,  2744,  4487, 17668, 10992,  1006,  1008,
         1007,  2738,  2084,  3659,  1012,   102])"
214,1,"['sample statistics', 'sample', 'statistics', 'distribution', 'data']", Systematic and Random Variation,seg_49,"in this chapter, we present the most important sample statistics used to characterize the distribution of data:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0.])","tensor([11778,  1998,  6721,  8386])","tensor([ 101, 1999, 2023, 3127, 1010, 2057, 2556, 1996, 2087, 2590, 7099, 6747,
        2109, 2000, 2839, 4697, 1996, 4353, 1997, 2951, 1024,  102])"
215,1,"['location', 'dispersion']", Systematic and Random Variation,seg_49,– measures of location (center) – measures of dispersion (spread),tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])","tensor([11778,  1998,  6721,  8386])","tensor([  101,  1516,  5761,  1997,  3295,  1006,  2415,  1007,  1516,  5761,
         1997,  4487, 17668, 10992,  1006,  3659,  1007,   102])"
216,1,"['average', 'data', 'distribution']", Average,seg_53,the average (*) is a measure of the center in the distribution of data values. the average is calculated as the sum of all the data values divided by their number.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0.])",tensor([2779]),"tensor([  101,  1996,  2779,  1006,  1008,  1007,  2003,  1037,  5468,  1997,
         1996,  2415,  1999,  1996,  4353,  1997,  2951,  5300,  1012,  1996,
         2779,  2003, 10174,  2004,  1996,  7680,  1997,  2035,  1996,  2951,
         5300,  4055,  2011,  2037,  2193,  1012,   102])"
217,1,['average'], Average,seg_53,"as a symbol of the average, we often use the term x, which is read as “x-bar”.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2779]),"tensor([ 101, 2004, 1037, 6454, 1997, 1996, 2779, 1010, 2057, 2411, 2224, 1996,
        2744, 1060, 1010, 2029, 2003, 3191, 2004, 1523, 1060, 1011, 3347, 1524,
        1012,  102])"
218,1,"['mean', 'sample', 'population', 'average']", Average,seg_53,"note: often people use the word mean (*) rather than average. strictly speaking, one should use the word average of a sample, and use the word mean of a population. many use the two terms interchangeably.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2779]),"tensor([ 101, 3602, 1024, 2411, 2111, 2224, 1996, 2773, 2812, 1006, 1008, 1007,
        2738, 2084, 2779, 1012, 9975, 4092, 1010, 2028, 2323, 2224, 1996, 2773,
        2779, 1997, 1037, 7099, 1010, 1998, 2224, 1996, 2773, 2812, 1997, 1037,
        2313, 1012, 2116, 2224, 1996, 2048, 3408, 8989, 8231, 1012,  102])"
219,1,"['average', 'data']", Average,seg_53,"the average is highly influenced by “extreme values” (i.e., very large or very small values). if for example, there are many very large data values, the average becomes “excessively” large.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2779]),"tensor([  101,  1996,  2779,  2003,  3811,  5105,  2011,  1523,  6034,  5300,
         1524,  1006,  1045,  1012,  1041,  1012,  1010,  2200,  2312,  2030,
         2200,  2235,  5300,  1007,  1012,  2065,  2005,  2742,  1010,  2045,
         2024,  2116,  2200,  2312,  2951,  5300,  1010,  1996,  2779,  4150,
         1523, 11664,  2135,  1524,  2312,  1012,   102])"
220,1,"['sample', 'data']", Average,seg_53,"in a sample of income data, a single dollar billionaire can “destroy” the whole picture, “counting” of course just as much as several hundred “ordinary” people...",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])",tensor([2779]),"tensor([  101,  1999,  1037,  7099,  1997,  3318,  2951,  1010,  1037,  2309,
         7922, 22301,  2064,  1523,  6033,  1524,  1996,  2878,  3861,  1010,
         1523, 10320,  1524,  1997,  2607,  2074,  2004,  2172,  2004,  2195,
         3634,  1523,  6623,  1524,  2111,  1012,  1012,  1012,   102])"
221,1,"['average', 'data', 'case']", Average,seg_53,"in case of many extreme data values, an alternative is to use themedian (*) rather than the average. see later.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])",tensor([2779]),"tensor([  101,  1999,  2553,  1997,  2116,  6034,  2951,  5300,  1010,  2019,
         4522,  2003,  2000,  2224, 11773,  2937,  1006,  1008,  1007,  2738,
         2084,  1996,  2779,  1012,  2156,  2101,  1012,   102])"
222,1,['data'], Average,seg_53,"data are the numbers 3, 5, 6, 4. the number of data values is obviously 4.",tensor(1),"tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0.])",tensor([2779]),"tensor([ 101, 2951, 2024, 1996, 3616, 1017, 1010, 1019, 1010, 1020, 1010, 1018,
        1012, 1996, 2193, 1997, 2951, 5300, 2003, 5525, 1018, 1012,  102])"
223,1,['data'], Average,seg_53,the sum of all data values is 3 þ 5 þ 6 þ 4 ¼ 18.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])",tensor([2779]),"tensor([ 101, 1996, 7680, 1997, 2035, 2951, 5300, 2003, 1017, 1101, 1019, 1101,
        1020, 1101, 1018, 1091, 2324, 1012,  102])"
224,1,['average'], Average,seg_53,the average is:,tensor(1),"tensor([0., 0., 1., 0., 0., 0.])",tensor([2779]),"tensor([ 101, 1996, 2779, 2003, 1024,  102])"
225,1,"['functions', 'statistical', 'average']", Average,seg_53,"most spreadsheets have many built-in statistical functions, including the average. this applies for example, to microsoft excel and open office calc. open office is free!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])",tensor([2779]),"tensor([  101,  2087, 20861, 21030,  3215,  2031,  2116,  2328,  1011,  1999,
         7778,  4972,  1010,  2164,  1996,  2779,  1012,  2023, 12033,  2005,
         2742,  1010,  2000,  7513, 24970,  1998,  2330,  2436, 10250,  2278,
         1012,  2330,  2436,  2003,  2489,   999,   102])"
226,1,"['function', 'statistical', 'average']", Average,seg_53,"to calculate the average, use the statistical function average. see an example later.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.])",tensor([2779]),"tensor([  101,  2000, 18422,  1996,  2779,  1010,  2224,  1996,  7778,  3853,
         2779,  1012,  2156,  2019,  2742,  2101,  1012,   102])"
227,1,"['average', 'data']", Average,seg_53,"with n data values x1 up to xn, the average can be calculated using this general formula:",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])",tensor([2779]),"tensor([  101,  2007,  1050,  2951,  5300,  1060,  2487,  2039,  2000,  1060,
         2078,  1010,  1996,  2779,  2064,  2022, 10174,  2478,  2023,  2236,
         5675,  1024,   102])"
228,1,['data'], Average,seg_53,"here x1 þ x2 þ þxn is the sum of all data values. this formula (and others) can be written shorter by using the “sum” symbol s, which corresponds to the “sum” button in a spreadsheet. we write sxi as a shorter way of writing the sum of all data values x1 þ x2 þ þxn.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])",tensor([2779]),"tensor([  101,  2182,  1060,  2487,  1101,  1060,  2475,  1101,  1101,  2595,
         2078,  2003,  1996,  7680,  1997,  2035,  2951,  5300,  1012,  2023,
         5675,  1006,  1998,  2500,  1007,  2064,  2022,  2517,  7820,  2011,
         2478,  1996,  1523,  7680,  1524,  6454,  1055,  1010,  2029, 14788,
         2000,  1996,  1523,  7680,  1524,  6462,  1999,  1037, 20861, 21030,
         2102,  1012,  2057,  4339,  1055,  9048,  2004,  1037,  7820,  2126,
         1997,  3015,  1996,  7680,  1997,  2035,  2951,  5300,  1060,  2487,
         1101,  1060,  2475,  1101,  1101,  2595,  2078,  1012,   102])"
229,1,['average'], Average,seg_53,then the formula for calculating the average can be written in a compact form:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2779]),"tensor([  101,  2059,  1996,  5675,  2005, 20177,  1996,  2779,  2064,  2022,
         2517,  1999,  1037,  9233,  2433,  1024,   102])"
230,1,"['data', 'median']", Median,seg_55,"the median (*) is the data value “in the middle”, i.e., a number that divides the data values into two parts with an equal number of values. the median can be found by first sorting the data values in ascending order.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])",tensor([3991]),"tensor([  101,  1996,  3991,  1006,  1008,  1007,  2003,  1996,  2951,  3643,
         1523,  1999,  1996,  2690,  1524,  1010,  1045,  1012,  1041,  1012,
         1010,  1037,  2193,  2008, 20487,  1996,  2951,  5300,  2046,  2048,
         3033,  2007,  2019,  5020,  2193,  1997,  5300,  1012,  1996,  3991,
         2064,  2022,  2179,  2011,  2034, 22210,  1996,  2951,  5300,  1999,
        22316,  2344,  1012,   102])"
231,1,"['average', 'data', 'median', 'case']", Median,seg_55,"– in case of an odd number of data values, the median is the middle value. – in case of an even number of data values, there is no single data value dividing data values into two equally large parts; we then define the median as the average of the two middle values.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0.])",tensor([3991]),"tensor([  101,  1516,  1999,  2553,  1997,  2019,  5976,  2193,  1997,  2951,
         5300,  1010,  1996,  3991,  2003,  1996,  2690,  3643,  1012,  1516,
         1999,  2553,  1997,  2019,  2130,  2193,  1997,  2951,  5300,  1010,
         2045,  2003,  2053,  2309,  2951,  3643, 16023,  2951,  5300,  2046,
         2048,  8053,  2312,  3033,  1025,  2057,  2059,  9375,  1996,  3991,
         2004,  1996,  2779,  1997,  1996,  2048,  2690,  5300,  1012,   102])"
232,1,"['extreme values', 'complement', 'average', 'median']", Median,seg_55,"the median is not as sensitive to extreme values as the average! often, you will therefore complement the average with the median.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 1., 0., 0.])",tensor([3991]),"tensor([  101,  1996,  3991,  2003,  2025,  2004,  7591,  2000,  6034,  5300,
         2004,  1996,  2779,   999,  2411,  1010,  2017,  2097,  3568, 13711,
         1996,  2779,  2007,  1996,  3991,  1012,   102])"
233,1,['data'], Median,seg_55,3.2.2.2 example: even number of data values,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])",tensor([3991]),"tensor([ 101, 1017, 1012, 1016, 1012, 1016, 1012, 1016, 2742, 1024, 2130, 2193,
        1997, 2951, 5300,  102])"
234,1,"['average', 'data']", Median,seg_55,"first we sort the data in ascending order: 3, 4, 5, 6. as there is an even number of data values, we take the average of the two middle values (in the sorted data), which are respectively 4 and 5.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([3991]),"tensor([  101,  2034,  2057,  4066,  1996,  2951,  1999, 22316,  2344,  1024,
         1017,  1010,  1018,  1010,  1019,  1010,  1020,  1012,  2004,  2045,
         2003,  2019,  2130,  2193,  1997,  2951,  5300,  1010,  2057,  2202,
         1996,  2779,  1997,  1996,  2048,  2690,  5300,  1006,  1999,  1996,
        19616,  2951,  1007,  1010,  2029,  2024,  4414,  1018,  1998,  1019,
         1012,   102])"
235,1,['median'], Median,seg_55,"therefore, the median is:",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0.])",tensor([3991]),"tensor([ 101, 3568, 1010, 1996, 3991, 2003, 1024,  102])"
236,1,"['average', 'median', 'distributions']", Median,seg_55,"in this example, the median and the average are the same. however, they need not be. in particular, they will be different in “skewed” distributions, see later.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0.])",tensor([3991]),"tensor([  101,  1999,  2023,  2742,  1010,  1996,  3991,  1998,  1996,  2779,
         2024,  1996,  2168,  1012,  2174,  1010,  2027,  2342,  2025,  2022,
         1012,  1999,  3327,  1010,  2027,  2097,  2022,  2367,  1999,  1523,
        15315,  7974,  2098,  1524, 20611,  1010,  2156,  2101,  1012,   102])"
237,1,['median'], Median,seg_55,"here we have used the symbolm for the median, as is common in many textbooks.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])",tensor([3991]),"tensor([  101,  2182,  2057,  2031,  2109,  1996,  6454,  2213,  2005,  1996,
         3991,  1010,  2004,  2003,  2691,  1999,  2116, 18841,  1012,   102])"
238,1,['data'], Median,seg_55,3.2.2.3 example: odd number of data values,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])",tensor([3991]),"tensor([ 101, 1017, 1012, 1016, 1012, 1016, 1012, 1017, 2742, 1024, 5976, 2193,
        1997, 2951, 5300,  102])"
239,1,"['data', 'table']", Median,seg_55,"we consider the example “fitness club”; data values are the age of the boys. data values (17 in total) are shown here, sorted in ascending order (see data in chap. 9) (table 3.1).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])",tensor([3991]),"tensor([  101,  2057,  5136,  1996,  2742,  1523, 10516,  2252,  1524,  1025,
         2951,  5300,  2024,  1996,  2287,  1997,  1996,  3337,  1012,  2951,
         5300,  1006,  2459,  1999,  2561,  1007,  2024,  3491,  2182,  1010,
        19616,  1999, 22316,  2344,  1006,  2156,  2951,  1999, 15775,  2361,
         1012,  1023,  1007,  1006,  2795,  1017,  1012,  1015,  1007,  1012,
          102])"
240,1,"['data', 'median']", Median,seg_55,"as there are 17 data values, the middle value is no. 9, i.e., the data value 14 as highlighted in bold. out of the 17 data values, there are 8 data values on both sides of data value no. 9. therefore, the median is 14.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0.])",tensor([3991]),"tensor([  101,  2004,  2045,  2024,  2459,  2951,  5300,  1010,  1996,  2690,
         3643,  2003,  2053,  1012,  1023,  1010,  1045,  1012,  1041,  1012,
         1010,  1996,  2951,  3643,  2403,  2004, 11548,  1999,  7782,  1012,
         2041,  1997,  1996,  2459,  2951,  5300,  1010,  2045,  2024,  1022,
         2951,  5300,  2006,  2119,  3903,  1997,  2951,  3643,  2053,  1012,
         1023,  1012,  3568,  1010,  1996,  3991,  2003,  2403,  1012,   102])"
241,1,"['function', 'statistical', 'median']", Median,seg_55,the statistical function is called median; see an example later.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])",tensor([3991]),"tensor([ 101, 1996, 7778, 3853, 2003, 2170, 3991, 1025, 2156, 2019, 2742, 2101,
        1012,  102])"
242,1,"['frequency', 'data']", Mode,seg_57,the mode (*) is simply the most frequent data value! all that is required is a frequency count for each data value! no calculations are needed!,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([5549]),"tensor([  101,  1996,  5549,  1006,  1008,  1007,  2003,  3432,  1996,  2087,
         6976,  2951,  3643,   999,  2035,  2008,  2003,  3223,  2003,  1037,
         6075,  4175,  2005,  2169,  2951,  3643,   999,  2053, 16268,  2024,
         2734,   999,   102])"
243,1,"['frequency', 'average', 'data', 'median']", Mode,seg_57,"if you have a complete list of all data values and their frequency, you can immediately find the mode. in this situation it is easier to find the mode than calculating the average or the median. this is the only advantage of the mode compared with the average and the median!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0.])",tensor([5549]),"tensor([  101,  2065,  2017,  2031,  1037,  3143,  2862,  1997,  2035,  2951,
         5300,  1998,  2037,  6075,  1010,  2017,  2064,  3202,  2424,  1996,
         5549,  1012,  1999,  2023,  3663,  2009,  2003,  6082,  2000,  2424,
         1996,  5549,  2084, 20177,  1996,  2779,  2030,  1996,  3991,  1012,
         2023,  2003,  1996,  2069,  5056,  1997,  1996,  5549,  4102,  2007,
         1996,  2779,  1998,  1996,  3991,   999,   102])"
244,1,"['contrast', 'data', 'statistical', 'case']", Mode,seg_57,"in contrast, the mode has one very big disadvantage: if you have many different data values (perhaps with multiple digits), there will often be only one occurrence of each value. should there by chance be 2 (or maybe even 3) occurrences of one single value, it may be just a statistical coincidence. the mode in this case is not a meaningful concept. therefore, the mode is not used very often in practice!",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([5549]),"tensor([  101,  1999,  5688,  1010,  1996,  5549,  2038,  2028,  2200,  2502,
        20502,  1024,  2065,  2017,  2031,  2116,  2367,  2951,  5300,  1006,
         3383,  2007,  3674, 16648,  1007,  1010,  2045,  2097,  2411,  2022,
         2069,  2028, 14404,  1997,  2169,  3643,  1012,  2323,  2045,  2011,
         3382,  2022,  1016,  1006,  2030,  2672,  2130,  1017,  1007, 27247,
         1997,  2028,  2309,  3643,  1010,  2009,  2089,  2022,  2074,  1037,
         7778, 16507,  1012,  1996,  5549,  1999,  2023,  2553,  2003,  2025,
         1037, 15902,  4145,  1012,  3568,  1010,  1996,  5549,  2003,  2025,
         2109,  2200,  2411,  1999,  3218,   999,   102])"
245,0,[], Mode,seg_57,"we use the example again “fitness club,” age of the boys.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])",tensor([5549]),"tensor([  101,  2057,  2224,  1996,  2742,  2153,  1523, 10516,  2252,  1010,
         1524,  2287,  1997,  1996,  3337,  1012,   102])"
246,1,"['frequency', 'table']", Mode,seg_57,"here are the various ages and their frequency (table 3.2). we observe that the highest frequency is that of the age 14 years, which is thus the mode.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])",tensor([5549]),"tensor([  101,  2182,  2024,  1996,  2536,  5535,  1998,  2037,  6075,  1006,
         2795,  1017,  1012,  1016,  1007,  1012,  2057, 11949,  2008,  1996,
         3284,  6075,  2003,  2008,  1997,  1996,  2287,  2403,  2086,  1010,
         2029,  2003,  2947,  1996,  5549,  1012,   102])"
247,1,"['sample', 'data', 'case']", Mode,seg_57,"in a sample of 17 randomly selected people of all ages (kids as well as adults), data values from 0 to perhaps over 90 could occur. in this case, the mode will not be an informative number. if by chance there are two persons having the same age, we would merely consider this an uninteresting coincidence!",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([5549]),"tensor([  101,  1999,  1037,  7099,  1997,  2459, 18154,  3479,  2111,  1997,
         2035,  5535,  1006,  4268,  2004,  2092,  2004,  6001,  1007,  1010,
         2951,  5300,  2013,  1014,  2000,  3383,  2058,  3938,  2071,  5258,
         1012,  1999,  2023,  2553,  1010,  1996,  5549,  2097,  2025,  2022,
         2019, 12367,  8082,  2193,  1012,  2065,  2011,  3382,  2045,  2024,
         2048,  5381,  2383,  1996,  2168,  2287,  1010,  2057,  2052,  6414,
         5136,  2023,  2019,  4895, 18447, 18702,  3436, 16507,   999,   102])"
248,1,['function'], Mode,seg_57,the function is called mode. see an example later.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([5549]),"tensor([ 101, 1996, 3853, 2003, 2170, 5549, 1012, 2156, 2019, 2742, 2101, 1012,
         102])"
249,1,"['median', 'distribution', 'average', 'data']", Choosing a Measure of Location,seg_59,"if the distribution is symmetrical, i.e., there are an equal number of large and small data values (see fig. 3.3), you will usually use the average, but the median provides virtually the same result.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([10549,  1037,  5468,  1997,  3295])","tensor([  101,  2065,  1996,  4353,  2003, 23476,  1010,  1045,  1012,  1041,
         1012,  1010,  2045,  2024,  2019,  5020,  2193,  1997,  2312,  1998,
         2235,  2951,  5300,  1006,  2156, 20965,  1012,  1017,  1012,  1017,
         1007,  1010,  2017,  2097,  2788,  2224,  1996,  2779,  1010,  2021,
         1996,  3991,  3640,  8990,  1996,  2168,  2765,  1012,   102])"
250,1,['median'], Choosing a Measure of Location,seg_59,mean vs. median,tensor(1),"tensor([0., 0., 0., 0., 1., 0.])","tensor([10549,  1037,  5468,  1997,  3295])","tensor([ 101, 2812, 5443, 1012, 3991,  102])"
251,1,"['skewed', 'distribution', 'average', 'median']", Choosing a Measure of Location,seg_59,"in a skewed (i.e., nonsymmetrical) distribution, the average and the median are not identical:",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([10549,  1037,  5468,  1997,  3295])","tensor([  101,  1999,  1037, 15315,  7974,  2098,  1006,  1045,  1012,  1041,
         1012,  1010,  2512,  6508, 20058, 12412,  2389,  1007,  4353,  1010,
         1996,  2779,  1998,  1996,  3991,  2024,  2025,  7235,  1024,   102])"
252,1,"['average', 'data', 'median', 'distribution']", Choosing a Measure of Location,seg_59,"– for a right-skewed distribution, i.e., a distribution with many large data values, the average is larger than the median. – for a left-skewed distribution, i.e., a distribution with many small data values, the average is smaller than the median.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0.])","tensor([10549,  1037,  5468,  1997,  3295])","tensor([  101,  1516,  2005,  1037,  2157,  1011, 15315,  7974,  2098,  4353,
         1010,  1045,  1012,  1041,  1012,  1010,  1037,  4353,  2007,  2116,
         2312,  2951,  5300,  1010,  1996,  2779,  2003,  3469,  2084,  1996,
         3991,  1012,  1516,  2005,  1037,  2187,  1011, 15315,  7974,  2098,
         4353,  1010,  1045,  1012,  1041,  1012,  1010,  1037,  4353,  2007,
         2116,  2235,  2951,  5300,  1010,  1996,  2779,  2003,  3760,  2084,
         1996,  3991,  1012,   102])"
253,1,['average'], Choosing a Measure of Location,seg_59,3.2.4.1 example: what is the average salary?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([10549,  1037,  5468,  1997,  3295])","tensor([  101,  1017,  1012,  1016,  1012,  1018,  1012,  1015,  2742,  1024,
         2054,  2003,  1996,  2779, 10300,  1029,   102])"
254,1,"['data', 'distribution']", Choosing a Measure of Location,seg_59,"many economic and administrative data follow a right-skewed distribution, i.e., there are many large data values. an example might be the salary for a group of employees.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([10549,  1037,  5468,  1997,  3295])","tensor([  101,  2116,  3171,  1998,  3831,  2951,  3582,  1037,  2157,  1011,
        15315,  7974,  2098,  4353,  1010,  1045,  1012,  1041,  1012,  1010,
         2045,  2024,  2116,  2312,  2951,  5300,  1012,  2019,  2742,  2453,
         2022,  1996, 10300,  2005,  1037,  2177,  1997,  5126,  1012,   102])"
255,1,"['average', 'median', 'distribution']", Choosing a Measure of Location,seg_59,"figure 3.5 shows such a distribution. also shown are the values of the mode, median and average salary.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0.])","tensor([10549,  1037,  5468,  1997,  3295])","tensor([  101,  3275,  1017,  1012,  1019,  3065,  2107,  1037,  4353,  1012,
         2036,  3491,  2024,  1996,  5300,  1997,  1996,  5549,  1010,  3991,
         1998,  2779, 10300,  1012,   102])"
256,1,"['average', 'median', 'distribution']", Choosing a Measure of Location,seg_59,"we see that in a right-skewed distribution, the mode is smaller than the median, and the median is smaller than the average!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.])","tensor([10549,  1037,  5468,  1997,  3295])","tensor([  101,  2057,  2156,  2008,  1999,  1037,  2157,  1011, 15315,  7974,
         2098,  4353,  1010,  1996,  5549,  2003,  3760,  2084,  1996,  3991,
         1010,  1998,  1996,  3991,  2003,  3760,  2084,  1996,  2779,   999,
          102])"
257,1,"['average', 'statistic']", Choosing a Measure of Location,seg_59,"now can we also see why most people find it so depressing to read a salary statistic? most people have indeed a salary around the mode, but at the same time compare themselves to the average!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0.])","tensor([10549,  1037,  5468,  1997,  3295])","tensor([  101,  2085,  2064,  2057,  2036,  2156,  2339,  2087,  2111,  2424,
         2009,  2061,  2139, 24128,  2000,  3191,  1037, 10300, 28093,  6553,
         1029,  2087,  2111,  2031,  5262,  1037, 10300,  2105,  1996,  5549,
         1010,  2021,  2012,  1996,  2168,  2051, 12826,  3209,  2000,  1996,
         2779,   999,   102])"
258,1,"['homogeneous', 'sample', 'distribution', 'data']", Choosing a Measure of Location,seg_59,whether the actual data will follow a distribution like the one shown in the figure depends on how homogeneous the sample is.,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0.])","tensor([10549,  1037,  5468,  1997,  3295])","tensor([  101,  3251,  1996,  5025,  2951,  2097,  3582,  1037,  4353,  2066,
         1996,  2028,  3491,  1999,  1996,  3275,  9041,  2006,  2129, 24854,
         1996,  7099,  2003,  1012,   102])"
259,1,"['homogeneous', 'skewed', 'sample', 'distribution']", Choosing a Measure of Location,seg_59,"the more the sample is divided into homogeneous groups, the less skewed the distribution will be.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 0., 0., 0.])","tensor([10549,  1037,  5468,  1997,  3295])","tensor([  101,  1996,  2062,  1996,  7099,  2003,  4055,  2046, 24854,  2967,
         1010,  1996,  2625, 15315,  7974,  2098,  1996,  4353,  2097,  2022,
         1012,   102])"
260,1,"['homogeneous', 'data', 'statistics', 'average', 'median']", Choosing a Measure of Location,seg_59,"if you have divided the data in many homogeneous groups, the average and the median (shown in most salary statistics) will be close to each other.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([10549,  1037,  5468,  1997,  3295])","tensor([  101,  2065,  2017,  2031,  4055,  1996,  2951,  1999,  2116, 24854,
         2967,  1010,  1996,  2779,  1998,  1996,  3991,  1006,  3491,  1999,
         2087, 10300,  6747,  1007,  2097,  2022,  2485,  2000,  2169,  2060,
         1012,   102])"
261,1,"['location', 'dispersion', 'distribution']", Measures of Dispersion,seg_61,"in this chapter, we have reviewed the main measures of location, i.e., the center of a distribution. now we look at various measures of dispersion, i.e., the spread of a distribution.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 5761,  1997,  4487, 17668, 10992])","tensor([  101,  1999,  2023,  3127,  1010,  2057,  2031,  8182,  1996,  2364,
         5761,  1997,  3295,  1010,  1045,  1012,  1041,  1012,  1010,  1996,
         2415,  1997,  1037,  4353,  1012,  2085,  2057,  2298,  2012,  2536,
         5761,  1997,  4487, 17668, 10992,  1010,  1045,  1012,  1041,  1012,
         1010,  1996,  3659,  1997,  1037,  4353,  1012,   102])"
262,1,"['range ', 'range', 'interval', 'dispersion', 'data']", Range,seg_63,the range (*) is simply the width of the interval of the data values. the range is used if you need a measure of dispersion that is easy to calculate and understand!,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])",tensor([2846]),"tensor([  101,  1996,  2846,  1006,  1008,  1007,  2003,  3432,  1996,  9381,
         1997,  1996, 13483,  1997,  1996,  2951,  5300,  1012,  1996,  2846,
         2003,  2109,  2065,  2017,  2342,  1037,  5468,  1997,  4487, 17668,
        10992,  2008,  2003,  3733,  2000, 18422,  1998,  3305,   999,   102])"
263,1,"['data', 'range']", Range,seg_63,"the range is calculated as the difference between the largest and smallest data values, xmax and xmin, respectively:",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])",tensor([2846]),"tensor([  101,  1996,  2846,  2003, 10174,  2004,  1996,  4489,  2090,  1996,
         2922,  1998, 10479,  2951,  5300,  1010,  1060, 17848,  1998,  1060,
        10020,  1010,  4414,  1024,   102])"
264,1,"['numerical', 'data', 'range']", Range,seg_63,"the range (denoted by the letter r) gives a numerical expression of the spread of data and of course has the advantage of being easy to calculate. the main advantage of range, however, is that it is easy to understand!",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2846]),"tensor([  101,  1996,  2846,  1006, 19537,  2011,  1996,  3661,  1054,  1007,
         3957,  1037, 15973,  3670,  1997,  1996,  3659,  1997,  2951,  1998,
         1997,  2607,  2038,  1996,  5056,  1997,  2108,  3733,  2000, 18422,
         1012,  1996,  2364,  5056,  1997,  2846,  1010,  2174,  1010,  2003,
         2008,  2009,  2003,  3733,  2000,  3305,   999,   102])"
265,1,"['data', 'range']", Range,seg_63,"the range depends largely on the number of data values. if there are many data values, the range gets larger, because there are more small or large (“extreme”) values.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])",tensor([2846]),"tensor([ 101, 1996, 2846, 9041, 4321, 2006, 1996, 2193, 1997, 2951, 5300, 1012,
        2065, 2045, 2024, 2116, 2951, 5300, 1010, 1996, 2846, 4152, 3469, 1010,
        2138, 2045, 2024, 2062, 2235, 2030, 2312, 1006, 1523, 6034, 1524, 1007,
        5300, 1012,  102])"
266,1,"['control chart', 'range', 'data', 'variation', 'random variation', 'control', 'quality control', 'sample', 'random', 'samples', 'systematic and random variation', 'process', 'statistical', 'chart', 'charts']", Range,seg_63,"the range is therefore used mainly for small samples. a typical application is statistical quality control in the construction of control charts, where the number of data values in a sample is often in the order of magnitude 5. the exact purpose of the control chart is to quickly detect any changes in a production process by separating systematic and random variation!",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0.])",tensor([2846]),"tensor([  101,  1996,  2846,  2003,  3568,  2109,  3701,  2005,  2235,  8168,
         1012,  1037,  5171,  4646,  2003,  7778,  3737,  2491,  1999,  1996,
         2810,  1997,  2491,  6093,  1010,  2073,  1996,  2193,  1997,  2951,
         5300,  1999,  1037,  7099,  2003,  2411,  1999,  1996,  2344,  1997,
        10194,  1019,  1012,  1996,  6635,  3800,  1997,  1996,  2491,  3673,
         2003,  2000,  2855, 11487,  2151,  3431,  1999,  1037,  2537,  2832,
         2011, 14443, 11778,  1998,  6721,  8386,   999,   102])"
267,1,"['data', 'range']", Range,seg_63,"first we sort the data values in ascending order: 3, 4, 5, 6. the smallest data value is xmin ¼ 3, and the largest data value is xmax ¼ 6. the range then becomes:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])",tensor([2846]),"tensor([  101,  2034,  2057,  4066,  1996,  2951,  5300,  1999, 22316,  2344,
         1024,  1017,  1010,  1018,  1010,  1019,  1010,  1020,  1012,  1996,
        10479,  2951,  3643,  2003,  1060, 10020,  1091,  1017,  1010,  1998,
         1996,  2922,  2951,  3643,  2003,  1060, 17848,  1091,  1020,  1012,
         1996,  2846,  2059,  4150,  1024,   102])"
268,1,"['function', 'range', 'functions']", Range,seg_63,"the range does not exist as a function of its own in spreadsheets. however, there are functions for the maximum value (max) and the minimum value (min). see an example later.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])",tensor([2846]),"tensor([  101,  1996,  2846,  2515,  2025,  4839,  2004,  1037,  3853,  1997,
         2049,  2219,  1999, 20861, 21030,  3215,  1012,  2174,  1010,  2045,
         2024,  4972,  2005,  1996,  4555,  3643,  1006,  4098,  1007,  1998,
         1996,  6263,  3643,  1006,  8117,  1007,  1012,  2156,  2019,  2742,
         2101,  1012,   102])"
269,1,"['deviation', 'deviation ', 'dispersion', 'standard deviation', 'standard', 'distribution', 'average', 'data']", Variance and Standard Deviation,seg_65,"the most common measure of dispersion (spread) is the standard deviation (*), which can be interpreted as “the average distance” between the data values and the average. the larger the standard deviation, the larger is the spread of the distribution.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  1996,  2087,  2691,  5468,  1997,  4487, 17668, 10992,  1006,
         3659,  1007,  2003,  1996,  3115, 24353,  1006,  1008,  1007,  1010,
         2029,  2064,  2022, 10009,  2004,  1523,  1996,  2779,  3292,  1524,
         2090,  1996,  2951,  5300,  1998,  1996,  2779,  1012,  1996,  3469,
         1996,  3115, 24353,  1010,  1996,  3469,  2003,  1996,  3659,  1997,
         1996,  4353,  1012,   102])"
270,1,"['deviation', 'standard deviation', 'standard', 'average']", Variance and Standard Deviation,seg_65,"however, the standard deviation is not calculated as an ordinary average distance. before explaining exactly how the standard deviation is calculated, we have to explain another concept:",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  2174,  1010,  1996,  3115, 24353,  2003,  2025, 10174,  2004,
         2019,  6623,  2779,  3292,  1012,  2077,  9990,  3599,  2129,  1996,
         3115, 24353,  2003, 10174,  1010,  2057,  2031,  2000,  4863,  2178,
         4145,  1024,   102])"
271,1,"['average', 'data', 'variance']", Variance and Standard Deviation,seg_65,the variance (*) is the average of the squared distances between the data values and the average.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  1996, 23284,  1006,  1008,  1007,  2003,  1996,  2779,  1997,
         1996, 19942, 12103,  2090,  1996,  2951,  5300,  1998,  1996,  2779,
         1012,   102])"
272,1,"['data', 'variance']", Variance and Standard Deviation,seg_65,"the variance is not measured in the same units as the original data values, but in square units (e.g., square meters, if the data values are meters). often, the term v is used for the variance.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  1996, 23284,  2003,  2025,  7594,  1999,  1996,  2168,  3197,
         2004,  1996,  2434,  2951,  5300,  1010,  2021,  1999,  2675,  3197,
         1006,  1041,  1012,  1043,  1012,  1010,  2675,  5563,  1010,  2065,
         1996,  2951,  5300,  2024,  5563,  1007,  1012,  2411,  1010,  1996,
         2744,  1058,  2003,  2109,  2005,  1996, 23284,  1012,   102])"
273,1,"['deviation', 'variance', 'deviation ', 'standard deviation', 'standard']", Variance and Standard Deviation,seg_65,the standard deviation (*) is the square root of the variance.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  1996,  3115, 24353,  1006,  1008,  1007,  2003,  1996,  2675,
         7117,  1997,  1996, 23284,  1012,   102])"
274,1,"['deviation', 'standard deviation', 'standard', 'data']", Variance and Standard Deviation,seg_65,"the standard deviation is measured in the same units as the data values, e.g., meters. often, the term s is used for the standard deviation (fig. 3.6).",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  1996,  3115, 24353,  2003,  7594,  1999,  1996,  2168,  3197,
         2004,  1996,  2951,  5300,  1010,  1041,  1012,  1043,  1012,  1010,
         5563,  1012,  2411,  1010,  1996,  2744,  1055,  2003,  2109,  2005,
         1996,  3115, 24353,  1006, 20965,  1012,  1017,  1012,  1020,  1007,
         1012,   102])"
275,1,"['average', 'variance']", Variance and Standard Deviation,seg_65,the average of these figures was previously calculated to be 4.5. the variance of these figures is:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  1996,  2779,  1997,  2122,  4481,  2001,  3130, 10174,  2000,
         2022,  1018,  1012,  1019,  1012,  1996, 23284,  1997,  2122,  4481,
         2003,  1024,   102])"
276,1,"['deviation', 'standard deviation', 'standard']", Variance and Standard Deviation,seg_65,the standard deviation is:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  1996,  3115, 24353,  2003,  1024,   102])"
277,1,['sample'], Variance and Standard Deviation,seg_65,"note: in the above formula, we divide by n 1, not by n. this is, for technical reasons and in practice, an unimportant detail, unless the sample is very small.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  3602,  1024,  1999,  1996,  2682,  5675,  1010,  2057, 11443,
         2011,  1050,  1015,  1010,  2025,  2011,  1050,  1012,  2023,  2003,
         1010,  2005,  4087,  4436,  1998,  1999,  3218,  1010,  2019,  4895,
         5714,  6442,  4630,  6987,  1010,  4983,  1996,  7099,  2003,  2200,
         2235,  1012,   102])"
278,1,"['deviation', 'variance', 'standard deviation', 'standard', 'distribution', 'data']", Variance and Standard Deviation,seg_65,"if n ¼ 1 (i.e., only one data value), the variance and the standard deviation cannot be calculated! this is consistent with the fact that in this situation we cannot talk about the spread of the distribution!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  2065,  1050,  1091,  1015,  1006,  1045,  1012,  1041,  1012,
         1010,  2069,  2028,  2951,  3643,  1007,  1010,  1996, 23284,  1998,
         1996,  3115, 24353,  3685,  2022, 10174,   999,  2023,  2003,  8335,
         2007,  1996,  2755,  2008,  1999,  2023,  3663,  2057,  3685,  2831,
         2055,  1996,  3659,  1997,  1996,  4353,   999,   102])"
279,1,"['deviation', 'variance', 'functions', 'standard deviation', 'standard']", Variance and Standard Deviation,seg_65,"many spreadsheets and calculators have built-in functions to calculate the variance and the standard deviation. often, there are two versions of these functions,",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  2116, 20861, 21030,  3215,  1998, 10250, 19879,  6591,  2031,
         2328,  1011,  1999,  4972,  2000, 18422,  1996, 23284,  1998,  1996,
         3115, 24353,  1012,  2411,  1010,  2045,  2024,  2048,  4617,  1997,
         2122,  4972,  1010,   102])"
280,0,['n'], Variance and Standard Deviation,seg_65,where division by n respectively division by n 1 is used. this has been very confusing to many people.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  2073,  2407,  2011,  1050,  4414,  2407,  2011,  1050,  1015,
         2003,  2109,  1012,  2023,  2038,  2042,  2200, 16801,  2000,  2116,
         2111,  1012,   102])"
281,1,"['deviation', 'variance', 'statisticians', 'sample', 'standard deviation', 'standard', 'population']", Variance and Standard Deviation,seg_65,"often, it is stated that the formulae with divisor n are used when calculating the variance or standard deviation of a population (as opposed to a sample). most statisticians, however, always use the formulae with divisor n 1!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  2411,  1010,  2009,  2003,  3090,  2008,  1996,  5675,  2063,
         2007,  4487, 11365,  2953,  1050,  2024,  2109,  2043, 20177,  1996,
        23284,  2030,  3115, 24353,  1997,  1037,  2313,  1006,  2004,  4941,
         2000,  1037,  7099,  1007,  1012,  2087, 28093,  6553,  7066,  1010,
         2174,  1010,  2467,  2224,  1996,  5675,  2063,  2007,  4487, 11365,
         2953,  1050,  1015,   999,   102])"
282,1,"['function', 'deviation', 'variance', 'standard deviation', 'standard']", Variance and Standard Deviation,seg_65,"for calculation of the variance we use the function var. to calculate the standard deviation, we use the function stdev. see an example later.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  2005, 17208,  1997,  1996, 23284,  2057,  2224,  1996,  3853,
        13075,  1012,  2000, 18422,  1996,  3115, 24353,  1010,  2057,  2224,
         1996,  3853,  2358, 24844,  1012,  2156,  2019,  2742,  2101,  1012,
          102])"
283,1,['variance'], Variance and Standard Deviation,seg_65,the variance is calculated using this general formula:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  1996, 23284,  2003, 10174,  2478,  2023,  2236,  5675,  1024,
          102])"
284,1,"['deviation', 'variance', 'standard deviation', 'standard']", Variance and Standard Deviation,seg_65,"the standard deviation is the square root of the variance, i.e.,",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  1996,  3115, 24353,  2003,  1996,  2675,  7117,  1997,  1996,
        23284,  1010,  1045,  1012,  1041,  1012,  1010,   102])"
285,1,"['cases', 'deviation', 'functions', 'standard deviation', 'standard', 'statistical']", Variance and Standard Deviation,seg_65,"there is an alternative calculation formula for the standard deviation, see the text frame. it is particularly useful in cases where the calculations must be done on a calculator without statistical functions.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  2045,  2003,  2019,  4522, 17208,  5675,  2005,  1996,  3115,
        24353,  1010,  2156,  1996,  3793,  4853,  1012,  2009,  2003,  3391,
         6179,  1999,  3572,  2073,  1996, 16268,  2442,  2022,  2589,  2006,
         1037, 10250, 19879,  4263,  2302,  7778,  4972,  1012,   102])"
286,1,"['deviation', 'functions', 'standard deviation', 'standard', 'statistical']", Variance and Standard Deviation,seg_65,technical note: alternative calculation formula for the standard deviation. this formula for the standard deviation is useful when the calculations must be done on a calculator without statistical functions:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([23284,  1998,  3115, 24353])","tensor([  101,  4087,  3602,  1024,  4522, 17208,  5675,  2005,  1996,  3115,
        24353,  1012,  2023,  5675,  2005,  1996,  3115, 24353,  2003,  6179,
         2043,  1996, 16268,  2442,  2022,  2589,  2006,  1037, 10250, 19879,
         4263,  2302,  7778,  4972,  1024,   102])"
287,1,"['range', 'interquartile range', 'dispersion']", Interquartile Range,seg_67,"another important measure of dispersion is the interquartile range (iqr), explained below.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  2178,  2590,  5468,  1997,  4487, 17668, 10992,  2003,  1996,
         6970, 16211, 28228,  2571,  2846,  1006, 26264,  2099,  1007,  1010,
         4541,  2917,  1012,   102])"
288,1,"['data', 'median']", Interquartile Range,seg_67,"when the median is calculated, you can further divide the two parts of the data values into two parts each.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  2043,  1996,  3991,  2003, 10174,  1010,  2017,  2064,  2582,
        11443,  1996,  2048,  3033,  1997,  1996,  2951,  5300,  2046,  2048,
         3033,  2169,  1012,   102])"
289,1,"['set', 'quartiles', 'data']", Interquartile Range,seg_67,"thus the entire set of data values is divided into four parts, with (roughly) the same number of data values. the new points of division are called the quartiles (*).",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        1., 0., 0., 0., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  2947,  1996,  2972,  2275,  1997,  2951,  5300,  2003,  4055,
         2046,  2176,  3033,  1010,  2007,  1006,  5560,  1007,  1996,  2168,
         2193,  1997,  2951,  5300,  1012,  1996,  2047,  2685,  1997,  2407,
         2024,  2170,  1996, 24209,  8445,  9463,  2015,  1006,  1008,  1007,
         1012,   102])"
290,1,"['location', 'range', 'interval', 'interquartile range', 'quartiles', 'data', 'median']", Interquartile Range,seg_67,"the difference between the quartiles is called the interquartile range, often denoted by the abbreviation iqr. the interpretation of iqr is that it is the length of the interval with the “middle 50%” of the data values and it is often used when you use the median as a measure of location.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  1996,  4489,  2090,  1996, 24209,  8445,  9463,  2015,  2003,
         2170,  1996,  6970, 16211, 28228,  2571,  2846,  1010,  2411, 19537,
         2011,  1996, 22498, 26264,  2099,  1012,  1996,  7613,  1997, 26264,
         2099,  2003,  2008,  2009,  2003,  1996,  3091,  1997,  1996, 13483,
         2007,  1996,  1523,  2690,  2753,  1003,  1524,  1997,  1996,  2951,
         5300,  1998,  2009,  2003,  2411,  2109,  2043,  2017,  2224,  1996,
         3991,  2004,  1037,  5468,  1997,  3295,  1012,   102])"
291,1,"['quartiles', 'data', 'median']", Interquartile Range,seg_67,"when we find the quartiles, we sort the data values in ascending order, as when calculating the median.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  2043,  2057,  2424,  1996, 24209,  8445,  9463,  2015,  1010,
         2057,  4066,  1996,  2951,  5300,  1999, 22316,  2344,  1010,  2004,
         2043, 20177,  1996,  3991,  1012,   102])"
292,1,"['quartile', 'data', 'first quartile']", Interquartile Range,seg_67,"the lower quartile (or first quartile) q1 is a number that divides the data values into two parts so that one-fourth of the data values are smaller than the lower quartile, and three-fourths of the data values are larger.",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  1996,  2896, 24209,  8445,  9463,  1006,  2030,  2034, 24209,
         8445,  9463,  1007,  1053,  2487,  2003,  1037,  2193,  2008, 20487,
         1996,  2951,  5300,  2046,  2048,  3033,  2061,  2008,  2028,  1011,
         2959,  1997,  1996,  2951,  5300,  2024,  3760,  2084,  1996,  2896,
        24209,  8445,  9463,  1010,  1998,  2093,  1011,  2959,  2015,  1997,
         1996,  2951,  5300,  2024,  3469,  1012,   102])"
293,1,"['quartile', 'median']", Interquartile Range,seg_67,"often, the median is considered to be the middle or second quartile and is sometimes denoted by q2.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  2411,  1010,  1996,  3991,  2003,  2641,  2000,  2022,  1996,
         2690,  2030,  2117, 24209,  8445,  9463,  1998,  2003,  2823, 19537,
         2011,  1053,  2475,  1012,   102])"
294,1,"['upper quartile', 'third quartile', 'data', 'quartile']", Interquartile Range,seg_67,the upper quartile (or third quartile) q3 is a figure that divides the data values into two parts so that three-fourths of the data values are smaller than the upper quartile and one-fourth of the data values are larger.,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  1996,  3356, 24209,  8445,  9463,  1006,  2030,  2353, 24209,
         8445,  9463,  1007,  1053,  2509,  2003,  1037,  3275,  2008, 20487,
         1996,  2951,  5300,  2046,  2048,  3033,  2061,  2008,  2093,  1011,
         2959,  2015,  1997,  1996,  2951,  5300,  2024,  3760,  2084,  1996,
         3356, 24209,  8445,  9463,  1998,  2028,  1011,  2959,  1997,  1996,
         2951,  5300,  2024,  3469,  1012,   102])"
295,1,"['range ', 'range', 'interquartile range', 'quartiles']", Interquartile Range,seg_67,the interquartile range (*) iqr is then the difference between the upper and lower quartiles:,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  1996,  6970, 16211, 28228,  2571,  2846,  1006,  1008,  1007,
        26264,  2099,  2003,  2059,  1996,  4489,  2090,  1996,  3356,  1998,
         2896, 24209,  8445,  9463,  2015,  1024,   102])"
296,0,[], Interquartile Range,seg_67,"a few books, however, use half of the difference as the definition of the iqr.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  1037,  2261,  2808,  1010,  2174,  1010,  2224,  2431,  1997,
         1996,  4489,  2004,  1996,  6210,  1997,  1996, 26264,  2099,  1012,
          102])"
297,1,"['data', 'table']", Interquartile Range,seg_67,"we consider again the example fitness club, age of the boys. data values (17 in total) are shown in table 3.4 in sorted order.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  2057,  5136,  2153,  1996,  2742, 10516,  2252,  1010,  2287,
         1997,  1996,  3337,  1012,  2951,  5300,  1006,  2459,  1999,  2561,
         1007,  2024,  3491,  1999,  2795,  1017,  1012,  1018,  1999, 19616,
         2344,  1012,   102])"
298,1,"['data', 'median']", Interquartile Range,seg_67,"we have previously found the median to be the data value no. 9, i.e., the median is 14. this data value divides the data values in two equally large parts. each of these two parts is now subdivided again:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  2057,  2031,  3130,  2179,  1996,  3991,  2000,  2022,  1996,
         2951,  3643,  2053,  1012,  1023,  1010,  1045,  1012,  1041,  1012,
         1010,  1996,  3991,  2003,  2403,  1012,  2023,  2951,  3643, 20487,
         1996,  2951,  5300,  1999,  2048,  8053,  2312,  3033,  1012,  2169,
         1997,  2122,  2048,  3033,  2003,  2085, 15369,  2153,  1024,   102])"
299,1,"['quartile', 'data']", Interquartile Range,seg_67,"the first half consists of data values no. 1–8. as there is an even number of data values, data values no. 4–5 form the point of division. both data values are 13, i.e., the lower quartile q1 equals 13.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  1996,  2034,  2431,  3774,  1997,  2951,  5300,  2053,  1012,
         1015,  1516,  1022,  1012,  2004,  2045,  2003,  2019,  2130,  2193,
         1997,  2951,  5300,  1010,  2951,  5300,  2053,  1012,  1018,  1516,
         1019,  2433,  1996,  2391,  1997,  2407,  1012,  2119,  2951,  5300,
         2024,  2410,  1010,  1045,  1012,  1041,  1012,  1010,  1996,  2896,
        24209,  8445,  9463,  1053,  2487, 19635,  2410,  1012,   102])"
300,1,"['quartile', 'data', 'upper quartile']", Interquartile Range,seg_67,"the other half consists of data values no. 10–17. as there is an even number of data values, data values no. 13–14 form the point of division. both data values are 15, i.e., the upper quartile q3 equals 15.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  1996,  2060,  2431,  3774,  1997,  2951,  5300,  2053,  1012,
         2184,  1516,  2459,  1012,  2004,  2045,  2003,  2019,  2130,  2193,
         1997,  2951,  5300,  1010,  2951,  5300,  2053,  1012,  2410,  1516,
         2403,  2433,  1996,  2391,  1997,  2407,  1012,  2119,  2951,  5300,
         2024,  2321,  1010,  1045,  1012,  1041,  1012,  1010,  1996,  3356,
        24209,  8445,  9463,  1053,  2509, 19635,  2321,  1012,   102])"
301,1,"['interquartile range', 'range']", Interquartile Range,seg_67,then the interquartile range is iqr ¼ 15 13 ¼ 2.,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  2059,  1996,  6970, 16211, 28228,  2571,  2846,  2003, 26264,
         2099,  1091,  2321,  2410,  1091,  1016,  1012,   102])"
302,1,"['quartiles', 'function', 'quartile']", Interquartile Range,seg_67,the quartiles are calculated using the function quartile. see an example later.,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  1996, 24209,  8445,  9463,  2015,  2024, 10174,  2478,  1996,
         3853, 24209,  8445,  9463,  1012,  2156,  2019,  2742,  2101,  1012,
          102])"
303,1,['quartiles'], Interquartile Range,seg_67,then the iqr is calculated by subtracting the quartiles from each other.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,
        0., 0., 0.])","tensor([ 6970, 16211, 28228,  2571,  2846])","tensor([  101,  2059,  1996, 26264,  2099,  2003, 10174,  2011,  4942,  6494,
        11873,  1996, 24209,  8445,  9463,  2015,  2013,  2169,  2060,  1012,
          102])"
304,1,"['location', 'dispersion']", Choosing a Measure of Dispersion,seg_69,the choice of a measure of dispersion will often reflect the measure of location that we have chosen.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0.])","tensor([10549,  1037,  5468,  1997,  4487, 17668, 10992])","tensor([  101,  1996,  3601,  1997,  1037,  5468,  1997,  4487, 17668, 10992,
         2097,  2411,  8339,  1996,  5468,  1997,  3295,  2008,  2057,  2031,
         4217,  1012,   102])"
305,1,"['average', 'distribution']", Choosing a Measure of Dispersion,seg_69,"– if the distribution is symmetrical, we often use the average as a measure of",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([10549,  1037,  5468,  1997,  4487, 17668, 10992])","tensor([  101,  1516,  2065,  1996,  4353,  2003, 23476,  1010,  2057,  2411,
         2224,  1996,  2779,  2004,  1037,  5468,  1997,   102])"
306,1,"['deviation', 'skewed', 'dispersion', 'standard deviation', 'standard', 'distribution', 'average', 'median', 'case']", Choosing a Measure of Dispersion,seg_69,"location. it is natural in this case to supplement with the standard deviation as a measure of dispersion. the standard deviation is, after all, based on the average. – if the distribution is skewed, you will often use the median as a measure of",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0.])","tensor([10549,  1037,  5468,  1997,  4487, 17668, 10992])","tensor([  101,  3295,  1012,  2009,  2003,  3019,  1999,  2023,  2553,  2000,
        12448,  2007,  1996,  3115, 24353,  2004,  1037,  5468,  1997,  4487,
        17668, 10992,  1012,  1996,  3115, 24353,  2003,  1010,  2044,  2035,
         1010,  2241,  2006,  1996,  2779,  1012,  1516,  2065,  1996,  4353,
         2003, 15315,  7974,  2098,  1010,  2017,  2097,  2411,  2224,  1996,
         3991,  2004,  1037,  5468,  1997,   102])"
307,1,"['dispersion', 'median', 'case']", Choosing a Measure of Dispersion,seg_69,"location. in this case it is natural to supplement with the iqr as a measure of dispersion. this is also natural, because the median might be perceived as the “middle quartile”.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([10549,  1037,  5468,  1997,  4487, 17668, 10992])","tensor([  101,  3295,  1012,  1999,  2023,  2553,  2009,  2003,  3019,  2000,
        12448,  2007,  1996, 26264,  2099,  2004,  1037,  5468,  1997,  4487,
        17668, 10992,  1012,  2023,  2003,  2036,  3019,  1010,  2138,  1996,
         3991,  2453,  2022,  8690,  2004,  1996,  1523,  2690, 24209,  8445,
         9463,  1524,  1012,   102])"
308,1,"['populations', 'samples', 'average', 'data']", Relative Spread Dispersion,seg_71,"when comparing samples from several time periods (e.g., several years), the average will often increase with time; this is valid for many financial and administrative data during periods of growth, but also for many human and biological populations.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 5816,  3659,  4487, 17668, 10992])","tensor([  101,  2043, 13599,  8168,  2013,  2195,  2051,  6993,  1006,  1041,
         1012,  1043,  1012,  1010,  2195,  2086,  1007,  1010,  1996,  2779,
         2097,  2411,  3623,  2007,  2051,  1025,  2023,  2003,  9398,  2005,
         2116,  3361,  1998,  3831,  2951,  2076,  6993,  1997,  3930,  1010,
         2021,  2036,  2005,  2116,  2529,  1998,  6897,  7080,  1012,   102])"
309,1,"['deviation', 'standard deviation', 'standard', 'average']", Relative Spread Dispersion,seg_71,"often, the spread increases with an increasing average. therefore, the spread is not interesting in itself. instead, the relative spread is more interesting, i.e., the standard deviation divided by the average.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 1., 0., 0.])","tensor([ 5816,  3659,  4487, 17668, 10992])","tensor([  101,  2411,  1010,  1996,  3659,  7457,  2007,  2019,  4852,  2779,
         1012,  3568,  1010,  1996,  3659,  2003,  2025,  5875,  1999,  2993,
         1012,  2612,  1010,  1996,  5816,  3659,  2003,  2062,  5875,  1010,
         1045,  1012,  1041,  1012,  1010,  1996,  3115, 24353,  4055,  2011,
         1996,  2779,  1012,   102])"
310,1,"['deviation', 'variation', 'percentage', 'coefficient', 'dispersion', 'standard deviation', 'standard', 'average', 'coefficient of variation']", Relative Spread Dispersion,seg_71,"as a measure of the relative spread (dispersion), we use the coefficient of variation (*), often denoted cv, defined as the standard deviation as a percentage of average: s cv ¼ 100%: x",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 5816,  3659,  4487, 17668, 10992])","tensor([  101,  2004,  1037,  5468,  1997,  1996,  5816,  3659,  1006,  4487,
        17668, 10992,  1007,  1010,  2057,  2224,  1996, 19064,  1997,  8386,
         1006,  1008,  1007,  1010,  2411, 19537, 26226,  1010,  4225,  2004,
         1996,  3115, 24353,  2004,  1037,  7017,  1997,  2779,  1024,  1055,
        26226,  1091,  2531,  1003,  1024,  1060,   102])"
311,1,"['deviation', 'standard deviation', 'standard']", Relative Spread Dispersion,seg_71,"sometimes, this is also known as the relative standard deviation and denoted rsd.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 5816,  3659,  4487, 17668, 10992])","tensor([  101,  2823,  1010,  2023,  2003,  2036,  2124,  2004,  1996,  5816,
         3115, 24353,  1998, 19537, 12667,  2094,  1012,   102])"
312,1,"['variation', 'coefficient', 'limit', 'average', 'data', 'coefficient of variation']", Relative Spread Dispersion,seg_71,"note: if the data values for instance are temperatures measured in c, you cannot use the coefficient of variation! the average temperature (used as the denominator) may be 0 c or even negative! if the cv has to be meaningful, there must therefore be a lower limit of 0, i.e., negative values must not occur!",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 5816,  3659,  4487, 17668, 10992])","tensor([  101,  3602,  1024,  2065,  1996,  2951,  5300,  2005,  6013,  2024,
         7715,  7594,  1999,  1039,  1010,  2017,  3685,  2224,  1996, 19064,
         1997,  8386,   999,  1996,  2779,  4860,  1006,  2109,  2004,  1996,
         7939, 20936, 27413,  1007,  2089,  2022,  1014,  1039,  2030,  2130,
         4997,   999,  2065,  1996, 26226,  2038,  2000,  2022, 15902,  1010,
         2045,  2442,  3568,  2022,  1037,  2896,  5787,  1997,  1014,  1010,
         1045,  1012,  1041,  1012,  1010,  4997,  5300,  2442,  2025,  5258,
          999,   102])"
313,1,"['deviation', 'standard deviation', 'standard', 'average']", Relative Spread Dispersion,seg_71,we have previously found the average as x ¼ 4:5 and the standard deviation as s ¼ 1.29.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 5816,  3659,  4487, 17668, 10992])","tensor([  101,  2057,  2031,  3130,  2179,  1996,  2779,  2004,  1060,  1091,
         1018,  1024,  1019,  1998,  1996,  3115, 24353,  2004,  1055,  1091,
         1015,  1012,  2756,  1012,   102])"
314,1,"['variation', 'coefficient', 'coefficient of variation']", Relative Spread Dispersion,seg_71,this gives us the coefficient of variation:,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 5816,  3659,  4487, 17668, 10992])","tensor([  101,  2023,  3957,  2149,  1996, 19064,  1997,  8386,  1024,   102])"
315,1,"['function', 'deviation', 'variation', 'coefficient', 'standard deviation', 'standard', 'average', 'coefficient of variation', 'independent']", Relative Spread Dispersion,seg_71,"the coefficient of variation does not exist as an independent function, but can be calculated manually using the standard deviation and the average.",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 1., 0., 0.])","tensor([ 5816,  3659,  4487, 17668, 10992])","tensor([  101,  1996, 19064,  1997,  8386,  2515,  2025,  4839,  2004,  2019,
         2981,  3853,  1010,  2021,  2064,  2022, 10174, 21118,  2478,  1996,
         3115, 24353,  1998,  1996,  2779,  1012,   102])"
316,1,"['range', 'functions', 'statistical']", Example Statistical Functions in Spreadsheets,seg_73,"in most spreadsheets such as microsoft excel and open office calc, there is a wide range of statistical functions. the most important functions are listed in chap. 9.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  1999,  2087, 20861, 21030,  3215,  2107,  2004,  7513, 24970,
         1998,  2330,  2436, 10250,  2278,  1010,  2045,  2003,  1037,  2898,
         2846,  1997,  7778,  4972,  1012,  1996,  2087,  2590,  4972,  2024,
         3205,  1999, 15775,  2361,  1012,  1023,  1012,   102])"
317,1,['data'], Example Statistical Functions in Spreadsheets,seg_73,"data values are once again the age of the boys from the fitness club survey, a total of 17 data values, which are entered in the first row of a spreadsheet in cells a1 up to q1.",tensor(1),"tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  2951,  5300,  2024,  2320,  2153,  1996,  2287,  1997,  1996,
         3337,  2013,  1996, 10516,  2252,  5002,  1010,  1037,  2561,  1997,
         2459,  2951,  5300,  1010,  2029,  2024,  3133,  1999,  1996,  2034,
         5216,  1997,  1037, 20861, 21030,  2102,  1999,  4442, 17350,  2039,
         2000,  1053,  2487,  1012,   102])"
318,1,"['data', 'functions', 'statistics', 'statistical']", Example Statistical Functions in Spreadsheets,seg_73,"the data area should be specified when using all these statistical functions. figure 3.7 shows how all the above statistics are calculated using the statistical functions. here the columns a to c in the spreadsheet are only shown, but the data values are located in the whole area a1:q1.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  1996,  2951,  2181,  2323,  2022,  9675,  2043,  2478,  2035,
         2122,  7778,  4972,  1012,  3275,  1017,  1012,  1021,  3065,  2129,
         2035,  1996,  2682,  6747,  2024, 10174,  2478,  1996,  7778,  4972,
         1012,  2182,  1996,  7753,  1037,  2000,  1039,  1999,  1996, 20861,
        21030,  2102,  2024,  2069,  3491,  1010,  2021,  1996,  2951,  5300,
         2024,  2284,  1999,  1996,  2878,  2181, 17350,  1024,  1053,  2487,
         1012,   102])"
319,1,"['function', 'deviation', 'standard deviation', 'standard', 'average']", Example Statistical Functions in Spreadsheets,seg_73,"the average is calculated by using the average function, the standard deviation by using the stdev function.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  1996,  2779,  2003, 10174,  2011,  2478,  1996,  2779,  3853,
         1010,  1996,  3115, 24353,  2011,  2478,  1996,  2358, 24844,  3853,
         1012,   102])"
320,1,"['function', 'deviation', 'variation', 'percentage', 'coefficient', 'standard deviation', 'standard', 'average', 'coefficient of variation']", Example Statistical Functions in Spreadsheets,seg_73,"the coefficient of variation does not exist as a function, but it is calculated by dividing the standard deviation by the average, the result being displayed as a percentage (click on the % key in the spreadsheet).",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  1996, 19064,  1997,  8386,  2515,  2025,  4839,  2004,  1037,
         3853,  1010,  2021,  2009,  2003, 10174,  2011, 16023,  1996,  3115,
        24353,  2011,  1996,  2779,  1010,  1996,  2765,  2108,  6913,  2004,
         1037,  7017,  1006, 11562,  2006,  1996,  1003,  3145,  1999,  1996,
        20861, 21030,  2102,  1007,  1012,   102])"
321,1,"['function', 'median']", Example Statistical Functions in Spreadsheets,seg_73,the median is calculated by the function median and the mode by using the function mode.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  1996,  3991,  2003, 10174,  2011,  1996,  3853,  3991,  1998,
         1996,  5549,  2011,  2478,  1996,  3853,  5549,  1012,   102])"
322,1,"['parameter', 'function', 'quartile']", Example Statistical Functions in Spreadsheets,seg_73,quartiles are calculated using the function quartile. this function has an additional parameter to indicate which quartile we are calculating.,tensor(1),"tensor([0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101, 24209,  8445,  9463,  2015,  2024, 10174,  2478,  1996,  3853,
        24209,  8445,  9463,  1012,  2023,  3853,  2038,  2019,  3176, 16381,
         2000,  5769,  2029, 24209,  8445,  9463,  2057,  2024, 20177,  1012,
          102])"
323,1,"['parameter', 'function', 'quartile', 'median']", Example Statistical Functions in Spreadsheets,seg_73,"the value of this parameter is 3, when calculating the upper (third) quartile. it is 1 when calculating the lower (first) quartile. the median can be calculated as the second quartile by using the value 2, but of course the median also has its own function.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  1996,  3643,  1997,  2023, 16381,  2003,  1017,  1010,  2043,
        20177,  1996,  3356,  1006,  2353,  1007, 24209,  8445,  9463,  1012,
         2009,  2003,  1015,  2043, 20177,  1996,  2896,  1006,  2034,  1007,
        24209,  8445,  9463,  1012,  1996,  3991,  2064,  2022, 10174,  2004,
         1996,  2117, 24209,  8445,  9463,  2011,  2478,  1996,  3643,  1016,
         1010,  2021,  1997,  2607,  1996,  3991,  2036,  2038,  2049,  2219,
         3853,  1012,   102])"
324,1,['quartiles'], Example Statistical Functions in Spreadsheets,seg_73,the iqr is calculated by subtracting the upper and lower quartiles.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,
        0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  1996, 26264,  2099,  2003, 10174,  2011,  4942,  6494, 11873,
         1996,  3356,  1998,  2896, 24209,  8445,  9463,  2015,  1012,   102])"
325,1,"['function', 'range', 'functions', 'data']", Example Statistical Functions in Spreadsheets,seg_73,the range does not exist as a function. on the other hand there are functions for the maximum (max) and minimum (min) data values. the range can then be calculated by subtraction.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  1996,  2846,  2515,  2025,  4839,  2004,  1037,  3853,  1012,
         2006,  1996,  2060,  2192,  2045,  2024,  4972,  2005,  1996,  4555,
         1006,  4098,  1007,  1998,  6263,  1006,  8117,  1007,  2951,  5300,
         1012,  1996,  2846,  2064,  2059,  2022, 10174,  2011,  4942,  6494,
         7542,  1012,   102])"
326,1,"['location', 'data', 'functions', 'descriptive statistics', 'dispersion', 'statistics', 'statistical']", Example Statistical Functions in Spreadsheets,seg_73,"the main measures for location and dispersion can also be calculated in microsoft excel using the add-inmenu data analysis, which has a menu item descriptive statistics. the menu is not available in open office calc; here you need the statistical functions.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  1996,  2364,  5761,  2005,  3295,  1998,  4487, 17668, 10992,
         2064,  2036,  2022, 10174,  1999,  7513, 24970,  2478,  1996,  5587,
         1011,  1999,  3549,  2226,  2951,  4106,  1010,  2029,  2038,  1037,
        12183,  8875, 22726,  6747,  1012,  1996, 12183,  2003,  2025,  2800,
         1999,  2330,  2436, 10250,  2278,  1025,  2182,  2017,  2342,  1996,
         7778,  4972,  1012,   102])"
327,1,['data'], Example Statistical Functions in Spreadsheets,seg_73,"as an example we use the data for age, height and weight of all 30 kids from the fitness club survey. the result of applying the menu data analysis is shown in fig. 3.8.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  2004,  2019,  2742,  2057,  2224,  1996,  2951,  2005,  2287,
         1010,  4578,  1998,  3635,  1997,  2035,  2382,  4268,  2013,  1996,
        10516,  2252,  5002,  1012,  1996,  2765,  1997, 11243,  1996, 12183,
         2951,  4106,  2003,  3491,  1999, 20965,  1012,  1017,  1012,  1022,
         1012,   102])"
328,0,[], Example Statistical Functions in Spreadsheets,seg_73,"the concepts “standard error,” “kurtosis”, “skewness” and “confidence level” are explained in chap. 4.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  1996,  8474,  1523,  3115,  7561,  1010,  1524,  1523,  9679,
        12650,  1524,  1010,  1523, 15315,  7974,  2791,  1524,  1998,  1523,
         7023,  2504,  1524,  2024,  4541,  1999, 15775,  2361,  1012,  1018,
         1012,   102])"
329,1,"['location', 'dispersion']", Example Statistical Functions in Spreadsheets,seg_73,"in this chapter, we have discussed the most important measures of location (center) and dispersion (spread).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  1999,  2023,  3127,  1010,  2057,  2031,  6936,  1996,  2087,
         2590,  5761,  1997,  3295,  1006,  2415,  1007,  1998,  4487, 17668,
        10992,  1006,  3659,  1007,  1012,   102])"
330,1,"['skewed', 'distribution', 'data']", Example Statistical Functions in Spreadsheets,seg_73,the choice of measures depends on the distribution of the data values: is it symmetrical or skewed?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  1996,  3601,  1997,  5761,  9041,  2006,  1996,  4353,  1997,
         1996,  2951,  5300,  1024,  2003,  2009, 23476,  2030, 15315,  7974,
         2098,  1029,   102])"
331,1,"['location', 'distributions', 'dispersion', 'data']", Example Statistical Functions in Spreadsheets,seg_73,"in the next chapter, we primarily deal with symmetrical distributions. first, we give a more detailed discussion of various types of data, as well as the measures (of location and dispersion) to use for different types of data.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 2742,  7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  1999,  1996,  2279,  3127,  1010,  2057,  3952,  3066,  2007,
        23476, 20611,  1012,  2034,  1010,  2057,  2507,  1037,  2062,  6851,
         6594,  1997,  2536,  4127,  1997,  2951,  1010,  2004,  2092,  2004,
         1996,  5761,  1006,  1997,  3295,  1998,  4487, 17668, 10992,  1007,
         2000,  2224,  2005,  2367,  4127,  1997,  2951,  1012,   102])"
332,1,['data'], Data Type and Descriptive Statistics,seg_75,"if you work a lot with questionnaire data, you may want to read this section; otherwise, it can be skipped.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2951,  2828,  1998, 22726,  6747])","tensor([  101,  2065,  2017,  2147,  1037,  2843,  2007,  3160, 20589,  2951,
         1010,  2017,  2089,  2215,  2000,  3191,  2023,  2930,  1025,  4728,
         1010,  2009,  2064,  2022, 16791,  1012,   102])"
333,1,['data'], Data Types,seg_77,the main types of data are:,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([2951, 4127])","tensor([ 101, 1996, 2364, 4127, 1997, 2951, 2024, 1024,  102])"
334,1,['data'], Data Types,seg_77,"quantitative data are data such as weight, height, temperature, amounts of dollars, etc. these data are used for calculations and for defining the axes in a graph. they are subdivided into two types:",tensor(1),"tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([2951, 4127])","tensor([  101, 20155,  2951,  2024,  2951,  2107,  2004,  3635,  1010,  4578,
         1010,  4860,  1010,  8310,  1997,  6363,  1010,  4385,  1012,  2122,
         2951,  2024,  2109,  2005, 16268,  1998,  2005, 12854,  1996, 19589,
         1999,  1037, 10629,  1012,  2027,  2024, 15369,  2046,  2048,  4127,
         1024,   102])"
335,1,['data'], Data Types,seg_77,"– ratio data: here ratios are well defined; there is a natural zero, negative values",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([2951, 4127])","tensor([  101,  1516,  6463,  2951,  1024,  2182, 21879,  2024,  2092,  4225,
         1025,  2045,  2003,  1037,  3019,  5717,  1010,  4997,  5300,   102])"
336,1,"['table', 'interval', 'measurements', 'data']", Data Types,seg_77,"do not occur. this applies to most physical measurements, e.g., length; a table might be twice as long as another. – interval data: here differences are well defined, and negative numbers may",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2951, 4127])","tensor([  101,  2079,  2025,  5258,  1012,  2023, 12033,  2000,  2087,  3558,
        11702,  1010,  1041,  1012,  1043,  1012,  1010,  3091,  1025,  1037,
         2795,  2453,  2022,  3807,  2004,  2146,  2004,  2178,  1012,  1516,
        13483,  2951,  1024,  2182,  5966,  2024,  2092,  4225,  1010,  1998,
         4997,  3616,  2089,   102])"
337,0,[], Data Types,seg_77,occur. an example is temperature measured in c; an increase in temperature of 5 makes sense.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2951, 4127])","tensor([ 101, 5258, 1012, 2019, 2742, 2003, 4860, 7594, 1999, 1039, 1025, 2019,
        3623, 1999, 4860, 1997, 1019, 3084, 3168, 1012,  102])"
338,1,"['sample', 'tables', 'population', 'data']", Data Types,seg_77,"qualitative data are data such as sex, education, occupation, etc. these data correspond to groupings of the sample (or the population), in tables or graphs. they are subdivided into three types:",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2951, 4127])","tensor([  101, 24209, 11475, 27453,  2951,  2024,  2951,  2107,  2004,  3348,
         1010,  2495,  1010,  6139,  1010,  4385,  1012,  2122,  2951, 17254,
         2000, 19765,  2015,  1997,  1996,  7099,  1006,  2030,  1996,  2313,
         1007,  1010,  1999,  7251,  2030, 19287,  1012,  2027,  2024, 15369,
         2046,  2093,  4127,  1024,   102])"
339,1,"['categories', 'ordinal', 'data', 'ordinal data']", Data Types,seg_77,– ordinal data: a number of categories with a natural ordering. this applies to,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([2951, 4127])","tensor([  101,  1516,  2030, 18979,  2140,  2951,  1024,  1037,  2193,  1997,
         7236,  2007,  1037,  3019, 13063,  1012,  2023, 12033,  2000,   102])"
340,1,"['categories', 'nominal', 'data']", Data Types,seg_77,"many questionnaire data on, e.g., attitudes (on a scale of 1–5,...), grades in schools, etc. – nominal data: a number of named categories. for instance, various types of",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2951, 4127])","tensor([  101,  2116,  3160, 20589,  2951,  2006,  1010,  1041,  1012,  1043,
         1012,  1010, 13818,  1006,  2006,  1037,  4094,  1997,  1015,  1516,
         1019,  1010,  1012,  1012,  1012,  1007,  1010,  7022,  1999,  2816,
         1010,  4385,  1012,  1516, 15087,  2951,  1024,  1037,  2193,  1997,
         2315,  7236,  1012,  2005,  6013,  1010,  2536,  4127,  1997,   102])"
341,1,"['categories', 'data']", Data Types,seg_77,"fruit: apples, pears and bananas, etc. – alternative (binary) data: two categories (i.e., two alternatives). these data can",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([2951, 4127])","tensor([  101,  5909,  1024, 18108,  1010, 28253,  2015,  1998, 26191,  1010,
         4385,  1012,  1516,  4522,  1006, 12441,  1007,  2951,  1024,  2048,
         7236,  1006,  1045,  1012,  1041,  1012,  1010,  2048, 15955,  1007,
         1012,  2122,  2951,  2064,   102])"
342,1,"['nominal', 'ordinal', 'data']", Data Types,seg_77,"optionally be viewed as ordinal or nominal data. examples are: agree or disagree, good or bad, defective or non-defective, etc. we deal more with alternative data in chap. 5",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0.])","tensor([2951, 4127])","tensor([  101, 11887,  2135,  2022,  7021,  2004,  2030, 18979,  2140,  2030,
        15087,  2951,  1012,  4973,  2024,  1024,  5993,  2030, 21090,  1010,
         2204,  2030,  2919,  1010, 28829,  2030,  2512,  1011, 28829,  1010,
         4385,  1012,  2057,  3066,  2062,  2007,  4522,  2951,  1999, 15775,
         2361,  1012,  1019,   102])"
343,1,"['ordinal', 'data', 'quantitative']", Data Types,seg_77,"integer data (counting data, i.e., the data values are 0, 1, 2, 3, etc.) is yet another type of data. they are really in-between quantitative (ratio) data and qualitative (ordinal) data.",tensor(1),"tensor([0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        1., 0., 0.])","tensor([2951, 4127])","tensor([  101, 16109,  2951,  1006, 10320,  2951,  1010,  1045,  1012,  1041,
         1012,  1010,  1996,  2951,  5300,  2024,  1014,  1010,  1015,  1010,
         1016,  1010,  1017,  1010,  4385,  1012,  1007,  2003,  2664,  2178,
         2828,  1997,  2951,  1012,  2027,  2024,  2428,  1999,  1011,  2090,
        20155,  1006,  6463,  1007,  2951,  1998, 24209, 11475, 27453,  1006,
         2030, 18979,  2140,  1007,  2951,  1012,   102])"
344,1,"['data', 'quantitative']", Data Types,seg_77,"if the counts result in very large numbers (such as strokes of lightning during a powerful thunderstorm), it nevertheless makes sense to consider integer data as quantitative (ratio) data.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,
        0., 0.])","tensor([2951, 4127])","tensor([  101,  2065,  1996,  9294,  2765,  1999,  2200,  2312,  3616,  1006,
         2107,  2004, 13692,  1997,  7407,  2076,  1037,  3928,  8505, 19718,
         1007,  1010,  2009,  6600,  3084,  3168,  2000,  5136, 16109,  2951,
         2004, 20155,  1006,  6463,  1007,  2951,  1012,   102])"
345,1,"['data', 'variation', 'coefficient', 'statistics', 'quantitative', 'coefficient of variation']", Descriptive Statistics and Type of Data,seg_79,"throughout this chapter, we have assumed that our data are quantitative. there it makes sense to calculate all statistics, except that the coefficient of variation requires ratio data.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0.])","tensor([22726,  6747,  1998,  2828,  1997,  2951])","tensor([  101,  2802,  2023,  3127,  1010,  2057,  2031,  5071,  2008,  2256,
         2951,  2024, 20155,  1012,  2045,  2009,  3084,  3168,  2000, 18422,
         2035,  6747,  1010,  3272,  2008,  1996, 19064,  1997,  8386,  5942,
         6463,  2951,  1012,   102])"
346,1,"['data', 'statistics', 'quantitative', 'average']", Descriptive Statistics and Type of Data,seg_79,"for integer data we can calculate all the statistics, as for quantitative ratio data. however, the actual value of the average might not be a real data value! for instance, it is not possible that there can be 2.3 persons in a household. . .",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([22726,  6747,  1998,  2828,  1997,  2951])","tensor([  101,  2005, 16109,  2951,  2057,  2064, 18422,  2035,  1996,  6747,
         1010,  2004,  2005, 20155,  6463,  2951,  1012,  2174,  1010,  1996,
         5025,  3643,  1997,  1996,  2779,  2453,  2025,  2022,  1037,  2613,
         2951,  3643,   999,  2005,  6013,  1010,  2009,  2003,  2025,  2825,
         2008,  2045,  2064,  2022,  1016,  1012,  1017,  5381,  1999,  1037,
         4398,  1012,  1012,  1012,   102])"
347,1,['average'], Descriptive Statistics and Type of Data,seg_79,"however, an average of 2.3 persons in a household is actually very informative! it tells us that two persons in a household will be the most frequent situation;",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([22726,  6747,  1998,  2828,  1997,  2951])","tensor([  101,  2174,  1010,  2019,  2779,  1997,  1016,  1012,  1017,  5381,
         1999,  1037,  4398,  2003,  2941,  2200, 12367,  8082,   999,  2009,
         4136,  2149,  2008,  2048,  5381,  1999,  1037,  4398,  2097,  2022,
         1996,  2087,  6976,  3663,  1025,   102])"
348,1,"['deviation', 'variation', 'coefficient', 'standard deviation', 'standard', 'coefficient of variation']", Descriptive Statistics and Type of Data,seg_79,"another frequent situation will be households with three people. with caution, we can also use the standard deviation and coefficient of variation as measures of spread.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([22726,  6747,  1998,  2828,  1997,  2951])","tensor([  101,  2178,  6976,  3663,  2097,  2022,  3911,  2007,  2093,  2111,
         1012,  2007, 14046,  1010,  2057,  2064,  2036,  2224,  1996,  3115,
        24353,  1998, 19064,  1997,  8386,  2004,  5761,  1997,  3659,  1012,
          102])"
349,1,"['ordinal data', 'ordinal', 'quartiles', 'data', 'median']", Descriptive Statistics and Type of Data,seg_79,"for ordinal data, the situation is a bit more complicated! at least, the mode, the median, and the quartiles are well defined; here we are by definition using the fact that data can be ordered.",tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([22726,  6747,  1998,  2828,  1997,  2951])","tensor([  101,  2005,  2030, 18979,  2140,  2951,  1010,  1996,  3663,  2003,
         1037,  2978,  2062,  8552,   999,  2012,  2560,  1010,  1996,  5549,
         1010,  1996,  3991,  1010,  1998,  1996, 24209,  8445,  9463,  2015,
         2024,  2092,  4225,  1025,  2182,  2057,  2024,  2011,  6210,  2478,
         1996,  2755,  2008,  2951,  2064,  2022,  3641,  1012,   102])"
350,1,"['average', 'data']", Descriptive Statistics and Type of Data,seg_79,"for typical questionnaire data on a scale, e.g., from 1 to 5, many people will also calculate the average. this is in actual fact not meaningful; however, as for integer data, the average may be informative, if used with caution.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([22726,  6747,  1998,  2828,  1997,  2951])","tensor([  101,  2005,  5171,  3160, 20589,  2951,  2006,  1037,  4094,  1010,
         1041,  1012,  1043,  1012,  1010,  2013,  1015,  2000,  1019,  1010,
         2116,  2111,  2097,  2036, 18422,  1996,  2779,  1012,  2023,  2003,
         1999,  5025,  2755,  2025, 15902,  1025,  2174,  1010,  2004,  2005,
        16109,  2951,  1010,  1996,  2779,  2089,  2022, 12367,  8082,  1010,
         2065,  2109,  2007, 14046,  1012,   102])"
351,1,"['range', 'data', 'condition', 'statistics']", Descriptive Statistics and Type of Data,seg_79,"similarly, for this type of data we can calculate the range and iqr. if these statistics are to be meaningful, the difference between two numbers must be meaningful. this condition is usually not fulfilled. however, both statistics can be used as rough measures of spread, if used with caution.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([22726,  6747,  1998,  2828,  1997,  2951])","tensor([  101,  6660,  1010,  2005,  2023,  2828,  1997,  2951,  2057,  2064,
        18422,  1996,  2846,  1998, 26264,  2099,  1012,  2065,  2122,  6747,
         2024,  2000,  2022, 15902,  1010,  1996,  4489,  2090,  2048,  3616,
         2442,  2022, 15902,  1012,  2023,  4650,  2003,  2788,  2025, 16829,
         1012,  2174,  1010,  2119,  6747,  2064,  2022,  2109,  2004,  5931,
         5761,  1997,  3659,  1010,  2065,  2109,  2007, 14046,  1012,   102])"
352,1,"['location', 'nominal', 'statistic', 'data']", Descriptive Statistics and Type of Data,seg_79,"for nominal data, there is only statistic that makes sense: this is the mode as a measure of location. the most frequent data value is well defined here. the concept of a spread does not make sense.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([22726,  6747,  1998,  2828,  1997,  2951])","tensor([  101,  2005, 15087,  2951,  1010,  2045,  2003,  2069, 28093,  6553,
         2008,  3084,  3168,  1024,  2023,  2003,  1996,  5549,  2004,  1037,
         5468,  1997,  3295,  1012,  1996,  2087,  6976,  2951,  3643,  2003,
         2092,  4225,  2182,  1012,  1996,  4145,  1997,  1037,  3659,  2515,
         2025,  2191,  3168,  1012,   102])"
353,1,"['statistics', 'data']", Descriptive Statistics and Type of Data,seg_79,table 3.5 shows the statistics that can be used in connection with different data types.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.])","tensor([22726,  6747,  1998,  2828,  1997,  2951])","tensor([ 101, 2795, 1017, 1012, 1019, 3065, 1996, 6747, 2008, 2064, 2022, 2109,
        1999, 4434, 2007, 2367, 2951, 4127, 1012,  102])"
354,1,"['deviation', 'descriptive statistics', 'sample', 'standard deviation', 'standard', 'statistics', 'average']",Chapter  The Normal Distribution,seg_81,"in chap. 3, we explained how to calculate descriptive statistics such as the average and standard deviation of a sample. now we will see what these measures can be used for.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([3127, 1996, 3671, 4353])","tensor([  101,  1999, 15775,  2361,  1012,  1017,  1010,  2057,  4541,  2129,
         2000, 18422, 22726,  6747,  2107,  2004,  1996,  2779,  1998,  3115,
        24353,  1997,  1037,  7099,  1012,  2085,  2057,  2097,  2156,  2054,
         2122,  5761,  2064,  2022,  2109,  2005,  1012,   102])"
355,0,[],Chapter  The Normal Distribution,seg_81,"imagine that you buy a bag with 500 g of coffee. you are curious and empty the contents onto a weight to check whether the bag actually contains 500 g. if you have a very precise weight, you will hardly expect that the content weighs exactly 500 g and you are probably not surprised if it is a little more or less.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([3127, 1996, 3671, 4353])","tensor([  101,  5674,  2008,  2017,  4965,  1037,  4524,  2007,  3156,  1043,
         1997,  4157,  1012,  2017,  2024,  8025,  1998,  4064,  1996,  8417,
         3031,  1037,  3635,  2000,  4638,  3251,  1996,  4524,  2941,  3397,
         3156,  1043,  1012,  2065,  2017,  2031,  1037,  2200, 10480,  3635,
         1010,  2017,  2097,  6684,  5987,  2008,  1996,  4180, 21094,  3599,
         3156,  1043,  1998,  2017,  2024,  2763,  2025,  4527,  2065,  2009,
         2003,  1037,  2210,  2062,  2030,  2625,  1012,   102])"
356,1,"['vary', 'average', 'experiment']",Chapter  The Normal Distribution,seg_81,"if you are repeating the experiment with many bags, you might expect that the weight of a bag will be very close to 500 g on average. you may also expect that there will not be too much spread. for instance, you do not expect to get less than 450 or 550 g, not even once. the weight of a bag can perhaps vary around 490–510 g, but it will rarely be more than 510 g or less than 490 g.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([3127, 1996, 3671, 4353])","tensor([  101,  2065,  2017,  2024, 15192,  1996,  7551,  2007,  2116,  8641,
         1010,  2017,  2453,  5987,  2008,  1996,  3635,  1997,  1037,  4524,
         2097,  2022,  2200,  2485,  2000,  3156,  1043,  2006,  2779,  1012,
         2017,  2089,  2036,  5987,  2008,  2045,  2097,  2025,  2022,  2205,
         2172,  3659,  1012,  2005,  6013,  1010,  2017,  2079,  2025,  5987,
         2000,  2131,  2625,  2084, 10332,  2030, 13274,  1043,  1010,  2025,
         2130,  2320,  1012,  1996,  3635,  1997,  1037,  4524,  2064,  3383,
         8137,  2105, 22288,  1516, 23475,  1043,  1010,  2021,  2009,  2097,
         6524,  2022,  2062,  2084, 23475,  1043,  2030,  2625,  2084, 22288,
         1043,  1012,   102])"
357,1,"['normal distribution', 'variation', 'normal', 'distribution', 'statistical', 'data']",Chapter  The Normal Distribution,seg_81,"this variation can be described by a statistical distribution. the most important statistical distribution is the normal distribution (*). several statistical techniques require that data “follow” (i.e., can be described by) a normal distribution. if data do not follow a normal distribution, it becomes more difficult to analyze the data.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([3127, 1996, 3671, 4353])","tensor([  101,  2023,  8386,  2064,  2022,  2649,  2011,  1037,  7778,  4353,
         1012,  1996,  2087,  2590,  7778,  4353,  2003,  1996,  3671,  4353,
         1006,  1008,  1007,  1012,  2195,  7778,  5461,  5478,  2008,  2951,
         1523,  3582,  1524,  1006,  1045,  1012,  1041,  1012,  1010,  2064,
         2022,  2649,  2011,  1007,  1037,  3671,  4353,  1012,  2065,  2951,
         2079,  2025,  3582,  1037,  3671,  4353,  1010,  2009,  4150,  2062,
         3697,  2000, 17908,  1996,  2951,  1012,   102])"
358,1,"['normal distribution', 'uncertainty', 'data', 'statistical uncertainty', 'estimate', 'sample average', 'sample', 'normal', 'distribution', 'statistical', 'average']",Chapter  The Normal Distribution,seg_81,"this chapter examines some important properties of the normal distribution. we also see how the fact that data follow a normal distribution can actually be verified. finally, we see how to estimate the statistical uncertainty (*) of a sample average.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([3127, 1996, 3671, 4353])","tensor([  101,  2023,  3127, 20798,  2070,  2590,  5144,  1997,  1996,  3671,
         4353,  1012,  2057,  2036,  2156,  2129,  1996,  2755,  2008,  2951,
         3582,  1037,  3671,  4353,  2064,  2941,  2022, 20119,  1012,  2633,
         1010,  2057,  2156,  2129,  2000, 10197,  1996,  7778, 12503,  1006,
         1008,  1007,  1997,  1037,  7099,  2779,  1012,   102])"
359,1,"['curve', 'normal distribution', 'histogram', 'normal', 'distribution', 'data', 'case']", Characteristics of the Normal Distribution,seg_83,"the normal distribution curve is a symmetrical, “bell-shaped” curve similar to a histogram – in this case showing the weight of a very large number of coffee bags. it has been proven in practice that the normal distribution often gives a good description of many types ofmeasurement data, such as weight, height, etc. but the normal distribution is very important also for economic and administrative data.",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([6459, 1997, 1996, 3671, 4353])","tensor([  101,  1996,  3671,  4353,  7774,  2003,  1037, 23476,  1010,  1523,
         4330,  1011,  5044,  1524,  7774,  2714,  2000,  1037,  2010,  3406,
        13113,  1516,  1999,  2023,  2553,  4760,  1996,  3635,  1997,  1037,
         2200,  2312,  2193,  1997,  4157,  8641,  1012,  2009,  2038,  2042,
        10003,  1999,  3218,  2008,  1996,  3671,  4353,  2411,  3957,  1037,
         2204,  6412,  1997,  2116,  4127,  1997,  4168,  3022,  5397,  3672,
         2951,  1010,  2107,  2004,  3635,  1010,  4578,  1010,  4385,  1012,
         2021,  1996,  3671,  4353,  2003,  2200,  2590,  2036,  2005,  3171,
         1998,  3831,  2951,  1012,   102])"
360,1,"['mean', 'deviation', 'normal distribution', 'normal', 'distribution', 'average']", Characteristics of the Normal Distribution,seg_83,a normal distribution is completely described by its mean (average) and standard deviation.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.])","tensor([6459, 1997, 1996, 3671, 4353])","tensor([  101,  1037,  3671,  4353,  2003,  3294,  2649,  2011,  2049,  2812,
         1006,  2779,  1007,  1998,  3115, 24353,  1012,   102])"
361,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution']", Characteristics of the Normal Distribution,seg_83,"the normal distribution in the example above describes the weight of all the coffee bags manufactured by the factory. since we do not know the mean and standard deviation, they are often written in greek letters:",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([6459, 1997, 1996, 3671, 4353])","tensor([  101,  1996,  3671,  4353,  1999,  1996,  2742,  2682,  5577,  1996,
         3635,  1997,  2035,  1996,  4157,  8641,  7609,  2011,  1996,  4713,
         1012,  2144,  2057,  2079,  2025,  2113,  1996,  2812,  1998,  3115,
        24353,  1010,  2027,  2024,  2411,  2517,  1999,  3306,  4144,  1024,
          102])"
362,1,"['mean', 'deviation', 'standard deviation', 'standard']", Characteristics of the Normal Distribution,seg_83,– mean: m (read “mju”) representing the “center” – standard deviation: s (read “sigma”) representing the “spread”,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6459, 1997, 1996, 3671, 4353])","tensor([  101,  1516,  2812,  1024,  1049,  1006,  3191,  1523,  1049,  9103,
         1524,  1007,  5052,  1996,  1523,  2415,  1524,  1516,  3115, 24353,
         1024,  1055,  1006,  3191,  1523, 13201,  1524,  1007,  5052,  1996,
         1523,  3659,  1524,   102])"
363,1,"['normal distributions', 'distributions', 'normal']", Characteristics of the Normal Distribution,seg_83,"in fig. 4.1, we see two normal distributions with small spread (s ¼ 1, above) and two normal distributions with large spread (s ¼ 2, below). the two distributions in each group have different means, m ¼ 10 and m ¼ 24.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([6459, 1997, 1996, 3671, 4353])","tensor([  101,  1999, 20965,  1012,  1018,  1012,  1015,  1010,  2057,  2156,
         2048,  3671, 20611,  2007,  2235,  3659,  1006,  1055,  1091,  1015,
         1010,  2682,  1007,  1998,  2048,  3671, 20611,  2007,  2312,  3659,
         1006,  1055,  1091,  1016,  1010,  2917,  1007,  1012,  1996,  2048,
        20611,  1999,  2169,  2177,  2031,  2367,  2965,  1010,  1049,  1091,
         2184,  1998,  1049,  1091,  2484,  1012,   102])"
364,1,"['deviation', 'normal distribution', 'histogram', 'population', 'sample', 'normal', 'standard deviation', 'standard', 'distribution']", Characteristics of the Normal Distribution,seg_83,fig. 4.2 shows the interpretation of the standard deviation in a normal distribution.here is shown a normal distribution representing the histogram of a population or a very large sample.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0.])","tensor([6459, 1997, 1996, 3671, 4353])","tensor([  101, 20965,  1012,  1018,  1012,  1016,  3065,  1996,  7613,  1997,
         1996,  3115, 24353,  1999,  1037,  3671,  4353,  1012,  2182,  2003,
         3491,  1037,  3671,  4353,  5052,  1996,  2010,  3406, 13113,  1997,
         1037,  2313,  2030,  1037,  2200,  2312,  7099,  1012,   102])"
365,0,[], Characteristics of the Normal Distribution,seg_83,we observe that:,tensor(0),"tensor([0, 0, 0, 0, 0, 0])","tensor([6459, 1997, 1996, 3671, 4353])","tensor([  101,  2057, 11949,  2008,  1024,   102])"
366,1,"['mean', 'deviation', 'interval', 'standard deviation', 'deviations', 'standard', 'data', 'standard deviations']", Characteristics of the Normal Distribution,seg_83,– 68% of the data values are in an interval around mean standard deviation – 95% of the data values are in an interval around mean 2 standard deviations – 99.7% of the data values are in an interval around mean 3 standard deviations,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.])","tensor([6459, 1997, 1996, 3671, 4353])","tensor([  101,  1516,  6273,  1003,  1997,  1996,  2951,  5300,  2024,  1999,
         2019, 13483,  2105,  2812,  3115, 24353,  1516,  5345,  1003,  1997,
         1996,  2951,  5300,  2024,  1999,  2019, 13483,  2105,  2812,  1016,
         3115, 24353,  2015,  1516,  5585,  1012,  1021,  1003,  1997,  1996,
         2951,  5300,  2024,  1999,  2019, 13483,  2105,  2812,  1017,  3115,
        24353,  2015,   102])"
367,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution', 'percentages']", Characteristics of the Normal Distribution,seg_83,"these percentages are unique to the normal distribution! in a way, the normal distribution is “thinking” as if 0 corresponds to the mean and 1 unit corresponds to the standard deviation.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0.])","tensor([6459, 1997, 1996, 3671, 4353])","tensor([  101,  2122,  7017,  2015,  2024,  4310,  2000,  1996,  3671,  4353,
          999,  1999,  1037,  2126,  1010,  1996,  3671,  4353,  2003,  1523,
         3241,  1524,  2004,  2065,  1014, 14788,  2000,  1996,  2812,  1998,
         1015,  3131, 14788,  2000,  1996,  3115, 24353,  1012,   102])"
368,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution']", Characteristics of the Normal Distribution,seg_83,"if x follows a normal distribution with mean m and standard deviation s, then",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([6459, 1997, 1996, 3671, 4353])","tensor([  101,  2065,  1060,  4076,  1037,  3671,  4353,  2007,  2812,  1049,
         1998,  3115, 24353,  1055,  1010,  2059,   102])"
369,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution']", Characteristics of the Normal Distribution,seg_83,follows a normal distribution with mean 0 and standard deviation 1.,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.])","tensor([6459, 1997, 1996, 3671, 4353])","tensor([  101,  4076,  1037,  3671,  4353,  2007,  2812,  1014,  1998,  3115,
        24353,  1015,  1012,   102])"
370,1,"['mean', 'standardized', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution']", Characteristics of the Normal Distribution,seg_83,"there exists in a way, only one normal distribution! the normal distribution with mean 0 and standard deviation 1 is therefore called the standardized normal distribution. see an example in the section “calculations in the normal distribution”.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([6459, 1997, 1996, 3671, 4353])","tensor([  101,  2045,  6526,  1999,  1037,  2126,  1010,  2069,  2028,  3671,
         4353,   999,  1996,  3671,  4353,  2007,  2812,  1014,  1998,  3115,
        24353,  1015,  2003,  3568,  2170,  1996, 16367,  3671,  4353,  1012,
         2156,  2019,  2742,  1999,  1996,  2930,  1523, 16268,  1999,  1996,
         3671,  4353,  1524,  1012,   102])"
371,1,"['curve', 'normal distribution', 'probabilities', 'normal', 'distribution']", Density Function and Distribution Function,seg_85,"in practice, it is therefore areas under normal distribution curve, which are interesting because they can be interpreted as probabilities. therefore, we are usually interested in the curve showing areas under the normal distribution curve. the relationship between these two graphs is shown below.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([4304, 3853, 1998, 4353, 3853])","tensor([  101,  1999,  3218,  1010,  2009,  2003,  3568,  2752,  2104,  3671,
         4353,  7774,  1010,  2029,  2024,  5875,  2138,  2027,  2064,  2022,
        10009,  2004,  4013,  3676, 14680,  1012,  3568,  1010,  2057,  2024,
         2788,  4699,  1999,  1996,  7774,  4760,  2752,  2104,  1996,  3671,
         4353,  7774,  1012,  1996,  3276,  2090,  2122,  2048, 19287,  2003,
         3491,  2917,  1012,   102])"
372,1,"['curve', 'function', 'density function', 'probabilities', 'function ', 'distribution', 'distribution function']", Density Function and Distribution Function,seg_85,"the bell-shaped curve (fig. 4.3) is called the density function (*), while the curve showing areas (probabilities) is called the distribution function (*).",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0.])","tensor([4304, 3853, 1998, 4353, 3853])","tensor([  101,  1996,  4330,  1011,  5044,  7774,  1006, 20965,  1012,  1018,
         1012,  1017,  1007,  2003,  2170,  1996,  4304,  3853,  1006,  1008,
         1007,  1010,  2096,  1996,  7774,  4760,  2752,  1006,  4013,  3676,
        14680,  1007,  2003,  2170,  1996,  4353,  3853,  1006,  1008,  1007,
         1012,   102])"
373,1,"['function', 'probability', 'distribution', 'distribution function', 'data']", Density Function and Distribution Function,seg_85,the distribution function is often written using the letter f. we can interpret the distribution function by noticing that f(x) is the probability of observing data values up to and including x.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([4304, 3853, 1998, 4353, 3853])","tensor([  101,  1996,  4353,  3853,  2003,  2411,  2517,  2478,  1996,  3661,
         1042,  1012,  2057,  2064, 17841,  1996,  4353,  3853,  2011, 15103,
         2008,  1042,  1006,  1060,  1007,  2003,  1996,  9723,  1997, 14158,
         2951,  5300,  2039,  2000,  1998,  2164,  1060,  1012,   102])"
374,1,"['function', 'density function', 'distribution', 'distribution function']", Density Function and Distribution Function,seg_85,"in real-world problems, we almost always need the distribution function. you only need the density function for constructing illustrations in a book.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([4304, 3853, 1998, 4353, 3853])","tensor([  101,  1999,  2613,  1011,  2088,  3471,  1010,  2057,  2471,  2467,
         2342,  1996,  4353,  3853,  1012,  2017,  2069,  2342,  1996,  4304,
         3853,  2005, 15696, 11249,  1999,  1037,  2338,  1012,   102])"
375,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution']", Fractiles,seg_87,let us assume that the weight of the coffee in a bag of coffee follows a normal distribution with mean 500 g and standard deviation 5 g. we want to answer questions such as the following:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([25312,  6593,  9463,  2015])","tensor([  101,  2292,  2149,  7868,  2008,  1996,  3635,  1997,  1996,  4157,
         1999,  1037,  4524,  1997,  4157,  4076,  1037,  3671,  4353,  2007,
         2812,  3156,  1043,  1998,  3115, 24353,  1019,  1043,  1012,  2057,
         2215,  2000,  3437,  3980,  2107,  2004,  1996,  2206,  1024,   102])"
376,0,[], Fractiles,seg_87,1. how many coffee bags are weighing at most 495 g?,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([25312,  6593,  9463,  2015])","tensor([  101,  1015,  1012,  2129,  2116,  4157,  8641,  2024, 15243,  2012,
         2087, 29302,  1043,  1029,   102])"
377,1,"['function', 'probability', 'distribution', 'distribution function', 'data']", Fractiles,seg_87,"we use the distribution function: find the value 495 on the x-axis and move vertically up to f(495), i.e., find the corresponding value on the y-axis. this is precisely the probability of data values up to 495 g. on the graph (fig. 4.4) below, we see that it is roughly 0.16, equivalent to 16%. 2. which weight value separates the lightest 80% of the coffee bags from the rest?",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([25312,  6593,  9463,  2015])","tensor([  101,  2057,  2224,  1996,  4353,  3853,  1024,  2424,  1996,  3643,
        29302,  2006,  1996,  1060,  1011,  8123,  1998,  2693, 20018,  2039,
         2000,  1042,  1006, 29302,  1007,  1010,  1045,  1012,  1041,  1012,
         1010,  2424,  1996,  7978,  3643,  2006,  1996,  1061,  1011,  8123,
         1012,  2023,  2003, 10785,  1996,  9723,  1997,  2951,  5300,  2039,
         2000, 29302,  1043,  1012,  2006,  1996, 10629,  1006, 20965,  1012,
         1018,  1012,  1018,  1007,  2917,  1010,  2057,  2156,  2008,  2009,
         2003,  5560,  1014,  1012,  2385,  1010,  5662,  2000,  2385,  1003,
         1012,  1016,  1012,  2029,  3635,  3643, 18600,  1996,  2422,  4355,
         3770,  1003,  1997,  1996,  4157,  8641,  2013,  1996,  2717,  1029,
          102])"
378,1,"['curve', 'function', 'probability', 'distribution', 'distribution function']", Fractiles,seg_87,"we now use the distribution function the opposite way: find the value 0.80 (equivalent to 80%) on the y-axis (i.e., a probability of 80%) and move horizontally to the distribution curve, then find the corresponding value of the x-axis. on the graph below, we can see that it is roughly 504 g (fig. 4.5).",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([25312,  6593,  9463,  2015])","tensor([  101,  2057,  2085,  2224,  1996,  4353,  3853,  1996,  4500,  2126,
         1024,  2424,  1996,  3643,  1014,  1012,  3770,  1006,  5662,  2000,
         3770,  1003,  1007,  2006,  1996,  1061,  1011,  8123,  1006,  1045,
         1012,  1041,  1012,  1010,  1037,  9723,  1997,  3770,  1003,  1007,
         1998,  2693, 23190,  2000,  1996,  4353,  7774,  1010,  2059,  2424,
         1996,  7978,  3643,  1997,  1996,  1060,  1011,  8123,  1012,  2006,
         1996, 10629,  2917,  1010,  2057,  2064,  2156,  2008,  2009,  2003,
         5560,  2753,  2549,  1043,  1006, 20965,  1012,  1018,  1012,  1019,
         1007,  1012,   102])"
379,1,"['function', 'normal distribution', 'normal', 'distribution', 'distribution function']", Fractiles,seg_87,"we therefore need to use the normal distribution function in “both ways”. when we are using the distribution function in the “reverse” way, for example, from 0.80 ¼ 80% on the y-axis to a value on the x-axis, we are talking",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([25312,  6593,  9463,  2015])","tensor([ 101, 2057, 3568, 2342, 2000, 2224, 1996, 3671, 4353, 3853, 1999, 1523,
        2119, 3971, 1524, 1012, 2043, 2057, 2024, 2478, 1996, 4353, 3853, 1999,
        1996, 1523, 7901, 1524, 2126, 1010, 2005, 2742, 1010, 2013, 1014, 1012,
        3770, 1091, 3770, 1003, 2006, 1996, 1061, 1011, 8123, 2000, 1037, 3643,
        2006, 1996, 1060, 1011, 8123, 1010, 2057, 2024, 3331,  102])"
380,1,"['quantile', 'normal distribution', 'fractile', 'normal', 'distribution', 'percentile']", Fractiles,seg_87,about finding a fractile (*) (also called a quantile or a percentile) in the distribution. the figure above shows the finding of 80% fractile in a normal distribution.,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 0., 0.])","tensor([25312,  6593,  9463,  2015])","tensor([  101,  2055,  4531,  1037, 25312,  6593,  9463,  1006,  1008,  1007,
         1006,  2036,  2170,  1037, 24110, 15286,  2030,  1037,  3867,  9463,
         1007,  1999,  1996,  4353,  1012,  1996,  3275,  2682,  3065,  1996,
         4531,  1997,  3770,  1003, 25312,  6593,  9463,  1999,  1037,  3671,
         4353,  1012,   102])"
381,1,"['fractiles', 'fractile', 'quartiles', 'median']", Fractiles,seg_87,"we have actually seen in chap. 3 the most important fractiles: the quartiles are the 25 and 75% fractiles, and the median is the 50% fractile.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([25312,  6593,  9463,  2015])","tensor([  101,  2057,  2031,  2941,  2464,  1999, 15775,  2361,  1012,  1017,
         1996,  2087,  2590, 25312,  6593,  9463,  2015,  1024,  1996, 24209,
         8445,  9463,  2015,  2024,  1996,  2423,  1998,  4293,  1003, 25312,
         6593,  9463,  2015,  1010,  1998,  1996,  3991,  2003,  1996,  2753,
         1003, 25312,  6593,  9463,  1012,   102])"
382,1,['fractiles'], Fractiles,seg_87,"the fractiles corresponding to 10, 20, 30%, etc., are called the deciles.",tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([25312,  6593,  9463,  2015])","tensor([  101,  1996, 25312,  6593,  9463,  2015,  7978,  2000,  2184,  1010,
         2322,  1010,  2382,  1003,  1010,  4385,  1012,  1010,  2024,  2170,
         1996, 11703,  9463,  2015,  1012,   102])"
383,1,"['table', 'normal', 'normal distribution', 'distribution']", Calculations in the Normal Distribution,seg_89,wewill now show how to do simple calculations in the normal distribution by using a table of the normal distribution.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 0., 1., 1., 0., 0.])","tensor([16268,  1999,  1996,  3671,  4353])","tensor([  101,  2057, 29602,  2140,  2085,  2265,  2129,  2000,  2079,  3722,
        16268,  1999,  1996,  3671,  4353,  2011,  2478,  1037,  2795,  1997,
         1996,  3671,  4353,  1012,   102])"
384,1,"['normal distribution', 'table', 'fractiles', 'normal', 'tables', 'distribution']", Calculations in the Normal Distribution,seg_89,the most important fractiles in the normal distribution are found in the table at the end of the book. more detailed tables on the normal distribution can be found in many books.,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([16268,  1999,  1996,  3671,  4353])","tensor([  101,  1996,  2087,  2590, 25312,  6593,  9463,  2015,  1999,  1996,
         3671,  4353,  2024,  2179,  1999,  1996,  2795,  2012,  1996,  2203,
         1997,  1996,  2338,  1012,  2062,  6851,  7251,  2006,  1996,  3671,
         4353,  2064,  2022,  2179,  1999,  2116,  2808,  1012,   102])"
385,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution']", Calculations in the Normal Distribution,seg_89,"example. let us assume that the weight, x, of (the coffee in) a bag of coffee follows a normal distribution with a mean m ¼ 500 g and standard deviation s ¼ 5 g.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0.])","tensor([16268,  1999,  1996,  3671,  4353])","tensor([  101,  2742,  1012,  2292,  2149,  7868,  2008,  1996,  3635,  1010,
         1060,  1010,  1997,  1006,  1996,  4157,  1999,  1007,  1037,  4524,
         1997,  4157,  4076,  1037,  3671,  4353,  2007,  1037,  2812,  1049,
         1091,  3156,  1043,  1998,  3115, 24353,  1055,  1091,  1019,  1043,
         1012,   102])"
386,0,[], Calculations in the Normal Distribution,seg_89,as mentioned earlier:,tensor(0),"tensor([0, 0, 0, 0, 0, 0])","tensor([16268,  1999,  1996,  3671,  4353])","tensor([ 101, 2004, 3855, 3041, 1024,  102])"
387,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution']", Calculations in the Normal Distribution,seg_89,follows a normal distribution with mean 0 and standard deviation 1.,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.])","tensor([16268,  1999,  1996,  3671,  4353])","tensor([  101,  4076,  1037,  3671,  4353,  2007,  2812,  1014,  1998,  3115,
        24353,  1015,  1012,   102])"
388,1,"['standardized', 'function', 'normal distribution', 'normal', 'distribution', 'distribution function']", Calculations in the Normal Distribution,seg_89,we say that we standardize x in this calculation. the standardized normal distribution function is tabulated in chap. 9.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([16268,  1999,  1996,  3671,  4353])","tensor([  101,  2057,  2360,  2008,  2057,  3115,  4697,  1060,  1999,  2023,
        17208,  1012,  1996, 16367,  3671,  4353,  3853,  2003, 21628,  8898,
         1999, 15775,  2361,  1012,  1023,  1012,   102])"
389,0,[], Calculations in the Normal Distribution,seg_89,now we can answer questions such as:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([16268,  1999,  1996,  3671,  4353])","tensor([ 101, 2085, 2057, 2064, 3437, 3980, 2107, 2004, 1024,  102])"
390,1,"['probability', 'random']", Calculations in the Normal Distribution,seg_89,1. what is the probability that a random coffee bag weighs at most 510 g?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([16268,  1999,  1996,  3671,  4353])","tensor([  101,  1015,  1012,  2054,  2003,  1996,  9723,  2008,  1037,  6721,
         4157,  4524, 21094,  2012,  2087, 23475,  1043,  1029,   102])"
391,0,[], Calculations in the Normal Distribution,seg_89,we standardize the 510 g and obtain:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([16268,  1999,  1996,  3671,  4353])","tensor([  101,  2057,  3115,  4697,  1996, 23475,  1043,  1998,  6855,  1024,
          102])"
392,1,"['standardized', 'normal distribution', 'table', 'probability', 'fractile', 'random', 'normal', 'distribution', 'data']", Calculations in the Normal Distribution,seg_89,"by looking in a table of the standardized normal distribution, we find that the probability of a data value 2 is 0.0977 ¼ 97.7%. this is then the probability that a random coffee bag weighs, at most, 510 g. 2. what is the 95% fractile in the distribution?",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0.])","tensor([16268,  1999,  1996,  3671,  4353])","tensor([  101,  2011,  2559,  1999,  1037,  2795,  1997,  1996, 16367,  3671,
         4353,  1010,  2057,  2424,  2008,  1996,  9723,  1997,  1037,  2951,
         3643,  1016,  2003,  1014,  1012,  5641,  2581,  2581,  1091,  5989,
         1012,  1021,  1003,  1012,  2023,  2003,  2059,  1996,  9723,  2008,
         1037,  6721,  4157,  4524, 21094,  1010,  2012,  2087,  1010, 23475,
         1043,  1012,  1016,  1012,  2054,  2003,  1996,  5345,  1003, 25312,
         6593,  9463,  1999,  1996,  4353,  1029,   102])"
393,1,"['standardized', 'normal distribution', 'table', 'fractile', 'normal', 'distribution']", Calculations in the Normal Distribution,seg_89,"in a table of the standardized normal distribution (at the end of the book), you find the 95% fractile to be 1.65. this represents a fractile in the distribution of",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 0.])","tensor([16268,  1999,  1996,  3671,  4353])","tensor([  101,  1999,  1037,  2795,  1997,  1996, 16367,  3671,  4353,  1006,
         2012,  1996,  2203,  1997,  1996,  2338,  1007,  1010,  2017,  2424,
         1996,  5345,  1003, 25312,  6593,  9463,  2000,  2022,  1015,  1012,
         3515,  1012,  2023,  5836,  1037, 25312,  6593,  9463,  1999,  1996,
         4353,  1997,   102])"
394,0,[], Calculations in the Normal Distribution,seg_89,where x is the weight of a randomly selected coffee bag. by solving the equation,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([16268,  1999,  1996,  3671,  4353])","tensor([  101,  2073,  1060,  2003,  1996,  3635,  1997,  1037, 18154,  3479,
         4157,  4524,  1012,  2011, 13729,  1996,  8522,   102])"
395,1,"['probability', 'fractile', 'random', 'distribution']", Calculations in the Normal Distribution,seg_89,"we obtain x ¼ 500 1.65 ¼ 508.25. this means that the 95% fractile in the distribution of a randomly chosen coffee bag is 508.25. in other words, the probability that a random coffee bag weighs less than 508.25 g is exactly 95%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([16268,  1999,  1996,  3671,  4353])","tensor([  101,  2057,  6855,  1060,  1091,  3156,  1015,  1012,  3515,  1091,
         2753,  2620,  1012,  2423,  1012,  2023,  2965,  2008,  1996,  5345,
         1003, 25312,  6593,  9463,  1999,  1996,  4353,  1997,  1037, 18154,
         4217,  4157,  4524,  2003,  2753,  2620,  1012,  2423,  1012,  1999,
         2060,  2616,  1010,  1996,  9723,  2008,  1037,  6721,  4157,  4524,
        21094,  2625,  2084,  2753,  2620,  1012,  2423,  1043,  2003,  3599,
         5345,  1003,  1012,   102])"
396,0,[], The Normal Distribution and Spreadsheets,seg_91,"if you do not use spreadsheets, you can skip this section.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 1996,  3671,  4353,  1998, 20861, 21030,  3215])","tensor([  101,  2065,  2017,  2079,  2025,  2224, 20861, 21030,  3215,  1010,
         2017,  2064, 13558,  2023,  2930,  1012,   102])"
397,1,"['normal distribution', 'functions', 'normal', 'distribution']", The Normal Distribution and Spreadsheets,seg_91,there are in microsoft excel and openoffice calc two important functions for the normal distribution.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 0., 0.])","tensor([ 1996,  3671,  4353,  1998, 20861, 21030,  3215])","tensor([  101,  2045,  2024,  1999,  7513, 24970,  1998,  2330,  7245,  6610,
        10250,  2278,  2048,  2590,  4972,  2005,  1996,  3671,  4353,  1012,
          102])"
398,1,"['function', 'normal distribution', 'density function', 'fractiles', 'normal', 'distribution', 'distribution function']", The Normal Distribution and Spreadsheets,seg_91,– normdist: provides the distribution function or density function for a normal distribution. – norminv: gives fractiles in a normal distribution.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.])","tensor([ 1996,  3671,  4353,  1998, 20861, 21030,  3215])","tensor([  101,  1516, 13373, 10521,  2102,  1024,  3640,  1996,  4353,  3853,
         2030,  4304,  3853,  2005,  1037,  3671,  4353,  1012,  1516, 13373,
         2378,  2615,  1024,  3957, 25312,  6593,  9463,  2015,  1999,  1037,
         3671,  4353,  1012,   102])"
399,1,"['curve', 'function', 'density function', 'table']", NORMDIST X Mean Stdev Cumulative,seg_93,"it is only if you have to make figures with the “bell-shaped” curve, that you need the density function. therefore, you should almost always use cumulative ¼ 1 (table 4.1).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0.])","tensor([13373, 10521,  2102,  1060,  2812,  2358, 24844, 23260])","tensor([  101,  2009,  2003,  2069,  2065,  2017,  2031,  2000,  2191,  4481,
         2007,  1996,  1523,  4330,  1011,  5044,  1524,  7774,  1010,  2008,
         2017,  2342,  1996,  4304,  3853,  1012,  3568,  1010,  2017,  2323,
         2471,  2467,  2224, 23260,  1091,  1015,  1006,  2795,  1018,  1012,
         1015,  1007,  1012,   102])"
400,1,"['mean', 'standardized', 'function', 'normal distribution', 'parameter', 'deviation', 'normal', 'standard deviation', 'standard', 'distribution', 'distribution function']", NORMDIST X Mean Stdev Cumulative,seg_93,the distribution function for the standardized normal distribution (with mean 0 and standard deviation 1) can also be obtained by using the function normsdist. this function only has one parameter: x.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0.])","tensor([13373, 10521,  2102,  1060,  2812,  2358, 24844, 23260])","tensor([  101,  1996,  4353,  3853,  2005,  1996, 16367,  3671,  4353,  1006,
         2007,  2812,  1014,  1998,  3115, 24353,  1015,  1007,  2064,  2036,
         2022,  4663,  2011,  2478,  1996,  3853, 17606, 10521,  2102,  1012,
         2023,  3853,  2069,  2038,  2028, 16381,  1024,  1060,  1012,   102])"
401,1,"['mean', 'standardized', 'function', 'normal distribution', 'parameter', 'deviation', 'table', 'probability', 'normal', 'standard deviation', 'standard', 'distribution']", NORMINV Probability Mean Stdev,seg_95,"for the standardized normal distribution (table 4.2) (with mean 0 and standard deviation 1), you can use the function normsinv. this function only has one parameter: probability.",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0.])","tensor([13373,  2378,  2615,  9723,  2812,  2358, 24844])","tensor([  101,  2005,  1996, 16367,  3671,  4353,  1006,  2795,  1018,  1012,
         1016,  1007,  1006,  2007,  2812,  1014,  1998,  3115, 24353,  1015,
         1007,  1010,  2017,  2064,  2224,  1996,  3853, 17606,  2378,  2615,
         1012,  2023,  3853,  2069,  2038,  2028, 16381,  1024,  9723,  1012,
          102])"
402,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution']", Example,seg_97,"let us assume that the weight, x, of (the coffee in) a bag of coffee follows a normal distribution with a mean m ¼ 500 g and standard deviation s ¼ 5 g.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2292,  2149,  7868,  2008,  1996,  3635,  1010,  1060,  1010,
         1997,  1006,  1996,  4157,  1999,  1007,  1037,  4524,  1997,  4157,
         4076,  1037,  3671,  4353,  2007,  1037,  2812,  1049,  1091,  3156,
         1043,  1998,  3115, 24353,  1055,  1091,  1019,  1043,  1012,   102])"
403,1,"['probability', 'random']", Example,seg_97,"1. what is the probability that a random coffee bag weighs, at most, 490 g?",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])",tensor([2742]),"tensor([  101,  1015,  1012,  2054,  2003,  1996,  9723,  2008,  1037,  6721,
         4157,  4524, 21094,  1010,  2012,  2087,  1010, 22288,  1043,  1029,
          102])"
404,1,"['probability', 'random']", Example,seg_97,we use normdist(490; 500; 5; 1) and get the result 0.023 ¼ 2.3%. 2. what is the probability that a random coffee bag weighs at most 510 g?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2057,  2224, 13373, 10521,  2102,  1006, 22288,  1025,  3156,
         1025,  1019,  1025,  1015,  1007,  1998,  2131,  1996,  2765,  1014,
         1012,  6185,  2509,  1091,  1016,  1012,  1017,  1003,  1012,  1016,
         1012,  2054,  2003,  1996,  9723,  2008,  1037,  6721,  4157,  4524,
        21094,  2012,  2087, 23475,  1043,  1029,   102])"
405,1,"['probability', 'fractile', 'random', 'distribution']", Example,seg_97,"similarly, we find the probability that a random coffee bag weighs at most 510 g: we use normdist(510; 500; 5; 1) and get the result 0.977 ¼ 97.7%. 3. what is the 5% fractile in the distribution?",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 1., 0., 0.])",tensor([2742]),"tensor([  101,  6660,  1010,  2057,  2424,  1996,  9723,  2008,  1037,  6721,
         4157,  4524, 21094,  2012,  2087, 23475,  1043,  1024,  2057,  2224,
        13373, 10521,  2102,  1006, 23475,  1025,  3156,  1025,  1019,  1025,
         1015,  1007,  1998,  2131,  1996,  2765,  1014,  1012,  5989,  2581,
         1091,  5989,  1012,  1021,  1003,  1012,  1017,  1012,  2054,  2003,
         1996,  1019,  1003, 25312,  6593,  9463,  1999,  1996,  4353,  1029,
          102])"
406,1,"['probabilities', 'percentages']", Example,seg_97,"remember that 5% equals 0.05. in the spreadsheet, we do not use percentages when writing probabilities. we therefore use norminv(0.05; 500; 5) and get the result 491.8.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  3342,  2008,  1019,  1003, 19635,  1014,  1012,  5709,  1012,
         1999,  1996, 20861, 21030,  2102,  1010,  2057,  2079,  2025,  2224,
         7017,  2015,  2043,  3015,  4013,  3676, 14680,  1012,  2057,  3568,
         2224, 13373,  2378,  2615,  1006,  1014,  1012,  5709,  1025,  3156,
         1025,  1019,  1007,  1998,  2131,  1996,  2765,  4749,  2487,  1012,
         1022,  1012,   102])"
407,1,"['deviation', 'standard deviation', 'standard']", Example,seg_97,5 standard deviation 5,tensor(1),"tensor([0., 0., 1., 1., 0., 0.])",tensor([2742]),"tensor([  101,  1019,  3115, 24353,  1019,   102])"
408,1,['probability'], Example,seg_97,probability of weight £ x 7 probability,tensor(1),"tensor([0., 1., 0., 0., 0., 0., 0., 1., 0.])",tensor([2742]),"tensor([ 101, 9723, 1997, 3635, 1069, 1060, 1021, 9723,  102])"
409,1,"['fractile', 'normal', 'normal distribution', 'distribution']", Example,seg_97,in this normal distribution the 11 fractile,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.])",tensor([2742]),"tensor([  101,  1999,  2023,  3671,  4353,  1996,  2340, 25312,  6593,  9463,
          102])"
410,1,"['probability', 'fractile', 'random', 'distribution']", Example,seg_97,"this means that the probability that the weight of a random bag of coffee is 491.8 g is precisely 0.05, which is equivalent to 5%. in other words, there is a 95% chance that a random coffee bag weighs more than 491.8 g. 4. what is the 80% fractile in the distribution?",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,
        0.])",tensor([2742]),"tensor([  101,  2023,  2965,  2008,  1996,  9723,  2008,  1996,  3635,  1997,
         1037,  6721,  4524,  1997,  4157,  2003,  4749,  2487,  1012,  1022,
         1043,  2003, 10785,  1014,  1012,  5709,  1010,  2029,  2003,  5662,
         2000,  1019,  1003,  1012,  1999,  2060,  2616,  1010,  2045,  2003,
         1037,  5345,  1003,  3382,  2008,  1037,  6721,  4157,  4524, 21094,
         2062,  2084,  4749,  2487,  1012,  1022,  1043,  1012,  1018,  1012,
         2054,  2003,  1996,  3770,  1003, 25312,  6593,  9463,  1999,  1996,
         4353,  1029,   102])"
411,1,"['probability', 'random', 'fractile']", Example,seg_97,"similarly, we find the 80% fractile as norminv(0.80; 500; 5) and get the result 504.2. this means that the probability that the weight of a random bag of coffee is 504.2 g is precisely 0.80, which is equivalent to 80%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  6660,  1010,  2057,  2424,  1996,  3770,  1003, 25312,  6593,
         9463,  2004, 13373,  2378,  2615,  1006,  1014,  1012,  3770,  1025,
         3156,  1025,  1019,  1007,  1998,  2131,  1996,  2765,  2753,  2549,
         1012,  1016,  1012,  2023,  2965,  2008,  1996,  9723,  2008,  1996,
         3635,  1997,  1037,  6721,  4524,  1997,  4157,  2003,  2753,  2549,
         1012,  1016,  1043,  2003, 10785,  1014,  1012,  3770,  1010,  2029,
         2003,  5662,  2000,  3770,  1003,  1012,   102])"
412,1,['random'], Example,seg_97,"in other words, there is a 20% chance that a random coffee bag weighs more than 504.2 g.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1999,  2060,  2616,  1010,  2045,  2003,  1037,  2322,  1003,
         3382,  2008,  1037,  6721,  4157,  4524, 21094,  2062,  2084,  2753,
         2549,  1012,  1016,  1043,  1012,   102])"
413,0,[], Example,seg_97,the spreadsheet shows how to make the calculations in a spreadsheet (fig. 4.6).,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0])",tensor([2742]),"tensor([  101,  1996, 20861, 21030,  2102,  3065,  2129,  2000,  2191,  1996,
        16268,  1999,  1037, 20861, 21030,  2102,  1006, 20965,  1012,  1018,
         1012,  1020,  1007,  1012,   102])"
414,1,"['function', 'normal distribution', 'density function', 'normal', 'distribution', 'distribution function', 'data']", Testing for the Normal Distribution,seg_99,"we have studied some key characteristics of the normal distribution, its density function and distribution function, and calculations in the normal distribution. in other words, the assumption has been that the data actually are following a normal distribution. there are several ways to check this. this is the topic of this section and the following section.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([5604, 2005, 1996, 3671, 4353])","tensor([  101,  2057,  2031,  3273,  2070,  3145,  6459,  1997,  1996,  3671,
         4353,  1010,  2049,  4304,  3853,  1998,  4353,  3853,  1010,  1998,
        16268,  1999,  1996,  3671,  4353,  1012,  1999,  2060,  2616,  1010,
         1996, 11213,  2038,  2042,  2008,  1996,  2951,  2941,  2024,  2206,
         1037,  3671,  4353,  1012,  2045,  2024,  2195,  3971,  2000,  4638,
         2023,  1012,  2023,  2003,  1996,  8476,  1997,  2023,  2930,  1998,
         1996,  2206,  2930,  1012,   102])"
415,1,['histogram'], Simple Methods,seg_101,1. the histogram,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0.])","tensor([3722, 4725])","tensor([  101,  1015,  1012,  1996,  2010,  3406, 13113,   102])"
416,1,"['histogram', 'average', 'data', 'median']", Simple Methods,seg_101,"it is always a good idea to study the histogram. this must show a symmetrical, “bell-shaped” appearance. depending on the number of data values, the histogram can be more or less irregular; we discuss this later in this chapter. 2. the average ¼ the median",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0.])","tensor([3722, 4725])","tensor([  101,  2009,  2003,  2467,  1037,  2204,  2801,  2000,  2817,  1996,
         2010,  3406, 13113,  1012,  2023,  2442,  2265,  1037, 23476,  1010,
         1523,  4330,  1011,  5044,  1524,  3311,  1012,  5834,  2006,  1996,
         2193,  1997,  2951,  5300,  1010,  1996,  2010,  3406, 13113,  2064,
         2022,  2062,  2030,  2625, 12052,  1025,  2057,  6848,  2023,  2101,
         1999,  2023,  3127,  1012,  1016,  1012,  1996,  2779,  1091,  1996,
         3991,   102])"
417,1,"['normal distribution', 'range', 'deviation', 'interquartile range', 'standard deviation', 'normal', 'standard', 'distribution', 'average', 'data', 'median']", Simple Methods,seg_101,"if data can be described by a normal distribution, the average and median must be nearly identical, because the normal distribution is symmetrical. this is very simple to check. 3. interquartile range larger than the standard deviation",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.])","tensor([3722, 4725])","tensor([  101,  2065,  2951,  2064,  2022,  2649,  2011,  1037,  3671,  4353,
         1010,  1996,  2779,  1998,  3991,  2442,  2022,  3053,  7235,  1010,
         2138,  1996,  3671,  4353,  2003, 23476,  1012,  2023,  2003,  2200,
         3722,  2000,  4638,  1012,  1017,  1012,  6970, 16211, 28228,  2571,
         2846,  3469,  2084,  1996,  3115, 24353,   102])"
418,1,"['normal distribution', 'range', 'deviation', 'interquartile range', 'standard deviation', 'normal', 'standard', 'distribution', 'quartile']", Simple Methods,seg_101,"in the normal distribution, the interquartile range (i.e., the difference between the upper and lower quartile) is somewhat larger than the standard deviation; actually, it is around 1.35 the standard deviation, i.e.,",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([3722, 4725])","tensor([  101,  1999,  1996,  3671,  4353,  1010,  1996,  6970, 16211, 28228,
         2571,  2846,  1006,  1045,  1012,  1041,  1012,  1010,  1996,  4489,
         2090,  1996,  3356,  1998,  2896, 24209,  8445,  9463,  1007,  2003,
         5399,  3469,  2084,  1996,  3115, 24353,  1025,  2941,  1010,  2009,
         2003,  2105,  1015,  1012,  3486,  1996,  3115, 24353,  1010,  1045,
         1012,  1041,  1012,  1010,   102])"
419,1,"['table', 'symmetric', 'mean', 'normal distribution', 'fractile', 'interquartile range', 'quartiles', 'normal', 'standard deviation', 'standardized', 'data', 'deviation', 'standard', 'upper quartile', 'range', 'intervals', 'distribution', 'quartile']", Simple Methods,seg_101,"this can be explained by the standardized normal distribution with mean 0 and standard deviation 1: here, the upper quartile is 0.674 (see table in chap. 9, 75% fractile). because the normal distribution is symmetrical, the lower quartile is 0.674. the interquartile range, i.e., the distance between the quartiles, is therefore 2 0.674 ¼ approx. 1.35. by comparison, the standard deviation is precisely 1 in the standardized normal distribution. 4. number of data values in symmetric intervals around the mean",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1.,
        0.])","tensor([3722, 4725])","tensor([  101,  2023,  2064,  2022,  4541,  2011,  1996, 16367,  3671,  4353,
         2007,  2812,  1014,  1998,  3115, 24353,  1015,  1024,  2182,  1010,
         1996,  3356, 24209,  8445,  9463,  2003,  1014,  1012,  6163,  2549,
         1006,  2156,  2795,  1999, 15775,  2361,  1012,  1023,  1010,  4293,
         1003, 25312,  6593,  9463,  1007,  1012,  2138,  1996,  3671,  4353,
         2003, 23476,  1010,  1996,  2896, 24209,  8445,  9463,  2003,  1014,
         1012,  6163,  2549,  1012,  1996,  6970, 16211, 28228,  2571,  2846,
         1010,  1045,  1012,  1041,  1012,  1010,  1996,  3292,  2090,  1996,
        24209,  8445,  9463,  2015,  1010,  2003,  3568,  1016,  1014,  1012,
         6163,  2549,  1091, 22480,  1012,  1015,  1012,  3486,  1012,  2011,
         7831,  1010,  1996,  3115, 24353,  2003, 10785,  1015,  1999,  1996,
        16367,  3671,  4353,  1012,  1018,  1012,  2193,  1997,  2951,  5300,
         1999, 19490, 14025,  2105,  1996,  2812,   102])"
420,1,"['mean', 'deviation', 'normal distribution', 'interval', 'normal', 'standard deviation', 'standard', 'deviations', 'distribution', 'data', 'standard deviations']", Simple Methods,seg_101,"we have seen that around 68% of the data values in a normal distribution are in an interval around mean standard deviation. if the data can be described by a normal distribution, the corresponding proportion for the data values therefore must be relatively close to 68%. if we have many data values (at least a couple of hundred), one can calculate the proportion of data values between the mean 2 standard deviations. this proportion should be relatively close to 95%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([3722, 4725])","tensor([  101,  2057,  2031,  2464,  2008,  2105,  6273,  1003,  1997,  1996,
         2951,  5300,  1999,  1037,  3671,  4353,  2024,  1999,  2019, 13483,
         2105,  2812,  3115, 24353,  1012,  2065,  1996,  2951,  2064,  2022,
         2649,  2011,  1037,  3671,  4353,  1010,  1996,  7978, 10817,  2005,
         1996,  2951,  5300,  3568,  2442,  2022,  4659,  2485,  2000,  6273,
         1003,  1012,  2065,  2057,  2031,  2116,  2951,  5300,  1006,  2012,
         2560,  1037,  3232,  1997,  3634,  1007,  1010,  2028,  2064, 18422,
         1996, 10817,  1997,  2951,  5300,  2090,  1996,  2812,  1016,  3115,
        24353,  2015,  1012,  2023, 10817,  2323,  2022,  4659,  2485,  2000,
         5345,  1003,  1012,   102])"
421,1,"['deviation', 'histogram', 'sample']", Simple Methods,seg_101,"example a histogram of the height of all 30 kids from the fitness club survey is shown below (see also chap. 2). this histogram seems roughly symmetrical. when the sample is small, we must accept some deviation from the ideal appearance (fig. 4.7).",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3722, 4725])","tensor([  101,  2742,  1037,  2010,  3406, 13113,  1997,  1996,  4578,  1997,
         2035,  2382,  4268,  2013,  1996, 10516,  2252,  5002,  2003,  3491,
         2917,  1006,  2156,  2036, 15775,  2361,  1012,  1016,  1007,  1012,
         2023,  2010,  3406, 13113,  3849,  5560, 23476,  1012,  2043,  1996,
         7099,  2003,  2235,  1010,  2057,  2442,  5138,  2070, 24353,  2013,
         1996,  7812,  3311,  1006, 20965,  1012,  1018,  1012,  1021,  1007,
         1012,   102])"
422,1,"['statistics', 'table']", Simple Methods,seg_101,"in chap. 3, we found a number of statistics for the height of the 30 kids. the most important statistics are shown in table 4.3 with one decimal:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([3722, 4725])","tensor([  101,  1999, 15775,  2361,  1012,  1017,  1010,  2057,  2179,  1037,
         2193,  1997,  6747,  2005,  1996,  4578,  1997,  1996,  2382,  4268,
         1012,  1996,  2087,  2590,  6747,  2024,  3491,  1999,  2795,  1018,
         1012,  1017,  2007,  2028, 26066,  1024,   102])"
423,1,"['range', 'deviation', 'interquartile range', 'factor', 'standard deviation', 'standard', 'average', 'median']", Simple Methods,seg_101,"we see that the average and the median are roughly equal. the interquartile range is slightly larger than the standard deviation, though not by a factor of 1.35.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0.])","tensor([3722, 4725])","tensor([  101,  2057,  2156,  2008,  1996,  2779,  1998,  1996,  3991,  2024,
         5560,  5020,  1012,  1996,  6970, 16211, 28228,  2571,  2846,  2003,
         3621,  3469,  2084,  1996,  3115, 24353,  1010,  2295,  2025,  2011,
         1037,  5387,  1997,  1015,  1012,  3486,  1012,   102])"
424,1,"['mean', 'deviation', 'interval', 'standard deviation', 'standard']", Simple Methods,seg_101,the interval mean standard deviation corresponds to the interval from 135.0 to 179.2.,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([3722, 4725])","tensor([  101,  1996, 13483,  2812,  3115, 24353, 14788,  2000,  1996, 13483,
         2013, 11502,  1012,  1014,  2000, 20311,  1012,  1016,  1012,   102])"
425,1,"['interval', 'data']", Simple Methods,seg_101,"in this interval, you can count 21 of the 30 data values, equivalent to 70%, i.e., very close to 68%.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3722, 4725])","tensor([  101,  1999,  2023, 13483,  1010,  2017,  2064,  4175,  2538,  1997,
         1996,  2382,  2951,  5300,  1010,  5662,  2000,  3963,  1003,  1010,
         1045,  1012,  1041,  1012,  1010,  2200,  2485,  2000,  6273,  1003,
         1012,   102])"
426,1,"['normal', 'normal distribution', 'data', 'distribution']", Simple Methods,seg_101,"overall, we conclude that it appears reasonable that data can be described by a normal distribution.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0.])","tensor([3722, 4725])","tensor([  101,  3452,  1010,  2057, 16519,  2008,  2009,  3544,  9608,  2008,
         2951,  2064,  2022,  2649,  2011,  1037,  3671,  4353,  1012,   102])"
427,1,"['normal distributions', 'distributions', 'normal']", Simple Methods,seg_101,"another issue is whether we should in fact use two normal distributions, one for each sex! we return to this issue in chap. 8.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3722, 4725])","tensor([  101,  2178,  3277,  2003,  3251,  2057,  2323,  1999,  2755,  2224,
         2048,  3671, 20611,  1010,  2028,  2005,  2169,  3348,   999,  2057,
         2709,  2000,  2023,  3277,  1999, 15775,  2361,  1012,  1022,  1012,
          102])"
428,1,"['normal distribution', 'data', 'skewness', 'normal', 'statistics', 'distribution', 'statistical', 'kurtosis']", Skewness and Kurtosis,seg_103,"the two statistics skewness and kurtosis can be used to check whether data follow a normal distribution. they are, however, complicated to calculate and therefore require the use of a spreadsheet or other statistical software.",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  1996,  2048,  6747, 15315,  7974,  2791,  1998,  9679, 12650,
         2064,  2022,  2109,  2000,  4638,  3251,  2951,  3582,  1037,  3671,
         4353,  1012,  2027,  2024,  1010,  2174,  1010,  8552,  2000, 18422,
         1998,  3568,  5478,  1996,  2224,  1997,  1037, 20861, 21030,  2102,
         2030,  2060,  7778,  4007,  1012,   102])"
429,1,"['normal distribution', 'normal', 'statistics', 'distribution', 'data']", Skewness and Kurtosis,seg_103,these two statistics can be easily calculated in most spreadsheets or other statistical software. they provide an easy opportunity to check whether the data can be described by a normal distribution.,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  2122,  2048,  6747,  2064,  2022,  4089, 10174,  1999,  2087,
        20861, 21030,  3215,  2030,  2060,  7778,  4007,  1012,  2027,  3073,
         2019,  3733,  4495,  2000,  4638,  3251,  1996,  2951,  2064,  2022,
         2649,  2011,  1037,  3671,  4353,  1012,   102])"
430,1,"['skewed', 'distribution']", Skewness and Kurtosis,seg_103,skewness (*) is a measure of how skewed the distribution is compared to a symmetrical distribution:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101, 15315,  7974,  2791,  1006,  1008,  1007,  2003,  1037,  5468,
         1997,  2129, 15315,  7974,  2098,  1996,  4353,  2003,  4102,  2000,
         1037, 23476,  4353,  1024,   102])"
431,1,"['skewness', 'distribution', 'data']", Skewness and Kurtosis,seg_103,"– if data can be described by a symmetrical distribution, the skewness must be close to 0. – positive skewness indicates a right-skewed distribution. – negative skewness indicates a left-skewed distribution.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  1516,  2065,  2951,  2064,  2022,  2649,  2011,  1037, 23476,
         4353,  1010,  1996, 15315,  7974,  2791,  2442,  2022,  2485,  2000,
         1014,  1012,  1516,  3893, 15315,  7974,  2791,  7127,  1037,  2157,
         1011, 15315,  7974,  2098,  4353,  1012,  1516,  4997, 15315,  7974,
         2791,  7127,  1037,  2187,  1011, 15315,  7974,  2098,  4353,  1012,
          102])"
432,1,"['skewness', 'sample', 'deviations']", Skewness and Kurtosis,seg_103,"as a very rough guide as to how large deviations from 0 can be accepted for the skewness for different sample sizes n, you can use the expression:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  2004,  1037,  2200,  5931,  5009,  2004,  2000,  2129,  2312,
        24353,  2015,  2013,  1014,  2064,  2022,  3970,  2005,  1996, 15315,
         7974,  2791,  2005,  2367,  7099, 10826,  1050,  1010,  2017,  2064,
         2224,  1996,  3670,  1024,   102])"
433,1,"['sample', 'sample size']", Skewness and Kurtosis,seg_103,where n is the sample size.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([ 101, 2073, 1050, 2003, 1996, 7099, 2946, 1012,  102])"
434,1,"['deviation', 'table', 'sample', 'sample size', 'deviations']", Skewness and Kurtosis,seg_103,"this gives table 4.4. the smaller sample, the greater deviations from 0 you have to accept. when the sample size is multiplied by 4, the maximum acceptable deviation from 0 is divided by 2.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  2023,  3957,  2795,  1018,  1012,  1018,  1012,  1996,  3760,
         7099,  1010,  1996,  3618, 24353,  2015,  2013,  1014,  2017,  2031,
         2000,  5138,  1012,  2043,  1996,  7099,  2946,  2003, 28608,  2011,
         1018,  1010,  1996,  4555, 11701, 24353,  2013,  1014,  2003,  4055,
         2011,  1016,  1012,   102])"
435,1,"['deviation', 'skewness', 'distribution']", Skewness and Kurtosis,seg_103,"so, you check whether the skewness is within the maximum acceptable deviation from 0. if not, we do not have a symmetrical distribution.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  2061,  1010,  2017,  4638,  3251,  1996, 15315,  7974,  2791,
         2003,  2306,  1996,  4555, 11701, 24353,  2013,  1014,  1012,  2065,
         2025,  1010,  2057,  2079,  2025,  2031,  1037, 23476,  4353,  1012,
          102])"
436,1,"['evaluating', 'statistic', 'distribution']", Skewness and Kurtosis,seg_103,"if the distribution is symmetrical, you can supplement with evaluating another statistic:",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  2065,  1996,  4353,  2003, 23476,  1010,  2017,  2064, 12448,
         2007, 23208,  2178, 28093,  6553,  1024,   102])"
437,1,['distribution'], Skewness and Kurtosis,seg_103,kurtosis (*) indicates how big “tails” the distribution has:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  9679, 12650,  1006,  1008,  1007,  7127,  2129,  2502,  1523,
        17448,  1524,  1996,  4353,  2038,  1024,   102])"
438,1,"['kurtosis', 'normal', 'normal distribution', 'distribution']", Skewness and Kurtosis,seg_103,– a normal distribution has kurtosis 0. – a positive kurtosis indicates larger “tails” than in the normal distribution. – a negative kurtosis indicates smaller “tails” than in the normal distribution.,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  1516,  1037,  3671,  4353,  2038,  9679, 12650,  1014,  1012,
         1516,  1037,  3893,  9679, 12650,  7127,  3469,  1523, 17448,  1524,
         2084,  1999,  1996,  3671,  4353,  1012,  1516,  1037,  4997,  9679,
        12650,  7127,  3760,  1523, 17448,  1524,  2084,  1999,  1996,  3671,
         4353,  1012,   102])"
439,1,"['kurtosis', 'normal', 'normal distribution', 'distribution']", Skewness and Kurtosis,seg_103,a distribution with positive kurtosis is often more “steep” in the top than the normal distribution.,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  1037,  4353,  2007,  3893,  9679, 12650,  2003,  2411,  2062,
         1523,  9561,  1524,  1999,  1996,  2327,  2084,  1996,  3671,  4353,
         1012,   102])"
440,1,"['kurtosis', 'normal', 'normal distribution', 'distribution']", Skewness and Kurtosis,seg_103,"conversely, a distribution with negative kurtosis is often more “flat” in the top than the normal distribution.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101, 18868,  1010,  1037,  4353,  2007,  4997,  9679, 12650,  2003,
         2411,  2062,  1523,  4257,  1524,  1999,  1996,  2327,  2084,  1996,
         3671,  4353,  1012,   102])"
441,1,"['normal distribution', 'normal', 'distribution', 'kurtosis']", Skewness and Kurtosis,seg_103,"however, these properties are not always true. for example, the t-distribution (*) has a positive kurtosis. nevertheless, the t-distribution is more “flat” in the top than the normal distribution. see examples of the t-distribution later in this chapter.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  2174,  1010,  2122,  5144,  2024,  2025,  2467,  2995,  1012,
         2005,  2742,  1010,  1996,  1056,  1011,  4353,  1006,  1008,  1007,
         2038,  1037,  3893,  9679, 12650,  1012,  6600,  1010,  1996,  1056,
         1011,  4353,  2003,  2062,  1523,  4257,  1524,  1999,  1996,  2327,
         2084,  1996,  3671,  4353,  1012,  2156,  4973,  1997,  1996,  1056,
         1011,  4353,  2101,  1999,  2023,  3127,  1012,   102])"
442,1,"['normal distribution', 'table', 'skewness', 'sample', 'normal', 'deviations', 'distribution', 'kurtosis']", Skewness and Kurtosis,seg_103,"we may accept larger deviations from 0 for the kurtosis than for the skewness. for small sample sizes, table 4.5 shows the minimum and maximum kurtosis that can be accepted if the distribution can be described by a normal distribution.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  2057,  2089,  5138,  3469, 24353,  2015,  2013,  1014,  2005,
         1996,  9679, 12650,  2084,  2005,  1996, 15315,  7974,  2791,  1012,
         2005,  2235,  7099, 10826,  1010,  2795,  1018,  1012,  1019,  3065,
         1996,  6263,  1998,  4555,  9679, 12650,  2008,  2064,  2022,  3970,
         2065,  1996,  4353,  2064,  2022,  2649,  2011,  1037,  3671,  4353,
         1012,   102])"
443,1,"['normal distribution', 'range', 'sample', 'normal', 'sample size', 'distribution', 'kurtosis', 'data']", Skewness and Kurtosis,seg_103,"if kurtosis for a given sample size is outside the range above, data cannot be described by a normal distribution.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  2065,  9679, 12650,  2005,  1037,  2445,  7099,  2946,  2003,
         2648,  1996,  2846,  2682,  1010,  2951,  3685,  2022,  2649,  2011,
         1037,  3671,  4353,  1012,   102])"
444,1,"['normal distribution', 'sample', 'normal', 'distribution', 'kurtosis']", Skewness and Kurtosis,seg_103,"assume, for example, that a sample of size n ¼ 100 has kurtosis >1.1. this is a sign of a distribution with larger “tails” than the normal distribution.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  7868,  1010,  2005,  2742,  1010,  2008,  1037,  7099,  1997,
         2946,  1050,  1091,  2531,  2038,  9679, 12650,  1028,  1015,  1012,
         1015,  1012,  2023,  2003,  1037,  3696,  1997,  1037,  4353,  2007,
         3469,  1523, 17448,  1524,  2084,  1996,  3671,  4353,  1012,   102])"
445,1,"['interval', 'sample', 'kurtosis']", Skewness and Kurtosis,seg_103,"notice that for small sample sizes, the acceptable interval of kurtosis is not symmetrical.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,
        0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  5060,  2008,  2005,  2235,  7099, 10826,  1010,  1996, 11701,
        13483,  1997,  9679, 12650,  2003,  2025, 23476,  1012,   102])"
446,1,"['deviation', 'skewness', 'deviations', 'samples', 'kurtosis']", Skewness and Kurtosis,seg_103,"for large samples sizes (about 1,000 or more), we can accept twice as large deviations from 0 for kurtosis as for the skewness, i.e., the maximum deviation from 0 for the kurtosis is",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  2005,  2312,  8168, 10826,  1006,  2055,  1015,  1010,  2199,
         2030,  2062,  1007,  1010,  2057,  2064,  5138,  3807,  2004,  2312,
        24353,  2015,  2013,  1014,  2005,  9679, 12650,  2004,  2005,  1996,
        15315,  7974,  2791,  1010,  1045,  1012,  1041,  1012,  1010,  1996,
         4555, 24353,  2013,  1014,  2005,  1996,  9679, 12650,  2003,   102])"
447,1,"['control', 'quality control', 'statistics', 'statistical']", Skewness and Kurtosis,seg_103,"for calculation formula on these statistics, we refer to the help in your spreadsheet (or other statistical software) or to textbooks, for example, montgomery: introduction to statistical quality control (2005).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  2005, 17208,  5675,  2006,  2122,  6747,  1010,  2057,  6523,
         2000,  1996,  2393,  1999,  2115, 20861, 21030,  2102,  1006,  2030,
         2060,  7778,  4007,  1007,  2030,  2000, 18841,  1010,  2005,  2742,
         1010,  8482,  1024,  4955,  2000,  7778,  3737,  2491,  1006,  2384,
         1007,  1012,   102])"
448,1,"['kurtosis', 'normal', 'normal distribution', 'distribution']", Skewness and Kurtosis,seg_103,"note: there is an alternative formula for calculating kurtosis, where a normal distribution has the value 3.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  3602,  1024,  2045,  2003,  2019,  4522,  5675,  2005, 20177,
         9679, 12650,  1010,  2073,  1037,  3671,  4353,  2038,  1996,  3643,
         1017,  1012,   102])"
449,1,"['normal distribution', 'normal', 'distribution', 'statistical', 'kurtosis']", Skewness and Kurtosis,seg_103,"in spreadsheets such as microsoft excel and open office calc, and in most statistical software, the kurtosis of a normal distribution is 0.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  1999, 20861, 21030,  3215,  2107,  2004,  7513, 24970,  1998,
         2330,  2436, 10250,  2278,  1010,  1998,  1999,  2087,  7778,  4007,
         1010,  1996,  9679, 12650,  1997,  1037,  3671,  4353,  2003,  1014,
         1012,   102])"
450,1,"['functions', 'skewness', 'kurtosis']", Skewness and Kurtosis,seg_103,both the skewness and kurtosis exist as functions in spreadsheets:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  2119,  1996, 15315,  7974,  2791,  1998,  9679, 12650,  4839,
         2004,  4972,  1999, 20861, 21030,  3215,  1024,   102])"
451,1,"['function', 'skew', 'skewness', 'kurtosis']", Skewness and Kurtosis,seg_103,– skewness can be obtained using the function skew. – kurtosis can be obtained using the function kurt.,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  1516, 15315,  7974,  2791,  2064,  2022,  4663,  2478,  1996,
         3853, 15315,  7974,  1012,  1516,  9679, 12650,  2064,  2022,  4663,
         2478,  1996,  3853,  9679,  1012,   102])"
452,1,"['table', 'functions', 'skewness', 'statistics', 'kurtosis']", Skewness and Kurtosis,seg_103,"in chap. 3, we found using spreadsheet functions a number of statistics for the height of the 30 kids from the fitness club survey, including skewness and kurtosis, which are reproduced here (table 4.6).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  1999, 15775,  2361,  1012,  1017,  1010,  2057,  2179,  2478,
        20861, 21030,  2102,  4972,  1037,  2193,  1997,  6747,  2005,  1996,
         4578,  1997,  1996,  2382,  4268,  2013,  1996, 10516,  2252,  5002,
         1010,  2164, 15315,  7974,  2791,  1998,  9679, 12650,  1010,  2029,
         2024, 22296,  2182,  1006,  2795,  1018,  1012,  1020,  1007,  1012,
          102])"
453,1,"['sample', 'skewness']", Skewness and Kurtosis,seg_103,"the skewness is close to 0. for a relatively small sample like this, we can accept values up to nearly 1.",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  1996, 15315,  7974,  2791,  2003,  2485,  2000,  1014,  1012,
         2005,  1037,  4659,  2235,  7099,  2066,  2023,  1010,  2057,  2064,
         5138,  5300,  2039,  2000,  3053,  1015,  1012,   102])"
454,1,"['normal distribution', 'normal', 'distribution', 'kurtosis', 'data']", Skewness and Kurtosis,seg_103,"therefore, data can be described by a symmetrical distribution. kurtosis is very close to 0. this confirms that data can be described by a normal distribution.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  3568,  1010,  2951,  2064,  2022,  2649,  2011,  1037, 23476,
         4353,  1012,  9679, 12650,  2003,  2200,  2485,  2000,  1014,  1012,
         2023, 23283,  2008,  2951,  2064,  2022,  2649,  2011,  1037,  3671,
         4353,  1012,   102])"
455,1,"['statistical tests', 'normal distribution', 'tests', 'normal', 'distribution', 'statistical']", Skewness and Kurtosis,seg_103,"there exist various statistical tests for the normal distribution, which are available in statistical software packages. these tests also confirm that the distribution of the height of the 30 kids can be described by a normal distribution.wewill not cover such tests in this book; see the help menu in your statistical software package.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([15315,  7974,  2791,  1998,  9679, 12650])","tensor([  101,  2045,  4839,  2536,  7778,  5852,  2005,  1996,  3671,  4353,
         1010,  2029,  2024,  2800,  1999,  7778,  4007, 14555,  1012,  2122,
         5852,  2036, 12210,  2008,  1996,  4353,  1997,  1996,  4578,  1997,
         1996,  2382,  4268,  2064,  2022,  2649,  2011,  1037,  3671,  4353,
         1012,  2057, 29602,  2140,  2025,  3104,  2107,  5852,  1999,  2023,
         2338,  1025,  2156,  1996,  2393, 12183,  1999,  2115,  7778,  4007,
         7427,  1012,   102])"
456,1,"['quantile', 'normal distribution', 'graphical', 'data', 'probability', 'normal', 'quantile plot', 'distribution', 'statistical', 'plot', 'probability plot']", Normal Plot,seg_105,"finally, there exists a simple graphical tool to check if your data follow a normal distribution. this is called a normal plot (probability plot or quantile plot). it is a built-in feature of many statistical software packages, e.g., sas, jmp, spss, minitab, etc., see a list in chap. 9.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([3671, 5436])","tensor([  101,  2633,  1010,  2045,  6526,  1037,  3722, 20477,  6994,  2000,
         4638,  2065,  2115,  2951,  3582,  1037,  3671,  4353,  1012,  2023,
         2003,  2170,  1037,  3671,  5436,  1006,  9723,  5436,  2030, 24110,
        15286,  5436,  1007,  1012,  2009,  2003,  1037,  2328,  1011,  1999,
         3444,  1997,  2116,  7778,  4007, 14555,  1010,  1041,  1012,  1043,
         1012,  1010, 21871,  1010,  1046,  8737,  1010, 11867,  4757,  1010,
         7163,  2696,  2497,  1010,  4385,  1012,  1010,  2156,  1037,  2862,
         1999, 15775,  2361,  1012,  1023,  1012,   102])"
457,1,"['statistical', 'plot']", Normal Plot,seg_105,"if you have a statistical software package, you can produce a plot like the one shown above. it is also quite easy to do in a spreadsheet.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3671, 5436])","tensor([  101,  2065,  2017,  2031,  1037,  7778,  4007,  7427,  1010,  2017,
         2064,  3965,  1037,  5436,  2066,  1996,  2028,  3491,  2682,  1012,
         2009,  2003,  2036,  3243,  3733,  2000,  2079,  1999,  1037, 20861,
        21030,  2102,  1012,   102])"
458,1,"['normal distribution', 'normal', 'distribution', 'data', 'case']", Normal Plot,seg_105,"if data follow a normal distribution, the points should be randomly scattered around the straight line. this seems to be the case here. this confirms that data can be described by a normal distribution (fig. 4.8).",tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3671, 5436])","tensor([  101,  2065,  2951,  3582,  1037,  3671,  4353,  1010,  1996,  2685,
         2323,  2022, 18154,  7932,  2105,  1996,  3442,  2240,  1012,  2023,
         3849,  2000,  2022,  1996,  2553,  2182,  1012,  2023, 23283,  2008,
         2951,  2064,  2022,  2649,  2011,  1037,  3671,  4353,  1006, 20965,
         1012,  1018,  1012,  1022,  1007,  1012,   102])"
459,1,"['method', 'statistical', 'plot']", Normal Plot,seg_105,"if you do not have a statistical software package, you can construct the plot in a spreadsheet. the method is outlined in the text box.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3671, 5436])","tensor([  101,  2065,  2017,  2079,  2025,  2031,  1037,  7778,  4007,  7427,
         1010,  2017,  2064,  9570,  1996,  5436,  1999,  1037, 20861, 21030,
         2102,  1012,  1996,  4118,  2003, 14801,  1999,  1996,  3793,  3482,
         1012,   102])"
460,1,"['plot', 'normal', 'data']", Normal Plot,seg_105,"technical note: construction of the normal plot in spreadsheets. first, you sort the data values in ascending order. here, we have used the height of the 30 kids from the fitness club survey; below we show only the two smallest data values, which are 112, respectively, 115 (fig. 4.9): (continued)",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3671, 5436])","tensor([  101,  4087,  3602,  1024,  2810,  1997,  1996,  3671,  5436,  1999,
        20861, 21030,  3215,  1012,  2034,  1010,  2017,  4066,  1996,  2951,
         5300,  1999, 22316,  2344,  1012,  2182,  1010,  2057,  2031,  2109,
         1996,  4578,  1997,  1996,  2382,  4268,  2013,  1996, 10516,  2252,
         5002,  1025,  2917,  2057,  2265,  2069,  1996,  2048, 10479,  2951,
         5300,  1010,  2029,  2024, 11176,  1010,  4414,  1010, 10630,  1006,
        20965,  1012,  1018,  1012,  1023,  1007,  1024,  1006,  2506,  1007,
          102])"
461,1,"['curve', 'regression', 'transformed', 'normal distribution', 'regression line', 'fractile', 'normal', 'scatter plot', 'plot', 'data', 'function', 'standard normal distribution', 'standard', 'standard normal', 'distribution', 'distribution function']", Normal Plot,seg_105,"you make a column with consecutive numbers: this is column a. data values are found in column b. column c is used to calculate the expression (i 0.5)/n, where i is the number of the data value (in column a) and n is the total number of values (here 30). apart from 0.5 in the numerator (a technical correction), i/n is exactly the proportion of data values up to and including data value no. i. for the first data value, we get the result (1 0.5)/30 ¼ 0.017. a scatter plot with column c as y and column b as x can be compared with the distribution function of a normal distribution. however, it is difficult to check if the points follow a normal distribution curve. therefore, the y values in column c are transformed. for each value, we find the corresponding fractile in the standard normal distribution using the spreadsheet function normsinv, giving 2.13 for the first data value. this value is written in column d. this corresponds to “twisting” the y-axis so that the curve becomes a straight line. a plot of column d as y and column b as x is shown above. here, we have added a regression line (see chap. 7).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([3671, 5436])","tensor([  101,  2017,  2191,  1037,  5930,  2007,  5486,  3616,  1024,  2023,
         2003,  5930,  1037,  1012,  2951,  5300,  2024,  2179,  1999,  5930,
         1038,  1012,  5930,  1039,  2003,  2109,  2000, 18422,  1996,  3670,
         1006,  1045,  1014,  1012,  1019,  1007,  1013,  1050,  1010,  2073,
         1045,  2003,  1996,  2193,  1997,  1996,  2951,  3643,  1006,  1999,
         5930,  1037,  1007,  1998,  1050,  2003,  1996,  2561,  2193,  1997,
         5300,  1006,  2182,  2382,  1007,  1012,  4237,  2013,  1014,  1012,
         1019,  1999,  1996, 16371,  5017,  8844,  1006,  1037,  4087, 18140,
         1007,  1010,  1045,  1013,  1050,  2003,  3599,  1996, 10817,  1997,
         2951,  5300,  2039,  2000,  1998,  2164,  2951,  3643,  2053,  1012,
         1045,  1012,  2005,  1996,  2034,  2951,  3643,  1010,  2057,  2131,
         1996,  2765,  1006,  1015,  1014,  1012,  1019,  1007,  1013,  2382,
         1091,  1014,  1012,  5890,  2581,  1012,  1037,  8040, 20097,  5436,
         2007,  5930,  1039,  2004,  1061,  1998,  5930,  1038,  2004,  1060,
         2064,  2022,  4102,  2007,  1996,  4353,  3853,  1997,  1037,  3671,
         4353,  1012,  2174,  1010,  2009,  2003,  3697,  2000,  4638,  2065,
         1996,  2685,  3582,  1037,  3671,  4353,  7774,  1012,  3568,  1010,
         1996,  1061,  5300,  1999,  5930,  1039,  2024,  8590,  1012,  2005,
         2169,  3643,  1010,  2057,  2424,  1996,  7978, 25312,  6593,  9463,
         1999,  1996,  3115,  3671,  4353,  2478,  1996, 20861, 21030,  2102,
         3853, 17606,  2378,  2615,  1010,  3228,  1016,  1012,  2410,  2005,
         1996,  2034,  2951,  3643,  1012,  2023,  3643,  2003,  2517,  1999,
         5930,  1040,  1012,  2023, 14788,  2000,  1523, 12814,  1524,  1996,
         1061,  1011,  8123,  2061,  2008,  1996,  7774,  4150,  1037,  3442,
         2240,  1012,  1037,  5436,  1997,  5930,  1040,  2004,  1061,  1998,
         5930,  1038,  2004,  1060,  2003,  3491,  2682,  1012,  2182,  1010,
         2057,  2031,  2794,  1037, 26237,  2240,  1006,  2156, 15775,  2361,
         1012,  1021,  1007,  1012,   102])"
462,1,"['data set', 'evaluating', 'normal distribution', 'set', 'random numbers', 'random', 'normal', 'distribution', 'charts', 'data']", Random Numbers,seg_107,"in evaluating how well a data set is consistent with the normal distribution, it can be a good benchmark to do the same calculations and charts for a similar number of random numbers from the normal distribution.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0.])","tensor([6721, 3616])","tensor([  101,  1999, 23208,  2129,  2092,  1037,  2951,  2275,  2003,  8335,
         2007,  1996,  3671,  4353,  1010,  2009,  2064,  2022,  1037,  2204,
         6847, 10665,  2000,  2079,  1996,  2168, 16268,  1998,  6093,  2005,
         1037,  2714,  2193,  1997,  6721,  3616,  2013,  1996,  3671,  4353,
         1012,   102])"
463,1,"['random numbers', 'random number', 'random', 'normal', 'data']", Random Numbers,seg_107,"in microsoft excel, you can construct random numbers from a normal distribution using the add-in menu data analysis, which has a sub-item random number generation. a similar option does not exist in open office calc.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6721, 3616])","tensor([  101,  1999,  7513, 24970,  1010,  2017,  2064,  9570,  6721,  3616,
         2013,  1037,  3671,  4353,  2478,  1996,  5587,  1011,  1999, 12183,
         2951,  4106,  1010,  2029,  2038,  1037,  4942,  1011,  8875,  6721,
         2193,  4245,  1012,  1037,  2714,  5724,  2515,  2025,  4839,  1999,
         2330,  2436, 10250,  2278,  1012,   102])"
464,1,"['mean', 'standardized', 'deviation', 'normal distribution', 'random numbers', 'histogram', 'random', 'normal', 'standard deviation', 'standard', 'distribution', 'data', 'case']", Random Numbers,seg_107,"in this way, it is simple to construct random numbers from a normal distribution. in comparison with the histogram of the height of the 30 kids from the fitness club survey (see chap. 2), we show a histogram based on 30 random data from a normal distribution (in this case with mean 0 and standard deviation 1, i.e., a standardized normal distribution).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.])","tensor([6721, 3616])","tensor([  101,  1999,  2023,  2126,  1010,  2009,  2003,  3722,  2000,  9570,
         6721,  3616,  2013,  1037,  3671,  4353,  1012,  1999,  7831,  2007,
         1996,  2010,  3406, 13113,  1997,  1996,  4578,  1997,  1996,  2382,
         4268,  2013,  1996, 10516,  2252,  5002,  1006,  2156, 15775,  2361,
         1012,  1016,  1007,  1010,  2057,  2265,  1037,  2010,  3406, 13113,
         2241,  2006,  2382,  6721,  2951,  2013,  1037,  3671,  4353,  1006,
         1999,  2023,  2553,  2007,  2812,  1014,  1998,  3115, 24353,  1015,
         1010,  1045,  1012,  1041,  1012,  1010,  1037, 16367,  3671,  4353,
         1007,  1012,   102])"
465,1,"['normal distribution', 'histogram', 'sample', 'normal', 'sample size', 'distribution']", Random Numbers,seg_107,"we know that these numbers come from a normal distribution. nevertheless, we can see that there are some irregularities in the histogram, which are due to the limited sample size (fig. 4.10).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6721, 3616])","tensor([  101,  2057,  2113,  2008,  2122,  3616,  2272,  2013,  1037,  3671,
         4353,  1012,  6600,  1010,  2057,  2064,  2156,  2008,  2045,  2024,
         2070, 28868,  1999,  1996,  2010,  3406, 13113,  1010,  2029,  2024,
         2349,  2000,  1996,  3132,  7099,  2946,  1006, 20965,  1012,  1018,
         1012,  2184,  1007,  1012,   102])"
466,1,"['normal distribution', 'random', 'normal', 'distribution', 'kurtosis', 'data']", Random Numbers,seg_107,"in fact, it is quite easy to create many columns with random data from a normal distribution. in this way, the author calculated the recommended limits for kurtosis",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.])","tensor([6721, 3616])","tensor([  101,  1999,  2755,  1010,  2009,  2003,  3243,  3733,  2000,  3443,
         2116,  7753,  2007,  6721,  2951,  2013,  1037,  3671,  4353,  1012,
         1999,  2023,  2126,  1010,  1996,  3166, 10174,  1996,  6749,  6537,
         2005,  9679, 12650,   102])"
467,1,"['statistical', 'table', 'data']", Random Numbers,seg_107,"in the table above; however, other statistical software has been used, which is more suitable for very large amounts of data.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([6721, 3616])","tensor([ 101, 1999, 1996, 2795, 2682, 1025, 2174, 1010, 2060, 7778, 4007, 2038,
        2042, 2109, 1010, 2029, 2003, 2062, 7218, 2005, 2200, 2312, 8310, 1997,
        2951, 1012,  102])"
468,1,"['simulation', 'random numbers', 'histogram', 'sample', 'sample size', 'random', 'statistical', 'charts']", Random Numbers,seg_107,doing statistical calculations with random numbers is called simulation. you can also use simulation to study how the histogram gradually changes appearance when the sample size increases. see the charts (fig. 4.11).,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6721, 3616])","tensor([  101,  2725,  7778, 16268,  2007,  6721,  3616,  2003,  2170, 12504,
         1012,  2017,  2064,  2036,  2224, 12504,  2000,  2817,  2129,  1996,
         2010,  3406, 13113,  6360,  3431,  3311,  2043,  1996,  7099,  2946,
         7457,  1012,  2156,  1996,  6093,  1006, 20965,  1012,  1018,  1012,
         2340,  1007,  1012,   102])"
469,1,"['curve', 'normal distribution', 'histogram', 'sample', 'normal', 'sample size', 'distribution']", Random Numbers,seg_107,"it is evident that in a sample size of 30, you have to accept some irregularities in the histogram. when the sample size increases to, for example, 1,000, the histogram looks very similar to a normal distribution curve.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([6721, 3616])","tensor([  101,  2009,  2003, 10358,  2008,  1999,  1037,  7099,  2946,  1997,
         2382,  1010,  2017,  2031,  2000,  5138,  2070, 28868,  1999,  1996,
         2010,  3406, 13113,  1012,  2043,  1996,  7099,  2946,  7457,  2000,
         1010,  2005,  2742,  1010,  1015,  1010,  2199,  1010,  1996,  2010,
         3406, 13113,  3504,  2200,  2714,  2000,  1037,  3671,  4353,  7774,
         1012,   102])"
470,1,"['histograms', 'samples']", Random Numbers,seg_107,"in these histograms, we have used the same number of bars for direct comparison. in practice, you will use more bars for large samples sizes, see chap. 2.",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([6721, 3616])","tensor([  101,  1999,  2122,  2010,  3406, 13113,  2015,  1010,  2057,  2031,
         2109,  1996,  2168,  2193,  1997,  6963,  2005,  3622,  7831,  1012,
         1999,  3218,  1010,  2017,  2097,  2224,  2062,  6963,  2005,  2312,
         8168, 10826,  1010,  2156, 15775,  2361,  1012,  1016,  1012,   102])"
471,1,"['normal distribution', 'uncertainty', 'associated', 'statistical uncertainty', 'control', 'sample', 'normal', 'distribution', 'statistical', 'average']", Confidence Intervals,seg_109,"after studying the main characteristics of the normal distribution and how to control for the normal distribution, we now look at one of the main applications of the normal distribution: how to find the statistical uncertainty (*) associated with the average of a sample.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  2044,  5702,  1996,  2364,  6459,  1997,  1996,  3671,  4353,
         1998,  2129,  2000,  2491,  2005,  1996,  3671,  4353,  1010,  2057,
         2085,  2298,  2012,  2028,  1997,  1996,  2364,  5097,  1997,  1996,
         3671,  4353,  1024,  2129,  2000,  2424,  1996,  7778, 12503,  1006,
         1008,  1007,  3378,  2007,  1996,  2779,  1997,  1037,  7099,  1012,
          102])"
472,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution']", Confidence Intervals,seg_109,assume the height of the kids follows a normal distribution with mean m and standard deviation s.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,
        0., 0.])","tensor([ 7023, 14025])","tensor([  101,  7868,  1996,  4578,  1997,  1996,  4268,  4076,  1037,  3671,
         4353,  2007,  2812,  1049,  1998,  3115, 24353,  1055,  1012,   102])"
473,1,['estimate'], Confidence Intervals,seg_109,"in practice, we do not know m and s, but we can calculate an estimate of m and s:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  1999,  3218,  1010,  2057,  2079,  2025,  2113,  1049,  1998,
         1055,  1010,  2021,  2057,  2064, 18422,  2019, 10197,  1997,  1049,
         1998,  1055,  1024,   102])"
474,1,"['deviation', 'estimate', 'sample', 'standard deviation', 'standard', 'average']", Confidence Intervals,seg_109,"– as an estimate of m, we use the average from a sample. – as an estimate of s, we use the standard deviation from a sample.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  1516,  2004,  2019, 10197,  1997,  1049,  1010,  2057,  2224,
         1996,  2779,  2013,  1037,  7099,  1012,  1516,  2004,  2019, 10197,
         1997,  1055,  1010,  2057,  2224,  1996,  3115, 24353,  2013,  1037,
         7099,  1012,   102])"
475,1,"['mean', 'sample', 'population', 'average']", Confidence Intervals,seg_109,we do not anticipate that the average calculated from a sample corresponds completely to the unknown mean of the population.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  2057,  2079,  2025,  3424,  6895, 17585,  2008,  1996,  2779,
        10174,  2013,  1037,  7099, 14788,  3294,  2000,  1996,  4242,  2812,
         1997,  1996,  2313,  1012,   102])"
476,1,"['probability', 'mean', 'interval']", Confidence Intervals,seg_109,"but maybe we can find an interval that with a high probability (e.g., 95%) contains the unknown mean.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  2021,  2672,  2057,  2064,  2424,  2019, 13483,  2008,  2007,
         1037,  2152,  9723,  1006,  1041,  1012,  1043,  1012,  1010,  5345,
         1003,  1007,  3397,  1996,  4242,  2812,  1012,   102])"
477,1,"['mean', 'confidence interval', 'interval', 'population', 'confidence']", Confidence Intervals,seg_109,such an interval is known as a 95% confidence interval (*) for the mean in the population. we will now show how to find such an interval (fig. 4.12).,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 7023, 14025])","tensor([  101,  2107,  2019, 13483,  2003,  2124,  2004,  1037,  5345,  1003,
         7023, 13483,  1006,  1008,  1007,  2005,  1996,  2812,  1999,  1996,
         2313,  1012,  2057,  2097,  2085,  2265,  2129,  2000,  2424,  2107,
         2019, 13483,  1006, 20965,  1012,  1018,  1012,  2260,  1007,  1012,
          102])"
478,1,"['mean', 'confidence interval', 'normal distribution', 'interval', 'estimate', 'normal', 'distribution', 'confidence', 'data']", Confidence Interval for the Mean,seg_111,the technique in this section requires that data can be described by a normal distribution. the purpose is to calculate an estimate of the mean and find a confidence interval for it.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  1996,  6028,  1999,  2023,  2930,  5942,  2008,  2951,  2064,
         2022,  2649,  2011,  1037,  3671,  4353,  1012,  1996,  3800,  2003,
         2000, 18422,  2019, 10197,  1997,  1996,  2812,  1998,  2424,  1037,
         7023, 13483,  2005,  2009,  1012,   102])"
479,0,[], Confidence Interval for the Mean,seg_111,"furthermore, we assume that",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([ 101, 7297, 1010, 2057, 7868, 2008,  102])"
480,1,"['deviation', 'sample', 'standard deviation', 'standard']", Confidence Interval for the Mean,seg_111,– we know the standard deviation in advance or – the sample is sufficiently large,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  1516,  2057,  2113,  1996,  3115, 24353,  1999,  5083,  2030,
         1516,  1996,  7099,  2003, 12949,  2312,   102])"
481,1,"['deviation', 'sample', 'standard deviation', 'standard']", Confidence Interval for the Mean,seg_111,"knowing the standard deviation in advance is not the usual situation. however, if the sample is large enough, we can use a calculated standard deviation, as if it is known.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  4209,  1996,  3115, 24353,  1999,  5083,  2003,  2025,  1996,
         5156,  3663,  1012,  2174,  1010,  2065,  1996,  7099,  2003,  2312,
         2438,  1010,  2057,  2064,  2224,  1037, 10174,  3115, 24353,  1010,
         2004,  2065,  2009,  2003,  2124,  1012,   102])"
482,1,"['sample', 'sample size']", Confidence Interval for the Mean,seg_111,"with a sample size of more than 30, we are on the safe side. if the sample size is just over 10, we will not make very big mistakes using the technique in this section.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2007,  1037,  7099,  2946,  1997,  2062,  2084,  2382,  1010,
         2057,  2024,  2006,  1996,  3647,  2217,  1012,  2065,  1996,  7099,
         2946,  2003,  2074,  2058,  2184,  1010,  2057,  2097,  2025,  2191,
         2200,  2502, 12051,  2478,  1996,  6028,  1999,  2023,  2930,  1012,
          102])"
483,1,"['mean', 'estimate', 'sample', 'population', 'average']", Confidence Interval for the Mean,seg_111,"as an estimate of the mean m in the population, we use the average x of the sample.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2004,  2019, 10197,  1997,  1996,  2812,  1049,  1999,  1996,
         2313,  1010,  2057,  2224,  1996,  2779,  1060,  1997,  1996,  7099,
         1012,   102])"
484,1,"['sample', 'average', 'sample size']", Confidence Interval for the Mean,seg_111,"an average is more precisely determined, the larger the sample size. more specifically, we have the following rule:",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2019,  2779,  2003,  2062, 10785,  4340,  1010,  1996,  3469,
         1996,  7099,  2946,  1012,  2062,  4919,  1010,  2057,  2031,  1996,
         2206,  3627,  1024,   102])"
485,1,"['mean', 'deviation', 'standard deviation', 'standard', 'standard error', 'average', 'data', 'error']", Confidence Interval for the Mean,seg_111,the standard deviation for an average is obtained by dividing the original standard deviation s with the square root of the number of data values n. this is called the standard error (*) of the mean and sometimes abbreviated se.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  1996,  3115, 24353,  2005,  2019,  2779,  2003,  4663,  2011,
        16023,  1996,  2434,  3115, 24353,  1055,  2007,  1996,  2675,  7117,
         1997,  1996,  2193,  1997,  2951,  5300,  1050,  1012,  2023,  2003,
         2170,  1996,  3115,  7561,  1006,  1008,  1007,  1997,  1996,  2812,
         1998,  2823, 12066,  7367,  1012,   102])"
486,1,"['mean', 'deviation', 'normal distribution', 'interval', 'normal', 'standard deviation', 'standard', 'deviations', 'distribution', 'data', 'standard deviations']", Confidence Interval for the Mean,seg_111,"we have seen that slightly more than 95% of all the data values in a normal distribution are in an interval around mean 2 standard deviations. if we want exactly 95% of all data values in the interval, we must multiply the standard deviation by 1.96 instead of 2.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2057,  2031,  2464,  2008,  3621,  2062,  2084,  5345,  1003,
         1997,  2035,  1996,  2951,  5300,  1999,  1037,  3671,  4353,  2024,
         1999,  2019, 13483,  2105,  2812,  1016,  3115, 24353,  2015,  1012,
         2065,  2057,  2215,  3599,  5345,  1003,  1997,  2035,  2951,  5300,
         1999,  1996, 13483,  1010,  2057,  2442,  4800, 22086,  1996,  3115,
        24353,  2011,  1015,  1012,  5986,  2612,  1997,  1016,  1012,   102])"
487,1,"['interval', 'mean', 'confidence', 'confidence interval']", Confidence Interval for the Mean,seg_111,a 95% confidence interval (*) for the mean is therefore,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  1037,  5345,  1003,  7023, 13483,  1006,  1008,  1007,  2005,
         1996,  2812,  2003,  3568,   102])"
488,1,"['uncertainty', 'statistical uncertainty', 'statistical', 'average']", Confidence Interval for the Mean,seg_111,the number after can be interpreted as the statistical uncertainty (*) of the average.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  1996,  2193,  2044,  2064,  2022, 10009,  2004,  1996,  7778,
        12503,  1006,  1008,  1007,  1997,  1996,  2779,  1012,   102])"
489,1,"['confidence interval', 'interval', 'statistics', 'confidence']", Confidence Interval for the Mean,seg_111,"the precise reason why the 95% confidence interval is calculated in this way, is fairly technical. see, for example, g.e.p. box, w.g. hunter, and j.s. hunter (wiley 2005, 2nd ed.): statistics for experimenters.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  1996, 10480,  3114,  2339,  1996,  5345,  1003,  7023, 13483,
         2003, 10174,  1999,  2023,  2126,  1010,  2003,  7199,  4087,  1012,
         2156,  1010,  2005,  2742,  1010,  1043,  1012,  1041,  1012,  1052,
         1012,  3482,  1010,  1059,  1012,  1043,  1012,  4477,  1010,  1998,
         1046,  1012,  1055,  1012,  4477,  1006, 18825,  2384,  1010,  3416,
         3968,  1012,  1007,  1024,  6747,  2005,  7551,  2545,  1012,   102])"
490,1,"['confidence interval', 'uncertainty', 'interval', 'statistical uncertainty', 'statistics', 'statistical', 'confidence']", Confidence Interval for the Mean,seg_111,"unfortunately, the term statistical uncertainty is not given a name in most books on statistics! it is referred to as “the half-length of a confidence interval for the mean” or just “the number after ”.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  6854,  1010,  1996,  2744,  7778, 12503,  2003,  2025,  2445,
         1037,  2171,  1999,  2087,  2808,  2006,  6747,   999,  2009,  2003,
         3615,  2000,  2004,  1523,  1996,  2431,  1011,  3091,  1997,  1037,
         7023, 13483,  2005,  1996,  2812,  1524,  2030,  2074,  1523,  1996,
         2193,  2044,  1524,  1012,   102])"
491,1,"['standardized', 'normal distribution', 'table', 'fractiles', 'fractile', 'normal', 'distribution', 'data']", Confidence Interval for the Mean,seg_111,"in fact, 1.96 is just the 97.5% fractile of the standardized normal distribution, with 2.5% of the data values larger than 1.96. this means that 95% of the data values are between the 1.96 and 1.96. in chap. 9, a table of the main fractiles of the standardized normal distribution can be found (fig. 4.13).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  1999,  2755,  1010,  1015,  1012,  5986,  2003,  2074,  1996,
         5989,  1012,  1019,  1003, 25312,  6593,  9463,  1997,  1996, 16367,
         3671,  4353,  1010,  2007,  1016,  1012,  1019,  1003,  1997,  1996,
         2951,  5300,  3469,  2084,  1015,  1012,  5986,  1012,  2023,  2965,
         2008,  5345,  1003,  1997,  1996,  2951,  5300,  2024,  2090,  1996,
         1015,  1012,  5986,  1998,  1015,  1012,  5986,  1012,  1999, 15775,
         2361,  1012,  1023,  1010,  1037,  2795,  1997,  1996,  2364, 25312,
         6593,  9463,  2015,  1997,  1996, 16367,  3671,  4353,  2064,  2022,
         2179,  1006, 20965,  1012,  1018,  1012,  2410,  1007,  1012,   102])"
492,0,[], Confidence Interval for the Mean,seg_111,4.8.1.3 example height of kids in fitness club survey,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  1018,  1012,  1022,  1012,  1015,  1012,  1017,  2742,  4578,
         1997,  4268,  1999, 10516,  2252,  5002,   102])"
493,1,"['mean', 'confidence interval', 'interval', 'population', 'confidence']", Confidence Interval for the Mean,seg_111,wewant to calculate a 95% confidence interval for the mean height of all the kids in the population.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2057,  7447,  2102,  2000, 18422,  1037,  5345,  1003,  7023,
        13483,  2005,  1996,  2812,  4578,  1997,  2035,  1996,  4268,  1999,
         1996,  2313,  1012,   102])"
494,1,"['mean', 'estimate', 'sample', 'data']", Confidence Interval for the Mean,seg_111,"we do not know the mean m, but we have data from a sample of n ¼ 30 kids to estimate it.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2057,  2079,  2025,  2113,  1996,  2812,  1049,  1010,  2021,
         2057,  2031,  2951,  2013,  1037,  7099,  1997,  1050,  1091,  2382,
         4268,  2000, 10197,  2009,  1012,   102])"
495,1,"['deviation', 'sample', 'standard deviation', 'sample size', 'standard', 'average']", Confidence Interval for the Mean,seg_111,"in the sample, we have an average height of x ¼ 157:10 cm and a standard deviation s ¼ 22.06 cm. as the sample size is 30, we can consider the standard deviation known, i.e., we can put s ¼ 22.06 cm.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  1999,  1996,  7099,  1010,  2057,  2031,  2019,  2779,  4578,
         1997,  1060,  1091, 17403,  1024,  2184,  4642,  1998,  1037,  3115,
        24353,  1055,  1091,  2570,  1012,  5757,  4642,  1012,  2004,  1996,
         7099,  2946,  2003,  2382,  1010,  2057,  2064,  5136,  1996,  3115,
        24353,  2124,  1010,  1045,  1012,  1041,  1012,  1010,  2057,  2064,
         2404,  1055,  1091,  2570,  1012,  5757,  4642,  1012,   102])"
496,1,"['interval', 'mean', 'confidence', 'confidence interval']", Confidence Interval for the Mean,seg_111,we calculate a 95% confidence interval for the mean using the formula:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2057, 18422,  1037,  5345,  1003,  7023, 13483,  2005,  1996,
         2812,  2478,  1996,  5675,  1024,   102])"
497,1,"['mean', 'confidence interval', 'probability', 'interval', 'population', 'confidence']", Confidence Interval for the Mean,seg_111,the 95% confidence interval for the mean height can be written as 157.10 7.90 cm. the endpoints of the interval can be calculated as 149.2 and 165.0 cm. this interval will with 95% probability include the unknown mean of the population.,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  1996,  5345,  1003,  7023, 13483,  2005,  1996,  2812,  4578,
         2064,  2022,  2517,  2004, 17403,  1012,  2184,  1021,  1012,  3938,
         4642,  1012,  1996,  2203, 26521,  1997,  1996, 13483,  2064,  2022,
        10174,  2004, 17332,  1012,  1016,  1998, 13913,  1012,  1014,  4642,
         1012,  2023, 13483,  2097,  2007,  5345,  1003,  9723,  2421,  1996,
         4242,  2812,  1997,  1996,  2313,  1012,   102])"
498,1,"['mean', 'confidence interval', 'probability', 'interval', 'standard', 'standard error', 'confidence', 'error']", Confidence Interval for the Mean,seg_111,"sometimes we want an interval that with a probability of 99% contains the unknown mean. then, we shall instead multiply the standard error with 2.576. the endpoints of the interval are then calculated as 146.7 cm and 167.5 cm. so, the confidence interval is wider, if we want a 99% probability.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2823,  2057,  2215,  2019, 13483,  2008,  2007,  1037,  9723,
         1997,  5585,  1003,  3397,  1996,  4242,  2812,  1012,  2059,  1010,
         2057,  4618,  2612,  4800, 22086,  1996,  3115,  7561,  2007,  1016,
         1012,  5401,  2575,  1012,  1996,  2203, 26521,  1997,  1996, 13483,
         2024,  2059, 10174,  2004, 16333,  1012,  1021,  4642,  1998, 16785,
         1012,  1019,  4642,  1012,  2061,  1010,  1996,  7023, 13483,  2003,
         7289,  1010,  2065,  2057,  2215,  1037,  5585,  1003,  9723,  1012,
          102])"
499,1,"['standardized', 'normal distribution', 'fractile', 'normal', 'distribution', 'data']", Confidence Interval for the Mean,seg_111,"actually, the number 2.576 is just the 99.5% fractile in the standardized normal distribution, i.e., exactly 99% of all data values are between 2.576 and 2.576. thus, 0.5% of the data values are larger than 2.576.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2941,  1010,  1996,  2193,  1016,  1012,  5401,  2575,  2003,
         2074,  1996,  5585,  1012,  1019,  1003, 25312,  6593,  9463,  1999,
         1996, 16367,  3671,  4353,  1010,  1045,  1012,  1041,  1012,  1010,
         3599,  5585,  1003,  1997,  2035,  2951,  5300,  2024,  2090,  1016,
         1012,  5401,  2575,  1998,  1016,  1012,  5401,  2575,  1012,  2947,
         1010,  1014,  1012,  1019,  1003,  1997,  1996,  2951,  5300,  2024,
         3469,  2084,  1016,  1012,  5401,  2575,  1012,   102])"
500,1,"['confidence interval', 'uncertainty', 'interval', 'statistical uncertainty', 'population', 'sample', 'standard', 'samples', 'statistical', 'standard error', 'confidence', 'average', 'error', 'case']", Confidence Interval for the Mean,seg_111,"technical note: the statistical uncertainty on an average in a finite population. usually, we take samples from a population with a finite number of individuals, and the sample is relatively small compared to the population, at most 10% of the population. in this case, the above formulas for standard error and confidence interval are valid. if the sample is larger than 10% of the population, we must modify the formulas. the correct formula for standard error is then:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  4087,  3602,  1024,  1996,  7778, 12503,  2006,  2019,  2779,
         1999,  1037, 10713,  2313,  1012,  2788,  1010,  2057,  2202,  8168,
         2013,  1037,  2313,  2007,  1037, 10713,  2193,  1997,  3633,  1010,
         1998,  1996,  7099,  2003,  4659,  2235,  4102,  2000,  1996,  2313,
         1010,  2012,  2087,  2184,  1003,  1997,  1996,  2313,  1012,  1999,
         2023,  2553,  1010,  1996,  2682, 25814,  2005,  3115,  7561,  1998,
         7023, 13483,  2024,  9398,  1012,  2065,  1996,  7099,  2003,  3469,
         2084,  2184,  1003,  1997,  1996,  2313,  1010,  2057,  2442, 19933,
         1996, 25814,  1012,  1996,  6149,  5675,  2005,  3115,  7561,  2003,
         2059,  1024,   102])"
501,1,"['mean', 'confidence interval', 'interval', 'sampling', 'population', 'confidence', 'sampling fraction']", Confidence Interval for the Mean,seg_111,"here, n ¼ number of individuals in the population. the fraction n/n is called the sampling fraction (*). similarly, the formula for the confidence interval is modified: a 95% confidence interval for the mean is",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2182,  1010,  1050,  1091,  2193,  1997,  3633,  1999,  1996,
         2313,  1012,  1996, 12884,  1050,  1013,  1050,  2003,  2170,  1996,
        16227, 12884,  1006,  1008,  1007,  1012,  6660,  1010,  1996,  5675,
         2005,  1996,  7023, 13483,  2003,  6310,  1024,  1037,  5345,  1003,
         7023, 13483,  2005,  1996,  2812,  2003,   102])"
502,1,['sample'], Confidence Interval for the Mean,seg_111,"when the sample is small, n/n is close to 0, and thus the square root of 1 n/n is very close to 1. this means that the simpler formula is valid.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2043,  1996,  7099,  2003,  2235,  1010,  1050,  1013,  1050,
         2003,  2485,  2000,  1014,  1010,  1998,  2947,  1996,  2675,  7117,
         1997,  1015,  1050,  1013,  1050,  2003,  2200,  2485,  2000,  1015,
         1012,  2023,  2965,  2008,  1996, 16325,  5675,  2003,  9398,  1012,
          102])"
503,0,[], Confidence Interval for the Mean,seg_111,this section can be skipped if you do not use spreadsheets.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2023,  2930,  2064,  2022, 16791,  2065,  2017,  2079,  2025,
         2224, 20861, 21030,  3215,  1012,   102])"
504,1,"['function', 'uncertainty', 'statistical uncertainty', 'sample', 'statistical', 'confidence', 'average']", Confidence Interval for the Mean,seg_111,"– first, calculate the average of the sample with the function average. – then, calculate the statistical uncertainty using the function confidence.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  1516,  2034,  1010, 18422,  1996,  2779,  1997,  1996,  7099,
         2007,  1996,  3853,  2779,  1012,  1516,  2059,  1010, 18422,  1996,
         7778, 12503,  2478,  1996,  3853,  7023,  1012,   102])"
505,1,"['function', 'confidence interval', 'table', 'interval', 'parameters', 'confidence']", Confidence Interval for the Mean,seg_111,"for the function confidence, we need to specify the parameters (table 4.7). this allows you to calculate the end points of the confidence interval. this is shown below (table 4.8):",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2005,  1996,  3853,  7023,  1010,  2057,  2342,  2000, 20648,
         1996, 11709,  1006,  2795,  1018,  1012,  1021,  1007,  1012,  2023,
         4473,  2017,  2000, 18422,  1996,  2203,  2685,  1997,  1996,  7023,
        13483,  1012,  2023,  2003,  3491,  2917,  1006,  2795,  1018,  1012,
         1022,  1007,  1024,   102])"
506,1,"['deviation', 'standard deviation', 'standard', 'average', 'data']", Confidence Interval for the Mean,seg_111,we imagine that data are located in the area b2: b31. the average is calculated using average. the standard deviation is calculated using the stdev.,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2057,  5674,  2008,  2951,  2024,  2284,  1999,  1996,  2181,
         1038,  2475,  1024,  1038, 21486,  1012,  1996,  2779,  2003, 10174,
         2478,  2779,  1012,  1996,  3115, 24353,  2003, 10174,  2478,  1996,
         2358, 24844,  1012,   102])"
507,1,"['function', 'deviation', 'uncertainty', 'statistical uncertainty', 'sample', 'standard deviation', 'sample size', 'standard', 'statistical', 'confidence']", Confidence Interval for the Mean,seg_111,"then, calculate the statistical uncertainty using function confidence. for this function, we must specify the “rest probability” 0.05 (equivalent to 5%), the standard deviation (calculated using the stdev), and the sample size (30).",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  2059,  1010, 18422,  1996,  7778, 12503,  2478,  3853,  7023,
         1012,  2005,  2023,  3853,  1010,  2057,  2442, 20648,  1996,  1523,
         2717,  9723,  1524,  1014,  1012,  5709,  1006,  5662,  2000,  1019,
         1003,  1007,  1010,  1996,  3115, 24353,  1006, 10174,  2478,  1996,
         2358, 24844,  1007,  1010,  1998,  1996,  7099,  2946,  1006,  2382,
         1007,  1012,   102])"
508,1,"['interval', 'mean', 'confidence', 'confidence interval']", Confidence Interval for the Mean,seg_111,the 95% confidence interval for the mean height then can be written as 157.10 7.90. the endpoints of the confidence interval thus become 149.0 and 165.0.,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812])","tensor([  101,  1996,  5345,  1003,  7023, 13483,  2005,  1996,  2812,  4578,
         2059,  2064,  2022,  2517,  2004, 17403,  1012,  2184,  1021,  1012,
         3938,  1012,  1996,  2203, 26521,  1997,  1996,  7023, 13483,  2947,
         2468, 17332,  1012,  1014,  1998, 13913,  1012,  1014,  1012,   102])"
509,1,"['sample', 'sample size']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"if your sample size is typically larger than 20, you can skip this section.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2065,  2115,  7099,  2946,  2003,  4050,  3469,  2084,  2322,
         1010,  2017,  2064, 13558,  2023,  2930,  1012,   102])"
510,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,let us assume that the weight of coffee in a bag of coffee follows a normal distribution with mean m and standard deviation s.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 1., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2292,  2149,  7868,  2008,  1996,  3635,  1997,  4157,  1999,
         1037,  4524,  1997,  4157,  4076,  1037,  3671,  4353,  2007,  2812,
         1049,  1998,  3115, 24353,  1055,  1012,   102])"
511,1,"['mean', 'deviation', 'normal', 'standard deviation', 'standard']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,we know neither the mean nor the standard deviation in this normal distribution.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2057,  2113,  4445,  1996,  2812,  4496,  1996,  3115, 24353,
         1999,  2023,  3671,  4353,  1012,   102])"
512,1,"['mean', 'confidence interval', 'deviation', 'interval', 'estimate', 'sample', 'sample standard deviation', 'standard deviation', 'standard', 'confidence']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"– we estimate both the mean and the standard deviation from a sample. – the purpose is to estimate the mean m and find a confidence interval for it. – we do not know the standard deviation s, but estimate it by the sample standard deviation s.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  1516,  2057, 10197,  2119,  1996,  2812,  1998,  1996,  3115,
        24353,  2013,  1037,  7099,  1012,  1516,  1996,  3800,  2003,  2000,
        10197,  1996,  2812,  1049,  1998,  2424,  1037,  7023, 13483,  2005,
         2009,  1012,  1516,  2057,  2079,  2025,  2113,  1996,  3115, 24353,
         1055,  1010,  2021, 10197,  2009,  2011,  1996,  7099,  3115, 24353,
         1055,  1012,   102])"
513,1,"['deviation', 'sample', 'standard deviation', 'standard']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"the sample is relatively small, less than 30 (or maybe even less than 10). therefore, the standard deviation from the sample cannot be considered to be known.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  1996,  7099,  2003,  4659,  2235,  1010,  2625,  2084,  2382,
         1006,  2030,  2672,  2130,  2625,  2084,  2184,  1007,  1012,  3568,
         1010,  1996,  3115, 24353,  2013,  1996,  7099,  3685,  2022,  2641,
         2000,  2022,  2124,  1012,   102])"
514,1,"['interval', 'mean', 'confidence', 'confidence interval']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"by analogy with the last section, we construct a 95% confidence interval for the mean:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2011, 23323,  2007,  1996,  2197,  2930,  1010,  2057,  9570,
         1037,  5345,  1003,  7023, 13483,  2005,  1996,  2812,  1024,   102])"
515,1,"['deviation', 'sample', 'standard deviation', 'standard', 'standard error', 'error']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"because the sample is not big enough for us to consider the standard deviation to be known, it appears that the multiplier t of the standard error becomes (maybe even much) larger than 1.96.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2138,  1996,  7099,  2003,  2025,  2502,  2438,  2005,  2149,
         2000,  5136,  1996,  3115, 24353,  2000,  2022,  2124,  1010,  2009,
         3544,  2008,  1996,  4800, 24759,  3771,  1056,  1997,  1996,  3115,
         7561,  4150,  1006,  2672,  2130,  2172,  1007,  3469,  2084,  1015,
         1012,  5986,  1012,   102])"
516,1,"['fractile', 'normal', 'normal distribution', 'distribution']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,the constant 1.96 previously used was the 97.5% fractile of a normal distribution.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 1., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  1996,  5377,  1015,  1012,  5986,  3130,  2109,  2001,  1996,
         5989,  1012,  1019,  1003, 25312,  6593,  9463,  1997,  1037,  3671,
         4353,  1012,   102])"
517,1,"['normal distribution', 'fractiles', 'degrees of freedom', 'distributions', 'normal', 'distribution', 'data', 'degrees of freedom ']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"instead of fractiles in the normal distribution, we must now apply fractiles in the t-distribution (*), also known as “students t-distribution”. this is not a single distribution, but a whole family of distributions. if there are n data values (at least 2), we say that the t-distribution has n 1 degrees of freedom (*).",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2612,  1997, 25312,  6593,  9463,  2015,  1999,  1996,  3671,
         4353,  1010,  2057,  2442,  2085,  6611, 25312,  6593,  9463,  2015,
         1999,  1996,  1056,  1011,  4353,  1006,  1008,  1007,  1010,  2036,
         2124,  2004,  1523,  2493,  1056,  1011,  4353,  1524,  1012,  2023,
         2003,  2025,  1037,  2309,  4353,  1010,  2021,  1037,  2878,  2155,
         1997, 20611,  1012,  2065,  2045,  2024,  1050,  2951,  5300,  1006,
         2012,  2560,  1016,  1007,  1010,  2057,  2360,  2008,  1996,  1056,
         1011,  4353,  2038,  1050,  1015,  5445,  1997,  4071,  1006,  1008,
         1007,  1012,   102])"
518,0,[], Confidence Interval for the Mean in Case of a Small Sample,seg_113,note: “degrees of freedom” is often abbreviated df.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  3602,  1024,  1523,  5445,  1997,  4071,  1524,  2003,  2411,
        12066,  1040,  2546,  1012,   102])"
519,1,"['mean', 'confidence interval', 'deviation', 'interval', 'fractile', 'degrees of freedom', 'sample', 'sample size', 'standard deviation', 'standard', 'confidence']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"if we want a 95% confidence interval for the mean m, we must use the 97.5% fractile in the t-distribution with n 1 degrees of freedom. if we want a 99% confidence interval for the mean m, we must use the 99.5% fractile in the t-distribution with n 1 degrees of freedom. the confidence interval, which we calculate in this situation, is wider than when the standard deviation s is known. if the sample size is small, the confidence interval is much wider.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2065,  2057,  2215,  1037,  5345,  1003,  7023, 13483,  2005,
         1996,  2812,  1049,  1010,  2057,  2442,  2224,  1996,  5989,  1012,
         1019,  1003, 25312,  6593,  9463,  1999,  1996,  1056,  1011,  4353,
         2007,  1050,  1015,  5445,  1997,  4071,  1012,  2065,  2057,  2215,
         1037,  5585,  1003,  7023, 13483,  2005,  1996,  2812,  1049,  1010,
         2057,  2442,  2224,  1996,  5585,  1012,  1019,  1003, 25312,  6593,
         9463,  1999,  1996,  1056,  1011,  4353,  2007,  1050,  1015,  5445,
         1997,  4071,  1012,  1996,  7023, 13483,  1010,  2029,  2057, 18422,
         1999,  2023,  3663,  1010,  2003,  7289,  2084,  2043,  1996,  3115,
        24353,  1055,  2003,  2124,  1012,  2065,  1996,  7099,  2946,  2003,
         2235,  1010,  1996,  7023, 13483,  2003,  2172,  7289,  1012,   102])"
520,1,"['normal distribution', 'table', 'fractiles', 'degrees of freedom', 'normal', 'distribution']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"themost important fractiles in the t-distribution can be found in a table in chap. 9. in the table, we see that the 97.5 and 99.5% fractiles in the t-distribution are larger than the corresponding fractiles in normal distribution; this is particular noticeable, when the number of degrees of freedom is less than 10.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2068, 14122,  2590, 25312,  6593,  9463,  2015,  1999,  1996,
         1056,  1011,  4353,  2064,  2022,  2179,  1999,  1037,  2795,  1999,
        15775,  2361,  1012,  1023,  1012,  1999,  1996,  2795,  1010,  2057,
         2156,  2008,  1996,  5989,  1012,  1019,  1998,  5585,  1012,  1019,
         1003, 25312,  6593,  9463,  2015,  1999,  1996,  1056,  1011,  4353,
         2024,  3469,  2084,  1996,  7978, 25312,  6593,  9463,  2015,  1999,
         3671,  4353,  1025,  2023,  2003,  3327, 17725,  1010,  2043,  1996,
         2193,  1997,  5445,  1997,  4071,  2003,  2625,  2084,  2184,  1012,
          102])"
521,1,"['normal distribution', 'table', 'fractiles', 'degrees of freedom', 'normal', 'distribution']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"when the number of degrees of freedom is at least 30, there is practically no difference between the t-distribution and the normal distribution. therefore, the table only shows fractiles for up to 30 degrees of freedom.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2043,  1996,  2193,  1997,  5445,  1997,  4071,  2003,  2012,
         2560,  2382,  1010,  2045,  2003,  8134,  2053,  4489,  2090,  1996,
         1056,  1011,  4353,  1998,  1996,  3671,  4353,  1012,  3568,  1010,
         1996,  2795,  2069,  3065, 25312,  6593,  9463,  2015,  2005,  2039,
         2000,  2382,  5445,  1997,  4071,  1012,   102])"
522,1,"['function', 'normal distribution', 'density function', 'probability', 'probability density function', 'degrees of freedom', 'normal', 'distribution', 'kurtosis']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"the following figure shows the probability density function of the t-distribution with 1, 2, and 5 degrees of freedom as well as the normal distribution. observe that even a t-distribution with 5 degrees of freedom at first glance does not seem very different from the normal distribution; however, there is still a big difference in the “tails” of the distribution. note: the t-distribution has “heavier tails” than the normal distribution, i.e., it has positive kurtosis (fig. 4.14).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  1996,  2206,  3275,  3065,  1996,  9723,  4304,  3853,  1997,
         1996,  1056,  1011,  4353,  2007,  1015,  1010,  1016,  1010,  1998,
         1019,  5445,  1997,  4071,  2004,  2092,  2004,  1996,  3671,  4353,
         1012, 11949,  2008,  2130,  1037,  1056,  1011,  4353,  2007,  1019,
         5445,  1997,  4071,  2012,  2034,  6054,  2515,  2025,  4025,  2200,
         2367,  2013,  1996,  3671,  4353,  1025,  2174,  1010,  2045,  2003,
         2145,  1037,  2502,  4489,  1999,  1996,  1523, 17448,  1524,  1997,
         1996,  4353,  1012,  3602,  1024,  1996,  1056,  1011,  4353,  2038,
         1523, 11907, 17448,  1524,  2084,  1996,  3671,  4353,  1010,  1045,
         1012,  1041,  1012,  1010,  2009,  2038,  3893,  9679, 12650,  1006,
        20965,  1012,  1018,  1012,  2403,  1007,  1012,   102])"
523,1,"['mean', 'normal distribution', 'estimate', 'sample', 'normal', 'distribution']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"let us assume that the weight of the coffee in bags of coffee follows a normal distribution. we do not know the mean m, but we take a sample of n ¼ 4 coffee bags to estimate it.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2292,  2149,  7868,  2008,  1996,  3635,  1997,  1996,  4157,
         1999,  8641,  1997,  4157,  4076,  1037,  3671,  4353,  1012,  2057,
         2079,  2025,  2113,  1996,  2812,  1049,  1010,  2021,  2057,  2202,
         1037,  7099,  1997,  1050,  1091,  1018,  4157,  8641,  2000, 10197,
         2009,  1012,   102])"
524,1,"['deviation', 'sample', 'standard deviation', 'standard', 'average']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,suppose that in the sample we have an average x ¼ 505:8 g and a standard deviation of s ¼ 5.30 g.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  6814,  2008,  1999,  1996,  7099,  2057,  2031,  2019,  2779,
         1060,  1091, 28952,  1024,  1022,  1043,  1998,  1037,  3115, 24353,
         1997,  1055,  1091,  1019,  1012,  2382,  1043,  1012,   102])"
525,1,"['interval', 'mean', 'confidence', 'confidence interval']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,we calculate a 95% confidence interval for the mean from the formula,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2057, 18422,  1037,  5345,  1003,  7023, 13483,  2005,  1996,
         2812,  2013,  1996,  5675,   102])"
526,1,"['mean', 'confidence interval', 'probability', 'fractile', 'interval', 'confidence']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"we need a confidence interval, which with probability 95% contains the unknown value of the mean m, so we must use the 97.5% fractile of the t-distribution.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2057,  2342,  1037,  7023, 13483,  1010,  2029,  2007,  9723,
         5345,  1003,  3397,  1996,  4242,  3643,  1997,  1996,  2812,  1049,
         1010,  2061,  2057,  2442,  2224,  1996,  5989,  1012,  1019,  1003,
        25312,  6593,  9463,  1997,  1996,  1056,  1011,  4353,  1012,   102])"
527,1,"['table', 'fractile', 'degrees of freedom', 'sample']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"since the sample consists of n ¼ 4 coffee bags, the number of degrees of freedom is df ¼ n 1 ¼ 3. in the table of the t-distribution in chap. 9, we find the 97.5% fractile for a t-distribution with 3 degrees of freedom as 3.182. notice that this fractile is substantially larger than 1.96.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2144,  1996,  7099,  3774,  1997,  1050,  1091,  1018,  4157,
         8641,  1010,  1996,  2193,  1997,  5445,  1997,  4071,  2003,  1040,
         2546,  1091,  1050,  1015,  1091,  1017,  1012,  1999,  1996,  2795,
         1997,  1996,  1056,  1011,  4353,  1999, 15775,  2361,  1012,  1023,
         1010,  2057,  2424,  1996,  5989,  1012,  1019,  1003, 25312,  6593,
         9463,  2005,  1037,  1056,  1011,  4353,  2007,  1017,  5445,  1997,
         4071,  2004,  1017,  1012, 17691,  1012,  5060,  2008,  2023, 25312,
         6593,  9463,  2003, 12381,  3469,  2084,  1015,  1012,  5986,  1012,
          102])"
528,1,"['uncertainty', 'interval', 'statistical uncertainty', 'statistical', 'average']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,the formula now gives us the statistical uncertainty of the average as 8.4. the endpoints of the interval can now be calculated as 497.4 and 514.2 g.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  1996,  5675,  2085,  3957,  2149,  1996,  7778, 12503,  1997,
         1996,  2779,  2004,  1022,  1012,  1018,  1012,  1996,  2203, 26521,
         1997,  1996, 13483,  2064,  2085,  2022, 10174,  2004,  4749,  2581,
         1012,  1018,  1998,  4868,  2549,  1012,  1016,  1043,  1012,   102])"
529,0,[], Confidence Interval for the Mean in Case of a Small Sample,seg_113,this section can be skipped if you do not use spreadsheets.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2023,  2930,  2064,  2022, 16791,  2065,  2017,  2079,  2025,
         2224, 20861, 21030,  3215,  1012,   102])"
530,1,"['mean', 'confidence interval', 'interval', 'sample', 'confidence']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"if the sample is small, you need the following to calculate the confidence interval for the mean:",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2065,  1996,  7099,  2003,  2235,  1010,  2017,  2342,  1996,
         2206,  2000, 18422,  1996,  7023, 13483,  2005,  1996,  2812,  1024,
          102])"
531,1,"['deviation', 'sample', 'standard deviation', 'standard', 'average']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,– the average of the sample calculated using the average. – the standard deviation calculated using the stdev. (continued),tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  1516,  1996,  2779,  1997,  1996,  7099, 10174,  2478,  1996,
         2779,  1012,  1516,  1996,  3115, 24353, 10174,  2478,  1996,  2358,
        24844,  1012,  1006,  2506,  1007,   102])"
532,1,"['function', 'confidence interval', 'interval', 'fractile', 'degrees of freedom', 'sample', 'sample size', 'confidence', 'case']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"– the sample size n. we need the square root of n obtained using the function sqrt. – a fractile in the t-distribution with n 1 degrees of freedom, for example, the 97.5% fractile in case of a 95% confidence interval: calculated using the function tinv.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  1516,  1996,  7099,  2946,  1050,  1012,  2057,  2342,  1996,
         2675,  7117,  1997,  1050,  4663,  2478,  1996,  3853,  5490,  5339,
         1012,  1516,  1037, 25312,  6593,  9463,  1999,  1996,  1056,  1011,
         4353,  2007,  1050,  1015,  5445,  1997,  4071,  1010,  2005,  2742,
         1010,  1996,  5989,  1012,  1019,  1003, 25312,  6593,  9463,  1999,
         2553,  1997,  1037,  5345,  1003,  7023, 13483,  1024, 10174,  2478,
         1996,  3853,  9543,  2615,  1012,   102])"
533,1,"['mean', 'function', 'confidence interval', 'probability', 'fractile', 'interval', 'degrees of freedom', 'confidence', 'data']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"in the example with coffee bags, there are four data values, i.e., the number of degrees of freedom is 3. if we want a confidence interval that with probability 95% contains the unknown value of the mean m, we use the 97.5% fractile. this fractile can be calculated in spreadsheets, use the function tinv.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  1999,  1996,  2742,  2007,  4157,  8641,  1010,  2045,  2024,
         2176,  2951,  5300,  1010,  1045,  1012,  1041,  1012,  1010,  1996,
         2193,  1997,  5445,  1997,  4071,  2003,  1017,  1012,  2065,  2057,
         2215,  1037,  7023, 13483,  2008,  2007,  9723,  5345,  1003,  3397,
         1996,  4242,  3643,  1997,  1996,  2812,  1049,  1010,  2057,  2224,
         1996,  5989,  1012,  1019,  1003, 25312,  6593,  9463,  1012,  2023,
        25312,  6593,  9463,  2064,  2022, 10174,  1999, 20861, 21030,  3215,
         1010,  2224,  1996,  3853,  9543,  2615,  1012,   102])"
534,1,"['function', 'probability']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,note: the special way the probability must be specified in the spreadsheet function:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  3602,  1024,  1996,  2569,  2126,  1996,  9723,  2442,  2022,
         9675,  1999,  1996, 20861, 21030,  2102,  3853,  1024,   102])"
535,1,"['parameter', 'degrees of freedom']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,– first parameter of tinv: find the “rest probability” and multiply by 2. – second parameter of tinv: number of degrees of freedom.,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  1516,  2034, 16381,  1997,  9543,  2615,  1024,  2424,  1996,
         1523,  2717,  9723,  1524,  1998,  4800, 22086,  2011,  1016,  1012,
         1516,  2117, 16381,  1997,  9543,  2615,  1024,  2193,  1997,  5445,
         1997,  4071,  1012,   102])"
536,1,"['fractile', 'degrees of freedom']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"an example: for the 97.5% ¼ 0.975 fractile, the “rest probability” is 2.5% ¼ 0.025. we multiply by 2 and obtain 5% ¼ 0.05. with 3 degrees of freedom we get the fractile tinv(0.05; 3) ¼ 3.182.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2019,  2742,  1024,  2005,  1996,  5989,  1012,  1019,  1003,
         1091,  1014,  1012,  5989,  2629, 25312,  6593,  9463,  1010,  1996,
         1523,  2717,  9723,  1524,  2003,  1016,  1012,  1019,  1003,  1091,
         1014,  1012,  6185,  2629,  1012,  2057,  4800, 22086,  2011,  1016,
         1998,  6855,  1019,  1003,  1091,  1014,  1012,  5709,  1012,  2007,
         1017,  5445,  1997,  4071,  2057,  2131,  1996, 25312,  6593,  9463,
         9543,  2615,  1006,  1014,  1012,  5709,  1025,  1017,  1007,  1091,
         1017,  1012, 17691,  1012,   102])"
537,1,"['mean', 'confidence interval', 'interval', 'functions', 'information', 'confidence']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"all the information needed is found in the output below, including the calculation formula using spreadsheet functions. the 95% confidence interval for mean is found as 505.8 8.4 or the interval from 497.4 to 514.2 (fig. 4.15).",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2035,  1996,  2592,  2734,  2003,  2179,  1999,  1996,  6434,
         2917,  1010,  2164,  1996, 17208,  5675,  2478, 20861, 21030,  2102,
         4972,  1012,  1996,  5345,  1003,  7023, 13483,  2005,  2812,  2003,
         2179,  2004, 28952,  1012,  1022,  1022,  1012,  1018,  2030,  1996,
        13483,  2013,  4749,  2581,  1012,  1018,  2000,  4868,  2549,  1012,
         1016,  1006, 20965,  1012,  1018,  1012,  2321,  1007,  1012,   102])"
538,1,"['mean', 'confidence level', 'data', 'descriptive statistics', 'level', 'statistics', 'confidence']", Confidence Interval for the Mean in Case of a Small Sample,seg_113,"if you have microsoft excel, another option is to use the add-in menu data analysis, which has a menu item descriptive statistics. remember to tick the confidence level for the mean box.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        1., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2065,  2017,  2031,  7513, 24970,  1010,  2178,  5724,  2003,
         2000,  2224,  1996,  5587,  1011,  1999, 12183,  2951,  4106,  1010,
         2029,  2038,  1037, 12183,  8875, 22726,  6747,  1012,  3342,  2000,
        16356,  1996,  7023,  2504,  2005,  1996,  2812,  3482,  1012,   102])"
539,1,['table'], Confidence Interval for the Mean in Case of a Small Sample,seg_113,below is the (partial) output from the microsoft excel menu descriptive statistics (table 4.9).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  2812,  1999,  2553,  1997,  1037,  2235,
         7099])","tensor([  101,  2917,  2003,  1996,  1006,  7704,  1007,  6434,  2013,  1996,
         7513, 24970, 12183, 22726,  6747,  1006,  2795,  1018,  1012,  1023,
         1007,  1012,   102])"
540,1,['loss'], Confidence Interval for the Standard Deviation,seg_115,this section can be skipped without loss of continuity.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  2023,  2930,  2064,  2022, 16791,  2302,  3279,  1997, 13717,
         1012,   102])"
541,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution']", Confidence Interval for the Standard Deviation,seg_115,let us assume that the weight of coffee in bags of coffee follows a normal distribution with mean m and standard deviation s. we know neither the mean nor the standard deviation in this normal distribution.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,
        1., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  2292,  2149,  7868,  2008,  1996,  3635,  1997,  4157,  1999,
         8641,  1997,  4157,  4076,  1037,  3671,  4353,  2007,  2812,  1049,
         1998,  3115, 24353,  1055,  1012,  2057,  2113,  4445,  1996,  2812,
         4496,  1996,  3115, 24353,  1999,  2023,  3671,  4353,  1012,   102])"
542,1,"['confidence interval', 'deviation', 'interval', 'estimate', 'standard deviation', 'standard', 'confidence']", Confidence Interval for the Standard Deviation,seg_115,the purpose of this section is to estimate the standard deviation s and find a confidence interval for it.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  1996,  3800,  1997,  2023,  2930,  2003,  2000, 10197,  1996,
         3115, 24353,  1055,  1998,  2424,  1037,  7023, 13483,  2005,  2009,
         1012,   102])"
543,1,"['confidence interval', 'deviation', 'interval', 'standard deviation', 'standard', 'confidence']", Confidence Interval for the Standard Deviation,seg_115,the confidence interval for the standard deviation s is found by specifying the lower and upper endpoint of the interval directly.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  1996,  7023, 13483,  2005,  1996,  3115, 24353,  1055,  2003,
         2179,  2011, 20648,  2075,  1996,  2896,  1998,  3356,  2203,  8400,
         1997,  1996, 13483,  3495,  1012,   102])"
544,1,"['distribution', 'fractiles', 'degrees of freedom']", Confidence Interval for the Standard Deviation,seg_115,"we need fractiles of the chi-squared distribution (*). we also here need to specify the number of degrees of freedom, which is again n 1.",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  2057,  2342, 25312,  6593,  9463,  2015,  1997,  1996,  9610,
         1011, 19942,  4353,  1006,  1008,  1007,  1012,  2057,  2036,  2182,
         2342,  2000, 20648,  1996,  2193,  1997,  5445,  1997,  4071,  1010,
         2029,  2003,  2153,  1050,  1015,  1012,   102])"
545,1,"['confidence interval', 'deviation', 'interval', 'standard deviation', 'standard', 'confidence']", Confidence Interval for the Standard Deviation,seg_115,a 95% confidence interval for the standard deviation s is:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  1037,  5345,  1003,  7023, 13483,  2005,  1996,  3115, 24353,
         1055,  2003,  1024,   102])"
546,0,[], Confidence Interval for the Standard Deviation,seg_115,the denominator under the square root is:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  1996,  7939, 20936, 27413,  2104,  1996,  2675,  7117,  2003,
         1024,   102])"
547,1,"['distribution', 'fractile', 'degrees of freedom', 'limit']", Confidence Interval for the Standard Deviation,seg_115,– 97.5% fractile (lower limit) – 2.5% fractile (upper limit) in a chi-squared distribution with n 1 degrees of freedom,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        1., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  1516,  5989,  1012,  1019,  1003, 25312,  6593,  9463,  1006,
         2896,  5787,  1007,  1516,  1016,  1012,  1019,  1003, 25312,  6593,
         9463,  1006,  3356,  5787,  1007,  1999,  1037,  9610,  1011, 19942,
         4353,  2007,  1050,  1015,  5445,  1997,  4071,   102])"
548,1,"['confidence interval', 'interval', 'fractile', 'degrees of freedom', 'limit', 'distribution', 'confidence']", Confidence Interval for the Standard Deviation,seg_115,"for a 99% confidence interval, we use the 99.5% fractile (lower limit) and the 0.5% fractile (upper limit) in a chi-squared distribution with n 1 degrees of freedom.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  2005,  1037,  5585,  1003,  7023, 13483,  1010,  2057,  2224,
         1996,  5585,  1012,  1019,  1003, 25312,  6593,  9463,  1006,  2896,
         5787,  1007,  1998,  1996,  1014,  1012,  1019,  1003, 25312,  6593,
         9463,  1006,  3356,  5787,  1007,  1999,  1037,  9610,  1011, 19942,
         4353,  2007,  1050,  1015,  5445,  1997,  4071,  1012,   102])"
549,1,"['deviation', 'normal distribution', 'estimate', 'sample', 'normal', 'standard deviation', 'standard', 'distribution']", Confidence Interval for the Standard Deviation,seg_115,"let us assume that the weight of the coffee in bags of coffee follows a normal distribution. we do not know the standard deviation s, but we take a sample of n ¼ 4 coffee bags to estimate it.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  2292,  2149,  7868,  2008,  1996,  3635,  1997,  1996,  4157,
         1999,  8641,  1997,  4157,  4076,  1037,  3671,  4353,  1012,  2057,
         2079,  2025,  2113,  1996,  3115, 24353,  1055,  1010,  2021,  2057,
         2202,  1037,  7099,  1997,  1050,  1091,  1018,  4157,  8641,  2000,
        10197,  2009,  1012,   102])"
550,1,"['confidence interval', 'deviation', 'probability', 'interval', 'standard deviation', 'standard', 'confidence']", Confidence Interval for the Standard Deviation,seg_115,"we need a confidence interval, which with probability 95% contains the unknown value of the standard deviation s.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  2057,  2342,  1037,  7023, 13483,  1010,  2029,  2007,  9723,
         5345,  1003,  3397,  1996,  4242,  3643,  1997,  1996,  3115, 24353,
         1055,  1012,   102])"
551,1,"['confidence interval', 'deviation', 'interval', 'sample', 'standard deviation', 'standard', 'confidence']", Confidence Interval for the Standard Deviation,seg_115,"in the sample, we have a standard deviation of s ¼ 5.30 g. we calculate a 95% confidence interval for the standard deviation from the formula above.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  1999,  1996,  7099,  1010,  2057,  2031,  1037,  3115, 24353,
         1997,  1055,  1091,  1019,  1012,  2382,  1043,  1012,  2057, 18422,
         1037,  5345,  1003,  7023, 13483,  2005,  1996,  3115, 24353,  2013,
         1996,  5675,  2682,  1012,   102])"
552,1,"['sample', 'degrees of freedom']", Confidence Interval for the Standard Deviation,seg_115,"since the sample consists of n ¼ 4 coffee bags, the number of degrees of freedom is df ¼ n 1 ¼ 3.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([ 101, 2144, 1996, 7099, 3774, 1997, 1050, 1091, 1018, 4157, 8641, 1010,
        1996, 2193, 1997, 5445, 1997, 4071, 2003, 1040, 2546, 1091, 1050, 1015,
        1091, 1017, 1012,  102])"
553,1,"['table', 'fractile', 'degrees of freedom']", Confidence Interval for the Standard Deviation,seg_115,"in the table of the t-distribution in chap. 9, we find the 97.5% fractile for a chi-squared distributionwith 3 degrees of freedom as 9.35 and the 2.5% fractile as 0.22.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  1999,  1996,  2795,  1997,  1996,  1056,  1011,  4353,  1999,
        15775,  2361,  1012,  1023,  1010,  2057,  2424,  1996,  5989,  1012,
         1019,  1003, 25312,  6593,  9463,  2005,  1037,  9610,  1011, 19942,
         4353, 24415,  1017,  5445,  1997,  4071,  2004,  1023,  1012,  3486,
         1998,  1996,  1016,  1012,  1019,  1003, 25312,  6593,  9463,  2004,
         1014,  1012,  2570,  1012,   102])"
554,1,"['confidence interval', 'deviation', 'interval', 'standard deviation', 'standard', 'confidence']", Confidence Interval for the Standard Deviation,seg_115,"thus, we get from the formula above the following 95% confidence interval for the standard deviation:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  2947,  1010,  2057,  2131,  2013,  1996,  5675,  2682,  1996,
         2206,  5345,  1003,  7023, 13483,  2005,  1996,  3115, 24353,  1024,
          102])"
555,1,['limit'], Confidence Interval for the Standard Deviation,seg_115,– lower limit is 3.0. – upper limit is 19.8.,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([ 101, 1516, 2896, 5787, 2003, 1017, 1012, 1014, 1012, 1516, 3356, 5787,
        2003, 2539, 1012, 1022, 1012,  102])"
556,1,"['deviation', 'probability', 'standard deviation', 'standard']", Confidence Interval for the Standard Deviation,seg_115,"this means, that with 95% probability, the standard deviation is between 3.0 and 19.8.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  2023,  2965,  1010,  2008,  2007,  5345,  1003,  9723,  1010,
         1996,  3115, 24353,  2003,  2090,  1017,  1012,  1014,  1998,  2539,
         1012,  1022,  1012,   102])"
557,1,"['interval', 'sample', 'sample size']", Confidence Interval for the Standard Deviation,seg_115,"this interval might seem rather wide. this is of course due to the small sample size. if we need a narrower interval, we must increase the sample size.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  2023, 13483,  2453,  4025,  2738,  2898,  1012,  2023,  2003,
         1997,  2607,  2349,  2000,  1996,  2235,  7099,  2946,  1012,  2065,
         2057,  2342,  1037, 22546, 13483,  1010,  2057,  2442,  3623,  1996,
         7099,  2946,  1012,   102])"
558,1,"['confidence interval', 'deviation', 'interval', 'standard deviation', 'standard', 'confidence']", Confidence Interval for the Standard Deviation,seg_115,you need the following for a 95% confidence interval for the standard deviation:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  2017,  2342,  1996,  2206,  2005,  1037,  5345,  1003,  7023,
        13483,  2005,  1996,  3115, 24353,  1024,   102])"
559,1,"['function', 'variance', 'fractile', 'degrees of freedom', 'sample', 'sample size', 'distribution']", Confidence Interval for the Standard Deviation,seg_115,– the variance (calculated using the function var). – the sample size n. – the number of degrees of freedom: df ¼ n 1. – the 2.5% fractile and the 97.5% fractile in the chi-squared distribution with,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  1516,  1996, 23284,  1006, 10174,  2478,  1996,  3853, 13075,
         1007,  1012,  1516,  1996,  7099,  2946,  1050,  1012,  1516,  1996,
         2193,  1997,  5445,  1997,  4071,  1024,  1040,  2546,  1091,  1050,
         1015,  1012,  1516,  1996,  1016,  1012,  1019,  1003, 25312,  6593,
         9463,  1998,  1996,  5989,  1012,  1019,  1003, 25312,  6593,  9463,
         1999,  1996,  9610,  1011, 19942,  4353,  2007,   102])"
560,1,['degrees of freedom'], Confidence Interval for the Standard Deviation,seg_115,n 1 degrees of freedom.,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([ 101, 1050, 1015, 5445, 1997, 4071, 1012,  102])"
561,1,"['function', 'fractiles']", Confidence Interval for the Standard Deviation,seg_115,the fractiles can be calculated in microsoft excel/open office calc using the function chiinv.,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  1996, 25312,  6593,  9463,  2015,  2064,  2022, 10174,  1999,
         7513, 24970,  1013,  2330,  2436, 10250,  2278,  2478,  1996,  3853,
         9610,  2378,  2615,  1012,   102])"
562,1,['probability'], Confidence Interval for the Standard Deviation,seg_115,note that you should specify the “rest” probability rather than the probability itself.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  3602,  2008,  2017,  2323, 20648,  1996,  1523,  2717,  1524,
         9723,  2738,  2084,  1996,  9723,  2993,  1012,   102])"
563,1,"['probability', 'fractile', 'degrees of freedom']", Confidence Interval for the Standard Deviation,seg_115,"example: for the 97.5% ¼ 0.975 fractile, the “rest” probability is 2.5% ¼ 0.025. with 3 degrees of freedom, you get the fractile chiinv(0.025;3) ¼ 9.35 (fig. 4.16).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  1996,  3115, 24353])","tensor([  101,  2742,  1024,  2005,  1996,  5989,  1012,  1019,  1003,  1091,
         1014,  1012,  5989,  2629, 25312,  6593,  9463,  1010,  1996,  1523,
         2717,  1524,  9723,  2003,  1016,  1012,  1019,  1003,  1091,  1014,
         1012,  6185,  2629,  1012,  2007,  1017,  5445,  1997,  4071,  1010,
         2017,  2131,  1996, 25312,  6593,  9463,  9610,  2378,  2615,  1006,
         1014,  1012,  6185,  2629,  1025,  1017,  1007,  1091,  1023,  1012,
         3486,  1006, 20965,  1012,  1018,  1012,  2385,  1007,  1012,   102])"
564,1,"['sample', 'normal', 'sample size', 'data']", More About the Normal Distribution,seg_117,"often, you hear people say that “you need at least a certain sample size in order that data should follow a normal distribution”. this is nonsense!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([  101,  2411,  1010,  2017,  2963,  2111,  2360,  2008,  1523,  2017,
         2342,  2012,  2560,  1037,  3056,  7099,  2946,  1999,  2344,  2008,
         2951,  2323,  3582,  1037,  3671,  4353,  1524,  1012,  2023,  2003,
        14652,   999,   102])"
565,1,"['population', 'sample', 'distribution']", More About the Normal Distribution,seg_117,"data from a sample of a population will follow the same distribution, no matter the size of the sample!",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([ 101, 2951, 2013, 1037, 7099, 1997, 1037, 2313, 2097, 3582, 1996, 2168,
        4353, 1010, 2053, 3043, 1996, 2946, 1997, 1996, 7099,  999,  102])"
566,0,[], More About the Normal Distribution,seg_117,"however, one can show the following:",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([ 101, 2174, 1010, 2028, 2064, 2265, 1996, 2206, 1024,  102])"
567,1,"['mean', 'confidence interval', 'normal distribution', 'interval', 'approximation', 'population', 'sample', 'sample average', 'normal', 'sample size', 'limit', 'distribution', 'confidence', 'average', 'data']", More About the Normal Distribution,seg_117,"when data do not follow a normal distribution: the calculation of the confidence interval for the mean as shown in this chapter can still be used. the sample average will follow a normal distribution, if the sample size is large. this is just one of the reasons why the normal distribution is so important: the normal distribution can be used as an approximation to describe the sample average, regardless of the distribution of data in the population. the literature refers to this as “the central limit theorem”.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([  101,  2043,  2951,  2079,  2025,  3582,  1037,  3671,  4353,  1024,
         1996, 17208,  1997,  1996,  7023, 13483,  2005,  1996,  2812,  2004,
         3491,  1999,  2023,  3127,  2064,  2145,  2022,  2109,  1012,  1996,
         7099,  2779,  2097,  3582,  1037,  3671,  4353,  1010,  2065,  1996,
         7099,  2946,  2003,  2312,  1012,  2023,  2003,  2074,  2028,  1997,
         1996,  4436,  2339,  1996,  3671,  4353,  2003,  2061,  2590,  1024,
         1996,  3671,  4353,  2064,  2022,  2109,  2004,  2019, 20167,  2000,
         6235,  1996,  7099,  2779,  1010,  7539,  1997,  1996,  4353,  1997,
         2951,  1999,  1996,  2313,  1012,  1996,  3906,  5218,  2000,  2023,
         2004,  1523,  1996,  2430,  5787,  9872,  1524,  1012,   102])"
568,1,"['skewed', 'population', 'sample', 'sample size', 'distribution', 'data']", More About the Normal Distribution,seg_117,"the requirement to the sample size is not huge: a sample size of 5 will often be enough. however, if the distribution of data in the population is extremely skewed, a larger sample size (e.g., 20) may be necessary.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([  101,  1996,  9095,  2000,  1996,  7099,  2946,  2003,  2025,  4121,
         1024,  1037,  7099,  2946,  1997,  1019,  2097,  2411,  2022,  2438,
         1012,  2174,  1010,  2065,  1996,  4353,  1997,  2951,  1999,  1996,
         2313,  2003,  5186, 15315,  7974,  2098,  1010,  1037,  3469,  7099,
         2946,  1006,  1041,  1012,  1043,  1012,  1010,  2322,  1007,  2089,
         2022,  4072,  1012,   102])"
569,1,"['mean', 'confidence interval', 'normal distribution', 'fractiles', 'interval', 'sample', 'normal', 'sample size', 'distribution', 'confidence', 'data']", More About the Normal Distribution,seg_117,"furthermore: even if data follow a normal distribution, we need a certain sample size in order to use fractiles from the normal distribution when constructing a confidence interval for the mean.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        1., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([  101,  7297,  1024,  2130,  2065,  2951,  3582,  1037,  3671,  4353,
         1010,  2057,  2342,  1037,  3056,  7099,  2946,  1999,  2344,  2000,
         2224, 25312,  6593,  9463,  2015,  2013,  1996,  3671,  4353,  2043,
        15696,  1037,  7023, 13483,  2005,  1996,  2812,  1012,   102])"
570,1,"['fractiles', 'sample', 'sample size', 'case']", More About the Normal Distribution,seg_117,"we have seen that for small sample sizes we must use fractiles from the t-distribution. this is the case, when the sample size is up to approx. 20.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([  101,  2057,  2031,  2464,  2008,  2005,  2235,  7099, 10826,  2057,
         2442,  2224, 25312,  6593,  9463,  2015,  2013,  1996,  1056,  1011,
         4353,  1012,  2023,  2003,  1996,  2553,  1010,  2043,  1996,  7099,
         2946,  2003,  2039,  2000, 22480,  1012,  2322,  1012,   102])"
571,1,"['sample', 'sample size']", More About the Normal Distribution,seg_117,"if the sample size is larger than 20, you are assured that:",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([ 101, 2065, 1996, 7099, 2946, 2003, 3469, 2084, 2322, 1010, 2017, 2024,
        8916, 2008, 1024,  102])"
572,1,"['mean', 'normal distribution', 'fractiles', 'sample average', 'sample', 'normal', 'intervals', 'distribution', 'confidence intervals', 'confidence', 'average']", More About the Normal Distribution,seg_117,1. the sample average will follow a normal distribution. 2. you can use fractiles from the normal distribution to construct confidence intervals for the mean.,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([  101,  1015,  1012,  1996,  7099,  2779,  2097,  3582,  1037,  3671,
         4353,  1012,  1016,  1012,  2017,  2064,  2224, 25312,  6593,  9463,
         2015,  2013,  1996,  3671,  4353,  2000,  9570,  7023, 14025,  2005,
         1996,  2812,  1012,   102])"
573,1,"['normal distribution', 'data', 'normal', 'distribution', 'statistical']", More About the Normal Distribution,seg_117,"many statistical methods (e.g., the methods described in chaps. 7 and 8) require that data follow a normal distribution. this is another reason why the normal distribution is so important.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([  101,  2116,  7778,  4725,  1006,  1041,  1012,  1043,  1012,  1010,
         1996,  4725,  2649,  1999, 15775,  4523,  1012,  1021,  1998,  1022,
         1007,  5478,  2008,  2951,  3582,  1037,  3671,  4353,  1012,  2023,
         2003,  2178,  3114,  2339,  1996,  3671,  4353,  2003,  2061,  2590,
         1012,   102])"
574,1,"['normal distribution', 'transforming', 'transform', 'lognormal', 'distributions', 'transformed', 'normal', 'distribution', 'lognormal distribution', 'data', 'case']", More About the Normal Distribution,seg_117,"if data do not follow a normal distribution, one solution (which often can be used for right-skewed distributions) is to transform the data values (e.g., taking the logarithm of data values), so that they can be described by a normal distribution, and then doing the calculations on the transformed data. in case of transforming with the logarithm, we say that the original data follow a lognormal distribution.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 1., 1., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([  101,  2065,  2951,  2079,  2025,  3582,  1037,  3671,  4353,  1010,
         2028,  5576,  1006,  2029,  2411,  2064,  2022,  2109,  2005,  2157,
         1011, 15315,  7974,  2098, 20611,  1007,  2003,  2000, 10938,  1996,
         2951,  5300,  1006,  1041,  1012,  1043,  1012,  1010,  2635,  1996,
         8833,  8486,  2705,  2213,  1997,  2951,  5300,  1007,  1010,  2061,
         2008,  2027,  2064,  2022,  2649,  2011,  1037,  3671,  4353,  1010,
         1998,  2059,  2725,  1996, 16268,  2006,  1996,  8590,  2951,  1012,
         1999,  2553,  1997, 17903,  2007,  1996,  8833,  8486,  2705,  2213,
         1010,  2057,  2360,  2008,  1996,  2434,  2951,  3582,  1037,  8833,
        12131,  9067,  4353,  1012,   102])"
575,1,"['statistical', 'statistics', 'distribution']", More About the Normal Distribution,seg_117,"another approach is to use nonparametric statistical techniques (also referred to as distribution free techniques). these techniques are described in more advanced books on statistics, and they are implemented in many statistical software packages.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([  101,  2178,  3921,  2003,  2000,  2224,  2512, 28689, 12589,  7778,
         5461,  1006,  2036,  3615,  2000,  2004,  4353,  2489,  5461,  1007,
         1012,  2122,  5461,  2024,  2649,  1999,  2062,  3935,  2808,  2006,
         6747,  1010,  1998,  2027,  2024,  7528,  1999,  2116,  7778,  4007,
        14555,  1012,   102])"
576,1,"['data', 'quantitative']", More About the Normal Distribution,seg_117,"in this chapter, we have primarily dealt with quantitative data, i.e., data values are numbers that can be used for calculations.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([  101,  1999,  2023,  3127,  1010,  2057,  2031,  3952,  9411,  2007,
        20155,  2951,  1010,  1045,  1012,  1041,  1012,  1010,  2951,  5300,
         2024,  3616,  2008,  2064,  2022,  2109,  2005, 16268,  1012,   102])"
577,1,"['normal distribution', 'uncertainty', 'associated', 'data', 'statistical uncertainty', 'quantitative', 'sample', 'sample average', 'normal', 'distribution', 'statistical', 'average']", More About the Normal Distribution,seg_117,"we have seen how to describe quantitative data using the normal distribution, and how we check for the normal distribution. also, we have seen how we can find the statistical uncertainty associated with a sample average.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 1., 1., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([  101,  2057,  2031,  2464,  2129,  2000,  6235, 20155,  2951,  2478,
         1996,  3671,  4353,  1010,  1998,  2129,  2057,  4638,  2005,  1996,
         3671,  4353,  1012,  2036,  1010,  2057,  2031,  2464,  2129,  2057,
         2064,  2424,  1996,  7778, 12503,  3378,  2007,  1037,  7099,  2779,
         1012,   102])"
578,1,"['data', 'population', 'statistical']", More About the Normal Distribution,seg_117,"in the next chapter, we deal with the statistical methods for qualitative data corresponding to groups in the population.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0.])","tensor([2062, 2055, 1996, 3671, 4353])","tensor([  101,  1999,  1996,  2279,  3127,  1010,  2057,  3066,  2007,  1996,
         7778,  4725,  2005, 24209, 11475, 27453,  2951,  7978,  2000,  2967,
         1999,  1996,  2313,  1012,   102])"
579,1,"['data', 'population']",Chapter  Analysis of Qualitative Data,seg_119,"in this chapter we look at qualitative data, i.e., data values correspond to groups in the population. one particularly important type of qualitative data is alternative (binary) data with only two groups (“alternatives”).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3127,  4106,  1997, 24209, 11475, 27453,  2951])","tensor([  101,  1999,  2023,  3127,  2057,  2298,  2012, 24209, 11475, 27453,
         2951,  1010,  1045,  1012,  1041,  1012,  1010,  2951,  5300, 17254,
         2000,  2967,  1999,  1996,  2313,  1012,  2028,  3391,  2590,  2828,
         1997, 24209, 11475, 27453,  2951,  2003,  4522,  1006, 12441,  1007,
         2951,  2007,  2069,  2048,  2967,  1006,  1523, 15955,  1524,  1007,
         1012,   102])"
580,1,['data'],Chapter  Analysis of Qualitative Data,seg_119,some examples of alternative data:,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 3127,  4106,  1997, 24209, 11475, 27453,  2951])","tensor([ 101, 2070, 4973, 1997, 4522, 2951, 1024,  102])"
581,1,['sample'],Chapter  Analysis of Qualitative Data,seg_119,"– sample surveys: for example, questionnaire surveys: the answers “yes”/“no”",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 3127,  4106,  1997, 24209, 11475, 27453,  2951])","tensor([  101,  1516,  7099, 12265,  1024,  2005,  2742,  1010,  3160, 20589,
        12265,  1024,  1996,  6998,  1523,  2748,  1524,  1013,  1523,  2053,
         1524,   102])"
582,1,"['categories', 'control', 'quality control', 'statistical']",Chapter  Analysis of Qualitative Data,seg_119,"to a question. – statistical quality control: for instance, classification of items in the categories",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0.])","tensor([ 3127,  4106,  1997, 24209, 11475, 27453,  2951])","tensor([ 101, 2000, 1037, 3160, 1012, 1516, 7778, 3737, 2491, 1024, 2005, 6013,
        1010, 5579, 1997, 5167, 1999, 1996, 7236,  102])"
583,1,"['data', 'distribution', 'statistical', 'binomial', 'binomial distribution']",Chapter  Analysis of Qualitative Data,seg_119,"alternative data are described by a statistical distribution called the binomial distribution (*). it is used very often when analyzing data from surveys, but also in many other contexts, such as social sciences, economics, administration, science and technology.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3127,  4106,  1997, 24209, 11475, 27453,  2951])","tensor([  101,  4522,  2951,  2024,  2649,  2011,  1037,  7778,  4353,  2170,
         1996,  8026, 20936,  2389,  4353,  1006,  1008,  1007,  1012,  2009,
         2003,  2109,  2200,  2411,  2043, 20253,  2951,  2013, 12265,  1010,
         2021,  2036,  1999,  2116,  2060, 18046,  1010,  2107,  2004,  2591,
         4163,  1010,  5543,  1010,  3447,  1010,  2671,  1998,  2974,  1012,
          102])"
584,1,"['binomial distribution', 'binomial', 'distribution']", The Binomial Distribution,seg_121,"the situation, in which the binomial distribution is used, can be characterized as follows:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353])","tensor([  101,  1996,  3663,  1010,  1999,  2029,  1996,  8026, 20936,  2389,
         4353,  2003,  2109,  1010,  2064,  2022,  7356,  2004,  4076,  1024,
          102])"
585,1,"['categories', 'observation', 'probability', 'observations', 'control', 'quality control', 'statistical', 'independent']", The Binomial Distribution,seg_121,"– each observation (“trial”) can be classified into two categories. often, we call them “success” and “failure” regardless of whether one of the categories can be said to be “better” than the other. – the probability that an observation is classified as “success” is constant. for example, in statistical quality control there must not be a trend that defective items become more frequent. – the observations are independent. this means, for example, that two respondents do not affect each other’s answers in a questionnaire survey.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353])","tensor([  101,  1516,  2169,  8089,  1006,  1523,  3979,  1524,  1007,  2064,
         2022,  6219,  2046,  2048,  7236,  1012,  2411,  1010,  2057,  2655,
         2068,  1523,  3112,  1524,  1998,  1523,  4945,  1524,  7539,  1997,
         3251,  2028,  1997,  1996,  7236,  2064,  2022,  2056,  2000,  2022,
         1523,  2488,  1524,  2084,  1996,  2060,  1012,  1516,  1996,  9723,
         2008,  2019,  8089,  2003,  6219,  2004,  1523,  3112,  1524,  2003,
         5377,  1012,  2005,  2742,  1010,  1999,  7778,  3737,  2491,  2045,
         2442,  2025,  2022,  1037,  9874,  2008, 28829,  5167,  2468,  2062,
         6976,  1012,  1516,  1996,  9420,  2024,  2981,  1012,  2023,  2965,
         1010,  2005,  2742,  1010,  2008,  2048, 25094,  2079,  2025,  7461,
         2169,  2060,  1521,  1055,  6998,  1999,  1037,  3160, 20589,  5002,
         1012,   102])"
586,1,"['probability theory', 'probability', 'observations', 'probability of success', 'successes', 'success']", The Binomial Distribution,seg_121,"using probability theory [see chap. 9], one can calculate the probability of getting exactly x successes out of n observations, given that the constant probability of success is p.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353])","tensor([  101,  2478,  9723,  3399,  1031,  2156, 15775,  2361,  1012,  1023,
         1033,  1010,  2028,  2064, 18422,  1996,  9723,  1997,  2893,  3599,
         1060, 14152,  2041,  1997,  1050,  9420,  1010,  2445,  2008,  1996,
         5377,  9723,  1997,  3112,  2003,  1052,  1012,   102])"
587,1,"['function', 'probabilities', 'distribution', 'statistical', 'binomial', 'binomial distribution']", The Binomial Distribution,seg_121,"the probabilities of the binomial distribution are tabulated in many books (for small values of n). they can also be calculated in most spreadsheets using a statistical function, see later.",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353])","tensor([  101,  1996,  4013,  3676, 14680,  1997,  1996,  8026, 20936,  2389,
         4353,  2024, 21628,  8898,  1999,  2116,  2808,  1006,  2005,  2235,
         5300,  1997,  1050,  1007,  1012,  2027,  2064,  2036,  2022, 10174,
         1999,  2087, 20861, 21030,  3215,  2478,  1037,  7778,  3853,  1010,
         2156,  2101,  1012,   102])"
588,0,[], The Binomial Distribution,seg_121,"also, one can show the following:",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 1996,  8026, 20936,  2389,  4353])","tensor([ 101, 2036, 1010, 2028, 2064, 2265, 1996, 2206, 1024,  102])"
589,1,"['observations', 'number of observations', 'distribution', 'binomial', 'success', 'binomial distribution']", The Binomial Distribution,seg_121,in a binomial distribution with the number of observations ¼ n and probability of success ¼ p:,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353])","tensor([  101,  1999,  1037,  8026, 20936,  2389,  4353,  2007,  1996,  2193,
         1997,  9420,  1091,  1050,  1998,  9723,  1997,  3112,  1091,  1052,
         1024,   102])"
590,1,['probability'], Example,seg_123,let us consider a dice where the probability of getting six eyes is 1/6 (approx. 16.7%). we throw the dice 48 times in total. we think of six eyes as “success” and everything else as a “failure.”,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2292,  2149,  5136,  1037, 18740,  2073,  1996,  9723,  1997,
         2893,  2416,  2159,  2003,  1015,  1013,  1020,  1006, 22480,  1012,
         2385,  1012,  1021,  1003,  1007,  1012,  2057,  5466,  1996, 18740,
         4466,  2335,  1999,  2561,  1012,  2057,  2228,  1997,  2416,  2159,
         2004,  1523,  3112,  1524,  1998,  2673,  2842,  2004,  1037,  1523,
         4945,  1012,  1524,   102])"
591,1,['binomial'], Example,seg_123,"in other words, the number of throws with six eyes follows a binomial distribution with",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0.])",tensor([2742]),"tensor([  101,  1999,  2060,  2616,  1010,  1996,  2193,  1997, 11618,  2007,
         2416,  2159,  4076,  1037,  8026, 20936,  2389,  4353,  2007,   102])"
592,1,['probability'], Example,seg_123,– n ¼ 48 ¼ number of throws – p ¼ 1/6 ¼ 16.7% ¼ probability of six eyes in each throw.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1516,  1050,  1091,  4466,  1091,  2193,  1997, 11618,  1516,
         1052,  1091,  1015,  1013,  1020,  1091,  2385,  1012,  1021,  1003,
         1091,  9723,  1997,  2416,  2159,  1999,  2169,  5466,  1012,   102])"
593,1,['distribution'], Example,seg_123,"in this distribution, we have:",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([ 101, 1999, 2023, 4353, 1010, 2057, 2031, 1024,  102])"
594,1,['mean'], Example,seg_123,1 mean ¼ 48 ¼ 8: 6,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([ 101, 1015, 2812, 1091, 4466, 1091, 1022, 1024, 1020,  102])"
595,1,['average'], Example,seg_123,"that is, on average we will have 8 out of 48 throws with six eyes, which probably is not surprising.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2008,  2003,  1010,  2006,  2779,  2057,  2097,  2031,  1022,
         2041,  1997,  4466, 11618,  2007,  2416,  2159,  1010,  2029,  2763,
         2003,  2025, 11341,  1012,   102])"
596,1,['variance'], Example,seg_123,1 5 5 variance ¼ 48 ¼ 8 ¼ app: 6:67: 6 6 6,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])",tensor([2742]),"tensor([  101,  1015,  1019,  1019, 23284,  1091,  4466,  1091,  1022,  1091,
        10439,  1024,  1020,  1024,  6163,  1024,  1020,  1020,  1020,   102])"
597,1,"['deviation', 'variance', 'standard deviation', 'standard']", Example,seg_123,"the standard deviation is the square root of the variance, i.e.:",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  3115, 24353,  2003,  1996,  2675,  7117,  1997,  1996,
        23284,  1010,  1045,  1012,  1041,  1012,  1024,   102])"
598,1,"['probabilities', 'distribution', 'binomial', 'binomial distribution']", Example,seg_123,fig. 5.1 shows the probabilities of this binomial distribution. it can be seen that it is very unlikely to get more than (about) 20 throws out of 48 with six eyes.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101, 20965,  1012,  1019,  1012,  1015,  3065,  1996,  4013,  3676,
        14680,  1997,  2023,  8026, 20936,  2389,  4353,  1012,  2009,  2064,
         2022,  2464,  2008,  2009,  2003,  2200,  9832,  2000,  2131,  2062,
         2084,  1006,  2055,  1007,  2322, 11618,  2041,  1997,  4466,  2007,
         2416,  2159,  1012,   102])"
599,1,"['mean', 'distribution']", Example,seg_123,"as expected, the distribution is concentrated around 8 (the mean).",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])",tensor([2742]),"tensor([ 101, 2004, 3517, 1010, 1996, 4353, 2003, 8279, 2105, 1022, 1006, 1996,
        2812, 1007, 1012,  102])"
600,1,"['normal distribution', 'normal', 'distribution', 'binomial', 'binomial distribution']", The Binomial Distribution and the Normal Distribution,seg_125,fig. 5.2 illustrates that the binomial distribution shows some similarity with the normal distribution.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        1., 1., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353,  1998,  1996,  3671,  4353])","tensor([  101, 20965,  1012,  1019,  1012,  1016, 24899,  2008,  1996,  8026,
        20936,  2389,  4353,  3065,  2070, 14402,  2007,  1996,  3671,  4353,
         1012,   102])"
601,1,"['binomial distributions', 'distributions', 'distribution', 'binomial']", The Binomial Distribution and the Normal Distribution,seg_125,"the figure shows two binomial distributions. both distributions have p ¼ 0.5, equivalent to tossing a coin. one distribution is with n ¼ 20 tosses and the other is with n ¼ 50 tosses.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353,  1998,  1996,  3671,  4353])","tensor([  101,  1996,  3275,  3065,  2048,  8026, 20936,  2389, 20611,  1012,
         2119, 20611,  2031,  1052,  1091,  1014,  1012,  1019,  1010,  5662,
         2000, 15021,  1037,  9226,  1012,  2028,  4353,  2003,  2007,  1050,
         1091,  2322, 10055,  2229,  1998,  1996,  2060,  2003,  2007,  1050,
         1091,  2753, 10055,  2229,  1012,   102])"
602,1,"['distribution', 'normal distribution', 'normal', 'chart']", The Binomial Distribution and the Normal Distribution,seg_125,"the chart shows that the similarity with a normal distribution is improving, the larger the n is.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353,  1998,  1996,  3671,  4353])","tensor([  101,  1996,  3673,  3065,  2008,  1996, 14402,  2007,  1037,  3671,
         4353,  2003,  9229,  1010,  1996,  3469,  1996,  1050,  2003,  1012,
          102])"
603,1,"['binomial distribution', 'binomial', 'distribution']", The Binomial Distribution and the Normal Distribution,seg_125,"sometimes we have a binomial distribution, where p is not 0.5 (as in the example with throwing a dice).",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353,  1998,  1996,  3671,  4353])","tensor([  101,  2823,  2057,  2031,  1037,  8026, 20936,  2389,  4353,  1010,
         2073,  1052,  2003,  2025,  1014,  1012,  1019,  1006,  2004,  1999,
         1996,  2742,  2007,  6886,  1037, 18740,  1007,  1012,   102])"
604,1,"['normal distribution', 'normal', 'distribution', 'binomial', 'binomial distribution']", The Binomial Distribution and the Normal Distribution,seg_125,"if p is close to 0 or 1, n must be very large before the binomial distribution becomes similar to a normal distribution.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353,  1998,  1996,  3671,  4353])","tensor([  101,  2065,  1052,  2003,  2485,  2000,  1014,  2030,  1015,  1010,
         1050,  2442,  2022,  2200,  2312,  2077,  1996,  8026, 20936,  2389,
         4353,  4150,  2714,  2000,  1037,  3671,  4353,  1012,   102])"
605,1,"['normal distribution', 'approximation', 'normal', 'distribution', 'binomial', 'binomial distribution']", The Binomial Distribution and the Normal Distribution,seg_125,"as a rule of thumb, we have the following: the normal distribution can be used as an approximation to the binomial distribution, if n p > 5 and n(1 p) > 5",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353,  1998,  1996,  3671,  4353])","tensor([  101,  2004,  1037,  3627,  1997,  7639,  1010,  2057,  2031,  1996,
         2206,  1024,  1996,  3671,  4353,  2064,  2022,  2109,  2004,  2019,
        20167,  2000,  1996,  8026, 20936,  2389,  4353,  1010,  2065,  1050,
         1052,  1028,  1019,  1998,  1050,  1006,  1015,  1052,  1007,  1028,
         1019,   102])"
606,1,['mean'], The Binomial Distribution and the Normal Distribution,seg_125,"note that n p is precisely the mean of the number of “successes” (e.g., throws with six eyes).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353,  1998,  1996,  3671,  4353])","tensor([  101,  3602,  2008,  1050,  1052,  2003, 10785,  1996,  2812,  1997,
         1996,  2193,  1997,  1523, 14152,  1524,  1006,  1041,  1012,  1043,
         1012,  1010, 11618,  2007,  2416,  2159,  1007,  1012,   102])"
607,1,['mean'], The Binomial Distribution and the Normal Distribution,seg_125,"similarly, n(1 p) is the mean of the number of “failures” (e.g., throws with less than six eyes).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353,  1998,  1996,  3671,  4353])","tensor([  101,  6660,  1010,  1050,  1006,  1015,  1052,  1007,  2003,  1996,
         2812,  1997,  1996,  2193,  1997,  1523, 15428,  1524,  1006,  1041,
         1012,  1043,  1012,  1010, 11618,  2007,  2625,  2084,  2416,  2159,
         1007,  1012,   102])"
608,1,"['mean', 'deviation', 'normal distribution', 'normal', 'standard deviation', 'standard', 'distribution', 'binomial', 'binomial distribution']", The Binomial Distribution and the Normal Distribution,seg_125,"instead of the binomial distribution, we can use a normal distribution with the same mean and standard deviation. see an example later in this chapter.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353,  1998,  1996,  3671,  4353])","tensor([  101,  2612,  1997,  1996,  8026, 20936,  2389,  4353,  1010,  2057,
         2064,  2224,  1037,  3671,  4353,  2007,  1996,  2168,  2812,  1998,
         3115, 24353,  1012,  2156,  2019,  2742,  2101,  1999,  2023,  3127,
         1012,   102])"
609,1,"['hypergeometric', 'population', 'sampling ', 'binomial distribution', 'hypergeometric distribution', 'sampling', 'statistics', 'samples', 'binomial', 'with replacement', 'probability', 'without replacement', 'sample', 'distribution', 'replacement']", The Binomial Distribution and the Normal Distribution,seg_125,"technical note: sampling with or without replacement? often, we take samples from a specific population with a (finite) number of individuals. ideally, sampling (*) should be carried out with replacement in order to use the binomial distribution to describe the distribution of the number of individuals with a certain characteristic (e.g., people with a specific hobby). replacement means that each individual is “put back” before selecting the next individual of the sample. this way you can, in theory, select an individual twice (or maybe several times) in the sample, which means constant probability of getting individuals with that characteristic. usually, the sample is relatively small compared to the population, for example, less than 10% of the population. if we use sampling with replacement in this situation, the probability of selecting an individual twice (or several times) will be very small. therefore, it corresponds roughly to sampling without replacement, i.e., each individual can only be selected once. in practice, the vast majority of samples are selected without replacement. also, the vast majority of samples are small compared to the population, typically less than 10% of the population. in this situation it is therefore possible to use the binomial distribution, although in principle it can only be used in conjunction with sampling with replacement. if the sample is larger than 10% of the population and sampling is without replacement, one must use the hypergeometric distribution, which is considerably more complicated than the binomial distribution. if the sample is small compared to the population, the hypergeometric distribution is very similar to the binomial distribution. we refer to more advanced books on statistics.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,
        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353,  1998,  1996,  3671,  4353])","tensor([  101,  4087,  3602,  1024, 16227,  2007,  2030,  2302,  6110,  1029,
         2411,  1010,  2057,  2202,  8168,  2013,  1037,  3563,  2313,  2007,
         1037,  1006, 10713,  1007,  2193,  1997,  3633,  1012, 28946,  1010,
        16227,  1006,  1008,  1007,  2323,  2022,  3344,  2041,  2007,  6110,
         1999,  2344,  2000,  2224,  1996,  8026, 20936,  2389,  4353,  2000,
         6235,  1996,  4353,  1997,  1996,  2193,  1997,  3633,  2007,  1037,
         3056,  8281,  1006,  1041,  1012,  1043,  1012,  1010,  2111,  2007,
         1037,  3563, 17792,  1007,  1012,  6110,  2965,  2008,  2169,  3265,
         2003,  1523,  2404,  2067,  1524,  2077, 17739,  1996,  2279,  3265,
         1997,  1996,  7099,  1012,  2023,  2126,  2017,  2064,  1010,  1999,
         3399,  1010,  7276,  2019,  3265,  3807,  1006,  2030,  2672,  2195,
         2335,  1007,  1999,  1996,  7099,  1010,  2029,  2965,  5377,  9723,
         1997,  2893,  3633,  2007,  2008,  8281,  1012,  2788,  1010,  1996,
         7099,  2003,  4659,  2235,  4102,  2000,  1996,  2313,  1010,  2005,
         2742,  1010,  2625,  2084,  2184,  1003,  1997,  1996,  2313,  1012,
         2065,  2057,  2224, 16227,  2007,  6110,  1999,  2023,  3663,  1010,
         1996,  9723,  1997, 17739,  2019,  3265,  3807,  1006,  2030,  2195,
         2335,  1007,  2097,  2022,  2200,  2235,  1012,  3568,  1010,  2009,
        14788,  5560,  2000, 16227,  2302,  6110,  1010,  1045,  1012,  1041,
         1012,  1010,  2169,  3265,  2064,  2069,  2022,  3479,  2320,  1012,
         1999,  3218,  1010,  1996,  6565,  3484,  1997,  8168,  2024,  3479,
         2302,  6110,  1012,  2036,  1010,  1996,  6565,  3484,  1997,  8168,
         2024,  2235,  4102,  2000,  1996,  2313,  1010,  4050,  2625,  2084,
         2184,  1003,  1997,  1996,  2313,  1012,  1999,  2023,  3663,  2009,
         2003,  3568,  2825,  2000,  2224,  1996,  8026, 20936,  2389,  4353,
         1010,  2348,  1999,  6958,  2009,  2064,  2069,  2022,  2109,  1999,
         9595,  2007, 16227,  2007,  6110,  1012,  2065,  1996,  7099,  2003,
         3469,  2084,  2184,  1003,  1997,  1996,  2313,  1998, 16227,  2003,
         2302,  6110,  1010,  2028,  2442,  2224,  1996, 23760,  3351, 28993,
         4353,  1010,  2029,  2003,  9839,  2062,  8552,  2084,  1996,  8026,
        20936,  2389,  4353,  1012,  2065,  1996,  7099,  2003,  2235,  4102,
         2000,  1996,  2313,  1010,  1996, 23760,  3351, 28993,  4353,  2003,
         2200,  2714,  2000,  1996,  8026, 20936,  2389,  4353,  1012,  2057,
         6523,  2000,  2062,  3935,  2808,  2006,  6747,  1012,   102])"
610,0,[], The Binomial Distribution in Spreadsheets,seg_127,this section can be skipped if you do not use spreadsheets.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 1996,  8026, 20936,  2389,  4353,  1999, 20861, 21030,  3215])","tensor([  101,  2023,  2930,  2064,  2022, 16791,  2065,  2017,  2079,  2025,
         2224, 20861, 21030,  3215,  1012,   102])"
611,1,"['function', 'probabilities', 'distribution', 'binomial', 'binomial distribution']", The Binomial Distribution in Spreadsheets,seg_127,"in microsoft excel and open office calc, the following function determines probabilities of the binomial distribution:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,
        0., 1., 1., 1., 1., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353,  1999, 20861, 21030,  3215])","tensor([  101,  1999,  7513, 24970,  1998,  2330,  2436, 10250,  2278,  1010,
         1996,  2206,  3853, 16463,  4013,  3676, 14680,  1997,  1996,  8026,
        20936,  2389,  4353,  1024,   102])"
612,1,"['function', 'table', 'probability', 'probabilities', 'sample', 'sample size', 'successes']", The Binomial Distribution in Spreadsheets,seg_127,"binomdist (value; sample size; probability; cumulative). the function can calculate probabilities (e.g., probability of exactly two successes) and cumulative probabilities (such as the probability of maximum two successes, i.e., 0, 1 or 2 successes) (table 5.1).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 1996,  8026, 20936,  2389,  4353,  1999, 20861, 21030,  3215])","tensor([  101,  8026,  5358, 10521,  2102,  1006,  3643,  1025,  7099,  2946,
         1025,  9723,  1025, 23260,  1007,  1012,  1996,  3853,  2064, 18422,
         4013,  3676, 14680,  1006,  1041,  1012,  1043,  1012,  1010,  9723,
         1997,  3599,  2048, 14152,  1007,  1998, 23260,  4013,  3676, 14680,
         1006,  2107,  2004,  1996,  9723,  1997,  4555,  2048, 14152,  1010,
         1045,  1012,  1041,  1012,  1010,  1014,  1010,  1015,  2030,  1016,
        14152,  1007,  1006,  2795,  1019,  1012,  1015,  1007,  1012,   102])"
613,0,[], Example,seg_129,"as an example,we throwa dice four times and count the number of throwswith six eyes.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])",tensor([2742]),"tensor([  101,  2004,  2019,  2742,  1010,  2057,  5466,  2050, 18740,  2176,
         2335,  1998,  4175,  1996,  2193,  1997, 11618, 24415,  2416,  2159,
         1012,   102])"
614,1,['binomial'], Example,seg_129,"in other words, the number of throws with six eyes follows a binomial distribution with:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0.])",tensor([2742]),"tensor([  101,  1999,  2060,  2616,  1010,  1996,  2193,  1997, 11618,  2007,
         2416,  2159,  4076,  1037,  8026, 20936,  2389,  4353,  2007,  1024,
          102])"
615,1,['probability'], Example,seg_129,– n ¼ 4 throws – p ¼ 1/6 ¼ 16.7% ¼ probability of six eyes in each throw,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1516,  1050,  1091,  1018, 11618,  1516,  1052,  1091,  1015,
         1013,  1020,  1091,  2385,  1012,  1021,  1003,  1091,  9723,  1997,
         2416,  2159,  1999,  2169,  5466,   102])"
616,1,['probability'], Example,seg_129,– the probability of maximum two throws with six eyes – the probability of exactly two throws with six eyes,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1516,  1996,  9723,  1997,  4555,  2048, 11618,  2007,  2416,
         2159,  1516,  1996,  9723,  1997,  3599,  2048, 11618,  2007,  2416,
         2159,   102])"
617,1,"['probability', 'information']", Example,seg_129,"we enter the information in a spreadsheet as shown below (fig. 5.3). we find that the probability of max. 2 (i.e., 0, 1 or 2) throws with six eyes is approx. 98%, i.e., it is very unlikely to have 3 or 4 throws (out of 4) with six eyes,",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2057,  4607,  1996,  2592,  1999,  1037, 20861, 21030,  2102,
         2004,  3491,  2917,  1006, 20965,  1012,  1019,  1012,  1017,  1007,
         1012,  2057,  2424,  2008,  1996,  9723,  1997,  4098,  1012,  1016,
         1006,  1045,  1012,  1041,  1012,  1010,  1014,  1010,  1015,  2030,
         1016,  1007, 11618,  2007,  2416,  2159,  2003, 22480,  1012,  5818,
         1003,  1010,  1045,  1012,  1041,  1012,  1010,  2009,  2003,  2200,
         9832,  2000,  2031,  1017,  2030,  1018, 11618,  1006,  2041,  1997,
         1018,  1007,  2007,  2416,  2159,  1010,   102])"
618,1,['probability'], Example,seg_129,which is hardly surprising. the probability of getting exactly 2 (out of 4) throws with six eyes is almost 12%.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2029,  2003,  6684, 11341,  1012,  1996,  9723,  1997,  2893,
         3599,  1016,  1006,  2041,  1997,  1018,  1007, 11618,  2007,  2416,
         2159,  2003,  2471,  2260,  1003,  1012,   102])"
619,1,"['mean', 'function', 'deviation', 'sample', 'normal', 'standard deviation', 'standard', 'sample size']", Example,seg_129,"note: the function binomdist can be used only when the sample size, n, is up to approximately 1,000. if n is much larger than in 1,000, we must approximate with the normal distributionwith the same mean and standard deviation. see an example later in this chapter.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  3602,  1024,  1996,  3853,  8026,  5358, 10521,  2102,  2064,
         2022,  2109,  2069,  2043,  1996,  7099,  2946,  1010,  1050,  1010,
         2003,  2039,  2000,  3155,  1015,  1010,  2199,  1012,  2065,  1050,
         2003,  2172,  3469,  2084,  1999,  1015,  1010,  2199,  1010,  2057,
         2442, 15796,  2007,  1996,  3671,  4353, 24415,  1996,  2168,  2812,
         1998,  3115, 24353,  1012,  2156,  2019,  2742,  2101,  1999,  2023,
         3127,  1012,   102])"
620,1,"['sample', 'distribution', 'binomial', 'binomial distribution']", Statistical Uncertainty in Sample Surveys,seg_131,"we have now studied the main characteristics of the binomial distribution. in this and the next section, we study the most important applications in sample surveys.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  2057,  2031,  2085,  3273,  1996,  2364,  6459,  1997,  1996,
         8026, 20936,  2389,  4353,  1012,  1999,  2023,  1998,  1996,  2279,
         2930,  1010,  2057,  2817,  1996,  2087,  2590,  5097,  1999,  7099,
        12265,  1012,   102])"
621,1,"['uncertainty', 'statistical uncertainty', 'estimate', 'frequency', 'information', 'relative frequency', 'sample', 'statistical', 'frequency ']", Statistical Uncertainty in Sample Surveys,seg_131,"let us imagine that we want to find an estimate of the relative frequency (*) of a particular activity among the kids in the fitness club survey. for instance, we want to find out which proportion of the kids is doing strength training. we can get this information through a sample survey. there will be some statistical uncertainty (*) connected with our estimate, and we alsowant to estimate this statistical uncertainty.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  2292,  2149,  5674,  2008,  2057,  2215,  2000,  2424,  2019,
        10197,  1997,  1996,  5816,  6075,  1006,  1008,  1007,  1997,  1037,
         3327,  4023,  2426,  1996,  4268,  1999,  1996, 10516,  2252,  5002,
         1012,  2005,  6013,  1010,  2057,  2215,  2000,  2424,  2041,  2029,
        10817,  1997,  1996,  4268,  2003,  2725,  3997,  2731,  1012,  2057,
         2064,  2131,  2023,  2592,  2083,  1037,  7099,  5002,  1012,  2045,
         2097,  2022,  2070,  7778, 12503,  1006,  1008,  1007,  4198,  2007,
         2256, 10197,  1010,  1998,  2057,  2036,  7447,  2102,  2000, 10197,
         2023,  7778, 12503,  1012,   102])"
622,1,"['probability', 'frequency', 'relative frequency', 'population']", Statistical Uncertainty in Sample Surveys,seg_131,the population consists of kids in the fitness club. the relative frequency in the population corresponds to the probability p that a randomly selected kid does strength training.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  1996,  2313,  3774,  1997,  4268,  1999,  1996, 10516,  2252,
         1012,  1996,  5816,  6075,  1999,  1996,  2313, 14788,  2000,  1996,
         9723,  1052,  2008,  1037, 18154,  3479,  4845,  2515,  3997,  2731,
         1012,   102])"
623,1,"['sample', 'distribution', 'binomial', 'binomial distribution']", Statistical Uncertainty in Sample Surveys,seg_131,"the number of kids in the sample, who do strength training, can be described by a binomial distribution with",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 1., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  1996,  2193,  1997,  4268,  1999,  1996,  7099,  1010,  2040,
         2079,  3997,  2731,  1010,  2064,  2022,  2649,  2011,  1037,  8026,
        20936,  2389,  4353,  2007,   102])"
624,1,"['frequency', 'relative frequency', 'sample', 'sample size', 'population']", Statistical Uncertainty in Sample Surveys,seg_131,– n ¼ sample size – p ¼ relative frequency in the population,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([ 101, 1516, 1050, 1091, 7099, 2946, 1516, 1052, 1091, 5816, 6075, 1999,
        1996, 2313,  102])"
625,1,"['estimate', 'frequency', 'relative frequency', 'sample', 'population']", Statistical Uncertainty in Sample Surveys,seg_131,"assume that in a sample of n kids, there are x kids who do strength training. the estimate of the relative frequency p in the population is the relative frequency in the sample x/n.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 0., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  7868,  2008,  1999,  1037,  7099,  1997,  1050,  4268,  1010,
         2045,  2024,  1060,  4268,  2040,  2079,  3997,  2731,  1012,  1996,
        10197,  1997,  1996,  5816,  6075,  1052,  1999,  1996,  2313,  2003,
         1996,  5816,  6075,  1999,  1996,  7099,  1060,  1013,  1050,  1012,
          102])"
626,1,"['percentage', 'frequency', 'relative frequency', 'frequency ']", Statistical Uncertainty in Sample Surveys,seg_131,"note: here the term relative frequency is interpreted as a proportion and used in much the same way as the term incidence (e.g., of a disease), which is often expressed as a percentage. in opposition to this is the (absolute) frequency (*), which is expressed as a number (of occurrences, individuals, etc.).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  3602,  1024,  2182,  1996,  2744,  5816,  6075,  2003, 10009,
         2004,  1037, 10817,  1998,  2109,  1999,  2172,  1996,  2168,  2126,
         2004,  1996,  2744, 18949,  1006,  1041,  1012,  1043,  1012,  1010,
         1997,  1037,  4295,  1007,  1010,  2029,  2003,  2411,  5228,  2004,
         1037,  7017,  1012,  1999,  4559,  2000,  2023,  2003,  1996,  1006,
         7619,  1007,  6075,  1006,  1008,  1007,  1010,  2029,  2003,  5228,
         2004,  1037,  2193,  1006,  1997, 27247,  1010,  3633,  1010,  4385,
         1012,  1007,  1012,   102])"
627,1,['probability'], Statistical Uncertainty in Sample Surveys,seg_131,two types of probability.,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([ 101, 2048, 4127, 1997, 9723, 1012,  102])"
628,1,['probability'], Statistical Uncertainty in Sample Surveys,seg_131,you can use the term probability in (at least) two different ways:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([ 101, 2017, 2064, 2224, 1996, 2744, 9723, 1999, 1006, 2012, 2560, 1007,
        2048, 2367, 3971, 1024,  102])"
629,1,"['probability', 'frequency', 'relative frequency', 'sample', 'population', 'experiment', 'distribution', 'binomial', 'case', 'binomial distribution']", Statistical Uncertainty in Sample Surveys,seg_131,– as an expression of a relative frequency (proportion) that can be found from a sample survey or an experiment. this is exactly the case here. we have a proportion (of kids doing strength training) in the population. we expect that this proportion is corresponding to the probability that a randomly chosen kid does strength training. this is the reason why we use the binomial distribution in this situation. (continued),tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  1516,  2004,  2019,  3670,  1997,  1037,  5816,  6075,  1006,
        10817,  1007,  2008,  2064,  2022,  2179,  2013,  1037,  7099,  5002,
         2030,  2019,  7551,  1012,  2023,  2003,  3599,  1996,  2553,  2182,
         1012,  2057,  2031,  1037, 10817,  1006,  1997,  4268,  2725,  3997,
         2731,  1007,  1999,  1996,  2313,  1012,  2057,  5987,  2008,  2023,
        10817,  2003,  7978,  2000,  1996,  9723,  2008,  1037, 18154,  4217,
         4845,  2515,  3997,  2731,  1012,  2023,  2003,  1996,  3114,  2339,
         2057,  2224,  1996,  8026, 20936,  2389,  4353,  1999,  2023,  3663,
         1012,  1006,  2506,  1007,   102])"
630,1,"['table', 'probability', 'results', 'expectation']", Statistical Uncertainty in Sample Surveys,seg_131,"– as an expression of an expectation. we may say that the probability that a certain football match will end with a home win is 40%. this is not necessarily based on knowledge about previous results of matches between the two teams; maybe, there are no previous matches! rather, we are using knowledge about the latest series of results of both teams and their position in the table, injured players, etc.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  1516,  2004,  2019,  3670,  1997,  2019, 17626,  1012,  2057,
         2089,  2360,  2008,  1996,  9723,  2008,  1037,  3056,  2374,  2674,
         2097,  2203,  2007,  1037,  2188,  2663,  2003,  2871,  1003,  1012,
         2023,  2003,  2025,  9352,  2241,  2006,  3716,  2055,  3025,  3463,
         1997,  3503,  2090,  1996,  2048,  2780,  1025,  2672,  1010,  2045,
         2024,  2053,  3025,  3503,   999,  2738,  1010,  2057,  2024,  2478,
         3716,  2055,  1996,  6745,  2186,  1997,  3463,  1997,  2119,  2780,
         1998,  2037,  2597,  1999,  1996,  2795,  1010,  5229,  2867,  1010,
         4385,  1012,   102])"
631,1,"['estimate', 'frequency', 'relative frequency', 'sample', 'population']", Example,seg_133,"let us assume that out of n ¼ 30 kids in the sample, x ¼ 12 do strength training. we can estimate the relative frequency p in the population by the relative frequency in the sample:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.,
        0., 0., 1., 0., 0.])",tensor([2742]),"tensor([  101,  2292,  2149,  7868,  2008,  2041,  1997,  1050,  1091,  2382,
         4268,  1999,  1996,  7099,  1010,  1060,  1091,  2260,  2079,  3997,
         2731,  1012,  2057,  2064, 10197,  1996,  5816,  6075,  1052,  1999,
         1996,  2313,  2011,  1996,  5816,  6075,  1999,  1996,  7099,  1024,
          102])"
632,1,"['normal distribution', 'estimate', 'population', 'sample', 'normal', 'distribution', 'binomial']", Example,seg_133,"that is, we estimate that 40% of the kids in the population do strength training. if the sample is large enough, we can approximate the binomial distributionwith the normal distribution.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,
        1., 1., 0., 0.])",tensor([2742]),"tensor([  101,  2008,  2003,  1010,  2057, 10197,  2008,  2871,  1003,  1997,
         1996,  4268,  1999,  1996,  2313,  2079,  3997,  2731,  1012,  2065,
         1996,  7099,  2003,  2312,  2438,  1010,  2057,  2064, 15796,  1996,
         8026, 20936,  2389,  4353, 24415,  1996,  3671,  4353,  1012,   102])"
633,1,"['normal distribution', 'estimate', 'frequency', 'relative frequency', 'normal', 'distribution']", Example,seg_133,the estimate x/n of the relative frequency p can also be approximated by a normal distribution.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0.])",tensor([2742]),"tensor([  101,  1996, 10197,  1060,  1013,  1050,  1997,  1996,  5816,  6075,
         1052,  2064,  2036,  2022, 15796,  2094,  2011,  1037,  3671,  4353,
         1012,   102])"
634,1,"['deviation', 'normal distribution', 'estimate', 'standard deviation', 'normal', 'standard', 'distribution']", Example,seg_133,we can show that an estimate of standard deviation in this normal distribution is,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.])",tensor([2742]),"tensor([  101,  2057,  2064,  2265,  2008,  2019, 10197,  1997,  3115, 24353,
         1999,  2023,  3671,  4353,  2003,   102])"
635,1,"['deviation', 'estimate', 'frequency', 'relative frequency', 'standard deviation', 'standard']", Example,seg_133,the standard deviation of our estimate x/n of the relative frequency p is thus 0.09 ¼ 9%.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  3115, 24353,  1997,  2256, 10197,  1060,  1013,  1050,
         1997,  1996,  5816,  6075,  1052,  2003,  2947,  1014,  1012,  5641,
         1091,  1023,  1003,  1012,   102])"
636,1,"['confidence interval', 'probability', 'interval', 'frequency', 'confidence', 'relative frequency', 'population']", Example,seg_133,"we can now construct a 95% confidence interval of p, which with probability 95% contains the relative frequency in the population.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 1., 0., 0.])",tensor([2742]),"tensor([  101,  2057,  2064,  2085,  9570,  1037,  5345,  1003,  7023, 13483,
         1997,  1052,  1010,  2029,  2007,  9723,  5345,  1003,  3397,  1996,
         5816,  6075,  1999,  1996,  2313,  1012,   102])"
637,1,"['mean', 'confidence interval', 'normal distribution', 'interval', 'normal', 'distribution', 'confidence']", Example,seg_133,this is done in the same way as when constructing a 95% confidence interval for the mean of a normal distribution (see chap. 4):,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2023,  2003,  2589,  1999,  1996,  2168,  2126,  2004,  2043,
        15696,  1037,  5345,  1003,  7023, 13483,  2005,  1996,  2812,  1997,
         1037,  3671,  4353,  1006,  2156, 15775,  2361,  1012,  1018,  1007,
         1024,   102])"
638,1,"['probability', 'frequency', 'relative frequency', 'population']", Example,seg_133,this means that with 95% probability the relative frequency p of kids doing strength training in the population is somewhere between 22% and 58%.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([ 101, 2023, 2965, 2008, 2007, 5345, 1003, 9723, 1996, 5816, 6075, 1052,
        1997, 4268, 2725, 3997, 2731, 1999, 1996, 2313, 2003, 4873, 2090, 2570,
        1003, 1998, 5388, 1003, 1012,  102])"
639,1,['sample'], Example,seg_133,"it seems, perhaps, that we really do not know much about the proportion of kids doing strength training! the reason is, of course, that the sample is not very large.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0.])",tensor([2742]),"tensor([  101,  2009,  3849,  1010,  3383,  1010,  2008,  2057,  2428,  2079,
         2025,  2113,  2172,  2055,  1996, 10817,  1997,  4268,  2725,  3997,
         2731,   999,  1996,  3114,  2003,  1010,  1997,  2607,  1010,  2008,
         1996,  7099,  2003,  2025,  2200,  2312,  1012,   102])"
640,1,"['uncertainty', 'statistical uncertainty', 'sample', 'sample size', 'statistical']", Example,seg_133,"if we increase the sample size, the statistical uncertainty will become smaller, and more about this is given below.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2065,  2057,  3623,  1996,  7099,  2946,  1010,  1996,  7778,
        12503,  2097,  2468,  3760,  1010,  1998,  2062,  2055,  2023,  2003,
         2445,  2917,  1012,   102])"
641,1,"['uncertainty', 'statistical uncertainty', 'estimate', 'frequency', 'relative frequency', 'population', 'statistical']", Example,seg_133,the term after is the statistical uncertainty (*) u of the estimate of the relative frequency in the population.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        1., 0., 0., 1., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  2744,  2044,  2003,  1996,  7778, 12503,  1006,  1008,
         1007,  1057,  1997,  1996, 10197,  1997,  1996,  5816,  6075,  1999,
         1996,  2313,  1012,   102])"
642,1,"['uncertainty', 'statistical uncertainty', 'frequency', 'relative frequency', 'statistical']", Example,seg_133,the general formula for the statistical uncertainty of a relative frequency is:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  2236,  5675,  2005,  1996,  7778, 12503,  1997,  1037,
         5816,  6075,  2003,  1024,   102])"
643,1,"['frequency', 'relative frequency', 'sample']", Example,seg_133,"when this formula is used in calculations, the relative frequency in the sample x/n is substituted for p.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2043,  2023,  5675,  2003,  2109,  1999, 16268,  1010,  1996,
         5816,  6075,  1999,  1996,  7099,  1060,  1013,  1050,  2003, 17316,
         2005,  1052,  1012,   102])"
644,1,"['sample', 'population']", Example,seg_133,"the above formula can only be used if the sample is substantially smaller than the population, for example, less than 10% of the population.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  2682,  5675,  2064,  2069,  2022,  2109,  2065,  1996,
         7099,  2003, 12381,  3760,  2084,  1996,  2313,  1010,  2005,  2742,
         1010,  2625,  2084,  2184,  1003,  1997,  1996,  2313,  1012,   102])"
645,1,"['sample', 'sample size', 'population']", Example,seg_133,"for example, if the population consists of 100 kids, we cannot use the formula with a sample size of n ¼ 30. see textbox p. 79–80 for the situation where the sample is larger than 10% of the population.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])",tensor([2742]),"tensor([ 101, 2005, 2742, 1010, 2065, 1996, 2313, 3774, 1997, 2531, 4268, 1010,
        2057, 3685, 2224, 1996, 5675, 2007, 1037, 7099, 2946, 1997, 1050, 1091,
        2382, 1012, 2156, 3793, 8758, 1052, 1012, 6535, 1516, 3770, 2005, 1996,
        3663, 2073, 1996, 7099, 2003, 3469, 2084, 2184, 1003, 1997, 1996, 2313,
        1012,  102])"
646,0,[], Example,seg_133,in other words:,tensor(0),"tensor([0, 0, 0, 0, 0, 0])",tensor([2742]),"tensor([ 101, 1999, 2060, 2616, 1024,  102])"
647,1,"['uncertainty', 'statistical uncertainty', 'sample', 'population', 'statistical', 'independent']", Example,seg_133,"as long as the sample is substantially smaller than the population, the statistical uncertainty is independent of the population size.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1.,
        0., 0., 1., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2004,  2146,  2004,  1996,  7099,  2003, 12381,  3760,  2084,
         1996,  2313,  1010,  1996,  7778, 12503,  2003,  2981,  1997,  1996,
         2313,  2946,  1012,   102])"
648,1,"['populations', 'random sample', 'sample', 'random']", Example,seg_133,"for example, a random sample of 1,000 persons inchina is statistically just as good (or bad!) as a sample of 1,000 persons in, for example, sweden, although the size of the populations is dramatically different. this comes as a surprise to many people!",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2005,  2742,  1010,  1037,  6721,  7099,  1997,  1015,  1010,
         2199,  5381,  4960,  3981,  2003,  7778,  2135,  2074,  2004,  2204,
         1006,  2030,  2919,   999,  1007,  2004,  1037,  7099,  1997,  1015,
         1010,  2199,  5381,  1999,  1010,  2005,  2742,  1010,  4701,  1010,
         2348,  1996,  2946,  1997,  1996,  7080,  2003, 12099,  2367,  1012,
         2023,  3310,  2004,  1037,  4474,  2000,  2116,  2111,   999,   102])"
649,1,"['uncertainty', 'statistical uncertainty', 'sample', 'sample size', 'statistical']", Example,seg_133,"the above formula also has the interesting consequence that the largest statistical uncertainty is obtained, when p ¼ 0.5 ¼ 50%. see fig. 5.4, where we have plotted the statistical uncertainty vs. p for a sample size of n ¼ 100.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  2682,  5675,  2036,  2038,  1996,  5875,  9509,  2008,
         1996,  2922,  7778, 12503,  2003,  4663,  1010,  2043,  1052,  1091,
         1014,  1012,  1019,  1091,  2753,  1003,  1012,  2156, 20965,  1012,
         1019,  1012,  1018,  1010,  2073,  2057,  2031, 27347,  1996,  7778,
        12503,  5443,  1012,  1052,  2005,  1037,  7099,  2946,  1997,  1050,
         1091,  2531,  1012,   102])"
650,1,"['uncertainty', 'statistical uncertainty', 'interval', 'statistical']", Example,seg_133,"it is clear from the figure that as long as p is not too close to 0 or 1 (for example, if p is somewhere in the interval from 0.2 ¼ 20% to 0.8 ¼ 80%), the statistical uncertainty is roughly the same as when p ¼ 0.5.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2009,  2003,  3154,  2013,  1996,  3275,  2008,  2004,  2146,
         2004,  1052,  2003,  2025,  2205,  2485,  2000,  1014,  2030,  1015,
         1006,  2005,  2742,  1010,  2065,  1052,  2003,  4873,  1999,  1996,
        13483,  2013,  1014,  1012,  1016,  1091,  2322,  1003,  2000,  1014,
         1012,  1022,  1091,  3770,  1003,  1007,  1010,  1996,  7778, 12503,
         2003,  5560,  1996,  2168,  2004,  2043,  1052,  1091,  1014,  1012,
         1019,  1012,   102])"
651,0,[], Example,seg_133,in other words:,tensor(0),"tensor([0, 0, 0, 0, 0, 0])",tensor([2742]),"tensor([ 101, 1999, 2060, 2616, 1024,  102])"
652,1,"['statistical', 'statistical uncertainty', 'uncertainty']", Example,seg_133,the statistical uncertainty is roughly constant as long as the relative frequency is not too close to the extremes.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  7778, 12503,  2003,  5560,  5377,  2004,  2146,  2004,
         1996,  5816,  6075,  2003,  2025,  2205,  2485,  2000,  1996, 28800,
         1012,   102])"
653,1,"['uncertainty', 'statistical uncertainty', 'sample', 'sample size', 'statistical']", Example,seg_133,"the relative statistical uncertainty is u/p; this number gets larger, the smaller p is, see fig. 5.5 which is still based on a sample size of n ¼ 100.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  5816,  7778, 12503,  2003,  1057,  1013,  1052,  1025,
         2023,  2193,  4152,  3469,  1010,  1996,  3760,  1052,  2003,  1010,
         2156, 20965,  1012,  1019,  1012,  1019,  2029,  2003,  2145,  2241,
         2006,  1037,  7099,  2946,  1997,  1050,  1091,  2531,  1012,   102])"
654,1,"['uncertainty', 'statistical uncertainty', 'frequency', 'relative frequency', 'statistical']", Example,seg_133,"when the relative frequency approaches 0, the relative statistical uncertainty can get infinitely large.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2043,  1996,  5816,  6075,  8107,  1014,  1010,  1996,  5816,
         7778, 12503,  2064,  2131, 25773,  2312,  1012,   102])"
655,1,"['contrast', 'uncertainty', 'statistical uncertainty', 'statistical']", Example,seg_133,"this can be translated to the opinion polls: the largest political parties, which come close to 50%, have the largest statistical uncertainty. in contrast, the smallest political parties have the largest relative statistical uncertainty.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0.])",tensor([2742]),"tensor([  101,  2023,  2064,  2022,  5421,  2000,  1996,  5448, 14592,  1024,
         1996,  2922,  2576,  4243,  1010,  2029,  2272,  2485,  2000,  2753,
         1003,  1010,  2031,  1996,  2922,  7778, 12503,  1012,  1999,  5688,
         1010,  1996, 10479,  2576,  4243,  2031,  1996,  2922,  5816,  7778,
        12503,  1012,   102])"
656,1,"['uncertainty', 'statistical uncertainty', 'sample', 'sample size', 'statistical']", Example,seg_133,"it appears from the formula that the statistical uncertainty u is inversely proportional to the square root of the sample size n. for example, if the sample gets four times larger, the statistical uncertainty is halved. conversely, if the sample gets four times smaller, the statistical uncertainty is doubled.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2009,  3544,  2013,  1996,  5675,  2008,  1996,  7778, 12503,
         1057,  2003, 19262,  2135, 14267,  2000,  1996,  2675,  7117,  1997,
         1996,  7099,  2946,  1050,  1012,  2005,  2742,  1010,  2065,  1996,
         7099,  4152,  2176,  2335,  3469,  1010,  1996,  7778, 12503,  2003,
        11085,  7178,  1012, 18868,  1010,  2065,  1996,  7099,  4152,  2176,
         2335,  3760,  1010,  1996,  7778, 12503,  2003, 11515,  1012,   102])"
657,1,"['uncertainty', 'statistical uncertainty', 'sample', 'sample size', 'statistical']", Example,seg_133,"this is illustrated in fig. 5.6, which shows the statistical uncertainty vs. n, where p ¼ 0.5. in other words, this is the maximum statistical uncertainty for a given sample size.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 1., 1., 0., 0.])",tensor([2742]),"tensor([  101,  2023,  2003,  7203,  1999, 20965,  1012,  1019,  1012,  1020,
         1010,  2029,  3065,  1996,  7778, 12503,  5443,  1012,  1050,  1010,
         2073,  1052,  1091,  1014,  1012,  1019,  1012,  1999,  2060,  2616,
         1010,  2023,  2003,  1996,  4555,  7778, 12503,  2005,  1037,  2445,
         7099,  2946,  1012,   102])"
658,1,"['uncertainty', 'table', 'statistical uncertainty', 'statistical']", Example,seg_133,see also the table in section 9.4.5 showing the statistical uncertainty for different values of n and p.,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2156,  2036,  1996,  2795,  1999,  2930,  1023,  1012,  1018,
         1012,  1019,  4760,  1996,  7778, 12503,  2005,  2367,  5300,  1997,
         1050,  1998,  1052,  1012,   102])"
659,1,"['uncertainty', 'statistical uncertainty', 'without replacement', 'frequency', 'relative frequency', 'sample', 'population', 'sampling', 'statistical', 'replacement', 'case']", Example,seg_133,"technical note: statistical uncertainty of the relative frequency for a large sample. if the sample is larger than 10% of the population and sampling is carried out without replacement, the formula for the statistical uncertainty of a relative frequency should be modified. the correct formula for the statistical uncertainty in this case is as follows: (continued)",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  4087,  3602,  1024,  7778, 12503,  1997,  1996,  5816,  6075,
         2005,  1037,  2312,  7099,  1012,  2065,  1996,  7099,  2003,  3469,
         2084,  2184,  1003,  1997,  1996,  2313,  1998, 16227,  2003,  3344,
         2041,  2302,  6110,  1010,  1996,  5675,  2005,  1996,  7778, 12503,
         1997,  1037,  5816,  6075,  2323,  2022,  6310,  1012,  1996,  6149,
         5675,  2005,  1996,  7778, 12503,  1999,  2023,  2553,  2003,  2004,
         4076,  1024,  1006,  2506,  1007,   102])"
660,1,"['uncertainty', 'statistical uncertainty', 'population', 'sample', 'sampling', 'statistical', 'sampling fraction']", Example,seg_133,"here n ¼ number of individuals in the population. the fraction n/n is called the sampling fraction (*). when the sample is small, n/n is close to 0, and thus the square root of 1 n/n is very close to 1. if the sample is larger than 10% of the population, 1 n/n is noticeably smaller than 1. therefore, the statistical uncertainty can never be larger than:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2182,  1050,  1091,  2193,  1997,  3633,  1999,  1996,  2313,
         1012,  1996, 12884,  1050,  1013,  1050,  2003,  2170,  1996, 16227,
        12884,  1006,  1008,  1007,  1012,  2043,  1996,  7099,  2003,  2235,
         1010,  1050,  1013,  1050,  2003,  2485,  2000,  1014,  1010,  1998,
         2947,  1996,  2675,  7117,  1997,  1015,  1050,  1013,  1050,  2003,
         2200,  2485,  2000,  1015,  1012,  2065,  1996,  7099,  2003,  3469,
         2084,  2184,  1003,  1997,  1996,  2313,  1010,  1015,  1050,  1013,
         1050,  2003, 25327,  3760,  2084,  1015,  1012,  3568,  1010,  1996,
         7778, 12503,  2064,  2196,  2022,  3469,  2084,  1024,   102])"
661,1,"['uncertainty', 'statistical uncertainty', 'sample', 'limit', 'population', 'statistical']", Example,seg_133,"in other words, you can see this number as an upper limit for the statistical uncertainty. if the sample is very large compared to the population, the actual statistical uncertainty will be considerably smaller.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1999,  2060,  2616,  1010,  2017,  2064,  2156,  2023,  2193,
         2004,  2019,  3356,  5787,  2005,  1996,  7778, 12503,  1012,  2065,
         1996,  7099,  2003,  2200,  2312,  4102,  2000,  1996,  2313,  1010,
         1996,  5025,  7778, 12503,  2097,  2022,  9839,  3760,  1012,   102])"
662,0,[], Is the Sample Representative,seg_135,"here, we illustrate an important application of the above calculations.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([2003, 1996, 7099, 4387])","tensor([  101,  2182,  1010,  2057, 19141,  2019,  2590,  4646,  1997,  1996,
         2682, 16268,  1012,   102])"
663,1,"['bias', 'sample', 'sampling', 'errors', 'sampling ', 'bias ', 'systematic errors']", Is the Sample Representative,seg_135,"we conducted a survey of kids in the fitness club. in the sample, there are 17 boys and 13 girls. we are interested in knowing whether the sample is representative with respect to sex or whether there are systematic errors or bias (*) related to the sampling (*). see more in chaps. 1 and 6.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2003, 1996, 7099, 4387])","tensor([  101,  2057,  4146,  1037,  5002,  1997,  4268,  1999,  1996, 10516,
         2252,  1012,  1999,  1996,  7099,  1010,  2045,  2024,  2459,  3337,
         1998,  2410,  3057,  1012,  2057,  2024,  4699,  1999,  4209,  3251,
         1996,  7099,  2003,  4387,  2007,  4847,  2000,  3348,  2030,  3251,
         2045,  2024, 11778, 10697,  2030, 13827,  1006,  1008,  1007,  3141,
         2000,  1996, 16227,  1006,  1008,  1007,  1012,  2156,  2062,  1999,
        15775,  4523,  1012,  1015,  1998,  1020,  1012,   102])"
664,1,"['systematic errors', 'biased', 'sample', 'errors', 'representative sample', 'representative', 'bias']", Is the Sample Representative,seg_135,"an example of systematic errors or bias: suppose that the sample is selected by visiting the fitness club a given day and asking the first 30 kids, we meet, to participate in the survey. assume that the boys are more frequent users of the fitness club than the girls. then we would have a biased composition of the sample, i.e., it is not a representative sample with respect to sex.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([2003, 1996, 7099, 4387])","tensor([  101,  2019,  2742,  1997, 11778, 10697,  2030, 13827,  1024,  6814,
         2008,  1996,  7099,  2003,  3479,  2011,  5873,  1996, 10516,  2252,
         1037,  2445,  2154,  1998,  4851,  1996,  2034,  2382,  4268,  1010,
         2057,  3113,  1010,  2000,  5589,  1999,  1996,  5002,  1012,  7868,
         2008,  1996,  3337,  2024,  2062,  6976,  5198,  1997,  1996, 10516,
         2252,  2084,  1996,  3057,  1012,  2059,  2057,  2052,  2031,  1037,
        25352,  5512,  1997,  1996,  7099,  1010,  1045,  1012,  1041,  1012,
         1010,  2009,  2003,  2025,  1037,  4387,  7099,  2007,  4847,  2000,
         3348,  1012,   102])"
665,1,"['registers', 'population']", Is the Sample Representative,seg_135,"the population consists of all kids in the fitness club. the club might know from its registers that there are 65% boys and 35% girls among the kids, who are customers.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([2003, 1996, 7099, 4387])","tensor([  101,  1996,  2313,  3774,  1997,  2035,  4268,  1999,  1996, 10516,
         2252,  1012,  1996,  2252,  2453,  2113,  2013,  2049, 18687,  2008,
         2045,  2024,  3515,  1003,  3337,  1998,  3486,  1003,  3057,  2426,
         1996,  4268,  1010,  2040,  2024,  6304,  1012,   102])"
666,1,"['frequencies', 'population', 'sample', 'relative frequencies', 'representative']", Is the Sample Representative,seg_135,"if you do not have knowledge about the relative frequencies in the population, you cannot use this approach to judge whether the sample is representative.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.])","tensor([2003, 1996, 7099, 4387])","tensor([  101,  2065,  2017,  2079,  2025,  2031,  3716,  2055,  1996,  5816,
        13139,  1999,  1996,  2313,  1010,  2017,  3685,  2224,  2023,  3921,
         2000,  3648,  3251,  1996,  7099,  2003,  4387,  1012,   102])"
667,1,"['sample', 'representative']", Is the Sample Representative,seg_135,now we will ask ourselves the question: is the sample representative with respect to sex?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0.])","tensor([2003, 1996, 7099, 4387])","tensor([ 101, 2085, 2057, 2097, 3198, 9731, 1996, 3160, 1024, 2003, 1996, 7099,
        4387, 2007, 4847, 2000, 3348, 1029,  102])"
668,1,"['confidence interval', 'interval', 'sample', 'population', 'confidence', 'data', 'representative']", Is the Sample Representative,seg_135,"this question can be answered by using data from the sample to calculate a confidence interval for the proportion of boys in the population. if this confidence interval contains the known proportion of boys (in the population), the sample is representative with respect to sex.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([2003, 1996, 7099, 4387])","tensor([  101,  2023,  3160,  2064,  2022,  4660,  2011,  2478,  2951,  2013,
         1996,  7099,  2000, 18422,  1037,  7023, 13483,  2005,  1996, 10817,
         1997,  3337,  1999,  1996,  2313,  1012,  2065,  2023,  7023, 13483,
         3397,  1996,  2124, 10817,  1997,  3337,  1006,  1999,  1996,  2313,
         1007,  1010,  1996,  7099,  2003,  4387,  2007,  4847,  2000,  3348,
         1012,   102])"
669,1,"['uncertainty', 'table', 'statistical uncertainty', 'statistical']", Is the Sample Representative,seg_135,with (table 5.2) the formula for the statistical uncertainty of p is:,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0.])","tensor([2003, 1996, 7099, 4387])","tensor([  101,  2007,  1006,  2795,  1019,  1012,  1016,  1007,  1996,  5675,
         2005,  1996,  7778, 12503,  1997,  1052,  2003,  1024,   102])"
670,1,"['confidence interval', 'uncertainty', 'interval', 'statistical uncertainty', 'sample', 'statistical', 'confidence']", Is the Sample Representative,seg_135,"the statistical uncertainty is then u ¼ 0.177 and the confidence interval for p has the endpoints 0.567 0.177 ¼ 0.389 and 0.567 þ 0.177 ¼ 0.744, i.e., the confidence interval goes from 38.9% to 74.4%. as we can see, the confidence interval is very wide. the reason is, of course, that the sample is not very large!",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0.])","tensor([2003, 1996, 7099, 4387])","tensor([  101,  1996,  7778, 12503,  2003,  2059,  1057,  1091,  1014,  1012,
        18118,  1998,  1996,  7023, 13483,  2005,  1052,  2038,  1996,  2203,
        26521,  1014,  1012,  5179,  2581,  1014,  1012, 18118,  1091,  1014,
         1012,  4229,  2683,  1998,  1014,  1012,  5179,  2581,  1101,  1014,
         1012, 18118,  1091,  1014,  1012,  6356,  2549,  1010,  1045,  1012,
         1041,  1012,  1010,  1996,  7023, 13483,  3632,  2013,  4229,  1012,
         1023,  1003,  2000,  6356,  1012,  1018,  1003,  1012,  2004,  2057,
         2064,  2156,  1010,  1996,  7023, 13483,  2003,  2200,  2898,  1012,
         1996,  3114,  2003,  1010,  1997,  2607,  1010,  2008,  1996,  7099,
         2003,  2025,  2200,  2312,   999,   102])"
671,1,"['confidence interval', 'interval', 'sample', 'population', 'confidence', 'representative']", Is the Sample Representative,seg_135,"the confidence interval contains the known value of the proportion of boys in the population, which is 65%. therefore, we consider the sample to be representative with respect to sex.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0.])","tensor([2003, 1996, 7099, 4387])","tensor([  101,  1996,  7023, 13483,  3397,  1996,  2124,  3643,  1997,  1996,
        10817,  1997,  3337,  1999,  1996,  2313,  1010,  2029,  2003,  3515,
         1003,  1012,  3568,  1010,  2057,  5136,  1996,  7099,  2000,  2022,
         4387,  2007,  4847,  2000,  3348,  1012,   102])"
672,1,"['statistical tests', 'binomial distribution', 'tests', 'intervals', 'experiments', 'distribution', 'statistical', 'confidence intervals', 'confidence', 'binomial']", Is the Sample Representative,seg_135,"we have so far in this chapter studied the binomial distribution, including confidence intervals. in the rest of the chapter, we look at statistical tests which are used for surveys as well as for experiments.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0.])","tensor([2003, 1996, 7099, 4387])","tensor([  101,  2057,  2031,  2061,  2521,  1999,  2023,  3127,  3273,  1996,
         8026, 20936,  2389,  4353,  1010,  2164,  7023, 14025,  1012,  1999,
         1996,  2717,  1997,  1996,  3127,  1010,  2057,  2298,  2012,  7778,
         5852,  2029,  2024,  2109,  2005, 12265,  2004,  2092,  2004,  2005,
         7885,  1012,   102])"
673,1,"['probability', 'hypothesis']", Statistical Tests,seg_137,"sometimes, you have a hypothesis, you want to confirm or reject. a simple example is examining whether a dice or coin is “genuine.” that is, the probability of, for example, six eyes when throwing a dice is 1/6 or the probability of heads is 0.5 when tossing a coin.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7778, 5852])","tensor([  101,  2823,  1010,  2017,  2031,  1037, 10744,  1010,  2017,  2215,
         2000, 12210,  2030, 15454,  1012,  1037,  3722,  2742,  2003, 12843,
         3251,  1037, 18740,  2030,  9226,  2003,  1523, 10218,  1012,  1524,
         2008,  2003,  1010,  1996,  9723,  1997,  1010,  2005,  2742,  1010,
         2416,  2159,  2043,  6886,  1037, 18740,  2003,  1015,  1013,  1020,
         2030,  1996,  9723,  1997,  4641,  2003,  1014,  1012,  1019,  2043,
        15021,  1037,  9226,  1012,   102])"
674,1,"['parameter', 'set', 'probability', 'hypothesis', 'population', 'statistical', 'null hypothesis', 'case']", Statistical Tests,seg_137,"you then set up a hypothesis, for instance, in this case the assumption that the probability of heads is p ¼ 0.5 when tossing a coin. generally, this is a hypothesis that a parameter in the population (e.g., a probability) equals a certain value. in statistical literature, the hypothesis is often called the null hypothesis (*).",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0.])","tensor([7778, 5852])","tensor([  101,  2017,  2059,  2275,  2039,  1037, 10744,  1010,  2005,  6013,
         1010,  1999,  2023,  2553,  1996, 11213,  2008,  1996,  9723,  1997,
         4641,  2003,  1052,  1091,  1014,  1012,  1019,  2043, 15021,  1037,
         9226,  1012,  3227,  1010,  2023,  2003,  1037, 10744,  2008,  1037,
        16381,  1999,  1996,  2313,  1006,  1041,  1012,  1043,  1012,  1010,
         1037,  9723,  1007, 19635,  1037,  3056,  3643,  1012,  1999,  7778,
         3906,  1010,  1996, 10744,  2003,  2411,  2170,  1996, 19701, 10744,
         1006,  1008,  1007,  1012,   102])"
675,1,"['hypothesis', 'sample', 'experiment', 'data', 'test']", Statistical Tests,seg_137,statistical test of a hypothesis: the objective is to decide whether the hypothesis is supported by data from a sample (or an experiment).,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.])","tensor([7778, 5852])","tensor([  101,  7778,  3231,  1997,  1037, 10744,  1024,  1996,  7863,  2003,
         2000,  5630,  3251,  1996, 10744,  2003,  3569,  2011,  2951,  2013,
         1037,  7099,  1006,  2030,  2019,  7551,  1007,  1012,   102])"
676,1,"['data', 'hypothesis']", Statistical Tests,seg_137,"– the hypothesis can be either true or false. – we consider the hypothesis true, unless data indicate that it is false.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([7778, 5852])","tensor([  101,  1516,  1996, 10744,  2064,  2022,  2593,  2995,  2030,  6270,
         1012,  1516,  2057,  5136,  1996, 10744,  2995,  1010,  4983,  2951,
         5769,  2008,  2009,  2003,  6270,  1012,   102])"
677,0,[], Statistical Tests,seg_137,the practical approach is as follows:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([7778, 5852])","tensor([ 101, 1996, 6742, 3921, 2003, 2004, 4076, 1024,  102])"
678,1,"['probability', 'outcomes', 'hypothesis', 'outcome']", Statistical Tests,seg_137,"1. assume that the hypothesis is true. 2. calculate the probability of outcomes at least as “rare” as the observed outcome. 3. if this probability is small (typically less than 5%), reject the hypothesis. otherwise, accept it.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([7778, 5852])","tensor([  101,  1015,  1012,  7868,  2008,  1996, 10744,  2003,  2995,  1012,
         1016,  1012, 18422,  1996,  9723,  1997, 13105,  2012,  2560,  2004,
         1523,  4678,  1524,  2004,  1996,  5159,  9560,  1012,  1017,  1012,
         2065,  2023,  9723,  2003,  2235,  1006,  4050,  2625,  2084,  1019,
         1003,  1007,  1010, 15454,  1996, 10744,  1012,  4728,  1010,  5138,
         2009,  1012,   102])"
679,0,[], Example,seg_139,this approach is best illustrated with an example.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])",tensor([2742]),"tensor([ 101, 2023, 3921, 2003, 2190, 7203, 2007, 2019, 2742, 1012,  102])"
680,1,['outcome'], Example,seg_139,let us assume that we toss a coin n ¼ 20 times and observe the outcome heads x ¼ 5 times.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2292,  2149,  7868,  2008,  2057, 10055,  1037,  9226,  1050,
         1091,  2322,  2335,  1998, 11949,  1996,  9560,  4641,  1060,  1091,
         1019,  2335,  1012,   102])"
681,0,[], Example,seg_139,we are now asking the question: is the coin genuine? the general approach is as follows:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])",tensor([2742]),"tensor([  101,  2057,  2024,  2085,  4851,  1996,  3160,  1024,  2003,  1996,
         9226, 10218,  1029,  1996,  2236,  3921,  2003,  2004,  4076,  1024,
          102])"
682,1,"['probabilities', 'distribution', 'binomial', 'binomial distribution']", Example,seg_139,"we assume that the coin is genuine, i.e., we can use a binomial distribution with n ¼ 20, p ¼ 0.5. thus, we would expect around 10 times heads out of 20 tosses with the coin. figure 5.7 shows the probabilities of this binomial distribution.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0.])",tensor([2742]),"tensor([  101,  2057,  7868,  2008,  1996,  9226,  2003, 10218,  1010,  1045,
         1012,  1041,  1012,  1010,  2057,  2064,  2224,  1037,  8026, 20936,
         2389,  4353,  2007,  1050,  1091,  2322,  1010,  1052,  1091,  1014,
         1012,  1019,  1012,  2947,  1010,  2057,  2052,  5987,  2105,  2184,
         2335,  4641,  2041,  1997,  2322, 10055,  2229,  2007,  1996,  9226,
         1012,  3275,  1019,  1012,  1021,  3065,  1996,  4013,  3676, 14680,
         1997,  2023,  8026, 20936,  2389,  4353,  1012,   102])"
683,1,"['probability', 'outcome']", Example,seg_139,2. we calculate the probability of getting an outcome that is at least as rare as the,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])",tensor([2742]),"tensor([  101,  1016,  1012,  2057, 18422,  1996,  9723,  1997,  2893,  2019,
         9560,  2008,  2003,  2012,  2560,  2004,  4678,  2004,  1996,   102])"
684,1,"['probability', 'probabilities', 'hypothesis', 'outcome', 'chart', 'test']", Example,seg_139,"observed outcome. we have observed five times heads in 20 tosses, i.e., somewhat less than expected. an outcome at least as rare will be at most five times heads in 20 tosses. usually you will add the probability of getting at least 15 (i.e., 20 5) times heads, which is just as rare to the “other extreme”; it is “just as bad” for the hypothesis. this is called a two-sided test (*). we can easily calculate the probability of at most five times heads in 20 coin tosses by using a spreadsheet. this is precisely the sum of the probabilities in the bars from 0 to 5 in the chart above.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0.])",tensor([2742]),"tensor([  101,  5159,  9560,  1012,  2057,  2031,  5159,  2274,  2335,  4641,
         1999,  2322, 10055,  2229,  1010,  1045,  1012,  1041,  1012,  1010,
         5399,  2625,  2084,  3517,  1012,  2019,  9560,  2012,  2560,  2004,
         4678,  2097,  2022,  2012,  2087,  2274,  2335,  4641,  1999,  2322,
        10055,  2229,  1012,  2788,  2017,  2097,  5587,  1996,  9723,  1997,
         2893,  2012,  2560,  2321,  1006,  1045,  1012,  1041,  1012,  1010,
         2322,  1019,  1007,  2335,  4641,  1010,  2029,  2003,  2074,  2004,
         4678,  2000,  1996,  1523,  2060,  6034,  1524,  1025,  2009,  2003,
         1523,  2074,  2004,  2919,  1524,  2005,  1996, 10744,  1012,  2023,
         2003,  2170,  1037,  2048,  1011, 11536,  3231,  1006,  1008,  1007,
         1012,  2057,  2064,  4089, 18422,  1996,  9723,  1997,  2012,  2087,
         2274,  2335,  4641,  1999,  2322,  9226, 10055,  2229,  2011,  2478,
         1037, 20861, 21030,  2102,  1012,  2023,  2003, 10785,  1996,  7680,
         1997,  1996,  4013,  3676, 14680,  1999,  1996,  6963,  2013,  1014,
         2000,  1019,  1999,  1996,  3673,  2682,  1012,   102])"
685,1,"['function', 'table', 'probability', 'probabilities', 'hypothesis', 'outcome', 'statistical', 'binomial', 'total probability']", Example,seg_139,"the function binomdist (5; 20; 0.5, 1) gives us the result 0.0207, i.e., 2.07%. (this can also be looked up in a table of binomial probabilities.) similarly, the probability of at least 15 times heads in 20 coin tosses is the same, i.e., 0.0207. the total probability of an outcome at least as rare as the observed outcome is therefore: 2 0.0207 ¼ 0.0414 ¼ 4.14%. this probability is called the p-value (*) in statistical “jargon.” 3. if this probability is small (typically less than 5%), reject the hypothesis.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  3853,  8026,  5358, 10521,  2102,  1006,  1019,  1025,
         2322,  1025,  1014,  1012,  1019,  1010,  1015,  1007,  3957,  2149,
         1996,  2765,  1014,  1012,  6185,  2692,  2581,  1010,  1045,  1012,
         1041,  1012,  1010,  1016,  1012,  5718,  1003,  1012,  1006,  2023,
         2064,  2036,  2022,  2246,  2039,  1999,  1037,  2795,  1997,  8026,
        20936,  2389,  4013,  3676, 14680,  1012,  1007,  6660,  1010,  1996,
         9723,  1997,  2012,  2560,  2321,  2335,  4641,  1999,  2322,  9226,
        10055,  2229,  2003,  1996,  2168,  1010,  1045,  1012,  1041,  1012,
         1010,  1014,  1012,  6185,  2692,  2581,  1012,  1996,  2561,  9723,
         1997,  2019,  9560,  2012,  2560,  2004,  4678,  2004,  1996,  5159,
         9560,  2003,  3568,  1024,  1016,  1014,  1012,  6185,  2692,  2581,
         1091,  1014,  1012,  5840, 16932,  1091,  1018,  1012,  2403,  1003,
         1012,  2023,  9723,  2003,  2170,  1996,  1052,  1011,  3643,  1006,
         1008,  1007,  1999,  7778,  1523, 15723,  7446,  1012,  1524,  1017,
         1012,  2065,  2023,  9723,  2003,  2235,  1006,  4050,  2625,  2084,
         1019,  1003,  1007,  1010, 15454,  1996, 10744,  1012,   102])"
686,1,"['probability', 'hypothesis', 'outcome', 'statistical']", Example,seg_139,"otherwise, accept it. the probability of an outcome at least as rare as the observed outcome is 4.14% <5%. the conclusion is, therefore, that we reject the hypothesis that p ¼ 0.5! this means that there is statistical evidence that the coin is false!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  4728,  1010,  5138,  2009,  1012,  1996,  9723,  1997,  2019,
         9560,  2012,  2560,  2004,  4678,  2004,  1996,  5159,  9560,  2003,
         1018,  1012,  2403,  1003,  1026,  1019,  1003,  1012,  1996,  7091,
         2003,  1010,  3568,  1010,  2008,  2057, 15454,  1996, 10744,  2008,
         1052,  1091,  1014,  1012,  1019,   999,  2023,  2965,  2008,  2045,
         2003,  7778,  3350,  2008,  1996,  9226,  2003,  6270,   999,   102])"
687,1,"['probability', 'outcome']", Example,seg_139,"the philosophy behind the approach outlined above is as follows: if the probability of a more “rare” outcome is small, there are two options:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  4695,  2369,  1996,  3921, 14801,  2682,  2003,  2004,
         4076,  1024,  2065,  1996,  9723,  1997,  1037,  2062,  1523,  4678,
         1524,  9560,  2003,  2235,  1010,  2045,  2024,  2048,  7047,  1024,
          102])"
688,1,"['hypothesis', 'event']", Example,seg_139,"– the hypothesis is true, but we have observed a rare event. – the hypothesis is actually false.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1516,  1996, 10744,  2003,  2995,  1010,  2021,  2057,  2031,
         5159,  1037,  4678,  2724,  1012,  1516,  1996, 10744,  2003,  2941,
         6270,  1012,   102])"
689,1,"['event', 'statisticians']", Example,seg_139,"of course it is conceivable that the first option is correct. if so, we have observed a rare event! however, statisticians do not believe in miracles. therefore, we prefer to believe the second option.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1997,  2607,  2009,  2003,  9530,  3401, 11444,  3468,  2008,
         1996,  2034,  5724,  2003,  6149,  1012,  2065,  2061,  1010,  2057,
         2031,  5159,  1037,  4678,  2724,   999,  2174,  1010, 28093,  6553,
         7066,  2079,  2025,  2903,  1999, 17861,  1012,  3568,  1010,  2057,
         9544,  2000,  2903,  1996,  2117,  5724,  1012,   102])"
690,1,"['normal distribution', 'approximation', 'normal', 'distribution', 'binomial', 'binomial distribution']", Approximation with the Normal Distribution,seg_141,"in some situations, you need to use the approximation of the binomial distribution with a normal distribution:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        0., 1., 1., 0., 0.])","tensor([20167,  2007,  1996,  3671,  4353])","tensor([  101,  1999,  2070,  8146,  1010,  2017,  2342,  2000,  2224,  1996,
        20167,  1997,  1996,  8026, 20936,  2389,  4353,  2007,  1037,  3671,
         4353,  1024,   102])"
691,1,"['sample', 'sample size', 'table']", Approximation with the Normal Distribution,seg_141,"– the sample size is very large, e.g., more than 1,000. – you have to perform the calculations using only a calculator and a table of the",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0.])","tensor([20167,  2007,  1996,  3671,  4353])","tensor([  101,  1516,  1996,  7099,  2946,  2003,  2200,  2312,  1010,  1041,
         1012,  1043,  1012,  1010,  2062,  2084,  1015,  1010,  2199,  1012,
         1516,  2017,  2031,  2000,  4685,  1996, 16268,  2478,  2069,  1037,
        10250, 19879,  4263,  1998,  1037,  2795,  1997,  1996,   102])"
692,1,['distribution'], Approximation with the Normal Distribution,seg_141,normal distribution.,tensor(1),"tensor([0., 0., 1., 0., 0.])","tensor([20167,  2007,  1996,  3671,  4353])","tensor([ 101, 3671, 4353, 1012,  102])"
693,1,"['mean', 'deviation', 'normal distribution', 'variance', 'normal', 'standard deviation', 'standard', 'distribution']", Approximation with the Normal Distribution,seg_141,"in the example above, we should use a normal distribution with mean n p ¼ 10 and variance n p(1 p) ¼ 5. the standard deviation is √5 ¼ 2.236.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([20167,  2007,  1996,  3671,  4353])","tensor([  101,  1999,  1996,  2742,  2682,  1010,  2057,  2323,  2224,  1037,
         3671,  4353,  2007,  2812,  1050,  1052,  1091,  2184,  1998, 23284,
         1050,  1052,  1006,  1015,  1052,  1007,  1091,  1019,  1012,  1996,
         3115, 24353,  2003,  1600,  2629,  1091,  1016,  1012, 23593,  1012,
          102])"
694,1,"['normal distribution', 'probability', 'normal', 'distribution', 'data']", Approximation with the Normal Distribution,seg_141,"in this normal distribution, we must calculate the probability of data values up to (and including) 5.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([20167,  2007,  1996,  3671,  4353])","tensor([  101,  1999,  2023,  3671,  4353,  1010,  2057,  2442, 18422,  1996,
         9723,  1997,  2951,  5300,  2039,  2000,  1006,  1998,  2164,  1007,
         1019,  1012,   102])"
695,1,"['normal distribution', 'probability', 'normal', 'distribution', 'data']", Approximation with the Normal Distribution,seg_141,"since the normal distribution covers the whole axis (not only integer values), we should actually find the probability of data values up to 5.5 rather than 5. we therefore add 0.5 to the value of x.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([20167,  2007,  1996,  3671,  4353])","tensor([  101,  2144,  1996,  3671,  4353,  4472,  1996,  2878,  8123,  1006,
         2025,  2069, 16109,  5300,  1007,  1010,  2057,  2323,  2941,  2424,
         1996,  9723,  1997,  2951,  5300,  2039,  2000,  1019,  1012,  1019,
         2738,  2084,  1019,  1012,  2057,  3568,  5587,  1014,  1012,  1019,
         2000,  1996,  3643,  1997,  1060,  1012,   102])"
696,1,"['parameter', 'function', 'density function', 'distribution', 'distribution function']", Approximation with the Normal Distribution,seg_141,"we must therefore find the value normdist (5.5; 10; 2.236; 1). we use 1 for the last parameter, because we need the distribution function (not the density function). the result is 2.2%. again, you multiply by 2 and get 4.4%, i.e., still below 5%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([20167,  2007,  1996,  3671,  4353])","tensor([  101,  2057,  2442,  3568,  2424,  1996,  3643, 13373, 10521,  2102,
         1006,  1019,  1012,  1019,  1025,  2184,  1025,  1016,  1012, 23593,
         1025,  1015,  1007,  1012,  2057,  2224,  1015,  2005,  1996,  2197,
        16381,  1010,  2138,  2057,  2342,  1996,  4353,  3853,  1006,  2025,
         1996,  4304,  3853,  1007,  1012,  1996,  2765,  2003,  1016,  1012,
         1016,  1003,  1012,  2153,  1010,  2017,  4800, 22086,  2011,  1016,
         1998,  2131,  1018,  1012,  1018,  1003,  1010,  1045,  1012,  1041,
         1012,  1010,  2145,  2917,  1019,  1003,  1012,   102])"
697,1,"['normal', 'normal distribution', 'distribution']", Approximation with the Normal Distribution,seg_141,that it is indeed permissible to use the normal distribution can be demonstrated as follows:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([20167,  2007,  1996,  3671,  4353])","tensor([  101,  2008,  2009,  2003,  5262,  2566, 26770,  2000,  2224,  1996,
         3671,  4353,  2064,  2022,  7645,  2004,  4076,  1024,   102])"
698,1,"['significance level', 'probability', 'hypothesis', 'level', 'limit', 'type i error', 'significance', 'error']", Significance Level,seg_143,"how small should the probability be, before we reject the hypothesis? this limit is called the significance level (*). usually, we choose the significance level 0.05, i.e., 5%. if the hypothesis is indeed true, there is a small probability (5%) that we commit an error, i.e., reject a true hypothesis. this kind of error is called a type i error (*). if not explicitly stated, the significance level is 5%.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([7784, 2504])","tensor([  101,  2129,  2235,  2323,  1996,  9723,  2022,  1010,  2077,  2057,
        15454,  1996, 10744,  1029,  2023,  5787,  2003,  2170,  1996,  7784,
         2504,  1006,  1008,  1007,  1012,  2788,  1010,  2057,  5454,  1996,
         7784,  2504,  1014,  1012,  5709,  1010,  1045,  1012,  1041,  1012,
         1010,  1019,  1003,  1012,  2065,  1996, 10744,  2003,  5262,  2995,
         1010,  2045,  2003,  1037,  2235,  9723,  1006,  1019,  1003,  1007,
         2008,  2057, 10797,  2019,  7561,  1010,  1045,  1012,  1041,  1012,
         1010, 15454,  1037,  2995, 10744,  1012,  2023,  2785,  1997,  7561,
         2003,  2170,  1037,  2828,  1045,  7561,  1006,  1008,  1007,  1012,
         2065,  2025, 12045,  3090,  1010,  1996,  7784,  2504,  2003,  1019,
         1003,  1012,   102])"
699,1,"['significance level', 'consequences', 'hypothesis', 'level', 'significance', 'error']", Significance Level,seg_143,"if the consequences of committing the error of rejecting a true hypothesis are very serious, we could choose the significance level 1%.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([7784, 2504])","tensor([  101,  2065,  1996,  8465,  1997, 16873,  1996,  7561,  1997, 21936,
         1037,  2995, 10744,  2024,  2200,  3809,  1010,  2057,  2071,  5454,
         1996,  7784,  2504,  1015,  1003,  1012,   102])"
700,1,"['significance', 'significance level', 'level']", Significance Level,seg_143,"the price we pay for choosing a 1% significance level is that it becomes more difficult to detect differences that actually exist, for example, to detect a false coin.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7784, 2504])","tensor([  101,  1996,  3976,  2057,  3477,  2005, 10549,  1037,  1015,  1003,
         7784,  2504,  2003,  2008,  2009,  4150,  2062,  3697,  2000, 11487,
         5966,  2008,  2941,  4839,  1010,  2005,  2742,  1010,  2000, 11487,
         1037,  6270,  9226,  1012,   102])"
701,1,"['significance level', 'hypothesis', 'level', 'significance', 'data']", Significance Level,seg_143,"in the above example with tossing a coin, the p-value is just below 5%, but well over 1%. if we use a significance level of 1%, the hypothesis p ¼ 0.5 is accepted. we need more convincing data to reject the hypothesis p ¼ 0.5, if we use the significance level of 1%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])","tensor([7784, 2504])","tensor([  101,  1999,  1996,  2682,  2742,  2007, 15021,  1037,  9226,  1010,
         1996,  1052,  1011,  3643,  2003,  2074,  2917,  1019,  1003,  1010,
         2021,  2092,  2058,  1015,  1003,  1012,  2065,  2057,  2224,  1037,
         7784,  2504,  1997,  1015,  1003,  1010,  1996, 10744,  1052,  1091,
         1014,  1012,  1019,  2003,  3970,  1012,  2057,  2342,  2062, 13359,
         2951,  2000, 15454,  1996, 10744,  1052,  1091,  1014,  1012,  1019,
         1010,  2065,  2057,  2224,  1996,  7784,  2504,  1997,  1015,  1003,
         1012,   102])"
702,1,"['significance level', 'hypothesis', 'level', 'statistical test', 'statistical', 'significance', 'test']", Significance Level,seg_143,"note: the significance level has to be decided before doing the statistical test. the example with tossing a coin shows why: depending on the choice of significance level, you can either accept or reject the hypothesis.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0.])","tensor([7784, 2504])","tensor([  101,  3602,  1024,  1996,  7784,  2504,  2038,  2000,  2022,  2787,
         2077,  2725,  1996,  7778,  3231,  1012,  1996,  2742,  2007, 15021,
         1037,  9226,  3065,  2339,  1024,  5834,  2006,  1996,  3601,  1997,
         7784,  2504,  1010,  2017,  2064,  2593,  5138,  2030, 15454,  1996,
        10744,  1012,   102])"
703,1,"['confidence interval', 'uncertainty', 'probability', 'interval', 'statistical uncertainty', 'estimate', 'frequency', 'confidence', 'relative frequency', 'statistical']", Statistical Test or Confidence Interval,seg_145,"let us in the example above construct a confidence interval for the probability of heads, p. we have observed a relative frequency of heads x/n ¼ 5/20 ¼ 0.25 ¼ 25%. this estimate for p is inserted in the formula for the statistical uncertainty u:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0.])","tensor([ 7778,  3231,  2030,  7023, 13483])","tensor([  101,  2292,  2149,  1999,  1996,  2742,  2682,  9570,  1037,  7023,
        13483,  2005,  1996,  9723,  1997,  4641,  1010,  1052,  1012,  2057,
         2031,  5159,  1037,  5816,  6075,  1997,  4641,  1060,  1013,  1050,
         1091,  1019,  1013,  2322,  1091,  1014,  1012,  2423,  1091,  2423,
         1003,  1012,  2023, 10197,  2005,  1052,  2003, 12889,  1999,  1996,
         5675,  2005,  1996,  7778, 12503,  1057,  1024,   102])"
704,1,"['interval', 'confidence', 'confidence interval', 'data']", Statistical Test or Confidence Interval,seg_145,"the result is u ¼ 0.22. the confidence interval is thus p ¼ 0.25 0.22, i.e., the confidence interval goes from 0.03 to 0.47. that is, a 95% confidence interval for p does not contain 0.5. so based on our data we are (at least) 95% certain that p is not 0.5, as it should be for a genuine coin.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 7778,  3231,  2030,  7023, 13483])","tensor([  101,  1996,  2765,  2003,  1057,  1091,  1014,  1012,  2570,  1012,
         1996,  7023, 13483,  2003,  2947,  1052,  1091,  1014,  1012,  2423,
         1014,  1012,  2570,  1010,  1045,  1012,  1041,  1012,  1010,  1996,
         7023, 13483,  3632,  2013,  1014,  1012,  6021,  2000,  1014,  1012,
         4700,  1012,  2008,  2003,  1010,  1037,  5345,  1003,  7023, 13483,
         2005,  1052,  2515,  2025,  5383,  1014,  1012,  1019,  1012,  2061,
         2241,  2006,  2256,  2951,  2057,  2024,  1006,  2012,  2560,  1007,
         5345,  1003,  3056,  2008,  1052,  2003,  2025,  1014,  1012,  1019,
         1010,  2004,  2009,  2323,  2022,  2005,  1037, 10218,  9226,  1012,
          102])"
705,1,"['confidence interval', 'interval', 'statistical test', 'statistical', 'confidence', 'test']", Statistical Test or Confidence Interval,seg_145,we thus get the same conclusion when using a confidence interval as when using a statistical test.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
        0., 0.])","tensor([ 7778,  3231,  2030,  7023, 13483])","tensor([  101,  2057,  2947,  2131,  1996,  2168,  7091,  2043,  2478,  1037,
         7023, 13483,  2004,  2043,  2478,  1037,  7778,  3231,  1012,   102])"
706,1,"['significance level', 'confidence interval', 'interval', 'hypothesis', 'level', 'statistical test', 'confidence', 'statistical', 'significance', 'test']", Statistical Test or Confidence Interval,seg_145,to perform a statistical test at 5% significance level for the hypothesis p ¼ 0.5 corresponds to constructing a 95% confidence interval for p and check whether the confidence interval contains the value 0.5.,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 7778,  3231,  2030,  7023, 13483])","tensor([  101,  2000,  4685,  1037,  7778,  3231,  2012,  1019,  1003,  7784,
         2504,  2005,  1996, 10744,  1052,  1091,  1014,  1012,  1019, 14788,
         2000, 15696,  1037,  5345,  1003,  7023, 13483,  2005,  1052,  1998,
         4638,  3251,  1996,  7023, 13483,  3397,  1996,  3643,  1014,  1012,
         1019,  1012,   102])"
707,0,[], Statistical Test or Confidence Interval,seg_145,what are the advantages and disadvantages of the two approaches?,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 7778,  3231,  2030,  7023, 13483])","tensor([  101,  2054,  2024,  1996, 12637,  1998, 20502,  2015,  1997,  1996,
         2048,  8107,  1029,   102])"
708,1,"['interval', 'confidence', 'confidence interval']", Statistical Test or Confidence Interval,seg_145,"– a confidence interval gives a yes/no conclusion. on the other hand, a confidence",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0.])","tensor([ 7778,  3231,  2030,  7023, 13483])","tensor([  101,  1516,  1037,  7023, 13483,  3957,  1037,  2748,  1013,  2053,
         7091,  1012,  2006,  1996,  2060,  2192,  1010,  1037,  7023,   102])"
709,1,"['set', 'statistical test', 'statistical', 'test']", Statistical Test or Confidence Interval,seg_145,"interval also gives us a set of values of p, which can be considered likely. – a statistical test gives both a yes/no conclusion (accept/reject) and a more",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 7778,  3231,  2030,  7023, 13483])","tensor([  101, 13483,  2036,  3957,  2149,  1037,  2275,  1997,  5300,  1997,
         1052,  1010,  2029,  2064,  2022,  2641,  3497,  1012,  1516,  1037,
         7778,  3231,  3957,  2119,  1037,  2748,  1013,  2053,  7091,  1006,
         5138,  1013, 15454,  1007,  1998,  1037,  2062,   102])"
710,1,"['probability', 'hypothesis', 'event', 'probability of a rarer event']", Statistical Test or Confidence Interval,seg_145,"graduated answer with the p-value. in the example with tossing a coin, the probability of a rarer event is below 5%, but only slightly below. that is, the hypothesis is only just rejected, which is not a very convincing conclusion. we would have felt more convinced, if the p-value was less than 1%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7778,  3231,  2030,  7023, 13483])","tensor([  101,  3852,  3437,  2007,  1996,  1052,  1011,  3643,  1012,  1999,
         1996,  2742,  2007, 15021,  1037,  9226,  1010,  1996,  9723,  1997,
         1037,  4678,  2099,  2724,  2003,  2917,  1019,  1003,  1010,  2021,
         2069,  3621,  2917,  1012,  2008,  2003,  1010,  1996, 10744,  2003,
         2069,  2074,  5837,  1010,  2029,  2003,  2025,  1037,  2200, 13359,
         7091,  1012,  2057,  2052,  2031,  2371,  2062,  6427,  1010,  2065,
         1996,  1052,  1011,  3643,  2001,  2625,  2084,  1015,  1003,  1012,
          102])"
711,1,"['hypothesis', 'distributions', 'binomial distribution', 'statistical test', 'distribution', 'statistical', 'binomial', 'test']", Statistical Test or Confidence Interval,seg_145,"in this section, we have dealt with a statistical test of a hypothesis in one distribution (the binomial distribution). the next section describes situations where two or more distributions are to be compared.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1.,
        0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0.])","tensor([ 7778,  3231,  2030,  7023, 13483])","tensor([  101,  1999,  2023,  2930,  1010,  2057,  2031,  9411,  2007,  1037,
         7778,  3231,  1997,  1037, 10744,  1999,  2028,  4353,  1006,  1996,
         8026, 20936,  2389,  4353,  1007,  1012,  1996,  2279,  2930,  5577,
         8146,  2073,  2048,  2030,  2062, 20611,  2024,  2000,  2022,  4102,
         1012,   102])"
712,1,"['sample', 'data', 'table']", Introduction to ChiSquared Test,seg_149,"let us take a look at some data from the fitness club sample survey. we are still interested in the proportion of kids doing strength training, but now we want to group data according to sex. this is given in table 5.3.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  2292,  2149,  2202,  1037,  2298,  2012,  2070,  2951,  2013,
         1996, 10516,  2252,  7099,  5002,  1012,  2057,  2024,  2145,  4699,
         1999,  1996, 10817,  1997,  4268,  2725,  3997,  2731,  1010,  2021,
         2085,  2057,  2215,  2000,  2177,  2951,  2429,  2000,  3348,  1012,
         2023,  2003,  2445,  1999,  2795,  1019,  1012,  1017,  1012,   102])"
713,1,"['frequency table', 'frequency', 'table']", Introduction to ChiSquared Test,seg_149,"we call this a frequency table. this is a 2 2 table (read “2 by 2 table”), i.e., two rows (boys/girls) and two columns (does strength training/no strength training).",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  2057,  2655,  2023,  1037,  6075,  2795,  1012,  2023,  2003,
         1037,  1016,  1016,  2795,  1006,  3191,  1523,  1016,  2011,  1016,
         2795,  1524,  1007,  1010,  1045,  1012,  1041,  1012,  1010,  2048,
        10281,  1006,  3337,  1013,  3057,  1007,  1998,  2048,  7753,  1006,
         2515,  3997,  2731,  1013,  2053,  3997,  2731,  1007,  1012,   102])"
714,1,"['percent', 'table']", Introduction to ChiSquared Test,seg_149,it looks as if (perhaps not very surprising) strength training is a boy’s thing. we can illustrate this by calculating the proportion (in percent) of kids who does strength training for each sex. this is given in table 5.4.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  2009,  3504,  2004,  2065,  1006,  3383,  2025,  2200, 11341,
         1007,  3997,  2731,  2003,  1037,  2879,  1521,  1055,  2518,  1012,
         2057,  2064, 19141,  2023,  2011, 20177,  1996, 10817,  1006,  1999,
         3867,  1007,  1997,  4268,  2040,  2515,  3997,  2731,  2005,  2169,
         3348,  1012,  2023,  2003,  2445,  1999,  2795,  1019,  1012,  1018,
         1012,   102])"
715,1,"['sample', 'contrast']", Introduction to ChiSquared Test,seg_149,"we see that as many as 59% of the boys do strength training. by contrast, only 15% of the girls do strength training. in total in the sample, 40% of the kids do strength training.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([ 101, 2057, 2156, 2008, 2004, 2116, 2004, 5354, 1003, 1997, 1996, 3337,
        2079, 3997, 2731, 1012, 2011, 5688, 1010, 2069, 2321, 1003, 1997, 1996,
        3057, 2079, 3997, 2731, 1012, 1999, 2561, 1999, 1996, 7099, 1010, 2871,
        1003, 1997, 1996, 4268, 2079, 3997, 2731, 1012,  102])"
716,1,"['statistical', 'statistical test', 'test']", Introduction to ChiSquared Test,seg_149,this is a “subjective” evaluation based on comparing the two proportions; it is not an objective statistical test of whether the proportions differ.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  2023,  2003,  1037,  1523, 20714,  1524,  9312,  2241,  2006,
        13599,  1996,  2048, 19173,  1025,  2009,  2003,  2025,  2019,  7863,
         7778,  3231,  1997,  3251,  1996, 19173, 11234,  1012,   102])"
717,1,"['cases', 'test', 'statistical test', 'statistical']", Introduction to ChiSquared Test,seg_149,"in the example above, it is probably quite obvious that there actually is a difference between the proportion of boys and girls doing strength training; we hardly need a statistical test. but in other cases there might be only minor differences in the proportions; then the conclusion is less obvious, and there is a need for an objective criterion like a statistical test, so we are not left in doubt.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  1999,  1996,  2742,  2682,  1010,  2009,  2003,  2763,  3243,
         5793,  2008,  2045,  2941,  2003,  1037,  4489,  2090,  1996, 10817,
         1997,  3337,  1998,  3057,  2725,  3997,  2731,  1025,  2057,  6684,
         2342,  1037,  7778,  3231,  1012,  2021,  1999,  2060,  3572,  2045,
         2453,  2022,  2069,  3576,  5966,  1999,  1996, 19173,  1025,  2059,
         1996,  7091,  2003,  2625,  5793,  1010,  1998,  2045,  2003,  1037,
         2342,  2005,  2019,  7863, 19229,  2066,  1037,  7778,  3231,  1010,
         2061,  2057,  2024,  2025,  2187,  1999,  4797,  1012,   102])"
718,1,"['frequencies', 'statistical test', 'relative frequencies', 'statistical', 'test']", Introduction to ChiSquared Test,seg_149,here we present a statistical test that can be used in situations where you have to compare two relative frequencies (proportions).,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  2182,  2057,  2556,  1037,  7778,  3231,  2008,  2064,  2022,
         2109,  1999,  8146,  2073,  2017,  2031,  2000, 12826,  2048,  5816,
        13139,  1006, 19173,  1007,  1012,   102])"
719,1,"['hypothesis', 'independence', 'independent']", Introduction to ChiSquared Test,seg_149,"the hypothesis is that there is the same proportion of boys and girls doing strength training. this means that there is independence between rows and columns, i.e., doing strength training is independent of sex.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  1996, 10744,  2003,  2008,  2045,  2003,  1996,  2168, 10817,
         1997,  3337,  1998,  3057,  2725,  3997,  2731,  1012,  2023,  2965,
         2008,  2045,  2003,  4336,  2090, 10281,  1998,  7753,  1010,  1045,
         1012,  1041,  1012,  1010,  2725,  3997,  2731,  2003,  2981,  1997,
         3348,  1012,   102])"
720,1,"['hypothesis', 'statistical test', 'statistical', 'test']", Introduction to ChiSquared Test,seg_149,we use a statistical test of the hypothesis. we use the general approach:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  2057,  2224,  1037,  7778,  3231,  1997,  1996, 10744,  1012,
         2057,  2224,  1996,  2236,  3921,  1024,   102])"
721,1,"['probability', 'outcome', 'hypothesis']", Introduction to ChiSquared Test,seg_149,1. we assume that the hypothesis is true. 2. we calculate the probability of getting an outcome that is at least as rare as the,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  1015,  1012,  2057,  7868,  2008,  1996, 10744,  2003,  2995,
         1012,  1016,  1012,  2057, 18422,  1996,  9723,  1997,  2893,  2019,
         9560,  2008,  2003,  2012,  2560,  2004,  4678,  2004,  1996,   102])"
722,1,"['frequencies', 'table', 'outcome', 'hypothesis']", Introduction to ChiSquared Test,seg_149,"observed outcome. first, let us consider how the frequencies would be if the hypothesis was correct. then the proportion of kids doing strength training would be the same for boys and girls, i.e., equal to 40%. this does not necessarily lead to integers as shown in table 5.5.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  5159,  9560,  1012,  2034,  1010,  2292,  2149,  5136,  2129,
         1996, 13139,  2052,  2022,  2065,  1996, 10744,  2001,  6149,  1012,
         2059,  1996, 10817,  1997,  4268,  2725,  3997,  2731,  2052,  2022,
         1996,  2168,  2005,  3337,  1998,  3057,  1010,  1045,  1012,  1041,
         1012,  1010,  5020,  2000,  2871,  1003,  1012,  2023,  2515,  2025,
         9352,  2599,  2000, 24028,  2004,  3491,  1999,  2795,  1019,  1012,
         1019,  1012,   102])"
723,1,"['frequencies', 'table', 'hypothesis', 'condition', 'case']", Introduction to ChiSquared Test,seg_149,"for example, we calculate the expected 6.80 boys doing strength training as 40% of 17 boys: 17 0.40 ¼ 6.80. the 40% can be written 12/30 ¼ 0.40 ¼ 40%. this means that we can write 17 12 the calculation of the “upper-left corner” of the table as: ¼ 6:80: 30 note which frequencies are used in this formula. they are highlighted in bold in the table above. similarly, the other frequencies are calculated. we call these frequencies the expected frequencies. the original frequencies are called the observed frequencies. the hypothesis must be considered false if the observed frequencies are far from the expected frequencies. before showing the calculation of the probability, it should be emphasized that a necessary condition is that all expected frequencies should be at least 5. this is the case here. we need a measure of the magnitude of the difference between the observed and expected frequencies.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  2005,  2742,  1010,  2057, 18422,  1996,  3517,  1020,  1012,
         3770,  3337,  2725,  3997,  2731,  2004,  2871,  1003,  1997,  2459,
         3337,  1024,  2459,  1014,  1012,  2871,  1091,  1020,  1012,  3770,
         1012,  1996,  2871,  1003,  2064,  2022,  2517,  2260,  1013,  2382,
         1091,  1014,  1012,  2871,  1091,  2871,  1003,  1012,  2023,  2965,
         2008,  2057,  2064,  4339,  2459,  2260,  1996, 17208,  1997,  1996,
         1523,  3356,  1011,  2187,  3420,  1524,  1997,  1996,  2795,  2004,
         1024,  1091,  1020,  1024,  3770,  1024,  2382,  3602,  2029, 13139,
         2024,  2109,  1999,  2023,  5675,  1012,  2027,  2024, 11548,  1999,
         7782,  1999,  1996,  2795,  2682,  1012,  6660,  1010,  1996,  2060,
        13139,  2024, 10174,  1012,  2057,  2655,  2122, 13139,  1996,  3517,
        13139,  1012,  1996,  2434, 13139,  2024,  2170,  1996,  5159, 13139,
         1012,  1996, 10744,  2442,  2022,  2641,  6270,  2065,  1996,  5159,
        13139,  2024,  2521,  2013,  1996,  3517, 13139,  1012,  2077,  4760,
         1996, 17208,  1997,  1996,  9723,  1010,  2009,  2323,  2022, 13155,
         2008,  1037,  4072,  4650,  2003,  2008,  2035,  3517, 13139,  2323,
         2022,  2012,  2560,  1019,  1012,  2023,  2003,  1996,  2553,  2182,
         1012,  2057,  2342,  1037,  5468,  1997,  1996, 10194,  1997,  1996,
         4489,  2090,  1996,  5159,  1998,  3517, 13139,  1012,   102])"
724,1,['frequencies'], Introduction to ChiSquared Test,seg_149,a measure of the difference between the observed and expected frequencies can be calculated using the following formula:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  1037,  5468,  1997,  1996,  4489,  2090,  1996,  5159,  1998,
         3517, 13139,  2064,  2022, 10174,  2478,  1996,  2206,  5675,  1024,
          102])"
725,1,['frequency'], Introduction to ChiSquared Test,seg_149,– o ¼ observed frequency – f ¼ expected frequency,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([ 101, 1516, 1051, 1091, 5159, 6075, 1516, 1042, 1091, 3517, 6075,  102])"
726,1,['frequencies'], Introduction to ChiSquared Test,seg_149,"s is the “sum symbol,” i.e., we must add all 4 numbers of this type, as there are 4 observed frequencies and 4 expected frequencies. the result is called w2. w is the greek letter chi (pronounced “ki”), which corresponds to the letters “ch”. w2 is read “chi-squared.”",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  1055,  2003,  1996,  1523,  7680,  6454,  1010,  1524,  1045,
         1012,  1041,  1012,  1010,  2057,  2442,  5587,  2035,  1018,  3616,
         1997,  2023,  2828,  1010,  2004,  2045,  2024,  1018,  5159, 13139,
         1998,  1018,  3517, 13139,  1012,  1996,  2765,  2003,  2170,  1059,
         2475,  1012,  1059,  2003,  1996,  3306,  3661,  9610,  1006,  8793,
         1523, 11382,  1524,  1007,  1010,  2029, 14788,  2000,  1996,  4144,
         1523, 10381,  1524,  1012,  1059,  2475,  2003,  3191,  1523,  9610,
         1011, 19942,  1012,  1524,   102])"
727,0,[], Introduction to ChiSquared Test,seg_149,the calculation of w2 is as follows:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  1996, 17208,  1997,  1059,  2475,  2003,  2004,  4076,  1024,
          102])"
728,1,"['frequencies', 'table', 'probability', 'estimate', 'degree of freedom', 'hypothesis', 'distribution', 'case']", Introduction to ChiSquared Test,seg_149,"if all the observed frequencies are the same as the expected frequencies, we will have w2 ¼ 0. small values of w2 are “good” for our hypothesis – this is evidence that the observed and expected frequencies are close. large values of w2 are “bad” for the hypothesis – this is evidence that the observed and expected frequencies are far apart. now we need to find the probability of getting a value of w2 that is larger (i.e., “worse” for the hypothesis) than the value calculated above. one can show that we have to use a so-called chi-squared distribution (*) (or w2-distribution). in this case, we say that we use a chi-squared distribution with one degree of freedom (*) (more on this later). we can get a rough estimate of the probability by looking at the table of chi-squared distribution (with one degree of freedom) in chap. 9. here it is found that",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  2065,  2035,  1996,  5159, 13139,  2024,  1996,  2168,  2004,
         1996,  3517, 13139,  1010,  2057,  2097,  2031,  1059,  2475,  1091,
         1014,  1012,  2235,  5300,  1997,  1059,  2475,  2024,  1523,  2204,
         1524,  2005,  2256, 10744,  1516,  2023,  2003,  3350,  2008,  1996,
         5159,  1998,  3517, 13139,  2024,  2485,  1012,  2312,  5300,  1997,
         1059,  2475,  2024,  1523,  2919,  1524,  2005,  1996, 10744,  1516,
         2023,  2003,  3350,  2008,  1996,  5159,  1998,  3517, 13139,  2024,
         2521,  4237,  1012,  2085,  2057,  2342,  2000,  2424,  1996,  9723,
         1997,  2893,  1037,  3643,  1997,  1059,  2475,  2008,  2003,  3469,
         1006,  1045,  1012,  1041,  1012,  1010,  1523,  4788,  1524,  2005,
         1996, 10744,  1007,  2084,  1996,  3643, 10174,  2682,  1012,  2028,
         2064,  2265,  2008,  2057,  2031,  2000,  2224,  1037,  2061,  1011,
         2170,  9610,  1011, 19942,  4353,  1006,  1008,  1007,  1006,  2030,
         1059,  2475,  1011,  4353,  1007,  1012,  1999,  2023,  2553,  1010,
         2057,  2360,  2008,  2057,  2224,  1037,  9610,  1011, 19942,  4353,
         2007,  2028,  3014,  1997,  4071,  1006,  1008,  1007,  1006,  2062,
         2006,  2023,  2101,  1007,  1012,  2057,  2064,  2131,  1037,  5931,
        10197,  1997,  1996,  9723,  2011,  2559,  2012,  1996,  2795,  1997,
         9610,  1011, 19942,  4353,  1006,  2007,  2028,  3014,  1997,  4071,
         1007,  1999, 15775,  2361,  1012,  1023,  1012,  2182,  2009,  2003,
         2179,  2008,   102])"
729,1,['fractile'], Introduction to ChiSquared Test,seg_149,– the 97.5% fractile is 5.02. – the 99% fractile is 6.63.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  1516,  1996,  5989,  1012,  1019,  1003, 25312,  6593,  9463,
         2003,  1019,  1012,  6185,  1012,  1516,  1996,  5585,  1003, 25312,
         6593,  9463,  2003,  1020,  1012,  6191,  1012,   102])"
730,1,"['probability', 'hypothesis']", Introduction to ChiSquared Test,seg_149,"this means that the probability of getting a value greater than 5.79 is somewhere between 1% and 2.5%. we can show (see section 5.7.4) that the probability is actually in 0.016 or 1.6%. 3. if this probability is small (typically less than 5%), reject the hypothesis.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  2023,  2965,  2008,  1996,  9723,  1997,  2893,  1037,  3643,
         3618,  2084,  1019,  1012,  6535,  2003,  4873,  2090,  1015,  1003,
         1998,  1016,  1012,  1019,  1003,  1012,  2057,  2064,  2265,  1006,
         2156,  2930,  1019,  1012,  1021,  1012,  1018,  1007,  2008,  1996,
         9723,  2003,  2941,  1999,  1014,  1012,  5890,  2575,  2030,  1015,
         1012,  1020,  1003,  1012,  1017,  1012,  2065,  2023,  9723,  2003,
         2235,  1006,  4050,  2625,  2084,  1019,  1003,  1007,  1010, 15454,
         1996, 10744,  1012,   102])"
731,0,[], Introduction to ChiSquared Test,seg_149,"otherwise, accept it.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([ 101, 4728, 1010, 5138, 2009, 1012,  102])"
732,1,"['probability', 'hypothesis', 'independent']", Introduction to ChiSquared Test,seg_149,"we have seen that the probability of getting a value of w2 above 5.79 is between 1% and 2.5%. as this is below 5%, we reject the hypothesis, that the proportion doing strength training is independent of sex. this was what we had expected.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  2057,  2031,  2464,  2008,  1996,  9723,  1997,  2893,  1037,
         3643,  1997,  1059,  2475,  2682,  1019,  1012,  6535,  2003,  2090,
         1015,  1003,  1998,  1016,  1012,  1019,  1003,  1012,  2004,  2023,
         2003,  2917,  1019,  1003,  1010,  2057, 15454,  1996, 10744,  1010,
         2008,  1996, 10817,  2725,  3997,  2731,  2003,  2981,  1997,  3348,
         1012,  2023,  2001,  2054,  2057,  2018,  3517,  1012,   102])"
733,1,"['function', 'density function', 'degree of freedom', 'distribution']", Introduction to ChiSquared Test,seg_149,"here, we see the density function of chi-squared distribution with one degree of freedom. it can be seen that the value 5.79 is pretty “extreme” in the distribution (fig. 5.8).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4955,  2000,  9610,  2015, 16211,  5596,  3231])","tensor([  101,  2182,  1010,  2057,  2156,  1996,  4304,  3853,  1997,  9610,
         1011, 19942,  4353,  2007,  2028,  3014,  1997,  4071,  1012,  2009,
         2064,  2022,  2464,  2008,  1996,  3643,  1019,  1012,  6535,  2003,
         3492,  1523,  6034,  1524,  1999,  1996,  4353,  1006, 20965,  1012,
         1019,  1012,  1022,  1007,  1012,   102])"
734,1,['loss'], Confidence Interval for Difference Between Two Proportions,seg_151,this section can be omitted without loss of continuity.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  4489,  2090,  2048, 19173])","tensor([  101,  2023,  2930,  2064,  2022, 16647,  2302,  3279,  1997, 13717,
         1012,   102])"
735,1,['statistical'], Confidence Interval for Difference Between Two Proportions,seg_151,we have seen above that there is statistical evidence that the proportions of girls and boys doing strength training are indeed different.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  4489,  2090,  2048, 19173])","tensor([  101,  2057,  2031,  2464,  2682,  2008,  2045,  2003,  7778,  3350,
         2008,  1996, 19173,  1997,  3057,  1998,  3337,  2725,  3997,  2731,
         2024,  5262,  2367,  1012,   102])"
736,0,[], Confidence Interval for Difference Between Two Proportions,seg_151,it seems natural to ask the question: how large is the difference between the two proportions?,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 7023, 13483,  2005,  4489,  2090,  2048, 19173])","tensor([  101,  2009,  3849,  3019,  2000,  3198,  1996,  3160,  1024,  2129,
         2312,  2003,  1996,  4489,  2090,  1996,  2048, 19173,  1029,   102])"
737,1,"['estimated', 'estimate']", Confidence Interval for Difference Between Two Proportions,seg_151,"the estimated proportions of boys and girls doing strength training are 59% ¼ 0.59 and 15% ¼ 0.15, so we estimate the difference between the two proportions to be 0.59 0.15 ¼ 0.44.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  4489,  2090,  2048, 19173])","tensor([  101,  1996,  4358, 19173,  1997,  3337,  1998,  3057,  2725,  3997,
         2731,  2024,  5354,  1003,  1091,  1014,  1012,  5354,  1998,  2321,
         1003,  1091,  1014,  1012,  2321,  1010,  2061,  2057, 10197,  1996,
         4489,  2090,  1996,  2048, 19173,  2000,  2022,  1014,  1012,  5354,
         1014,  1012,  2321,  1091,  1014,  1012,  4008,  1012,   102])"
738,1,"['uncertainty', 'statistical uncertainty', 'estimated', 'statistical']", Confidence Interval for Difference Between Two Proportions,seg_151,the statistical uncertainty of the estimated difference between the two proportions is:,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  4489,  2090,  2048, 19173])","tensor([  101,  1996,  7778, 12503,  1997,  1996,  4358,  4489,  2090,  1996,
         2048, 19173,  2003,  1024,   102])"
739,1,"['sample', 'frequencies']", Confidence Interval for Difference Between Two Proportions,seg_151,"here p is the proportion in the whole sample doing strength training, and n1 and n2 are the total frequencies of boys and girls in the sample.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  4489,  2090,  2048, 19173])","tensor([  101,  2182,  1052,  2003,  1996, 10817,  1999,  1996,  2878,  7099,
         2725,  3997,  2731,  1010,  1998,  1050,  2487,  1998,  1050,  2475,
         2024,  1996,  2561, 13139,  1997,  3337,  1998,  3057,  1999,  1996,
         7099,  1012,   102])"
740,1,"['uncertainty', 'table', 'statistical uncertainty', 'tables', 'statistical']", Confidence Interval for Difference Between Two Proportions,seg_151,"from the tables earlier in the chapter, we obtain table 5.6. using the formula above, we obtain the statistical uncertainty for the difference between the two proportions as u ¼ 0.35.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  4489,  2090,  2048, 19173])","tensor([  101,  2013,  1996,  7251,  3041,  1999,  1996,  3127,  1010,  2057,
         6855,  2795,  1019,  1012,  1020,  1012,  2478,  1996,  5675,  2682,
         1010,  2057,  6855,  1996,  7778, 12503,  2005,  1996,  4489,  2090,
         1996,  2048, 19173,  2004,  1057,  1091,  1014,  1012,  3486,  1012,
          102])"
741,1,"['interval', 'confidence', 'confidence interval']", Confidence Interval for Difference Between Two Proportions,seg_151,"the difference between the two proportions is 0.44. thus, a 95% confidence interval for the difference is 0.44 0.35.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  4489,  2090,  2048, 19173])","tensor([  101,  1996,  4489,  2090,  1996,  2048, 19173,  2003,  1014,  1012,
         4008,  1012,  2947,  1010,  1037,  5345,  1003,  7023, 13483,  2005,
         1996,  4489,  2003,  1014,  1012,  4008,  1014,  1012,  3486,  1012,
          102])"
742,1,"['interval', 'confidence', 'confidence interval', 'hypothesis']", Confidence Interval for Difference Between Two Proportions,seg_151,"the confidence interval goes from 0.09 ¼ 9% to 0.79 ¼ 79%. the confidence interval does not contain 0, in accordance with the fact that the hypothesis, that the two proportions are identical, is rejected.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7023, 13483,  2005,  4489,  2090,  2048, 19173])","tensor([  101,  1996,  7023, 13483,  3632,  2013,  1014,  1012,  5641,  1091,
         1023,  1003,  2000,  1014,  1012,  6535,  1091,  6535,  1003,  1012,
         1996,  7023, 13483,  2515,  2025,  5383,  1014,  1010,  1999, 10388,
         2007,  1996,  2755,  2008,  1996, 10744,  1010,  2008,  1996,  2048,
        19173,  2024,  7235,  1010,  2003,  5837,  1012,   102])"
743,1,"['confidence interval', 'interval', 'sample', 'sample size', 'confidence']", Confidence Interval for Difference Between Two Proportions,seg_151,"it may seem that we do not knowmuch about the size of this difference (except that it is not 0). if we need a narrower confidence interval, wemust increase the sample size.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0.])","tensor([ 7023, 13483,  2005,  4489,  2090,  2048, 19173])","tensor([  101,  2009,  2089,  4025,  2008,  2057,  2079,  2025,  2113, 12274,
         2818,  2055,  1996,  2946,  1997,  2023,  4489,  1006,  3272,  2008,
         2009,  2003,  2025,  1014,  1007,  1012,  2065,  2057,  2342,  1037,
        22546,  7023, 13483,  1010,  2057,  7606,  2102,  3623,  1996,  7099,
         2946,  1012,   102])"
744,1,"['categories', 'table', 'distributions', 'statistical test', 'statistical', 'test']", Several Rows andor Columns,seg_153,the statistical test we have described above can be used generally for the comparison of several distributions of a grouping of several categories. there can therefore be more than two rows and/or columns in the table.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  1996,  7778,  3231,  2057,  2031,  2649,  2682,  2064,  2022,
         2109,  3227,  2005,  1996,  7831,  1997,  2195, 20611,  1997,  1037,
        19765,  1997,  2195,  7236,  1012,  2045,  2064,  3568,  2022,  2062,
         2084,  2048, 10281,  1998,  1013,  2030,  7753,  1999,  1996,  2795,
         1012,   102])"
745,1,"['table', 'frequency', 'combinations']", Several Rows andor Columns,seg_153,"as an example, let us return to table 2.12, which was shown in chap. 2. the fitness club kids were asked whether they do cardiovascular workouts or not. moreover, they were asked how they perceive their physical fitness. here is the frequency of kids in all combinations of the two questions (table 5.7).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  2004,  2019,  2742,  1010,  2292,  2149,  2709,  2000,  2795,
         1016,  1012,  2260,  1010,  2029,  2001,  3491,  1999, 15775,  2361,
         1012,  1016,  1012,  1996, 10516,  2252,  4268,  2020,  2356,  3251,
         2027,  2079, 22935, 27090,  2015,  2030,  2025,  1012,  9308,  1010,
         2027,  2020,  2356,  2129,  2027, 23084,  2037,  3558, 10516,  1012,
         2182,  2003,  1996,  6075,  1997,  4268,  1999,  2035, 14930,  1997,
         1996,  2048,  3980,  1006,  2795,  1019,  1012,  1021,  1007,  1012,
          102])"
746,1,"['hypothesis', 'frequencies', 'relative frequencies', 'independent']", Several Rows andor Columns,seg_153,"we will examine whether physical fitness is independent of cardiovascular workouts. the hypothesis is that there are the same relative frequencies (proportions) among the rows, i.e., cardiovascular workouts do not affect physical fitness.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  2057,  2097, 11628,  3251,  3558, 10516,  2003,  2981,  1997,
        22935, 27090,  2015,  1012,  1996, 10744,  2003,  2008,  2045,  2024,
         1996,  2168,  5816, 13139,  1006, 19173,  1007,  2426,  1996, 10281,
         1010,  1045,  1012,  1041,  1012,  1010, 22935, 27090,  2015,  2079,
         2025,  7461,  3558, 10516,  1012,   102])"
747,1,"['frequencies', 'frequency', 'table']", Several Rows andor Columns,seg_153,"you can use the same procedure as above, i.e., first we calculate the expected frequencies. they are shown in table 5.8. for example, the expected frequency of kids doing cardiovascular workouts, who have a bad physical fitness, is calculated as:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  2017,  2064,  2224,  1996,  2168,  7709,  2004,  2682,  1010,
         1045,  1012,  1041,  1012,  1010,  2034,  2057, 18422,  1996,  3517,
        13139,  1012,  2027,  2024,  3491,  1999,  2795,  1019,  1012,  1022,
         1012,  2005,  2742,  1010,  1996,  3517,  6075,  1997,  4268,  2725,
        22935, 27090,  2015,  1010,  2040,  2031,  1037,  2919,  3558, 10516,
         1010,  2003, 10174,  2004,  1024,   102])"
748,1,"['method', 'frequencies']", Several Rows andor Columns,seg_153,"15 9/30 ¼ 4.5. we observe that several expected frequencies are smaller than 5, but none are smaller than 4.5. strictly speaking, it is not allowed to use the method described above; however, it will hardly be a very big mistake.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  2321,  1023,  1013,  2382,  1091,  1018,  1012,  1019,  1012,
         2057, 11949,  2008,  2195,  3517, 13139,  2024,  3760,  2084,  1019,
         1010,  2021,  3904,  2024,  3760,  2084,  1018,  1012,  1019,  1012,
         9975,  4092,  1010,  2009,  2003,  2025,  3039,  2000,  2224,  1996,
         4118,  2649,  2682,  1025,  2174,  1010,  2009,  2097,  6684,  2022,
         1037,  2200,  2502,  6707,  1012,   102])"
749,0,[], Several Rows andor Columns,seg_153,we can now calculate w2 using the formula:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  2057,  2064,  2085, 18422,  1059,  2475,  2478,  1996,  5675,
         1024,   102])"
750,0,[], Several Rows andor Columns,seg_153,this gives the result 2.00.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([ 101, 2023, 3957, 1996, 2765, 1016, 1012, 4002, 1012,  102])"
751,1,"['distribution', 'degrees of freedom']", Several Rows andor Columns,seg_153,how many degrees of freedom should be used for the chi-square distribution? it turns out that the general formula is: degrees of freedom ¼ (number of rows 1) (number of columns 1),tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  2129,  2116,  5445,  1997,  4071,  2323,  2022,  2109,  2005,
         1996,  9610,  1011,  2675,  4353,  1029,  2009,  4332,  2041,  2008,
         1996,  2236,  5675,  2003,  1024,  5445,  1997,  4071,  1091,  1006,
         2193,  1997, 10281,  1015,  1007,  1006,  2193,  1997,  7753,  1015,
         1007,   102])"
752,1,['degrees of freedom'], Several Rows andor Columns,seg_153,"in this example, there are two rows (no/yes) and three columns (bad/medium/ good). therefore, the number of degrees of freedom is df ¼ (2 1) (3 1) ¼ 1 2 ¼ 2.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  1999,  2023,  2742,  1010,  2045,  2024,  2048, 10281,  1006,
         2053,  1013,  2748,  1007,  1998,  2093,  7753,  1006,  2919,  1013,
         5396,  1013,  2204,  1007,  1012,  3568,  1010,  1996,  2193,  1997,
         5445,  1997,  4071,  2003,  1040,  2546,  1091,  1006,  1016,  1015,
         1007,  1006,  1017,  1015,  1007,  1091,  1015,  1016,  1091,  1016,
         1012,   102])"
753,1,"['table', 'fractile', 'degrees of freedom']", Several Rows andor Columns,seg_153,"in the table in chap. 9, we find that the 95% fractile of the chi-squared distribution with two degrees of freedom is 5.99. we have calculated a value w2 ¼ 2.00.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  1999,  1996,  2795,  1999, 15775,  2361,  1012,  1023,  1010,
         2057,  2424,  2008,  1996,  5345,  1003, 25312,  6593,  9463,  1997,
         1996,  9610,  1011, 19942,  4353,  2007,  2048,  5445,  1997,  4071,
         2003,  1019,  1012,  5585,  1012,  2057,  2031, 10174,  1037,  3643,
         1059,  2475,  1091,  1016,  1012,  4002,  1012,   102])"
754,1,"['probability', 'fractile']", Several Rows andor Columns,seg_153,"this is far smaller than the 95% fractile, i.e., the probability of getting a larger value is (probably a lot) larger than 5%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  2023,  2003,  2521,  3760,  2084,  1996,  5345,  1003, 25312,
         6593,  9463,  1010,  1045,  1012,  1041,  1012,  1010,  1996,  9723,
         1997,  2893,  1037,  3469,  3643,  2003,  1006,  2763,  1037,  2843,
         1007,  3469,  2084,  1019,  1003,  1012,   102])"
755,1,"['mean', 'hypothesis', 'statistical', 'independent']", Several Rows andor Columns,seg_153,"therefore, we accept the hypothesis that physical fitness is independent of doing cardiovascular workouts. this does not necessarily mean that the hypothesis is true. we just do not have statistical evidence to reject it.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  3568,  1010,  2057,  5138,  1996, 10744,  2008,  3558, 10516,
         2003,  2981,  1997,  2725, 22935, 27090,  2015,  1012,  2023,  2515,
         2025,  9352,  2812,  2008,  1996, 10744,  2003,  2995,  1012,  2057,
         2074,  2079,  2025,  2031,  7778,  3350,  2000, 15454,  2009,  1012,
          102])"
756,1,"['function', 'probability']", Several Rows andor Columns,seg_153,"we can also calculate the probability directly using the chitest function in a spreadsheet (see the next section) and get approx. 37%, much larger than 5%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  2057,  2064,  2036, 18422,  1996,  9723,  3495,  2478,  1996,
         9610, 22199,  3853,  1999,  1037, 20861, 21030,  2102,  1006,  2156,
         1996,  2279,  2930,  1007,  1998,  2131, 22480,  1012,  4261,  1003,
         1010,  2172,  3469,  2084,  1019,  1003,  1012,   102])"
757,1,"['function', 'density function', 'degrees of freedom', 'distribution']", Several Rows andor Columns,seg_153,the following is the density function for a chi-squared distribution with two degrees of freedom. it is evident that the value 2.00 by no means is “extreme” in the distribution (fig. 5.9).,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  1996,  2206,  2003,  1996,  4304,  3853,  2005,  1037,  9610,
         1011, 19942,  4353,  2007,  2048,  5445,  1997,  4071,  1012,  2009,
         2003, 10358,  2008,  1996,  3643,  1016,  1012,  4002,  2011,  2053,
         2965,  2003,  1523,  6034,  1524,  1999,  1996,  4353,  1006, 20965,
         1012,  1019,  1012,  1023,  1007,  1012,   102])"
758,1,"['frequencies', 'table', 'data', 'statistical']", Several Rows andor Columns,seg_153,"from the table of frequencies, one might suggest a trend that kids not doing cardiovascular workouts have a worse physical fitness compared to kids doing cardiovascular workouts. however, there is no statistical evidence in the data to support this assumption!",tensor(1),"tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  2013,  1996,  2795,  1997, 13139,  1010,  2028,  2453,  6592,
         1037,  9874,  2008,  4268,  2025,  2725, 22935, 27090,  2015,  2031,
         1037,  4788,  3558, 10516,  4102,  2000,  4268,  2725, 22935, 27090,
         2015,  1012,  2174,  1010,  2045,  2003,  2053,  7778,  3350,  1999,
         1996,  2951,  2000,  2490,  2023, 11213,   999,   102])"
759,1,"['case', 'hypothesis', 'statistical test', 'independence', 'quantitative', 'statistical', 'variables', 'variable', 'test']", Several Rows andor Columns,seg_153,"note: the hypothesis that there is independence between the row and column variables is accepted. in the case, when the hypothesis is rejected, this is itself not a guarantee that there is a causal relationship between the two variables! and if there is a causal relationship, the statistical test cannot tell which variable is cause and which variable is effect. it might also be that there exists an “indirect relationship” between the two variables, i.e., both variables are related to a third variable. we return to this issue in chap. 7 in relation to quantitative variables.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  3602,  1024,  1996, 10744,  2008,  2045,  2003,  4336,  2090,
         1996,  5216,  1998,  5930, 10857,  2003,  3970,  1012,  1999,  1996,
         2553,  1010,  2043,  1996, 10744,  2003,  5837,  1010,  2023,  2003,
         2993,  2025,  1037, 11302,  2008,  2045,  2003,  1037, 28102,  3276,
         2090,  1996,  2048, 10857,   999,  1998,  2065,  2045,  2003,  1037,
        28102,  3276,  1010,  1996,  7778,  3231,  3685,  2425,  2029,  8023,
         2003,  3426,  1998,  2029,  8023,  2003,  3466,  1012,  2009,  2453,
         2036,  2022,  2008,  2045,  6526,  2019,  1523, 14958,  3276,  1524,
         2090,  1996,  2048, 10857,  1010,  1045,  1012,  1041,  1012,  1010,
         2119, 10857,  2024,  3141,  2000,  1037,  2353,  8023,  1012,  2057,
         2709,  2000,  2023,  3277,  1999, 15775,  2361,  1012,  1021,  1999,
         7189,  2000, 20155, 10857,  1012,   102])"
760,1,"['sample', 'experiments', 'statistical']", Several Rows andor Columns,seg_153,"in this chapter we have seen some techniques, which are useful in the statistical analysis of sample surveys and experiments. in the next chapter, we look at some issues within planning of sample surveys and experiments. first, we show how the calculations above can most easily be done by calculator or spreadsheet.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2195, 10281,  1998,  2953,  7753])","tensor([  101,  1999,  2023,  3127,  2057,  2031,  2464,  2070,  5461,  1010,
         2029,  2024,  6179,  1999,  1996,  7778,  4106,  1997,  7099, 12265,
         1998,  7885,  1012,  1999,  1996,  2279,  3127,  1010,  2057,  2298,
         2012,  2070,  3314,  2306,  4041,  1997,  7099, 12265,  1998,  7885,
         1012,  2034,  1010,  2057,  2265,  2129,  1996, 16268,  2682,  2064,
         2087,  4089,  2022,  2589,  2011, 10250, 19879,  4263,  2030, 20861,
        21030,  2102,  1012,   102])"
761,0,[], Calculations in Spreadsheets,seg_155,"if you do not use spreadsheets, you can skip this section.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([16268,  1999, 20861, 21030,  3215])","tensor([  101,  2065,  2017,  2079,  2025,  2224, 20861, 21030,  3215,  1010,
         2017,  2064, 13558,  2023,  2930,  1012,   102])"
762,1,"['frequencies', 'table']", Calculations in Spreadsheets,seg_155,we use the table of observed frequencies of fitness club kids doing strength training grouped according to sex (fig. 5.10).,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([16268,  1999, 20861, 21030,  3215])","tensor([  101,  2057,  2224,  1996,  2795,  1997,  5159, 13139,  1997, 10516,
         2252,  4268,  2725,  3997,  2731, 15131,  2429,  2000,  3348,  1006,
        20965,  1012,  1019,  1012,  2184,  1007,  1012,   102])"
763,1,['frequencies'], Calculations in Spreadsheets,seg_155,"first an explanation regarding the expected frequencies: for instance, cell b9 is calculated as 17 12/30.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([16268,  1999, 20861, 21030,  3215])","tensor([  101,  2034,  2019,  7526,  4953,  1996,  3517, 13139,  1024,  2005,
         6013,  1010,  3526,  1038,  2683,  2003, 10174,  2004,  2459,  2260,
         1013,  2382,  1012,   102])"
764,0,[], Calculations in Spreadsheets,seg_155,the formula in cell b9 can thus be programmed like this:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([16268,  1999, 20861, 21030,  3215])","tensor([  101,  1996,  5675,  1999,  3526,  1038,  2683,  2064,  2947,  2022,
        16984,  2066,  2023,  1024,   102])"
765,0,[], Calculations in Spreadsheets,seg_155,"however, it is an advantage, if you program cell b9 like this:",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([16268,  1999, 20861, 21030,  3215])","tensor([ 101, 2174, 1010, 2009, 2003, 2019, 5056, 1010, 2065, 2017, 2565, 3526,
        1038, 2683, 2066, 2023, 1024,  102])"
766,1,['frequencies'], Calculations in Spreadsheets,seg_155,"the dollar signs are “absolute references”; see more in the help of your spreadsheet. if you copy cell b9 over the area b9:c10 (i.e., all the expected frequencies), the references will be kept properly. they will always be referred to the correct cells.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([16268,  1999, 20861, 21030,  3215])","tensor([  101,  1996,  7922,  5751,  2024,  1523,  7619,  7604,  1524,  1025,
         2156,  2062,  1999,  1996,  2393,  1997,  2115, 20861, 21030,  2102,
         1012,  2065,  2017,  6100,  3526,  1038,  2683,  2058,  1996,  2181,
         1038,  2683,  1024, 27723,  2692,  1006,  1045,  1012,  1041,  1012,
         1010,  2035,  1996,  3517, 13139,  1007,  1010,  1996,  7604,  2097,
         2022,  2921,  7919,  1012,  2027,  2097,  2467,  2022,  3615,  2000,
         1996,  6149,  4442,  1012,   102])"
767,1,"['frequencies', 'function']", Calculations in Spreadsheets,seg_155,"when you have calculated the expected frequencies, the rest is easy. see cell b13, how to use the spreadsheet function chitest.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([16268,  1999, 20861, 21030,  3215])","tensor([  101,  2043,  2017,  2031, 10174,  1996,  3517, 13139,  1010,  1996,
         2717,  2003,  3733,  1012,  2156,  3526, 29491,  2509,  1010,  2129,
         2000,  2224,  1996, 20861, 21030,  2102,  3853,  9610, 22199,  1012,
          102])"
768,1,"['frequencies', 'function']", Calculations in Spreadsheets,seg_155,"for the function chitest, you specify the cells with the observed frequencies (cell b3:c4) and the cells with the expected frequencies (cell b9:c10).",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([16268,  1999, 20861, 21030,  3215])","tensor([  101,  2005,  1996,  3853,  9610, 22199,  1010,  2017, 20648,  1996,
         4442,  2007,  1996,  5159, 13139,  1006,  3526,  1038,  2509,  1024,
         1039,  2549,  1007,  1998,  1996,  4442,  2007,  1996,  3517, 13139,
         1006,  3526,  1038,  2683,  1024, 27723,  2692,  1007,  1012,   102])"
769,1,"['probability', 'outcome']", Calculations in Spreadsheets,seg_155,"the result is the p-value, i.e., the probability of getting an outcome that is at least as rare as the observed outcome. it is approximately 0.016 or 1.6%. previously, we found that the p-value is between 1% and 2.5%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([16268,  1999, 20861, 21030,  3215])","tensor([ 101, 1996, 2765, 2003, 1996, 1052, 1011, 3643, 1010, 1045, 1012, 1041,
        1012, 1010, 1996, 9723, 1997, 2893, 2019, 9560, 2008, 2003, 2012, 2560,
        2004, 4678, 2004, 1996, 5159, 9560, 1012, 2009, 2003, 3155, 1014, 1012,
        5890, 2575, 2030, 1015, 1012, 1020, 1003, 1012, 3130, 1010, 2057, 2179,
        2008, 1996, 1052, 1011, 3643, 2003, 2090, 1015, 1003, 1998, 1016, 1012,
        1019, 1003, 1012,  102])"
770,1,['function'], Calculations in Spreadsheets,seg_155,"we do not get the value of w2, but we really do not need it for anything! if we want it after all, it can be calculated as shown in cell a14. we do an “inverse calculation” from the p-value using the function chiinv. for the function",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 0.])","tensor([16268,  1999, 20861, 21030,  3215])","tensor([  101,  2057,  2079,  2025,  2131,  1996,  3643,  1997,  1059,  2475,
         1010,  2021,  2057,  2428,  2079,  2025,  2342,  2009,  2005,  2505,
          999,  2065,  2057,  2215,  2009,  2044,  2035,  1010,  2009,  2064,
         2022, 10174,  2004,  3491,  1999,  3526, 17350,  2549,  1012,  2057,
         2079,  2019,  1523, 19262, 17208,  1524,  2013,  1996,  1052,  1011,
         3643,  2478,  1996,  3853,  9610,  2378,  2615,  1012,  2005,  1996,
         3853,   102])"
771,1,"['degrees of freedom', 'case']", Calculations in Spreadsheets,seg_155,"chiinv, you specify the p-value and the number of degrees of freedom, in this case 1. we get the same result 5.79, as was obtained previously.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([16268,  1999, 20861, 21030,  3215])","tensor([  101,  9610,  2378,  2615,  1010,  2017, 20648,  1996,  1052,  1011,
         3643,  1998,  1996,  2193,  1997,  5445,  1997,  4071,  1010,  1999,
         2023,  2553,  1015,  1012,  2057,  2131,  1996,  2168,  2765,  1019,
         1012,  6535,  1010,  2004,  2001,  4663,  3130,  1012,   102])"
772,0,[], Calculations by Calculator,seg_157,"if you do not use a calculator, you can skip this section. you need a mathematical calculator with logarithms.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0])","tensor([16268,  2011, 10250, 19879,  4263])","tensor([  101,  2065,  2017,  2079,  2025,  2224,  1037, 10250, 19879,  4263,
         1010,  2017,  2064, 13558,  2023,  2930,  1012,  2017,  2342,  1037,
         8045, 10250, 19879,  4263,  2007,  8833,  8486,  2705,  5244,  1012,
          102])"
773,1,"['frequencies', 'table']", Calculations by Calculator,seg_157,we use the table of observed frequencies of fitness club kids doing strength training grouped according to sex (table 5.9).,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([16268,  2011, 10250, 19879,  4263])","tensor([  101,  2057,  2224,  1996,  2795,  1997,  5159, 13139,  1997, 10516,
         2252,  4268,  2725,  3997,  2731, 15131,  2429,  2000,  3348,  1006,
         2795,  1019,  1012,  1023,  1007,  1012,   102])"
774,1,['sample'], Calculations by Calculator,seg_157,"there is another formula for calculating w2, which is much easier to use on a calculator. it will give practically the same result as the above formula. the larger the sample, the better the agreement will be.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([16268,  2011, 10250, 19879,  4263])","tensor([  101,  2045,  2003,  2178,  5675,  2005, 20177,  1059,  2475,  1010,
         2029,  2003,  2172,  6082,  2000,  2224,  2006,  1037, 10250, 19879,
         4263,  1012,  2009,  2097,  2507,  8134,  1996,  2168,  2765,  2004,
         1996,  2682,  5675,  1012,  1996,  3469,  1996,  7099,  1010,  1996,
         2488,  1996,  3820,  2097,  2022,  1012,   102])"
775,1,['table'], Calculations by Calculator,seg_157,"in the table, we calculate a contribution from each number, including the subtotals and the total.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([16268,  2011, 10250, 19879,  4263])","tensor([  101,  1999,  1996,  2795,  1010,  2057, 18422,  1037,  6691,  2013,
         2169,  2193,  1010,  2164,  1996,  4942,  3406,  9080,  2015,  1998,
         1996,  2561,  1012,   102])"
776,1,"['function', 'table']", Calculations by Calculator,seg_157,"every contribution is of the form x·ln(x), where x is one of the numbers of the table and ln is the natural logarithm function, which is available on most mathematical calculators.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([16268,  2011, 10250, 19879,  4263])","tensor([  101,  2296,  6691,  2003,  1997,  1996,  2433,  1060,  1087,  1048,
         2078,  1006,  1060,  1007,  1010,  2073,  1060,  2003,  2028,  1997,
         1996,  3616,  1997,  1996,  2795,  1998,  1048,  2078,  2003,  1996,
         3019,  8833,  8486,  2705,  2213,  3853,  1010,  2029,  2003,  2800,
         2006,  2087,  8045, 10250, 19879,  6591,  1012,   102])"
777,1,"['frequencies', 'table']", Calculations by Calculator,seg_157,"the contribution corresponding to the observed frequencies in the 2 2 table and the total is added, the remaining contributions (corresponding to subtotals of each row and each column) are subtracted. finally, we multiply by 2.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([16268,  2011, 10250, 19879,  4263])","tensor([  101,  1996,  6691,  7978,  2000,  1996,  5159, 13139,  1999,  1996,
         1016,  1016,  2795,  1998,  1996,  2561,  2003,  2794,  1010,  1996,
         3588,  5857,  1006,  7978,  2000,  4942,  3406,  9080,  2015,  1997,
         2169,  5216,  1998,  2169,  5930,  1007,  2024,  4942, 24301,  1012,
         2633,  1010,  2057,  4800, 22086,  2011,  1016,  1012,   102])"
778,0,[], Calculations by Calculator,seg_157,"in the example, the calculations look like this:",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([16268,  2011, 10250, 19879,  4263])","tensor([  101,  1999,  1996,  2742,  1010,  1996, 16268,  2298,  2066,  2023,
         1024,   102])"
779,0,[], Calculations by Calculator,seg_157,"we get a value of w2, which is pretty close to the one obtained previously. the conclusion remains the same.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0])","tensor([16268,  2011, 10250, 19879,  4263])","tensor([ 101, 2057, 2131, 1037, 3643, 1997, 1059, 2475, 1010, 2029, 2003, 3492,
        2485, 2000, 1996, 2028, 4663, 3130, 1012, 1996, 7091, 3464, 1996, 2168,
        1012,  102])"
780,1,['frequencies'], Calculations by Calculator,seg_157,the advantage of this formula is that we do not need to calculate the expected frequencies. this can be pretty cumbersome if there are many rows and columns.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([16268,  2011, 10250, 19879,  4263])","tensor([  101,  1996,  5056,  1997,  2023,  5675,  2003,  2008,  2057,  2079,
         2025,  2342,  2000, 18422,  1996,  3517, 13139,  1012,  2023,  2064,
         2022,  3492, 13988, 17198,  8462,  2065,  2045,  2024,  2116, 10281,
         1998,  7753,  1012,   102])"
781,1,"['column total', 'frequencies', 'frequency', 'control', 'row total', 'condition']", Calculations by Calculator,seg_157,"with this approach, we cannot control the necessary condition that all expected frequencies are at least 5. one easy solution is to calculate the minimum expected frequency using the minimum row total and the minimum column total and this should be >5.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([16268,  2011, 10250, 19879,  4263])","tensor([  101,  2007,  2023,  3921,  1010,  2057,  3685,  2491,  1996,  4072,
         4650,  2008,  2035,  3517, 13139,  2024,  2012,  2560,  1019,  1012,
         2028,  3733,  5576,  2003,  2000, 18422,  1996,  6263,  3517,  6075,
         2478,  1996,  6263,  5216,  2561,  1998,  1996,  6263,  5930,  2561,
         1998,  2023,  2323,  2022,  1028,  1019,  1012,   102])"
782,1,"['row totals', 'column totals', 'frequency']", Calculations by Calculator,seg_157,"here the row totals are 17 and 13; the smallest value is 13 from row 2. the column totals are 12 and 18; the smallest value is 12 from column 1. thus, in row 2, column 1, we get theminimum expected frequency. this was previously calculated to be 5.20> 5.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([16268,  2011, 10250, 19879,  4263])","tensor([  101,  2182,  1996,  5216, 21948,  2024,  2459,  1998,  2410,  1025,
         1996, 10479,  3643,  2003,  2410,  2013,  5216,  1016,  1012,  1996,
         5930, 21948,  2024,  2260,  1998,  2324,  1025,  1996, 10479,  3643,
         2003,  2260,  2013,  5930,  1015,  1012,  2947,  1010,  1999,  5216,
         1016,  1010,  5930,  1015,  1010,  2057,  2131,  2068,  5498, 27147,
         3517,  6075,  1012,  2023,  2001,  3130, 10174,  2000,  2022,  1019,
         1012,  2322,  1028,  1019,  1012,   102])"
783,1,"['variation', 'systematic variation', 'random variation', 'random', 'data']", Two Kinds of Errors,seg_161,we saw in chap. 3 that there are two types of variation in data: systematic variation and random variation.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        1., 1., 0., 1., 1., 0., 0.])","tensor([ 2048,  7957,  1997, 10697])","tensor([  101,  2057,  2387,  1999, 15775,  2361,  1012,  1017,  2008,  2045,
         2024,  2048,  4127,  1997,  8386,  1999,  2951,  1024, 11778,  8386,
         1998,  6721,  8386,  1012,   102])"
784,1,"['sources of errors', 'sample', 'errors', 'experiments']", Two Kinds of Errors,seg_161,there are therefore two sources of errors in sample surveys as well as in planned experiments:,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0.])","tensor([ 2048,  7957,  1997, 10697])","tensor([  101,  2045,  2024,  3568,  2048,  4216,  1997, 10697,  1999,  7099,
        12265,  2004,  2092,  2004,  1999,  3740,  7885,  1024,   102])"
785,1,"['bias', 'mean', 'random errors', 'dispersion', 'random', 'errors', 'bias ', 'systematic errors']", Two Kinds of Errors,seg_161,"– systematic errors, usually called bias (*): the difference between the true value and the mean. – random errors (*): the dispersion (spread) around the mean.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 1., 0., 0.])","tensor([ 2048,  7957,  1997, 10697])","tensor([  101,  1516, 11778, 10697,  1010,  2788,  2170, 13827,  1006,  1008,
         1007,  1024,  1996,  4489,  2090,  1996,  2995,  3643,  1998,  1996,
         2812,  1012,  1516,  6721, 10697,  1006,  1008,  1007,  1024,  1996,
         4487, 17668, 10992,  1006,  3659,  1007,  2105,  1996,  2812,  1012,
          102])"
786,1,"['random errors', 'sample', 'random', 'errors', 'experiment']", Two Kinds of Errors,seg_161,this is illustrated in fig. 6.1. increasing the size of the sample (or experiment) can reduce the random errors. this can be used in the planning stages to determine an appropriate size of the sample (or experiment). we discuss this in the first part of the chapter.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 2048,  7957,  1997, 10697])","tensor([  101,  2023,  2003,  7203,  1999, 20965,  1012,  1020,  1012,  1015,
         1012,  4852,  1996,  2946,  1997,  1996,  7099,  1006,  2030,  7551,
         1007,  2064,  5547,  1996,  6721, 10697,  1012,  2023,  2064,  2022,
         2109,  1999,  1996,  4041,  5711,  2000,  5646,  2019,  6413,  2946,
         1997,  1996,  7099,  1006,  2030,  7551,  1007,  1012,  2057,  6848,
         2023,  1999,  1996,  2034,  2112,  1997,  1996,  3127,  1012,   102])"
787,1,"['sources of systematic errors', 'sample', 'sampling', 'errors', 'sampling ', 'systematic errors']", Two Kinds of Errors,seg_161,"after that follows some topics exclusively related to sample surveys. first we discuss various sources of systematic errors in sample surveys, in continuation of the discussion in chap. 1. we also give an overview of the principles of sampling (*) or sample selection.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 2048,  7957,  1997, 10697])","tensor([  101,  2044,  2008,  4076,  2070,  7832,  7580,  3141,  2000,  7099,
        12265,  1012,  2034,  2057,  6848,  2536,  4216,  1997, 11778, 10697,
         1999,  7099, 12265,  1010,  1999, 13633,  1997,  1996,  6594,  1999,
        15775,  2361,  1012,  1015,  1012,  2057,  2036,  2507,  2019, 19184,
         1997,  1996,  6481,  1997, 16227,  1006,  1008,  1007,  2030,  7099,
         4989,  1012,   102])"
788,1,"['variability', 'mean', 'random errors', 'dispersion', 'sample', 'random', 'errors', 'experiment']", Random Error and Sample Size,seg_163,"the random errors (*) are coming from general causes of variability, which give rise to a natural variability. the natural variability is reflected in dispersion (spread) around the mean. this dispersion is always present to some extent. the individuals in a sample survey (or an experiment) will never be completely identical.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6721, 7561, 1998, 7099, 2946])","tensor([  101,  1996,  6721, 10697,  1006,  1008,  1007,  2024,  2746,  2013,
         2236,  5320,  1997, 28436,  1010,  2029,  2507,  4125,  2000,  1037,
         3019, 28436,  1012,  1996,  3019, 28436,  2003,  7686,  1999,  4487,
        17668, 10992,  1006,  3659,  1007,  2105,  1996,  2812,  1012,  2023,
         4487, 17668, 10992,  2003,  2467,  2556,  2000,  2070,  6698,  1012,
         1996,  3633,  1999,  1037,  7099,  5002,  1006,  2030,  2019,  7551,
         1007,  2097,  2196,  2022,  3294,  7235,  1012,   102])"
789,1,"['random', 'variation', 'random variation', 'systematic and random variation']", Random Error and Sample Size,seg_163,fig. 6.1 systematic and systematic and random variation random variation,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.])","tensor([6721, 7561, 1998, 7099, 2946])","tensor([  101, 20965,  1012,  1020,  1012,  1015, 11778,  1998, 11778,  1998,
         6721,  8386,  6721,  8386,   102])"
790,1,"['uncertainty', 'statistical uncertainty', 'estimate', 'sample', 'random', 'sampling', 'condition', 'experiment', 'randomization', 'statistical']", Random Error and Sample Size,seg_163,"the following considerations are based on a necessary condition: the experiment or the sample must be organized through randomization (*). this means that an experiment should be conducted in random order, or that sampling should be done by a random selection mechanism (more on this last in the chapter). randomization is necessary in order to be able to estimate the statistical uncertainty!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        1., 0., 0.])","tensor([6721, 7561, 1998, 7099, 2946])","tensor([  101,  1996,  2206, 16852,  2024,  2241,  2006,  1037,  4072,  4650,
         1024,  1996,  7551,  2030,  1996,  7099,  2442,  2022,  4114,  2083,
         6721,  3989,  1006,  1008,  1007,  1012,  2023,  2965,  2008,  2019,
         7551,  2323,  2022,  4146,  1999,  6721,  2344,  1010,  2030,  2008,
        16227,  2323,  2022,  2589,  2011,  1037,  6721,  4989,  7337,  1006,
         2062,  2006,  2023,  2197,  1999,  1996,  3127,  1007,  1012,  6721,
         3989,  2003,  4072,  1999,  2344,  2000,  2022,  2583,  2000, 10197,
         1996,  7778, 12503,   999,   102])"
791,1,"['sample', 'sample size', 'experiment']", Random Error and Sample Size,seg_163,when planning a sample survey or an experiment we often face the question: how large a sample size should we choose? the first question one must consider is what to record on each individual in the sample or experiment.,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0.])","tensor([6721, 7561, 1998, 7099, 2946])","tensor([ 101, 2043, 4041, 1037, 7099, 5002, 2030, 2019, 7551, 2057, 2411, 2227,
        1996, 3160, 1024, 2129, 2312, 1037, 7099, 2946, 2323, 2057, 5454, 1029,
        1996, 2034, 3160, 2028, 2442, 5136, 2003, 2054, 2000, 2501, 2006, 2169,
        3265, 1999, 1996, 7099, 2030, 7551, 1012,  102])"
792,0,[], Random Error and Sample Size,seg_163,"roughly speaking, there are two situations:",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([6721, 7561, 1998, 7099, 2946])","tensor([ 101, 5560, 4092, 1010, 2045, 2024, 2048, 8146, 1024,  102])"
793,1,"['variable', 'quantitative']", Random Error and Sample Size,seg_163,"1. we record a qualitative variable, often an alternative variable, e.g., “yes/no”. 2. we record a quantitative variable, i.e., a number for each individual.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([6721, 7561, 1998, 7099, 2946])","tensor([  101,  1015,  1012,  2057,  2501,  1037, 24209, 11475, 27453,  8023,
         1010,  2411,  2019,  4522,  8023,  1010,  1041,  1012,  1043,  1012,
         1010,  1523,  2748,  1013,  2053,  1524,  1012,  1016,  1012,  2057,
         2501,  1037, 20155,  8023,  1010,  1045,  1012,  1041,  1012,  1010,
         1037,  2193,  2005,  2169,  3265,  1012,   102])"
794,1,"['associated', 'sample', 'experiments', 'variables']", A Qualitative Variable,seg_165,"qualitative variables are usually associated with sample surveys, but may also arise in connection with planned experiments.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101, 24209, 11475, 27453, 10857,  2024,  2788,  3378,  2007,  7099,
        12265,  1010,  2021,  2089,  2036, 13368,  1999,  4434,  2007,  3740,
         7885,  1012,   102])"
795,1,"['uncertainty', 'statistical uncertainty', 'frequency', 'relative frequency', 'statistical']", A Qualitative Variable,seg_165,we saw in chap. 5 that the statistical uncertainty of a relative frequency is given by,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  2057,  2387,  1999, 15775,  2361,  1012,  1019,  2008,  1996,
         7778, 12503,  1997,  1037,  5816,  6075,  2003,  2445,  2011,   102])"
796,1,"['estimate', 'frequency', 'relative frequency', 'sample', 'sample size']", A Qualitative Variable,seg_165,"here n is the sample size and p the relative frequency of one answer category (e.g., the answer “yes” to a question). in this formula, we use the relative frequency x/n from the sample as estimate of p.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  2182,  1050,  2003,  1996,  7099,  2946,  1998,  1052,  1996,
         5816,  6075,  1997,  2028,  3437,  4696,  1006,  1041,  1012,  1043,
         1012,  1010,  1996,  3437,  1523,  2748,  1524,  2000,  1037,  3160,
         1007,  1012,  1999,  2023,  5675,  1010,  2057,  2224,  1996,  5816,
         6075,  1060,  1013,  1050,  2013,  1996,  7099,  2004, 10197,  1997,
         1052,  1012,   102])"
797,1,"['without replacement', 'population', 'sample', 'sampling', 'sample size', 'replacement']", A Qualitative Variable,seg_165,"the formula requires that the sample size n is large enough for the conditions n p > 5 and n(1 p) > 5 to be fulfilled. on the other hand, the sample size should be at most 10% of the population (if sampling is without replacement), see chap. 5.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  1996,  5675,  5942,  2008,  1996,  7099,  2946,  1050,  2003,
         2312,  2438,  2005,  1996,  3785,  1050,  1052,  1028,  1019,  1998,
         1050,  1006,  1015,  1052,  1007,  1028,  1019,  2000,  2022, 16829,
         1012,  2006,  1996,  2060,  2192,  1010,  1996,  7099,  2946,  2323,
         2022,  2012,  2087,  2184,  1003,  1997,  1996,  2313,  1006,  2065,
        16227,  2003,  2302,  6110,  1007,  1010,  2156, 15775,  2361,  1012,
         1019,  1012,   102])"
798,0,[], A Qualitative Variable,seg_165,"in practical calculations one can safely replace 1.96 with 2. there is no important difference in the result, and it makes the calculations much simpler. therefore, we use the simpler formula",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  1999,  6742, 16268,  2028,  2064,  9689,  5672,  1015,  1012,
         5986,  2007,  1016,  1012,  2045,  2003,  2053,  2590,  4489,  1999,
         1996,  2765,  1010,  1998,  2009,  3084,  1996, 16268,  2172, 16325,
         1012,  3568,  1010,  2057,  2224,  1996, 16325,  5675,   102])"
799,0,[], A Qualitative Variable,seg_165,as shown in chap. 5 the formula gives the maximum value for p ¼ 0.5 ¼ 50%.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  2004,  3491,  1999, 15775,  2361,  1012,  1019,  1996,  5675,
         3957,  1996,  4555,  3643,  2005,  1052,  1091,  1014,  1012,  1019,
         1091,  2753,  1003,  1012,   102])"
800,1,"['uncertainty', 'statistical uncertainty', 'frequency', 'relative frequency', 'statistical']", A Qualitative Variable,seg_165,"if we put p ¼ 0.5, the formula can be reduced to the following expression for the maximum statistical uncertainty of a relative frequency:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  2065,  2057,  2404,  1052,  1091,  1014,  1012,  1019,  1010,
         1996,  5675,  2064,  2022,  4359,  2000,  1996,  2206,  3670,  2005,
         1996,  4555,  7778, 12503,  1997,  1037,  5816,  6075,  1024,   102])"
801,1,['statistics'], A Qualitative Variable,seg_165,"this formula is remarkably simple!! yet, i have found it in no other books on statistics!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  2023,  5675,  2003, 17431,  3722,   999,   999,  2664,  1010,
         1045,  2031,  2179,  2009,  1999,  2053,  2060,  2808,  2006,  6747,
          999,   102])"
802,0,[], A Qualitative Variable,seg_165,a few examples:,tensor(0),"tensor([0, 0, 0, 0, 0, 0])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([ 101, 1037, 2261, 4973, 1024,  102])"
803,1,"['statistical', 'statistical uncertainty', 'uncertainty']", A Qualitative Variable,seg_165,– the maximum statistical uncertainty for n ¼ 100 is u ¼ 1/√100 ¼ 1/10 ¼,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  1516,  1996,  4555,  7778, 12503,  2005,  1050,  1091,  2531,
         2003,  1057,  1091,  1015,  1013,  1600, 18613,  1091,  1015,  1013,
         2184,  1091,   102])"
804,1,"['statistical', 'statistical uncertainty', 'uncertainty']", A Qualitative Variable,seg_165,"0.1 ¼ 10%. – the maximum statistical uncertainty for n ¼ 10,000 is u ¼ 1/√10,000 ¼",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  1014,  1012,  1015,  1091,  2184,  1003,  1012,  1516,  1996,
         4555,  7778, 12503,  2005,  1050,  1091,  2184,  1010,  2199,  2003,
         1057,  1091,  1015,  1013,  1600, 10790,  1010,  2199,  1091,   102])"
805,1,"['uncertainty', 'statistical uncertainty', 'estimate', 'sample', 'sample size', 'statistical']", A Qualitative Variable,seg_165,"the formula can be used to estimate the sample size to achieve a given maximum statistical uncertainty. if the maximum statistical uncertainty must be u, we get: (continued)",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  1996,  5675,  2064,  2022,  2109,  2000, 10197,  1996,  7099,
         2946,  2000,  6162,  1037,  2445,  4555,  7778, 12503,  1012,  2065,
         1996,  4555,  7778, 12503,  2442,  2022,  1057,  1010,  2057,  2131,
         1024,  1006,  2506,  1007,   102])"
806,1,"['sample', 'sample size']", A Qualitative Variable,seg_165,"this is the minimum sample size, we can use. this formula is extremely useful!",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([ 101, 2023, 2003, 1996, 6263, 7099, 2946, 1010, 2057, 2064, 2224, 1012,
        2023, 5675, 2003, 5186, 6179,  999,  102])"
807,1,"['uncertainty', 'statistical uncertainty', 'sample', 'sample size', 'population', 'statistical']", A Qualitative Variable,seg_165,"note: the formula above assumes that the sample size is less than 10% of the population. if the sample size found from the formula is larger than 10% of the population, the statistical uncertainty becomes smaller. thus, we are on the safe side, if we use the formula above to determine the sample size.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  3602,  1024,  1996,  5675,  2682, 15980,  2008,  1996,  7099,
         2946,  2003,  2625,  2084,  2184,  1003,  1997,  1996,  2313,  1012,
         2065,  1996,  7099,  2946,  2179,  2013,  1996,  5675,  2003,  3469,
         2084,  2184,  1003,  1997,  1996,  2313,  1010,  1996,  7778, 12503,
         4150,  3760,  1012,  2947,  1010,  2057,  2024,  2006,  1996,  3647,
         2217,  1010,  2065,  2057,  2224,  1996,  5675,  2682,  2000,  5646,
         1996,  7099,  2946,  1012,   102])"
808,1,"['uncertainty', 'statistical uncertainty', 'sample', 'sample size', 'population', 'statistical', 'case']", A Qualitative Variable,seg_165,"technical note: if the sample is large compared to the population. in this case, the statistical uncertainty can be considerably less than the value determined from the above formula. see end of chap. 5, where the correct formula for the statistical uncertainty is indicated. the required sample size then becomes much smaller. the easiest way is to program the formula from the chap. 5 in a spreadsheet and try different sample sizes, until you get the desired maximum statistical uncertainty. as a starting value, you can use n ¼ 1/u2.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  4087,  3602,  1024,  2065,  1996,  7099,  2003,  2312,  4102,
         2000,  1996,  2313,  1012,  1999,  2023,  2553,  1010,  1996,  7778,
        12503,  2064,  2022,  9839,  2625,  2084,  1996,  3643,  4340,  2013,
         1996,  2682,  5675,  1012,  2156,  2203,  1997, 15775,  2361,  1012,
         1019,  1010,  2073,  1996,  6149,  5675,  2005,  1996,  7778, 12503,
         2003,  5393,  1012,  1996,  3223,  7099,  2946,  2059,  4150,  2172,
         3760,  1012,  1996, 25551,  2126,  2003,  2000,  2565,  1996,  5675,
         2013,  1996, 15775,  2361,  1012,  1019,  1999,  1037, 20861, 21030,
         2102,  1998,  3046,  2367,  7099, 10826,  1010,  2127,  2017,  2131,
         1996,  9059,  4555,  7778, 12503,  1012,  2004,  1037,  3225,  3643,
         1010,  2017,  2064,  2224,  1050,  1091,  1015,  1013, 23343,  1012,
          102])"
809,1,"['uncertainty', 'statistical uncertainty', 'frequency', 'relative frequency', 'sample', 'statistical']", A Qualitative Variable,seg_165,"we take an example of a situation from a sample survey. in chap. 5, we found that among the n ¼ 30 kids in the fitness club survey, we find that x ¼ 12 do strength training. the relative frequency in the sample is thus x/n ¼ 12/30 ¼ 0.4 ¼ 40%. we also found that the statistical uncertainty is 0.18 or 18%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  2057,  2202,  2019,  2742,  1997,  1037,  3663,  2013,  1037,
         7099,  5002,  1012,  1999, 15775,  2361,  1012,  1019,  1010,  2057,
         2179,  2008,  2426,  1996,  1050,  1091,  2382,  4268,  1999,  1996,
        10516,  2252,  5002,  1010,  2057,  2424,  2008,  1060,  1091,  2260,
         2079,  3997,  2731,  1012,  1996,  5816,  6075,  1999,  1996,  7099,
         2003,  2947,  1060,  1013,  1050,  1091,  2260,  1013,  2382,  1091,
         1014,  1012,  1018,  1091,  2871,  1003,  1012,  2057,  2036,  2179,
         2008,  1996,  7778, 12503,  2003,  1014,  1012,  2324,  2030,  2324,
         1003,  1012,   102])"
810,1,"['uncertainty', 'statistical uncertainty', 'sample', 'sample size', 'statistical']", A Qualitative Variable,seg_165,"if we find this statistical uncertainty too large, we can use the formula for n. if we want to have a maximum statistical uncertainty of 0.1 ¼ 10%, we find the minimum sample size n ¼ 1/u2 ¼ 1/0.12 ¼ 100.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  2065,  2057,  2424,  2023,  7778, 12503,  2205,  2312,  1010,
         2057,  2064,  2224,  1996,  5675,  2005,  1050,  1012,  2065,  2057,
         2215,  2000,  2031,  1037,  4555,  7778, 12503,  1997,  1014,  1012,
         1015,  1091,  2184,  1003,  1010,  2057,  2424,  1996,  6263,  7099,
         2946,  1050,  1091,  1015,  1013, 23343,  1091,  1015,  1013,  1014,
         1012,  2260,  1091,  2531,  1012,   102])"
811,1,['population'], A Qualitative Variable,seg_165,the above formula can obviously be used in subgroups of the population. the formula then finds the value of n for each subgroup separately.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  1996,  2682,  5675,  2064,  5525,  2022,  2109,  1999, 20576,
         2015,  1997,  1996,  2313,  1012,  1996,  5675,  2059,  4858,  1996,
         3643,  1997,  1050,  2005,  2169, 20576, 10329,  1012,   102])"
812,1,"['uncertainty', 'statistical uncertainty', 'estimate', 'sample', 'statistical']", A Qualitative Variable,seg_165,"for example, we want to be sure of getting a fairly accurate estimate for both boys and girls separately. we then apply the formula for each sex separately. we specify the maximum statistical uncertainty acceptable for the relative frequency of boys doing strength training. this gives us the necessary number of boys to be included in the sample. the same calculation can be performed with the girls.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 1037, 24209, 11475, 27453,  8023])","tensor([  101,  2005,  2742,  1010,  2057,  2215,  2000,  2022,  2469,  1997,
         2893,  1037,  7199,  8321, 10197,  2005,  2119,  3337,  1998,  3057,
        10329,  1012,  2057,  2059,  6611,  1996,  5675,  2005,  2169,  3348,
        10329,  1012,  2057, 20648,  1996,  4555,  7778, 12503, 11701,  2005,
         1996,  5816,  6075,  1997,  3337,  2725,  3997,  2731,  1012,  2023,
         3957,  2149,  1996,  4072,  2193,  1997,  3337,  2000,  2022,  2443,
         1999,  1996,  7099,  1012,  1996,  2168, 17208,  2064,  2022,  2864,
         2007,  1996,  3057,  1012,   102])"
813,1,"['sample', 'experiments', 'data']", A Quantitative Variable,seg_167,quantitative data occur often in sample surveys as well as planned experiments.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([  101, 20155,  2951,  5258,  2411,  1999,  7099, 12265,  2004,  2092,
         2004,  3740,  7885,  1012,   102])"
814,1,"['deviation', 'uncertainty', 'statistical uncertainty', 'standard deviation', 'standard', 'statistical', 'average']", A Quantitative Variable,seg_167,"we saw in chap. 4 that the statistical uncertainty of an average, if the standard deviation is known, is approximately",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([  101,  2057,  2387,  1999, 15775,  2361,  1012,  1018,  2008,  1996,
         7778, 12503,  1997,  2019,  2779,  1010,  2065,  1996,  3115, 24353,
         2003,  2124,  1010,  2003,  3155,   102])"
815,1,"['deviation', 'sample', 'standard deviation', 'sample size', 'standard']", A Quantitative Variable,seg_167,where s is the standard deviation and n is the sample size.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([  101,  2073,  1055,  2003,  1996,  3115, 24353,  1998,  1050,  2003,
         1996,  7099,  2946,  1012,   102])"
816,1,['mean'], A Quantitative Variable,seg_167,"strictly speaking, we should replace 2 by 1.96. this does not mean much in practice, however.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([ 101, 9975, 4092, 1010, 2057, 2323, 5672, 1016, 2011, 1015, 1012, 5986,
        1012, 2023, 2515, 2025, 2812, 2172, 1999, 3218, 1010, 2174, 1012,  102])"
817,1,"['deviation', 'sample', 'standard deviation', 'sample size', 'standard', 'statistical', 'average']", A Quantitative Variable,seg_167,"if we know the standard deviation s and want a maximum statistical uncertainty u of the average, we find the necessary sample size as:",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([  101,  2065,  2057,  2113,  1996,  3115, 24353,  1055,  1998,  2215,
         1037,  4555,  7778, 12503,  1057,  1997,  1996,  2779,  1010,  2057,
         2424,  1996,  4072,  7099,  2946,  2004,  1024,   102])"
818,1,"['sample', 'sample size']", A Quantitative Variable,seg_167,we cannot determine the sample size without knowledge about the dispersion of what we measure!,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 1037, 20155,  8023])","tensor([  101,  2057,  3685,  5646,  1996,  7099,  2946,  2302,  3716,  2055,
         1996,  4487, 17668, 10992,  1997,  2054,  2057,  5468,   999,   102])"
819,1,"['sample', 'sample size', 'experiment']", A Quantitative Variable,seg_167,note: we often use the term sample size even when planning an experiment!,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([ 101, 3602, 1024, 2057, 2411, 2224, 1996, 2744, 7099, 2946, 2130, 2043,
        4041, 2019, 7551,  999,  102])"
820,1,"['sample', 'experiment']", A Quantitative Variable,seg_167,"we show an example from a planned experiment, but it might as well be a sample survey.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([ 101, 2057, 2265, 2019, 2742, 2013, 1037, 3740, 7551, 1010, 2021, 2009,
        2453, 2004, 2092, 2022, 1037, 7099, 5002, 1012,  102])"
821,1,['experiment'], A Quantitative Variable,seg_167,"we want to perform an experiment with baking of breads with a special wheat variety. we want to assess the impact on bread volume of the addition of a special additive, aimed at increasing the volume.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([  101,  2057,  2215,  2000,  4685,  2019,  7551,  2007, 21522,  1997,
         7852,  2015,  2007,  1037,  2569, 10500,  3528,  1012,  2057,  2215,
         2000, 14358,  1996,  4254,  2006,  7852,  3872,  1997,  1996,  2804,
         1997,  1037,  2569, 29167,  1010,  6461,  2012,  4852,  1996,  3872,
         1012,   102])"
822,1,"['deviation', 'uncertainty', 'statistical uncertainty', 'standard deviation', 'standard', 'experiments', 'statistical']", A Quantitative Variable,seg_167,we know from previous experiments that we can expect a standard deviation of app. s ¼ 10 ml of the bread volumes. we want a maximum statistical uncertainty on both averages (bread baked with and without additive) of u ¼ 5 ml.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([  101,  2057,  2113,  2013,  3025,  7885,  2008,  2057,  2064,  5987,
         1037,  3115, 24353,  1997, 10439,  1012,  1055,  1091,  2184, 19875,
         1997,  1996,  7852,  6702,  1012,  2057,  2215,  1037,  4555,  7778,
        12503,  2006,  2119, 20185,  1006,  7852, 17776,  2007,  1998,  2302,
        29167,  1007,  1997,  1057,  1091,  1019, 19875,  1012,   102])"
823,1,"['deviation', 'uncertainty', 'statistical uncertainty', 'standard deviation', 'standard', 'experiment', 'statistical']", A Quantitative Variable,seg_167,"in total, in the experiment we should bake 2 16 ¼ 32 breads. strictly speaking, we do not need to specify both the standard deviation s and the statistical uncertainty u. it is enough that we specify the desired ratio between s and u.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([  101,  1999,  2561,  1010,  1999,  1996,  7551,  2057,  2323,  8670,
         3489,  1016,  2385,  1091,  3590,  7852,  2015,  1012,  9975,  4092,
         1010,  2057,  2079,  2025,  2342,  2000, 20648,  2119,  1996,  3115,
        24353,  1055,  1998,  1996,  7778, 12503,  1057,  1012,  2009,  2003,
         2438,  2008,  2057, 20648,  1996,  9059,  6463,  2090,  1055,  1998,
         1057,  1012,   102])"
824,1,"['deviation', 'uncertainty', 'statistical uncertainty', 'standard deviation', 'standard', 'statistical']", A Quantitative Variable,seg_167,"maybe we do not have precise knowledge of the standard deviation. however, we may want a maximum statistical uncertainty, which is half of the standard deviation obtained. this means that we require that s/u is at least 2.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([  101,  2672,  2057,  2079,  2025,  2031, 10480,  3716,  1997,  1996,
         3115, 24353,  1012,  2174,  1010,  2057,  2089,  2215,  1037,  4555,
         7778, 12503,  1010,  2029,  2003,  2431,  1997,  1996,  3115, 24353,
         4663,  1012,  2023,  2965,  2008,  2057,  5478,  2008,  1055,  1013,
         1057,  2003,  2012,  2560,  1016,  1012,   102])"
825,1,"['sample', 'results', 'sample size']", A Quantitative Variable,seg_167,we now use the formula with s/u ¼ 2. this results again in the required sample size n ¼ 16 for each group.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([ 101, 2057, 2085, 2224, 1996, 5675, 2007, 1055, 1013, 1057, 1091, 1016,
        1012, 2023, 3463, 2153, 1999, 1996, 3223, 7099, 2946, 1050, 1091, 2385,
        2005, 2169, 2177, 1012,  102])"
826,0,['n'], A Quantitative Variable,seg_167,"1. if the value of n determined from the formula is small (e.g., smaller than 10), we",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0])","tensor([ 1037, 20155,  8023])","tensor([ 101, 1015, 1012, 2065, 1996, 3643, 1997, 1050, 4340, 2013, 1996, 5675,
        2003, 2235, 1006, 1041, 1012, 1043, 1012, 1010, 3760, 2084, 2184, 1007,
        1010, 2057,  102])"
827,1,"['mean', 'confidence interval', 'table', 'interval', 'fractile', 'degrees of freedom', 'sample', 'factor', 'experiment', 'confidence']", A Quantitative Variable,seg_167,"must be careful. in practice, the necessary size of the experiment may be larger. we have used the factor 2 to construct a confidence interval for the mean. strictly speaking, we should use the 97.5% fractile of a t-distribution with n 1 degrees of freedom (see chap. 4). if n is smaller than 10, the 97.5% fractile is larger than 2, see table of the t-distribution at the end of the book. 2. in most sample surveys, we often have only one group. thus, we get directly the",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([  101,  2442,  2022,  6176,  1012,  1999,  3218,  1010,  1996,  4072,
         2946,  1997,  1996,  7551,  2089,  2022,  3469,  1012,  2057,  2031,
         2109,  1996,  5387,  1016,  2000,  9570,  1037,  7023, 13483,  2005,
         1996,  2812,  1012,  9975,  4092,  1010,  2057,  2323,  2224,  1996,
         5989,  1012,  1019,  1003, 25312,  6593,  9463,  1997,  1037,  1056,
         1011,  4353,  2007,  1050,  1015,  5445,  1997,  4071,  1006,  2156,
        15775,  2361,  1012,  1018,  1007,  1012,  2065,  1050,  2003,  3760,
         2084,  2184,  1010,  1996,  5989,  1012,  1019,  1003, 25312,  6593,
         9463,  2003,  3469,  2084,  1016,  1010,  2156,  2795,  1997,  1996,
         1056,  1011,  4353,  2012,  1996,  2203,  1997,  1996,  2338,  1012,
         1016,  1012,  1999,  2087,  7099, 12265,  1010,  2057,  2411,  2031,
         2069,  2028,  2177,  1012,  2947,  1010,  2057,  2131,  3495,  1996,
          102])"
828,1,"['sample', 'sample size', 'experiments', 'population']", A Quantitative Variable,seg_167,"necessary sample size from the entire population using the formula for n. 3. in most experiments, we are often interested in comparing two or more groups",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([  101,  4072,  7099,  2946,  2013,  1996,  2972,  2313,  2478,  1996,
         5675,  2005,  1050,  1012,  1017,  1012,  1999,  2087,  7885,  1010,
         2057,  2024,  2411,  4699,  1999, 13599,  2048,  2030,  2062,  2967,
          102])"
829,1,"['uncertainty', 'statistical uncertainty', 'sample', 'sample size', 'statistical']", A Quantitative Variable,seg_167,"(“treatments”). therefore, you might prefer to determine the necessary sample size to obtain a given statistical uncertainty of the difference between two means. see more in chap. 8.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([  101,  1006,  1523, 13441,  1524,  1007,  1012,  3568,  1010,  2017,
         2453,  9544,  2000,  5646,  1996,  4072,  7099,  2946,  2000,  6855,
         1037,  2445,  7778, 12503,  1997,  1996,  4489,  2090,  2048,  2965,
         1012,  2156,  2062,  1999, 15775,  2361,  1012,  1022,  1012,   102])"
830,1,"['sample', 'random', 'experiment', 'random error', 'error']", A Quantitative Variable,seg_167,"so far, we have been dealing with the necessary size of a sample (or an experiment). this depends on how large a random error, we can accept.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([ 101, 2061, 2521, 1010, 2057, 2031, 2042, 7149, 2007, 1996, 4072, 2946,
        1997, 1037, 7099, 1006, 2030, 2019, 7551, 1007, 1012, 2023, 9041, 2006,
        2129, 2312, 1037, 6721, 7561, 1010, 2057, 2064, 5138, 1012,  102])"
831,1,"['bias', 'sample', 'sampling', 'errors', 'systematic errors']", A Quantitative Variable,seg_167,"the rest of this chapter focuses on sample surveys. we look in more detail at bias (systematic errors). also, we discuss how to do the sampling (sample selection), so that bias is avoided as much as possible.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 1037, 20155,  8023])","tensor([  101,  1996,  2717,  1997,  2023,  3127,  7679,  2006,  7099, 12265,
         1012,  2057,  2298,  1999,  2062,  6987,  2012, 13827,  1006, 11778,
        10697,  1007,  1012,  2036,  1010,  2057,  6848,  2129,  2000,  2079,
         1996, 16227,  1006,  7099,  4989,  1007,  1010,  2061,  2008, 13827,
         2003,  9511,  2004,  2172,  2004,  2825,  1012,   102])"
832,1,"['associated', 'sample', 'errors']", Bias Systematic Errors,seg_169,"in chap. 1, we discussed some errors associated with sample surveys, in particular questionnaires.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([13827, 11778, 10697])","tensor([  101,  1999, 15775,  2361,  1012,  1015,  1010,  2057,  6936,  2070,
        10697,  3378,  2007,  7099, 12265,  1010,  1999,  3327,  3160, 20589,
         2015,  1012,   102])"
833,1,"['bias ', 'bias', 'error']", Bias Systematic Errors,seg_169,"bias (*) or systematic error comes from specific causes, which often can be identified. by removing these specific causes, the bias can in principle be avoided.",tensor(1),"tensor([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([13827, 11778, 10697])","tensor([  101, 13827,  1006,  1008,  1007,  2030, 11778,  7561,  3310,  2013,
         3563,  5320,  1010,  2029,  2411,  2064,  2022,  4453,  1012,  2011,
         9268,  2122,  3563,  5320,  1010,  1996, 13827,  2064,  1999,  6958,
         2022,  9511,  1012,   102])"
834,1,"['sample', 'bias']", Bias Systematic Errors,seg_169,the main causes of bias in sample surveys (questionnaires) are:,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([13827, 11778, 10697])","tensor([  101,  1996,  2364,  5320,  1997, 13827,  1999,  7099, 12265,  1006,
         3160, 20589,  2015,  1007,  2024,  1024,   102])"
835,1,"['sample', 'errors', 'sampling', 'process']", Bias Systematic Errors,seg_169,1. errors caused by the interviewing process and wording of the questions. 2. errors caused by nonresponse. 3. errors in the sampling (sample selection). 4. errors in the definition of the sample.,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([13827, 11778, 10697])","tensor([  101,  1015,  1012, 10697,  3303,  2011,  1996, 27805,  2832,  1998,
         2773,  2075,  1997,  1996,  3980,  1012,  1016,  1012, 10697,  3303,
         2011,  2512,  6072, 26029,  3366,  1012,  1017,  1012, 10697,  1999,
         1996, 16227,  1006,  7099,  4989,  1007,  1012,  1018,  1012, 10697,
         1999,  1996,  6210,  1997,  1996,  7099,  1012,   102])"
836,1,"['sample', 'sampling', 'randomization', 'sampling ']", Errors in the Sampling Sample Selection,seg_171,the sampling (*) or sample selection should preferably be done using randomization (*). we discuss this in the next section.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([10697,  1999,  1996, 16227,  7099,  4989])","tensor([  101,  1996, 16227,  1006,  1008,  1007,  2030,  7099,  4989,  2323,
         9544,  8231,  2022,  2589,  2478,  6721,  3989,  1006,  1008,  1007,
         1012,  2057,  6848,  2023,  1999,  1996,  2279,  2930,  1012,   102])"
837,1,"['randomization', 'bias']", Errors in the Sampling Sample Selection,seg_171,"the danger of not using randomization, but instead using some kind of “conve- nience sample” is that there may be a bias, because we get too few of one type of people and too many of another type.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([10697,  1999,  1996, 16227,  7099,  4989])","tensor([  101,  1996,  5473,  1997,  2025,  2478,  6721,  3989,  1010,  2021,
         2612,  2478,  2070,  2785,  1997,  1523,  9530,  3726,  1011,  9152,
        10127,  7099,  1524,  2003,  2008,  2045,  2089,  2022,  1037, 13827,
         1010,  2138,  2057,  2131,  2205,  2261,  1997,  2028,  2828,  1997,
         2111,  1998,  2205,  2116,  1997,  2178,  2828,  1012,   102])"
838,0,[], Errors in the Sampling Sample Selection,seg_171,typical examples are internet polls and telephone polls during television programs.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([10697,  1999,  1996, 16227,  7099,  4989])","tensor([  101,  5171,  4973,  2024,  4274, 14592,  1998,  7026, 14592,  2076,
         2547,  3454,  1012,   102])"
839,0,[], Errors in the Sampling Sample Selection,seg_171,– who are using a certain website at any given day and bother to vote? – who are viewing a television program and bother to make a phone call to,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([10697,  1999,  1996, 16227,  7099,  4989])","tensor([  101,  1516,  2040,  2024,  2478,  1037,  3056,  4037,  2012,  2151,
         2445,  2154,  1998,  8572,  2000,  3789,  1029,  1516,  2040,  2024,
        10523,  1037,  2547,  2565,  1998,  8572,  2000,  2191,  1037,  3042,
         2655,  2000,   102])"
840,0,[], Errors in the Sampling Sample Selection,seg_171,express an opinion?,tensor(0),"tensor([0, 0, 0, 0, 0, 0])","tensor([10697,  1999,  1996, 16227,  7099,  4989])","tensor([ 101, 4671, 2019, 5448, 1029,  102])"
841,1,"['sample', 'representative', 'population']", Errors in the Sampling Sample Selection,seg_171,this need not be a sample representative of the population!,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.])","tensor([10697,  1999,  1996, 16227,  7099,  4989])","tensor([ 101, 2023, 2342, 2025, 2022, 1037, 7099, 4387, 1997, 1996, 2313,  999,
         102])"
842,0,[], Errors in the Sampling Sample Selection,seg_171,"the fitness club survey could be organized by letting interviewers visit the club and “haphazardly” selecting some of the kids, who are present.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([10697,  1999,  1996, 16227,  7099,  4989])","tensor([  101,  1996, 10516,  2252,  5002,  2071,  2022,  4114,  2011,  5599,
         4357,  2545,  3942,  1996,  2252,  1998,  1523,  5292, 21890, 26154,
         2135,  1524, 17739,  2070,  1997,  1996,  4268,  1010,  2040,  2024,
         2556,  1012,   102])"
843,0,[], Errors in the Sampling Sample Selection,seg_171,"the disadvantage of organizing the survey in this way is that we do not know what type of kids, we select. we will probably get many kids, who are frequent users of the club! maybe we are interested the other kids also... perhaps we want to find out, why some of the kids are less frequent users of the club!",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([10697,  1999,  1996, 16227,  7099,  4989])","tensor([  101,  1996, 20502,  1997, 10863,  1996,  5002,  1999,  2023,  2126,
         2003,  2008,  2057,  2079,  2025,  2113,  2054,  2828,  1997,  4268,
         1010,  2057,  7276,  1012,  2057,  2097,  2763,  2131,  2116,  4268,
         1010,  2040,  2024,  6976,  5198,  1997,  1996,  2252,   999,  2672,
         2057,  2024,  4699,  1996,  2060,  4268,  2036,  1012,  1012,  1012,
         3383,  2057,  2215,  2000,  2424,  2041,  1010,  2339,  2070,  1997,
         1996,  4268,  2024,  2625,  6976,  5198,  1997,  1996,  2252,   999,
          102])"
844,1,"['sample', 'sampling', 'sampling frame', 'population', 'register']", Errors in the Definition of the Sample,seg_173,"the ideal situation is that you have a database (a register) of the whole population. this makes it easy to provide a sampling frame, from which you select the individuals included in the sample. the sampling frame can be a separate copy of the database, as it looks when the sample is selected.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0.])","tensor([10697,  1999,  1996,  6210,  1997,  1996,  7099])","tensor([  101,  1996,  7812,  3663,  2003,  2008,  2017,  2031,  1037,  7809,
         1006,  1037,  4236,  1007,  1997,  1996,  2878,  2313,  1012,  2023,
         3084,  2009,  3733,  2000,  3073,  1037, 16227,  4853,  1010,  2013,
         2029,  2017,  7276,  1996,  3633,  2443,  1999,  1996,  7099,  1012,
         1996, 16227,  4853,  2064,  2022,  1037,  3584,  6100,  1997,  1996,
         7809,  1010,  2004,  2009,  3504,  2043,  1996,  7099,  2003,  3479,
         1012,   102])"
845,1,"['sampling units', 'sampling', 'sampling frame', 'population', 'sampling unit']", Errors in the Definition of the Sample,seg_173,the individuals that are selected from the sampling frame are called sampling units (*). a sampling unit may be the same as an individual in the population. it may also be a group of individuals from the population.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([10697,  1999,  1996,  6210,  1997,  1996,  7099])","tensor([  101,  1996,  3633,  2008,  2024,  3479,  2013,  1996, 16227,  4853,
         2024,  2170, 16227,  3197,  1006,  1008,  1007,  1012,  1037, 16227,
         3131,  2089,  2022,  1996,  2168,  2004,  2019,  3265,  1999,  1996,
         2313,  1012,  2009,  2089,  2036,  2022,  1037,  2177,  1997,  3633,
         2013,  1996,  2313,  1012,   102])"
846,1,"['population', 'sampling frame', 'sampling']", Errors in the Definition of the Sample,seg_173,the population will often consist of persons. the sampling frame may consist of households. a number of households are selected from the sampling frame. from each household one or more persons are selected. in this context we call a person the analysis unit.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([10697,  1999,  1996,  6210,  1997,  1996,  7099])","tensor([  101,  1996,  2313,  2097,  2411,  8676,  1997,  5381,  1012,  1996,
        16227,  4853,  2089,  8676,  1997,  3911,  1012,  1037,  2193,  1997,
         3911,  2024,  3479,  2013,  1996, 16227,  4853,  1012,  2013,  2169,
         4398,  2028,  2030,  2062,  5381,  2024,  3479,  1012,  1999,  2023,
         6123,  2057,  2655,  1037,  2711,  1996,  4106,  3131,  1012,   102])"
847,1,"['population', 'sampling', 'sampling frame', 'bias']", Errors in the Definition of the Sample,seg_173,an incomplete sampling frame is a frequent source of bias in sampling surveys. this means that the sampling frame does not correspond exactly to the population. this can be because:,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([10697,  1999,  1996,  6210,  1997,  1996,  7099])","tensor([  101,  2019, 12958, 16227,  4853,  2003,  1037,  6976,  3120,  1997,
        13827,  1999, 16227, 12265,  1012,  2023,  2965,  2008,  1996, 16227,
         4853,  2515,  2025, 17254,  3599,  2000,  1996,  2313,  1012,  2023,
         2064,  2022,  2138,  1024,   102])"
848,1,"['sample', 'population']", Errors in the Definition of the Sample,seg_173,– some individuals from the population cannot be part of the sample. for,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([10697,  1999,  1996,  6210,  1997,  1996,  7099])","tensor([ 101, 1516, 2070, 3633, 2013, 1996, 2313, 3685, 2022, 2112, 1997, 1996,
        7099, 1012, 2005,  102])"
849,1,"['sample', 'sampling frame', 'sampling']", Errors in the Definition of the Sample,seg_173,"instance, people living in institutions cannot be part of the sample, if the sampling frame consists of private households only. – the sampling frame is not up-to-date. there is often a time lag from the time of",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([10697,  1999,  1996,  6210,  1997,  1996,  7099])","tensor([  101,  6013,  1010,  2111,  2542,  1999,  4896,  3685,  2022,  2112,
         1997,  1996,  7099,  1010,  2065,  1996, 16227,  4853,  3774,  1997,
         2797,  3911,  2069,  1012,  1516,  1996, 16227,  4853,  2003,  2025,
         2039,  1011,  2000,  1011,  3058,  1012,  2045,  2003,  2411,  1037,
         2051,  2474,  2290,  2013,  1996,  2051,  1997,   102])"
850,1,"['population', 'sampling frame', 'sampling']", Errors in the Definition of the Sample,seg_173,"selection to the time of the interview. during this period the population may change! for example, a person from the sampling frame may die before the interview. – the sampling frame is incomplete or incorrect for other reasons. a list of",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([10697,  1999,  1996,  6210,  1997,  1996,  7099])","tensor([  101,  4989,  2000,  1996,  2051,  1997,  1996,  4357,  1012,  2076,
         2023,  2558,  1996,  2313,  2089,  2689,   999,  2005,  2742,  1010,
         1037,  2711,  2013,  1996, 16227,  4853,  2089,  3280,  2077,  1996,
         4357,  1012,  1516,  1996, 16227,  4853,  2003, 12958,  2030, 16542,
         2005,  2060,  4436,  1012,  1037,  2862,  1997,   102])"
851,0,[], Errors in the Definition of the Sample,seg_173,households could for example be based on an incomplete or incorrect list of roads and road numbers.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([10697,  1999,  1996,  6210,  1997,  1996,  7099])","tensor([  101,  3911,  2071,  2005,  2742,  2022,  2241,  2006,  2019, 12958,
         2030, 16542,  2862,  1997,  4925,  1998,  2346,  3616,  1012,   102])"
852,1,['sample'], Errors in the Definition of the Sample,seg_173,"once every month fitness club prints a list of all the kids, who are using the club. the sample survey is organized by taking a number of kids from this list.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([10697,  1999,  1996,  6210,  1997,  1996,  7099])","tensor([  101,  2320,  2296,  3204, 10516,  2252, 11204,  1037,  2862,  1997,
         2035,  1996,  4268,  1010,  2040,  2024,  2478,  1996,  2252,  1012,
         1996,  7099,  5002,  2003,  4114,  2011,  2635,  1037,  2193,  1997,
         4268,  2013,  2023,  2862,  1012,   102])"
853,0,[], Errors in the Definition of the Sample,seg_173,problems may arise in this context:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([10697,  1999,  1996,  6210,  1997,  1996,  7099])","tensor([  101,  3471,  2089, 13368,  1999,  2023,  6123,  1024,   102])"
854,0,[], Errors in the Definition of the Sample,seg_173,– a kid from the list has stopped in the club at the time of interview. – a new kid has started in the club after the list has been printed.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([10697,  1999,  1996,  6210,  1997,  1996,  7099])","tensor([ 101, 1516, 1037, 4845, 2013, 1996, 2862, 2038, 3030, 1999, 1996, 2252,
        2012, 1996, 2051, 1997, 4357, 1012, 1516, 1037, 2047, 4845, 2038, 2318,
        1999, 1996, 2252, 2044, 1996, 2862, 2038, 2042, 6267, 1012,  102])"
855,1,"['sample', 'representative sample', 'representative']", What Is a Representative Sample,seg_175,"the term representative sample is used in many ways, without being defined. use of this term should preferable be avoided. if we need a definition, it might be something like this:",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([2054, 2003, 1037, 4387, 7099])","tensor([ 101, 1996, 2744, 4387, 7099, 2003, 2109, 1999, 2116, 3971, 1010, 2302,
        2108, 4225, 1012, 2224, 1997, 2023, 2744, 2323, 9544, 3085, 2022, 9511,
        1012, 2065, 2057, 2342, 1037, 6210, 1010, 2009, 2453, 2022, 2242, 2066,
        2023, 1024,  102])"
856,1,"['random errors', 'sample', 'random', 'errors', 'representative', 'bias']", What Is a Representative Sample,seg_175,"a sample can be called representative, if there are only random errors, i.e., no bias.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0.])","tensor([2054, 2003, 1037, 4387, 7099])","tensor([  101,  1037,  7099,  2064,  2022,  2170,  4387,  1010,  2065,  2045,
         2024,  2069,  6721, 10697,  1010,  1045,  1012,  1041,  1012,  1010,
         2053, 13827,  1012,   102])"
857,1,"['sample', 'representative sample', 'representative', 'bias']", What Is a Representative Sample,seg_175,"under this definition, representative sample surveys do not exist! a representative sample can therefore be seen as an ideal! there will always be bias, but we can do much to reduce it!",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([2054, 2003, 1037, 4387, 7099])","tensor([  101,  2104,  2023,  6210,  1010,  4387,  7099, 12265,  2079,  2025,
         4839,   999,  1037,  4387,  7099,  2064,  3568,  2022,  2464,  2004,
         2019,  7812,   999,  2045,  2097,  2467,  2022, 13827,  1010,  2021,
         2057,  2064,  2079,  2172,  2000,  5547,  2009,   999,   102])"
858,1,"['random errors', 'sample', 'random', 'sample size', 'errors', 'bias']", What Is a Representative Sample,seg_175,"– only the random errors will become smaller, when the sample size increases! – the bias will not become smaller when the sample increases!",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([2054, 2003, 1037, 4387, 7099])","tensor([  101,  1516,  2069,  1996,  6721, 10697,  2097,  2468,  3760,  1010,
         2043,  1996,  7099,  2946,  7457,   999,  1516,  1996, 13827,  2097,
         2025,  2468,  3760,  2043,  1996,  7099,  7457,   999,   102])"
859,1,"['sampling', 'sampling unit']", What Is a Representative Sample,seg_175,"if, for example, the sampling unit is a household, and we select the first available person from each household, we will often get too many women and too few men,",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([2054, 2003, 1037, 4387, 7099])","tensor([  101,  2065,  1010,  2005,  2742,  1010,  1996, 16227,  3131,  2003,
         1037,  4398,  1010,  1998,  2057,  7276,  1996,  2034,  2800,  2711,
         2013,  2169,  4398,  1010,  2057,  2097,  2411,  2131,  2205,  2116,
         2308,  1998,  2205,  2261,  2273,  1010,   102])"
860,1,"['sample', 'average', 'sample size']", What Is a Representative Sample,seg_175,"because women on the average have shorter working hours than men. this will not change by increasing the sample size! we will still have too many women and too few men, regardless of sample size...",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0.])","tensor([2054, 2003, 1037, 4387, 7099])","tensor([ 101, 2138, 2308, 2006, 1996, 2779, 2031, 7820, 2551, 2847, 2084, 2273,
        1012, 2023, 2097, 2025, 2689, 2011, 4852, 1996, 7099, 2946,  999, 2057,
        2097, 2145, 2031, 2205, 2116, 2308, 1998, 2205, 2261, 2273, 1010, 7539,
        1997, 7099, 2946, 1012, 1012, 1012,  102])"
861,1,"['sample', 'sampling', 'randomization', 'errors', 'sampling ', 'systematic errors']", Sampling Sample Selection,seg_177,"in this section we describe the main principles of sampling (*) or sample selection. as mentioned earlier, sampling should be based on randomization (*); otherwise it may cause systematic errors. therefore, the most important methods of sampling are based on randomization.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,
        0.])","tensor([16227,  7099,  4989])","tensor([  101,  1999,  2023,  2930,  2057,  6235,  1996,  2364,  6481,  1997,
        16227,  1006,  1008,  1007,  2030,  7099,  4989,  1012,  2004,  3855,
         3041,  1010, 16227,  2323,  2022,  2241,  2006,  6721,  3989,  1006,
         1008,  1007,  1025,  4728,  2009,  2089,  3426, 11778, 10697,  1012,
         3568,  1010,  1996,  2087,  2590,  4725,  1997, 16227,  2024,  2241,
         2006,  6721,  3989,  1012,   102])"
862,1,['randomization'], Sampling Sample Selection,seg_177,"we also give a short description of some other methods, which are not based on randomization.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0.])","tensor([16227,  7099,  4989])","tensor([ 101, 2057, 2036, 2507, 1037, 2460, 6412, 1997, 2070, 2060, 4725, 1010,
        2029, 2024, 2025, 2241, 2006, 6721, 3989, 1012,  102])"
863,1,"['random sampling', 'simple random sampling', 'method', 'sampling', 'random', 'sampling ']", Simple Random Sampling,seg_179,this is the basic method. simple random sampling (*) is in fact a gigantic lottery!,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 3722,  6721, 16227])","tensor([  101,  2023,  2003,  1996,  3937,  4118,  1012,  3722,  6721, 16227,
         1006,  1008,  1007,  2003,  1999,  2755,  1037, 20193, 15213,   999,
          102])"
864,1,"['random sampling', 'uncertainty', 'simple random sampling', 'statistical uncertainty', 'sample', 'random', 'sampling', 'statistical']", Simple Random Sampling,seg_179,"the formulas for the determination of the statistical uncertainty from the start of this chapter assume simple random sampling. if the sample has been selected by some other mechanism, the formulas are not correct!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 3722,  6721, 16227])","tensor([  101,  1996, 25814,  2005,  1996,  9128,  1997,  1996,  7778, 12503,
         2013,  1996,  2707,  1997,  2023,  3127,  7868,  3722,  6721, 16227,
         1012,  2065,  1996,  7099,  2038,  2042,  3479,  2011,  2070,  2060,
         7337,  1010,  1996, 25814,  2024,  2025,  6149,   999,   102])"
865,1,"['uncertainty', 'associated', 'statistical uncertainty', 'sampling', 'statistical']", Simple Random Sampling,seg_179,"we refer to specialized books on survey sampling, if you need to calculate the statistical uncertainty associated with other methods of sampling.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 3722,  6721, 16227])","tensor([  101,  2057,  6523,  2000,  7772,  2808,  2006,  5002, 16227,  1010,
         2065,  2017,  2342,  2000, 18422,  1996,  7778, 12503,  3378,  2007,
         2060,  4725,  1997, 16227,  1012,   102])"
866,1,"['random sampling', 'simple random sampling', 'random numbers', 'random', 'sampling', 'statistical']", Simple Random Sampling,seg_179,"nowadays, simple random sampling is done using random numbers in statistical software or a spreadsheet.",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 3722,  6721, 16227])","tensor([  101, 13367,  1010,  3722,  6721, 16227,  2003,  2589,  2478,  6721,
         3616,  1999,  7778,  4007,  2030,  1037, 20861, 21030,  2102,  1012,
          102])"
867,1,"['parameters', 'function']", Simple Random Sampling,seg_179,"in a spreadsheet, you can use the function rand. this function is used without any parameters:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0.])","tensor([ 3722,  6721, 16227])","tensor([  101,  1999,  1037, 20861, 21030,  2102,  1010,  2017,  2064,  2224,
         1996,  3853, 14566,  1012,  2023,  3853,  2003,  2109,  2302,  2151,
        11709,  1024,   102])"
868,1,"['random number', 'random']", Simple Random Sampling,seg_179,¼rand() this provides a random number between 0 and 1.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 3722,  6721, 16227])","tensor([  101,  1091, 13033,  1006,  1007,  2023,  3640,  1037,  6721,  2193,
         2090,  1014,  1998,  1015,  1012,   102])"
869,1,"['sampling units', 'sampling frame', 'sampling']", Simple Random Sampling,seg_179,the general approach for selecting n sampling units from a sampling frame can be summarized as follows:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 3722,  6721, 16227])","tensor([  101,  1996,  2236,  3921,  2005, 17739,  1050, 16227,  3197,  2013,
         1037, 16227,  4853,  2064,  2022, 22539,  2004,  4076,  1024,   102])"
870,1,"['function', 'random numbers', 'sampling units', 'sample', 'sampling', 'sample size', 'random', 'sampling frame', 'sampling unit']", Simple Random Sampling,seg_179,"1. use the function rand for each sampling unit in the sampling frame. 2. sort all sampling units according to the value of the random numbers. 3. select the first n sampling units, where n is the required sample size.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 3722,  6721, 16227])","tensor([  101,  1015,  1012,  2224,  1996,  3853, 14566,  2005,  2169, 16227,
         3131,  1999,  1996, 16227,  4853,  1012,  1016,  1012,  4066,  2035,
        16227,  3197,  2429,  2000,  1996,  3643,  1997,  1996,  6721,  3616,
         1012,  1017,  1012,  7276,  1996,  2034,  1050, 16227,  3197,  1010,
         2073,  1050,  2003,  1996,  3223,  7099,  2946,  1012,   102])"
871,1,"['homogeneous', 'method', 'sampling', 'random', 'sampling ', 'population', 'strata']", Stratified Sampling,seg_181,"stratified sampling (*) is a method of dividing the population into homogeneous groups, called strata (singular: stratum). within each group simple random sampling is used!",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0.])","tensor([ 2358,  8609,  7810, 16227])","tensor([  101,  2358,  8609,  7810, 16227,  1006,  1008,  1007,  2003,  1037,
         4118,  1997, 16023,  1996,  2313,  2046, 24854,  2967,  1010,  2170,
        22913,  1006, 13048,  1024,  2358,  8609,  2819,  1007,  1012,  2306,
         2169,  2177,  3722,  6721, 16227,  2003,  2109,   999,   102])"
872,1,"['homogeneous', 'uncertainty', 'statistical uncertainty', 'sample', 'sample size', 'statistical', 'strata']", Stratified Sampling,seg_181,"the main reason to do stratification is that we can reduce the statistical uncertainty significantly. conversely, we can reduce the sample size significantly without increasing the statistical uncertainty! this happens, if the strata are homogeneous, i.e., the spread within strata is small, while on the other hand, the spread between strata is large.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 2358,  8609,  7810, 16227])","tensor([  101,  1996,  2364,  3114,  2000,  2079,  2358,  8609,  9031,  2003,
         2008,  2057,  2064,  5547,  1996,  7778, 12503,  6022,  1012, 18868,
         1010,  2057,  2064,  5547,  1996,  7099,  2946,  6022,  2302,  4852,
         1996,  7778, 12503,   999,  2023,  6433,  1010,  2065,  1996, 22913,
         2024, 24854,  1010,  1045,  1012,  1041,  1012,  1010,  1996,  3659,
         2306, 22913,  2003,  2235,  1010,  2096,  2006,  1996,  2060,  2192,
         1010,  1996,  3659,  2090, 22913,  2003,  2312,  1012,   102])"
873,1,"['information', 'sample', 'sample size', 'population']", Stratified Sampling,seg_181,"let us as a hypothetical (?) example imagine that men and women are 100% divided with respect to the opinion on a particular issue. for instance, all men will answer “yes” to a certain question, while all women will answer “no”. in this situation the sample need not be very large in order to cover the population! in fact, a sample size of 2 is enough (one man and one woman)... there is no further information in a larger sample!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.])","tensor([ 2358,  8609,  7810, 16227])","tensor([  101,  2292,  2149,  2004,  1037, 25613,  1006,  1029,  1007,  2742,
         5674,  2008,  2273,  1998,  2308,  2024,  2531,  1003,  4055,  2007,
         4847,  2000,  1996,  5448,  2006,  1037,  3327,  3277,  1012,  2005,
         6013,  1010,  2035,  2273,  2097,  3437,  1523,  2748,  1524,  2000,
         1037,  3056,  3160,  1010,  2096,  2035,  2308,  2097,  3437,  1523,
         2053,  1524,  1012,  1999,  2023,  3663,  1996,  7099,  2342,  2025,
         2022,  2200,  2312,  1999,  2344,  2000,  3104,  1996,  2313,   999,
         1999,  2755,  1010,  1037,  7099,  2946,  1997,  1016,  2003,  2438,
         1006,  2028,  2158,  1998,  2028,  2450,  1007,  1012,  1012,  1012,
         2045,  2003,  2053,  2582,  2592,  1999,  1037,  3469,  7099,   999,
          102])"
874,0,[], Stratified Sampling,seg_181,"this example is of course entirely hypothetical! however, if the situation has a certain similarity with the hypothetical example, there will still be a huge gain by stratification. this is true if e.g., most men will answer “yes” to the question and most women will answer “no”.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2358,  8609,  7810, 16227])","tensor([  101,  2023,  2742,  2003,  1997,  2607,  4498, 25613,   999,  2174,
         1010,  2065,  1996,  3663,  2038,  1037,  3056, 14402,  2007,  1996,
        25613,  2742,  1010,  2045,  2097,  2145,  2022,  1037,  4121,  5114,
         2011,  2358,  8609,  9031,  1012,  2023,  2003,  2995,  2065,  1041,
         1012,  1043,  1012,  1010,  2087,  2273,  2097,  3437,  1523,  2748,
         1524,  2000,  1996,  3160,  1998,  2087,  2308,  2097,  3437,  1523,
         2053,  1524,  1012,   102])"
875,1,"['data', 'factors', 'sample', 'statistical', 'variable']", Stratified Sampling,seg_181,"if we have data from an earlier sample survey, we can use these data to plan a new sample survey. often, there is one particular variable, which is the most important variable. then we can carry out some statistical calculations to determine the factors that have the greatest influence on this variable. the statistical techniques used for this purpose are extensions of the methods discussed in chaps. 5, 7, and 8.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2358,  8609,  7810, 16227])","tensor([  101,  2065,  2057,  2031,  2951,  2013,  2019,  3041,  7099,  5002,
         1010,  2057,  2064,  2224,  2122,  2951,  2000,  2933,  1037,  2047,
         7099,  5002,  1012,  2411,  1010,  2045,  2003,  2028,  3327,  8023,
         1010,  2029,  2003,  1996,  2087,  2590,  8023,  1012,  2059,  2057,
         2064,  4287,  2041,  2070,  7778, 16268,  2000,  5646,  1996,  5876,
         2008,  2031,  1996,  4602,  3747,  2006,  2023,  8023,  1012,  1996,
         7778,  5461,  2109,  2005,  2023,  3800,  2024, 14305,  1997,  1996,
         4725,  6936,  1999, 15775,  4523,  1012,  1019,  1010,  1021,  1010,
         1998,  1022,  1012,   102])"
876,0,[], Stratified Sampling,seg_181,other reasons for stratification may be:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2358,  8609,  7810, 16227])","tensor([ 101, 2060, 4436, 2005, 2358, 8609, 9031, 2089, 2022, 1024,  102])"
877,0,[], Stratified Sampling,seg_181,1. administrative reasons. this might be stratification according to geographical,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 2358,  8609,  7810, 16227])","tensor([  101,  1015,  1012,  3831,  4436,  1012,  2023,  2453,  2022,  2358,
         8609,  9031,  2429,  2000, 10056,   102])"
878,1,"['uncertainty', 'random sampling', 'simple random sampling', 'statistical uncertainty', 'random', 'sampling', 'statistical', 'strata']", Stratified Sampling,seg_181,"criteria. in this situation, the statistical uncertainty will rarely be reduced much compared to simple random sampling; it may even be larger. 2. groups (strata) have an interest in itself. we want to ensure that all strata are",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 2358,  8609,  7810, 16227])","tensor([  101,  9181,  1012,  1999,  2023,  3663,  1010,  1996,  7778, 12503,
         2097,  6524,  2022,  4359,  2172,  4102,  2000,  3722,  6721, 16227,
         1025,  2009,  2089,  2130,  2022,  3469,  1012,  1016,  1012,  2967,
         1006, 22913,  1007,  2031,  2019,  3037,  1999,  2993,  1012,  2057,
         2215,  2000,  5676,  2008,  2035, 22913,  2024,   102])"
879,1,"['sample', 'sampling', 'population']", Stratified Sampling,seg_181,"adequately represented in the sample. this is particularly important, if there are small groups in the population, which have our particular interest. 3. sampling is conducted according to different principles in different groups of",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 2358,  8609,  7810, 16227])","tensor([  101, 23613,  3421,  1999,  1996,  7099,  1012,  2023,  2003,  3391,
         2590,  1010,  2065,  2045,  2024,  2235,  2967,  1999,  1996,  2313,
         1010,  2029,  2031,  2256,  3327,  3037,  1012,  1017,  1012, 16227,
         2003,  4146,  2429,  2000,  2367,  6481,  1999,  2367,  2967,  1997,
          102])"
880,1,"['population', 'sampling']", Stratified Sampling,seg_181,"the population. for example, some people live in institutions rather than ordinary households. the practical problems involved in sampling are very different.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 2358,  8609,  7810, 16227])","tensor([  101,  1996,  2313,  1012,  2005,  2742,  1010,  2070,  2111,  2444,
         1999,  4896,  2738,  2084,  6623,  3911,  1012,  1996,  6742,  3471,
         2920,  1999, 16227,  2024,  2200,  2367,  1012,   102])"
881,1,"['sample', 'strata']", Stratified Sampling,seg_181,"in the fitness club sample survey, stratifying by age will probably be a good idea. there is in virtually every aspect very big differences between a 12-year-old and a 17-year-old. . .we could have two strata: kids 12–14 years old and kids 15–17 years old. it might also make sense to stratify by sex. it depends on the main purpose of the survey. you can also stratify after both age and sex, for example, using four strata in total.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0.])","tensor([ 2358,  8609,  7810, 16227])","tensor([  101,  1999,  1996, 10516,  2252,  7099,  5002,  1010,  2358,  8609,
        11787,  2011,  2287,  2097,  2763,  2022,  1037,  2204,  2801,  1012,
         2045,  2003,  1999,  8990,  2296,  7814,  2200,  2502,  5966,  2090,
         1037,  2260,  1011,  2095,  1011,  2214,  1998,  1037,  2459,  1011,
         2095,  1011,  2214,  1012,  1012,  1012,  2057,  2071,  2031,  2048,
        22913,  1024,  4268,  2260,  1516,  2403,  2086,  2214,  1998,  4268,
         2321,  1516,  2459,  2086,  2214,  1012,  2009,  2453,  2036,  2191,
         3168,  2000,  2358,  8609,  8757,  2011,  3348,  1012,  2009,  9041,
         2006,  1996,  2364,  3800,  1997,  1996,  5002,  1012,  2017,  2064,
         2036,  2358,  8609,  8757,  2044,  2119,  2287,  1998,  3348,  1010,
         2005,  2742,  1010,  2478,  2176, 22913,  1999,  2561,  1012,   102])"
882,1,"['cases', 'weighted average', 'sample', 'population', 'average']", Stratified Sampling,seg_181,"how many individuals should be selected from each stratum? normally, we will select a number of individuals in each stratum, corresponding to its size in the population. if one group is twice as large as another, we must select twice as many individuals in this group compared to the other. in some cases we will over-sample some groups and under-sample others. for example, some groups are of particular interest. in these cases, we must calculate a weighted average, when calculating an average from the sample. that is, we multiply each group average with its weight from the population. if one group in the population is twice as large as another, this average should count twice as much as the other average.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0.])","tensor([ 2358,  8609,  7810, 16227])","tensor([  101,  2129,  2116,  3633,  2323,  2022,  3479,  2013,  2169,  2358,
         8609,  2819,  1029,  5373,  1010,  2057,  2097,  7276,  1037,  2193,
         1997,  3633,  1999,  2169,  2358,  8609,  2819,  1010,  7978,  2000,
         2049,  2946,  1999,  1996,  2313,  1012,  2065,  2028,  2177,  2003,
         3807,  2004,  2312,  2004,  2178,  1010,  2057,  2442,  7276,  3807,
         2004,  2116,  3633,  1999,  2023,  2177,  4102,  2000,  1996,  2060,
         1012,  1999,  2070,  3572,  2057,  2097,  2058,  1011,  7099,  2070,
         2967,  1998,  2104,  1011,  7099,  2500,  1012,  2005,  2742,  1010,
         2070,  2967,  2024,  1997,  3327,  3037,  1012,  1999,  2122,  3572,
         1010,  2057,  2442, 18422,  1037, 18215,  2779,  1010,  2043, 20177,
         2019,  2779,  2013,  1996,  7099,  1012,  2008,  2003,  1010,  2057,
         4800, 22086,  2169,  2177,  2779,  2007,  2049,  3635,  2013,  1996,
         2313,  1012,  2065,  2028,  2177,  1999,  1996,  2313,  2003,  3807,
         2004,  2312,  2004,  2178,  1010,  2023,  2779,  2323,  4175,  3807,
         2004,  2172,  2004,  1996,  2060,  2779,  1012,   102])"
883,1,"['sampling units', 'sampling frame', 'sampling', 'sampling ']", Cluster Sampling,seg_183,"cluster sampling (*) is based on a sampling frame, consisting of sampling units, which in turn contain several analysis units.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 9324, 16227])","tensor([  101,  9324, 16227,  1006,  1008,  1007,  2003,  2241,  2006,  1037,
        16227,  4853,  1010,  5398,  1997, 16227,  3197,  1010,  2029,  1999,
         2735,  5383,  2195,  4106,  3197,  1012,   102])"
884,1,"['random sampling', 'simple random sampling', 'random', 'sampling', 'sampling unit']", Cluster Sampling,seg_183,"the classic example is a household, which consists of several people. we select a number of households by simple random sampling. we can then select one person, all persons or for instance half of the persons from the household. in this context the sampling unit (a household) is often called a cluster.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 9324, 16227])","tensor([  101,  1996,  4438,  2742,  2003,  1037,  4398,  1010,  2029,  3774,
         1997,  2195,  2111,  1012,  2057,  7276,  1037,  2193,  1997,  3911,
         2011,  3722,  6721, 16227,  1012,  2057,  2064,  2059,  7276,  2028,
         2711,  1010,  2035,  5381,  2030,  2005,  6013,  2431,  1997,  1996,
         5381,  2013,  1996,  4398,  1012,  1999,  2023,  6123,  1996, 16227,
         3131,  1006,  1037,  4398,  1007,  2003,  2411,  2170,  1037,  9324,
         1012,   102])"
885,1,"['sampling frame', 'sampling']", Cluster Sampling,seg_183,"cluster sampling is used mainly for administrative reasons. you may have access to a sampling frame consisting of households, but not a sampling frame of persons.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([ 9324, 16227])","tensor([  101,  9324, 16227,  2003,  2109,  3701,  2005,  3831,  4436,  1012,
         2017,  2089,  2031,  3229,  2000,  1037, 16227,  4853,  5398,  1997,
         3911,  1010,  2021,  2025,  1037, 16227,  4853,  1997,  5381,  1012,
          102])"
886,1,"['cluster sampling', 'uncertainty', 'random sampling', 'associated', 'simple random sampling', 'statistical uncertainty', 'random', 'sampling', 'statistical']", Cluster Sampling,seg_183,the statistical uncertainty associated with cluster sampling will usually be larger than when simple random sampling is used. this happens when the individuals in a cluster are similar.,tensor(1),"tensor([0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 9324, 16227])","tensor([  101,  1996,  7778, 12503,  3378,  2007,  9324, 16227,  2097,  2788,
         2022,  3469,  2084,  2043,  3722,  6721, 16227,  2003,  2109,  1012,
         2023,  6433,  2043,  1996,  3633,  1999,  1037,  9324,  2024,  2714,
         1012,   102])"
887,0,[], Cluster Sampling,seg_183,"assume that we are interested in the television viewing of adult persons. in most households consisting of two adults, they will watch a television program together. if you ask them, which television programs they saw the day before, you",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 9324, 16227])","tensor([  101,  7868,  2008,  2057,  2024,  4699,  1999,  1996,  2547, 10523,
         1997,  4639,  5381,  1012,  1999,  2087,  3911,  5398,  1997,  2048,
         6001,  1010,  2027,  2097,  3422,  1037,  2547,  2565,  2362,  1012,
         2065,  2017,  3198,  2068,  1010,  2029,  2547,  3454,  2027,  2387,
         1996,  2154,  2077,  1010,  2017,   102])"
888,1,"['information', 'response']", Cluster Sampling,seg_183,"will get the same response from both adults in the household! there is, in other words, no additional information asking both adults as compared to asking only one of them!",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 9324, 16227])","tensor([ 101, 2097, 2131, 1996, 2168, 3433, 2013, 2119, 6001, 1999, 1996, 4398,
         999, 2045, 2003, 1010, 1999, 2060, 2616, 1010, 2053, 3176, 2592, 4851,
        2119, 6001, 2004, 4102, 2000, 4851, 2069, 2028, 1997, 2068,  999,  102])"
889,1,['population'], Cluster Sampling,seg_183,"of course, this is a simplified description. for example one of them may be shopping, while the other watches television at home. as long as the population consists of only adults, this description is, however, reasonably correct.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 9324, 16227])","tensor([  101,  1997,  2607,  1010,  2023,  2003,  1037, 11038,  6412,  1012,
         2005,  2742,  2028,  1997,  2068,  2089,  2022,  6023,  1010,  2096,
         1996,  2060, 12197,  2547,  2012,  2188,  1012,  2004,  2146,  2004,
         1996,  2313,  3774,  1997,  2069,  6001,  1010,  2023,  6412,  2003,
         1010,  2174,  1010, 16286,  6149,  1012,   102])"
890,1,"['cases', 'cluster sampling', 'uncertainty', 'random sampling', 'simple random sampling', 'statistical uncertainty', 'random', 'sampling', 'statistical']", Cluster Sampling,seg_183,in some cases cluster sampling has smaller statistical uncertainty than when simple random sampling is used. this happens if there are large differences between the individuals in a cluster.,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 9324, 16227])","tensor([  101,  1999,  2070,  3572,  9324, 16227,  2038,  3760,  7778, 12503,
         2084,  2043,  3722,  6721, 16227,  2003,  2109,  1012,  2023,  6433,
         2065,  2045,  2024,  2312,  5966,  2090,  1996,  3633,  1999,  1037,
         9324,  1012,   102])"
891,1,['population'], Cluster Sampling,seg_183,"assume that we still restrict the population to adults, and we are interested in their consumption of sanitary towels. this situation is just the reverse: households with two adults will most often be one person of each sex; therefore, there will be a very large difference in their consumption of sanitary towels!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 9324, 16227])","tensor([  101,  7868,  2008,  2057,  2145, 21573,  1996,  2313,  2000,  6001,
         1010,  1998,  2057,  2024,  4699,  1999,  2037,  8381,  1997, 25480,
        24213,  1012,  2023,  3663,  2003,  2074,  1996,  7901,  1024,  3911,
         2007,  2048,  6001,  2097,  2087,  2411,  2022,  2028,  2711,  1997,
         2169,  3348,  1025,  3568,  1010,  2045,  2097,  2022,  1037,  2200,
         2312,  4489,  1999,  2037,  8381,  1997, 25480, 24213,   999,   102])"
892,1,['sampling'], Cluster Sampling,seg_183,the sampling of school children in a specific school may be carried out in two stages:,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 9324, 16227])","tensor([  101,  1996, 16227,  1997,  2082,  2336,  1999,  1037,  3563,  2082,
         2089,  2022,  3344,  2041,  1999,  2048,  5711,  1024,   102])"
893,1,"['random sampling', 'simple random sampling', 'random', 'sampling']", Cluster Sampling,seg_183,"– first, we select a number of classes by simple random sampling from a sampling",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.])","tensor([ 9324, 16227])","tensor([  101,  1516,  2034,  1010,  2057,  7276,  1037,  2193,  1997,  4280,
         2011,  3722,  6721, 16227,  2013,  1037, 16227,   102])"
894,1,['random'], Cluster Sampling,seg_183,frame of all classes. – then we select a number of children from each selected class by simple random,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0.])","tensor([ 9324, 16227])","tensor([ 101, 4853, 1997, 2035, 4280, 1012, 1516, 2059, 2057, 7276, 1037, 2193,
        1997, 2336, 2013, 2169, 3479, 2465, 2011, 3722, 6721,  102])"
895,1,"['cluster sampling', 'sampling']", Cluster Sampling,seg_183,"we use cluster sampling, because it only requires lists of children in the classes, which have been selected!",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 9324, 16227])","tensor([  101,  2057,  2224,  9324, 16227,  1010,  2138,  2009,  2069,  5942,
         7201,  1997,  2336,  1999,  1996,  4280,  1010,  2029,  2031,  2042,
         3479,   999,   102])"
896,1,"['random sampling', 'simple random sampling', 'sampling units', 'random', 'sampling']", Cluster Sampling,seg_183,how to select sampling units and analysis units? we select a number of sampling units by simple random sampling. this step has (at least) two options:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 9324, 16227])","tensor([  101,  2129,  2000,  7276, 16227,  3197,  1998,  4106,  3197,  1029,
         2057,  7276,  1037,  2193,  1997, 16227,  3197,  2011,  3722,  6721,
        16227,  1012,  2023,  3357,  2038,  1006,  2012,  2560,  1007,  2048,
         7047,  1024,   102])"
897,1,"['probability', 'sampling units', 'sampling', 'sampling unit']", Cluster Sampling,seg_183,"– sampling units are selected with the same probability. – sampling units are selected with a probability proportional to their size. the larger a sampling unit (e.g., household or school class), the larger the probability of selection.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 9324, 16227])","tensor([  101,  1516, 16227,  3197,  2024,  3479,  2007,  1996,  2168,  9723,
         1012,  1516, 16227,  3197,  2024,  3479,  2007,  1037,  9723, 14267,
         2000,  2037,  2946,  1012,  1996,  3469,  1037, 16227,  3131,  1006,
         1041,  1012,  1043,  1012,  1010,  4398,  2030,  2082,  2465,  1007,
         1010,  1996,  3469,  1996,  9723,  1997,  4989,  1012,   102])"
898,1,"['sampling', 'sampling unit']", Cluster Sampling,seg_183,"from each sampling unit, one or more analysis units are selected. this step has several options:",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 9324, 16227])","tensor([  101,  2013,  2169, 16227,  3131,  1010,  2028,  2030,  2062,  4106,
         3197,  2024,  3479,  1012,  2023,  3357,  2038,  2195,  7047,  1024,
          102])"
899,0,[], Cluster Sampling,seg_183,– to select one analysis unit. – to select all the analysis units. (continued),tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 9324, 16227])","tensor([ 101, 1516, 2000, 7276, 2028, 4106, 3131, 1012, 1516, 2000, 7276, 2035,
        1996, 4106, 3197, 1012, 1006, 2506, 1007,  102])"
900,1,"['sampling', 'sampling unit']", Cluster Sampling,seg_183,"– to select a number of analysis units proportional to the size of the sampling unit. the larger a sampling unit, the more analysis units must be selected.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 9324, 16227])","tensor([  101,  1516,  2000,  7276,  1037,  2193,  1997,  4106,  3197, 14267,
         2000,  1996,  2946,  1997,  1996, 16227,  3131,  1012,  1996,  3469,
         1037, 16227,  3131,  1010,  1996,  2062,  4106,  3197,  2442,  2022,
         3479,  1012,   102])"
901,0,[], Cluster Sampling,seg_183,the topic is very large. we refer to specialized books on the topic.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 9324, 16227])","tensor([ 101, 1996, 8476, 2003, 2200, 2312, 1012, 2057, 6523, 2000, 7772, 2808,
        2006, 1996, 8476, 1012,  102])"
902,1,"['sampling', 'sampling frame', 'randomization']", Systematic Sampling,seg_185,"if you do not have a sampling frame in the form of a database, you can use systematic sampling, which despite the name is based on randomization!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([11778, 16227])","tensor([  101,  2065,  2017,  2079,  2025,  2031,  1037, 16227,  4853,  1999,
         1996,  2433,  1997,  1037,  7809,  1010,  2017,  2064,  2224, 11778,
        16227,  1010,  2029,  2750,  1996,  2171,  2003,  2241,  2006,  6721,
         3989,   999,   102])"
903,1,"['random sampling', 'simple random sampling', 'method', 'sampling', 'random']", Systematic Sampling,seg_185,systematic sampling can be seen as a practically feasible approach in situations where simple random sampling is not feasible! the method is best illustrated by an example.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([11778, 16227])","tensor([  101, 11778, 16227,  2064,  2022,  2464,  2004,  1037,  8134, 22945,
         3921,  1999,  8146,  2073,  3722,  6721, 16227,  2003,  2025, 22945,
          999,  1996,  4118,  2003,  2190,  7203,  2011,  2019,  2742,  1012,
          102])"
904,1,['sample'], Systematic Sampling,seg_185,"in the fitness club survey, we have a list of all kids in the club. there are 300 in total; we select 30 kids for the sample. we must therefore select every tenth kid from the list.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([11778, 16227])","tensor([  101,  1999,  1996, 10516,  2252,  5002,  1010,  2057,  2031,  1037,
         2862,  1997,  2035,  4268,  1999,  1996,  2252,  1012,  2045,  2024,
         3998,  1999,  2561,  1025,  2057,  7276,  2382,  4268,  2005,  1996,
         7099,  1012,  2057,  2442,  3568,  7276,  2296,  7891,  4845,  2013,
         1996,  2862,  1012,   102])"
905,1,"['sampling', 'randomization']", Systematic Sampling,seg_185,the only randomization in this sampling approach is the selection of the first kid!,tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([11778, 16227])","tensor([  101,  1996,  2069,  6721,  3989,  1999,  2023, 16227,  3921,  2003,
         1996,  4989,  1997,  1996,  2034,  4845,   999,   102])"
906,1,"['random number', 'random']", Systematic Sampling,seg_185,"– we choose a random number between 1 and 10, for example 7. we choose kid",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([11778, 16227])","tensor([ 101, 1516, 2057, 5454, 1037, 6721, 2193, 2090, 1015, 1998, 2184, 1010,
        2005, 2742, 1021, 1012, 2057, 5454, 4845,  102])"
907,0,[], Systematic Sampling,seg_185,"no. 7 from the list. – then we select every tenth kid from the list. this means, that we will select",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0])","tensor([11778, 16227])","tensor([ 101, 2053, 1012, 1021, 2013, 1996, 2862, 1012, 1516, 2059, 2057, 7276,
        2296, 7891, 4845, 2013, 1996, 2862, 1012, 2023, 2965, 1010, 2008, 2057,
        2097, 7276,  102])"
908,1,['method'], Systematic Sampling,seg_185,"the method can be used to select customers entering a shop, who should participate in a questionnaire survey. if every tenth customer is to be selected, we can use exactly the same approach.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([11778, 16227])","tensor([  101,  1996,  4118,  2064,  2022,  2109,  2000,  7276,  6304,  5738,
         1037,  4497,  1010,  2040,  2323,  5589,  1999,  1037,  3160, 20589,
         5002,  1012,  2065,  2296,  7891,  8013,  2003,  2000,  2022,  3479,
         1010,  2057,  2064,  2224,  3599,  1996,  2168,  3921,  1012,   102])"
909,1,"['uncertainty', 'random sampling', 'simple random sampling', 'statistical uncertainty', 'random', 'sampling', 'statistical', 'case']", Systematic Sampling,seg_185,"the statistical uncertainty may be larger or smaller, than when simple random sampling is used. it is very difficult to tell a priori, what will be the case.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([11778, 16227])","tensor([  101,  1996,  7778, 12503,  2089,  2022,  3469,  2030,  3760,  1010,
         2084,  2043,  3722,  6721, 16227,  2003,  2109,  1012,  2009,  2003,
         2200,  3697,  2000,  2425,  1037,  3188,  2072,  1010,  2054,  2097,
         2022,  1996,  2553,  1012,   102])"
910,1,"['sampling', 'random', 'randomization', 'random error', 'error']", Quota Sampling,seg_187,"we now briefly discuss some methods of sampling, which are not based on a randomization mechanism. this should be avoided if possible, because we are not able to assess the size of the random error!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0.])","tensor([20563, 16227])","tensor([  101,  2057,  2085,  4780,  6848,  2070,  4725,  1997, 16227,  1010,
         2029,  2024,  2025,  2241,  2006,  1037,  6721,  3989,  7337,  1012,
         2023,  2323,  2022,  9511,  2065,  2825,  1010,  2138,  2057,  2024,
         2025,  2583,  2000, 14358,  1996,  2946,  1997,  1996,  6721,  7561,
          999,   102])"
911,1,"['random sampling', 'simple random sampling', 'random', 'sampling', 'sampling frame', 'randomization']", Quota Sampling,seg_187,"having said that, there are situations where randomization is not feasible. quota sampling is typically used in situations where the interviewer is in e.g., a shopping mall. simple random sampling of customers is not feasible. it is not possible to prepare a sampling frame.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0.])","tensor([20563, 16227])","tensor([  101,  2383,  2056,  2008,  1010,  2045,  2024,  8146,  2073,  6721,
         3989,  2003,  2025, 22945,  1012, 20563, 16227,  2003,  4050,  2109,
         1999,  8146,  2073,  1996,  4357,  2121,  2003,  1999,  1041,  1012,
         1043,  1012,  1010,  1037,  6023,  6670,  1012,  3722,  6721, 16227,
         1997,  6304,  2003,  2025, 22945,  1012,  2009,  2003,  2025,  2825,
         2000,  7374,  1037, 16227,  4853,  1012,   102])"
912,0,[], Quota Sampling,seg_187,"instead, the interviewer will have a number quota to be “filled”. often we use a number of age groups for each sex: e.g., men aged 15–29, 30–44, etc. the interviewer needs a number of persons in each group.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([20563, 16227])","tensor([  101,  2612,  1010,  1996,  4357,  2121,  2097,  2031,  1037,  2193,
        20563,  2000,  2022,  1523,  3561,  1524,  1012,  2411,  2057,  2224,
         1037,  2193,  1997,  2287,  2967,  2005,  2169,  3348,  1024,  1041,
         1012,  1043,  1012,  1010,  2273,  4793,  2321,  1516,  2756,  1010,
         2382,  1516,  4008,  1010,  4385,  1012,  1996,  4357,  2121,  3791,
         1037,  2193,  1997,  5381,  1999,  2169,  2177,  1012,   102])"
913,1,['population'], Quota Sampling,seg_187,"the interviewer has opportunity to “spot” potential candidates for each group, as they appear. in this way it seems that we cover the population in a reasonable “representative” manner. however, we do not have any opportunity to demonstrate, that this is true. . .",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([20563, 16227])","tensor([  101,  1996,  4357,  2121,  2038,  4495,  2000,  1523,  3962,  1524,
         4022,  5347,  2005,  2169,  2177,  1010,  2004,  2027,  3711,  1012,
         1999,  2023,  2126,  2009,  3849,  2008,  2057,  3104,  1996,  2313,
         1999,  1037,  9608,  1523,  4387,  1524,  5450,  1012,  2174,  1010,
         2057,  2079,  2025,  2031,  2151,  4495,  2000, 10580,  1010,  2008,
         2023,  2003,  2995,  1012,  1012,  1012,   102])"
914,1,"['uncertainty', 'statistical uncertainty', 'biased', 'sample', 'statistical']", Quota Sampling,seg_187,"the sample may be “representative” in terms of sex and age. but there may be a number of other criteria, where the sample is biased (“unbalanced”), and we do not know anything about it! at the same time we have no idea about the size of the statistical uncertainty!",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([20563, 16227])","tensor([  101,  1996,  7099,  2089,  2022,  1523,  4387,  1524,  1999,  3408,
         1997,  3348,  1998,  2287,  1012,  2021,  2045,  2089,  2022,  1037,
         2193,  1997,  2060,  9181,  1010,  2073,  1996,  7099,  2003, 25352,
         1006,  1523,  4895, 26657,  2094,  1524,  1007,  1010,  1998,  2057,
         2079,  2025,  2113,  2505,  2055,  2009,   999,  2012,  1996,  2168,
         2051,  2057,  2031,  2053,  2801,  2055,  1996,  2946,  1997,  1996,
         7778, 12503,   999,   102])"
915,1,"['sampling units', 'population', 'sample', 'sampling', 'randomization']", Purposive Sampling,seg_189,"this technique is used as an easy way to get a quick sample, which is similar to the population. we may have detailed knowledge about the population, so that we can sample a few “typical” sampling units. this is done quite purposively, without any kind of randomization.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0.])","tensor([16405, 14536, 20049,  3726, 16227])","tensor([  101,  2023,  6028,  2003,  2109,  2004,  2019,  3733,  2126,  2000,
         2131,  1037,  4248,  7099,  1010,  2029,  2003,  2714,  2000,  1996,
         2313,  1012,  2057,  2089,  2031,  6851,  3716,  2055,  1996,  2313,
         1010,  2061,  2008,  2057,  2064,  7099,  1037,  2261,  1523,  5171,
         1524, 16227,  3197,  1012,  2023,  2003,  2589,  3243, 16405, 14536,
        20049, 15985,  2100,  1010,  2302,  2151,  2785,  1997,  6721,  3989,
         1012,   102])"
916,1,"['sample', 'population']", Purposive Sampling,seg_189,"if we are lucky (and clever!) we can thereby obtain a sample, which is very similar to the population.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.])","tensor([16405, 14536, 20049,  3726, 16227])","tensor([  101,  2065,  2057,  2024,  5341,  1006,  1998, 12266,   999,  1007,
         2057,  2064,  8558,  6855,  1037,  7099,  1010,  2029,  2003,  2200,
         2714,  2000,  1996,  2313,  1012,   102])"
917,1,"['sampling', 'sampling unit']", Purposive Sampling,seg_189,the sampling unit will often be an administrative unit consisting of several analysis units.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([16405, 14536, 20049,  3726, 16227])","tensor([  101,  1996, 16227,  3131,  2097,  2411,  2022,  2019,  3831,  3131,
         5398,  1997,  2195,  4106,  3197,  1012,   102])"
918,1,"['method', 'sampling']", Purposive Sampling,seg_189,the drawbacks of this method are the same as for quota sampling.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([16405, 14536, 20049,  3726, 16227])","tensor([  101,  1996,  4009, 12221,  1997,  2023,  4118,  2024,  1996,  2168,
         2004,  2005, 20563, 16227,  1012,   102])"
919,1,['sampling'], Purposive Sampling,seg_189,"early exit polls after elections can be produced by selection a few “typical” municipalities, which “reflect” the nation quite precisely. in each municipality we select a few polling stations, using our knowledge about the municipality to “cover” the municipality “representatively”. in each polling station we ask a number of voters, how they voted. the selection of these voters could be done by e.g., systematic sampling.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([16405, 14536, 20049,  3726, 16227])","tensor([  101,  2220,  6164, 14592,  2044,  3864,  2064,  2022,  2550,  2011,
         4989,  1037,  2261,  1523,  5171,  1524,  7602,  1010,  2029,  1523,
         8339,  1524,  1996,  3842,  3243, 10785,  1012,  1999,  2169,  3250,
         2057,  7276,  1037,  2261, 17888,  3703,  1010,  2478,  2256,  3716,
         2055,  1996,  3250,  2000,  1523,  3104,  1524,  1996,  3250,  1523,
         4387,  2135,  1524,  1012,  1999,  2169, 17888,  2276,  2057,  3198,
         1037,  2193,  1997,  7206,  1010,  2129,  2027,  5444,  1012,  1996,
         4989,  1997,  2122,  7206,  2071,  2022,  2589,  2011,  1041,  1012,
         1043,  1012,  1010, 11778, 16227,  1012,   102])"
920,1,['sample'], Convenience Sampling,seg_191,"the sample is selected “haphazardly”. it may be volunteers, “friends and relatives,” etc.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([15106, 16227])","tensor([  101,  1996,  7099,  2003,  3479,  1523,  5292, 21890, 26154,  2135,
         1524,  1012,  2009,  2089,  2022,  7314,  1010,  1523,  2814,  1998,
         9064,  1010,  1524,  4385,  1012,   102])"
921,1,"['statistical', 'sample']", Convenience Sampling,seg_191,such sample surveys have no statistical value!,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0.])","tensor([15106, 16227])","tensor([  101,  2107,  7099, 12265,  2031,  2053,  7778,  3643,   999,   102])"
922,1,"['samples', 'statistical', 'test']", Convenience Sampling,seg_191,"see discussion on internet polls and telephone polls, etc. earlier in this chapter. however, this type of samples need not totally be condemned: they can be useful to test (parts of) a questionnaire in order to assess whether the wording of one or more questions needs to be changed. this is often called a pilot survey. we are only interested in a qualitative (not a statistical) evaluation of the questionnaire!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([15106, 16227])","tensor([  101,  2156,  6594,  2006,  4274, 14592,  1998,  7026, 14592,  1010,
         4385,  1012,  3041,  1999,  2023,  3127,  1012,  2174,  1010,  2023,
         2828,  1997,  8168,  2342,  2025,  6135,  2022, 10033,  1024,  2027,
         2064,  2022,  6179,  2000,  3231,  1006,  3033,  1997,  1007,  1037,
         3160, 20589,  1999,  2344,  2000, 14358,  3251,  1996,  2773,  2075,
         1997,  2028,  2030,  2062,  3980,  3791,  2000,  2022,  2904,  1012,
         2023,  2003,  2411,  2170,  1037,  4405,  5002,  1012,  2057,  2024,
         2069,  4699,  1999,  1037, 24209, 11475, 27453,  1006,  2025,  1037,
         7778,  1007,  9312,  1997,  1996,  3160, 20589,   999,   102])"
923,1,"['sample', 'experiment', 'data']", Convenience Sampling,seg_191,"in chap. 5, we discussed how to analyze qualitative data from a sample survey or an experiment.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 1., 0., 0.])","tensor([15106, 16227])","tensor([  101,  1999, 15775,  2361,  1012,  1019,  1010,  2057,  6936,  2129,
         2000, 17908, 24209, 11475, 27453,  2951,  2013,  1037,  7099,  5002,
         2030,  2019,  7551,  1012,   102])"
924,1,"['sample', 'experiment', 'quantitative', 'data']", Convenience Sampling,seg_191,"in this chapter we have discussed various aspects in connection with planning a sample survey or an experiment. in the next two chapters, we describe, how we can analyze quantitative data from a sample survey or an experiment.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 1., 0., 0.])","tensor([15106, 16227])","tensor([  101,  1999,  2023,  3127,  2057,  2031,  6936,  2536,  5919,  1999,
         4434,  2007,  4041,  1037,  7099,  5002,  2030,  2019,  7551,  1012,
         1999,  1996,  2279,  2048,  9159,  1010,  2057,  6235,  1010,  2129,
         2057,  2064, 17908, 20155,  2951,  2013,  1037,  7099,  5002,  2030,
         2019,  7551,  1012,   102])"
925,1,['variables'],Chapter  Assessment of Relationship,seg_193,"in many different disciplines you need to assess, whether there is a relationship between two variables. this can be in administration, social sciences, economics, industry, and science.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  1999,  2116,  2367, 12736,  2017,  2342,  2000, 14358,  1010,
         3251,  2045,  2003,  1037,  3276,  2090,  2048, 10857,  1012,  2023,
         2064,  2022,  1999,  3447,  1010,  2591,  4163,  1010,  5543,  1010,
         3068,  1010,  1998,  2671,  1012,   102])"
926,0,[],Chapter  Assessment of Relationship,seg_193,the purpose could be one of the following:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([3127, 7667, 1997, 3276])","tensor([ 101, 1996, 3800, 2071, 2022, 2028, 1997, 1996, 2206, 1024,  102])"
927,0,[],Chapter  Assessment of Relationship,seg_193,– to get a basic understanding of a subject area – to find reasons or explanations of phenomena – to try to predict future developments,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  1516,  2000,  2131,  1037,  3937,  4824,  1997,  1037,  3395,
         2181,  1516,  2000,  2424,  4436,  2030, 17959,  1997, 13352,  1516,
         2000,  3046,  2000, 16014,  2925,  8973,   102])"
928,1,['statistical'],Chapter  Assessment of Relationship,seg_193,we study some techniques to assess a relationship and assess whether an apparent relationship is real or just a statistical coincidence.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0.])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  2057,  2817,  2070,  5461,  2000, 14358,  1037,  3276,  1998,
        14358,  3251,  2019,  6835,  3276,  2003,  2613,  2030,  2074,  1037,
         7778, 16507,  1012,   102])"
929,1,"['regression', 'linear', 'regression analysis', 'linear regression', 'variables', 'plot']",Chapter  Assessment of Relationship,seg_193,"the technique is called regression analysis (*). we will only consider the basic technique, linear regression, which assumes that there is a linear relationship between two variables, i.e., a plot of y against x shows a number of points scattered around a straight line.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  1996,  6028,  2003,  2170, 26237,  4106,  1006,  1008,  1007,
         1012,  2057,  2097,  2069,  5136,  1996,  3937,  6028,  1010,  7399,
        26237,  1010,  2029, 15980,  2008,  2045,  2003,  1037,  7399,  3276,
         2090,  2048, 10857,  1010,  1045,  1012,  1041,  1012,  1010,  1037,
         5436,  1997,  1061,  2114,  1060,  3065,  1037,  2193,  1997,  2685,
         7932,  2105,  1037,  3442,  2240,  1012,   102])"
930,1,"['dependent variable', 'independent variable', 'dependent', 'variables', 'variable', 'independent']",Chapter  Assessment of Relationship,seg_193,one of the variables is the y-variable or dependent variable. the other variable is the x-variable or the independent variable.,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 1., 1., 0., 0.])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  2028,  1997,  1996, 10857,  2003,  1996,  1061,  1011,  8023,
         2030,  7790,  8023,  1012,  1996,  2060,  8023,  2003,  1996,  1060,
         1011,  8023,  2030,  1996,  2981,  8023,  1012,   102])"
931,0,[],Chapter  Assessment of Relationship,seg_193,"the subject is treated fairly briefly. we refer to the literature list, if you want to study the issue thoroughly.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  1996,  3395,  2003,  5845,  7199,  4780,  1012,  2057,  6523,
         2000,  1996,  3906,  2862,  1010,  2065,  2017,  2215,  2000,  2817,
         1996,  3277, 12246,  1012,   102])"
932,1,"['statistical', 'regression', 'regression analysis']",Chapter  Assessment of Relationship,seg_193,"the calculations are quite complicated. you can use an advanced calculator with built-in regression analysis, but you are better off with a spreadsheet or statistical software.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0.])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  1996, 16268,  2024,  3243,  8552,  1012,  2017,  2064,  2224,
         2019,  3935, 10250, 19879,  4263,  2007,  2328,  1011,  1999, 26237,
         4106,  1010,  2021,  2017,  2024,  2488,  2125,  2007,  1037, 20861,
        21030,  2102,  2030,  7778,  4007,  1012,   102])"
933,1,"['regression', 'multiple regression', 'regression analysis', 'nonlinear regression', 'nonlinear', 'statistical']",Chapter  Assessment of Relationship,seg_193,"there are also more advanced types of regression analysis, for instance nonlinear regression or multiple regression (multiple x-variables). you can read more in many of the books from the literature list or study the help facility in your spreadsheet or other statistical software.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  2045,  2024,  2036,  2062,  3935,  4127,  1997, 26237,  4106,
         1010,  2005,  6013, 27400, 26237,  2030,  3674, 26237,  1006,  3674,
         1060,  1011, 10857,  1007,  1012,  2017,  2064,  3191,  2062,  1999,
         2116,  1997,  1996,  2808,  2013,  1996,  3906,  2862,  2030,  2817,
         1996,  2393,  4322,  1999,  2115, 20861, 21030,  2102,  2030,  2060,
         7778,  4007,  1012,   102])"
934,1,"['regression', 'linear regression', 'linear']",Chapter  Assessment of Relationship,seg_193,"this chapter discusses the fundamental concepts of linear regression, with a practical example. we do not show the calculation formulas for the different",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  2023,  3127, 15841,  1996,  8050,  8474,  1997,  7399, 26237,
         1010,  2007,  1037,  6742,  2742,  1012,  2057,  2079,  2025,  2265,
         1996, 17208, 25814,  2005,  1996,  2367,   102])"
935,0,[],Chapter  Assessment of Relationship,seg_193,"statistics. these formulas are only important, if you do not have a spreadsheet (or other software) or an advanced calculator to do the calculations!",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  6747,  1012,  2122, 25814,  2024,  2069,  2590,  1010,  2065,
         2017,  2079,  2025,  2031,  1037, 20861, 21030,  2102,  1006,  2030,
         2060,  4007,  1007,  2030,  2019,  3935, 10250, 19879,  4263,  2000,
         2079,  1996, 16268,   999,   102])"
936,1,"['statistical', 'functions']",Chapter  Assessment of Relationship,seg_193,"here, we use statistical functions of microsoft excel, open office calc, and several other spreadsheets. microsoft excel also has additional options in the add-in menu “data analysis”, under the item “regression”.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  2182,  1010,  2057,  2224,  7778,  4972,  1997,  7513, 24970,
         1010,  2330,  2436, 10250,  2278,  1010,  1998,  2195,  2060, 20861,
        21030,  3215,  1012,  7513, 24970,  2036,  2038,  3176,  7047,  1999,
         1996,  5587,  1011,  1999, 12183,  1523,  2951,  4106,  1524,  1010,
         2104,  1996,  8875,  1523, 26237,  1524,  1012,   102])"
937,0,[],Chapter  Assessment of Relationship,seg_193,it is important to use these techniques critically. ask questions such as:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  2009,  2003,  2590,  2000,  2224,  2122,  5461, 11321,  1012,
         3198,  3980,  2107,  2004,  1024,   102])"
938,1,['linear'],Chapter  Assessment of Relationship,seg_193,– is there a relationship? – is it a linear relationship? – is there causality?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  1516,  2003,  2045,  1037,  3276,  1029,  1516,  2003,  2009,
         1037,  7399,  3276,  1029,  1516,  2003,  2045, 28102,  3012,  1029,
          102])"
939,1,"['linear', 'nonlinear', 'statistics', 'statistical', 'variables']",Chapter  Assessment of Relationship,seg_193,"note: statistics can tell, whether there is a statistical relationship between two variables, whether it is linear or possibly more complicated (nonlinear). but statistics cannot tell, whether one phenomenon is the cause of another, i.e., whether there is a causal relationship. here professional knowledge is needed.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  3602,  1024,  6747,  2064,  2425,  1010,  3251,  2045,  2003,
         1037,  7778,  3276,  2090,  2048, 10857,  1010,  3251,  2009,  2003,
         7399,  2030,  4298,  2062,  8552,  1006, 27400,  1007,  1012,  2021,
         6747,  3685,  2425,  1010,  3251,  2028,  9575,  2003,  1996,  3426,
         1997,  2178,  1010,  1045,  1012,  1041,  1012,  1010,  3251,  2045,
         2003,  1037, 28102,  3276,  1012,  2182,  2658,  3716,  2003,  2734,
         1012,   102])"
940,1,"['variables', 'plot', 'variable']",Chapter  Assessment of Relationship,seg_193,"many variables in the social and natural sciences increase with time. in this situation, a plot of any variable against any other variable will show a reasonably clear relationship.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  2116, 10857,  1999,  1996,  2591,  1998,  3019,  4163,  3623,
         2007,  2051,  1012,  1999,  2023,  3663,  1010,  1037,  5436,  1997,
         2151,  8023,  2114,  2151,  2060,  8023,  2097,  2265,  1037, 16286,
         3154,  3276,  1012,   102])"
941,1,"['error', 'results', 'statistical', 'variables']",Chapter  Assessment of Relationship,seg_193,"however, this is a statistical relationship, not a causal relationship. the real relationship, which is hidden in this way, is an increase with time for both variables. this is a fairly common error in connection with interpretation of statistical results.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  2174,  1010,  2023,  2003,  1037,  7778,  3276,  1010,  2025,
         1037, 28102,  3276,  1012,  1996,  2613,  3276,  1010,  2029,  2003,
         5023,  1999,  2023,  2126,  1010,  2003,  2019,  3623,  2007,  2051,
         2005,  2119, 10857,  1012,  2023,  2003,  1037,  7199,  2691,  7561,
         1999,  4434,  2007,  7613,  1997,  7778,  3463,  1012,   102])"
942,0,[],Chapter  Assessment of Relationship,seg_193,"assume that the number of storks and the number of children in a given area increase in a certain period of time; you cannot conclude that the storks are coming with the children! this is a kind of false conclusion, we often find in newspaper articles.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  7868,  2008,  1996,  2193,  1997,  2358,  2953,  5705,  1998,
         1996,  2193,  1997,  2336,  1999,  1037,  2445,  2181,  3623,  1999,
         1037,  3056,  2558,  1997,  2051,  1025,  2017,  3685, 16519,  2008,
         1996,  2358,  2953,  5705,  2024,  2746,  2007,  1996,  2336,   999,
         2023,  2003,  1037,  2785,  1997,  6270,  7091,  1010,  2057,  2411,
         2424,  1999,  3780,  4790,  1012,   102])"
943,1,['variable'],Chapter  Assessment of Relationship,seg_193,"the underlying (third) variable need not be time, but often it is.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([3127, 7667, 1997, 3276])","tensor([  101,  1996, 10318,  1006,  2353,  1007,  8023,  2342,  2025,  2022,
         2051,  1010,  2021,  2411,  2009,  2003,  1012,   102])"
944,0,[], Example,seg_195,let us consider the height and weight of the 17 boys in the fitness club survey.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])",tensor([2742]),"tensor([  101,  2292,  2149,  5136,  1996,  4578,  1998,  3635,  1997,  1996,
         2459,  3337,  1999,  1996, 10516,  2252,  5002,  1012,   102])"
945,1,['linear'], Example,seg_195,we ask the following question: is there a (possibly linear) relationship between height (x) and weight (y)?,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([ 101, 2057, 3198, 1996, 2206, 3160, 1024, 2003, 2045, 1037, 1006, 4298,
        7399, 1007, 3276, 2090, 4578, 1006, 1060, 1007, 1998, 3635, 1006, 1061,
        1007, 1029,  102])"
946,1,['dependent'], Example,seg_195,"we assume that the weight is dependent on the height. therefore, we put height as x and weight as y.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([ 101, 2057, 7868, 2008, 1996, 3635, 2003, 7790, 2006, 1996, 4578, 1012,
        3568, 1010, 2057, 2404, 4578, 2004, 1060, 1998, 3635, 2004, 1061, 1012,
         102])"
947,1,['loss'], Example,seg_195,"one purpose could be to identify boys, who weigh too much compared to their height; these boys might be interested in an intensive weight loss program!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2028,  3800,  2071,  2022,  2000,  6709,  3337,  1010,  2040,
        17042,  2205,  2172,  4102,  2000,  2037,  4578,  1025,  2122,  3337,
         2453,  2022,  4699,  1999,  2019, 11806,  3635,  3279,  2565,   999,
          102])"
948,1,"['method of least squares', 'method', 'least squares', 'plot', 'data']", Example,seg_195,"the first step is always to plot data. the following is a plot of weight vs. height. in addition, we can see the straight line that makes the best fit to data. this line is established by the method of least squares (*). see the literature list for books with a detailed review of the",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  2034,  3357,  2003,  2467,  2000,  5436,  2951,  1012,
         1996,  2206,  2003,  1037,  5436,  1997,  3635,  5443,  1012,  4578,
         1012,  1999,  2804,  1010,  2057,  2064,  2156,  1996,  3442,  2240,
         2008,  3084,  1996,  2190,  4906,  2000,  2951,  1012,  2023,  2240,
         2003,  2511,  2011,  1996,  4118,  1997,  2560, 14320,  1006,  1008,
         1007,  1012,  2156,  1996,  3906,  2862,  2005,  2808,  2007,  1037,
         6851,  3319,  1997,  1996,   102])"
949,1,"['regression', 'regression line']", Example,seg_195,method. the line is called the regression line (*). see the help of your spreadsheet how to do this (fig. 7.1).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  4118,  1012,  1996,  2240,  2003,  2170,  1996, 26237,  2240,
         1006,  1008,  1007,  1012,  2156,  1996,  2393,  1997,  2115, 20861,
        21030,  2102,  2129,  2000,  2079,  2023,  1006, 20965,  1012,  1021,
         1012,  1015,  1007,  1012,   102])"
950,1,"['statistical', 'model', 'data', 'statistical model']", Example,seg_195,"a statistical model can describe these data. it describes the weight y of a randomly chosen boy, knowing his height x, through the equation",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1037,  7778,  2944,  2064,  6235,  2122,  2951,  1012,  2009,
         5577,  1996,  3635,  1061,  1997,  1037, 18154,  4217,  2879,  1010,
         4209,  2010,  4578,  1060,  1010,  2083,  1996,  8522,   102])"
951,0,['e'], Example,seg_195,if we ignore the term e that is precisely the equation for a straight line. here,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])",tensor([2742]),"tensor([  101,  2065,  2057,  8568,  1996,  2744,  1041,  2008,  2003, 10785,
         1996,  8522,  2005,  1037,  3442,  2240,  1012,  2182,   102])"
952,1,"['residual', 'regression', 'regression line', 'slope', 'variation', 'intercept', 'random variation', 'random']", Example,seg_195,"x ¼ height of a boy y ¼ weight of a boy a ¼ intercept (on y-axis) of the regression line b ¼ slope of the regression line e ¼ random variation of y (for a given value of x), often called residual",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])",tensor([2742]),"tensor([  101,  1060,  1091,  4578,  1997,  1037,  2879,  1061,  1091,  3635,
         1997,  1037,  2879,  1037,  1091, 19115,  1006,  2006,  1061,  1011,
         8123,  1007,  1997,  1996, 26237,  2240,  1038,  1091,  9663,  1997,
         1996, 26237,  2240,  1041,  1091,  6721,  8386,  1997,  1061,  1006,
         2005,  1037,  2445,  3643,  1997,  1060,  1007,  1010,  2411,  2170,
        21961,   102])"
953,1,"['curve', 'function', 'linear', 'transforming', 'nonlinear', 'transformed', 'data']", Example,seg_195,"sometimes, data need to be transformed in order to obtain a linear relationship. if the points tend to be grouped around a (nonlinear) curve, a seemingly nonlinear relationship can become linear by transforming y and/or x, e.g., with the logarithm function. in the graph with weight plotted against height, there is no immediate sign of a nonlinear relationship.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2823,  1010,  2951,  2342,  2000,  2022,  8590,  1999,  2344,
         2000,  6855,  1037,  7399,  3276,  1012,  2065,  1996,  2685,  7166,
         2000,  2022, 15131,  2105,  1037,  1006, 27400,  1007,  7774,  1010,
         1037,  9428, 27400,  3276,  2064,  2468,  7399,  2011, 17903,  1061,
         1998,  1013,  2030,  1060,  1010,  1041,  1012,  1043,  1012,  1010,
         2007,  1996,  8833,  8486,  2705,  2213,  3853,  1012,  1999,  1996,
        10629,  2007,  3635, 27347,  2114,  4578,  1010,  2045,  2003,  2053,
         6234,  3696,  1997,  1037, 27400,  3276,  1012,   102])"
954,1,"['linear', 'table', 'coefficient', 'correlation coefficient', 'correlation']", Example,seg_195,"to describe the degree of (linear) relationship between x and y we use the correlation coefficient (*), often simply called the correlation and labeled with the letter r (table 7.1).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2000,  6235,  1996,  3014,  1997,  1006,  7399,  1007,  3276,
         2090,  1060,  1998,  1061,  2057,  2224,  1996, 16902, 19064,  1006,
         1008,  1007,  1010,  2411,  3432,  2170,  1996, 16902,  1998, 12599,
         2007,  1996,  3661,  1054,  1006,  2795,  1021,  1012,  1015,  1007,
         1012,   102])"
955,0,[], Example,seg_195,"this is illustrated in fig. 7.3. practical situations are usually not that clear-cut! the graph with weight plotted against height is equivalent to a situation where r > 0. by visual inspection, most people will probably find that r may be closer to 1 than to 0. we will determine r below.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])",tensor([2742]),"tensor([  101,  2023,  2003,  7203,  1999, 20965,  1012,  1021,  1012,  1017,
         1012,  6742,  8146,  2024,  2788,  2025,  2008,  3154,  1011,  3013,
          999,  1996, 10629,  2007,  3635, 27347,  2114,  4578,  2003,  5662,
         2000,  1037,  3663,  2073,  1054,  1028,  1014,  1012,  2011,  5107,
        10569,  1010,  2087,  2111,  2097,  2763,  2424,  2008,  1054,  2089,
         2022,  3553,  2000,  1015,  2084,  2000,  1014,  1012,  2057,  2097,
         5646,  1054,  2917,  1012,   102])"
956,1,"['variation', 'linear']", Example,seg_195,"sometimes we use r2, i.e., the squared value of r, which is a number between 0 and 1, as a measure of the degree of (linear) relationship between x and y. it is often written r2 (i.e., with capital “r”) or r-square. it is also called the coefficient of determination. you can say that r2 expresses how large a part of the variation in y is, which is “explained” by x.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2823,  2057,  2224,  1054,  2475,  1010,  1045,  1012,  1041,
         1012,  1010,  1996, 19942,  3643,  1997,  1054,  1010,  2029,  2003,
         1037,  2193,  2090,  1014,  1998,  1015,  1010,  2004,  1037,  5468,
         1997,  1996,  3014,  1997,  1006,  7399,  1007,  3276,  2090,  1060,
         1998,  1061,  1012,  2009,  2003,  2411,  2517,  1054,  2475,  1006,
         1045,  1012,  1041,  1012,  1010,  2007,  3007,  1523,  1054,  1524,
         1007,  2030,  1054,  1011,  2675,  1012,  2009,  2003,  2036,  2170,
         1996, 19064,  1997,  9128,  1012,  2017,  2064,  2360,  2008,  1054,
         2475, 16783,  2129,  2312,  1037,  2112,  1997,  1996,  8386,  1999,
         1061,  2003,  1010,  2029,  2003,  1523,  4541,  1524,  2011,  1060,
         1012,   102])"
957,1,"['model', 'transform']", Example,seg_195,"this can be used to compare different models. it can sometimes be difficult to see from a graph, whether we should transform, e.g., the y-variable or not (typically with the logarithm). if it is difficult to see from the graph, we can choose the model that gives the highest value of r-square.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])",tensor([2742]),"tensor([  101,  2023,  2064,  2022,  2109,  2000, 12826,  2367,  4275,  1012,
         2009,  2064,  2823,  2022,  3697,  2000,  2156,  2013,  1037, 10629,
         1010,  3251,  2057,  2323, 10938,  1010,  1041,  1012,  1043,  1012,
         1010,  1996,  1061,  1011,  8023,  2030,  2025,  1006,  4050,  2007,
         1996,  8833,  8486,  2705,  2213,  1007,  1012,  2065,  2009,  2003,
         3697,  2000,  2156,  2013,  1996, 10629,  1010,  2057,  2064,  5454,
         1996,  2944,  2008,  3957,  1996,  3284,  3643,  1997,  1054,  1011,
         2675,  1012,   102])"
958,1,"['plot', 'linear', 'data']", Example,seg_195,please note that a relatively high value of r-square is not a guarantee that a linear relationship is an adequate description of data. always study the plot also!,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])",tensor([2742]),"tensor([  101,  3531,  3602,  2008,  1037,  4659,  2152,  3643,  1997,  1054,
         1011,  2675,  2003,  2025,  1037, 11302,  2008,  1037,  7399,  3276,
         2003,  2019, 11706,  6412,  1997,  2951,  1012,  2467,  2817,  1996,
         5436,  2036,   999,   102])"
959,1,['data'], Linear Regression with Spreadsheets,seg_197,"data for the boys are shown below, but only the first few rows. data continue until row 18 in the spreadsheet (fig. 7.4).",tensor(1),"tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  2951,  2005,  1996,  3337,  2024,  3491,  2917,  1010,  2021,
         2069,  1996,  2034,  2261, 10281,  1012,  2951,  3613,  2127,  5216,
         2324,  1999,  1996, 20861, 21030,  2102,  1006, 20965,  1012,  1021,
         1012,  1018,  1007,  1012,   102])"
960,1,"['regression', 'linear', 'functions', 'linear regression']", Linear Regression with Spreadsheets,seg_197,"to perform linear regression, we use the following spreadsheet functions:",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  2000,  4685,  7399, 26237,  1010,  2057,  2224,  1996,  2206,
        20861, 21030,  2102,  4972,  1024,   102])"
961,1,['slope'], Linear Regression with Spreadsheets,seg_197,intercept slope correl rsq forecast,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101, 19115,  9663,  2522, 14343,  2140, 12667,  4160, 19939,   102])"
962,1,"['function', 'pearson', 'coefficient', 'correlation coefficient', 'pearson correlation coefficient', 'correlation']", Linear Regression with Spreadsheets,seg_197,note: there is another worksheet function called pearson to calculate the correlation coefficient. this is just another name for the same function as correl; the reason is that the correlation coefficient is often called the pearson correlation coefficient.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  3602,  1024,  2045,  2003,  2178,  2573, 21030,  2102,  3853,
         2170, 12874,  2000, 18422,  1996, 16902, 19064,  1012,  2023,  2003,
         2074,  2178,  2171,  2005,  1996,  2168,  3853,  2004,  2522, 14343,
         2140,  1025,  1996,  3114,  2003,  2008,  1996, 16902, 19064,  2003,
         2411,  2170,  1996, 12874, 16902, 19064,  1012,   102])"
963,1,"['functions', 'parameters', 'variables', 'data']", Linear Regression with Spreadsheets,seg_197,input parameters to the first four functions are the data cells for the y and x variables. see column f for the formulas; the result of applying the formulas is given in column e.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  7953, 11709,  2000,  1996,  2034,  2176,  4972,  2024,  1996,
         2951,  4442,  2005,  1996,  1061,  1998,  1060, 10857,  1012,  2156,
         5930,  1042,  2005,  1996, 25814,  1025,  1996,  2765,  1997, 11243,
         1996, 25814,  2003,  2445,  1999,  5930,  1041,  1012,   102])"
964,1,"['model', 'regression', 'predicted', 'regression line']", Linear Regression with Spreadsheets,seg_197,"column c shows the forecasts of weight, as predicted by the model. they correspond to points on the regression line, i.e., we move vertically from a point (up or down), until we hit the regression line.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  5930,  1039,  3065,  1996, 19939,  2015,  1997,  3635,  1010,
         2004, 10173,  2011,  1996,  2944,  1012,  2027, 17254,  2000,  2685,
         2006,  1996, 26237,  2240,  1010,  1045,  1012,  1041,  1012,  1010,
         2057,  2693, 20018,  2013,  1037,  2391,  1006,  2039,  2030,  2091,
         1007,  1010,  2127,  2057,  2718,  1996, 26237,  2240,  1012,   102])"
965,1,"['function', 'range', 'table', 'prediction', 'predicted', 'parameters', 'data']", Linear Regression with Spreadsheets,seg_197,"the predicted values are calculated using the function forecast. input parameters for this function are the following: first the value of x, for which you want a prediction (forecast) of the y-value. then the relevant range of data cells for y and x. here is, for example, how the content of the cell c2 is programmed (table 7.2):",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  1996, 10173,  5300,  2024, 10174,  2478,  1996,  3853, 19939,
         1012,  7953, 11709,  2005,  2023,  3853,  2024,  1996,  2206,  1024,
         2034,  1996,  3643,  1997,  1060,  1010,  2005,  2029,  2017,  2215,
         1037, 17547,  1006, 19939,  1007,  1997,  1996,  1061,  1011,  3643,
         1012,  2059,  1996,  7882,  2846,  1997,  2951,  4442,  2005,  1061,
         1998,  1060,  1012,  2182,  2003,  1010,  2005,  2742,  1010,  2129,
         1996,  4180,  1997,  1996,  3526, 29248,  2003, 16984,  1006,  2795,
         1021,  1012,  1016,  1007,  1024,   102])"
966,1,"['residual', 'results']", Linear Regression with Spreadsheets,seg_197,1 weight forecast residual results formula in spreadsheet height,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  1015,  3635, 19939, 21961,  3463,  5675,  1999, 20861, 21030,
         2102,  4578,   102])"
967,1,"['range', 'frequency', 'frequency tables', 'tables', 'data']", Linear Regression with Spreadsheets,seg_197,"note: we have used “absolute references” (dollar signs) to display the range of data cells of y and x (see chap. 5, section on frequency tables). this means that you can copy the contents of cell c2 down over the whole area c2: c18, only the reference to the actual value of x will change! see also the help to your spreadsheet.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  3602,  1024,  2057,  2031,  2109,  1523,  7619,  7604,  1524,
         1006,  7922,  5751,  1007,  2000,  4653,  1996,  2846,  1997,  2951,
         4442,  1997,  1061,  1998,  1060,  1006,  2156, 15775,  2361,  1012,
         1019,  1010,  2930,  2006,  6075,  7251,  1007,  1012,  2023,  2965,
         2008,  2017,  2064,  6100,  1996,  8417,  1997,  3526, 29248,  2091,
         2058,  1996,  2878,  2181, 29248,  1024, 27723,  2620,  1010,  2069,
         1996,  4431,  2000,  1996,  5025,  3643,  1997,  1060,  2097,  2689,
          999,  2156,  2036,  1996,  2393,  2000,  2115, 20861, 21030,  2102,
         1012,   102])"
968,1,"['confidence interval', 'interval', 'intercept', 'confidence', 'statistic']", Linear Regression with Spreadsheets,seg_197,"we see that the intercept (on y-axis) is negative. we can calculate a 95% confidence interval of this statistic; this confidence interval ranges from 66.7 to 12.1. (this could, for example, be calculated in microsoft excel using the add-in menu “data analysis”, menu-item “regression”.) this means that 0 is in the confidence interval, which makes sense. this corresponds to the fact that a boy of 0 cm weighs 0 kg!",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  2057,  2156,  2008,  1996, 19115,  1006,  2006,  1061,  1011,
         8123,  1007,  2003,  4997,  1012,  2057,  2064, 18422,  1037,  5345,
         1003,  7023, 13483,  1997,  2023, 28093,  6553,  1025,  2023,  7023,
        13483,  8483,  2013,  5764,  1012,  1021,  2000,  2260,  1012,  1015,
         1012,  1006,  2023,  2071,  1010,  2005,  2742,  1010,  2022, 10174,
         1999,  7513, 24970,  2478,  1996,  5587,  1011,  1999, 12183,  1523,
         2951,  4106,  1524,  1010, 12183,  1011,  8875,  1523, 26237,  1524,
         1012,  1007,  2023,  2965,  2008,  1014,  2003,  1999,  1996,  7023,
        13483,  1010,  2029,  3084,  3168,  1012,  2023, 14788,  2000,  1996,
         2755,  2008,  1037,  2879,  1997,  1014,  4642, 21094,  1014,  4705,
          999,   102])"
969,1,['slope'], Linear Regression with Spreadsheets,seg_197,"the slope (slope) is about 0.5, representing an increase of 0.5 kg in body weight for each additional cm in height.",tensor(1),"tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([ 101, 1996, 9663, 1006, 9663, 1007, 2003, 2055, 1014, 1012, 1019, 1010,
        5052, 2019, 3623, 1997, 1014, 1012, 1019, 4705, 1999, 2303, 3635, 2005,
        2169, 3176, 4642, 1999, 4578, 1012,  102])"
970,1,"['correlation', 'coefficient', 'correlation coefficient']", Linear Regression with Spreadsheets,seg_197,"the correlation coefficient is about 0.76 (positive, and as expected closer to 1 than 0).",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  1996, 16902, 19064,  2003,  2055,  1014,  1012,  6146,  1006,
         3893,  1010,  1998,  2004,  3517,  3553,  2000,  1015,  2084,  1014,
         1007,  1012,   102])"
971,1,"['correlation', 'coefficient', 'correlation coefficient']", Linear Regression with Spreadsheets,seg_197,the value of r-square is 0.58; this is the same as the squared value of the correlation coefficient.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  1996,  3643,  1997,  1054,  1011,  2675,  2003,  1014,  1012,
         5388,  1025,  2023,  2003,  1996,  2168,  2004,  1996, 19942,  3643,
         1997,  1996, 16902, 19064,  1012,   102])"
972,1,"['plot', 'regression', 'regression line']", Linear Regression with Spreadsheets,seg_197,"a plot of weight against height is shown in the beginning of this chapter, with the regression line.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  1037,  5436,  1997,  3635,  2114,  4578,  2003,  3491,  1999,
         1996,  2927,  1997,  2023,  3127,  1010,  2007,  1996, 26237,  2240,
         1012,   102])"
973,1,"['regression', 'residuals', 'regression line']", Linear Regression with Spreadsheets,seg_197,"column d contains the residuals, i.e., the vertical distances between each point and the regression line. they can be calculated as the difference between the columns b (weight) and c (forecast), i.e., they are calculated as weight-forecast.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  5930,  1040,  3397,  1996, 21961,  2015,  1010,  1045,  1012,
         1041,  1012,  1010,  1996,  7471, 12103,  2090,  2169,  2391,  1998,
         1996, 26237,  2240,  1012,  2027,  2064,  2022, 10174,  2004,  1996,
         4489,  2090,  1996,  7753,  1038,  1006,  3635,  1007,  1998,  1039,
         1006, 19939,  1007,  1010,  1045,  1012,  1041,  1012,  1010,  2027,
         2024, 10174,  2004,  3635,  1011, 19939,  1012,   102])"
974,1,"['normal distribution', 'residuals', 'control', 'normal', 'distribution', 'model']", Linear Regression with Spreadsheets,seg_197,"residuals are very useful for model control. you can examine, if the residuals follow a normal distribution using some of the methods from chap. 4. this is skipped here.",tensor(1),"tensor([0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101, 21961,  2015,  2024,  2200,  6179,  2005,  2944,  2491,  1012,
         2017,  2064, 11628,  1010,  2065,  1996, 21961,  2015,  3582,  1037,
         3671,  4353,  2478,  2070,  1997,  1996,  4725,  2013, 15775,  2361,
         1012,  1018,  1012,  2023,  2003, 16791,  2182,  1012,   102])"
975,1,"['residuals', 'residuals plotted', 'predicted', 'variables', 'plot']", Linear Regression with Spreadsheets,seg_197,"moreover, we can plot the residuals against the x-variable or other variables. in figs. 7.5 and 7.6 we show diagrams with the residuals plotted against height, and the predicted value of weight (the “forecast”).",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  9308,  1010,  2057,  2064,  5436,  1996, 21961,  2015,  2114,
         1996,  1060,  1011,  8023,  2030,  2060, 10857,  1012,  1999, 20965,
         2015,  1012,  1021,  1012,  1019,  1998,  1021,  1012,  1020,  2057,
         2265, 26309,  2007,  1996, 21961,  2015, 27347,  2114,  4578,  1010,
         1998,  1996, 10173,  3643,  1997,  3635,  1006,  1996,  1523, 19939,
         1524,  1007,  1012,   102])"
976,1,"['variation', 'residuals', 'random variation', 'random']", Linear Regression with Spreadsheets,seg_197,"looking at these graphs, there is no obvious “pattern”. that is exactly what we hope for: the residuals should show only random variation!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  2559,  2012,  2122, 19287,  1010,  2045,  2003,  2053,  5793,
         1523,  5418,  1524,  1012,  2008,  2003,  3599,  2054,  2057,  3246,
         2005,  1024,  1996, 21961,  2015,  2323,  2265,  2069,  6721,  8386,
          999,   102])"
977,1,"['deviation', 'variation', 'observations', 'residuals', 'random variation', 'random', 'standard deviation', 'standard', 'average']", Linear Regression with Spreadsheets,seg_197,"the residuals can also be used to identify extreme observations, for instance boys weighing too much. this could be done by calculating the standard deviation of the residuals; the average of the residuals will always be 0. the standard deviation of the residuals can easily be found to be 8.68; this is the standard deviation for random variation.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  1996, 21961,  2015,  2064,  2036,  2022,  2109,  2000,  6709,
         6034,  9420,  1010,  2005,  6013,  3337, 15243,  2205,  2172,  1012,
         2023,  2071,  2022,  2589,  2011, 20177,  1996,  3115, 24353,  1997,
         1996, 21961,  2015,  1025,  1996,  2779,  1997,  1996, 21961,  2015,
         2097,  2467,  2022,  1014,  1012,  1996,  3115, 24353,  1997,  1996,
        21961,  2015,  2064,  4089,  2022,  2179,  2000,  2022,  1022,  1012,
         6273,  1025,  2023,  2003,  1996,  3115, 24353,  2005,  6721,  8386,
         1012,   102])"
978,1,"['deviation', 'normal distribution', 'table', 'probability', 'fractile', 'variation', 'residuals', 'random variation', 'random', 'standard deviation', 'normal', 'standard', 'distribution']", Linear Regression with Spreadsheets,seg_197,"the 95% fractile of the normal distribution is 1.645. this can be seen from the table of the normal distribution in appendices (chap. 9). thus, if the residuals roughly follow a normal distribution, values larger than 1.645 times the standard deviation for random variation ¼ 14.3 should occur with 5% probability.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  1996,  5345,  1003, 25312,  6593,  9463,  1997,  1996,  3671,
         4353,  2003,  1015,  1012,  4185,  2629,  1012,  2023,  2064,  2022,
         2464,  2013,  1996,  2795,  1997,  1996,  3671,  4353,  1999, 10439,
        10497, 23522,  1006, 15775,  2361,  1012,  1023,  1007,  1012,  2947,
         1010,  2065,  1996, 21961,  2015,  5560,  3582,  1037,  3671,  4353,
         1010,  5300,  3469,  2084,  1015,  1012,  4185,  2629,  2335,  1996,
         3115, 24353,  2005,  6721,  8386,  1091,  2403,  1012,  1017,  2323,
         5258,  2007,  1019,  1003,  9723,  1012,   102])"
979,1,"['slope', 'information', 'intercept', 'loss']", Linear Regression with Spreadsheets,seg_197,"this could also be used to identify future boys weighing too much. a future boy customer giving information about his height and weight could be offered an intensive weight loss program, if his weight exceeds the expected weight by more than 14.3 kg. his expected weight can be calculated using the intercept and slope found above as 27.31 þ 0.515·height.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  2023,  2071,  2036,  2022,  2109,  2000,  6709,  2925,  3337,
        15243,  2205,  2172,  1012,  1037,  2925,  2879,  8013,  3228,  2592,
         2055,  2010,  4578,  1998,  3635,  2071,  2022,  3253,  2019, 11806,
         3635,  3279,  2565,  1010,  2065,  2010,  3635, 23651,  1996,  3517,
         3635,  2011,  2062,  2084,  2403,  1012,  1017,  4705,  1012,  2010,
         3517,  3635,  2064,  2022, 10174,  2478,  1996, 19115,  1998,  9663,
         2179,  2682,  2004,  2676,  1012,  2861,  1101,  1014,  1012,  4868,
         2629,  1087,  4578,  1012,   102])"
980,1,"['slope', 'intercept', 'data']", Linear Regression with Spreadsheets,seg_197,similar calculations can of course be done using data for the girls in the survey. it is not a priori certain that the intercept and slope for the girls will be similar to those for the boys.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 7399, 26237,  2007, 20861, 21030,  3215])","tensor([  101,  2714, 16268,  2064,  1997,  2607,  2022,  2589,  2478,  2951,
         2005,  1996,  3057,  1999,  1996,  5002,  1012,  2009,  2003,  2025,
         1037,  3188,  2072,  3056,  2008,  1996, 19115,  1998,  9663,  2005,
         1996,  3057,  2097,  2022,  2714,  2000,  2216,  2005,  1996,  3337,
         1012,   102])"
981,1,"['regression', 'linear', 'regression line', 'variables', 'average']", Is There a Relationship,seg_199,"we accept from the above that the relationship between height (as x) and weight (as y) can be described by a linear relationship. now we ask the next question: is the regression line different from a simple average, i.e., could the regression line in fact just as well be horizontal? if so, there is no (linear) relationship between the two variables.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0.])","tensor([2003, 2045, 1037, 3276])","tensor([  101,  2057,  5138,  2013,  1996,  2682,  2008,  1996,  3276,  2090,
         4578,  1006,  2004,  1060,  1007,  1998,  3635,  1006,  2004,  1061,
         1007,  2064,  2022,  2649,  2011,  1037,  7399,  3276,  1012,  2085,
         2057,  3198,  1996,  2279,  3160,  1024,  2003,  1996, 26237,  2240,
         2367,  2013,  1037,  3722,  2779,  1010,  1045,  1012,  1041,  1012,
         1010,  2071,  1996, 26237,  2240,  1999,  2755,  2074,  2004,  2092,
         2022,  9876,  1029,  2065,  2061,  1010,  2045,  2003,  2053,  1006,
         7399,  1007,  3276,  2090,  1996,  2048, 10857,  1012,   102])"
982,1,"['coefficient', 'hypothesis', 'correlation coefficient', 'correlation', 'variables']", Is There a Relationship,seg_199,"we therefore formulate the following hypothesis: the line is horizontal, i.e., b ¼ 0. this is the same as: the correlation coefficient between the two variables is 0.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 0., 0.])","tensor([2003, 2045, 1037, 3276])","tensor([  101,  2057,  3568,  5675,  2618,  1996,  2206, 10744,  1024,  1996,
         2240,  2003,  9876,  1010,  1045,  1012,  1041,  1012,  1010,  1038,
         1091,  1014,  1012,  2023,  2003,  1996,  2168,  2004,  1024,  1996,
        16902, 19064,  2090,  1996,  2048, 10857,  2003,  1014,  1012,   102])"
983,1,['hypothesis'], Is There a Relationship,seg_199,we use the general approach from chap. 5 for testing a hypothesis:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([2003, 2045, 1037, 3276])","tensor([  101,  2057,  2224,  1996,  2236,  3921,  2013, 15775,  2361,  1012,
         1019,  2005,  5604,  1037, 10744,  1024,   102])"
984,1,"['probability', 'hypothesis']", Is There a Relationship,seg_199,"1. we assume that this hypothesis is true. 2. calculate the p-value (*), i.e., the probability of getting a more “rare” result.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([2003, 2045, 1037, 3276])","tensor([  101,  1015,  1012,  2057,  7868,  2008,  2023, 10744,  2003,  2995,
         1012,  1016,  1012, 18422,  1996,  1052,  1011,  3643,  1006,  1008,
         1007,  1010,  1045,  1012,  1041,  1012,  1010,  1996,  9723,  1997,
         2893,  1037,  2062,  1523,  4678,  1524,  2765,  1012,   102])"
985,1,['case'], Is There a Relationship,seg_199,it may in this case be shown that one should calculate,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([2003, 2045, 1037, 3276])","tensor([  101,  2009,  2089,  1999,  2023,  2553,  2022,  3491,  2008,  2028,
         2323, 18422,   102])"
986,1,"['slope', 'observations', 'coefficient', 'intercept', 'number of observations', 'correlation coefficient', 'degrees of freedom', 'sample', 'correlation', 'sample size', 'statistics', 'parameters']", Is There a Relationship,seg_199,"here, n ¼ number of observations ¼ 17 in the example and r ¼ correlation coefficient ¼ 0.7645. inserting these values in the formula, we get t ¼ 4.59. this statistics can be described by a t-distribution with n 2 degrees of freedom. we subtract 2 from the sample size instead of 1, due to the fact that we must calculate two parameters, intercept and slope. here n ¼ 17, i.e., there are 15 degrees of freedom.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([2003, 2045, 1037, 3276])","tensor([  101,  2182,  1010,  1050,  1091,  2193,  1997,  9420,  1091,  2459,
         1999,  1996,  2742,  1998,  1054,  1091, 16902, 19064,  1091,  1014,
         1012,  6146, 19961,  1012, 19274,  2075,  2122,  5300,  1999,  1996,
         5675,  1010,  2057,  2131,  1056,  1091,  1018,  1012,  5354,  1012,
         2023,  6747,  2064,  2022,  2649,  2011,  1037,  1056,  1011,  4353,
         2007,  1050,  1016,  5445,  1997,  4071,  1012,  2057,  4942,  6494,
         6593,  1016,  2013,  1996,  7099,  2946,  2612,  1997,  1015,  1010,
         2349,  2000,  1996,  2755,  2008,  2057,  2442, 18422,  2048, 11709,
         1010, 19115,  1998,  9663,  1012,  2182,  1050,  1091,  2459,  1010,
         1045,  1012,  1041,  1012,  1010,  2045,  2024,  2321,  5445,  1997,
         4071,  1012,   102])"
987,1,['hypothesis'], Is There a Relationship,seg_199,"if r ¼ 0, we get t ¼ 0. if r > 0, we get t > 0. if r < 0, we get t < 0. if r is far from 0, t will also be far from 0. the hypothesis r ¼ 0 is rejected for both negative and positive values of r, which are far from 0. in other words: we reject the hypothesis in “both ends” of the t-distribution.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([2003, 2045, 1037, 3276])","tensor([  101,  2065,  1054,  1091,  1014,  1010,  2057,  2131,  1056,  1091,
         1014,  1012,  2065,  1054,  1028,  1014,  1010,  2057,  2131,  1056,
         1028,  1014,  1012,  2065,  1054,  1026,  1014,  1010,  2057,  2131,
         1056,  1026,  1014,  1012,  2065,  1054,  2003,  2521,  2013,  1014,
         1010,  1056,  2097,  2036,  2022,  2521,  2013,  1014,  1012,  1996,
        10744,  1054,  1091,  1014,  2003,  5837,  2005,  2119,  4997,  1998,
         3893,  5300,  1997,  1054,  1010,  2029,  2024,  2521,  2013,  1014,
         1012,  1999,  2060,  2616,  1024,  2057, 15454,  1996, 10744,  1999,
         1523,  2119,  4515,  1524,  1997,  1996,  1056,  1011,  4353,  1012,
          102])"
988,1,"['function', 'density function', 'table', 'probability', 'fractile', 'degrees of freedom', 'distribution']", Is There a Relationship,seg_199,"we can now look up in the table of the t-distribution with 15 degrees of freedom [see appendices (chap. 9)] and find that the 99.5% fractile is 2.947. thus, the probability of getting a larger value than 4.59 is (probably a lot) less than 0.5%. the probability of getting values below 4.59 is also less than 0.5%. in total, the probability of values of t “more extreme” than 4.59 is less than 1%. the following is the density function for a t-distribution with 15 degrees of freedom. it is evident that the value in 4.59 is rather “extreme” in this distribution (fig. 7.7).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2003, 2045, 1037, 3276])","tensor([  101,  2057,  2064,  2085,  2298,  2039,  1999,  1996,  2795,  1997,
         1996,  1056,  1011,  4353,  2007,  2321,  5445,  1997,  4071,  1031,
         2156, 10439, 10497, 23522,  1006, 15775,  2361,  1012,  1023,  1007,
         1033,  1998,  2424,  2008,  1996,  5585,  1012,  1019,  1003, 25312,
         6593,  9463,  2003,  1016,  1012,  6365,  2581,  1012,  2947,  1010,
         1996,  9723,  1997,  2893,  1037,  3469,  3643,  2084,  1018,  1012,
         5354,  2003,  1006,  2763,  1037,  2843,  1007,  2625,  2084,  1014,
         1012,  1019,  1003,  1012,  1996,  9723,  1997,  2893,  5300,  2917,
         1018,  1012,  5354,  2003,  2036,  2625,  2084,  1014,  1012,  1019,
         1003,  1012,  1999,  2561,  1010,  1996,  9723,  1997,  5300,  1997,
         1056,  1523,  2062,  6034,  1524,  2084,  1018,  1012,  5354,  2003,
         2625,  2084,  1015,  1003,  1012,  1996,  2206,  2003,  1996,  4304,
         3853,  2005,  1037,  1056,  1011,  4353,  2007,  2321,  5445,  1997,
         4071,  1012,  2009,  2003, 10358,  2008,  1996,  3643,  1999,  1018,
         1012,  5354,  2003,  2738,  1523,  6034,  1524,  1999,  2023,  4353,
         1006, 20965,  1012,  1021,  1012,  1021,  1007,  1012,   102])"
989,1,"['probability', 'hypothesis']", Is There a Relationship,seg_199,"3. if this probability is small, reject the hypothesis, otherwise accept it.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([2003, 2045, 1037, 3276])","tensor([  101,  1017,  1012,  2065,  2023,  9723,  2003,  2235,  1010, 15454,
         1996, 10744,  1010,  4728,  5138,  2009,  1012,   102])"
990,1,"['confidence interval', 'linear', 'probability', 'interval', 'slope', 'hypothesis', 'statistical', 'confidence']", Is There a Relationship,seg_199,"here we have found a probability of more “extreme” values of t less than 1%, i.e., the hypothesis b ¼ 0 is rejected. we conclude that there is statistical evidence of a (linear) relationship between height and weight. using the excel add-in menu “data analysis”, menu-item “regression”, you can create a 95% confidence interval for the slope. this goes from 0.28 to 0.75. once again, we conclude that the slope is not 0, i.e., the line cannot be horizontal.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2003, 2045, 1037, 3276])","tensor([  101,  2182,  2057,  2031,  2179,  1037,  9723,  1997,  2062,  1523,
         6034,  1524,  5300,  1997,  1056,  2625,  2084,  1015,  1003,  1010,
         1045,  1012,  1041,  1012,  1010,  1996, 10744,  1038,  1091,  1014,
         2003,  5837,  1012,  2057, 16519,  2008,  2045,  2003,  7778,  3350,
         1997,  1037,  1006,  7399,  1007,  3276,  2090,  4578,  1998,  3635,
         1012,  2478,  1996, 24970,  5587,  1011,  1999, 12183,  1523,  2951,
         4106,  1524,  1010, 12183,  1011,  8875,  1523, 26237,  1524,  1010,
         2017,  2064,  3443,  1037,  5345,  1003,  7023, 13483,  2005,  1996,
         9663,  1012,  2023,  3632,  2013,  1014,  1012,  2654,  2000,  1014,
         1012,  4293,  1012,  2320,  2153,  1010,  2057, 16519,  2008,  1996,
         9663,  2003,  2025,  1014,  1010,  1045,  1012,  1041,  1012,  1010,
         1996,  2240,  3685,  2022,  9876,  1012,   102])"
991,1,"['statistical', 'statistics']", Note,seg_201,"the t-test above is exactly the same as the t-test for b ¼ 0, which is presented in many other books on statistics, and calculated in the excel menu “data analysis” under “regression” or in other statistical software.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])",tensor([3602]),"tensor([  101,  1996,  1056,  1011,  3231,  2682,  2003,  3599,  1996,  2168,
         2004,  1996,  1056,  1011,  3231,  2005,  1038,  1091,  1014,  1010,
         2029,  2003,  3591,  1999,  2116,  2060,  2808,  2006,  6747,  1010,
         1998, 10174,  1999,  1996, 24970, 12183,  1523,  2951,  4106,  1524,
         2104,  1523, 26237,  1524,  2030,  1999,  2060,  7778,  4007,  1012,
          102])"
992,1,"['coefficient', 'correlation coefficient', 'correlation', 'statistical']", Note,seg_201,the advantage of the above formula for t-test is that it is much easier to calculate. it requires no special statistical software. it requires only calculating the correlation coefficient.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])",tensor([3602]),"tensor([  101,  1996,  5056,  1997,  1996,  2682,  5675,  2005,  1056,  1011,
         3231,  2003,  2008,  2009,  2003,  2172,  6082,  2000, 18422,  1012,
         2009,  5942,  2053,  2569,  7778,  4007,  1012,  2009,  5942,  2069,
        20177,  1996, 16902, 19064,  1012,   102])"
993,1,"['correlation', 'coefficient', 'correlation coefficient']", Note,seg_201,"the correlation coefficient is easy to calculate in almost all spreadsheets, including open office calc. there are also many advanced calculators, which can calculate the correlation coefficient.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0.])",tensor([3602]),"tensor([  101,  1996, 16902, 19064,  2003,  3733,  2000, 18422,  1999,  2471,
         2035, 20861, 21030,  3215,  1010,  2164,  2330,  2436, 10250,  2278,
         1012,  2045,  2024,  2036,  2116,  3935, 10250, 19879,  6591,  1010,
         2029,  2064, 18422,  1996, 16902, 19064,  1012,   102])"
994,1,['statistical'], Note,seg_201,"if you have excel (add-in menu “data analysis”, menu-item “regression”) or more advanced statistical software, you do not need to calculate the t-statistics manually.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0.])",tensor([3602]),"tensor([  101,  2065,  2017,  2031, 24970,  1006,  5587,  1011,  1999, 12183,
         1523,  2951,  4106,  1524,  1010, 12183,  1011,  8875,  1523, 26237,
         1524,  1007,  2030,  2062,  3935,  7778,  4007,  1010,  2017,  2079,
         2025,  2342,  2000, 18422,  1996,  1056,  1011,  6747, 21118,  1012,
          102])"
995,1,['loss'], Multiple Linear Regression,seg_203,this section can safely be omitted without loss of continuity.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 3674,  7399, 26237])","tensor([  101,  2023,  2930,  2064,  9689,  2022, 16647,  2302,  3279,  1997,
        13717,  1012,   102])"
996,1,"['regression', 'linear', 'linear regression', 'multiple linear regression', 'statistical']", Multiple Linear Regression,seg_203,"we give a brief example of multiple linear regression with two or more x-variables. this can be performed using statistical software or the microsoft excel add-in “data analysis”, item “regression”.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0.])","tensor([ 3674,  7399, 26237])","tensor([  101,  2057,  2507,  1037,  4766,  2742,  1997,  3674,  7399, 26237,
         2007,  2048,  2030,  2062,  1060,  1011, 10857,  1012,  2023,  2064,
         2022,  2864,  2478,  7778,  4007,  2030,  1996,  7513, 24970,  5587,
         1011,  1999,  1523,  2951,  4106,  1524,  1010,  8875,  1523, 26237,
         1524,  1012,   102])"
997,1,"['regression', 'linear regression', 'linear', 'multiple linear regression']", Multiple Linear Regression,seg_203,"we have seen that the height of the boys has a significant influence on their weight. suppose we want to investigate the possible relationship of both height and age of kids with their weight. this is exactly, what multiple linear regression does.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.])","tensor([ 3674,  7399, 26237])","tensor([  101,  2057,  2031,  2464,  2008,  1996,  4578,  1997,  1996,  3337,
         2038,  1037,  3278,  3747,  2006,  2037,  3635,  1012,  6814,  2057,
         2215,  2000,  8556,  1996,  2825,  3276,  1997,  2119,  4578,  1998,
         2287,  1997,  4268,  2007,  2037,  3635,  1012,  2023,  2003,  3599,
         1010,  2054,  3674,  7399, 26237,  2515,  1012,   102])"
998,1,"['data', 'results', 'statistical', 'case']", Multiple Linear Regression,seg_203,"in this example, we use all 30 kids, data values for age and height (as x) and weight (as y). you might want to do this statistical analysis on boys and girls separately first, to ensure that the results are similar; this is actually found to be the case.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0.])","tensor([ 3674,  7399, 26237])","tensor([  101,  1999,  2023,  2742,  1010,  2057,  2224,  2035,  2382,  4268,
         1010,  2951,  5300,  2005,  2287,  1998,  4578,  1006,  2004,  1060,
         1007,  1998,  3635,  1006,  2004,  1061,  1007,  1012,  2017,  2453,
         2215,  2000,  2079,  2023,  7778,  4106,  2006,  3337,  1998,  3057,
        10329,  2034,  1010,  2000,  5676,  2008,  1996,  3463,  2024,  2714,
         1025,  2023,  2003,  2941,  2179,  2000,  2022,  1996,  2553,  1012,
          102])"
999,1,['statistical'], Multiple Linear Regression,seg_203,"if you use the microsoft excel add-in “data analysis” item “regression” with this input, you will get the following output. output from other statistical software packages will look similar (fig. 7.8).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3674,  7399, 26237])","tensor([  101,  2065,  2017,  2224,  1996,  7513, 24970,  5587,  1011,  1999,
         1523,  2951,  4106,  1524,  8875,  1523, 26237,  1524,  2007,  2023,
         7953,  1010,  2017,  2097,  2131,  1996,  2206,  6434,  1012,  6434,
         2013,  2060,  7778,  4007, 14555,  2097,  2298,  2714,  1006, 20965,
         1012,  1021,  1012,  1022,  1007,  1012,   102])"
1000,1,['statistical'], Multiple Linear Regression,seg_203,the p-values of age and height are highlighted. it will be seen that both age and height have a statistical significant relationship with weight (though the p-value of age is just below 0.05).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3674,  7399, 26237])","tensor([  101,  1996,  1052,  1011,  5300,  1997,  2287,  1998,  4578,  2024,
        11548,  1012,  2009,  2097,  2022,  2464,  2008,  2119,  2287,  1998,
         4578,  2031,  1037,  7778,  3278,  3276,  2007,  3635,  1006,  2295,
         1996,  1052,  1011,  3643,  1997,  2287,  2003,  2074,  2917,  1014,
         1012,  5709,  1007,  1012,   102])"
1001,0,[], Multiple Linear Regression,seg_203,our interpretation is that age has an influence on weight in addition to the influence of height.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 3674,  7399, 26237])","tensor([ 101, 2256, 7613, 2003, 2008, 2287, 2038, 2019, 3747, 2006, 3635, 1999,
        2804, 2000, 1996, 3747, 1997, 4578, 1012,  102])"
1002,1,"['observations', 'residuals', 'control', 'model']", Multiple Linear Regression,seg_203,"again, residuals from this model could be used for model control, as well as for identifying extreme observations, e.g., kids weighing too much.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 3674,  7399, 26237])","tensor([  101,  2153,  1010, 21961,  2015,  2013,  2023,  2944,  2071,  2022,
         2109,  2005,  2944,  2491,  1010,  2004,  2092,  2004,  2005, 12151,
         6034,  9420,  1010,  1041,  1012,  1043,  1012,  1010,  4268, 15243,
         2205,  2172,  1012,   102])"
1003,1,['statistics'], Multiple Linear Regression,seg_203,"this topic is a huge topic, and we refer to more advanced books on statistics for more details.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0.])","tensor([ 3674,  7399, 26237])","tensor([ 101, 2023, 8476, 2003, 1037, 4121, 8476, 1010, 1998, 2057, 6523, 2000,
        2062, 3935, 2808, 2006, 6747, 2005, 2062, 4751, 1012,  102])"
1004,1,"['variables', 'coefficient', 'variable']", Final Remarks,seg_205,"in this chapter, we have considered the x-variable as a “random” variable like the y-variable. therefore, it makes sense to talk about a relationship (i.e., the correlation coefficient) between the two variables.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 2345, 12629])","tensor([  101,  1999,  2023,  3127,  1010,  2057,  2031,  2641,  1996,  1060,
         1011,  8023,  2004,  1037,  1523,  6721,  1524,  8023,  2066,  1996,
         1061,  1011,  8023,  1012,  3568,  1010,  2009,  3084,  3168,  2000,
         2831,  2055,  1037,  3276,  1006,  1045,  1012,  1041,  1012,  1010,
         1996, 16902, 19064,  1007,  2090,  1996,  2048, 10857,  1012,   102])"
1005,1,"['vary', 'random', 'variable']", Final Remarks,seg_205,"sometimes, x is a variable considered “given”, such as time. we do not imagine time to vary in a random way. in this situation, there is no sense in talking about “correlation” between x and y.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2345, 12629])","tensor([  101,  2823,  1010,  1060,  2003,  1037,  8023,  2641,  1523,  2445,
         1524,  1010,  2107,  2004,  2051,  1012,  2057,  2079,  2025,  5674,
         2051,  2000,  8137,  1999,  1037,  6721,  2126,  1012,  1999,  2023,
         3663,  1010,  2045,  2003,  2053,  3168,  1999,  3331,  2055,  1523,
        16902,  1524,  2090,  1060,  1998,  1061,  1012,   102])"
1006,1,['test'], Final Remarks,seg_205,"however, all the calculations above can be performed in exactly the same way. the t-test is now interpreted merely as a test that the line is horizontal, i.e., b ¼ 0. it is just the interpretation of the t-test that is different!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0.])","tensor([ 2345, 12629])","tensor([  101,  2174,  1010,  2035,  1996, 16268,  2682,  2064,  2022,  2864,
         1999,  3599,  1996,  2168,  2126,  1012,  1996,  1056,  1011,  3231,
         2003,  2085, 10009,  6414,  2004,  1037,  3231,  2008,  1996,  2240,
         2003,  9876,  1010,  1045,  1012,  1041,  1012,  1010,  1038,  1091,
         1014,  1012,  2009,  2003,  2074,  1996,  7613,  1997,  1996,  1056,
         1011,  3231,  2008,  2003,  2367,   999,   102])"
1007,1,['variables'], Final Remarks,seg_205,"in this chapter we have discussed how to assess the relationship between two variables. in the next chapter, we discuss another important issue: comparing two groups.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2345, 12629])","tensor([  101,  1999,  2023,  3127,  2057,  2031,  6936,  2129,  2000, 14358,
         1996,  3276,  2090,  2048, 10857,  1012,  1999,  1996,  2279,  3127,
         1010,  2057,  6848,  2178,  2590,  3277,  1024, 13599,  2048,  2967,
         1012,   102])"
1008,0,[],Chapter  Comparing Two Groups,seg_207,we may for example want to evaluate the effect of physical exercise on the weight of kids. this could be evaluated in two different ways:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0])","tensor([ 3127, 13599,  2048,  2967])","tensor([  101,  2057,  2089,  2005,  2742,  2215,  2000, 16157,  1996,  3466,
         1997,  3558,  6912,  2006,  1996,  3635,  1997,  4268,  1012,  2023,
         2071,  2022, 16330,  1999,  2048,  2367,  3971,  1024,   102])"
1009,1,"['sample', 'experiment']",Chapter  Comparing Two Groups,seg_207,"– in a planned experiment: we select a group of subjects (i.e., kids) and measure their weight. then they must exercise daily for a period, after which we measure their weight again. we compare the weight before and after the experiment. – in a sample survey: we consider two groups of kids: one group of kids who do not exercise regularly and another group of kids who do. we compare the weight of the kids in the two groups.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 3127, 13599,  2048,  2967])","tensor([  101,  1516,  1999,  1037,  3740,  7551,  1024,  2057,  7276,  1037,
         2177,  1997,  5739,  1006,  1045,  1012,  1041,  1012,  1010,  4268,
         1007,  1998,  5468,  2037,  3635,  1012,  2059,  2027,  2442,  6912,
         3679,  2005,  1037,  2558,  1010,  2044,  2029,  2057,  5468,  2037,
         3635,  2153,  1012,  2057, 12826,  1996,  3635,  2077,  1998,  2044,
         1996,  7551,  1012,  1516,  1999,  1037,  7099,  5002,  1024,  2057,
         5136,  2048,  2967,  1997,  4268,  1024,  2028,  2177,  1997,  4268,
         2040,  2079,  2025,  6912,  5570,  1998,  2178,  2177,  1997,  4268,
         2040,  2079,  1012,  2057, 12826,  1996,  3635,  1997,  1996,  4268,
         1999,  1996,  2048,  2967,  1012,   102])"
1010,1,"['sets', 'data']",Chapter  Comparing Two Groups,seg_207,"these two approaches, which are introduced in this chapter, illustrate the two main techniques of comparing the two sets of data.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 1., 0., 0.])","tensor([ 3127, 13599,  2048,  2967])","tensor([  101,  2122,  2048,  8107,  1010,  2029,  2024,  3107,  1999,  2023,
         3127,  1010, 19141,  1996,  2048,  2364,  5461,  1997, 13599,  1996,
         2048,  4520,  1997,  2951,  1012,   102])"
1011,1,['statistical'],Chapter  Comparing Two Groups,seg_207,"the techniques are reviewed on the basis of specific examples. finally, we mention some extensions of these statistical techniques.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0.])","tensor([ 3127, 13599,  2048,  2967])","tensor([  101,  1996,  5461,  2024,  8182,  2006,  1996,  3978,  1997,  3563,
         4973,  1012,  2633,  1010,  2057,  5254,  2070, 14305,  1997,  2122,
         7778,  5461,  1012,   102])"
1012,1,['experiment'], Example,seg_211,"the girls among kids in the fitness club survey are selected for an experiment, where they must exercise at least 1 h daily over a period of 4 weeks. otherwise, they do not change their lifestyle.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  3057,  2426,  4268,  1999,  1996, 10516,  2252,  5002,
         2024,  3479,  2005,  2019,  7551,  1010,  2073,  2027,  2442,  6912,
         2012,  2560,  1015,  1044,  3679,  2058,  1037,  2558,  1997,  1018,
         3134,  1012,  4728,  1010,  2027,  2079,  2025,  2689,  2037,  9580,
         1012,   102])"
1013,1,"['loss', 'experiment']", Example,seg_211,the purpose of this experiment is to investigate the potential for intensive weight loss programs among the girl customers.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  3800,  1997,  2023,  7551,  2003,  2000,  8556,  1996,
         4022,  2005, 11806,  3635,  3279,  3454,  2426,  1996,  2611,  6304,
         1012,   102])"
1014,1,"['table', 'experiment', 'data']", Example,seg_211,"in this context, we need more precise numbers than can be obtained from asking the kids about their weight. therefore, their weight before and after the experiment is measured; see data in table 8.1.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
        0., 1., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1999,  2023,  6123,  1010,  2057,  2342,  2062, 10480,  3616,
         2084,  2064,  2022,  4663,  2013,  4851,  1996,  4268,  2055,  2037,
         3635,  1012,  3568,  1010,  2037,  3635,  2077,  1998,  2044,  1996,
         7551,  2003,  7594,  1025,  2156,  2951,  1999,  2795,  1022,  1012,
         1015,  1012,   102])"
1015,1,['experiment'], Example,seg_211,difference before-after experiment 6,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0.])",tensor([2742]),"tensor([ 101, 4489, 2077, 1011, 2044, 7551, 1020,  102])"
1016,1,"['cases', 'loss', 'table', 'histogram', 'experiment']", Example,seg_211,"at first glance, the difference in weight before and after the experiment seems small. the table does, however, also provide the difference in weight for each kid before and after the experiment. it is seen that inmost cases (9 out of 13) there is a smallweight loss, but there are also some kids who weigh the same or even slightly more as before the experiment. this is illustrated in fig. 8.1 showing the histogram of differences.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2012,  2034,  6054,  1010,  1996,  4489,  1999,  3635,  2077,
         1998,  2044,  1996,  7551,  3849,  2235,  1012,  1996,  2795,  2515,
         1010,  2174,  1010,  2036,  3073,  1996,  4489,  1999,  3635,  2005,
         2169,  4845,  2077,  1998,  2044,  1996,  7551,  1012,  2009,  2003,
         2464,  2008,  1999, 11800,  3572,  1006,  1023,  2041,  1997,  2410,
         1007,  2045,  2003,  1037,  2235, 11179,  3279,  1010,  2021,  2045,
         2024,  2036,  2070,  4268,  2040, 17042,  1996,  2168,  2030,  2130,
         3621,  2062,  2004,  2077,  1996,  7551,  1012,  2023,  2003,  7203,
         1999, 20965,  1012,  1022,  1012,  1015,  4760,  1996,  2010,  3406,
        13113,  1997,  5966,  1012,   102])"
1017,1,['distribution'], Example,seg_211,it is evident from the graph that the “center” of the distribution is to the right of 0.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2009,  2003, 10358,  2013,  1996, 10629,  2008,  1996,  1523,
         2415,  1524,  1997,  1996,  4353,  2003,  2000,  1996,  2157,  1997,
         1014,  1012,   102])"
1018,1,['data'], Description,seg_213,we have a number of pairs of data values. the two data values in a pair belong to two different groups. we are interested in whether there is a difference between the two groups.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])",tensor([6412]),"tensor([ 101, 2057, 2031, 1037, 2193, 1997, 7689, 1997, 2951, 5300, 1012, 1996,
        2048, 2951, 5300, 1999, 1037, 3940, 7141, 2000, 2048, 2367, 2967, 1012,
        2057, 2024, 4699, 1999, 3251, 2045, 2003, 1037, 4489, 2090, 1996, 2048,
        2967, 1012,  102])"
1019,1,"['data', 'experiments', 'statistical']", Description,seg_213,"the most common application of this technique is in statistical analysis of data from planned experiments. the situation could be the following: we have n individuals,",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([6412]),"tensor([ 101, 1996, 2087, 2691, 4646, 1997, 2023, 6028, 2003, 1999, 7778, 4106,
        1997, 2951, 2013, 3740, 7885, 1012, 1996, 3663, 2071, 2022, 1996, 2206,
        1024, 2057, 2031, 1050, 3633, 1010,  102])"
1020,1,['average'], Description,seg_213,each of whom has been subjected to two “treatments”. we want to examine whether there is a difference between the two treatments and possibly find the average difference.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])",tensor([6412]),"tensor([  101,  2169,  1997,  3183,  2038,  2042, 13532,  2000,  2048,  1523,
        13441,  1524,  1012,  2057,  2215,  2000, 11628,  3251,  2045,  2003,
         1037,  4489,  2090,  1996,  2048, 13441,  1998,  4298,  2424,  1996,
         2779,  4489,  1012,   102])"
1021,1,['matched pairs'], Description,seg_213,this situation is referred to as matched pairs.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])",tensor([6412]),"tensor([  101,  2023,  3663,  2003,  3615,  2000,  2004, 10349,  7689,  1012,
          102])"
1022,1,"['mean', 'hypothesis']", Calculation,seg_215,"the hypothesis is that the mean of the differences is 0, i.e., there is no difference between the two groups.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([17208]),"tensor([  101,  1996, 10744,  2003,  2008,  1996,  2812,  1997,  1996,  5966,
         2003,  1014,  1010,  1045,  1012,  1041,  1012,  1010,  2045,  2003,
         2053,  4489,  2090,  1996,  2048,  2967,  1012,   102])"
1023,0,[], Calculation,seg_215,we use the general approach:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0])",tensor([17208]),"tensor([ 101, 2057, 2224, 1996, 2236, 3921, 1024,  102])"
1024,1,"['probability', 'hypothesis']", Calculation,seg_215,"1. we assume that the hypothesis is true. 2. calculate the p value, i.e., the probability of getting a more “rare” result.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([17208]),"tensor([  101,  1015,  1012,  2057,  7868,  2008,  1996, 10744,  2003,  2995,
         1012,  1016,  1012, 18422,  1996,  1052,  3643,  1010,  1045,  1012,
         1041,  1012,  1010,  1996,  9723,  1997,  2893,  1037,  2062,  1523,
         4678,  1524,  2765,  1012,   102])"
1025,1,"['mean', 'estimated', 'average']", Calculation,seg_215,"the mean difference is estimated by the average of the differences, which is calculated to be:",tensor(1),"tensor([0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])",tensor([17208]),"tensor([  101,  1996,  2812,  4489,  2003,  4358,  2011,  1996,  2779,  1997,
         1996,  5966,  1010,  2029,  2003, 10174,  2000,  2022,  1024,   102])"
1026,1,"['deviation', 'estimate', 'standard deviation', 'standard', 'standard error', 'average', 'error']", Calculation,seg_215,"we also calculate the standard deviation of the differences to be s ¼ 1.19. it is natural to relate the average difference to s/÷n, the estimate of the standard error (see chap. 4).",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([17208]),"tensor([  101,  2057,  2036, 18422,  1996,  3115, 24353,  1997,  1996,  5966,
         2000,  2022,  1055,  1091,  1015,  1012,  2539,  1012,  2009,  2003,
         3019,  2000, 14396,  1996,  2779,  4489,  2000,  1055,  1013,  1099,
         2078,  1010,  1996, 10197,  1997,  1996,  3115,  7561,  1006,  2156,
        15775,  2361,  1012,  1018,  1007,  1012,   102])"
1027,0,[], Calculation,seg_215,we therefore calculate:,tensor(0),"tensor([0, 0, 0, 0, 0, 0])",tensor([17208]),"tensor([  101,  2057,  3568, 18422,  1024,   102])"
1028,1,"['mean', 'paired', 'interval', 'degrees of freedom', 'statistic', 'data', 'case']", Calculation,seg_215,"this is called the paired t-test. in the example we get t ¼ 2.803. this statistic follows a t-distribution. the number of degrees of freedom is n 1 because we have n differences (see chap. 4, section “confidence interval for the mean in case of a small sample”). once we have calculated the differences, the original data are unimportant, i.e., it is the number of differences that count. in this example, there are 13 differences, i.e., the number of degrees of freedom is 12.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])",tensor([17208]),"tensor([  101,  2023,  2003,  2170,  1996, 12739,  1056,  1011,  3231,  1012,
         1999,  1996,  2742,  2057,  2131,  1056,  1091,  1016,  1012,  3770,
         2509,  1012,  2023, 28093,  6553,  4076,  1037,  1056,  1011,  4353,
         1012,  1996,  2193,  1997,  5445,  1997,  4071,  2003,  1050,  1015,
         2138,  2057,  2031,  1050,  5966,  1006,  2156, 15775,  2361,  1012,
         1018,  1010,  2930,  1523,  7023, 13483,  2005,  1996,  2812,  1999,
         2553,  1997,  1037,  2235,  7099,  1524,  1007,  1012,  2320,  2057,
         2031, 10174,  1996,  5966,  1010,  1996,  2434,  2951,  2024,  4895,
         5714,  6442,  4630,  1010,  1045,  1012,  1041,  1012,  1010,  2009,
         2003,  1996,  2193,  1997,  5966,  2008,  4175,  1012,  1999,  2023,
         2742,  1010,  2045,  2024,  2410,  5966,  1010,  1045,  1012,  1041,
         1012,  1010,  1996,  2193,  1997,  5445,  1997,  4071,  2003,  2260,
         1012,   102])"
1029,1,"['average', 'hypothesis']", Calculation,seg_215,"if all differences are 0, we get t ¼ 0. values of t close to 0 are “good” for our hypothesis. values of t far from 0 are “bad” for the hypothesis. if t is far from 0, we therefore reject the hypothesis. this corresponds to an average difference far from 0.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])",tensor([17208]),"tensor([  101,  2065,  2035,  5966,  2024,  1014,  1010,  2057,  2131,  1056,
         1091,  1014,  1012,  5300,  1997,  1056,  2485,  2000,  1014,  2024,
         1523,  2204,  1524,  2005,  2256, 10744,  1012,  5300,  1997,  1056,
         2521,  2013,  1014,  2024,  1523,  2919,  1524,  2005,  1996, 10744,
         1012,  2065,  1056,  2003,  2521,  2013,  1014,  1010,  2057,  3568,
        15454,  1996, 10744,  1012,  2023, 14788,  2000,  2019,  2779,  4489,
         2521,  2013,  1014,  1012,   102])"
1030,1,['table'], Calculation,seg_215,"from the table at the end of the book, we get:",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([17208]),"tensor([ 101, 2013, 1996, 2795, 2012, 1996, 2203, 1997, 1996, 2338, 1010, 2057,
        2131, 1024,  102])"
1031,1,"['fractile', 'degrees of freedom']", Calculation,seg_215,– the 99% fractile of a t-distribution with 12 degrees of freedom is 2.681. – the 99.5% fractile in a t-distribution with 12 degrees of freedom is 3.055.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.])",tensor([17208]),"tensor([  101,  1516,  1996,  5585,  1003, 25312,  6593,  9463,  1997,  1037,
         1056,  1011,  4353,  2007,  2260,  5445,  1997,  4071,  2003,  1016,
         1012,  6273,  2487,  1012,  1516,  1996,  5585,  1012,  1019,  1003,
        25312,  6593,  9463,  1999,  1037,  1056,  1011,  4353,  2007,  2260,
         5445,  1997,  4071,  2003,  1017,  1012,  5709,  2629,  1012,   102])"
1032,1,"['probability', 'degrees of freedom', 'hypothesis', 'distribution']", Calculation,seg_215,"the probability of getting a larger value of t is thus between 0.5% and 1%. normally, we add the probability of getting a value of t, which is at least as “far out” to the opposite side. this is just as “bad” for the hypothesis! the probability of a rarer result is thus between 1% and 2%. the graph in fig. 8.2 shows a t-distribution with 12 degrees of freedom. it is evident that the value 2.803 is quite “far out” in the distribution. 3. if this probability is small, we reject the hypothesis.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0.])",tensor([17208]),"tensor([  101,  1996,  9723,  1997,  2893,  1037,  3469,  3643,  1997,  1056,
         2003,  2947,  2090,  1014,  1012,  1019,  1003,  1998,  1015,  1003,
         1012,  5373,  1010,  2057,  5587,  1996,  9723,  1997,  2893,  1037,
         3643,  1997,  1056,  1010,  2029,  2003,  2012,  2560,  2004,  1523,
         2521,  2041,  1524,  2000,  1996,  4500,  2217,  1012,  2023,  2003,
         2074,  2004,  1523,  2919,  1524,  2005,  1996, 10744,   999,  1996,
         9723,  1997,  1037,  4678,  2099,  2765,  2003,  2947,  2090,  1015,
         1003,  1998,  1016,  1003,  1012,  1996, 10629,  1999, 20965,  1012,
         1022,  1012,  1016,  3065,  1037,  1056,  1011,  4353,  2007,  2260,
         5445,  1997,  4071,  1012,  2009,  2003, 10358,  2008,  1996,  3643,
         1016,  1012,  3770,  2509,  2003,  3243,  1523,  2521,  2041,  1524,
         1999,  1996,  4353,  1012,  1017,  1012,  2065,  2023,  9723,  2003,
         2235,  1010,  2057, 15454,  1996, 10744,  1012,   102])"
1033,1,"['loss', 'mean', 'confidence interval', 'probability', 'interval', 'hypothesis', 'experiment', 'statistical', 'confidence']", Calculation,seg_215,"as the probability is less than 2%, we reject the hypothesis. this means that there is statistical evidence that the mean difference in weight before and after the experiment is not 0. in this example, the mean difference is positive, i.e., there is a weight loss. now we have demonstrated that there is indeed a difference in weight. the question that follows is: how big is the mean difference? this question can be answered by calculating a 95% confidence interval for the mean difference; see chap. 4 about this. the confidence interval is calculated as follows:",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0.])",tensor([17208]),"tensor([  101,  2004,  1996,  9723,  2003,  2625,  2084,  1016,  1003,  1010,
         2057, 15454,  1996, 10744,  1012,  2023,  2965,  2008,  2045,  2003,
         7778,  3350,  2008,  1996,  2812,  4489,  1999,  3635,  2077,  1998,
         2044,  1996,  7551,  2003,  2025,  1014,  1012,  1999,  2023,  2742,
         1010,  1996,  2812,  4489,  2003,  3893,  1010,  1045,  1012,  1041,
         1012,  1010,  2045,  2003,  1037,  3635,  3279,  1012,  2085,  2057,
         2031,  7645,  2008,  2045,  2003,  5262,  1037,  4489,  1999,  3635,
         1012,  1996,  3160,  2008,  4076,  2003,  1024,  2129,  2502,  2003,
         1996,  2812,  4489,  1029,  2023,  3160,  2064,  2022,  4660,  2011,
        20177,  1037,  5345,  1003,  7023, 13483,  2005,  1996,  2812,  4489,
         1025,  2156, 15775,  2361,  1012,  1018,  2055,  2023,  1012,  1996,
         7023, 13483,  2003, 10174,  2004,  4076,  1024,   102])"
1034,1,"['mean', 'confidence interval', 'table', 'probability', 'fractile', 'interval', 'degrees of freedom', 'confidence']", Calculation,seg_215,"for t we use the 97.5% fractile in a t-distribution with n 1 degrees of freedom. this gives us a 95% confidence interval, i.e., with probability 95% the interval contains the true value of the mean difference. in the table with the t-distribution at the end of the book we find the 97.5% fractile in a t-distribution with 12 degrees of freedom as 2.179. if we insert this in the formula, we get the confidence interval 0.923 0.718, i.e., with probability 95% the mean difference is somewhere between 0.205 and 1.641.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([17208]),"tensor([  101,  2005,  1056,  2057,  2224,  1996,  5989,  1012,  1019,  1003,
        25312,  6593,  9463,  1999,  1037,  1056,  1011,  4353,  2007,  1050,
         1015,  5445,  1997,  4071,  1012,  2023,  3957,  2149,  1037,  5345,
         1003,  7023, 13483,  1010,  1045,  1012,  1041,  1012,  1010,  2007,
         9723,  5345,  1003,  1996, 13483,  3397,  1996,  2995,  3643,  1997,
         1996,  2812,  4489,  1012,  1999,  1996,  2795,  2007,  1996,  1056,
         1011,  4353,  2012,  1996,  2203,  1997,  1996,  2338,  2057,  2424,
         1996,  5989,  1012,  1019,  1003, 25312,  6593,  9463,  1999,  1037,
         1056,  1011,  4353,  2007,  2260,  5445,  1997,  4071,  2004,  1016,
         1012, 20311,  1012,  2065,  2057, 19274,  2023,  1999,  1996,  5675,
         1010,  2057,  2131,  1996,  7023, 13483,  1014,  1012,  6227,  2509,
         1014,  1012,  6390,  2620,  1010,  1045,  1012,  1041,  1012,  1010,
         2007,  9723,  5345,  1003,  1996,  2812,  4489,  2003,  4873,  2090,
         1014,  1012, 16327,  1998,  1015,  1012,  4185,  2487,  1012,   102])"
1035,1,"['function', 'probability']", Spreadsheets,seg_217,"with a spreadsheet, we can directly calculate the p value, i.e., the probability of a value of t rarer than the above calculated value 2.803, using the ttest function:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([20861, 21030,  3215])","tensor([  101,  2007,  1037, 20861, 21030,  2102,  1010,  2057,  2064,  3495,
        18422,  1996,  1052,  3643,  1010,  1045,  1012,  1041,  1012,  1010,
         1996,  9723,  1997,  1037,  3643,  1997,  1056,  4678,  2099,  2084,
         1996,  2682, 10174,  3643,  1016,  1012,  3770,  2509,  1010,  2478,
         1996, 23746,  4355,  3853,  1024,   102])"
1036,1,['tails'], Spreadsheets,seg_217,ttest (data1; data2; tails; type),tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([20861, 21030,  3215])","tensor([  101, 23746,  4355,  1006,  2951,  2487,  1025,  2951,  2475,  1025,
        17448,  1025,  2828,  1007,   102])"
1037,1,"['function', 'table', 'data']", Spreadsheets,seg_217,"calculates the p value of a t-test (table 8.2). these data could be in cells a2:a14 (“before” data), and b2:b14 (“after” data). we now use the function as follows:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0.])","tensor([20861, 21030,  3215])","tensor([  101, 18422,  2015,  1996,  1052,  3643,  1997,  1037,  1056,  1011,
         3231,  1006,  2795,  1022,  1012,  1016,  1007,  1012,  2122,  2951,
         2071,  2022,  1999,  4442, 22441,  1024, 17350,  2549,  1006,  1523,
         2077,  1524,  2951,  1007,  1010,  1998,  1038,  2475,  1024, 29491,
         2549,  1006,  1523,  2044,  1524,  2951,  1007,  1012,  2057,  2085,
         2224,  1996,  3853,  2004,  4076,  1024,   102])"
1038,1,"['matched pairs', 'tails', 'hypothesis', 'distribution']", Spreadsheets,seg_217,"– tails ¼ 2, since we reject the hypothesis in both sides of the distribution. – type ¼ 1, since we perform a t-test for matched pairs.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([20861, 21030,  3215])","tensor([  101,  1516, 17448,  1091,  1016,  1010,  2144,  2057, 15454,  1996,
        10744,  1999,  2119,  3903,  1997,  1996,  4353,  1012,  1516,  2828,
         1091,  1015,  1010,  2144,  2057,  4685,  1037,  1056,  1011,  3231,
         2005, 10349,  7689,  1012,   102])"
1039,1,"['probability', 'test']", Spreadsheets,seg_217,"the result is 0.016 ¼ 1.6%. the probability of a rarer value of t is thus app. 1.6%. by comparison, we found out in the above test that the p value is between 1% and 2%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([20861, 21030,  3215])","tensor([  101,  1996,  2765,  2003,  1014,  1012,  5890,  2575,  1091,  1015,
         1012,  1020,  1003,  1012,  1996,  9723,  1997,  1037,  4678,  2099,
         3643,  1997,  1056,  2003,  2947, 10439,  1012,  1015,  1012,  1020,
         1003,  1012,  2011,  7831,  1010,  2057,  2179,  2041,  1999,  1996,
         2682,  3231,  2008,  1996,  1052,  3643,  2003,  2090,  1015,  1003,
         1998,  1016,  1003,  1012,   102])"
1040,1,['function'], Spreadsheets,seg_217,"when using the ttest function, we do not need to calculate the differences! we get the p value directly and can compare this with 0.05.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([20861, 21030,  3215])","tensor([  101,  2043,  2478,  1996, 23746,  4355,  3853,  1010,  2057,  2079,
         2025,  2342,  2000, 18422,  1996,  5966,   999,  2057,  2131,  1996,
         1052,  3643,  3495,  1998,  2064, 12826,  2023,  2007,  1014,  1012,
         5709,  1012,   102])"
1041,1,['loss'], Example,seg_221,we want to examine whether there is a difference between the physical fitness of boys and girls in the fitness club survey. the purpose is to investigate whether potential boy customers and potential girl customers should be addressed in the same way when recruiting new customers for intensive weight loss programs.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0.])",tensor([2742]),"tensor([  101,  2057,  2215,  2000, 11628,  3251,  2045,  2003,  1037,  4489,
         2090,  1996,  3558, 10516,  1997,  3337,  1998,  3057,  1999,  1996,
        10516,  2252,  5002,  1012,  1996,  3800,  2003,  2000,  8556,  3251,
         4022,  2879,  6304,  1998,  4022,  2611,  6304,  2323,  2022,  8280,
         1999,  1996,  2168,  2126,  2043, 14357,  2047,  6304,  2005, 11806,
         3635,  3279,  3454,  1012,   102])"
1042,1,['parameters'], Example,seg_221,"several different parameters are relevant in this context: one could for example compare their weight. however, this would not be appropriate, as a difference may be due to differences in height and/or age.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2195,  2367, 11709,  2024,  7882,  1999,  2023,  6123,  1024,
         2028,  2071,  2005,  2742, 12826,  2037,  3635,  1012,  2174,  1010,
         2023,  2052,  2025,  2022,  6413,  1010,  2004,  1037,  4489,  2089,
         2022,  2349,  2000,  5966,  1999,  4578,  1998,  1013,  2030,  2287,
         1012,   102])"
1043,0,[], Example,seg_221,"therefore, we calculate their body mass index (bmi), i.e.,",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])",tensor([2742]),"tensor([  101,  3568,  1010,  2057, 18422,  2037,  2303,  3742,  5950,  1006,
         1038,  4328,  1007,  1010,  1045,  1012,  1041,  1012,  1010,   102])"
1044,0,[], Example,seg_221,"this is an internationally accepted measure. for instance, a person who is 2.00 m tall and weighs 100 kg has a bmi of 100/22 ¼ 100/4 ¼ 25.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])",tensor([2742]),"tensor([  101,  2023,  2003,  2019,  7587,  3970,  5468,  1012,  2005,  6013,
         1010,  1037,  2711,  2040,  2003,  1016,  1012,  4002,  1049,  4206,
         1998, 21094,  2531,  4705,  2038,  1037,  1038,  4328,  1997,  2531,
         1013,  2570,  1091,  2531,  1013,  1018,  1091,  2423,  1012,   102])"
1045,1,['normal'], Example,seg_221,– a bmi below 20 is considered to be under normal. – a bmi of 20–25 is considered normal. – a bmi of 25–30 is considered overweight. – a bmi over 30 is considered heavily overweight.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1516,  1037,  1038,  4328,  2917,  2322,  2003,  2641,  2000,
         2022,  2104,  3671,  1012,  1516,  1037,  1038,  4328,  1997,  2322,
         1516,  2423,  2003,  2641,  3671,  1012,  1516,  1037,  1038,  4328,
         1997,  2423,  1516,  2382,  2003,  2641,  2058, 11179,  1012,  1516,
         1037,  1038,  4328,  2058,  2382,  2003,  2641,  4600,  2058, 11179,
         1012,   102])"
1046,1,"['data', 'table']", Example,seg_221,the values here are shown to one decimal place for all kids. we used the questionnaire data on height and weight; see the table with data at the end of the book (table 8.3).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  1996,  5300,  2182,  2024,  3491,  2000,  2028, 26066,  2173,
         2005,  2035,  4268,  1012,  2057,  2109,  1996,  3160, 20589,  2951,
         2006,  4578,  1998,  3635,  1025,  2156,  1996,  2795,  2007,  2951,
         2012,  1996,  2203,  1997,  1996,  2338,  1006,  2795,  1022,  1012,
         1017,  1007,  1012,   102])"
1047,1,"['histogram', 'statistical test', 'distribution', 'statistical', 'test']", Example,seg_221,"figure 8.3 is a combined histogram of bmi for boys and girls. immediately, there seems to be no major differences in the distribution of bmi for girls and boys. we want to confirm this using a statistical test.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])",tensor([2742]),"tensor([  101,  3275,  1022,  1012,  1017,  2003,  1037,  4117,  2010,  3406,
        13113,  1997,  1038,  4328,  2005,  3337,  1998,  3057,  1012,  3202,
         1010,  2045,  3849,  2000,  2022,  2053,  2350,  5966,  1999,  1996,
         4353,  1997,  1038,  4328,  2005,  3057,  1998,  3337,  1012,  2057,
         2215,  2000, 12210,  2023,  2478,  1037,  7778,  3231,  1012,   102])"
1048,1,"['deviation', 'table', 'standard deviation', 'standard', 'average']", Example,seg_221,"we start by calculating the average and standard deviation in each group, i.e., for girls and boys separately (table 8.4).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2057,  2707,  2011, 20177,  1996,  2779,  1998,  3115, 24353,
         1999,  2169,  2177,  1010,  1045,  1012,  1041,  1012,  1010,  2005,
         3057,  1998,  3337, 10329,  1006,  2795,  1022,  1012,  1018,  1007,
         1012,   102])"
1049,1,['average'], Example,seg_221,"we might, e.g., name girls “group 1” (i ¼ 1) and boys “group 2” (i ¼ 2). the average difference (i.e., the difference between the averages) is 22.22 21.34 ¼ 0.88.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])",tensor([2742]),"tensor([  101,  2057,  2453,  1010,  1041,  1012,  1043,  1012,  1010,  2171,
         3057,  1523,  2177,  1015,  1524,  1006,  1045,  1091,  1015,  1007,
         1998,  3337,  1523,  2177,  1016,  1524,  1006,  1045,  1091,  1016,
         1007,  1012,  1996,  2779,  4489,  1006,  1045,  1012,  1041,  1012,
         1010,  1996,  4489,  2090,  1996, 20185,  1007,  2003,  2570,  1012,
         2570,  2538,  1012,  4090,  1091,  1014,  1012,  6070,  1012,   102])"
1050,1,"['data', 'sample', 'experiments', 'statistical']", Description,seg_223,this technique can be used for statistical analysis of data from sample surveys and planned experiments.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,
        0.])",tensor([6412]),"tensor([  101,  2023,  6028,  2064,  2022,  2109,  2005,  7778,  4106,  1997,
         2951,  2013,  7099, 12265,  1998,  3740,  7885,  1012,   102])"
1051,1,"['mean', 'estimate', 'data']", Description,seg_223,"we have two groups of data values. we are interested in whether there is a difference between the mean of the two groups (and if there is, we want to estimate the mean difference).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        1., 0., 0., 0., 0.])",tensor([6412]),"tensor([  101,  2057,  2031,  2048,  2967,  1997,  2951,  5300,  1012,  2057,
         2024,  4699,  1999,  3251,  2045,  2003,  1037,  4489,  2090,  1996,
         2812,  1997,  1996,  2048,  2967,  1006,  1998,  2065,  2045,  2003,
         1010,  2057,  2215,  2000, 10197,  1996,  2812,  4489,  1007,  1012,
          102])"
1052,1,"['sample', 'experiment', 'population']", Description,seg_223,"the two groups may be two different groups of individuals in a population that we want to compare using a sample survey. or they might be two groups of individuals, which have been subject to two different treatments in a planned experiment.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])",tensor([6412]),"tensor([  101,  1996,  2048,  2967,  2089,  2022,  2048,  2367,  2967,  1997,
         3633,  1999,  1037,  2313,  2008,  2057,  2215,  2000, 12826,  2478,
         1037,  7099,  5002,  1012,  2030,  2027,  2453,  2022,  2048,  2967,
         1997,  3633,  1010,  2029,  2031,  2042,  3395,  2000,  2048,  2367,
        13441,  1999,  1037,  3740,  7551,  1012,   102])"
1053,1,"['mean', 'hypothesis']", Calculation,seg_225,"the hypothesis is that the mean difference is 0, i.e., that the two means are identical.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])",tensor([17208]),"tensor([  101,  1996, 10744,  2003,  2008,  1996,  2812,  4489,  2003,  1014,
         1010,  1045,  1012,  1041,  1012,  1010,  2008,  1996,  2048,  2965,
         2024,  7235,  1012,   102])"
1054,1,"['probability', 'hypothesis']", Calculation,seg_225,"1. we assume that the hypothesis is true. 2. we calculate the p value, i.e., the probability of getting a more “rare” result.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([17208]),"tensor([  101,  1015,  1012,  2057,  7868,  2008,  1996, 10744,  2003,  2995,
         1012,  1016,  1012,  2057, 18422,  1996,  1052,  3643,  1010,  1045,
         1012,  1041,  1012,  1010,  1996,  9723,  1997,  2893,  1037,  2062,
         1523,  4678,  1524,  2765,  1012,   102])"
1055,0,[], Calculation,seg_225,note: calculation of the p value is easy in a spreadsheet; see the next section. we now calculate the following:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])",tensor([17208]),"tensor([  101,  3602,  1024, 17208,  1997,  1996,  1052,  3643,  2003,  3733,
         1999,  1037, 20861, 21030,  2102,  1025,  2156,  1996,  2279,  2930,
         1012,  2057,  2085, 18422,  1996,  2206,  1024,   102])"
1056,1,"['deviation', 'contrast', 'data', 'fractile', 'variances', 'degrees of freedom', 'standard deviation', 'standard', 'samples', 'average', 'statistic']", Calculation,seg_225,"this statistic contains the average, standard deviation and number of data values for each group. this is called a t-test for two samples with unequal variances. this t-test allows the variances in the two groups to be unequal, in contrast to a t-test for two samples with equal variances (see later). in the example we get t ¼ 0.50. this value of t is to be compared to a fractile in a t-distribution. so what is the number of degrees of freedom?",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0.])",tensor([17208]),"tensor([  101,  2023, 28093,  6553,  3397,  1996,  2779,  1010,  3115, 24353,
         1998,  2193,  1997,  2951,  5300,  2005,  2169,  2177,  1012,  2023,
         2003,  2170,  1037,  1056,  1011,  3231,  2005,  2048,  8168,  2007,
        16655, 26426, 23284,  2015,  1012,  2023,  1056,  1011,  3231,  4473,
         1996, 23284,  2015,  1999,  1996,  2048,  2967,  2000,  2022, 16655,
        26426,  1010,  1999,  5688,  2000,  1037,  1056,  1011,  3231,  2005,
         2048,  8168,  2007,  5020, 23284,  2015,  1006,  2156,  2101,  1007,
         1012,  1999,  1996,  2742,  2057,  2131,  1056,  1091,  1014,  1012,
         2753,  1012,  2023,  3643,  1997,  1056,  2003,  2000,  2022,  4102,
         2000,  1037, 25312,  6593,  9463,  1999,  1037,  1056,  1011,  4353,
         1012,  2061,  2054,  2003,  1996,  2193,  1997,  5445,  1997,  4071,
         1029,   102])"
1057,1,"['deviation', 'degrees of freedom', 'standard deviation', 'standard', 'data', 'case']", Calculation,seg_225,– the number of degrees of freedom can never be smaller than the number of degrees of freedom in the smallest group. – the number of degrees of freedom can never be larger than the sum of the number of degrees of freedom in each group. (this will be the case when both the standard deviation and number of data values are identical in both groups.),tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])",tensor([17208]),"tensor([  101,  1516,  1996,  2193,  1997,  5445,  1997,  4071,  2064,  2196,
         2022,  3760,  2084,  1996,  2193,  1997,  5445,  1997,  4071,  1999,
         1996, 10479,  2177,  1012,  1516,  1996,  2193,  1997,  5445,  1997,
         4071,  2064,  2196,  2022,  3469,  2084,  1996,  7680,  1997,  1996,
         2193,  1997,  5445,  1997,  4071,  1999,  2169,  2177,  1012,  1006,
         2023,  2097,  2022,  1996,  2553,  2043,  2119,  1996,  3115, 24353,
         1998,  2193,  1997,  2951,  5300,  2024,  7235,  1999,  2119,  2967,
         1012,  1007,   102])"
1058,1,"['variances', 'degrees of freedom', 'samples']", Calculation,seg_225,technical note: degrees of freedom in t-test for two samples with unequal variances. there is a fairly complicated formula to determine the precise number of degrees of freedom:,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0.])",tensor([17208]),"tensor([  101,  4087,  3602,  1024,  5445,  1997,  4071,  1999,  1056,  1011,
         3231,  2005,  2048,  8168,  2007, 16655, 26426, 23284,  2015,  1012,
         2045,  2003,  1037,  7199,  8552,  5675,  2000,  5646,  1996, 10480,
         2193,  1997,  5445,  1997,  4071,  1024,   102])"
1059,1,['degrees of freedom'], Calculation,seg_225,"in the example, we have minimum 12 degrees of freedom and maximum 28 degrees of freedom. this is in agreement with the value f ¼ 19, found using the formula in the box above.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])",tensor([17208]),"tensor([ 101, 1999, 1996, 2742, 1010, 2057, 2031, 6263, 2260, 5445, 1997, 4071,
        1998, 4555, 2654, 5445, 1997, 4071, 1012, 2023, 2003, 1999, 3820, 2007,
        1996, 3643, 1042, 1091, 2539, 1010, 2179, 2478, 1996, 5675, 1999, 1996,
        3482, 2682, 1012,  102])"
1060,1,['hypothesis'], Calculation,seg_225,"if the two averages are identical, we get t ¼ 0! values of t close to 0 are “good” for the hypothesis. values of t far from 0 are “bad” for the hypothesis. if t is far from 0, we will therefore reject the hypothesis.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0.])",tensor([17208]),"tensor([  101,  2065,  1996,  2048, 20185,  2024,  7235,  1010,  2057,  2131,
         1056,  1091,  1014,   999,  5300,  1997,  1056,  2485,  2000,  1014,
         2024,  1523,  2204,  1524,  2005,  1996, 10744,  1012,  5300,  1997,
         1056,  2521,  2013,  1014,  2024,  1523,  2919,  1524,  2005,  1996,
        10744,  1012,  2065,  1056,  2003,  2521,  2013,  1014,  1010,  2057,
         2097,  3568, 15454,  1996, 10744,  1012,   102])"
1061,1,"['table', 'probability', 'fractile', 'degrees of freedom', 'hypothesis']", Calculation,seg_225,"from the table at the end of the book we get: the 90% fractile of a t-distribution with 19 degrees of freedom is 1.328. we have calculated the value of t to be 0.50, which is smaller than 1.328. the probability of getting a larger value of t is therefore (probably much) more than 10%. normally, we add the probability of getting a value of t, which is at least as “far out” to the opposite side. this is just as “bad” for the hypothesis! the probability of a rarer result is thus more than 20%. 3. if this probability is small, we reject the hypothesis.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.])",tensor([17208]),"tensor([  101,  2013,  1996,  2795,  2012,  1996,  2203,  1997,  1996,  2338,
         2057,  2131,  1024,  1996,  3938,  1003, 25312,  6593,  9463,  1997,
         1037,  1056,  1011,  4353,  2007,  2539,  5445,  1997,  4071,  2003,
         1015,  1012, 25256,  1012,  2057,  2031, 10174,  1996,  3643,  1997,
         1056,  2000,  2022,  1014,  1012,  2753,  1010,  2029,  2003,  3760,
         2084,  1015,  1012, 25256,  1012,  1996,  9723,  1997,  2893,  1037,
         3469,  3643,  1997,  1056,  2003,  3568,  1006,  2763,  2172,  1007,
         2062,  2084,  2184,  1003,  1012,  5373,  1010,  2057,  5587,  1996,
         9723,  1997,  2893,  1037,  3643,  1997,  1056,  1010,  2029,  2003,
         2012,  2560,  2004,  1523,  2521,  2041,  1524,  2000,  1996,  4500,
         2217,  1012,  2023,  2003,  2074,  2004,  1523,  2919,  1524,  2005,
         1996, 10744,   999,  1996,  9723,  1997,  1037,  4678,  2099,  2765,
         2003,  2947,  2062,  2084,  2322,  1003,  1012,  1017,  1012,  2065,
         2023,  9723,  2003,  2235,  1010,  2057, 15454,  1996, 10744,  1012,
          102])"
1062,1,"['mean', 'confidence interval', 'probability', 'interval', 'hypothesis', 'confidence']", Calculation,seg_225,"since the observed probability is larger than 20%, we accept the hypothesis. we can also calculate a 95% confidence interval for the mean difference using the following formula:",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])",tensor([17208]),"tensor([  101,  2144,  1996,  5159,  9723,  2003,  3469,  2084,  2322,  1003,
         1010,  2057,  5138,  1996, 10744,  1012,  2057,  2064,  2036, 18422,
         1037,  5345,  1003,  7023, 13483,  2005,  1996,  2812,  4489,  2478,
         1996,  2206,  5675,  1024,   102])"
1063,1,"['mean', 'confidence interval', 'table', 'probability', 'fractile', 'interval', 'degrees of freedom', 'confidence']", Calculation,seg_225,"for t we use the 97.5% fractile in a t-distribution. this gives us a 95% confidence interval, i.e., with probability 95% the interval contains the value of the mean difference. the number of degrees of freedom is determined as shown in the text box; in the example, we get 19 degrees of freedom. in the table with the t-distribution at",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])",tensor([17208]),"tensor([  101,  2005,  1056,  2057,  2224,  1996,  5989,  1012,  1019,  1003,
        25312,  6593,  9463,  1999,  1037,  1056,  1011,  4353,  1012,  2023,
         3957,  2149,  1037,  5345,  1003,  7023, 13483,  1010,  1045,  1012,
         1041,  1012,  1010,  2007,  9723,  5345,  1003,  1996, 13483,  3397,
         1996,  3643,  1997,  1996,  2812,  4489,  1012,  1996,  2193,  1997,
         5445,  1997,  4071,  2003,  4340,  2004,  3491,  1999,  1996,  3793,
         3482,  1025,  1999,  1996,  2742,  1010,  2057,  2131,  2539,  5445,
         1997,  4071,  1012,  1999,  1996,  2795,  2007,  1996,  1056,  1011,
         4353,  2012,   102])"
1064,1,"['mean', 'confidence interval', 'probability', 'fractile', 'interval', 'degrees of freedom', 'hypothesis', 'confidence']", Calculation,seg_225,"the end of the book we find the 97.5% fractile in a t-distribution with 19 degrees of freedom to be 2.093. if we insert this in the formula, we get the confidence interval 0.88 3.68. with probability 95% the mean difference is somewhere between 2.79 and 3.68. this interval contains 0, in agreement with the fact that the hypothesis is accepted.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])",tensor([17208]),"tensor([  101,  1996,  2203,  1997,  1996,  2338,  2057,  2424,  1996,  5989,
         1012,  1019,  1003, 25312,  6593,  9463,  1999,  1037,  1056,  1011,
         4353,  2007,  2539,  5445,  1997,  4071,  2000,  2022,  1016,  1012,
         5641,  2509,  1012,  2065,  2057, 19274,  2023,  1999,  1996,  5675,
         1010,  2057,  2131,  1996,  7023, 13483,  1014,  1012,  6070,  1017,
         1012,  6273,  1012,  2007,  9723,  5345,  1003,  1996,  2812,  4489,
         2003,  4873,  2090,  1016,  1012,  6535,  1998,  1017,  1012,  6273,
         1012,  2023, 13483,  3397,  1014,  1010,  1999,  3820,  2007,  1996,
         2755,  2008,  1996, 10744,  2003,  3970,  1012,   102])"
1065,1,"['normal distribution', 'fractiles', 'fractile', 'normal', 'distribution', 'data', 'error']", Calculation,seg_225,"note: if both the groups are large, e.g., more than ten data values, we can without too much error use fractiles in the normal distribution instead of the t-distribution, i.e., the 97.5% fractile is app. 2. in the example above, the t-fractile is 2.09 instead of 1.96, which is not a very large difference.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])",tensor([17208]),"tensor([  101,  3602,  1024,  2065,  2119,  1996,  2967,  2024,  2312,  1010,
         1041,  1012,  1043,  1012,  1010,  2062,  2084,  2702,  2951,  5300,
         1010,  2057,  2064,  2302,  2205,  2172,  7561,  2224, 25312,  6593,
         9463,  2015,  1999,  1996,  3671,  4353,  2612,  1997,  1996,  1056,
         1011,  4353,  1010,  1045,  1012,  1041,  1012,  1010,  1996,  5989,
         1012,  1019,  1003, 25312,  6593,  9463,  2003, 10439,  1012,  1016,
         1012,  1999,  1996,  2742,  2682,  1010,  1996,  1056,  1011, 25312,
         6593,  9463,  2003,  1016,  1012,  5641,  2612,  1997,  1015,  1012,
         5986,  1010,  2029,  2003,  2025,  1037,  2200,  2312,  4489,  1012,
          102])"
1066,1,"['function', 'probability']", Spreadsheets,seg_227,"with a spreadsheet, we can directly calculate the p value, i.e., the probability of a value of t rarer than the above calculated value 0.50 using the ttest function; see the description earlier in this chapter.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([20861, 21030,  3215])","tensor([  101,  2007,  1037, 20861, 21030,  2102,  1010,  2057,  2064,  3495,
        18422,  1996,  1052,  3643,  1010,  1045,  1012,  1041,  1012,  1010,
         1996,  9723,  1997,  1037,  3643,  1997,  1056,  4678,  2099,  2084,
         1996,  2682, 10174,  3643,  1014,  1012,  2753,  2478,  1996, 23746,
         4355,  3853,  1025,  2156,  1996,  6412,  3041,  1999,  2023,  3127,
         1012,   102])"
1067,1,"['function', 'data']", Spreadsheets,seg_227,"the data could be in cells b1:n1 (girls), and b2:r2 (boys). we now use the function as follows: ¼ttest(b1:n1;b2:r2;2;3) we use:",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([20861, 21030,  3215])","tensor([  101,  1996,  2951,  2071,  2022,  1999,  4442, 29491,  1024,  1050,
         2487,  1006,  3057,  1007,  1010,  1998,  1038,  2475,  1024,  1054,
         2475,  1006,  3337,  1007,  1012,  2057,  2085,  2224,  1996,  3853,
         2004,  4076,  1024,  1091, 14581,  2102,  1006, 29491,  1024,  1050,
         2487,  1025,  1038,  2475,  1024,  1054,  2475,  1025,  1016,  1025,
         1017,  1007,  2057,  2224,  1024,   102])"
1068,1,"['mean', 'tails', 'hypothesis', 'distribution']", Spreadsheets,seg_227,"– tails ¼ 2, since we reject the hypothesis in both sides of the distribution. – type ¼ 3, since we perform a t-test for comparing the mean of two groups.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0.])","tensor([20861, 21030,  3215])","tensor([  101,  1516, 17448,  1091,  1016,  1010,  2144,  2057, 15454,  1996,
        10744,  1999,  2119,  3903,  1997,  1996,  4353,  1012,  1516,  2828,
         1091,  1017,  1010,  2144,  2057,  4685,  1037,  1056,  1011,  3231,
         2005, 13599,  1996,  2812,  1997,  2048,  2967,  1012,   102])"
1069,1,"['probability', 'test']", Spreadsheets,seg_227,"the result is 0.62 ¼ 62%. the probability of a rarer value of t is thus app. 62%. by comparison, we found out in the above test that the p value is more than 20%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([20861, 21030,  3215])","tensor([  101,  1996,  2765,  2003,  1014,  1012,  5786,  1091,  5786,  1003,
         1012,  1996,  9723,  1997,  1037,  4678,  2099,  3643,  1997,  1056,
         2003,  2947, 10439,  1012,  5786,  1003,  1012,  2011,  7831,  1010,
         2057,  2179,  2041,  1999,  1996,  2682,  3231,  2008,  1996,  1052,
         3643,  2003,  2062,  2084,  2322,  1003,  1012,   102])"
1070,1,['function'], Spreadsheets,seg_227,"when using the ttest function, we do not need to do any calculations! we get the p value directly and can compare this with 0.05.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([20861, 21030,  3215])","tensor([  101,  2043,  2478,  1996, 23746,  4355,  3853,  1010,  2057,  2079,
         2025,  2342,  2000,  2079,  2151, 16268,   999,  2057,  2131,  1996,
         1052,  3643,  3495,  1998,  2064, 12826,  2023,  2007,  1014,  1012,
         5709,  1012,   102])"
1071,1,"['uncertainty', 'statistical uncertainty', 'variances', 'sample', 'statistical']", Size of an Experiment,seg_229,"let us assume that the variances in both the groups are identical and equal to s, and the sample sizes are identical and equal to n (larger than 10). then the statistical uncertainty of the difference between the two group means is",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2946, 1997, 2019, 7551])","tensor([  101,  2292,  2149,  7868,  2008,  1996, 23284,  2015,  1999,  2119,
         1996,  2967,  2024,  7235,  1998,  5020,  2000,  1055,  1010,  1998,
         1996,  7099, 10826,  2024,  7235,  1998,  5020,  2000,  1050,  1006,
         3469,  2084,  2184,  1007,  1012,  2059,  1996,  7778, 12503,  1997,
         1996,  4489,  2090,  1996,  2048,  2177,  2965,  2003,   102])"
1072,1,"['interval', 'confidence', 'confidence interval']", Size of an Experiment,seg_229,this number is just “the number after ” in the formula for the confidence interval for the difference between two means; see above.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2946, 1997, 2019, 7551])","tensor([  101,  2023,  2193,  2003,  2074,  1523,  1996,  2193,  2044,  1524,
         1999,  1996,  5675,  2005,  1996,  7023, 13483,  2005,  1996,  4489,
         2090,  2048,  2965,  1025,  2156,  2682,  1012,   102])"
1073,1,"['uncertainty', 'statistical uncertainty', 'sample', 'sample size', 'statistical']", Size of an Experiment,seg_229,this can be used to determine the necessary sample size in order to obtain a given statistical uncertainty u of the difference between two means. the necessary sample size is,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([2946, 1997, 2019, 7551])","tensor([  101,  2023,  2064,  2022,  2109,  2000,  5646,  1996,  4072,  7099,
         2946,  1999,  2344,  2000,  6855,  1037,  2445,  7778, 12503,  1057,
         1997,  1996,  4489,  2090,  2048,  2965,  1012,  1996,  4072,  7099,
         2946,  2003,   102])"
1074,1,"['sample', 'sample size']", Size of an Experiment,seg_229,"here, n is the necessary sample size in each group, i.e., the total sample size is 2 n. in general, if we have more than two groups, n is multiplied with the number of groups.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([2946, 1997, 2019, 7551])","tensor([  101,  2182,  1010,  1050,  2003,  1996,  4072,  7099,  2946,  1999,
         2169,  2177,  1010,  1045,  1012,  1041,  1012,  1010,  1996,  2561,
         7099,  2946,  2003,  1016,  1050,  1012,  1999,  2236,  1010,  2065,
         2057,  2031,  2062,  2084,  2048,  2967,  1010,  1050,  2003, 28608,
         2007,  1996,  2193,  1997,  2967,  1012,   102])"
1075,1,"['sample', 'experiments']", Size of an Experiment,seg_229,"this formula can be used in the same way, as the formula given in chap. 6. it will most often be used in connection with planning of experiments. experiments usually involve comparing two or more groups; in most sample surveys, on the contrary, there is usually just one group.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([2946, 1997, 2019, 7551])","tensor([  101,  2023,  5675,  2064,  2022,  2109,  1999,  1996,  2168,  2126,
         1010,  2004,  1996,  5675,  2445,  1999, 15775,  2361,  1012,  1020,
         1012,  2009,  2097,  2087,  2411,  2022,  2109,  1999,  4434,  2007,
         4041,  1997,  7885,  1012,  7885,  2788,  9125, 13599,  2048,  2030,
         2062,  2967,  1025,  1999,  2087,  7099, 12265,  1010,  2006,  1996,
        10043,  1010,  2045,  2003,  2788,  2074,  2028,  2177,  1012,   102])"
1076,1,"['deviation', 'variance', 'standard deviation', 'standard']", Test for the Same Variance in the Two Groups,seg_233,"sometimes you will be interested in whether the variance (or standard deviation) is the same in both groups. this can be examined by an f-test, which we will not cover in detail here.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([ 3231,  2005,  1996,  2168, 23284,  1999,  1996,  2048,  2967])","tensor([  101,  2823,  2017,  2097,  2022,  4699,  1999,  3251,  1996, 23284,
         1006,  2030,  3115, 24353,  1007,  2003,  1996,  2168,  1999,  2119,
         2967,  1012,  2023,  2064,  2022,  8920,  2011,  2019,  1042,  1011,
         3231,  1010,  2029,  2057,  2097,  2025,  3104,  1999,  6987,  2182,
         1012,   102])"
1077,1,"['function', 'degrees of freedom', 'distribution', 'data']", Test for the Same Variance in the Two Groups,seg_233,"there is a function ftest for this purpose in most spreadsheets. the f-test uses an f-distribution. this distribution is relatively complicated because it requires two numbers of degrees of freedom (one for each group). for the ftest function, however, you only need to specify the areas with data for both the groups.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 3231,  2005,  1996,  2168, 23284,  1999,  1996,  2048,  2967])","tensor([  101,  2045,  2003,  1037,  3853,  3027,  4355,  2005,  2023,  3800,
         1999,  2087, 20861, 21030,  3215,  1012,  1996,  1042,  1011,  3231,
         3594,  2019,  1042,  1011,  4353,  1012,  2023,  4353,  2003,  4659,
         8552,  2138,  2009,  5942,  2048,  3616,  1997,  5445,  1997,  4071,
         1006,  2028,  2005,  2169,  2177,  1007,  1012,  2005,  1996,  3027,
         4355,  3853,  1010,  2174,  1010,  2017,  2069,  2342,  2000, 20648,
         1996,  2752,  2007,  2951,  2005,  2119,  1996,  2967,  1012,   102])"
1078,1,"['function', 'deviation', 'variance', 'table', 'hypothesis', 'standard deviation', 'standard', 'data', 'test']", Test for the Same Variance in the Two Groups,seg_233,"in the example, we have bmi data from girls in cells b1:n1 and from the boys in cells b2:r2. we want to test the hypothesis that the variance (or standard deviation) is the same for girls and boys. we could then use the ftest function as given in table 8.5.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,
        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 3231,  2005,  1996,  2168, 23284,  1999,  1996,  2048,  2967])","tensor([  101,  1999,  1996,  2742,  1010,  2057,  2031,  1038,  4328,  2951,
         2013,  3057,  1999,  4442, 29491,  1024,  1050,  2487,  1998,  2013,
         1996,  3337,  1999,  4442,  1038,  2475,  1024,  1054,  2475,  1012,
         2057,  2215,  2000,  3231,  1996, 10744,  2008,  1996, 23284,  1006,
         2030,  3115, 24353,  1007,  2003,  1996,  2168,  2005,  3057,  1998,
         3337,  1012,  2057,  2071,  2059,  2224,  1996,  3027,  4355,  3853,
         2004,  2445,  1999,  2795,  1022,  1012,  1019,  1012,   102])"
1079,1,"['hypothesis', 'variances']", Test for the Same Variance in the Two Groups,seg_233,"the result is the p value for the hypothesis that the two variances are equal. this p value is found to be 0.093 or 9.3%. therefore, we accept the hypothesis that the two variances are equal.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([ 3231,  2005,  1996,  2168, 23284,  1999,  1996,  2048,  2967])","tensor([  101,  1996,  2765,  2003,  1996,  1052,  3643,  2005,  1996, 10744,
         2008,  1996,  2048, 23284,  2015,  2024,  5020,  1012,  2023,  1052,
         3643,  2003,  2179,  2000,  2022,  1014,  1012,  5641,  2509,  2030,
         1023,  1012,  1017,  1003,  1012,  3568,  1010,  2057,  5138,  1996,
        10744,  2008,  1996,  2048, 23284,  2015,  2024,  5020,  1012,   102])"
1080,1,"['mean', 'deviation', 'variance', 'standard deviation', 'standard']", Comparing Two Group Means Two Samples with Equal Variances,seg_235,"there is a third kind of t-test: a t-test for comparing the mean of two groups, assuming that there is the same variance (or standard deviation) in both the groups. this might be examined first by an f-test.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([13599,  2048,  2177,  2965,  2048,  8168,  2007,  5020, 23284,  2015])","tensor([  101,  2045,  2003,  1037,  2353,  2785,  1997,  1056,  1011,  3231,
         1024,  1037,  1056,  1011,  3231,  2005, 13599,  1996,  2812,  1997,
         2048,  2967,  1010, 10262,  2008,  2045,  2003,  1996,  2168, 23284,
         1006,  2030,  3115, 24353,  1007,  1999,  2119,  1996,  2967,  1012,
         2023,  2453,  2022,  8920,  2034,  2011,  2019,  1042,  1011,  3231,
         1012,   102])"
1081,1,['function'], Comparing Two Group Means Two Samples with Equal Variances,seg_235,"you can do this t-test in a spreadsheet, by selecting type ¼ 2 in the ttest function.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0.])","tensor([13599,  2048,  2177,  2965,  2048,  8168,  2007,  5020, 23284,  2015])","tensor([  101,  2017,  2064,  2079,  2023,  1056,  1011,  3231,  1999,  1037,
        20861, 21030,  2102,  1010,  2011, 17739,  2828,  1091,  1016,  1999,
         1996, 23746,  4355,  3853,  1012,   102])"
1082,0,[], Comparing Two Group Means Two Samples with Equal Variances,seg_235,the situation where this t-test should be considered is the following:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([13599,  2048,  2177,  2965,  2048,  8168,  2007,  5020, 23284,  2015])","tensor([ 101, 1996, 3663, 2073, 2023, 1056, 1011, 3231, 2323, 2022, 2641, 2003,
        1996, 2206, 1024,  102])"
1083,1,"['variances', 'deviations', 'standard', 'standard deviations', 'test']", Comparing Two Group Means Two Samples with Equal Variances,seg_235,– the two variances (or standard deviations) are virtually identical. test this with,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0.])","tensor([13599,  2048,  2177,  2965,  2048,  8168,  2007,  5020, 23284,  2015])","tensor([  101,  1516,  1996,  2048, 23284,  2015,  1006,  2030,  3115, 24353,
         2015,  1007,  2024,  8990,  7235,  1012,  3231,  2023,  2007,   102])"
1084,1,"['sample', 'data']", Comparing Two Group Means Two Samples with Equal Variances,seg_235,"the f-test. – one sample is substantially smaller than the other, with less than ten data values.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0.])","tensor([13599,  2048,  2177,  2965,  2048,  8168,  2007,  5020, 23284,  2015])","tensor([  101,  1996,  1042,  1011,  3231,  1012,  1516,  2028,  7099,  2003,
        12381,  3760,  2084,  1996,  2060,  1010,  2007,  2625,  2084,  2702,
         2951,  5300,  1012,   102])"
1085,1,['degrees of freedom'], Comparing Two Group Means Two Samples with Equal Variances,seg_235,"in this situation you get more degrees of freedom for this t-test, than when using the t-test from the last section. this is an advantage, because it will be easier to detect differences that actually exist!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([13599,  2048,  2177,  2965,  2048,  8168,  2007,  5020, 23284,  2015])","tensor([  101,  1999,  2023,  3663,  2017,  2131,  2062,  5445,  1997,  4071,
         2005,  2023,  1056,  1011,  3231,  1010,  2084,  2043,  2478,  1996,
         1056,  1011,  3231,  2013,  1996,  2197,  2930,  1012,  2023,  2003,
         2019,  5056,  1010,  2138,  2009,  2097,  2022,  6082,  2000, 11487,
         5966,  2008,  2941,  4839,   999,   102])"
1086,1,"['variances', 'hypothesis', 'data', 'case']", Comparing Two Group Means Two Samples with Equal Variances,seg_235,"in general, however, there is not much need for this t-test! in the example with the bmi data, we have accepted the hypothesis that the two variances are equal. thus, we could use this t-test (i.e., type ¼ 2) in this case. then we get a p value of 0.60, i.e., practically the same as before.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([13599,  2048,  2177,  2965,  2048,  8168,  2007,  5020, 23284,  2015])","tensor([  101,  1999,  2236,  1010,  2174,  1010,  2045,  2003,  2025,  2172,
         2342,  2005,  2023,  1056,  1011,  3231,   999,  1999,  1996,  2742,
         2007,  1996,  1038,  4328,  2951,  1010,  2057,  2031,  3970,  1996,
        10744,  2008,  1996,  2048, 23284,  2015,  2024,  5020,  1012,  2947,
         1010,  2057,  2071,  2224,  2023,  1056,  1011,  3231,  1006,  1045,
         1012,  1041,  1012,  1010,  2828,  1091,  1016,  1007,  1999,  2023,
         2553,  1012,  2059,  2057,  2131,  1037,  1052,  3643,  1997,  1014,
         1012,  3438,  1010,  1045,  1012,  1041,  1012,  1010,  8134,  1996,
         2168,  2004,  2077,  1012,   102])"
1087,1,"['anova', 'analysis of variance', 'variance']", Final Remarks,seg_237,"we have in this chapter studied the two main techniques to compare the two groups. there is another technique that can be seen as an extension of the t-test: analysis of variance (*), often abbreviated anova.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 2345, 12629])","tensor([  101,  2057,  2031,  1999,  2023,  3127,  3273,  1996,  2048,  2364,
         5461,  2000, 12826,  1996,  2048,  2967,  1012,  2045,  2003,  2178,
         6028,  2008,  2064,  2022,  2464,  2004,  2019,  5331,  1997,  1996,
         1056,  1011,  3231,  1024,  4106,  1997, 23284,  1006,  1008,  1007,
         1010,  2411, 12066,  2019,  7103,  1012,   102])"
1088,1,"['sample', 'factor', 'anova', 'experiments', 'data']", Final Remarks,seg_237,"anova is, despite the name, used to compare means. a simple anova example could be comparing several group means (i.e., more than two groups). this is available in microsoft excel using the add-in menu “data analysis,” menu-item “anova: single factor”. single factor anova is useful when analyzing data from sample surveys as well as planned experiments.",tensor(1),"tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 2345, 12629])","tensor([  101,  2019,  7103,  2003,  1010,  2750,  1996,  2171,  1010,  2109,
         2000, 12826,  2965,  1012,  1037,  3722,  2019,  7103,  2742,  2071,
         2022, 13599,  2195,  2177,  2965,  1006,  1045,  1012,  1041,  1012,
         1010,  2062,  2084,  2048,  2967,  1007,  1012,  2023,  2003,  2800,
         1999,  7513, 24970,  2478,  1996,  5587,  1011,  1999, 12183,  1523,
         2951,  4106,  1010,  1524, 12183,  1011,  8875,  1523,  2019,  7103,
         1024,  2309,  5387,  1524,  1012,  2309,  5387,  2019,  7103,  2003,
         6179,  2043, 20253,  2951,  2013,  7099, 12265,  2004,  2092,  2004,
         3740,  7885,  1012,   102])"
1089,1,"['data', 'factors', 'anova', 'statistical', 'case']", Final Remarks,seg_237,"some situations involve two or several factors, which define the groups. this is an anova with two or more factors and is available in statistical software. see the list of statistical software at the end of the book. in microsoft excel, you can do anova with two factors, but only in the simple case, where there is the same number of data values (or just 1) in all of the groups defined by the two factors.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 2345, 12629])","tensor([  101,  2070,  8146,  9125,  2048,  2030,  2195,  5876,  1010,  2029,
         9375,  1996,  2967,  1012,  2023,  2003,  2019,  2019,  7103,  2007,
         2048,  2030,  2062,  5876,  1998,  2003,  2800,  1999,  7778,  4007,
         1012,  2156,  1996,  2862,  1997,  7778,  4007,  2012,  1996,  2203,
         1997,  1996,  2338,  1012,  1999,  7513, 24970,  1010,  2017,  2064,
         2079,  2019,  7103,  2007,  2048,  5876,  1010,  2021,  2069,  1999,
         1996,  3722,  2553,  1010,  2073,  2045,  2003,  1996,  2168,  2193,
         1997,  2951,  5300,  1006,  2030,  2074,  1015,  1007,  1999,  2035,
         1997,  1996,  2967,  4225,  2011,  1996,  2048,  5876,  1012,   102])"
1090,1,"['factors', 'experiments', 'data']", Final Remarks,seg_237,anova with two or more factors is mostly used when analyzing data from planned experiments!,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0.])","tensor([ 2345, 12629])","tensor([  101,  2019,  7103,  2007,  2048,  2030,  2062,  5876,  2003,  3262,
         2109,  2043, 20253,  2951,  2013,  3740,  7885,   999,   102])"
1091,1,"['anova', 'statistics']", Final Remarks,seg_237,"we cannot cover anova in this book. see more advanced books from the literature list. at this point, you have sufficient background to read more advanced books on statistics, if needed.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0.])","tensor([ 2345, 12629])","tensor([ 101, 2057, 3685, 3104, 2019, 7103, 1999, 2023, 2338, 1012, 2156, 2062,
        3935, 2808, 2013, 1996, 3906, 2862, 1012, 2012, 2023, 2391, 1010, 2017,
        2031, 7182, 4281, 2000, 3191, 2062, 3935, 2808, 2006, 6747, 1010, 2065,
        2734, 1012,  102])"
1092,1,['statistics'], Final Remarks,seg_237,you have now sufficient knowledge about statistics to go out and use it in practice! i wish you good luck in your further work with statistics!,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([ 2345, 12629])","tensor([ 101, 2017, 2031, 2085, 7182, 3716, 2055, 6747, 2000, 2175, 2041, 1998,
        2224, 2009, 1999, 3218,  999, 1045, 4299, 2017, 2204, 6735, 1999, 2115,
        2582, 2147, 2007, 6747,  999,  102])"
1093,1,['statistical'], Probability Theory,seg_241,"the reader, who is mainly interested in applying statistical methods, can safely skip this appendix.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([9723, 3399])","tensor([  101,  1996,  8068,  1010,  2040,  2003,  3701,  4699,  1999, 11243,
         7778,  4725,  1010,  2064,  9689, 13558,  2023, 22524,  1012,   102])"
1094,1,"['normal distribution', 'distributions', 'normal', 'distribution', 'statistical', 'binomial', 'binomial distribution']", Probability Theory,seg_241,"probability theory gives the mathematically oriented reader a better understanding of fundamental statistical concepts, such as statistical distributions (e.g., the binomial distribution and the normal distribution), i.e., the concepts explained in chaps. 4 and 5. in particular, one purpose of this appendix is to obtain a better understanding of the binomial distribution.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 1., 0., 0.])","tensor([9723, 3399])","tensor([  101,  9723,  3399,  3957,  1996,  8045,  2135,  8048,  8068,  1037,
         2488,  4824,  1997,  8050,  7778,  8474,  1010,  2107,  2004,  7778,
        20611,  1006,  1041,  1012,  1043,  1012,  1010,  1996,  8026, 20936,
         2389,  4353,  1998,  1996,  3671,  4353,  1007,  1010,  1045,  1012,
         1041,  1012,  1010,  1996,  8474,  4541,  1999, 15775,  4523,  1012,
         1018,  1998,  1019,  1012,  1999,  3327,  1010,  2028,  3800,  1997,
         2023, 22524,  2003,  2000,  6855,  1037,  2488,  4824,  1997,  1996,
         8026, 20936,  2389,  4353,  1012,   102])"
1095,1,"['probability', 'statistical', 'probability theory']", Probability Theory,seg_241,"only the fundamental concepts of probability theory, which are relevant for explaining the statistical concepts in this book, are explained here. other books must be consulted for a more thorough introduction to probability theory.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0.])","tensor([9723, 3399])","tensor([  101,  2069,  1996,  8050,  8474,  1997,  9723,  3399,  1010,  2029,
         2024,  7882,  2005,  9990,  1996,  7778,  8474,  1999,  2023,  2338,
         1010,  2024,  4541,  2182,  1012,  2060,  2808,  2442,  2022, 17535,
         2005,  1037,  2062, 16030,  4955,  2000,  9723,  3399,  1012,   102])"
1096,1,"['probability', 'probability theory']", Probability Theory,seg_241,"probability theory was historically founded in medieval times when analyzing problems in games, e.g., throwing dice. and even today, most introductions to probability theory use examples from games. this also has the advantage that these examples are (relatively) simple compared to other (maybe more practically relevant) examples.",tensor(1),"tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([9723, 3399])","tensor([  101,  9723,  3399,  2001,  7145,  2631,  1999,  5781,  2335,  2043,
        20253,  3471,  1999,  2399,  1010,  1041,  1012,  1043,  1012,  1010,
         6886, 18740,  1012,  1998,  2130,  2651,  1010,  2087, 25795,  2000,
         9723,  3399,  2224,  4973,  2013,  2399,  1012,  2023,  2036,  2038,
         1996,  5056,  2008,  2122,  4973,  2024,  1006,  4659,  1007,  3722,
         4102,  2000,  2060,  1006,  2672,  2062,  8134,  7882,  1007,  4973,
         1012,   102])"
1097,1,['probability'], Probability Theory,seg_241,"in this appendix, the basic terms of probability are explained intuitively by examples using only a minimum of mathematical notation.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([9723, 3399])","tensor([  101,  1999,  2023, 22524,  1010,  1996,  3937,  3408,  1997,  9723,
         2024,  4541, 29202,  2135,  2011,  4973,  2478,  2069,  1037,  6263,
         1997,  8045, 14869,  1012,   102])"
1098,1,"['probability', 'statistics', 'probability theory']", Probability Theory,seg_241,"for a more complete explanation of probability theory, see other books, e.g., sincich tl, levine dm, stephan d, sincich t and berenson m (2002) practical statistics by example – usingmicrosoft excel andminitab. 2nd ed. prentice hall, nj.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([9723, 3399])","tensor([  101,  2005,  1037,  2062,  3143,  7526,  1997,  9723,  3399,  1010,
         2156,  2060,  2808,  1010,  1041,  1012,  1043,  1012,  1010,  8254,
        19053,  2232,  1056,  2140,  1010, 17780,  1040,  2213,  1010, 15963,
         1040,  1010,  8254, 19053,  2232,  1056,  1998,  2022,  7389,  3385,
         1049,  1006,  2526,  1007,  6742,  6747,  2011,  2742,  1516,  2478,
         7712,  7352, 15794, 24970,  1998, 25300,  2696,  2497,  1012,  3416,
         3968,  1012, 23429,  2534,  1010, 19193,  1012,   102])"
1099,1,"['observation', 'outcomes', 'experiment', 'measurement']", Sample Space Events and Probability,seg_243,when recording an observation (for example in a survey) or a measurement (for example in an experiment) there are a number of outcomes (*).,tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2043,  3405,  2019,  8089,  1006,  2005,  2742,  1999,  1037,
         5002,  1007,  2030,  1037, 10903,  1006,  2005,  2742,  1999,  2019,
         7551,  1007,  2045,  2024,  1037,  2193,  1997, 13105,  1006,  1008,
         1007,  1012,   102])"
1100,1,"['sample space', 'set', 'outcomes', 'event', 'sample', 'event ']", Sample Space Events and Probability,seg_243,the set of all possible outcomes is called the sample space (*). a subset of the sample space is called an event (*).,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  1996,  2275,  1997,  2035,  2825, 13105,  2003,  2170,  1996,
         7099,  2686,  1006,  1008,  1007,  1012,  1037, 16745,  1997,  1996,
         7099,  2686,  2003,  2170,  2019,  2724,  1006,  1008,  1007,  1012,
          102])"
1101,1,"['sample space', 'probability', 'outcomes', 'event', 'outcome', 'sample', 'likelihood', 'probability of event']", Sample Space Events and Probability,seg_243,"the probability (*) of an event (or an outcome) is a number between 0 and 1, which indicates the likelihood that the event will occur. if all outcomes in a finite sample space have the same probability (are equally likely), we have the following: probability of event ¼ (# of outcomes of the event)/(# of outcomes of the sample space).",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 1., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  1996,  9723,  1006,  1008,  1007,  1997,  2019,  2724,  1006,
         2030,  2019,  9560,  1007,  2003,  1037,  2193,  2090,  1014,  1998,
         1015,  1010,  2029,  7127,  1996, 16593,  2008,  1996,  2724,  2097,
         5258,  1012,  2065,  2035, 13105,  1999,  1037, 10713,  7099,  2686,
         2031,  1996,  2168,  9723,  1006,  2024,  8053,  3497,  1007,  1010,
         2057,  2031,  1996,  2206,  1024,  9723,  1997,  2724,  1091,  1006,
         1001,  1997, 13105,  1997,  1996,  2724,  1007,  1013,  1006,  1001,
         1997, 13105,  1997,  1996,  7099,  2686,  1007,  1012,   102])"
1102,1,"['risk', 'probability', 'outcome', 'case']", Sample Space Events and Probability,seg_243,"other words for probability are risk, in case of an undesirable outcome, and chance, in case of a desirable outcome.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2060,  2616,  2005,  9723,  2024,  3891,  1010,  1999,  2553,
         1997,  2019,  6151,  2229,  7895,  3468,  9560,  1010,  1998,  3382,
         1010,  1999,  2553,  1997,  1037, 16166,  9560,  1012,   102])"
1103,1,"['sample space', 'outcomes', 'sample spaces', 'event', 'outcome', 'sample']", Sample Space Events and Probability,seg_243,"in this appendix, we consider only finite sample spaces. an event may consist of just one outcome or an event may even be empty, i.e., contain no outcomes. on the other hand, an event may cover the whole sample space.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  1999,  2023, 22524,  1010,  2057,  5136,  2069, 10713,  7099,
         7258,  1012,  2019,  2724,  2089,  8676,  1997,  2074,  2028,  9560,
         2030,  2019,  2724,  2089,  2130,  2022,  4064,  1010,  1045,  1012,
         1041,  1012,  1010,  5383,  2053, 13105,  1012,  2006,  1996,  2060,
         2192,  1010,  2019,  2724,  2089,  3104,  1996,  2878,  7099,  2686,
         1012,   102])"
1104,1,"['sample space', 'outcomes', 'sample']", Sample Space Events and Probability,seg_243,"when throwing a dice, the possible outcomes are 1, 2, 3, 4, 5, or 6 eyes. the sample space consists of all these six outcomes.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2043,  6886,  1037, 18740,  1010,  1996,  2825, 13105,  2024,
         1015,  1010,  1016,  1010,  1017,  1010,  1018,  1010,  1019,  1010,
         2030,  1020,  2159,  1012,  1996,  7099,  2686,  3774,  1997,  2035,
         2122,  2416, 13105,  1012,   102])"
1105,1,['events'], Sample Space Events and Probability,seg_243,examples of events are:,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([ 101, 4973, 1997, 2824, 2024, 1024,  102])"
1106,1,"['events', 'event', 'outcome']", Sample Space Events and Probability,seg_243,"as the last example shows, an event can consist of just one outcome. the events in example 1 and 2 are complementary events. this means that",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2004,  1996,  2197,  2742,  3065,  1010,  2019,  2724,  2064,
         8676,  1997,  2074,  2028,  9560,  1012,  1996,  2824,  1999,  2742,
         1015,  1998,  1016,  2024, 21053,  2824,  1012,  2023,  2965,  2008,
          102])"
1107,1,"['events', 'outcomes', 'mutually exclusive']", Sample Space Events and Probability,seg_243,"– the two events are mutually exclusive, i.e., the two events have no outcomes in",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  1516,  1996,  2048,  2824,  2024, 20271,  7262,  1010,  1045,
         1012,  1041,  1012,  1010,  1996,  2048,  2824,  2031,  2053, 13105,
         1999,   102])"
1108,1,"['sample space', 'events', 'outcomes', 'sample']", Sample Space Events and Probability,seg_243,common. – the two events together contain all outcomes of the sample space.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2691,  1012,  1516,  1996,  2048,  2824,  2362,  5383,  2035,
        13105,  1997,  1996,  7099,  2686,  1012,   102])"
1109,1,"['events', 'probabilities', 'probabilities of two complementary events']", Sample Space Events and Probability,seg_243,the sum of the probabilities of two complementary events is 1.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  1996,  7680,  1997,  1996,  4013,  3676, 14680,  1997,  2048,
        21053,  2824,  2003,  1015,  1012,   102])"
1110,1,"['probability of the complementary event', 'probability', 'complementary event', 'event', 'probability of an event']", Sample Space Events and Probability,seg_243,"if we can calculate the probability of an event, the probability of the complementary event can thus be calculated by subtraction from 1. sometimes it is easier to calculate the probability of the complementary event!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 1., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2065,  2057,  2064, 18422,  1996,  9723,  1997,  2019,  2724,
         1010,  1996,  9723,  1997,  1996, 21053,  2724,  2064,  2947,  2022,
        10174,  2011,  4942,  6494,  7542,  2013,  1015,  1012,  2823,  2009,
         2003,  6082,  2000, 18422,  1996,  9723,  1997,  1996, 21053,  2724,
          999,   102])"
1111,1,['outcomes'], Sample Space Events and Probability,seg_243,throwing one dice twice: 36 outcomes; all of them are considered equally likely. below the total number of eyes is indicated for all outcomes (fig. 9.1).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  6886,  2028, 18740,  3807,  1024,  4029, 13105,  1025,  2035,
         1997,  2068,  2024,  2641,  8053,  3497,  1012,  2917,  1996,  2561,
         2193,  1997,  2159,  2003,  5393,  2005,  2035, 13105,  1006, 20965,
         1012,  1023,  1012,  1015,  1007,  1012,   102])"
1112,1,"['probability', 'complementary event', 'outcomes', 'event', 'probability of the event', 'probability of this event']", Sample Space Events and Probability,seg_243,"in this example, we study the event: “the total number of eyes is more than 3.” what is the probability of this event? the complementary event is “the total number of eyes is at most 3.” it is easily seen that the number of outcomes in this event is 3 (shaded area, see upper left corner of the figure), i.e., the probability of the event is 3/36 ¼ 1/12.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  1999,  2023,  2742,  1010,  2057,  2817,  1996,  2724,  1024,
         1523,  1996,  2561,  2193,  1997,  2159,  2003,  2062,  2084,  1017,
         1012,  1524,  2054,  2003,  1996,  9723,  1997,  2023,  2724,  1029,
         1996, 21053,  2724,  2003,  1523,  1996,  2561,  2193,  1997,  2159,
         2003,  2012,  2087,  1017,  1012,  1524,  2009,  2003,  4089,  2464,
         2008,  1996,  2193,  1997, 13105,  1999,  2023,  2724,  2003,  1017,
         1006, 25273,  2181,  1010,  2156,  3356,  2187,  3420,  1997,  1996,
         3275,  1007,  1010,  1045,  1012,  1041,  1012,  1010,  1996,  9723,
         1997,  1996,  2724,  2003,  1017,  1013,  4029,  1091,  1015,  1013,
         2260,  1012,   102])"
1113,1,"['probability of the complementary event', 'probability of the original event', 'complementary event', 'probability', 'event', 'case']", Sample Space Events and Probability,seg_243,"hence, the probability of the original event is 1 1/12 ¼ 11/12. in this case, calculation of the probability of the complementary event was easier.",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  6516,  1010,  1996,  9723,  1997,  1996,  2434,  2724,  2003,
         1015,  1015,  1013,  2260,  1091,  2340,  1013,  2260,  1012,  1999,
         2023,  2553,  1010, 17208,  1997,  1996,  9723,  1997,  1996, 21053,
         2724,  2001,  6082,  1012,   102])"
1114,1,['outcomes'], Sample Space Events and Probability,seg_243,throwing one dice twice: 36 outcomes; all of them are considered equally likely (fig. 9.2):,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  6886,  2028, 18740,  3807,  1024,  4029, 13105,  1025,  2035,
         1997,  2068,  2024,  2641,  8053,  3497,  1006, 20965,  1012,  1023,
         1012,  1016,  1007,  1024,   102])"
1115,1,"['probability', 'outcomes', 'event', 'probability of the event']", Sample Space Events and Probability,seg_243,"in this example, we study the event: “the total number of eyes is a multiple of 6.” it is seen that the total number of outcomes in this event is 6 (shaded area). the probability of the event “the total number of eyes is a multiple of 6” is thus 6/36 ¼ 1/6.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  1999,  2023,  2742,  1010,  2057,  2817,  1996,  2724,  1024,
         1523,  1996,  2561,  2193,  1997,  2159,  2003,  1037,  3674,  1997,
         1020,  1012,  1524,  2009,  2003,  2464,  2008,  1996,  2561,  2193,
         1997, 13105,  1999,  2023,  2724,  2003,  1020,  1006, 25273,  2181,
         1007,  1012,  1996,  9723,  1997,  1996,  2724,  1523,  1996,  2561,
         2193,  1997,  2159,  2003,  1037,  3674,  1997,  1020,  1524,  2003,
         2947,  1020,  1013,  4029,  1091,  1015,  1013,  1020,  1012,   102])"
1116,1,"['events', 'outcomes', 'probabilities', 'event', 'union', 'outcome', 'mutually exclusive events', 'mutually exclusive']", Sample Space Events and Probability,seg_243,"the event is equivalent to occurrence of either the event “the total number of eyes is 6” (5 outcomes) or the event “the total number of eyes is 12” (1 outcome). their probabilities are seen to be 5/36 and 1/36, respectively. mathematically, this event is the union of two mutually exclusive events.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,
        0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  1996,  2724,  2003,  5662,  2000, 14404,  1997,  2593,  1996,
         2724,  1523,  1996,  2561,  2193,  1997,  2159,  2003,  1020,  1524,
         1006,  1019, 13105,  1007,  2030,  1996,  2724,  1523,  1996,  2561,
         2193,  1997,  2159,  2003,  2260,  1524,  1006,  1015,  9560,  1007,
         1012,  2037,  4013,  3676, 14680,  2024,  2464,  2000,  2022,  1019,
         1013,  4029,  1998,  1015,  1013,  4029,  1010,  4414,  1012,  8045,
         2135,  1010,  2023,  2724,  2003,  1996,  2586,  1997,  2048, 20271,
         7262,  2824,  1012,   102])"
1117,1,"['probability', 'events', 'probabilities', 'event', 'probability of the event']", Sample Space Events and Probability,seg_243,another way to find the probability of the event is to add the probabilities of the two separate events; this again gives 5/36 + 1/36 ¼ 6/36 ¼ 1/6.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2178,  2126,  2000,  2424,  1996,  9723,  1997,  1996,  2724,
         2003,  2000,  5587,  1996,  4013,  3676, 14680,  1997,  1996,  2048,
         3584,  2824,  1025,  2023,  2153,  3957,  1019,  1013,  4029,  1009,
         1015,  1013,  4029,  1091,  1020,  1013,  4029,  1091,  1015,  1013,
         1020,  1012,   102])"
1118,1,"['probability', 'events', 'probabilities', 'mutually exclusive events', 'mutually exclusive']", Sample Space Events and Probability,seg_243,the probability of the occurrence of either one or the other of two mutually exclusive events is the sum of the probabilities of the individual events.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  1996,  9723,  1997,  1996, 14404,  1997,  2593,  2028,  2030,
         1996,  2060,  1997,  2048, 20271,  7262,  2824,  2003,  1996,  7680,
         1997,  1996,  4013,  3676, 14680,  1997,  1996,  3265,  2824,  1012,
          102])"
1119,1,['outcomes'], Sample Space Events and Probability,seg_243,throwing one dice twice: 36 outcomes; all of them are considered equally likely (fig. 9.3):,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  6886,  2028, 18740,  3807,  1024,  4029, 13105,  1025,  2035,
         1997,  2068,  2024,  2641,  8053,  3497,  1006, 20965,  1012,  1023,
         1012,  1017,  1007,  1024,   102])"
1120,1,"['probability', 'outcome', 'event', 'probability of this event']", Sample Space Events and Probability,seg_243,"in this example, we study the event: “the total number of eyes is 12.” this event contains just 1 outcome (lower right corner, shaded). the probability of this event is 1/36.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 1., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  1999,  2023,  2742,  1010,  2057,  2817,  1996,  2724,  1024,
         1523,  1996,  2561,  2193,  1997,  2159,  2003,  2260,  1012,  1524,
         2023,  2724,  3397,  2074,  1015,  9560,  1006,  2896,  2157,  3420,
         1010, 25273,  1007,  1012,  1996,  9723,  1997,  2023,  2724,  2003,
         1015,  1013,  4029,  1012,   102])"
1121,1,"['probability', 'events', 'outcomes', 'event', 'intersection']", Sample Space Events and Probability,seg_243,"it can be seen that the event is equivalent to the occurrence of both the event “6 eyes in throw 1” and the event “6 eyes in throw 2.” both of these events have a probability of 6/36 ¼ 1/6, as they consist of 6 outcomes (row 6, respectively, column 6, shown in bold). mathematically, this event is the intersection of the two separate events.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2009,  2064,  2022,  2464,  2008,  1996,  2724,  2003,  5662,
         2000,  1996, 14404,  1997,  2119,  1996,  2724,  1523,  1020,  2159,
         1999,  5466,  1015,  1524,  1998,  1996,  2724,  1523,  1020,  2159,
         1999,  5466,  1016,  1012,  1524,  2119,  1997,  2122,  2824,  2031,
         1037,  9723,  1997,  1020,  1013,  4029,  1091,  1015,  1013,  1020,
         1010,  2004,  2027,  8676,  1997,  1020, 13105,  1006,  5216,  1020,
         1010,  4414,  1010,  5930,  1020,  1010,  3491,  1999,  7782,  1007,
         1012,  8045,  2135,  1010,  2023,  2724,  2003,  1996,  6840,  1997,
         1996,  2048,  3584,  2824,  1012,   102])"
1122,1,"['probability', 'events', 'probabilities', 'event', 'probability of the event']", Sample Space Events and Probability,seg_243,"it is seen that the probability of the event also can be found as the product of the probabilities of the two separate events, i.e., as 1/6 1/6 ¼ 1/36.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2009,  2003,  2464,  2008,  1996,  9723,  1997,  1996,  2724,
         2036,  2064,  2022,  2179,  2004,  1996,  4031,  1997,  1996,  4013,
         3676, 14680,  1997,  1996,  2048,  3584,  2824,  1010,  1045,  1012,
         1041,  1012,  1010,  2004,  1015,  1013,  1020,  1015,  1013,  1020,
         1091,  1015,  1013,  4029,  1012,   102])"
1123,1,"['probability', 'events', 'probabilities', 'intersection', 'probability of the intersection of two events']", Sample Space Events and Probability,seg_243,"if the probability of the intersection of two events is exactly the product of the probabilities of the individual events, the two events are said to be independent (*).",tensor(1),"tensor([0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2065,  1996,  9723,  1997,  1996,  6840,  1997,  2048,  2824,
         2003,  3599,  1996,  4031,  1997,  1996,  4013,  3676, 14680,  1997,
         1996,  3265,  2824,  1010,  1996,  2048,  2824,  2024,  2056,  2000,
         2022,  2981,  1006,  1008,  1007,  1012,   102])"
1124,1,['probability'], Sample Space Events and Probability,seg_243,this means that the probability of obtaining six eyes in the second throw does not depend on whether or not we obtained six eyes in the first throw.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2023,  2965,  2008,  1996,  9723,  1997, 11381,  2416,  2159,
         1999,  1996,  2117,  5466,  2515,  2025, 12530,  2006,  3251,  2030,
         2025,  2057,  4663,  2416,  2159,  1999,  1996,  2034,  5466,  1012,
          102])"
1125,1,"['probability', 'conditional probability', 'conditional']", Sample Space Events and Probability,seg_243,"this is also expressed by stating that the conditional probability of obtaining six eyes in the second throw, given that we obtained six eyes in the first throw is 1/6, i.e., the same as the unconditional probability.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2023,  2003,  2036,  5228,  2011,  5517,  2008,  1996, 18462,
         9723,  1997, 11381,  2416,  2159,  1999,  1996,  2117,  5466,  1010,
         2445,  2008,  2057,  4663,  2416,  2159,  1999,  1996,  2034,  5466,
         2003,  1015,  1013,  1020,  1010,  1045,  1012,  1041,  1012,  1010,
         1996,  2168,  2004,  1996,  4895,  8663, 27064,  9723,  1012,   102])"
1126,1,['probability'], Sample Space Events and Probability,seg_243,"no matter what the result of the first throw is, the probability of six eyes in the second throw will still be 1/6.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([ 101, 2053, 3043, 2054, 1996, 2765, 1997, 1996, 2034, 5466, 2003, 1010,
        1996, 9723, 1997, 2416, 2159, 1999, 1996, 2117, 5466, 2097, 2145, 2022,
        1015, 1013, 1020, 1012,  102])"
1127,1,['outcomes'], Sample Space Events and Probability,seg_243,throwing one dice twice: 36 outcomes; all of them are considered equally likely (fig. 9.4):,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  6886,  2028, 18740,  3807,  1024,  4029, 13105,  1025,  2035,
         1997,  2068,  2024,  2641,  8053,  3497,  1006, 20965,  1012,  1023,
         1012,  1018,  1007,  1024,   102])"
1128,1,"['events', 'event']", Sample Space Events and Probability,seg_243,"in this example, we study the events: event a: “number of eyes in first throw is 1.” event b: “total number of eyes is at least 7.”",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([ 101, 1999, 2023, 2742, 1010, 2057, 2817, 1996, 2824, 1024, 2724, 1037,
        1024, 1523, 2193, 1997, 2159, 1999, 2034, 5466, 2003, 1015, 1012, 1524,
        2724, 1038, 1024, 1523, 2561, 2193, 1997, 2159, 2003, 2012, 2560, 1021,
        1012, 1524,  102])"
1129,1,"['probability', 'outcomes', 'event']", Sample Space Events and Probability,seg_243,"it is seen that event a consists of six outcomes (first row, shown in bold), hence the probability of a is p(a) ¼ 6/36 ¼ 1/6 (the letter p is used as abbreviation for “probability”).",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2009,  2003,  2464,  2008,  2724,  1037,  3774,  1997,  2416,
        13105,  1006,  2034,  5216,  1010,  3491,  1999,  7782,  1007,  1010,
         6516,  1996,  9723,  1997,  1037,  2003,  1052,  1006,  1037,  1007,
         1091,  1020,  1013,  4029,  1091,  1015,  1013,  1020,  1006,  1996,
         3661,  1052,  2003,  2109,  2004, 22498,  2005,  1523,  9723,  1524,
         1007,  1012,   102])"
1130,1,"['probability', 'outcomes', 'event']", Sample Space Events and Probability,seg_243,"it is seen that event b consists of 21 outcomes (shaded area, diagonal plus lower right half), hence the probability of b is p(b) ¼ 21/36 ¼ 7/12.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2009,  2003,  2464,  2008,  2724,  1038,  3774,  1997,  2538,
        13105,  1006, 25273,  2181,  1010, 19754,  4606,  2896,  2157,  2431,
         1007,  1010,  6516,  1996,  9723,  1997,  1038,  2003,  1052,  1006,
         1038,  1007,  1091,  2538,  1013,  4029,  1091,  1021,  1013,  2260,
         1012,   102])"
1131,1,"['probability', 'outcome', 'event', 'intersection']", Sample Space Events and Probability,seg_243,"the intersection a\b of event a and b consists of just one outcome, the upper right corner. this event has the probability p(a\b) ¼ 1/36.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([ 101, 1996, 6840, 1037, 1032, 1038, 1997, 2724, 1037, 1998, 1038, 3774,
        1997, 2074, 2028, 9560, 1010, 1996, 3356, 2157, 3420, 1012, 2023, 2724,
        2038, 1996, 9723, 1052, 1006, 1037, 1032, 1038, 1007, 1091, 1015, 1013,
        4029, 1012,  102])"
1132,1,['independent'], Sample Space Events and Probability,seg_243,"thus, a and b are not independent by the definition in example 4.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([ 101, 2947, 1010, 1037, 1998, 1038, 2024, 2025, 2981, 2011, 1996, 6210,
        1999, 2742, 1018, 1012,  102])"
1133,1,"['probabilities', 'conditional', 'independent']", Sample Space Events and Probability,seg_243,another way to see that a and b are not independent is to calculate the conditional as well as unconditional probabilities:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2178,  2126,  2000,  2156,  2008,  1037,  1998,  1038,  2024,
         2025,  2981,  2003,  2000, 18422,  1996, 18462,  2004,  2092,  2004,
         4895,  8663, 27064,  4013,  3676, 14680,  1024,   102])"
1134,1,"['conditional probability', 'probability', 'events', 'event', 'conditional', 'probability of event', 'independent']", Sample Space Events and Probability,seg_243,"two events a and b are independent, if the conditional probability of event b occurring given that event a occurred is equal to the unconditional probability of b occurring. in other words, knowledge of occurrence of a has no influence on the probability of b.",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2048,  2824,  1037,  1998,  1038,  2024,  2981,  1010,  2065,
         1996, 18462,  9723,  1997,  2724,  1038, 10066,  2445,  2008,  2724,
         1037,  4158,  2003,  5020,  2000,  1996,  4895,  8663, 27064,  9723,
         1997,  1038, 10066,  1012,  1999,  2060,  2616,  1010,  3716,  1997,
        14404,  1997,  1037,  2038,  2053,  3747,  2006,  1996,  9723,  1997,
         1038,  1012,   102])"
1135,1,"['conditional probability', 'probability', 'outcomes', 'event', 'outcome', 'conditional', 'probability of event']", Sample Space Events and Probability,seg_243,"the conditional probability of event b given that a occurred is denoted by p(b|a). in the first row, there are six outcomes (event a). out of these, one outcome is included in b. hence, we see that p(b|a) ¼ 1/6.",tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  1996, 18462,  9723,  1997,  2724,  1038,  2445,  2008,  1037,
         4158,  2003, 19537,  2011,  1052,  1006,  1038,  1064,  1037,  1007,
         1012,  1999,  1996,  2034,  5216,  1010,  2045,  2024,  2416, 13105,
         1006,  2724,  1037,  1007,  1012,  2041,  1997,  2122,  1010,  2028,
         9560,  2003,  2443,  1999,  1038,  1012,  6516,  1010,  2057,  2156,
         2008,  1052,  1006,  1038,  1064,  1037,  1007,  1091,  1015,  1013,
         1020,  1012,   102])"
1136,1,['probability'], Sample Space Events and Probability,seg_243,the probability of b was found to be 7/12.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([ 101, 1996, 9723, 1997, 1038, 2001, 2179, 2000, 2022, 1021, 1013, 2260,
        1012,  102])"
1137,1,['independent'], Sample Space Events and Probability,seg_243,we once again see that a and b are not independent.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([ 101, 2057, 2320, 2153, 2156, 2008, 1037, 1998, 1038, 2024, 2025, 2981,
        1012,  102])"
1138,1,"['probability', 'table']", Sample Space Events and Probability,seg_243,"actually, in the table above, we can easily see how the probability of getting at least seven eyes in total depends on the number of eyes obtained in the first throw.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([ 101, 2941, 1010, 1999, 1996, 2795, 2682, 1010, 2057, 2064, 4089, 2156,
        2129, 1996, 9723, 1997, 2893, 2012, 2560, 2698, 2159, 1999, 2561, 9041,
        2006, 1996, 2193, 1997, 2159, 4663, 1999, 1996, 2034, 5466, 1012,  102])"
1139,1,['probability'], Sample Space Events and Probability,seg_243,"the larger the number of eyes in the first throw, the larger the probability of getting at least seven eyes in total.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([ 101, 1996, 3469, 1996, 2193, 1997, 2159, 1999, 1996, 2034, 5466, 1010,
        1996, 3469, 1996, 9723, 1997, 2893, 2012, 2560, 2698, 2159, 1999, 2561,
        1012,  102])"
1140,1,['probability'], Sample Space Events and Probability,seg_243,"if the number of eyes in the first throw is 6, the probability of at least 7 eyes in total is actually 100%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([ 101, 2065, 1996, 2193, 1997, 2159, 1999, 1996, 2034, 5466, 2003, 1020,
        1010, 1996, 9723, 1997, 2012, 2560, 1021, 2159, 1999, 2561, 2003, 2941,
        2531, 1003, 1012,  102])"
1141,1,"['risk', 'probability', 'events', 'dependent events', 'dependent', 'independent']", Sample Space Events and Probability,seg_243,"if two events are not independent, they are said to be dependent. dependent events occur frequently in real life! for instance, the probability (or risk) of a person developing lung cancer is dependent on whether or not that person is a smoker: for a nonsmoker, the risk might be 1%; for a smoker, the risk might be as high as 10%.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([7099, 2686, 2824, 1998, 9723])","tensor([  101,  2065,  2048,  2824,  2024,  2025,  2981,  1010,  2027,  2024,
         2056,  2000,  2022,  7790,  1012,  7790,  2824,  5258,  4703,  1999,
         2613,  2166,   999,  2005,  6013,  1010,  1996,  9723,  1006,  2030,
         3891,  1007,  1997,  1037,  2711,  4975, 11192,  4456,  2003,  7790,
         2006,  3251,  2030,  2025,  2008,  2711,  2003,  1037,  5610,  2099,
         1024,  2005,  1037,  2512, 25855,  5484,  1010,  1996,  3891,  2453,
         2022,  1015,  1003,  1025,  2005,  1037,  5610,  2099,  1010,  1996,
         3891,  2453,  2022,  2004,  2152,  2004,  2184,  1003,  1012,   102])"
1142,1,"['function', 'sample space', 'random variable', 'sample', 'random', 'variable ', 'variable']", Random Variables the Binomial Distribution,seg_245,a random variable (*) is a mathematical function on the sample space.,tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([ 101, 1037, 6721, 8023, 1006, 1008, 1007, 2003, 1037, 8045, 3853, 2006,
        1996, 7099, 2686, 1012,  102])"
1143,1,"['vary', 'function', 'sample space', 'outcomes', 'random variable', 'sample', 'random', 'variable']", Random Variables the Binomial Distribution,seg_245,"the mathematical function will often be the identity! for instance, in example 1, the sample space consists of the outcomes 1, 2, 3, 4, 5, or 6 (eyes on a dice). the random variable is simply the number itself, i.e., the number of eyes shown; no mathematical operation is done! this number will vary randomly, hence the term random variable.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1996,  8045,  3853,  2097,  2411,  2022,  1996,  4767,   999,
         2005,  6013,  1010,  1999,  2742,  1015,  1010,  1996,  7099,  2686,
         3774,  1997,  1996, 13105,  1015,  1010,  1016,  1010,  1017,  1010,
         1018,  1010,  1019,  1010,  2030,  1020,  1006,  2159,  2006,  1037,
        18740,  1007,  1012,  1996,  6721,  8023,  2003,  3432,  1996,  2193,
         2993,  1010,  1045,  1012,  1041,  1012,  1010,  1996,  2193,  1997,
         2159,  3491,  1025,  2053,  8045,  3169,  2003,  2589,   999,  2023,
         2193,  2097,  8137, 18154,  1010,  6516,  1996,  2744,  6721,  8023,
         1012,   102])"
1144,1,"['sample space', 'outcomes', 'random variable', 'sample', 'random', 'variable']", Random Variables the Binomial Distribution,seg_245,"in example 2, the total number of eyes in two throws with a dice is a random variable: the sample space consists of 36 pairs of numbers of eyes in each throw. for each of the possible outcomes, the total number of eyes can be calculated by adding the number of eyes in each throw.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1999,  2742,  1016,  1010,  1996,  2561,  2193,  1997,  2159,
         1999,  2048, 11618,  2007,  1037, 18740,  2003,  1037,  6721,  8023,
         1024,  1996,  7099,  2686,  3774,  1997,  4029,  7689,  1997,  3616,
         1997,  2159,  1999,  2169,  5466,  1012,  2005,  2169,  1997,  1996,
         2825, 13105,  1010,  1996,  2561,  2193,  1997,  2159,  2064,  2022,
        10174,  2011,  5815,  1996,  2193,  1997,  2159,  1999,  2169,  5466,
         1012,   102])"
1145,1,"['function', 'discrete random variable', 'sample space', 'discrete', 'outcomes', 'random variable', 'sample', 'random', 'variable']", Random Variables the Binomial Distribution,seg_245,"the result is a mathematical function of the sample space. as the sample space in the example is finite (36 outcomes), the random variable is a discrete random variable.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1996,  2765,  2003,  1037,  8045,  3853,  1997,  1996,  7099,
         2686,  1012,  2004,  1996,  7099,  2686,  1999,  1996,  2742,  2003,
        10713,  1006,  4029, 13105,  1007,  1010,  1996,  6721,  8023,  2003,
         1037, 16246,  6721,  8023,  1012,   102])"
1146,1,"['discrete random variable', 'discrete', 'sample space', 'random variable', 'sample', 'random', 'limit', 'variable']", Random Variables the Binomial Distribution,seg_245,"a discrete random variable need not have a finite sample space. one example is the number of flashes of lightning in a thunderstorm. there is no upper limit to this random variable; however, the sample space is still discrete, as only integer values are possible (0, 1, 2, 3, 4, etc.).",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,
        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1037, 16246,  6721,  8023,  2342,  2025,  2031,  1037, 10713,
         7099,  2686,  1012,  2028,  2742,  2003,  1996,  2193,  1997, 16121,
         1997,  7407,  1999,  1037,  8505, 19718,  1012,  2045,  2003,  2053,
         3356,  5787,  2000,  2023,  6721,  8023,  1025,  2174,  1010,  1996,
         7099,  2686,  2003,  2145, 16246,  1010,  2004,  2069, 16109,  5300,
         2024,  2825,  1006,  1014,  1010,  1015,  1010,  1016,  1010,  1017,
         1010,  1018,  1010,  4385,  1012,  1007,  1012,   102])"
1147,1,"['continuous', 'discrete random variable', 'normal distribution', 'discrete', 'contrast', 'random variable', 'random', 'normal', 'measurement', 'distribution', 'data', 'variable', 'case']", Random Variables the Binomial Distribution,seg_245,"in contrast to a discrete random variable, a continuous random variablemay take any fractional value; this will be the case with many measurement data, where data values can be any real number (or any non-negative number). such data are often described by the normal distribution.",tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        1., 1., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1999,  5688,  2000,  1037, 16246,  6721,  8023,  1010,  1037,
         7142,  6721,  8023, 27871,  2202,  2151, 12884,  2389,  3643,  1025,
         2023,  2097,  2022,  1996,  2553,  2007,  2116, 10903,  2951,  1010,
         2073,  2951,  5300,  2064,  2022,  2151,  2613,  2193,  1006,  2030,
         2151,  2512,  1011,  4997,  2193,  1007,  1012,  2107,  2951,  2024,
         2411,  2649,  2011,  1996,  3671,  4353,  1012,   102])"
1148,1,"['discrete', 'discrete random variables', 'random', 'random variables', 'distribution', 'variables', 'binomial', 'binomial distribution']", Random Variables the Binomial Distribution,seg_245,"in this appendix, we will only cover discrete random variables. the binomial distribution is the most important distribution used to describe discrete random variables.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
        0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1999,  2023, 22524,  1010,  2057,  2097,  2069,  3104, 16246,
         6721, 10857,  1012,  1996,  8026, 20936,  2389,  4353,  2003,  1996,
         2087,  2590,  4353,  2109,  2000,  6235, 16246,  6721, 10857,  1012,
          102])"
1149,1,"['distribution', 'binomial', 'binomial distribution']", Random Variables the Binomial Distribution,seg_245,the binomial distribution (*) is used when the following conditions are satisfied:,tensor(1),"tensor([0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1996,  8026, 20936,  2389,  4353,  1006,  1008,  1007,  2003,
         2109,  2043,  1996,  2206,  3785,  2024,  8510,  1024,   102])"
1150,1,"['categories', 'observation', 'probability', 'observations', 'control', 'quality control', 'statistical', 'independent']", Random Variables the Binomial Distribution,seg_245,"– each observation (or “trial”) can be classified into two categories. often, we call them “success” and “failure” regardless of whether one of the categories can be said to be “better” than the other. – the probability that an observation is classified as “success” is constant. for example, in statistical quality control there must not be a trend that defective items become more frequent. – the observations are independent. this means, for example, that two respondents do not affect each others answers in a questionnaire survey.",tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1516,  2169,  8089,  1006,  2030,  1523,  3979,  1524,  1007,
         2064,  2022,  6219,  2046,  2048,  7236,  1012,  2411,  1010,  2057,
         2655,  2068,  1523,  3112,  1524,  1998,  1523,  4945,  1524,  7539,
         1997,  3251,  2028,  1997,  1996,  7236,  2064,  2022,  2056,  2000,
         2022,  1523,  2488,  1524,  2084,  1996,  2060,  1012,  1516,  1996,
         9723,  2008,  2019,  8089,  2003,  6219,  2004,  1523,  3112,  1524,
         2003,  5377,  1012,  2005,  2742,  1010,  1999,  7778,  3737,  2491,
         2045,  2442,  2025,  2022,  1037,  9874,  2008, 28829,  5167,  2468,
         2062,  6976,  1012,  1516,  1996,  9420,  2024,  2981,  1012,  2023,
         2965,  1010,  2005,  2742,  1010,  2008,  2048, 25094,  2079,  2025,
         7461,  2169,  2500,  6998,  1999,  1037,  3160, 20589,  5002,  1012,
          102])"
1151,1,"['trial', 'probability', 'observations', 'random variable', 'number of observations', 'trials', 'sample', 'random', 'sample size', 'successes', 'variable']", Random Variables the Binomial Distribution,seg_245,"– n is the sample size, i.e., number of observations (trials) – p is (the constant) probability of “success” in each trial – 1 p is probability of “failure” in each trial – x is a random variable indicating the number of successes out of n trials – x is the actual number of successes in a specific sample of n trials",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1516,  1050,  2003,  1996,  7099,  2946,  1010,  1045,  1012,
         1041,  1012,  1010,  2193,  1997,  9420,  1006,  7012,  1007,  1516,
         1052,  2003,  1006,  1996,  5377,  1007,  9723,  1997,  1523,  3112,
         1524,  1999,  2169,  3979,  1516,  1015,  1052,  2003,  9723,  1997,
         1523,  4945,  1524,  1999,  2169,  3979,  1516,  1060,  2003,  1037,
         6721,  8023,  8131,  1996,  2193,  1997, 14152,  2041,  1997,  1050,
         7012,  1516,  1060,  2003,  1996,  5025,  2193,  1997, 14152,  1999,
         1037,  3563,  7099,  1997,  1050,  7012,   102])"
1152,1,"['probability', 'observations', 'trials', 'successes']", Random Variables the Binomial Distribution,seg_245,"p(x ¼ x) is the probability of obtaining exactly x successes out of n observations or trials. in chap. 5, we showed graphs of this probability. we also showed how to calculate this probability using a spreadsheet.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1052,  1006,  1060,  1091,  1060,  1007,  2003,  1996,  9723,
         1997, 11381,  3599,  1060, 14152,  2041,  1997,  1050,  9420,  2030,
         7012,  1012,  1999, 15775,  2361,  1012,  1019,  1010,  2057,  3662,
        19287,  1997,  2023,  9723,  1012,  2057,  2036,  3662,  2129,  2000,
        18422,  2023,  9723,  2478,  1037, 20861, 21030,  2102,  1012,   102])"
1153,1,"['trial', 'probability', 'probability of success', 'probabilities', 'trials', 'successes', 'success', 'independent']", Random Variables the Binomial Distribution,seg_245,"here, we will derive the mathematical expression of this probability. step 1: the probability of x successes in the first x trials is px (where p is the probability of success in each trial), as the probabilities should be multiplied. this follows by the fact that the trials are independent.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,
        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  2182,  1010,  2057,  2097, 18547,  1996,  8045,  3670,  1997,
         2023,  9723,  1012,  3357,  1015,  1024,  1996,  9723,  1997,  1060,
        14152,  1999,  1996,  2034,  1060,  7012,  2003,  1052,  2595,  1006,
         2073,  1052,  2003,  1996,  9723,  1997,  3112,  1999,  2169,  3979,
         1007,  1010,  2004,  1996,  4013,  3676, 14680,  2323,  2022, 28608,
         1012,  2023,  4076,  2011,  1996,  2755,  2008,  1996,  7012,  2024,
         2981,  1012,   102])"
1154,1,"['probability', 'failures', 'trials']", Random Variables the Binomial Distribution,seg_245,the probability of obtaining n x failures in the remaining n x trials can be calculated in the same way and is found to be (1 p)n x.,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1996,  9723,  1997, 11381,  1050,  1060, 15428,  1999,  1996,
         3588,  1050,  1060,  7012,  2064,  2022, 10174,  1999,  1996,  2168,
         2126,  1998,  2003,  2179,  2000,  2022,  1006,  1015,  1052,  1007,
         1050,  1060,  1012,   102])"
1155,1,"['probability', 'failures', 'successes', 'combination']", Random Variables the Binomial Distribution,seg_245,the expression px(1 p)n x is thus the probability of a certain combination of x successes (each having probability p) and n x failures (each having probability 1 p).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1996,  3670,  1052,  2595,  1006,  1015,  1052,  1007,  1050,
         1060,  2003,  2947,  1996,  9723,  1997,  1037,  3056,  5257,  1997,
         1060, 14152,  1006,  2169,  2383,  9723,  1052,  1007,  1998,  1050,
         1060, 15428,  1006,  2169,  2383,  9723,  1015,  1052,  1007,  1012,
          102])"
1156,1,"['probability', 'failures', 'combinations', 'successes']", Random Variables the Binomial Distribution,seg_245,"step 2: however, there are several different combinations of x successes and n x failures. and all of them will have the same probability px(1 p)n x.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  3357,  1016,  1024,  2174,  1010,  2045,  2024,  2195,  2367,
        14930,  1997,  1060, 14152,  1998,  1050,  1060, 15428,  1012,  1998,
         2035,  1997,  2068,  2097,  2031,  1996,  2168,  9723,  1052,  2595,
         1006,  1015,  1052,  1007,  1050,  1060,  1012,   102])"
1157,1,"['events', 'mutually exclusive']", Random Variables the Binomial Distribution,seg_245,"as these events are mutually exclusive (they cannot occur at the same time), we can use the rule of addition.",tensor(1),"tensor([0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  2004,  2122,  2824,  2024, 20271,  7262,  1006,  2027,  3685,
         5258,  2012,  1996,  2168,  2051,  1007,  1010,  2057,  2064,  2224,
         1996,  3627,  1997,  2804,  1012,   102])"
1158,1,"['probability', 'failures', 'trials', 'combinations', 'successes']", Random Variables the Binomial Distribution,seg_245,"thus, the probability of x successes in n trials will be px(1 p)n x multiplied by the number of different combinations of x successes and n x failures.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,
        0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  2947,  1010,  1996,  9723,  1997,  1060, 14152,  1999,  1050,
         7012,  2097,  2022,  1052,  2595,  1006,  1015,  1052,  1007,  1050,
         1060, 28608,  2011,  1996,  2193,  1997,  2367, 14930,  1997,  1060,
        14152,  1998,  1050,  1060, 15428,  1012,   102])"
1159,1,"['failures', 'combinations', 'successes']", Random Variables the Binomial Distribution,seg_245,step 3: we need an expression for the number of different combinations of x successes and n x failures.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        0., 0., 1., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  3357,  1017,  1024,  2057,  2342,  2019,  3670,  2005,  1996,
         2193,  1997,  2367, 14930,  1997,  1060, 14152,  1998,  1050,  1060,
        15428,  1012,   102])"
1160,1,"['function', 'binomial coefficient', 'coefficient', 'sample', 'combinations', 'binomial']", Random Variables the Binomial Distribution,seg_245,"the number of combinations (*) of n individuals, when we take a sample of x is often written ðn xþ, reading “n over x.” this is also called the binomial coefficient. it is tabulated in many textbooks for small values of n. it can also be found in a spreadsheet using the function combin. a mathematical formula for the binomial coefficient can be found (see technical note at end of this appendix):",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1996,  2193,  1997, 14930,  1006,  1008,  1007,  1997,  1050,
         3633,  1010,  2043,  2057,  2202,  1037,  7099,  1997,  1060,  2003,
         2411,  2517,  1098,  2078,  1060, 29670,  1010,  3752,  1523,  1050,
         2058,  1060,  1012,  1524,  2023,  2003,  2036,  2170,  1996,  8026,
        20936,  2389, 19064,  1012,  2009,  2003, 21628,  8898,  1999,  2116,
        18841,  2005,  2235,  5300,  1997,  1050,  1012,  2009,  2064,  2036,
         2022,  2179,  1999,  1037, 20861, 21030,  2102,  2478,  1996,  3853,
        22863,  2378,  1012,  1037,  8045,  5675,  2005,  1996,  8026, 20936,
         2389, 19064,  2064,  2022,  2179,  1006,  2156,  4087,  3602,  2012,
         2203,  1997,  2023, 22524,  1007,  1024,   102])"
1161,1,"['probability', 'trials', 'successes']", Random Variables the Binomial Distribution,seg_245,in a spreadsheet cell gives the result 6. this gives us the desired formula for the probability of x successes in n trials:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.])","tensor([ 6721, 10857,  1996,  8026, 20936,  2389,  4353])","tensor([  101,  1999,  1037, 20861, 21030,  2102,  3526,  3957,  1996,  2765,
         1020,  1012,  2023,  3957,  2149,  1996,  9059,  5675,  2005,  1996,
         9723,  1997,  1060, 14152,  1999,  1050,  7012,  1024,   102])"
1162,1,"['discrete random variable', 'discrete', 'random variable', 'random', 'variable']", Random Variables Mean and Variance,seg_247,let x be a discrete random variable.,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])","tensor([ 6721, 10857,  2812,  1998, 23284])","tensor([  101,  2292,  1060,  2022,  1037, 16246,  6721,  8023,  1012,   102])"
1163,1,"['mean', 'expectation']", Random Variables Mean and Variance,seg_247,the mean (or expectation) of x is defined as,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10857,  2812,  1998, 23284])","tensor([  101,  1996,  2812,  1006,  2030, 17626,  1007,  1997,  1060,  2003,
         4225,  2004,   102])"
1164,1,['variance'], Random Variables Mean and Variance,seg_247,the variance of x is defined as,tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10857,  2812,  1998, 23284])","tensor([  101,  1996, 23284,  1997,  1060,  2003,  4225,  2004,   102])"
1165,1,"['deviation', 'variance', 'standard deviation', 'standard']", Random Variables Mean and Variance,seg_247,"the standard deviation of x is defined as the square root of the variance, i.e.,",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 6721, 10857,  2812,  1998, 23284])","tensor([  101,  1996,  3115, 24353,  1997,  1060,  2003,  4225,  2004,  1996,
         2675,  7117,  1997,  1996, 23284,  1010,  1045,  1012,  1041,  1012,
         1010,   102])"
1166,1,"['mean', 'variance', 'random variable', 'random', 'summation', 'variable']", Random Variables Mean and Variance,seg_247,"in the expressions for the mean and variance, summation is over all possible values x of the random variable x.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0.])","tensor([ 6721, 10857,  2812,  1998, 23284])","tensor([  101,  1999,  1996, 11423,  2005,  1996,  2812,  1998, 23284,  1010,
         7680, 28649,  2003,  2058,  2035,  2825,  5300,  1060,  1997,  1996,
         6721,  8023,  1060,  1012,   102])"
1167,1,"['continuous', 'mean', 'variance', 'continuous random variables', 'random', 'random variables', 'variables']", Random Variables Mean and Variance,seg_247,"for continuous random variables, the concepts mean and variance can also be defined.",tensor(1),"tensor([0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10857,  2812,  1998, 23284])","tensor([  101,  2005,  7142,  6721, 10857,  1010,  1996,  8474,  2812,  1998,
        23284,  2064,  2036,  2022,  4225,  1012,   102])"
1168,1,"['probability', 'summation', 'probability theory']", Random Variables Mean and Variance,seg_247,"however, summation should be replaced by integration, which is defined in the mathematical discipline calculus. we will not go further into this; see advanced textbooks on probability theory.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.])","tensor([ 6721, 10857,  2812,  1998, 23284])","tensor([  101,  2174,  1010,  7680, 28649,  2323,  2022,  2999,  2011,  8346,
         1010,  2029,  2003,  4225,  1999,  1996,  8045,  9009, 19276,  1012,
         2057,  2097,  2025,  2175,  2582,  2046,  2023,  1025,  2156,  3935,
        18841,  2006,  9723,  3399,  1012,   102])"
1169,1,"['successes', 'random variable', 'trials', 'random', 'distribution', 'summation', 'binomial', 'case', 'variable', 'binomial distribution']", Random Variables Mean and Variance,seg_247,"let the random variable x be the number of successes out of n trials, which follows a binomial distribution. in this case, summation in the expressions for e(x) and v(x) is over all values from 0 to n.",tensor(1),"tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10857,  2812,  1998, 23284])","tensor([  101,  2292,  1996,  6721,  8023,  1060,  2022,  1996,  2193,  1997,
        14152,  2041,  1997,  1050,  7012,  1010,  2029,  4076,  1037,  8026,
        20936,  2389,  4353,  1012,  1999,  2023,  2553,  1010,  7680, 28649,
         1999,  1996, 11423,  2005,  1041,  1006,  1060,  1007,  1998,  1058,
         1006,  1060,  1007,  2003,  2058,  2035,  5300,  2013,  1014,  2000,
         1050,  1012,   102])"
1170,1,['probabilities'], Random Variables Mean and Variance,seg_247,we have derived the probabilities p(x ¼ x) above. it can be shown mathematically that inserting,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10857,  2812,  1998, 23284])","tensor([  101,  2057,  2031,  5173,  1996,  4013,  3676, 14680,  1052,  1006,
         1060,  1091,  1060,  1007,  2682,  1012,  2009,  2064,  2022,  3491,
         8045,  2135,  2008, 19274,  2075,   102])"
1171,1,"['mean', 'variance', 'distribution', 'binomial', 'binomial distribution']", Random Variables Mean and Variance,seg_247,these are exactly the expressions for the mean and variance of a binomial distribution shown in chap. 5.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 6721, 10857,  2812,  1998, 23284])","tensor([  101,  2122,  2024,  3599,  1996, 11423,  2005,  1996,  2812,  1998,
        23284,  1997,  1037,  8026, 20936,  2389,  4353,  3491,  1999, 15775,
         2361,  1012,  1019,  1012,   102])"
1172,1,"['sample', 'combinations']", Technical Note The Binomial Coefficient,seg_249,we want to determine the number of combinations (or groups) of n individuals when taking a sample of x.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0.])","tensor([ 4087,  3602,  1996,  8026, 20936,  2389, 19064])","tensor([  101,  2057,  2215,  2000,  5646,  1996,  2193,  1997, 14930,  1006,
         2030,  2967,  1007,  1997,  1050,  3633,  2043,  2635,  1037,  7099,
         1997,  1060,  1012,   102])"
1173,1,"['sample', 'permutations']", Technical Note The Binomial Coefficient,seg_249,"first, determine the number of permutations (ordered groups) of n individuals, when taking a sample of x. in this way, (a, b) and (b, a) are considered two different groups.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4087,  3602,  1996,  8026, 20936,  2389, 19064])","tensor([  101,  2034,  1010,  5646,  1996,  2193,  1997,  2566, 28120, 10708,
         1006,  3641,  2967,  1007,  1997,  1050,  3633,  1010,  2043,  2635,
         1037,  7099,  1997,  1060,  1012,  1999,  2023,  2126,  1010,  1006,
         1037,  1010,  1038,  1007,  1998,  1006,  1038,  1010,  1037,  1007,
         2024,  2641,  2048,  2367,  2967,  1012,   102])"
1174,0,['n'], Technical Note The Binomial Coefficient,seg_249,"for instance, with n ¼ 4 persons labeled a, b, c, d, we select ordered groups of x ¼ 2 persons. in order to find out how many ordered groups exist, we first select person no. 1. this can be done in n ¼ 4 ways. then we select person no. 2. with three persons left, this can be done in n 1 ¼ 3 ways.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 4087,  3602,  1996,  8026, 20936,  2389, 19064])","tensor([  101,  2005,  6013,  1010,  2007,  1050,  1091,  1018,  5381, 12599,
         1037,  1010,  1038,  1010,  1039,  1010,  1040,  1010,  2057,  7276,
         3641,  2967,  1997,  1060,  1091,  1016,  5381,  1012,  1999,  2344,
         2000,  2424,  2041,  2129,  2116,  3641,  2967,  4839,  1010,  2057,
         2034,  7276,  2711,  2053,  1012,  1015,  1012,  2023,  2064,  2022,
         2589,  1999,  1050,  1091,  1018,  3971,  1012,  2059,  2057,  7276,
         2711,  2053,  1012,  1016,  1012,  2007,  2093,  5381,  2187,  1010,
         2023,  2064,  2022,  2589,  1999,  1050,  1015,  1091,  1017,  3971,
         1012,   102])"
1175,0,[], Technical Note The Binomial Coefficient,seg_249,"in total, we can select 4 3 ¼ 12 ordered groups.",tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tensor([ 4087,  3602,  1996,  8026, 20936,  2389, 19064])","tensor([ 101, 1999, 2561, 1010, 2057, 2064, 7276, 1018, 1017, 1091, 2260, 3641,
        2967, 1012,  102])"
1176,1,"['factors', 'sample', 'permutations']", Technical Note The Binomial Coefficient,seg_249,"generally, the number of permutations of n objects, when taking a sample of x objects, is equal to n(n 1) (n x þ 1). in this expression, the total number of factors is x.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])","tensor([ 4087,  3602,  1996,  8026, 20936,  2389, 19064])","tensor([  101,  3227,  1010,  1996,  2193,  1997,  2566, 28120, 10708,  1997,
         1050,  5200,  1010,  2043,  2635,  1037,  7099,  1997,  1060,  5200,
         1010,  2003,  5020,  2000,  1050,  1006,  1050,  1015,  1007,  1006,
         1050,  1060,  1101,  1015,  1007,  1012,  1999,  2023,  3670,  1010,
         1996,  2561,  2193,  1997,  5876,  2003,  1060,  1012,   102])"
1177,1,['function'], Technical Note The Binomial Coefficient,seg_249,"this number can be found in spreadsheets using the function permut. for instance, entering",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0.])","tensor([ 4087,  3602,  1996,  8026, 20936,  2389, 19064])","tensor([  101,  2023,  2193,  2064,  2022,  2179,  1999, 20861, 21030,  3215,
         2478,  1996,  3853,  2566, 28120,  1012,  2005,  6013,  1010,  5738,
          102])"
1178,1,"['sample', 'combinations', 'permutations']", Technical Note The Binomial Coefficient,seg_249,"in a spreadsheet cell gives the result 12. from the number of permutations, we can find the number of combinations. in the example, we have counted (a, b) and (b, a) as two different groups; we determined the number of permutations to be 12. in order to find the number of combinations of four objects, when taking a sample of 2, we divide 12 by 2, obtaining 6.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])","tensor([ 4087,  3602,  1996,  8026, 20936,  2389, 19064])","tensor([  101,  1999,  1037, 20861, 21030,  2102,  3526,  3957,  1996,  2765,
         2260,  1012,  2013,  1996,  2193,  1997,  2566, 28120, 10708,  1010,
         2057,  2064,  2424,  1996,  2193,  1997, 14930,  1012,  1999,  1996,
         2742,  1010,  2057,  2031,  8897,  1006,  1037,  1010,  1038,  1007,
         1998,  1006,  1038,  1010,  1037,  1007,  2004,  2048,  2367,  2967,
         1025,  2057,  4340,  1996,  2193,  1997,  2566, 28120, 10708,  2000,
         2022,  2260,  1012,  1999,  2344,  2000,  2424,  1996,  2193,  1997,
        14930,  1997,  2176,  5200,  1010,  2043,  2635,  1037,  7099,  1997,
         1016,  1010,  2057, 11443,  2260,  2011,  1016,  1010, 11381,  1020,
         1012,   102])"
1179,1,"['case', 'permutations']", Technical Note The Binomial Coefficient,seg_249,"in the general case, we divide the number of permutations n(n 1) (n x þ1) by the number of permutations of x individuals, which is x(x 1) 2 1.",tensor(1),"tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 4087,  3602,  1996,  8026, 20936,  2389, 19064])","tensor([  101,  1999,  1996,  2236,  2553,  1010,  2057, 11443,  1996,  2193,
         1997,  2566, 28120, 10708,  1050,  1006,  1050,  1015,  1007,  1006,
         1050,  1060,  1101,  2487,  1007,  2011,  1996,  2193,  1997,  2566,
        28120, 10708,  1997,  1060,  3633,  1010,  2029,  2003,  1060,  1006,
         1060,  1015,  1007,  1016,  1015,  1012,   102])"
1180,1,"['sample', 'combinations']", Technical Note The Binomial Coefficient,seg_249,"thus, we obtain the number of combinations of n individuals, when taking a sample of x:",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0.])","tensor([ 4087,  3602,  1996,  8026, 20936,  2389, 19064])","tensor([  101,  2947,  1010,  2057,  6855,  1996,  2193,  1997, 14930,  1997,
         1050,  3633,  1010,  2043,  2635,  1037,  7099,  1997,  1060,  1024,
          102])"
1181,1,"['binomial coefficient', 'coefficient', 'binomial']", Technical Note The Binomial Coefficient,seg_249,"with n! (read “n factorial”) meaning all the numbers 1, 2, etc., up to n multiplied together, the binomial coefficient can also be written as",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,
        0., 0., 0., 0.])","tensor([ 4087,  3602,  1996,  8026, 20936,  2389, 19064])","tensor([  101,  2007,  1050,   999,  1006,  3191,  1523,  1050,  5387,  4818,
         1524,  1007,  3574,  2035,  1996,  3616,  1015,  1010,  1016,  1010,
         4385,  1012,  1010,  2039,  2000,  1050, 28608,  2362,  1010,  1996,
         8026, 20936,  2389, 19064,  2064,  2036,  2022,  2517,  2004,   102])"
1182,1,['statistical'], Summary of Statistical Methods,seg_251,important points to clarify when doing a statistical analysis:,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])","tensor([12654,  1997,  7778,  4725])","tensor([  101,  2590,  2685,  2000, 25037,  2043,  2725,  1037,  7778,  4106,
         1024,   102])"
1183,1,"['confidence interval', 'interval', 'statistical test', 'quantitative', 'statistical', 'confidence', 'data', 'test']", Summary of Statistical Methods,seg_251,1. quantitative or qualitative data? 2. one group or two groups? 3. confidence interval or statistical test?,tensor(1),"tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 1., 1., 0., 0.])","tensor([12654,  1997,  7778,  4725])","tensor([  101,  1015,  1012, 20155,  2030, 24209, 11475, 27453,  2951,  1029,
         1016,  1012,  2028,  2177,  2030,  2048,  2967,  1029,  1017,  1012,
         7023, 13483,  2030,  7778,  3231,  1029,   102])"
1184,1,"['descriptive statistics', 'statistics']", Quantitative Data,seg_253,9.2.1.1 descriptive statistics,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.])","tensor([20155,  2951])","tensor([  101,  1023,  1012,  1016,  1012,  1015,  1012,  1015, 22726,  6747,
          102])"
1185,1,['table'], Quantitative Data,seg_253,see chap. 3 (table 9.1).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([20155,  2951])","tensor([  101,  2156, 15775,  2361,  1012,  1017,  1006,  2795,  1023,  1012,
         1015,  1007,  1012,   102])"
1186,1,"['normal', 'normal distribution', 'distribution']", Quantitative Data,seg_253,9.2.1.2 the normal distribution,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.])","tensor([20155,  2951])","tensor([ 101, 1023, 1012, 1016, 1012, 1015, 1012, 1016, 1996, 3671, 4353,  102])"
1187,1,['table'], Quantitative Data,seg_253,probabilities (table 9.2).,tensor(1),"tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([20155,  2951])","tensor([  101,  4013,  3676, 14680,  1006,  2795,  1023,  1012,  1016,  1007,
         1012,   102])"
1188,1,"['normal', 'normal distribution', 'distribution']", Quantitative Data,seg_253,testing for the normal distribution,tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0.])","tensor([20155,  2951])","tensor([ 101, 5604, 2005, 1996, 3671, 4353,  102])"
1189,0,[], Quantitative Data,seg_253,1. simple methods,tensor(0),"tensor([0, 0, 0, 0, 0, 0])","tensor([20155,  2951])","tensor([ 101, 1015, 1012, 3722, 4725,  102])"
1190,1,"['mean', 'range', 'deviation', 'symmetric', 'interquartile range', 'histogram', 'standard deviation', 'intervals', 'standard', 'average', 'data', 'median']", Quantitative Data,seg_253,– the histogram – the average ¼ the median – interquartile range larger than the standard deviation – number of data values in symmetric intervals around the mean,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,
        0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.])","tensor([20155,  2951])","tensor([  101,  1516,  1996,  2010,  3406, 13113,  1516,  1996,  2779,  1091,
         1996,  3991,  1516,  6970, 16211, 28228,  2571,  2846,  3469,  2084,
         1996,  3115, 24353,  1516,  2193,  1997,  2951,  5300,  1999, 19490,
        14025,  2105,  1996,  2812,   102])"
1191,1,"['kurtosis', 'skewness']", Quantitative Data,seg_253,2. skewness and kurtosis,tensor(1),"tensor([0., 0., 0., 1., 1., 1., 0., 1., 1., 0.])","tensor([20155,  2951])","tensor([  101,  1016,  1012, 15315,  7974,  2791,  1998,  9679, 12650,   102])"
1192,0,[], Quantitative Data,seg_253,– calculate in spreadsheet – reasonably close to 0? compare with min. and max. limits in chap. 4.,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])","tensor([20155,  2951])","tensor([  101,  1516, 18422,  1999, 20861, 21030,  2102,  1516, 16286,  2485,
         2000,  1014,  1029, 12826,  2007,  8117,  1012,  1998,  4098,  1012,
         6537,  1999, 15775,  2361,  1012,  1018,  1012,   102])"
1193,1,"['plot', 'normal']", Quantitative Data,seg_253,3. normal plot,tensor(1),"tensor([0., 0., 0., 1., 1., 0.])","tensor([20155,  2951])","tensor([ 101, 1017, 1012, 3671, 5436,  102])"
1194,1,"['intervals', 'confidence intervals', 'confidence']", Quantitative Data,seg_253,9.2.1.3 confidence intervals: one group,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([20155,  2951])","tensor([  101,  1023,  1012,  1016,  1012,  1015,  1012,  1017,  7023, 14025,
         1024,  2028,  2177,   102])"
1195,1,['table'], Quantitative Data,seg_253,see chap. 4 (table 9.3).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([20155,  2951])","tensor([  101,  2156, 15775,  2361,  1012,  1018,  1006,  2795,  1023,  1012,
         1017,  1007,  1012,   102])"
1196,1,"['intervals', 'confidence intervals', 'confidence']", Quantitative Data,seg_253,9.2.1.4 confidence intervals: two groups,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([20155,  2951])","tensor([  101,  1023,  1012,  1016,  1012,  1015,  1012,  1018,  7023, 14025,
         1024,  2048,  2967,   102])"
1197,1,['table'], Quantitative Data,seg_253,see chap. 8 (table 9.4).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([20155,  2951])","tensor([  101,  2156, 15775,  2361,  1012,  1022,  1006,  2795,  1023,  1012,
         1018,  1007,  1012,   102])"
1198,1,"['sample', 'sample size']", Quantitative Data,seg_253,9.2.1.5 sample size,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.])","tensor([20155,  2951])","tensor([ 101, 1023, 1012, 1016, 1012, 1015, 1012, 1019, 7099, 2946,  102])"
1199,1,"['deviation', 'uncertainty', 'statistical uncertainty', 'sample', 'standard deviation', 'sample size', 'standard', 'statistical', 'average']", Quantitative Data,seg_253,"if we know the standard deviation s and want a maximum statistical uncertainty u of the average, we find the minimum necessary sample size n as",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.])","tensor([20155,  2951])","tensor([  101,  2065,  2057,  2113,  1996,  3115, 24353,  1055,  1998,  2215,
         1037,  4555,  7778, 12503,  1057,  1997,  1996,  2779,  1010,  2057,
         2424,  1996,  6263,  4072,  7099,  2946,  1050,  2004,   102])"
1200,1,"['sample', 'sample size']", Quantitative Data,seg_253,"if there are two (or more) groups, this is the necessary sample size in each group.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,
        0., 0., 0., 0.])","tensor([20155,  2951])","tensor([ 101, 2065, 2045, 2024, 2048, 1006, 2030, 2062, 1007, 2967, 1010, 2023,
        2003, 1996, 4072, 7099, 2946, 1999, 2169, 2177, 1012,  102])"
1201,1,"['statistical tests', 'tests', 'statistical', 'variables']", Quantitative Data,seg_253,9.2.1.6 statistical tests: two variables or two groups,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.])","tensor([20155,  2951])","tensor([  101,  1023,  1012,  1016,  1012,  1015,  1012,  1020,  7778,  5852,
         1024,  2048, 10857,  2030,  2048,  2967,   102])"
1202,1,['table'], Quantitative Data,seg_253,see chap. 7 and 8 (table 9.5).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([20155,  2951])","tensor([  101,  2156, 15775,  2361,  1012,  1021,  1998,  1022,  1006,  2795,
         1023,  1012,  1019,  1007,  1012,   102])"
1203,1,"['intervals', 'confidence intervals', 'confidence']", Qualitative Data,seg_255,9.2.2.1 confidence intervals: one group,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([24209, 11475, 27453,  2951])","tensor([  101,  1023,  1012,  1016,  1012,  1016,  1012,  1015,  7023, 14025,
         1024,  2028,  2177,   102])"
1204,1,['table'], Qualitative Data,seg_255,see chap. 5 (table 9.6).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([24209, 11475, 27453,  2951])","tensor([  101,  2156, 15775,  2361,  1012,  1019,  1006,  2795,  1023,  1012,
         1020,  1007,  1012,   102])"
1205,1,"['intervals', 'confidence intervals', 'confidence']", Qualitative Data,seg_255,9.2.2.2 confidence intervals: two groups,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])","tensor([24209, 11475, 27453,  2951])","tensor([  101,  1023,  1012,  1016,  1012,  1016,  1012,  1016,  7023, 14025,
         1024,  2048,  2967,   102])"
1206,1,['table'], Qualitative Data,seg_255,see chap. 5 (table 9.7).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([24209, 11475, 27453,  2951])","tensor([  101,  2156, 15775,  2361,  1012,  1019,  1006,  2795,  1023,  1012,
         1021,  1007,  1012,   102])"
1207,1,"['sample', 'sample size']", Qualitative Data,seg_255,9.2.2.3 sample size,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.])","tensor([24209, 11475, 27453,  2951])","tensor([ 101, 1023, 1012, 1016, 1012, 1016, 1012, 1017, 7099, 2946,  102])"
1208,1,"['uncertainty', 'statistical uncertainty', 'sample', 'sample size', 'statistical']", Qualitative Data,seg_255,"if the maximum statistical uncertainty of a proportion is u, we find the minimum necessary sample size n as",tensor(1),"tensor([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0., 0.])","tensor([24209, 11475, 27453,  2951])","tensor([  101,  2065,  1996,  4555,  7778, 12503,  1997,  1037, 10817,  2003,
         1057,  1010,  2057,  2424,  1996,  6263,  4072,  7099,  2946,  1050,
         2004,   102])"
1209,1,['population'], Qualitative Data,seg_255,the above formula can obviously be used in subgroups of the population. the formula then finds the value of n for each subgroup separately.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([24209, 11475, 27453,  2951])","tensor([  101,  1996,  2682,  5675,  2064,  5525,  2022,  2109,  1999, 20576,
         2015,  1997,  1996,  2313,  1012,  1996,  5675,  2059,  4858,  1996,
         3643,  1997,  1050,  2005,  2169, 20576, 10329,  1012,   102])"
1210,1,"['statistical tests', 'tests', 'statistical', 'variables']", Qualitative Data,seg_255,9.2.2.4 statistical tests: two groups or two variables,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.])","tensor([24209, 11475, 27453,  2951])","tensor([  101,  1023,  1012,  1016,  1012,  1016,  1012,  1018,  7778,  5852,
         1024,  2048,  2967,  2030,  2048, 10857,   102])"
1211,1,['table'], Qualitative Data,seg_255,see chap. 5 (table 9.8).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([24209, 11475, 27453,  2951])","tensor([  101,  2156, 15775,  2361,  1012,  1019,  1006,  2795,  1023,  1012,
         1022,  1007,  1012,   102])"
1212,1,"['statistical', 'functions']", Statistical Functions in Spreadsheets,seg_257,this is an overview of the most important statistical (and a few mathematical) functions in microsoft excel and the free spreadsheet openoffice calc. all these functions have the same name and work in the same way in both spreadsheets (and in some other spreadsheets).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0.])","tensor([ 7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  2023,  2003,  2019, 19184,  1997,  1996,  2087,  2590,  7778,
         1006,  1998,  1037,  2261,  8045,  1007,  4972,  1999,  7513, 24970,
         1998,  1996,  2489, 20861, 21030,  2102,  2330,  7245,  6610, 10250,
         2278,  1012,  2035,  2122,  4972,  2031,  1996,  2168,  2171,  1998,
         2147,  1999,  1996,  2168,  2126,  1999,  2119, 20861, 21030,  3215,
         1006,  1998,  1999,  2070,  2060, 20861, 21030,  3215,  1007,  1012,
          102])"
1213,1,['functions'], Statistical Functions in Spreadsheets,seg_257,note: the names of these functions are translated when using spreadsheets in other languages than english!,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])","tensor([ 7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  3602,  1024,  1996,  3415,  1997,  2122,  4972,  2024,  5421,
         2043,  2478, 20861, 21030,  3215,  1999,  2060,  4155,  2084,  2394,
          999,   102])"
1214,1,['table'], Statistical Functions in Spreadsheets,seg_257,we refer to the “help” menu of your spreadsheet for more details (table 9.9).,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0.])","tensor([ 7778,  4972,  1999, 20861, 21030,  3215])","tensor([  101,  2057,  6523,  2000,  1996,  1523,  2393,  1524, 12183,  1997,
         2115, 20861, 21030,  2102,  2005,  2062,  4751,  1006,  2795,  1023,
         1012,  1023,  1007,  1012,   102])"
1215,1,"['function', 'normal distribution', 'normal', 'standard', 'distribution']", Fractiles in the Normal Distribution,seg_261,fractiles in the (standard) normal distribution are calculated in microsoft excel/ openoffice calc using the function normsinv.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([25312,  6593,  9463,  2015,  1999,  1996,  3671,  4353])","tensor([  101, 25312,  6593,  9463,  2015,  1999,  1996,  1006,  3115,  1007,
         3671,  4353,  2024, 10174,  1999,  7513, 24970,  1013,  2330,  7245,
         6610, 10250,  2278,  2478,  1996,  3853, 17606,  2378,  2615,  1012,
          102])"
1216,1,"['fractile', 'table']", Fractiles in the Normal Distribution,seg_261,"example: for the 97.5% ¼ 0.975 fractile, we obtain normsinv(0.975) ¼ 1.960 (table 9.10).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0.])","tensor([25312,  6593,  9463,  2015,  1999,  1996,  3671,  4353])","tensor([  101,  2742,  1024,  2005,  1996,  5989,  1012,  1019,  1003,  1091,
         1014,  1012,  5989,  2629, 25312,  6593,  9463,  1010,  2057,  6855,
        17606,  2378,  2615,  1006,  1014,  1012,  5989,  2629,  1007,  1091,
         1015,  1012, 26637,  1006,  2795,  1023,  1012,  2184,  1007,  1012,
          102])"
1217,1,"['function', 'normal distribution', 'normal', 'standard', 'distribution']", Probabilities in the Normal Distribution,seg_263,probabilities in the (standard) normal distribution are calculated in microsoft excel/ openoffice calc using the function normsdist.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 4013,  3676, 14680,  1999,  1996,  3671,  4353])","tensor([  101,  4013,  3676, 14680,  1999,  1996,  1006,  3115,  1007,  3671,
         4353,  2024, 10174,  1999,  7513, 24970,  1013,  2330,  7245,  6610,
        10250,  2278,  2478,  1996,  3853, 17606, 10521,  2102,  1012,   102])"
1218,1,"['probability', 'table']", Probabilities in the Normal Distribution,seg_263,"example: normsdist(2) ¼ 0.9772. thus, the probability of values 2 is 97.72% (table 9.11).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 4013,  3676, 14680,  1999,  1996,  3671,  4353])","tensor([  101,  2742,  1024, 17606, 10521,  2102,  1006,  1016,  1007,  1091,
         1014,  1012,  5989,  2581,  2475,  1012,  2947,  1010,  1996,  9723,
         1997,  5300,  1016,  2003,  5989,  1012,  5824,  1003,  1006,  2795,
         1023,  1012,  2340,  1007,  1012,   102])"
1219,1,"['degrees of freedom', 'sample', 'sample size']", Table of the tDistribution,seg_265,"note: the number of degrees of freedom in a sample is n 1, where n is the sample size.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0.])","tensor([ 2795,  1997,  1996, 14595,  2923,  3089, 29446])","tensor([ 101, 3602, 1024, 1996, 2193, 1997, 5445, 1997, 4071, 1999, 1037, 7099,
        2003, 1050, 1015, 1010, 2073, 1050, 2003, 1996, 7099, 2946, 1012,  102])"
1220,1,"['normal distribution', 'fractiles', 'approximation', 'degrees of freedom', 'normal', 'distribution']", Table of the tDistribution,seg_265,"if the number of degrees of freedom is more than 30, you can with very good approximation use fractiles of the normal distribution instead.",tensor(1),"tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.])","tensor([ 2795,  1997,  1996, 14595,  2923,  3089, 29446])","tensor([  101,  2065,  1996,  2193,  1997,  5445,  1997,  4071,  2003,  2062,
         2084,  2382,  1010,  2017,  2064,  2007,  2200,  2204, 20167,  2224,
        25312,  6593,  9463,  2015,  1997,  1996,  3671,  4353,  2612,  1012,
          102])"
1221,1,"['function', 'probability']", Table of the tDistribution,seg_265,fractiles in the t-distribution are calculated in microsoft excel or openoffice calc using the function tinv. notice the peculiar way to specify the probability: find the “rest” probability and multiply by 2.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 2795,  1997,  1996, 14595,  2923,  3089, 29446])","tensor([  101, 25312,  6593,  9463,  2015,  1999,  1996,  1056,  1011,  4353,
         2024, 10174,  1999,  7513, 24970,  2030,  2330,  7245,  6610, 10250,
         2278,  2478,  1996,  3853,  9543,  2615,  1012,  5060,  1996, 14099,
         2126,  2000, 20648,  1996,  9723,  1024,  2424,  1996,  1523,  2717,
         1524,  9723,  1998,  4800, 22086,  2011,  1016,  1012,   102])"
1222,1,"['table', 'probability', 'fractile', 'degrees of freedom']", Table of the tDistribution,seg_265,"example: for the 97.5% ¼ 0.975 fractile, the “rest” probability is 2.5% ¼ 0.025. when multiplied by 2, we get 5% ¼ 0.05. with, for example, 9 degrees of freedom we obtain the fractile as tinv(0.05;9) ¼ 2.262 (table 9.12).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])","tensor([ 2795,  1997,  1996, 14595,  2923,  3089, 29446])","tensor([  101,  2742,  1024,  2005,  1996,  5989,  1012,  1019,  1003,  1091,
         1014,  1012,  5989,  2629, 25312,  6593,  9463,  1010,  1996,  1523,
         2717,  1524,  9723,  2003,  1016,  1012,  1019,  1003,  1091,  1014,
         1012,  6185,  2629,  1012,  2043, 28608,  2011,  1016,  1010,  2057,
         2131,  1019,  1003,  1091,  1014,  1012,  5709,  1012,  2007,  1010,
         2005,  2742,  1010,  1023,  5445,  1997,  4071,  2057,  6855,  1996,
        25312,  6593,  9463,  2004,  9543,  2615,  1006,  1014,  1012,  5709,
         1025,  1023,  1007,  1091,  1016,  1012, 21950,  1006,  2795,  1023,
         1012,  2260,  1007,  1012,   102])"
1223,1,"['function', 'distribution']", Table of the ChiSquared Distribution,seg_267,fractiles in the chi-squared distribution are calculated in microsoft excel or openoffice calc using the function chiinv.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])","tensor([ 2795,  1997,  1996,  9610,  2015, 16211,  5596,  4353])","tensor([  101, 25312,  6593,  9463,  2015,  1999,  1996,  9610,  1011, 19942,
         4353,  2024, 10174,  1999,  7513, 24970,  2030,  2330,  7245,  6610,
        10250,  2278,  2478,  1996,  3853,  9610,  2378,  2615,  1012,   102])"
1224,1,['probability'], Table of the ChiSquared Distribution,seg_267,notice that you should specify the “rest” probability rather than the probability itself.,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.])","tensor([ 2795,  1997,  1996,  9610,  2015, 16211,  5596,  4353])","tensor([  101,  5060,  2008,  2017,  2323, 20648,  1996,  1523,  2717,  1524,
         9723,  2738,  2084,  1996,  9723,  2993,  1012,   102])"
1225,1,"['table', 'probability', 'fractile', 'degrees of freedom']", Table of the ChiSquared Distribution,seg_267,"example: for the 97.5% ¼ 0.975 fractile, the “rest” probability is 2.5% ¼ 0.025. with, for example, 9 degrees of freedom we obtain the fractile as chiinv (0.025;9) ¼ 19.02 (table 9.13).",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0.])","tensor([ 2795,  1997,  1996,  9610,  2015, 16211,  5596,  4353])","tensor([  101,  2742,  1024,  2005,  1996,  5989,  1012,  1019,  1003,  1091,
         1014,  1012,  5989,  2629, 25312,  6593,  9463,  1010,  1996,  1523,
         2717,  1524,  9723,  2003,  1016,  1012,  1019,  1003,  1091,  1014,
         1012,  6185,  2629,  1012,  2007,  1010,  2005,  2742,  1010,  1023,
         5445,  1997,  4071,  2057,  6855,  1996, 25312,  6593,  9463,  2004,
         9610,  2378,  2615,  1006,  1014,  1012,  6185,  2629,  1025,  1023,
         1007,  1091,  2539,  1012,  6185,  1006,  2795,  1023,  1012,  2410,
         1007,  1012,   102])"
1226,1,"['categories', 'data', 'table']", Statistical Uncertainty in Sample Surveys,seg_269,"this table can be used for questionnaire data with two answer categories, e.g., “yes/ no.”",tensor(1),"tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  2023,  2795,  2064,  2022,  2109,  2005,  3160, 20589,  2951,
         2007,  2048,  3437,  7236,  1010,  1041,  1012,  1043,  1012,  1010,
         1523,  2748,  1013,  2053,  1012,  1524,   102])"
1227,1,"['confidence interval', 'uncertainty', 'table', 'interval', 'statistical uncertainty', 'sample', 'statistical', 'confidence']", Statistical Uncertainty in Sample Surveys,seg_269,the table shows the statistical uncertainty of the result of a sample survey. the number in the table is “the number after .” it is used to construct a 95% confidence interval.,tensor(1),"tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  1996,  2795,  3065,  1996,  7778, 12503,  1997,  1996,  2765,
         1997,  1037,  7099,  5002,  1012,  1996,  2193,  1999,  1996,  2795,
         2003,  1523,  1996,  2193,  2044,  1012,  1524,  2009,  2003,  2109,
         2000,  9570,  1037,  5345,  1003,  7023, 13483,  1012,   102])"
1228,1,"['random sampling', 'uncertainty', 'statistical uncertainty', 'random', 'sampling', 'statistical', 'stratified sampling']", Statistical Uncertainty in Sample Surveys,seg_269,"simple random sampling is assumed. by stratified sampling, the statistical uncertainty will often be smaller.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,
        0., 0., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  3722,  6721, 16227,  2003,  5071,  1012,  2011,  2358,  8609,
         7810, 16227,  1010,  1996,  7778, 12503,  2097,  2411,  2022,  3760,
         1012,   102])"
1229,1,"['cluster sampling', 'uncertainty', 'table', 'statistical uncertainty', 'percentage', 'sample', 'sampling', 'sample size', 'statistical']", Statistical Uncertainty in Sample Surveys,seg_269,"by cluster sampling, the statistical uncertainty will often be larger (table 9.14). example: a result (e.g., percentage answering “yes” to a question) in a sample survey is 25%; the sample size is 500. the statistical uncertainty of the result is found in the table to be 3.8%.",tensor(1),"tensor([0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,
        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  2011,  9324, 16227,  1010,  1996,  7778, 12503,  2097,  2411,
         2022,  3469,  1006,  2795,  1023,  1012,  2403,  1007,  1012,  2742,
         1024,  1037,  2765,  1006,  1041,  1012,  1043,  1012,  1010,  7017,
        10739,  1523,  2748,  1524,  2000,  1037,  3160,  1007,  1999,  1037,
         7099,  5002,  2003,  2423,  1003,  1025,  1996,  7099,  2946,  2003,
         3156,  1012,  1996,  7778, 12503,  1997,  1996,  2765,  2003,  2179,
         1999,  1996,  2795,  2000,  2022,  1017,  1012,  1022,  1003,  1012,
          102])"
1230,1,"['probability', 'interval', 'population']", Statistical Uncertainty in Sample Surveys,seg_269,"this means that if interviewing the whole population, the result would with 95% probability be in the interval 25% 3.8%, i.e., an interval from 21.2% to 28.8%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  2023,  2965,  2008,  2065, 27805,  1996,  2878,  2313,  1010,
         1996,  2765,  2052,  2007,  5345,  1003,  9723,  2022,  1999,  1996,
        13483,  2423,  1003,  1017,  1012,  1022,  1003,  1010,  1045,  1012,
         1041,  1012,  1010,  2019, 13483,  2013,  2538,  1012,  1016,  1003,
         2000,  2654,  1012,  1022,  1003,  1012,   102])"
1231,1,"['uncertainty', 'statistical uncertainty', 'sample', 'statistical']", Statistical Uncertainty in Sample Surveys,seg_269,"note: you get the same statistical uncertainty, if the result in the sample survey is 75%.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.])","tensor([ 7778, 12503,  1999,  7099, 12265])","tensor([  101,  3602,  1024,  2017,  2131,  1996,  2168,  7778, 12503,  1010,
         2065,  1996,  2765,  1999,  1996,  7099,  5002,  2003,  4293,  1003,
         1012,   102])"
1232,1,"['data', 'table']", Fitness Club Data from the Sample Survey,seg_271,data from the example used throughout the book. data are sorted by sex and age (table 9.15).,tensor(1),"tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0.])","tensor([10516,  2252,  2951,  2013,  1996,  7099,  5002])","tensor([  101,  2951,  2013,  1996,  2742,  2109,  2802,  1996,  2338,  1012,
         2951,  2024, 19616,  2011,  3348,  1998,  2287,  1006,  2795,  1023,
         1012,  2321,  1007,  1012,   102])"
1233,0,[], Literature,seg_275,the following books can be recommended:,tensor(0),"tensor([0, 0, 0, 0, 0, 0, 0, 0, 0])",tensor([3906]),"tensor([ 101, 1996, 2206, 2808, 2064, 2022, 6749, 1024,  102])"
1234,1,['statistics'], Literature,seg_275,darrell huff (1991). how to lie with statistics. penguin books a classic. put it in your pocket and read it in the bus or in the train. . .,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.])",tensor([3906]),"tensor([  101, 23158, 21301,  1006,  2889,  1007,  1012,  2129,  2000,  4682,
         2007,  6747,  1012, 13987,  2808,  1037,  4438,  1012,  2404,  2009,
         1999,  2115,  4979,  1998,  3191,  2009,  1999,  1996,  3902,  2030,
         1999,  1996,  3345,  1012,  1012,  1012,   102])"
1235,1,['statistics'], Literature,seg_275,larry gonick and woolcott smith (1993). the cartoon guide to statistics. harperperennial statistics as a cartoon!,tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0.])",tensor([3906]),"tensor([  101,  6554,  2175, 13542,  1998, 12121, 13124,  3044,  1006,  2857,
         1007,  1012,  1996,  9476,  5009,  2000,  6747,  1012,  8500,  4842,
        22929,  6747,  2004,  1037,  9476,   999,   102])"
1236,1,['statistics'], Literature,seg_275,"sincich tl, levine dm, stephan d, sincich t and berenson m (2002) 2nd ed practical statistics by example – using microsoft excel and minitab. prentice hall. plenty of examples in virtually all disciplines. both for users of microsoft and excel and the widely used statistics software minitab",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])",tensor([3906]),"tensor([  101,  8254, 19053,  2232,  1056,  2140,  1010, 17780,  1040,  2213,
         1010, 15963,  1040,  1010,  8254, 19053,  2232,  1056,  1998,  2022,
         7389,  3385,  1049,  1006,  2526,  1007,  3416,  3968,  6742,  6747,
         2011,  2742,  1516,  2478,  7513, 24970,  1998,  7163,  2696,  2497,
         1012, 23429,  2534,  1012,  7564,  1997,  4973,  1999,  8990,  2035,
        12736,  1012,  2119,  2005,  5198,  1997,  7513,  1998, 24970,  1998,
         1996,  4235,  2109,  6747,  4007,  7163,  2696,  2497,   102])"
1237,1,"['statistical', 'statistics']", Literature,seg_275,"agresti and finlay b (1997). statistical methods for the social sciences. 3rd ed. prentice hall, nj detailed book on statistics for the social sciences.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])",tensor([3906]),"tensor([  101, 12943, 28533,  2072,  1998, 10346,  8485,  1038,  1006,  2722,
         1007,  1012,  7778,  4725,  2005,  1996,  2591,  4163,  1012,  3822,
         3968,  1012, 23429,  2534,  1010, 19193,  6851,  2338,  2006,  6747,
         2005,  1996,  2591,  4163,  1012,   102])"
1238,1,['sample'], Literature,seg_275,"vic barnett (2003). sample survey principles and methods, 3rd ed. arnold publishers, new delhi an excellent book on sample surveys.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])",tensor([3906]),"tensor([  101, 10967, 20073,  1006,  2494,  1007,  1012,  7099,  5002,  6481,
         1998,  4725,  1010,  3822,  3968,  1012,  7779,  8544,  1010,  2047,
         6768,  2019,  6581,  2338,  2006,  7099, 12265,  1012,   102])"
1239,1,"['sample', 'errors']", Literature,seg_275,"groves rm (2004). survey errors and survey costs. wiley, nj thorough book on the practical aspects of sample surveys. not mathematically advanced.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([3906]),"tensor([  101, 21695, 28549,  1006,  2432,  1007,  1012,  5002, 10697,  1998,
         5002,  5366,  1012, 18825,  1010, 19193, 16030,  2338,  2006,  1996,
         6742,  5919,  1997,  7099, 12265,  1012,  2025,  8045,  2135,  3935,
         1012,   102])"
1240,1,['sampling'], Literature,seg_275,"cochran wg (1978): sampling techniques, 3rd ed. wiley, nj still the bible on survey sampling!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0.])",tensor([3906]),"tensor([  101, 28506,  1059,  2290,  1006,  3301,  1007,  1024, 16227,  5461,
         1010,  3822,  3968,  1012, 18825,  1010, 19193,  2145,  1996,  6331,
         2006,  5002, 16227,   999,   102])"
1241,1,['experiments'], Literature,seg_275,"cox dr (1992) planning of experiments. wiley, nj elementary, yet thorough book on planned experiments. focus is on applications, not on the theory. not mathematically advanced.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0.])",tensor([3906]),"tensor([  101,  9574,  2852,  1006,  2826,  1007,  4041,  1997,  7885,  1012,
        18825,  1010, 19193,  4732,  1010,  2664, 16030,  2338,  2006,  3740,
         7885,  1012,  3579,  2003,  2006,  5097,  1010,  2025,  2006,  1996,
         3399,  1012,  2025,  8045,  2135,  3935,  1012,   102])"
1242,1,"['design', 'experimental', 'experiments', 'tables']", Literature,seg_275,"cochran wg and cox gm (1992) 2nd ed. experimental design. wiley, nj planning of experiments for practitioners. tables with specific experimental designs.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.])",tensor([3906]),"tensor([  101, 28506,  1059,  2290,  1998,  9574, 13938,  1006,  2826,  1007,
         3416,  3968,  1012,  6388,  2640,  1012, 18825,  1010, 19193,  4041,
         1997,  7885,  2005, 14617,  1012,  7251,  2007,  3563,  6388,  5617,
         1012,   102])"
1243,1,"['results', 'statistics', 'experiments']", Literature,seg_275,"box gep, hunter wg and hunter js (2005) statistics for experimenters, 2nd ed. wiley, nj excellent book on statistics with an emphasis on planning of experiments and analyzing the results, but it is useful for most people working with statistics. a legendary book!",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0.])",tensor([3906]),"tensor([  101,  3482, 16216,  2361,  1010,  4477,  1059,  2290,  1998,  4477,
         1046,  2015,  1006,  2384,  1007,  6747,  2005,  7551,  2545,  1010,
         3416,  3968,  1012, 18825,  1010, 19193,  6581,  2338,  2006,  6747,
         2007,  2019,  7902,  2006,  4041,  1997,  7885,  1998, 20253,  1996,
         3463,  1010,  2021,  2009,  2003,  6179,  2005,  2087,  2111,  2551,
         2007,  6747,  1012,  1037,  8987,  2338,   999,   102])"
1244,1,"['control', 'quality control', 'statistics', 'statistical']", Literature,seg_275,"douglas montgomery (2005). introduction to statistical quality control, 6th ed. wiley, n. basic statistics with a thorough introduction to statistical quality control.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])",tensor([3906]),"tensor([  101,  5203,  8482,  1006,  2384,  1007,  1012,  4955,  2000,  7778,
         3737,  2491,  1010,  5351,  3968,  1012, 18825,  1010,  1050,  1012,
         3937,  6747,  2007,  1037, 16030,  4955,  2000,  7778,  3737,  2491,
         1012,   102])"
1245,1,"['regression', 'variance', 'regression analysis', 'design', 'analysis of variance', 'level', 'experiments', 'statistical']", Literature,seg_275,"douglasmontgomery (2005).design and analysis of experiments, 6th ed.wiley, nj design and statistical analysis of experiments, analysis of variance, regression analysis, etc. somewhat higher level of mathematics than the other books in this list.",tensor(1),"tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])",tensor([3906]),"tensor([  101,  5203,  9629,  3995,  5017,  2100,  1006,  2384,  1007,  1012,
         2640,  1998,  4106,  1997,  7885,  1010,  5351,  3968,  1012, 18825,
         1010, 19193,  2640,  1998,  7778,  4106,  1997,  7885,  1010,  4106,
         1997, 23284,  1010, 26237,  4106,  1010,  4385,  1012,  5399,  3020,
         2504,  1997,  5597,  2084,  1996,  2060,  2808,  1999,  2023,  2862,
         1012,   102])"
