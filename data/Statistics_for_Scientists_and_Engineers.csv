,Relevance,Tags,Heading,Seg,Sentence
0,0,[], DESCRIPTIVE STATISTICS,seg_1,"after finishing the chapter, students will be able to"
1,1,['statistics'], DESCRIPTIVE STATISTICS,seg_1,◾ explain the meaning and uses of statistics
2,1,"['measurement', 'scales of measurement', 'standard']", DESCRIPTIVE STATISTICS,seg_1,◾ describe the standard scales of measurement
3,1,['summation'], DESCRIPTIVE STATISTICS,seg_1,◾ interpret various summation (∑) and product (∏) notations
4,1,"['data', 'transformations']", DESCRIPTIVE STATISTICS,seg_1,◾ apply different types of data transformations
5,1,"['data', 'discretization']", DESCRIPTIVE STATISTICS,seg_1,◾ distinguish various data discretization algorithms (ddas)
6,1,"['discretization', 'range', 'statistics', 'factorial', 'falling factorials', 'data', 'permutation', 'inferential statistics', 'scales of measurement', 'combination', 'summation', 'rising and falling factorials', 'continuous', 'measurement', 'transform']", INTRODUCTION,seg_3,"statistics has borrowed several ideas and notations from various other fields. this section summarizes some important concepts and notations that will be used in this book. as examples, the factorial, permutation and combination, summation, product (including rising and falling factorials), and other mathematical notations including set-theoretic operators are discussed in this chapter. these notations are extensively used in descriptive and inferential statistics. a discussion on the most commonly used scales of measurement in this chapter gives better insight into the types of data most often encountered. various techniques to transform these data into any desired range are also given. data discretization techniques to categorize continuous data are exemplified. these notations, tools, and techniques are immensely useful to better grasp the rest of the chapters."
7,1,"['trimmed means', 'kurtosis', 'case', 'location', 'harmonic', 'geometric', 'sample median', 'sample', 'quartiles', 'data', 'percentiles', 'trials', 'population', 'weighted means', 'skewness', 'statistical', 'median', 'location measures']", INTRODUCTION,seg_3,"most of the statistical analyses use a sample of data values. these data are either collected from a target population, obtained from field trials, or generated randomly. they are then either summarized (using summary measures) or subjected to one or more analyses. popular summarization measures include location measures, spread measures, skewness and kurtosis measures, dependency measures, and other measures. chapter 2 introduces the most common location measures. these include arithmetic, geometric, and harmonic means; median; and the mode. trimmed versions of these measures are obtained by dropping data values at either or both of the extremes. as a special case, as the geometric and harmonic means are defined only for positive data values ($ $0), an analyst may drop all zero and negative data values to obtain left-trimmed versions of them. the trimmed means, weighted means, and their updating formula are given. the sample median, mode, quartiles, and percentiles are also explained."
8,1,"['coefficient of variation', 'sample', 'linear', 'sample variance', 'deviation', 'coefficient', 'range', 'nonlinear', 'variation', 'categorization', 'standard', 'standard deviation', 'variance']", INTRODUCTION,seg_3,"dispersion measures are discussed in chapter 3. a categorization of spread measures helps the reader to distinguish between linear and nonlinear measures, pivotal and pivotless measures, additive and nonadditive measures, absolute and relative measures, and distance-based measures. the most frequently used spread measures are the sample range, variance, or standard deviation. these are discussed and some updating formulae for sample variance are derived. the sample variance and covariance can be computed recursively using a divide-and-conquer strategy by dividing the sample into two subsamples and pooling the corresponding variance or covariance of subsamples in a sensible and orderly way. the coefficient of variation and gini coefficient are also discussed."
9,1,"['complete enumeration', 'kurtosis', 'conditional', 'location', 'probability', 'bayes', 'derangements', 'partitions', 'probability theory', 'permutation', 'skewness', 'recurrence', 'functions', 'urn models', 'combination', 'bayes theorem', 'distribution', 'recurrence relations', 'conditional probability']", INTRODUCTION,seg_3,"popular measures of skewness and kurtosis are discussed in chapter 4. different types of skewness measures such as location and scale-based measures, quartile-based measures, moment-based measures, measures that utilize inverse of distribution functions, or l-moments are discussed. the measures of kurtosis discussed are skewness–kurtosis bounds, l-kurtosis, and spectral kurtosis. chapter 5 discusses probability theory with emphasis on solving problems from various fields. essential tools and techniques such as permutation and combination, cyclic permutation, complete enumeration, trees, principle of inclusion and exclusion, recurrence relations, derangements, urn models, and partitions used to solve probability problems are discussed, and conditional probability and bayes theorem are introduced."
10,1,"['discrete', 'bernoulli trials', 'binomial distribution', 'tail areas', 'discrete distribution', 'linear', 'rate', 'successes', 'trials', 'mean', 'distributions', 'distribution', 'expectation', 'binomial', 'continuous', 'convergence', 'variance', 'poisson', 'mathematical expectation', 'absolute value', 'independent', 'deviation', 'method', 'failures', 'bernoulli', 'tail', 'continuous distributions']", INTRODUCTION,seg_3,"discrete distributions and their properties are discussed in chapter 6. a novel method to easily find the mean deviation (md) of any discrete distribution is introduced in this chapter, and its practical use is illustrated. it is shown that the rate of convergence of binomial distribution to the poisson [307,308] law is quadratic in p and linear in n. distribution of the absolute value of the difference between successes and failures in independent bernoulli trials is obtained in simple form. commonly encountered continuous distributions are discussed in chapter 7. an impressive method of immense practical value to find the mean deviation of continuous distributions is derived and is extensively used throughout the chapter. variance of continuous distributions is shown to be related to the tail areas. mathematical expectation and its properties are discussed in chapter 8. an introduction to various"
11,1,"['deviation', 'functions', 'statistics', 'distribution', 'mean']", INTRODUCTION,seg_3,generating functions used in statistics appears in chapter 9. two new families of generating functions (for generating cumulative distribution functions (cdf-gf) and mean deviation (md-gf)) are introduced in this chapter.
12,1,"['marginal', 'integer part', 'conditional', 'random', 'conditional distributions', 'distributions', 'functions', 'continuous random variables', 'transformations', 'distribution', 'probabilistic', 'continuous', 'jacobian', 'polar transformations', 'random variables', 'method', 'table', 'variables', 'joint']", INTRODUCTION,seg_3,"functions of random variables appear in chapter 10. different techniques such as method of distribution functions, jacobian method, probabilistic methods, and area-based methods are discussed. distribution of squares, square roots, reciprocals, sums, products, quotients, integer part, and fractional part of continuous random variables and distributions of trigonometric and transcendental functions are also discussed. joint, marginal, and conditional distributions are discussed in chapter 11. the concept of jacobians is introduced, and an immensely useful summary table of 2d transformations is given. various polar transformations such as plane polar, spherical polar, and toroidal polar and their inverses are also discussed and summarized."
13,1,"['random samples', 'range', 'statistics', 'correlation', 'random', 'statistical hypotheses', 'numerical', 'graphical', 'testing statistical', 'data', 'information', 'samples', 'mean', 'population', 'inferential statistics', 'quality control', 'statistical', 'parameters', 'descriptive statistics', 'scatterplots', 'variance', 'control', 'hypotheses']", STATISTICS AS A SCIENTIFIC DISCIPLINE,seg_5,"statistics has its origin in describing (collecting, organizing, and interpreting) numeric data collected on subjects of interest. this branch of statistics is called descriptive statistics. it deals with finite random samples drawn from the totality of all elements concerned (which is assumed to be large and is called the population (pp. 1–11)). it uses numerical measures (such as mean, variance, and correlation) and graphical techniques to summarize information in a concise and comprehensible form. these are intended for communication, interpretation, or subsequent processing by humans or machines. inferential statistics is concerned with making inferences about the parameters or on the functional form of a population (defined in the following) using small random samples drawn from it. a great majority of inferential statistics do make assumptions on the form of the underlying density, on the parameters, or on the data range. data for descriptive and inferential statistics are often numeric. large samples are typically used in descriptive statistics than in inferential statistics. for example, scatterplots and other visualization tools require more data points than those used in statistical quality control or testing statistical hypotheses."
14,1,"['experimental', 'statistics', 'data', 'information', 'hypotheses']", STATISTICS AS A SCIENTIFIC DISCIPLINE,seg_5,"definition 1.1 statistics is a branch of scientific discipline that deals with the systematic collection, tabulation, summarization, classification, analysis and modeling of data, extracting summary information from numeric data, and drawing potentially useful conclusions from past or observed data, or verifying experimental hypotheses."
15,1,"['levels', 'statistics', 'statistic', 'process', 'statistical hypotheses', 'parameter', 'population', 'statistical', 'quality control', 'experimental', 'test statistic', 'distribution', 'tolerance limits', 'control', 'test', 'hypothesis', 'deviations', 'hypotheses']", STATISTICS AS A SCIENTIFIC DISCIPLINE,seg_5,"this definition does not cover every branch of statistics, as the subject continues to diversify into various applied sciences. for example, statistical quality control is used to check process deviations to see if they are well within the tolerance limits or if they fall beyond predefined levels. testing of statistical hypotheses involve well-defined experimental steps that use tabulated values of a test statistic to draw reasonable conclusions (accepting or rejecting a research hypothesis) about an unknown population parameter that describes some characteristic of the distribution."
16,1,"['statistics', 'data', 'regression', 'least squares']", STATISTICS AS A SCIENTIFIC DISCIPLINE,seg_5,"similarly, engineering applications of statistics include searching for potentially useful patterns and trends in large data collections using regression [304] models, hierarchical (tree) models, cluster analysis, partial least squares, and so on. the least-squares principle finds applications in neural networks (nns), digital signal processing, data compression, and many engineering fields."
17,1,"['quantitative', 'range', 'states', 'statistics', 'distribution function', 'location', 'random', 'function', 'predictions', 'data', 'trials', 'samples', 'simulated', 'population', 'expectations', 'sampling distributions', 'inferential statistics', 'statistical', 'strata', 'distributions', 'experimental', 'parameters', 'categorical', 'estimation', 'distribution', 'random variables', 'variables', 'sampling', 'nonparametric statistics', 'experiments']", STATISTICS AS A SCIENTIFIC DISCIPLINE,seg_5,"population census was conducted long ago in 3340 bc in egypt [en.wikipedia.org/ wiki/census]. william-i of england conducted a complete census of adults and households in ad 1086. regular population census started in the united states in 1790 and in the united kingdom in 1801 at 10-year periods. numeric data on birth, death, and marriages were collected in several countries of western europe subsequently. this branch of statistics that deals with vital (life-connected) data is known as vital statistics. study of numeric data on education, housing, and social welfare came to be known as social statistics. economic statistics deals with data analysis on unemployment, economic indicators (consumer price index of essential commodities, purchasing power of people in various strata, etc.), industrial production, import, and export. experimental statistics deals with data analysis or comparison techniques in field experiments using samples or simulated trials. agricultural statistics is a related field that deals with analysis of yield in field experimentation. data analysis techniques that utilize data spread across a frame-of-reference (such as the earth’s surface or deep space) are known as spatial statistics. they involve data of categorical or quantitative types along with a spatial frame of reference that serves as a window. medical statistics is a mixture of the above that deals with summary measures, experimental designs, sampling, predictions, classifications, and clustering, to name a few. mathematical statistics comprises an umbrella of properties of random variables, sampling distributions, expectations, estimation, and inference (see inferential statistics). nonparametric statistics have minimal assumptions on the data—on the data range, distribution function, or the location and scale parameters of the population. statistical techniques are also applied in various other fields such as insurance (actuarial statistics), engineering (engineering statistics), psychology, and education."
18,1,"['ordinal', 'measurement', 'standard', 'quantitative', 'categorical', 'interval', 'scales of measurement', 'data', 'nominal', 'ordinal data', 'categories', 'statistical']", Scales of Measurement,seg_7,"the most popular scales of measurement are nominal, ordinal, interval, and ratio (noir) scales originated by stevens [265]. a great majority of common data during the 1940s belonged to one of these four categories. these are called standard data scales. these scales were well known in statistical analysis much before 1946 as categorical and quantitative scales. the nominal and ordinal data are together called categorical, nonmetric, or qualitative data because they are labeled using a finite alphabet of distinct and consistent symbols. these need not be numbers because we can use letters (in various languages such as english and greek), enumerated constants, or strings to label them. the interval and ratio type of data are together called quantitative data. these are always numeric (either integers or floating point"
19,1,"['ordinal', 'quantitative', 'categorical', 'interval', 'statisticians', 'data', 'nominal', 'statistical']", Scales of Measurement,seg_7,"numbers with integer and fractional parts). most statisticians are familiar with categorical (nominal and ordinal) and quantitative (interval and ratio) (or c q for short) data scales1, as the majority of statistical methods works on them. different statistical techniques are used with c q data, as discussed in subsequent chapters."
20,1,"['ordinal', 'interval', 'data', 'nominal', 'set', 'ordinal data', 'level']", Scales of Measurement,seg_7,"text data are built on a nominal or ordinal set of characters [53]. one example is the binary string data that uses a sequence of 0’s and 1’s (figure 1.1). majority of text data use alphabetic characters of natural languages that are ordinal data. subsets of alphabetic characters or special characters can also be used to encode text data as in genomics and bioinformatics. audio, video, animation, and multimedia (avam) data are always compressed when stored in digital form. these use interval or ratio type of data at the basic level."
21,1,"['categorical data', 'quantitative', 'data', 'categorical']", Scales of Measurement,seg_7,"spatial data are stored with reference to a coordinate frame, a surface, or a map. it consists of quantitative or categorical data that are tied to unique points in the corresponding frame. one example is gis data that use the earth’s surface as the base. earth-centric data can be represented in multiple ways (using (longitude, latitude, altitude) coordinates, using gps coordinates, etc.)."
22,1,"['efficient', 'statistics', 'dependent', 'data', 'intervals', 'least squares', 'statistical', 'vary']", Scales of Measurement,seg_7,"temporal data are time dependent. one common example is the stock market index, which varies continuously during the trading time. spatiotemporal data are spatial data that vary over time. examples are data generated by tornadoes, hurricanes, and other natural disasters that persist for some time, sea surface temperatures (which vary during 24-hour intervals with maximum occurring when the sun has passed over the particular sea), http connection requests generated across the world (which subsides down after midnight at particular regions), and so on. the extended data types can be structured or unstructured, compressed or uncompressed. newer data types and standards are also being devised to store data collected using special hardware devices, wireless devices, and satellites. these data are transmitted as radio waves in analog form, which are then converted into digital form. high-quality compression standards that use quantization and least squares principle are used for efficient storage of this type of data. newer applications of statistics are also emerging in extended data analysis. for instance, statistical measures computed"
23,1,"['data', 'information', 'statistical', 'temporal data']", Scales of Measurement,seg_7,"from summary information extracted from extended data are being used in information retrieval using latent semantic analysis [53, 294]. well-developed statistical techniques also exist for spatial and temporal data analysis."
24,1,"['measurement', 'scales of measurement', 'statistics', 'data', 'information', 'variable', 'standard', 'level', 'data collection']", THE NOIR SCALE,seg_9,"this section briefly reviews the standard scales of measurement, which we call the noir typology. a basic understanding of this concept is helpful to students and researchers in statistics and other fields, who are faced with the data collection task. data can be collected using questionnaires, web forms, or machines or special devices. a precise granularity level is important for choosing each variable and to extract the maximum information from subjects that generate the data."
25,1,"['mutually exclusive', 'factor', 'nominal', 'variable', 'set', 'categorization', 'categories']", The Nominal Scale,seg_11,"it is a categorization of a variable into a set of mutually exclusive (distinct) values. a most common example is the human blood group = {a,b,o,ab}. there is no logical order among the blood groups. hence, we cannot say that a person with blood group “a” is better or worse than a person with blood group “b” (owing to the advances being made in the mapping and analysis of human genes, it is now possible to identify humans who are more susceptible to certain illnesses using their blood type. someday, it may be possible to logically order the blood groups conditionally on various disease categories.). they are just distinguishing labels given to persons based on a surface marker on the red blood cells. by convention, they are denoted by capital letters of the english alphabet. another common nominal variable is the rhesus factor (rh-f) coded as {+,−}. the labels + and − are chosen historically, but it could be any two distinct symbols, or numbers such as {1, 0}."
26,1,"['mutually exclusive', 'data', 'nominal', 'variable', 'set']", The Nominal Scale,seg_11,"definition 1.2 a variable that takes a value among a finite set of mutually exclusive codes that have no logical order among themselves is called a nominal variable, and the data that it generates is called nominal data."
27,1,"['goodness of fit', 'linear', 'mutually exclusive', 'coefficient', 'variables', 'data', 'correlation', 'dummy variables', 'nominal', 'categories', 'tests']", The Nominal Scale,seg_11,"some nominal data are always coded numerically. examples are binary support vector machines (svms) whose class labels are always coded as (−1,+1), dummy variables in linear programming (coded as 0 or 1). decision trees (dts) and nns use any type of numeric or nonnumeric codes for nominal data. pearson’s  2-statistic based goodness of fit tests use mutually exclusive and collectively exhaustive nominal categories that are numerically coded. it uses the contingency coefficient (section 1.12, pp. 1–20) to find the correlation between two numerically coded nominal variables."
28,1,"['variables', 'nominal']", The Ordinal Scale,seg_13,"some of the nominal variables can be ordered using the values they take on two or more subjects. examples are the severity of an accident = {mild, severe,"
29,0,[], The Ordinal Scale,seg_13,"critical, deadly}, patient-type = {child, teenager, youth, adult, senile}, and various ratings = {poor, fair, good, excellent}."
30,1,"['ordinal', 'data', 'nominal', 'variable', 'ordinal data']", The Ordinal Scale,seg_13,definition 1.3 nominal data that can be logically ordered are called ordinal data and the corresponding variable is called an ordinal variable.
31,1,"['ordinal', 'data', 'variable', 'ordinal data']", The Ordinal Scale,seg_13,"one of the relational operators is used to order them. for instance, poor fair good excellent. sometimes, this ordering is done literally. an example is the seasons = {spring, summer, autumn, winter}. these seasons repeat among themselves. hence, it is called a cyclic ordinal variable. other examples are the days of the week, months of the year, letters of an alphabet, and so on, which are literally ordered ordinal data. we could categorize ordinal data as alphabetically ordered, numerically ordered, and literally ordered; each of them can be cyclically ordered if there is a natural succession among them. alphabetically ordered ordinal data use the precedence order among the letters of a natural language (e.g., english and japanese) to decide which comes first."
32,1,"['quartiles', 'ordinal', 'median', 'data', 'location', 'percentiles', 'ordinal data', 'coefficient']", The Ordinal Scale,seg_13,"numerically coded ordinal data can be compared using relational operators. hence, the median and mode are appropriate measures of location for them. we could also use the quartiles, percentiles, contingency coefficient, and so on."
33,1,"['functions', 'ordinal', 'data', 'ordinal data']", The Interval Scale,seg_15,some of the ordinal data can be coded as integers or functions thereof (as in date data type).
34,1,"['ordinal', 'interval', 'data', 'ordinal data']", The Interval Scale,seg_15,"definition 1.4 if numerically coded ordinal data have the property that the differences between any two values represent equal difference in the amount of the characteristic measured, it is called an interval data."
35,1,"['ordinal', 'interval', 'variables', 'data', 'intervals', 'ordinal data']", The Interval Scale,seg_15,"in other words, all ordinal data with well-defined intervals are interval data. one common example is the date data type. a characteristic property of interval data is that there is no natural zero. the natural zero need not always be the zero point. this can differ among variables. as an example, the fever is measured in fahrenheit or"
36,1,['normal'], The Interval Scale,seg_15,"∘ centigrade scales. the normal [305] human body temperature is 98.4 f. if a person has higher body temperature, we say that there is fever. thus, the origin for fever is"
37,1,['limit'], The Interval Scale,seg_15,"∘ 98.4 f, as it is the cutoff limit."
38,1,"['measurement', 'variables', 'data']", The Ratio Scale,seg_17,"interval data with a clear definition of zero are called ratio data. thus, both the differences between data values and their ratios are meaningful. common examples are the height and weight. if a father is 5 feet 4 inches and his sibling is 2 feet 8 inches, then the father is twice as tall as the sibling, and the same relationship holds irrespective of the unit of measurement. hence even if we change to the metric system, the same relationship will hold. other examples are the price of an article, speed of vehicles, capacity of disks or pen drives, and so on. the zero point for some ratio variables may never be materialized. for instance, consider the task of classifying a patient as overfat or underfat. this is done using the body mass index (bmi) measure."
39,0,[], The Ratio Scale,seg_17,the bmi is defined as follows:
40,1,"['variable', 'vary', 'limit']", The Ratio Scale,seg_17,"this is a ratio variable. typical cutoff limit varies from country to country, but a bmi of 25 is a common cutoff limit for adults. if the person’s bmi is above 25, we say that he or she is overfat. the bmi can never be zero (not even for newborns). this means that a zero bmi is hypothetical. as the square of a number between zero and one is less than itself, the cutoff will vary for infants shorter than 1 meter."
41,1,"['sample', 'variables', 'random sample', 'set', 'population', 'random', 'statistical', 'statistical population']", POPULATION VERSUS SAMPLE,seg_19,"most statistical procedures are based on a random sample drawn from a “population” of interest. the meaning of statistical population is slightly different from the literal meaning of population. literally, it denotes a group of living organisms that are often large in size. a statistical population can have temporary or permanent existence. it can be small in size. as examples, the set of all http requests on the web on any day is a statistical population that is large in size. however, majority of http requests last at most a few seconds. the set of gps satellites in orbit is another population that is small in size, as also the set of atomic powered interplanetary spacecrafts. each element of a population is assumed to be measurable on a set of variables. these variables often follow well-defined statistical laws."
42,1,['population'], POPULATION VERSUS SAMPLE,seg_19,definition 1.5 the totality of all elements of interest in a study is called the population.
43,1,"['sample', 'location', 'population', 'statistical', 'statistical population']", POPULATION VERSUS SAMPLE,seg_19,"a statistical population may comprise animate or inanimate objects, symbols, entities, and so on. it may or may not be finite. it can be confined to a specific location or could be spread around a known or unknown locality. the first step in obtaining a sample is identifying the correct population. hence, the population is often defined unambiguously by the experimenter for each research study. the study of elements of the entire population is called a census."
44,1,"['population', 'vary']", POPULATION VERSUS SAMPLE,seg_19,"illustration 1 consider a study to correlate the marks obtained by students in a science subject to ownership of a computer or ipad and the use of the internet. the population in this study is the totality of all students in that subject who owns a computer or ipad with internet connectivity. this can be confined to a university, a country, a geographic region, and so on. thus, the population could vary depending on the spread of the subjects."
45,1,"['set', 'measurements', 'population']", POPULATION VERSUS SAMPLE,seg_19,"illustration 2 toy manufacturers make toys for kids in specific age groups. in a study to find out toys that are injurious to kids in 1–5 age group, the population is the set of all toys for this age group. as measurements are taken on subjects, the true population is the totality of kids in the above-mentioned age group who use these toys."
46,1,"['sample', 'random', 'data', 'population', 'random sample', 'representative']", POPULATION VERSUS SAMPLE,seg_19,"definition 1.6 a random sample is a true representative subset of a population, which is much smaller in size and each element of which generates recordable and meaningful data."
47,1,"['sample', 'method', 'with replacement', 'results', 'without replacement', 'random sample', 'replacement', 'populations', 'sampling', 'mean', 'population', 'random', 'representative']", POPULATION VERSUS SAMPLE,seg_19,"by the true representative, we mean that each and every element of the population has an equal chance of being included in our random sample. if the population is of finite size, sampling with replacement will ensure that the chance of drawing a sample from the population remains the same. the method in which a sampled item from a population is not replaced (back into the population) before the next item is drawn is called sampling without replacement. this does not matter for infinite populations. if the population size is finite and small, sampling without replacement results in a nonrandom sample."
48,1,"['sample', 'without replacement', 'random sample', 'replacement', 'sampling', 'random']", POPULATION VERSUS SAMPLE,seg_19,"illustration 3 a foundation offered five scholarships to students in a college who secured distinction in their final exam. if there are 25 eligible students, we need to select a random sample of five students from among them. to preclude the possibility of a selected student receiving two scholarships, we need to do a sampling without replacement."
49,1,"['estimates', 'statistics', 'random sample', 'random', 'sample', 'stratified sampling', 'results', 'data', 'samples', 'populations', 'population', 'statistical', 'sample size', 'parameters', 'asymptotic', 'distribution', 'sampling distribution', 'sampling']", POPULATION VERSUS SAMPLE,seg_19,"random samples are much easier to work with because they are much smaller in size. owing to some asymptotic properties of sample estimates, we often restrict the random sample size between 30 and a few hundred. bigger sample sizes give better results in some statistical procedures. a researcher decides upon an optimal sample size using the cost of sampling an item, population characteristics, number of unknown parameters, sampling distribution of statistics, and so on. for instance, if the population has distinct data clusters, a technique called stratified sampling can be used to select a random sample from each cluster based on the cluster size. sampling of elements from populations is an extensively studied field called sampling theory in statistics."
50,1,"['parameter', 'population']", Parameter Versus Statistic,seg_21,a parameter describes the population of interest. a population can have zero or more
51,1,"['parameters', 'cauchy', 'cauchy distribution', 'distribution']", Parameter Versus Statistic,seg_21,1 1 parameters. consider the cauchy distribution f (x) =   (1+x2) for −∞   x   ∞. it has
52,1,"['poisson', 'parameters', 'range', 'distribution', 'population', 'poisson distribution']", Parameter Versus Statistic,seg_21,"no parameters, although it describes a population2. parameters are values that characterize the population. they may be a part of the functional form or the range of values assumed. for instance, a left-truncated poisson distribution with truncation point k has the functional form"
53,1,"['poisson', 'functions', 'degrees of freedom', 'parameters', 'range', 'location', 'distribution', 'set', 'parameter', 'population']", Parameter Versus Statistic,seg_21,"where p(j) = e−  j∕j! here k is a range parameter. the location (central value) and spread (scale) are the most important characteristics of a population. they may either be described as functions of the same set of parameters (as in  2 distribution with   = n,  2 = 2n, where n is the degrees of freedom; or the poisson law with   =  2 =  )"
54,1,"['parameters', 'normal', 'mean', 'variance']", Parameter Versus Statistic,seg_21,"or by separate parameters as in the univariate normal law n( ,  2) with mean   and variance  2; the laplace law which has pdf"
55,1,['logistic'], Parameter Versus Statistic,seg_21,with   = a and  2 = 2b2 or the logistic law with cdf
56,1,"['functions', 'parameters', 'distribution', 'asymmetric', 'mean', 'beta distribution', 'noncentral', 'variance', 'distributions']", Parameter Versus Statistic,seg_21,"for several distributions, the mean and variance are complex functions of the parameters. as examples, the bino(n, p) distribution has mean = np and variance = ∗ q = npq. similarly, the noncentral chi-square distribution has mean = n + and variance 2 = 2( + ) = 2n + 4 . the shape of the distribution can be very sensitive to the parameters. one example is the beta distribution beta-i(p, q) with mean = p∕(p + q), which is asymmetric in the parameters p and q and variance"
57,1,"['distribution', 'symmetric', 'logarithmic distribution']", Parameter Versus Statistic,seg_21,"which is symmetric in p and q. if p is increased or decreased by keeping q fixed, the distribution changes shape rapidly. the logarithmic distribution"
58,0,[], Parameter Versus Statistic,seg_21,1 q − 1). this discussion shows that various parame-
59,1,"['functions', 'parameters', 'asymptotic', 'location', 'moments', 'mean', 'parameter', 'standard', 'convergence', 'dependence', 'variance', 'distributions']", Parameter Versus Statistic,seg_21,"ters contribute differently to the mean and variance in particular and to other moments in general. this complexity can be used as a measure of the parameter dependence on the shape of distributions. this also affects the asymptotic convergence behavior of various distributions to other standard distributions. when there exist three or more parameters, they may all contribute as functions to the location and scale. for"
60,1,"['f distribution', 'distribution', 'mean', 'noncentral', 'noncentral f']", Parameter Versus Statistic,seg_21,"n m+  example, the noncentral f distribution ncf(m, n,  ) has mean   = , for n   2,"
61,1,"['sample', 'linear', 'degrees of freedom', 'estimated', 'parameters', 'nonlinear', 'population']", Parameter Versus Statistic,seg_21,"n−2 m which is linear in , and nonlinear in the degrees of freedom parameters. if the population parameters are unknown, they are estimated from a sample drawn from that population."
62,1,"['sample', 'statistic', 'function']", Parameter Versus Statistic,seg_21,definition 1.7 a well-defined function of the sample values is called a statistic.
63,1,"['sample size', 'sample', 'functions', 'estimated', 'sample mean', 'parameters', 'integer part', 'random sample', 'data', 'statistic', 'mean', 'population', 'random', 'function']", Parameter Versus Statistic,seg_21,"by our definition, a statistic does not involve the unknown population parameters, but it could involve the sample size or any function of it. for example, if x is a random sample of size n from a population with sample mean x, then n − x is a statistic as it involves the sample values and sample size. the unknown population parameters are estimated using a statistic (or a function of it, which is also a statistic if it does not involve unknowns). we have used the word “well-defined function” in our definition of statistic. this includes not only arithmetic and other mathematical functions but also special functions such as minimum and maximum of sample values and integer part of data values (ceil and floor functions in computer programming)."
64,1,"['combination', 'negative binomial', 'binomial', 'hypergeometric', 'statistical', 'distributions']", COMBINATION NOTATION,seg_23,"the combination notation is used in several statistical distributions. examples include the binomial, negative binomial, hypergeometric, and beta-binomial distributions. we"
65,0,[], COMBINATION NOTATION,seg_23,"x), which is read as “n choose x.” other notations include ncx and"
66,1,['combination'], COMBINATION NOTATION,seg_23,"c(n, x). this represents the combination of n things taken x at a time. symbolically,"
67,0,[], COMBINATION NOTATION,seg_23,"where n! is pronounced either as “n factorial” or as “factorial n” and abbreviated as “fact n.” by convention 0! = 1! = 1. as the numerator and denominator involve products of integers, it can be evaluated in multiple ways. write the n! in the numerator as n ∗ (n − 1)!, and the x! in the denominator as x ∗ (x − 1)! to get"
68,0,['n'], COMBINATION NOTATION,seg_23,"as it represents the number of ways to select x items out of n distinct items, ( n"
69,0,['n'], COMBINATION NOTATION,seg_23,x) is always an integer when n and x are integers. formula (1.8) can result in approx-
70,0,[], COMBINATION NOTATION,seg_23,"imations (for example, ( 3"
71,0,[], COMBINATION NOTATION,seg_23,owing to floating point truncations (5/3 is truncated to 1.6666). a solution is to use pascal’s identity
72,1,"['negative binomial', 'binomial', 'hypergeometric', 'hypergeometric distributions', 'distributions']", COMBINATION NOTATION,seg_23,"as this involves only addition, it will always give an integer as the final result. a related notation used in negative binomial and negative hypergeometric distributions"
73,1,"['design of experiments', 'design', 'statistics', 'discrete', 'probability', 'random', 'numerical', 'sample', 'probability distributions', 'expectations', 'sampling distributions', 'distributions', 'functions', 'contingency tables', 'regression', 'summation', 'continuous', 'random variables', 'variables', 'sampling', 'order statistics', 'tables', 'experiments', 'continuous distributions']", SUMMATION NOTATION,seg_25,"the ubiquitous ∑ notation is extremely useful to express functions of sample values, random variables, complicated sequences, series, and so on in concise form. this section introduces several summation notations that are extensively used in the present and subsequent chapters. a good grasp of various summation notations is essential for students and practitioners of statistics and for people in many other disciplines such as algorithmics, numerical methods, digital signal processing, and parallel computing. it is extensively used in probability distributions (more so in discrete than in continuous distributions), sampling distributions, mathematical probability and expectations, generating functions, design of experiments, regression and correlation, contingency tables, and order statistics, to name a few. there are many variants to the summation notation. all of them starts with the greek capital symbol ∑"
74,1,"['range', 'variable', 'associated']", SUMMATION NOTATION,seg_25,"(which is read as sum). each ∑ has an associated index variable (indexvar)3 and has an implied meaning of an iteration or enumeration of the expression that follows it (summand) over the range of the implicit indexvar (which is assumed to be an integer and is usually denoted by i, j, k, p, q, or r with or without subscripts). thus, all of the following expressions are equivalent:"
75,1,['set'], SUMMATION NOTATION,seg_25,"the indexvar could also be explicit when the set notation is used for iteration. the subscript of ∑ denotes the initial values, conditions, or initializations, and the superscript denotes the terminal (final) values or conditions. the subscript, superscript, or both can also be missing, as in the above-mentioned example, if they can be inferred from the context."
76,1,"['probabilities', 'binomial', 'function', 'varying']", SUMMATION NOTATION,seg_25,"in the great majority of applications, the indexvar varies from low values to high values. however, there are a few applications in which this can either be in the reverse (high to low) or two-way varying. as an example of reverse summing, consider the problem of accumulating binomial right-tail probabilities (survival function) until it accumulates to say c. this is mathematically expressed as"
77,1,"['poisson', 'incomplete beta', 'memory', 'precision', 'probabilities', 'beta function', 'noncentral beta', 'incomplete beta function', 'distribution', 'function', 'summation', 'memory overflow', 'poisson distribution', 'noncentral', 'error', 'distributions']", SUMMATION NOTATION,seg_25,"the indexvar here is chosen as x to indicate that we are summing probabilities. this can be easily accumulated by starting the summation at x = n and iterating backward (x = n − 1, x = n − 2, · · · , x = k) until the desired sum is accumulated (this can also be expressed in terms of the incomplete beta function; see page 6–34). similarly, infinite poisson-weighted distributions such as noncentral  2 and noncentral beta need to evaluate the poisson weights e−  x∕x! for large   values. if the computations are carried out in single precision arithmetic, the first poisson term e−  0∕0! = e−  will result in a memory underflow (a machine will misinterpret it as zero) for     104 and a memory overflow for     183.805 (see [47], pp. 231–232 for details). this leads to error propagation, as each subsequent term is evaluated iteratively. as the mode of the poisson distribution is  , we could start the computations at   and iterate in both directions [49, 51, 52]. this will allow one to compute such pdf and cdf for much higher   values. alternatively, we could use double precision arithmetic."
78,1,"['variables', 'nested sum']", Nested Sums,seg_27,"nested sums and iterations are usually denoted by multiple∑ symbols4. for instance, ∑j∑k denotes a nested sum over two index variables j and k. when multiple ∑ occur"
79,1,"['statistic', 'process']", Nested Sums,seg_27,"as a block, they are evaluated from right to left. in other words, the leftmost indexvar is fixed and right indexvars are varied in the summand until they are complete, thereby accumulating the partialsum. then, the leftmost indexvar is incremented using the step size, and the process is continued. one common example is pearson’s 2 statistic:"
80,1,"['nested sums', 'table', 'variables', 'range', 'frequencies', 'symmetric', 'variable', 'set', 'summation', 'dummy variables']", Nested Sums,seg_27,"where ojk are the observed and ejk are the expected frequencies in an r × c table. the order in which nested sums are found is sometimes important (as in matrix multiplications). in the above-mentioned sum, we iterate first over the (inner) k variable, followed by the (outer) j variable. thus, j is set to its lowest value (=1), and k is varied over its full range (1 to c). then, j is incremented by 1 and k is again varied over its full range, and so on. if the expression to be evaluated is symmetric in the dummy variables (as in equation 1.12), the order of summation can be interchanged. the summand may sometimes be tightly coupled with the indexvars. consider the minimization criterion used in k-means clustering as"
81,1,['data'], Nested Sums,seg_27,"where |cj| is the number of data points in cluster j, cj’s are the cluster centroids, k is the"
82,0,[], Nested Sums,seg_27,"number of clusters, and xi"
83,1,['data'], Nested Sums,seg_27,"(j) denotes ith data value in jth cluster. here, the summand is tightly coupled with the outer indexvar. thus, we cannot easily interchange the summations."
84,1,"['nested summations', 'variable', 'dependence', 'variable names']", Nested Sums,seg_27,"when there are several nested summations in one block, each of them must be assigned a unique index variable. the variable names do not actually matter, as they are dummy indexvars. any constant multiplier(s) that does not depend upon the indexvars could be taken outside all summations. any multiplicand expressions that do not depend on the inner indexvars could be moved as much outside as possible such that their dependence is only on the indexvars to their left. for example, consider the"
85,1,['independent'], Nested Sums,seg_27,"as  j is independent of i and c is a constant, we could rewrite it as"
86,0,[], Nested Sums,seg_27,"(see exercise 1.51). similarly,"
87,1,"['dependent', 'limit']", Nested Sums,seg_27,"we could also combine multiple ∑ into a single ∑ if there is no scope for confusion. thus, the above-mentioned double sum in equation (1.12) can also be written using ∑j,k. however, multiple ∑ is the recommended notation, as it is easy to comprehend and useful to convert such expressions into computer programs. moreover, the indexvars in inner sums are sometimes dependent on the outer indexvars (as in equation 1.13, where the upper limit of the inner indexvar “i” depends on the outer indexvar j. see also equations 1.30 and 1.31 in page 1–26 and 1–38 in page 1–30)."
88,1,"['statistics', 'numerical', 'exponentially']", Increment Step Sizes,seg_29,"the increments of the indexvar are assumed to be in steps of 1 by default. this is true for most of the summations that we encounter in statistics and computer science. however, there exist some applications in engineering and numerical computing where the increments are fractions. if this increment is in steps of c (≠1), it is indicated at the middle of the ∑ symbol. the increment step c can in general be a multiple of an integer or a fraction. it can rarely grow exponentially in some applications (see exercise 1.48 pp. 1–68)."
89,1,['summation'], Increment Step Sizes,seg_29,"1.6.2.1 fractional incremental steps (fiss) when the increment is a fraction, we could recode the indexvar to force it to be an integer and adjust the summand accordingly. for example, consider the summation ∑j"
90,1,"['function', 'parameters']", Increment Step Sizes,seg_29,"n =1 f (j), where f(j) is any arbitrary function of j (along with other parameters), the indexvar j varies in steps of c = 0.5, and n is an integer or half-integer 1. we could write it as∑j"
91,1,['vary'], Increment Step Sizes,seg_29,"here, the indexvar has been “inflated” to vary from 1 to 2n − 1 in steps of 1 and ( j + 1)∕2 has been substituted in the summand to compensate for the inflated index. in general, if we need to increment j from u to v in steps of a proper fraction c = 1∕k, then the indexvar is inflated to vary from u to (  ∗ k − u ∗ (k − 1)), and the sum is"
92,0,[], Increment Step Sizes,seg_29,"as u and k are known constants, we could also write this as"
93,0,[], Increment Step Sizes,seg_29,"where k′ = u(1 − 1∕k). indeed, changing the indexvar as i = j − u, this could also be written in the alternative form"
94,0,[], Increment Step Sizes,seg_29,which is much better suited for computer implementation.
95,0,[], Increment Step Sizes,seg_29,example 1.1 simplified sum
96,0,[], Increment Step Sizes,seg_29,"simplify∑j f (j) for index j from 1 to 3 in steps of 1/3, using the above-mentioned technique."
97,0,[], Increment Step Sizes,seg_29,"1.6.2.2 integral incremental steps (iiss) if the indexvar increments in steps of 2, we evaluate the sum ∑j"
98,0,[], Increment Step Sizes,seg_29,"where ⌊x⌋ denotes the floor operator that returns the largest integer less than x. if c is an integer  2, we modify the sum as"
99,1,"['vary', 'discrete']", Increment Step Sizes,seg_29,"these are used in discrete signal processing and transforms. in general, if we need to increment j from u to v in steps of an integer multiple c ≥ 2, then the index is deflated to vary from 0 to (v − u)/c, and the sum is evaluated as"
100,1,['case'], Increment Step Sizes,seg_29,a special case is accumulating the sum ∑j
101,1,['limit'], Increment Step Sizes,seg_29,"equation (1.24) is valid for both c an integer and a fraction. when c is a fraction, the upper limit ⌊2k∕c⌋ is simply blown up."
102,1,['symmetric'], Increment Step Sizes,seg_29,example 1.2 simplified symmetric sum
103,0,[], Increment Step Sizes,seg_29,"if j varies in fractional steps of 1/3, simplify the following sums:– (i)"
104,1,['limit'], Increment Step Sizes,seg_29,"solution 1.2 (i) using (1.24), the first sum has upper limit ⌊2 × 1∕(1∕3)⌋ = 6, so that it can be unfolded as ∑j"
105,0,[], Increment Step Sizes,seg_29,example 1.3 simplified double sum
106,0,[], Increment Step Sizes,seg_29,"if j varies in fractional steps of 1/4, and k varies in integer steps of 3, simplify following sums:– (i) s1 = ∑j"
107,1,['case'], Increment Step Sizes,seg_29,"solution 1.3 in case (i), u = 2,    = 5, and c = 1∕4 so that we apply fis first to get"
108,1,['case'], Increment Step Sizes,seg_29,"in case (ii), we first apply fis to get"
109,0,[], Increment Step Sizes,seg_29,"where the inner index still increments in step size 1/4. next, we apply iis to indexvar k to get"
110,1,"['statistics', 'summation']", Increment Step Sizes,seg_29,"the most frequent use of ∑ notation in statistics is to denote the arithmetic sum of n quantities that are distinguished only by one or more subscripts. in the following discussion, we introduce the most common summation notations."
111,1,"['varying', 'summation']", Increment Step Sizes,seg_29,1. subscript fully varying summation
112,1,['summation'], Increment Step Sizes,seg_29,consider the summation
113,1,"['statistics', 'disjoint events', 'probability', 'random', 'data', 'probability theory', 'events', 'samples', 'disjoint', 'summation', 'random variables', 'variables', 'variable', 'order statistics', 'bivariate data', 'bivariate']", Increment Step Sizes,seg_29,"where xj is the jth value of the series x[]. here, each of the xj’s are either known data values or random variables. the j is called the summation variable or index of summation. this notation is used in arithmetic means of samples, of random variables, in order statistics, and in probability theory for disjoint events. because bivariate data are arranged as a matrix and identified by a row and a column, we could extend the above-mentioned notation as ∑i"
114,1,['summation'], Increment Step Sizes,seg_29,"n =1 xij to denote the sum of all of the m × n entries or values. this is sometimes compactly written as ∑i∑jxij or as ∑i,jxij. the summation notation is also used in vari-"
115,1,['summation'], Increment Step Sizes,seg_29,n =1 (xj − x)2∕(n − 1). consider the summation ∑j
116,1,"['case', 'data', 'bivariate data', 'bivariate']", Increment Step Sizes,seg_29,"· · · + (xn − c)k, where xj is the jth value of the series x[], and c,k are constants. as in the above-mentioned case, we could extend this to bivariate data as"
117,1,['variables'], Increment Step Sizes,seg_29,"n =1 xjyj, where xj and yj are the values of two traits generated by the jth subject. when there are many traits, we may represent them by an additional subscript, rather than by separate variables as in ∑i"
118,1,"['varying', 'summation']", Increment Step Sizes,seg_29,"2. subscript partially varying summation this is a variant of the above-mentioned notation, where we use ≠ or ≥ to restrict one or more summation indexes. consider the problem of summing the elements in the upper triangular portion (above the diagonal) of a 2d array. if xij denotes the (i, j)th element, this sum is given by s = ∑i"
119,1,['summation'], Increment Step Sizes,seg_29,"n =1 ∑j≥ixij. in some applications, we may have to omit particular values of the summation index as in ∑i"
120,1,"['sample', 'variance', 'sample variance']", Increment Step Sizes,seg_29,"n =1;j≠i xij. as another example, the sample variance can be represented as"
121,1,['symmetric'], Increment Step Sizes,seg_29,"where the inner indexvar depends on the outer indexvar. as this is symmetric in xi and xj, we could also write it as"
122,1,"['information', 'sets', 'set', 'summation']", Increment Step Sizes,seg_29,"3. summation over a set this is an extension of the above-mentioned notation. in some applications, we have distinct nonoverlapping subsets that makeup a set. we may have to accumulate some information about each of the subsets. the above-mentioned summation notation can be modified such that the summation index varies over each subset:∑j∈sj f (xj). these sets can be specified either explicitly or implicitly"
123,1,"['condition', 'set', 'summation', 'probability']", Increment Step Sizes,seg_29,"using a condition. for instance, the set of all odd integers can be specified as “j is odd,” where j is the summation index. suppose that we are interested in finding the probability of an even number of heads when six coins are thrown. let y"
124,1,"['distribution', 'binomial', 'probability', 'event', 'binomial distribution', 'tail', 'tails']", Increment Step Sizes,seg_29,"denotes the event that an even number of heads appear. the possible values of y are s1 = 0, 2, 4, 6. here, y = 0 indicates that all six coins ended up as tails and six indicates that all of them were heads. we know that this is solvable using the binomial distribution with n = 6. if p denotes the probability of a head showing up and q = 1 − p denotes the probability of a tail showing up, we are interested in finding the probability"
125,1,"['function', 'variable', 'summation']", Increment Step Sizes,seg_29,4. function of summation variable
126,1,['summation'], Increment Step Sizes,seg_29,consider the simple summation ∑j
127,1,"['poisson', 'tail areas', 'probability', 'function', 'tail']", Increment Step Sizes,seg_29,"n =1 j2. here, our summand (the quantity summed) is either j itself or a function of it. a more complicated example is the tail areas of poisson probability defined as pj(  ) = e−  j∕j! or"
128,1,"['probabilities', 'binomial', 'function', 'tail']", Increment Step Sizes,seg_29,"the binomial density bj(n, p) = ( n j ) pj(1 − p)n−j = (1 − p)n ( n j ) (p∕q)j, for j = 0, 1, · · · , n, and q = 1 − p. the sum of the probabilities on the left tail is called the cdf and that on the right tail is called the survival function. symbolically,"
129,1,"['function', 'binomial']", Increment Step Sizes,seg_29,"k =0 e−  j∕j! is the cdf up to and including k, where k is a number in (0,∞). similarly, the binomial survival function is given by gx(k, n, p) ="
130,1,"['varying', 'summation']", Increment Step Sizes,seg_29,5. superscript varying summation
131,1,['summation'], Increment Step Sizes,seg_29,consider the summation ∑k
132,0,[], Increment Step Sizes,seg_29,"n =1 (xj − c)k or ∑k(xj − c)k for short. this denotes the expanded sum (xj − c)1 + (xj − c)2 + · · · + (xj − c)k, where xj, the jth value of the series x[], and c are constants. this notation is used in generating func-"
133,0,[], Increment Step Sizes,seg_29,"tions. in differential calculus, it denotes jth derivative as in d"
134,0,[], Increment Step Sizes,seg_29,which is interpreted as applying the differential operator ( d
135,1,"['function', 'cumulant']", Increment Step Sizes,seg_29,"d x ) repeatedly j times. as another example, the jth derivative of cumulant generating function is"
136,1,['cumulant'], Increment Step Sizes,seg_29,"( ∕ t)jkx(t) = ( ∕ t)j ln(mx(t)) =  j +  j+1(t∕1) +  j+2(t2∕(1 ∗ 2)) + · · · , (1.33) from which by putting t = 0, we could separate out the jth cumulant. putting"
137,1,"['states', 'processes', 'discrete', 'mean', 'function', 'stochastic processes']", Increment Step Sizes,seg_29,"r (see also 9.1 (pp. 9–2)). note that the superscript may or may not mean powers. for instance, it means various states in stochastic processes and game theory. it could be negative in time-dependent autoregressive processes and discrete signal processing. for example, the generating function a(z, n) = ∑j"
138,1,['process'], Increment Step Sizes,seg_29,n =1 aj(n)z−j denotes the autoregressive process x(n) = −∑i
139,1,"['sample', 'data', 'parameter', 'estimator']", Increment Step Sizes,seg_29,"n =1 ai(n)x(n − i) +  (n), where negative powers of z denote time lags to the past from the reference point. if it does not imply powers, we could enclose them in parenthesis to avoid confusion. a superscript could denote an omitted data value as in the jackknife estimator of a parameter using a sample of size"
140,1,"['sample', 'data', 'estimate']", Increment Step Sizes,seg_29,where tn is the estimate using all sample values and t(j) denotes the estimate n−1 without jth data value (using other n-1 values).
141,1,"['moment generating function', 'function', 'moment', 'noncentral beta', 'mean', 'parameter', 'parameters', 'shape parameters', 'combination', 'distribution', 'summation', 'noncentrality parameter', 'noncentral', 'beta distribution', 'noncentrality', 'dummy variable', 'variable']", Increment Step Sizes,seg_29,"6. combination summation this type of summation may involve a combination of the above-mentioned two types. consider the expression mx(t) = ∑∞ j=0(t j∕j!) j, which is called the ordinary moment generating function. here, the summation index appears as a subscript on  , as superscript on the dummy variable t, and as a function 1∕j!. as another example, consider the noncentral beta distribution ncb(p, q,  ) with shape parameters (p, q), and noncentrality parameter    0. the mean of this distribution is expressed as an infinite sum of poisson-weighted central beta means as"
142,1,['table'], Increment Step Sizes,seg_29,"where c = p + q + ∕2 [52]. here, j appears as powers of ( ∕2) and as a funcp+j tion (see table 8.3, pp. 8–41 in chapter 8). j!(p+q+j)"
143,1,"['maximum likelihood estimation', 'geometric mean', 'conditional', 'discrete', 'factorial', 'likelihood function', 'function', 'geometric', 'falling factorial', 'numerical', 'conditional distributions', 'correlations', 'mean', 'distributions', 'parameters', 'estimation', 'distribution', 'moments', 'summation', 'factorial moments', 'likelihood', 'maximum likelihood']", PRODUCT NOTATION,seg_31,"we have used the + operator in the summation notation discussed earlier. there are many situations where we need to use the product (*) operator instead of +. examples are the rising and falling factorial moments, geometric mean (section 2.7, pp. 2–29), likelihood function in maximum likelihood estimation (mle) of statistical parameters, multivariate distribution theory, conditional distributions, multiple and partial correlations, and some special numbers. it is also used in inverse discrete fourier transforms, numerical interpolation, and many other engineering fields. if x1, x2, · · · , xn are nonzero numbers (positive or negative), their product is denoted as"
144,1,"['function', 'geometric']", PRODUCT NOTATION,seg_31,the objective function in geometric programming (gp) is the posynomial
145,0,[], PRODUCT NOTATION,seg_31,"ajt , where ct  0, ajt are real and xj  0 ∀j. applying"
146,0,[], PRODUCT NOTATION,seg_31,equation 1.36 immediately gives
147,1,"['probabilities', 'multinomial']", PRODUCT NOTATION,seg_31,n! the multinomial probabilities are expressed as x1!.x2!···xn!px
148,1,['probabilities'], PRODUCT NOTATION,seg_31,"2 … pn xn , where p′ js are probabilities that add up to 1 and ‘.’ denotes multiplication (*) (see chapter 6). this"
149,0,[], PRODUCT NOTATION,seg_31,can be concisely written as p = ∏n
150,0,[], PRODUCT NOTATION,seg_31,"equation 1.36, this becomes"
151,0,[], PRODUCT NOTATION,seg_31,′s are integers. this
152,0,[], PRODUCT NOTATION,seg_31,can be written as
153,1,"['sample', 'likelihood', 'distribution', 'likelihood function', 'function']", PRODUCT NOTATION,seg_31,"for an mle example, let fx(x,  ) be the pdf of a distribution from which a sample x1, x2, · · · , xn of size n is drawn. then, the likelihood function is l(x1, x2, · · · , xn;  ) = ∏j"
154,1,"['estimated', 'parameters', 'likelihood']", PRODUCT NOTATION,seg_31,"n =1 fx(xi,  ). the unknown parameters are then estimated by maximizing the likelihood or equivalently maximizing the log-likelihood."
155,1,['independent'], PRODUCT NOTATION,seg_31,"expressions independent of the indexvars can be taken outside all independent products. for example, consider the product p = ∏j"
156,1,['symmetric'], PRODUCT NOTATION,seg_31,"(note that j varies k+1 times and i varies m+1 times). as the expression within the product5 is symmetric in i and j, we could also write it as p = ck+m+2 ∏i"
157,1,['case'], PRODUCT NOTATION,seg_31,"k =0  j). in the particular case when u =   (uj =  j), this simplifies to"
158,1,"['poisson', 'gamma', 'probability distributions', 'pareto', 'probability', 'poisson distributions', 'weibull', 'distributions']", Evaluating Large Powers,seg_33,"expressions of the form xn, (1 − x)n, or  x occur in many probability distributions such as gamma, beta, weibull, pareto, power series, and poisson distributions. in some applications, we need to compute the pdf for just a few x values for a large"
159,1,"['efficient', 'method']", Evaluating Large Powers,seg_33,fixed n or large integer values of x for fixed  . there are many ways to evaluate them. the following example first considers an efficient method when the power n is large and then explains the computational details when both x and n are large.
160,0,['n'], Evaluating Large Powers,seg_33,evaluate xn where n is a large integer.
161,1,['cases'], Evaluating Large Powers,seg_33,"solution 1.4 we consider two cases depending on whether n is a power of 2 or not. (i) let n = 2m, where m is an integer. we could evaluate it as xn∕2 ∗ xn∕2, where each of the terms are recursively evaluated."
162,1,['case'], Evaluating Large Powers,seg_33,"case (ii): n is not a power of 2. if n is of the form 2k ±  , where v is a small number (say 1,2, or 3), we could still utilize case (i). as examples, n = 15 = 24 − 1, so that x15 = x16∕x, and x67 = x64 ∗ x3. otherwise, we convert n into its binary representation as n = bkbk−1 · · · b1b0, where b0 is the least significant bit (lsb) and bk is the most significant bit (msb). the n and bj"
163,1,['summation'], Evaluating Large Powers,seg_33,"k =0 bj2j, where we have rearranged the summation index to match significant bits from right"
164,1,['case'], Evaluating Large Powers,seg_33,"k =0 (x2j )bj . as the powers of all x’s are of the form 2k, the case (i) applies for large k. because bj"
165,0,[], Evaluating Large Powers,seg_33,′s are binary digits
166,0,[], Evaluating Large Powers,seg_33,the above-mentioned product could also be expressed as xn = ∏j
167,1,"['method', 'prime number']", Evaluating Large Powers,seg_33,"k =0, bj=1 (x2j )bj . sequential algorithms convert a decimal number (n 0) into its binary representation using the repeated division by base (=2) method. this generates the binary digits from lsb to msb. we could then check each and every bit to see if it is 1 and accumulate the corresponding product term (x2j )bj immediately. this is especially useful in cryptography applications that work with expressions of the form xp − 1 where p is a very large prime number. as fractions are converted into binary using the repeated multiplication by base method, the above-mentioned discussion is equally applicable to evaluate expressions involving x1∕n too, where n is large."
168,1,"['case', 'factorization', 'factorization theorem']", Evaluating Large Powers,seg_33,"in the particular case when x and n are both very large, we break x using the prime factorization theorem into x = p1"
169,0,[], Evaluating Large Powers,seg_33,k =0 bj2j . this can be simplified into the form
170,0,[], Evaluating Large Powers,seg_33,j+1 j j where the inner product is carried out only for bj = 1. note that p2
171,0,[], Evaluating Large Powers,seg_33,"i = p2 i ∗ p2 i . hence, these can be kept in an array and updated in each pass."
172,1,"['function', 'statistics']", Evaluating Large Powers,seg_33,"we could also combine multiple products as well as sums and products. multiple products are, however, not of much use in engineering statistics. the index of summation and products could also be a function (say r(j)) of the respective indexvars"
173,0,[], Evaluating Large Powers,seg_33,"as used in [157], page 27–36. an extension is to allow the step size to be any integer  1. for example, when the step size is 2, we get"
174,1,"['discrete', 'factorial', 'coefficients', 'falling factorials', 'distributions', 'binomial coefficients', 'discrete distributions', 'moments', 'binomial', 'rising and falling factorials', 'factorial moments']", RISING AND FALLING FACTORIALS,seg_35,"this section introduces a particular type of the product form presented earlier. these expressions are useful in finding factorial moments of discrete distributions whose pdf involves factorials or binomial coefficients. in the literature, theseare known as pochhammer’s notation for rising and falling factorials. this will be explored in subsequent chapters."
175,1,"['factorial notation', 'rising factorial', 'factorial']", RISING AND FALLING FACTORIALS,seg_35,1. rising factorial notation
176,1,"['rising factorial', 'factorial', 'variable']", RISING AND FALLING FACTORIALS,seg_35,"factorial products come in two flavors. in the rising factorial, a variable is incremented successively in each iteration. this is denoted as"
177,1,"['factorial', 'factorial notation', 'falling factorial']", RISING AND FALLING FACTORIALS,seg_35,2. falling factorial notation
178,1,"['factorial', 'variable', 'falling factorial']", RISING AND FALLING FACTORIALS,seg_35,"in the falling factorial, a variable is decremented successively at each iteration. this is denoted as"
179,1,"['distribution', 'moments', 'significance', 'statistical']", MOMENTS AND CUMULANTS,seg_37,moments of a distribution are denoted by the greek letter . they have great theoretical significance in statistical distribution theory. moments can be defined about any arbitrary constant “c” as
180,1,"['discrete distributions', 'discrete', 'distributions']", MOMENTS AND CUMULANTS,seg_37,"⎧ (x − c)nf (x), for discrete distributions ⎪∑  n(c) = e(x − c)n = ⎨ x (1.44)"
181,1,"['continuous', 'distributions', 'continuous distributions']", MOMENTS AND CUMULANTS,seg_37,"⎪⎩∫x(x − c)nf (x)dx, for continuous distributions."
182,1,['moments'], MOMENTS AND CUMULANTS,seg_37,"if c = 0 in this definition, we get raw moments, and when c =  1 =  , we get the central moments."
183,1,"['sample', 'moments', 'sample moments']", MOMENTS AND CUMULANTS,seg_37,the corresponding sample moments are analogously defined as mk(c) =
184,1,"['sample', 'method', 'moment', 'sample mean', 'cauchy', 'moment generating function', 'cauchy distribution', 'distribution', 'moments', 'mean', 'population', 'standard', 'function', 'sample moments']", MOMENTS AND CUMULANTS,seg_37,"n =1 (xj − c)kf (xj). the population moments may not always exist, but the sample moments will always exist. for example, the mean of a standard cauchy distribution does not exist, however, the sample mean exists. the moment generating function (mgf) provides a convenient method to express population moments. it is defined"
185,1,"['function', 'cumulant']", MOMENTS AND CUMULANTS,seg_37,"the logarithm of the mgf is called cumulant generating function (kgf). symbolically,"
186,1,['functions'], MOMENTS AND CUMULANTS,seg_37,see also equation (1.33) in pp. 1–31. l-moment is an extension that is discussed in ref. 145. a thorough discussion of generating functions appears in chapter 9.
187,1,"['linear', 'test statistic', 'change of scale', 'data', 'statistical', 'statistic', 'mean', 'variance', 'linear transformation', 'transformation', 'analysis of variance', 'test', 'anova']", DATA TRANSFORMATIONS,seg_39,"data transformation is used in various statistical analyses. it is especially useful in hand computations when the numbers involved are too large or too small. computing summary measures such as mean and variance of large numbers can be simplified using linear transformation techniques discussed in the following. similarly, as the test statistic in analysis of variance (anova) computations involves the ratio of sums of squares, a change of scale transformation is applicable. if the spread (variance) of data are too large or too small, an appropriate change of scale transformation can ease the visualization of data."
188,1,"['sample', 'data']", Change of Origin,seg_41,"as the name implies, this shifts all data points linearly (by subtracting or adding a constant from each sample value) as y = x − c. the constant c (positive or negative) is preferably an integer when sample values are all integers."
189,0,[], Change of Origin,seg_41,example 1.5 reservoir inflow
190,1,"['change of origin', 'mean', 'method']", Change of Origin,seg_41,"the amount of water inflow into a reservoir during 6 hours in cubic feet is x = {286, 254, 242, 247, 255, 270}. apply the change of origin method and find the mean."
191,1,['observation'], Change of Origin,seg_41,"solution 1.5 we subtract 240 (chosen arbitrarily) from each observation to get xi ′ = xi − 240 as x′ = {46, 14, 2, 7, 15, 30}, from which ∑ixi"
192,1,"['mean', 'data']", Change of Origin,seg_41,′ = 114. the mean ′ of x′ is x = 114∕6 = 19. the mean of the original data is 240 + 19 = 259 = x.
193,1,"['range', 'change of origin', 'data', 'transformation']", Change of Origin,seg_41,it is trivial to prove that the range of original data is preserved by a change of origin transformation because (xn − k) − (x1 − k) = xn − x1 and (xn + k) − (x1 + k) = xn − x1.
194,1,"['sample', 'variability', 'observations', 'observation']", Change of Scale,seg_43,"this technique divides each large observation by the same constant ( 1). this is useful when numbers are large and has high variability. examples are family income, total insured amounts, annual insurance premiums, defaulting loan amounts, advertising expenses in various media and regions, and so on. let x1, x2, ..xn be “n” sample values and c be a nonzero constant. define yi = xi∕c. if c is less than the minimum of the observations, each of the yi’s are greater than 1, if xi’s are positive. similarly, if c is greater than the maximum of the observations, each yi’s are less than 1. for values of c between minimum and maximum of the sample, we get values on the real line (positive real line if all xi’s are positive). if all values are small fractions, we may multiply by a constant to scale them up."
195,1,"['change of scale', 'mean']", Change of Scale,seg_43,example 1.6 change of scale to find the mean
196,1,"['measurements', 'mean', 'data']", Change of Scale,seg_43,"error measurements of a device are as follows. scale the data and find the mean:- x = {0.001, 0.006, 0.0095, 0.015, 0.03}."
197,1,['mean'], Change of Scale,seg_43,"solution 1.6 choose c = 1000 and scale using yi = cxi to get y = {1, 6, 9.5, 15, 30}. this is an example of decimal scaling in which the decimal point is moved by multiplying/dividing by a power of 10. the mean of y is y = 61.5∕5 = 12.3 and thus the mean of x is x = 0.0123."
198,1,"['transformed', 'data', 'intervals']", Change of Origin and Scale,seg_45,"this is the most frequently used technique to standardize data values. depending on the constants used to change the origin and scale, a variety of transformed intervals can be obtained."
199,1,"['sample', 'transformed', 'interval', 'range', 'transformation']", Change of Origin and Scale,seg_45,"theorem 1.1 a sample in the range (a,b) can be transformed to a new interval (c, d) by the transformation y = c + [(d − c)∕(b − a)] ∗ (x − a)."
200,1,"['range', 'intervals']", Change of Origin and Scale,seg_45,"proof: by putting x = a in the expression gives y = c. putting x = b gives y = c + [(d − c)∕(b − a)] ∗ (b − a) = c + (d − c) = d. as (x − a)∕(b − a) and (y − c)∕(d − c) both map points in the respective intervals to the [0,1] range, all intermediate values in (a,b) get mapped to a value in (c,d) range. this proves the result."
201,1,"['range', 'transform', 'data']", Change of Origin and Scale,seg_45,"amount of fluoride (in milligrams) in drinking water collected from six places are [60, 90, 118, 150, 165, 170]. transform the data to the range [10, 60]."
202,1,['transformation'], Change of Origin and Scale,seg_45,"solution 1.7 here a = 60, b = 170, c = 10, d = 60. thus (d − c)∕(b − a) = (60 − 10)∕(170 − 60) = 5∕11. hence, the required transformation is yi = 10 + (5∕11)(xi − 60). substitute each successive value of x to get y = [10, 23.6364, 36.3636, 50.9091, 57.7273, 60]."
203,1,"['sample', 'transformed', 'interval', 'range', 'transformation']", Change of Origin and Scale,seg_45,"corollary 1 prove that the sample x in the range (a,b) can be transformed to a new interval (c,d) by the transformation"
204,1,"['sample', 'interval', 'transformation']", MinMax Transformation,seg_47,"this transformation is used to map any sample values to the interval [0, 1], [−1, +1], and so on."
205,1,"['sample', 'interval', 'range', 'variable', 'transformation', 'numeric variable']", MinMax Transformation,seg_47,"theorem 1.2 any numeric variable x in the interval (xmin, xmax) can be transformed to a new interval [0, 1] by the transformation y = (x − xmin)∕r, where r = (xmax − xmin) is the range and xmin, xmax are the minimum and maximum of the sample values."
206,0,[], MinMax Transformation,seg_47,"proof: substituting x = xmin gives y = 0 and x = xmax gives y = 1. hence, the transformed values are mapped to [0, 1]."
207,1,"['range', 'data']", MinMax Transformation,seg_47,"transform the fluoride data in page 1–38 into the [0,1] range."
208,1,"['sample', 'interval', 'range', 'change of origin', 'transformation', 'change of origin and scale']", MinMax Transformation,seg_47,"lemma 1 a sample x in any finite range can be mapped to the interval [−1, +1] by a simple change of origin and scale transformation."
209,1,"['transformation', 'interval']", MinMax Transformation,seg_47,"proof: consider the transformation y = 2 ∗ [(x − xmin)∕r] − 1, where r = (xmax − xmin). when x = xmin, y becomes −1 and when x = xmax, y = +1. all intermediate values are mapped to points within the interval [−1,+1]. thus the result. this holds even if xmin is negative."
210,1,"['interval', 'data']", MinMax Transformation,seg_47,"transform the data [ 34, 43, 55, 62, 68, 74] to the interval [−1, +1]."
211,1,"['transformed', 'range', 'data']", MinMax Transformation,seg_47,"solution 1.9 here, the minimum is 34 and maximum is 74. thus, the range is 40. using lemma 1, we get the transformed data as y = [2 ∗ (x − 34)∕40] − 1 = [−1,−0.55, 0.05, 0.40, 0.70,+1]."
212,1,"['sample', 'range', 'transformation']", MinMax Transformation,seg_47,"theorem 1.3 a sample x in any finite range with at least two elements can be mapped to the range y = [−k,+k] by the transformation"
213,0,[], MinMax Transformation,seg_47,proof: putting x = xmin gives y = r
214,0,[], MinMax Transformation,seg_47,because r = (xmax − xmin). putting x = xmax gives y = r
215,1,['interval'], MinMax Transformation,seg_47,"k [2 ∗ xmax − (xmin + xmax)] = +k. all intermediate values are mapped to the interval (−k,+k). for instance xc = (xmin + xmax)∕2 gets mapped to 0. this proves the result."
216,1,"['data', 'intervals']", MinMax Transformation,seg_47,"transform the above-mentioned data into the intervals [−0.5, +0.5], and [−3, +3]."
217,1,"['range', 'transformation']", MinMax Transformation,seg_47,"solution 1.10 here k = 0.5, so that k∕r = 0.5∕(74 − 34) = 0.0125, xmin + xmax = 74 + 34 = 108, giving the transformation y = 0.0125 ∗ (2 ∗ x − 108). resulting y vector is [−0.5,−0.275, 0.025, 0.20, 0.35, 0.5]. for the [−3,+3] range, k = 3 and k∕r = 3∕40 = 0.075, giving the transformation y = 0.075 ∗ (2 ∗ x − 108). resulting values are [−3,−1.65, 0.15, 1.2, 2.1, 3]."
218,1,"['sample', 'case']", MinMax Transformation,seg_47,"remark 1 in the particular case when the sample has just two elements, they are mapped exactly to −k and +k respectively."
219,1,['sample'], MinMax Transformation,seg_47,"proof: consider a sample (x,y) of size two. rearrange them such that xmin = x, xmax = y or vice versa. substituting in equation 1.3, xmin and xmax gets mapped to −k and +k, respectively."
220,1,"['interval', 'transform', 'data']", MinMax Transformation,seg_47,"the incomes of six families are [34,000, 43,000, 55,000, 62,000, 68,000, 74,000]. transform the data to the interval [−1, +1]."
221,1,"['transformation', 'data']", MinMax Transformation,seg_47,"solution 1.11 we will choose c = 10, 000 and divide each value by c to get x′ = [3.4, 4.3, 5.5, 6.2, 6.8, 7.4]. here r = (7.4 − 3.4) = 4, k = 1 and (xmin + xmax) = 3.4 + 7.4 = 10.8, giving the transformation y = 0.25 ∗ (2 ∗ x − 10.8) where x varies over the original data values. the resulting y vector is [−1,−0.55, 0.05, 0.40, 0.70,+1]."
222,1,"['nonlinear', 'data', 'transformations', 'exponential', 'transformation', 'variance']", Nonlinear Transformations,seg_49,"linear data transformations may be insufficient in some engineering applications. the popular nonlinear transformations are square-root transformation, trigonometric and hyperbolic transformations, logarithmic and exponential transformations, power transformations, and polynomial transformations. these transformations are used either to stabilize the variance of the data or to bring the data into one of the well-known distributional form."
223,1,"['interval', 'statistics', 'standard normalization', 'transformation', 'sample', 'data', 'mean', 'scores', 'standard', 'standard deviation', 'error', 'variance', 'deviation']", Standard Normalization,seg_51,"this transformation is so called because it is extensively used in statistics to standardize arbitrary scores. here, the origin is changed using the mean of the sample, and the scale is changed using the standard deviation of the sample. symbolically yi = (xi − x)∕s, where s is the standard deviation. the resulting values of y are called z-scores and will almost always lie in the interval [−3, +3]. a disadvantage of this transformation is that it uses the mean and variance that need a single pass through the data. if standard normalization is applied manually, a quick check can be carried out as follows. if the sum of the z-scores is nonzero, it is an indication that either the calculation is wrong or error has propagated. ideally, we expect the sum of the z-scores to be less than a small number (say 0.00001)."
224,0,[], Standard Normalization,seg_51,example 1.12 shear strength of bonded joints
225,1,"['standard normalization', 'standard']", Standard Normalization,seg_51,"the shear strength of bonded joints (in mpa) are x=(22, 30, 81, 26, 44, 29, 61, 35). apply the standard normalization."
226,1,"['data', 'sum of squares', 'mean', 'transformation', 'variance']", Standard Normalization,seg_51,"solution 1.12 the sum of the data is 328, from which the mean is 41. sum of squares is 16,344 so that the variance is 413.7142857 (and s is 20.33996769). thus, the transformation y=(x − 41)∕20.33996769, which gives y=(−0.93412, −0.54081, 1.96657,−0.737464, 0.14749,−0.58997, 0.983286,−0.294986)."
227,0,[], Standard Normalization,seg_51,example 1.13 carbon nanoparticles
228,0,[], Standard Normalization,seg_51,"the amount of carbon particles in a nano-device is x = {32, 148, 21, 940, 36, 182, 39, 276, 14, 260, 43, 769, 25, 313, 25, 312}. compute the z-scores."
229,1,"['deviation', 'change of scale', 'data', 'mean', 'standard', 'transformation', 'standard deviation', 'variance']", Standard Normalization,seg_51,"solution 1.13 as the numbers are large, apply a change of scale transformation. divide each data by 10,000 to get s′={0.32148, 0.2194, 0.36182, 0.39276, 0.1426, 0.43760, 0.25322, 0.25312}. the mean of scaled data is 0.29775, and variance is s2 = 0.009634, from which the standard deviation is obtained as 0.098152944. the corresponding z-scores are easily found as [00.24176,−0.79824, 0.65276, 0.967979,−1.580696, 1.42482,−0.4536797,−0.454698]. the corresponding z-scores for s are also the same, which can be verified using the transformation z = (x − 29775)∕9815.294."
230,1,"['bins', 'ratio scale', 'interval', 'discretization', 'information', 'loss', 'variable', 'continuous', 'process']", DATA DISCRETIZATION,seg_53,"as the name implies, discretization (also known as binning) is the process of categorizing a continuous variable (called source variable) measured in the interval or ratio scale of noir typology into a small number of groups (called bins) with minimal loss of information."
231,1,"['interval', 'categorical', 'range', 'intervals', 'continuous']", DATA DISCRETIZATION,seg_53,"definition 1.8 ddas divide the global range of a continuous attribute into nonoverlapping and piece-wise continuous intervals in an optimal way, where each continuous interval is assigned a categorical label."
232,1,"['functions', 'continuous', 'range', 'data', 'sampling', 'variable', 'distribution', 'statistical', 'uniformly distributed']", DATA DISCRETIZATION,seg_53,univariate dda has only one source variable with well-defined logical boundaries (upper and lower). continuous periodic data are discretized using a technique called sampling using shannon’s law. we will consider only aperiodic functions in the rest of the chapter. these boundaries can also be ∓∞. nothing is assumed on the distribution of the source variable—it can be uniformly distributed over its range or can follow one of the other statistical laws.
233,1,"['bin', 'interval', 'data', 'information', 'interactions']", CATEGORIZATION OF DATA DISCRETIZATION,seg_55,"the dda can be classified into the following categories—(i) supervised, semisupervised, or unsupervised; (ii) global or local, and (iii) static or dynamic. supervised dda explores the class information (category labels) in the data intensively. entropy-based binning and purity-based binning are supervised algorithms. static dda discretizes each attribute independently without regard to attribute interactions. dynamic dda on the other hand searches for all attributes simultaneously and takes care of attribute interactions. in the following discussion, we will use a simple parenthesis ‘(’ to denote an open interval and a ‘[’ to denote a closed interval. we have a choice of either keeping the right margin open, except for the last bin, or keeping the left margin open, except for the first bin."
234,1,"['bins', 'outliers', 'method', 'interval', 'range', 'observations', 'algorithm', 'data', 'intervals', 'information', 'percentiles']", Equal Interval Binning EIB,seg_57,"this method is also called equal width binning (ewb). it is the simplest unsupervised dda, as it does not use the class label information of training data. moreover, it does not require data sorting. only inputs to this algorithm are the minimum x1 and maximum xn of n observations, and a user-supplied constant k(≥ 2) that represents the number of bins. this minimum and maximum can be found without data sorting, either using a single iteration over the data or using a recursive divide-and-conquer strategy. the range r = xn − x1 is then divided into k equal width intervals (say s = r∕k), so that xn = x1 + k ∗ s. the ith interval is then given by (x1 + (i − 1) ∗ s, x1 + i ∗ s) for i = 1, 2, · · · , k, where boundaries are properly taken care of. a disadvantage of eib is that it is sensitive to data outliers on both sides. a simple solution is to use percentiles of the data x, say p5 and p95, as the minimum and maximum and use it in the range calculation, so that s = (p95 − p5)∕k. the leftmost and rightmost intervals can finally be made unequal widths as [x1,p5 + s)"
235,1,"['intervals', 'case', 'categorical']", Equal Interval Binning EIB,seg_57,"and [p95 − s, xn]. in this case, the intervals 1 and k are unequal and all others are equal width. this does not matter as we assign categorical labels to these intervals."
236,1,"['factors', 'skewed', 'distribution', 'indicator']", Equal Interval Binning EIB,seg_57,"it is well known that excessive fat intake and a compulsion to over-eat are the major contributing factors in the pathogenesis of obesityx. sedentary life styles and fatty food eating habits make many people overfat in developed countries, resulting in a negatively skewed bmi distribution. the bmi of most adults varies between 17 and 35. this of course is country specific. an ideal bmi value is a key indicator of the overall health and fitness of an individual. too low or too high bmi values quite often indicate ill-health. very low values can be due to immunity-related illnesses and anemia. some genetic disorders and addiction to fatty food can result in very high bmi values (there are less than a dozen genetic markers that increase the bmi and contribute to obesity). those with bmi values between 30 and 35 are called obese. those above 35 bmi are called morbidly obese, for which surgical options (bariatric surgery) are available. management of obesity is important in adolescence as it could lead to heart problems in later life."
237,1,"['sample', 'rate', 'interval', 'range', 'results', 'data', 'distribution', 'normal', 'categorization', 'population', 'categories', 'error']", Equal Interval Binning EIB,seg_57,"because the normal bmi range for adults is a narrow interval, we expect a smaller fraction of the people to fall in this range than in the other ranges. an ewd will probably produce wrong results, as the “normal” range is too narrow. an efb could give better results if the population were naturally divided equally among under-fats, normals, and over-fats. in addition, as females are in general shorter than males, the bmi distribution for males and females is different. thus, the proportion of females in our sample could impact the binning boundary. note that the bmi-based categorization of an individual into the three body-fatness groups is very explicit as there are no overlaps. on the contrary, consider binning a group of people into {diabetic, nondiabetic} categories based on the bmi value. all overfat people are not diabetic, and there are few underfat diabetic patients too. thus, the classes have high overlaps. the extent of the overlap determines the error rate in binning. the entropy-based binning algorithms can give good binning in this kind of situations if the class labels of the training data are exactly known."
238,1,"['skewed', 'distribution', 'percentiles']", Equal Interval Binning EIB,seg_57,"instead of fixing the lower and upper cutoff at p5 and p95, we could arbitrarily choose two percentiles, which are often taken symmetrically (this is not necessary if the distribution is highly skewed). if pl and p100−l are the lower and upper percentiles, the above-mentioned formula becomes"
239,1,"['bin', 'correlations', 'data', 'outlier', 'interactions', 'continuous', 'test']", Equal Interval Binning EIB,seg_57,"for i = 1, 2, · · · , k − 1 and [(1 − 1∕k)p100−l + (1∕k)pl, xn]. this is recommended only when the data size is large. for smaller data sizes, an outlier test may be carried out to individually remove them one by one and then use the remaining data for binning. multiple continuous attributes are discretized one at a time (simultaneous discretization algorithms that care for attribute interactions (correlations) are also reported in the literature). this can also be done in parallel, as the majority of computation time is spent in assigning the correct bin to each data value. because nothing is assumed"
240,1,"['rate', 'method', 'estimate', 'range', 'predicted', 'data', 'distribution', 'blind binning', 'categories', 'error']", Equal Interval Binning EIB,seg_57,"about the data distribution, other than the range, this method is called blind binning. if the class labels (categories) of the data are already known, we could estimate the error rate (discussed in the following) by comparing the actual and predicted class labels. obviously, the error rate is maximum with ewb when compared to other dda. thus, the real question is whether we should compromise on the high predictive accuracy attainable by supervised dda at the expense of extra computations over the simplicity of unsupervised algorithms such as eib."
241,1,"['bins', 'data']", Equal Interval Binning EIB,seg_57,"the body mass index (bmi) of 15 patients is as follows. discretize the data using eib with (i) k = 3, (ii) k = 4, and (iii) k = 7 bins. x = {26.2, 25.6, 25.1, 23.3, 23.7, 23.4, 29.7, 28.5, 25.2, 21.4, 28.3, 33.4, 27.8, 24.4, 25.9}."
242,1,"['bins', 'range', 'case', 'data', 'frequency', 'normal']", Equal Interval Binning EIB,seg_57,"solution 1.14 here, the minimum is 21.4 and maximum is 33.4, so that the range is 12. for case (i), we need to divide the range into three equal widths, so that s = r∕k = 12∕3 = 4. the bins are b1 = [21.4, 25.4), b2 = [25.4, 29.4), and b3 = [29.4, 33.4]. if the labels are u = underweight, n = normal, and o = overweight, the new data are y = {n,n,u,u,u,u,o,n,u,u,n,o,n,u,n}. for case (ii), we get s = r∕k = 12∕4 = 3. the bins are b1 = [21.4, 24.4), b2 = [24.4, 27.4), b3 = [27.4, 30.4), and b4 = [30.4, 33.4]. let the labels be u = underweight, n = normal, o = over weight, and h = heavy. note that the “normal” category has lost its significance because the normal6 bmi is in the range (25–26). discretized data are y = {n,n,n,u,u,u,o,o,n,u,o,h,o,n,n}. for case (iii), s = r∕k = 12∕7 = 1.7143. the bins are a = [21.40, 23.114),b = [23.114, 24.829),c = [24.829, 26.543),d = [26.5435, 28.257),e = [28.257, 29.971),f = [29.971, 31.686),g = [31.686, 33.4]. the discretized data becomes {c,c,c,b,b,b,e,e, c,a,e,g,d,b,c}. note that the class f = [29.971, 31.686) has zero frequency. this is a common problem when narrow range data are discretized into a large number of bins."
243,1,"['bin', 'method', 'median', 'range', 'results', 'algorithm', 'data', 'distribution', 'cases', 'uniformly distributed']", Equal Frequency Binning EFB,seg_59,"this method divides the total range such that each subinterval has more or less the same number of data items. as this obviously requires some knowledge about the data distribution, efb is in general computationally more complex than eib. if data are known to be approximately uniform, we could first apply the eib and perturb the boundary, if necessary, to get the efb. the results obtained by eib and efb are often different, except in particular cases (when data are uniformly distributed, when (n = 2, k = 2), (n = 3, k = 2), etc.). if k is a power of 2, we could apply the median finding algorithm repeatedly using the divide-and-conquer principle to easily get the bin boundaries. otherwise, we sort the data values and pick out the bin boundaries using the following algorithm. if the number of data points is large and k is small,"
244,1,"['bins', 'range', 'results', 'case', 'algorithm', 'data', 'frequency']", Equal Frequency Binning EFB,seg_59,"the frequency of each class will be more or less equal. a problem with this binning is the duplicate values that could get split across the boundary of two adjacent classes. consider for example discretizing x = {1, 1, 2, 2, 2, 3} into k = 2 bins. as n = 6, we would split it into b1 = {1, 1, 2} and b2 = {2, 2, 3}. here 2 appears in both the bins. by our algorithm, each value in b1 is assigned one label (say x), and all values in b2 are assigned another label (say y), so that the discretized data becomes {x, x, x, y, y, y}. if the eib algorithm is used, we have r = 3 − 1 = 2 and s = r∕2 = 1 so that we either get b1 = [1, 2) and b2 = [2, 3] or b1 = [1, 2] and b2 = (2, 3]. in the first case, the discretized data becomes {x, x, y, y, y, y}, and in the second case, we get {x, x, x, x, x, y}. this example shows that eib and efb could give totally different results. eliminating all duplicates solves the problem because the data range will remain the same after a duplicates deletion (but it could result in a smaller n if there were at least one pair of duplicates). above data without duplicates is x = {1, 2, 3} with new n = 3. for k = 2 bins, we get the boundaries as b1 = [1, 1.5) and b2 = [1.5, 3]. discretized data becomes {x, x, y, y, y, y}."
245,1,['data'], Equal Frequency Binning EFB,seg_59,example 1.15 discretize bmi data using efb
246,1,"['bins', 'data']", Equal Frequency Binning EFB,seg_59,discretize the 15 bmi data in example 1.14 using efb with (i) k = 3 and (ii) k = 7 bins.
247,1,"['case', 'data']", Equal Frequency Binning EFB,seg_59,"solution 1.15 data in sorted order is x={21.4, 23.3, 23.4, 23.7, 24.4, 25.1, 25.2, 25.6, 25.9, 26.2, 27.8, 28.3, 28.5, 29.7, 33.4}. here n = 15, s = 15∕3 = 5. for case (i), we need to assign the same label to all values in (x[1 + (i − 1) ∗ s],x[1 + i ∗ s]) so that b1 = (x[1],x[5]) = [21.4, 24.4], b2 = (x[6],x[10]) = [25.1, 26.2], and b3 = (x[11],x[15]) = [27.8, 28.3, 28.5, 29.7, 33, 33.4]. the binned original data (unsorted) using labels (u, n, o) is y={n,n,n,u,u,u,o,o,n, u,o,o,o,u,n}."
248,1,"['case', 'data']", Equal Frequency Binning EFB,seg_59,"for case (ii), we have k = 7, s = [15∕7] = 2 so that the binned data is y = {oo uuuu oo uu ooo uo} or y = {o uuuuu oo uu ooo uo}."
249,1,"['bin', 'interval', 'results', 'algorithm', 'data', 'information', 'entropy', 'loss', 'test']", Equal Frequency Binning EFB,seg_59,note that the class labels of test data are not utilized in any of the above-mentioned unsupervised dda. this results in loss of classification information (when dda is used in engineering context). other unsupervised ddas include holte’s 1r algorithm [125] that constrains each bin to have at least m prespecified data instances of a majority class and kerber’s chi–merge algorithm [153]. the following algorithm adjusts the boundaries to decrease entropy at each interval.
250,1,"['level', 'discretization', 'algorithm', 'data', 'entropy', 'continuous', 'control']", EntropyBased Discretization EBD,seg_61,"these are hierarchical discretization methods that maximize shannon’s entropy in the resulting discretized space or minimize entropy to control the number of intervals induced in the continuous space. as ebd considers the class labels of the data, it is a supervised learning algorithm. the ebd induces a binary tree in the data by recursively splitting it using a fixed attribute at each level. if the data are unsorted,"
251,1,"['bin', 'bins', 'data', 'entropy', 'representative']", EntropyBased Discretization EBD,seg_61,"each value is considered one-by-one as a pivot for a possible split. let t = x[j] be the current pivot. then, we split the data into two bins as b1 = (x[i] t]∀ (i) and b2 = (x[i] ≥ t∀ (i). we assume that each of the bins contains representative data items of each of the classes. in other words, if there are k classes, we assume that at least one of the data items in each bin will belong to one of the classes. sometimes, this assumption may not hold, as our classes become more and more pure. the entropy for this split is calculated as |s1| ∗ ent(s1) + |s2| ∗ ent(s2), where |s1|"
252,1,"['bin', 'entropy', 'data']", EntropyBased Discretization EBD,seg_61,|s| |s| is the number of elements in bin b1 and |s| the total number of data items under current consideration. the entropy is calculated using all of the classes as ent(si) = −∑j
253,1,"['bins', 'information']", EntropyBased Discretization EBD,seg_61,"k =1 p(cj) ∗ log2p(cj), where k is the number of classes and p(cj) the fraction of items belonging to class cj in the respective subset si. as log (0) = −∞, irrespective of the base of the logarithm, we will drop those classes not represented in the bins (or combine the corresponding bins with its neighbors). the information gain resulting from the split at t is found as the difference between the entropies before and after split:"
254,1,['set'], EntropyBased Discretization EBD,seg_61,k =1 p′(cj) ∗ log2p′(cj) (here p′(cj) is the fraction of items belonging to class cj in the original set before split).
255,1,"['mdl principle', 'data', 'intervals', 'entropy']", EntropyBased Discretization EBD,seg_61,"those intervals with entropy 0 or with only one data value are kept. others are split recursively. the splitting is stopped using the mdl principle when infgain(s,t)     ="
256,1,"['outliers', 'interval', 'data', 'algorithm', 'intervals', 'entropy', 'correlated']", EntropyBased Discretization EBD,seg_61,"where n is the size of data and k the total number of classes. alternately, the difference between the entropy of the parent and maximum of the child node entropies is computed at each step, and iterations are terminated if this difference is small. as stated earlier, further splitting is continued only for impure intervals (if an interval is totally pure, then all values in it belong to the same class and its entropy is 0). if the class labels are highly correlated with an (increasing or decreasing) sort order of one or more attributes, we could considerably speedup the above-mentioned algorithm by taking t at the boundary of each class. as an example, consider discretizing medical patients as high-blood pressure (hbp) (c1) and low-blood pressure (c2) groups. majority of hbp patients are also overfat. thus, the bmi and hbp are highly correlated. if we sort the data in increasing order of bmi, there will be some overlap along the class boundary (on occasion there could also be some outliers in both classes)."
257,1,['data'], EntropyBased Discretization EBD,seg_61,example 1.16 discretize bmi data using ebd
258,1,['data'], EntropyBased Discretization EBD,seg_61,discretize the 15 bmi data in example 1.14 (pp. 1–14) using ebd.
259,1,"['sample', 'data', 'algorithm', 'information', 'entropy', 'set']", EntropyBased Discretization EBD,seg_61,"solution 1.16 as ebd is a supervised learning algorithm, we will label7 the data as x = {26.2(o), 25.6(n), 25.1(n), 23.3(u), 23.7(u), 23.4(u), 29.7(o), 28.5(o), 25.2(n), 21.4(u), 28.3(o), 33.4(o), 27.8(o), 24.4(u), 25.9(n)}. there are 6 o’s, 4 n’s, and 5 u’s in the original data. the entropy before split is −6∕15 ∗ log2 (6∕15) − 4∕15 ∗ log2(4∕15)−5∕15 ∗ log2(5∕15) =−0.4 ∗ (−1.3219)−0.26667 ∗ (−1.90689)−0.33333 ∗ (−1.58496) = 0.52877+0.5085 + 0.52832 = 1.565596. the set s1 contains all sample values 26.2 and s2 contains all sample values ≥26.2. for convenience, we represent only the class labels as s1 = {n,n,u,u,u,n,u,u,n} and s2 = {o,o,o,o,o,o}. here, s1 contains only four n’s and five u’s; and s2 contains only six o’s. the corresponding entropies are easily computed as ent(s1) = −(4∕9) ∗ log2(4∕9) − (5∕9) ∗ log2(5∕9) =−0.444444 ∗ (−1.169925)− 0.555556∗(−0.8479969) = 0.5199667+ 0.4711094= 0.991076, and ent(s2) = 0.0. information gain for this split is"
260,1,"['table', 'results', 'information', 'set']", EntropyBased Discretization EBD,seg_61,"0.991076 − (6∕16) ∗ 0.0 = 0.97095. iterations are continued using each of the subsequent values as split points. the results are summarized in table 1.1. the maximum information gain 0.97095 occurs for split point 26.2. hence, we will keep the set s2 intact and recursively split s1. proceeding as above, we get the entropies as 25.60 (0.31976), 25.10 (0.99108), 23.30 (0.10219), 23.70 (0.37888), 23.40 (0.22479), 25.20 (0.55773), 21.40 (0.00000), 24.40 (0.59000), and 25.90 (0.14269). the optimal split point is 25.1 with maximum value"
261,1,['intervals'], EntropyBased Discretization EBD,seg_61,"0.99108. hence, the three intervals are u = bmi   25.1,n = (25.1, 26.2), and o = bmi ≥ 26.2."
262,1,"['rate', 'estimate', 'data', 'error']", Error in Discretization,seg_63,"if the true class labels of data to be discretized are known apriori, we could estimate the error in discretizing as follows. let there be k original classes. denote original data by x and discretized data by y. for class cj, let nj be the total number of items in x. if all of them are correctly classified in y, the error rate for class j is zero. let np be the"
263,1,['case'], Error in Discretization,seg_63,"j number of correctly classified items of cj and nqj be the wrongly classified number of items, so that nj = npj + nqj . note that in the case of just two classes, all nqj items will"
264,0,[], Error in Discretization,seg_63,"belong to the other class. however, if there are  2 classes, nq will contain all items"
265,1,"['rate', 'error']", Error in Discretization,seg_63,"j belonging to cj. then, the error rate for class j is nqj∕nj. the error rate for the entire"
266,0,[], Error in Discretization,seg_63,data is obtained by summing over all of the classes as   = ∑j
267,1,"['bin', 'bins', 'model', 'disjoint', 'process', 'discretization', 'data', 'intervals', 'information', 'set', 'continuous', 'test']", Error in Discretization,seg_63,"k =1 nqj∕nj. the minimum of occurs when all items are correctly classified with minimum value 0. the maximum occurs when all items are incorrectly classified with maximum value 1. when there are no class overlaps ( is very small), the predictive accuracy is maximum. the information extracted by the discretization process can be used to classify new data instances. as the dda returns a set of disjoint, piece-wise continuous set of intervals, these intervals define the boundaries for various classes. we could divide the available data into a training set and a test set. the training set can then be used to construct the class boundaries (bins). these bin boundaries are then put to use in discretizing test data (in a classification context). in this sense, the dda is a semisupervised learning model."
268,1,"['normally distributed', 'numerical', 'linear', 'data', 'mean', 'tests', 'statistical', 'error', 'regression', 'linear regression', 'categories', 'anova', 'normality']", TESTING FOR NORMALITY,seg_65,"several statistical procedures such as anova tests and t-tests assume normality of data. similarly, the error terms are assumed to be normally distributed with zero mean in linear regression models. there are two categories of normality tests— (i) visual displays and (ii) numerical tests."
269,1,"['sample', 'curve', 'graphical', 'range', 'data', 'location', 'distribution', 'location measure', 'tails', 'normal', 'variance', 'normal distribution']", Graphical Methods for Normality Checking,seg_67,"visual displays (also known as graphical methods) use one or more graphs or diagrams to visually display the data distribution. they can be drawn as overlapping diagrams with a normal distribution for reference comparison. if the data have distinct mode, the normal curve with the same variance as the data is drawn so as to align the normal mode with the data mode. if the data mode is not unique, the normal curve uses the means for alignment. note that the theoretical normal curve extends from −∞ to ∞, whereas the sample data are always in a finite range. hence, we will look for alignment with a normal curve at the central part of the data distribution rather than at the tails (away from the location measure)."
270,1,"['plots', 'frequency', 'graphical', 'quantiles', 'data', 'symmetry', 'histograms', 'tests', 'distributions', 'curve', 'dot plots', 'isochronous', 'distribution', 'box plots', 'ogive', 'plot', 'box plot', 'normality', 'normal', 'normal distribution']", Graphical Methods for Normality Checking,seg_67,"the popular graphical methods include histograms, frequency polygons and curves, box plots, quantile-quantile (q-q) plots, moran plots, ogive curves, and dot plots. these can be categorized into two types:– (i) isochronous graphical methods produce shapes that resemble a normal curve. examples are the histograms, frequency curves. these graphs can be used as quick tests to check for normality. (ii) nonisochronous graphical methods produce particular shapes or patterns that do not have direct resemblance to a normal curve but are similar to the shapes obtained for data from a normal distribution. for instance, if points lie close to a straight line in a q–q plot, it is an indication of normality of data. this is due to the fact that quantiles of any two identical distributions when plotted along the x and y axes gives rise to a straight line plot. checking for symmetry using a box plot is easy, but checking for normality is more involved."
271,1,"['negative ogive', 'plots', 'variates', 'normally distributed', 'numerical', 'sample', 'linear', 'graphical', 'symmetric', 'data', 'standardized', 'standard', 'population', 'tests', 'distributions', 'functions', 'median', 'density functions', 'standard normal', 'distribution', 'linear combinations', 'ogive', 'plot', 'intersection', 'deviations', 'normality', 'normal', 'combinations', 'tail', 'tails']", Ogive Plots,seg_69,"ogives are graphical plots of cumulative distribution functions f(x) or survival probabilities g(x) = 1 − f(x). they are of two types, called less-than ogive (positive ogive) and more-than ogive (negative ogive). mathematically, we plot (x,f(x)) for each sample value arranged in increasing order for the less-than ogive and plot (x, 1.0 − f(x)) for the more-than ogive. the positive ogive is more popular among data analysts because several software packages support only this option. in addition, the positive ogive passes through the origin, whereas the negative ogive touches the y-axis at y = 1. if both of them are plotted in the same graph, they will intersect at the median. if the intersection point is less than the normal median and more toward the origin, it is an indication that the sample has come from a right-skewed distribution. on the other hand, an intersection point away from the normal median indicates that the sample has come from a left-skewed distribution. similar reasoning holds for leftand right-truncated distributions. the ogive curves exhibit anti-symmetry (lower left-tail and upper right-tail both tails off similarly for data from symmetric distributions). thus, the tail shapes can throw some light on whether the parent population is symmetric or not. note that a variate is symmetric around a constant if fx( − xk) + fx( + xk) = 1 ∀ k 0. in terms of density functions, this can be expressed as fx( − xk) = fx( + xk). a standard normal ogive may be superimposed on an ogive obtained from standardized empirical data to check for deviations from normality. if the original data are normal, both ogives will almost overlap. however, as linear combinations of normal variates are normally distributed, these methods cannot reveal whether the original data are linear combinations of normal laws or purely normal. nevertheless, these methods are less technical than the numerical tests."
272,1,"['plot', 'sample', 'probabilities', 'method', 'standardized', 'distribution']", PP and QQ Plots,seg_71,"a probability–probability (p–p) plot is another method to check if a sample has come from a known distribution. we plot the cdf of the standardized variate along the x-axis and the corresponding cumulative probabilities from a theoretical distribution along the y-axis. mathematically, we plot (f(xj − x)∕s,g(x)) for each sample value"
273,1,"['sample', 'cumulative distribution function', 'distribution function', 'empirical cumulative distribution function', 'distribution', 'function']", PP and QQ Plots,seg_71,"arranged in increasing order, where g(x) is the cdf of the hypothesized distribution from which the sample came. in other words, it compares the empirical cumulative distribution function of a variate (say along the x-axis) with the cdf of a theoretical distribution (say along the y-axis). thus, both of the axes are calibrated from"
274,1,"['plot', 'sample', 'kurtosis', 'quantiles', 'data', 'distribution', 'samples', 'normal', 'population', 'skewness', 'normal distribution', 'slope']", PP and QQ Plots,seg_71,"∘ 0 to 1.0, starting with the origin. a reference line with slope 45 is also drawn in the positive quadrant along the stretch of the data. if the sample is indeed drawn from the hypothesized population, the data points will clutter around the reference line. any major scattering away from the reference line indicates that the hypothesis is wrong. the q–q plot is very similar, except that we plot the quantiles of the data along the x-axis and quantiles of theoretical distribution along the y-axis. a skewness–kurtosis plot can also be used if multiple samples are available. this plot uses the x-axis for skewness and the y-axis for kurtosis or vice versa. as the normal distribution has skewness 0 and kurtosis 3, the sample values must clutter around the point with coordinates (0,3) (or (3,0) if skewness is plotted along the y-axis) if the parent population is normal. several researchers have modeled the skewness–kurtosis relationships empirically."
275,1,"['plot', 'quantiles', 'data', 'distribution', 'normality', 'normal', 'populations', 'ogive', 'distributions']", PP and QQ Plots,seg_71,"using the above-mentioned techniques, even beginning practitioners and analysts can easily be trained to check normality. all of the positive ogive plot, p–p plot and q–q plot pass through the origin. an advantage of ogive plot and q–q plot is that they can be used to check if data came from any theoretical distributions and not only for normal populations. as an example, if data are known to come from a student-t distribution, we could plot the data quantiles along the x-axis and quantiles of student’s t along the y-axis."
276,1,"['outliers', 'histogram', 'interval', 'range', 'plots', 'statistics', 'case', 'discrete', 'frequency', 'binomial distribution', 'class interval', 'data', 'success', 'parameter', 'standard', 'trial', 'error', 'distributions', 'curve', 'standard normal', 'discrete distributions', 'distribution', 'binomial', 'continuous', 'plot', 'method', 'deviations', 'normality', 'normal', 'tails', 'normal distribution', 'dispersion']", StemandLeaf Plots,seg_73,"if all data values are integers with a fixed range (say they have two or three digits), one could also use the stem-and-leaf (s l) plot to check if data are approximately normal (this method, however, cannot distinguish between continuous and discrete distributions. for example, the s l plot of data from a binomial distribution with a p close to 0.5 and large n will resemble that from a normal law). they are unsuitable for higher dimensional data. this method depends on the user’s familiarity with the normal law too. most statistics textbooks give the figure of only the standard normal law n(0, 1). as the dispersion parameter  2  0, the normal curve can take a variety of shapes. hence unless a normal distribution is superimposed on the observed data, slight deviations from normality are difficult to judge. in addition, the success also depends on the class width chosen for some of these plots. if a histogram is prepared with a small class interval, some of the classes may be empty (there may not be any data points falling in this range, so that their frequency counts are zeros). this is more likely to occur in classes toward the tails, especially when data contain outliers (exceptions do exist as in the case of u-shaped distributions). thus, a trial and error method with many class widths may be needed to reasonably conclude that the data are indeed drawn from a normal law."
277,1,"['outliers', 'sample', 'median', 'plots', 'normality']", StemandLeaf Plots,seg_73,"box plots are more appropriate to check for outliers than for normality. it uses the five-number summary of a sample, namely, the (minimum, q1, median= q2,q3,"
278,1,"['outliers', 'median', 'symmetric', 'observations', 'symmetric distributions', 'data', 'errors', 'sampling', 'mean', 'distributions']", StemandLeaf Plots,seg_73,"maximum). if the data are symmetrically distributed, the q1 and q3 are equidistant from the median (q2 − q1 = q3 − q2). this is easy to catch if the scale of the graph is large enough (so that the boxes are long). in addition, the mean and median should coincide for symmetric distributions. as the box is drawn from q1 to q3, the mean and median should bisect the box area (they must be approximately at the center of the box; considering any sampling errors). the difference q3 − q1 is called iqr. all observations that fall below−1.5*iqr of q1 and above 1.5*iqr of q3 are considered to be outliers."
279,1,"['kurtosis', 'symmetric distributions', 'gaussian distribution', 'location', 'sample', 'symmetric', 'data', 'samples', 'symmetry', 'mean', 'population', 'skewness', 'tests', 'distributions', 'sample size', 'curve', 'combination', 'median', 'inverse gaussian distribution', 'frequencies', 'distribution', 'jointly', 'asymmetric', 'test', 'inverse gaussian', 'method', 'normality', 'normal', 'dispersion']", Numerical Methods for Normality Testing,seg_75,"numerical tests are more reliable as they can catch all kinds of normality violations (normality may be violated due to dispersion, skewness or kurtosis, or a combination of these. in a normal curve, 68.26% of the frequencies lie in ∓ , 95.44% of the frequencies lie in ∓ 2 , and 99.74% of the frequencies lie in ∓ 3 (see chapter 8). location and spread measures computed from the data cannot in general reveal if the parent population is normal or not. as the mean, median, and mode coincide for symmetric distributions, these measures can quite often reveal symmetry for large samples. if the sample size is small, the above-mentioned measures may be in proximity (close-by) even for asymmetric distributions. a symmetry test and a skewness measure can jointly be used to check for normality. as discussed in the following, there are many symmetric distributions with the same skewness. hence, this method cannot always guarantee the normality of a population. similarly, linear combination of several symmetric distributions is known to be symmetric. if data are known to be asymmetric, the inverse gaussian distribution ig( , ) with pdf"
280,1,"['model', 'data']", Numerical Methods for Normality Testing,seg_75,"1   f (x; ,  ) = ( ∕2 x3) 2 exp{− 2 2x (x −  )2} is the preferred choice for data model-"
281,1,"['graphical', 'table', 'normality']", Numerical Methods for Normality Testing,seg_75,ing and fitting. table 1.2 summarizes popular normality testing using graphical and analytical methods.
282,1,"['sample size', 'sample', 'plot', 'lack of symmetry', 'data', 'samples', 'symmetry', 'asymmetric', 'transformation', 'distributions']", Numerical Methods for Normality Testing,seg_75,"a data plot can reveal any possible lack of symmetry for 2d samples. this is more difficult to visualize when dimensionality is more than three. in addition, if the variables (in 2d or more) are measured in different units, one may have to do a data transformation to concisely visualize the data. any slight departures from symmetry may not be apparent in such situations. this is more challenging when the sample size is small. suppose we have data from two asymmetric distributions. deciding whether one is more asymmetric than the other is harder when they are mirror image"
283,1,"['graphical', 'lack of symmetry', 'plots', 'skewed', 'data', 'normality', 'symmetry', 'skewness', 'tests', 'geometric', 'distributions']", Numerical Methods for Normality Testing,seg_75,"asymmetric—one is skewed to the left and the other is skewed to the right. setting aside the geometric intuition behind skewness as evinced through graphical plots and displays, a numeric score derived from the data can certainly help to understand the amount of lack of symmetry. if such a measure takes positive and negative values, we could even distinguish between left-skewed and right-skewed distributions. several skewness measures have been reported for this purpose. these are interpreted as measures of lack of symmetry because increasing values indicate how far they are away from symmetry. see refs 60, 134, and 170 for other normality tests."
284,1,"['bayesian', 'level', 'discretization', 'statistics', 'data', 'algorithm', 'statistical', 'summation', 'transformation']", SUMMARY,seg_77,"this chapter introduced different data types encountered in statistical analysis. some notations to better understand statistics in particular and mathematical sciences in general are given below. most students are familiar with the summation and product notations. however, these can sometimes be intricate and often tricky solutions exist to simplify them. some of the concepts such as sum and product notations, data discretization, and transformation may be skipped depending on the level of the course. readers who are already familiar with summation and product notations, combinations, and so on can have a bird’s eye view of the respective sections. equations that are unfamiliar or tedious can be skipped in the first reading as these are meant only to familiarize the reader with various notations. see ref. 298 for an unsupervised and ref. 174 for a bayesian data discretization algorithm."
285,1,"['interval', 'data']", SUMMARY,seg_77,a) interval data have no natural zero point
286,1,"['quartile', 'interval', 'data']", SUMMARY,seg_77,b) quartile differences are interval data
287,1,['sample'], SUMMARY,seg_77,c) the mode of a sample can coincide with the minimum of the sample
288,1,"['ordinal', 'median', 'data', 'ordinal data']", SUMMARY,seg_77,d) the median is meaningless for ordinal data
289,1,"['ordinal', 'data', 'ordinal data']", SUMMARY,seg_77,e) a scale of proportionality exist among values of numeric ordinal data
290,1,"['entropy', 'set']", SUMMARY,seg_77,f) the entropy of a set can be negative
291,1,"['variables', 'nominal']", SUMMARY,seg_77,g) all arithmetic operations are allowed on numeric nominal variables
292,1,"['data', 'discretization']", SUMMARY,seg_77,h) data discretization works only for unlabeled data.
293,1,['categorical'], SUMMARY,seg_77,1.2 what are the main branches of 1.4 distinguish between categorical
294,1,"['sample size', 'sample', 'quantitative', 'data']", SUMMARY,seg_77,statistics? how does sample size and quantitative data. what are
295,1,['statistical'], SUMMARY,seg_77,differ among these branches? some statistical procedures that
296,1,"['nominal', 'information']", SUMMARY,seg_77,use each of them? which encap1.3 give examples of nominal and sulates more information?
297,1,['data'], SUMMARY,seg_77,ordinal data. what are some
298,1,['standard'], SUMMARY,seg_77,restrictions on coding these types 1.5 distinguish between standard and
299,1,['data'], SUMMARY,seg_77,of data? extended data types. identify some
300,1,"['data', 'variable', 'set', 'standard', 'varying']", SUMMARY,seg_77,numeric measures used in each of 1.17 give an example of a summathe standard data types. tion over a set. give examples of subscript varying and superscript 1.6 what type of variable is each of varying summations.
301,1,"['dependent', 'intensity', 'scores']", SUMMARY,seg_77,"the following: (i) bmi, (ii) sys1.18 give examples of double summatolic blood pressure, (iii) earthtions where the inner indexvar is quake intensity, (iv) consumer dependent on the outer indexvar. price index, (v) gre scores."
302,0,[], SUMMARY,seg_77,1.19 what is the most appropriate 1.7 can you apply the change of ori-
303,1,"['sample', 'model', 'median']", SUMMARY,seg_77,indexvar to model thermal congin technique to find the median ductivity problems? of a sample containing very large
304,1,['sample'], SUMMARY,seg_77,numbers? mode of same sample? 1.20 in what situations can you
305,0,[], SUMMARY,seg_77,exchange the indexvars in a dou1.8 consider the alphabet of any natuble sum?
306,1,['data'], SUMMARY,seg_77,ral language. what type of data are
307,0,[], SUMMARY,seg_77,these? 1.21 give an example where the indexvar increments in fractions. 1.9 which means are easier to evalu-
308,1,"['mass function', 'probability mass function', 'data', 'probability', 'function', 'distributions']", SUMMARY,seg_77,ate for distributions that have ( n x) 1.22 give an example where the indexin the probability mass function?. var is varied from high to low values. is it possible to convert such 1.10 what data are the basic building summations in the low to high
309,1,['data'], SUMMARY,seg_77,blocks of text encoded data? indexvar values using an index
310,1,"['statistic', 'parameter']", SUMMARY,seg_77,transformation? 1.11 define parameter and statistic.
311,1,"['statistic', 'summation']", SUMMARY,seg_77,can a statistic take arbitrarily large 1.23 what type of summation will you
312,0,[], SUMMARY,seg_77,values? use in very large matrix mul-
313,1,['population'], SUMMARY,seg_77,tiplication problems where each 1.12 distinguish between population
314,1,"['sample', 'populations']", SUMMARY,seg_77,matrix is decomposed into several and sample. give examples of submatrices of appropriate order? enumerable populations.
315,1,['limit'], SUMMARY,seg_77,1.24 give examples of summations 1.13 what are some problems encounin which the upper limit for
316,1,"['poisson', 'parameter']", SUMMARY,seg_77,tered in computing the poisson the indexvar is known only at pdf for large parameter values? run-time.
317,0,[], SUMMARY,seg_77,1.14 what are some situations in which 1.25 distinguish between supervised
318,1,"['variables', 'data', 'summation']", SUMMARY,seg_77,the summation variables in a douand unsupervised data discretizable sum can be interchanged?. tion algorithms.
319,0,[], SUMMARY,seg_77,1.15 give an example situation where
320,1,['variable'], SUMMARY,seg_77,"the index variable is varied from 1.26 if xmxm is a square matrix, use the ∑ notation to find the sum of high to low values. each of the following:–(i) diagonal"
321,1,['nested sum'], SUMMARY,seg_77,"1.16 what is a nested sum? how are elements, (ii) tridiagonal elements"
322,0,[], SUMMARY,seg_77,"they evaluated? what are the pos(main diagonal plus adjacent diagsible simplifications in a nested onals), and (iii) lower triangular sum evaluation? elements (including the diagonal)."
323,0,[], SUMMARY,seg_77,1.27 the number of hours that a battery where x[n] is the signal value
324,1,['data'], SUMMARY,seg_77,can be continuously operated in recorded at time t = n. rewrite the different devices after a 30-minute expression where n varies from 0 recharge is given below. transto 2n. what is the power when form the data into the interthe signals are either compressed
325,1,['data'], SUMMARY,seg_77,n ≠i=1 (xi − xj)2. discontinuous data (with gaps in
326,1,['algorithm'], SUMMARY,seg_77,n between)? which algorithm is 1.29 express (∑j=1 xj)2 in terms of
327,1,['cases'], SUMMARY,seg_77,"n ≠k; j,k=1 xj ∗ xk. use best in such cases?"
328,1,['data'], SUMMARY,seg_77,"n =1 xj)2) as a 1.37 discretize the data x = {56, product. 62, 68, 73, 75, 78, 81, 88, 90, 93} using epb if the labels are 1.30 if the indexvar increments in steps"
329,1,"['data', 'discretization']", SUMMARY,seg_77,1.38 what is data discretization? what 1.31 if the indexvar increments in steps
330,0,[], SUMMARY,seg_77,"k are some of its applications in of c, evaluate the sum ∑j=−k f (j) engineering? describe how you"
331,1,['range'], SUMMARY,seg_77,using unit incrementing indexvar. will discretize if the range (spread)
332,0,[], SUMMARY,seg_77,1.32 if gini diversity index is defined as of values is too large.
333,1,['transform'], SUMMARY,seg_77,"n ≠i=1 (xi −xj)2, 1.39 transform the above-mentioned"
334,1,['data'], SUMMARY,seg_77,"prove that d2 ≤ 2sn2. data to the [−1, +1] and [−3, +3]"
335,1,['data'], SUMMARY,seg_77,1.33 what is a data requirement for ranges using min–max transfor-
336,1,['results'], SUMMARY,seg_77,using the entropy-based dismation (pp. 1–48) and compare cretization? in what situations is the results using the z-score transit best? formation.
337,1,['data'], SUMMARY,seg_77,1.34 the following data gives the 1.40 can the minimum data value in
338,0,[], SUMMARY,seg_77,marks scored by 10 students in eib (pp. 1–45) be negative? can
339,1,"['range', 'statistics', 'data', 'intervals', 'transform']", SUMMARY,seg_77,"engineering statistics. discretize both the minimum and maximum the data using eib and efb. x = be negative? {56,62,68,73,75,78,81,88,90,93}. 1.41 how will the efb (pp. 1–48) transform the data to [−3,+3] divide n data items into k intervals range using min–max transforif n is not a multiple of k? mation. obtain the z-scores and"
340,0,[], SUMMARY,seg_77,compare with min–max trans1.42 the first-order bragg reflection of
341,1,['data'], SUMMARY,seg_77,formed data. x-ray at different angles through
342,1,['discrete'], SUMMARY,seg_77,1.35 the power of a discrete signal a crystal gave the nanometer
343,1,['data'], SUMMARY,seg_77,"transform the data into the inter[−1,+1] and [−3,+3] ranges. x = vals [−0.5,+0.5] and [−1,+1]. {32, 19, 24, 31, 20, 27}."
344,0,[], SUMMARY,seg_77,1.43 the resistance of an electronic 1.46 a plastic polymer thread is sub-
345,1,"['percentage', 'range', 'data', 'test']", SUMMARY,seg_77,"circuit was measured using five jected to an elongation stress test to see how much it can be different components as {5.2ω, stretched before it breaks. elonga4.9ω, 5.12ω, 4.95ω, 5.1ω}. transtion at break point is expressed as a form the data to [−1,+1] range. percentage of its original length as convert data into z-scores. x = {9.2%, 6.7%, 15.3%, 18.0%,"
346,1,"['range', 'transform', 'data']", SUMMARY,seg_77,"some thermoplastics is given 12.0%}. transform the data to the below. discretize the data into [−3,+3] range."
347,1,"['transform', 'data', 'intervals']", SUMMARY,seg_77,three intervals using efb and 1.47 soluble dissolvents (in mg/l) in eib. transform the data to the drinking water are measured at
348,1,['standard'], SUMMARY,seg_77,"{15.6,19.5,17.2, 18.1, 17.6, 15.3, {560, 458, 490, 525, 482, 554, 499, 18.0, 16.8, 16.4, 19.0}. 538, 540, 507, 481, 513}. standard-"
349,1,['data'], SUMMARY,seg_77,1.45 the number of hours that a battery ize the data. will you prefer the
350,1,"['interval', 'range', 'change of origin', 'change of scale', 'data', 'transformations', 'transform']", SUMMARY,seg_77,"can be continuously operated in change of origin, change of scale, different devices after a 30-minute or both transformations? transrecharge is given below. transform the data to the [−1,+1] form the data into the interval range.?"
351,0,[], SUMMARY,seg_77,"1.48 if the index varies in powers of b (in steps of bj), prove that ∑j"
352,1,['estimation'], SUMMARY,seg_77,1.50 consider an expression for echo delay estimation in audio echo cancellation
353,0,[], SUMMARY,seg_77,k =−lag x[j] ∗ x[j + lag]. use loop rerolling technique to express it in terms of an indexvar that is always positive.
354,1,['summation'], SUMMARY,seg_77,1.51 rewrite the summation ∑k
355,1,['independent'], SUMMARY,seg_77,m =1 c ∗ ui ∗  j+k ∗  k by taking terms independent of indexvars outside the summations.
356,0,[], MEASURES OF LOCATION,seg_79,"after finishing the chapter, students will be able to"
357,1,"['parameters', 'population', 'location']", MEASURES OF LOCATION,seg_79,◾ distinguish between location and scale population parameters
358,1,['location'], MEASURES OF LOCATION,seg_79,◾ describe important measures of location (central tendency)
359,1,"['weighted mean', 'mean', 'trimmed mean']", MEASURES OF LOCATION,seg_79,◾ understand trimmed mean and weighted mean
360,1,"['quartiles', 'percentiles']", MEASURES OF LOCATION,seg_79,"◾ comprehend quartiles, deciles, and percentiles"
361,1,"['data', 'transformations']", MEASURES OF LOCATION,seg_79,◾ use data transformations to compute various measures
362,1,"['harmonic', 'geometric']", MEASURES OF LOCATION,seg_79,"◾ apply updating formula for arithmetic, geometric, and harmonic means"
363,0,[], MEASURES OF LOCATION,seg_79,◾ prudently choose the correct measure for each situation
364,1,"['statistics', 'data', 'location']", MEANING OF LOCATION MEASURE,seg_81,"the literal meaning of “location” is a place or point of interest with respect to (wrt) a frame of reference. in statistics, a location indicates a single point (for univariate data) that best describes the data at hand."
365,1,"['sample', 'data', 'location', 'information', 'function']", MEANING OF LOCATION MEASURE,seg_81,definition 2.1 a well-defined function of the sample values that purports to summarize the locational information of data into a concise number is called a measure of location or central tendency.
366,1,"['sample', 'parameters', 'locations', 'location', 'population']", MEANING OF LOCATION MEASURE,seg_81,"the concept of location is applicable to a sample as well as to a population. population locations are indicated by parameters (described below). for example,"
367,1,"['sample', 'functions', 'range', 'data', 'location', 'distribution', 'discrete', 'locations', 'parameter', 'discrete distribution', 'location parameter']", MEANING OF LOCATION MEASURE,seg_81,"a parameter   is called a location parameter if the functional form of the pdf is f (x ∓  ). here,   is a nonzero real number. sample locations are measured by functions of sample values that return a real number within the range of the sample. it need not coincide with the sample data (i.e., x-value) for a sample drawn from a discrete distribution. these are also called measures of central tendency."
368,1,"['observations', 'geometric mean', 'change of origin', 'location', 'frequency', 'transformation', 'harmonic', 'medoids', 'geometric', 'sample', 'sample median', 'quartiles', 'data', 'percentiles', 'samples', 'mean', 'functions', 'median', 'arithmetic mean', 'grouped data', 'change of scale', 'location measures', 'harmonic mean', 'medoid']", Categorization of Location Measures,seg_83,"many meaningful functions of sample values can be used as sample location measures. such a measure is expected to locate the central part of the data. naturally, a measure that uses each and every sample value is more meaningful in engineering applications. the arithmetic mean (simply called mean), geometric mean, and harmonic mean (hm) belong to this category. trimmed versions of them remove a small amount of extreme observations, and compute the value for the rest of the data. weighted version of them give different importance to different data. the mean need not always coincide with one of the data values. a medoid is that data value that is closest to the mean in a distance sense. medoids for large samples need not be unique (as there could exist multiple data points at equal distance from the mean). as it depends on the mean, it also belongs to the above category. yet other types of measures that use the frequency of data rather than data values are available. one example is the mode that locates the data value with maximum frequency. this is more meaningful for grouped data. the sample median uses the count of data values to divide the total frequency into two equal parts. an extension of this concept uses quartiles, deciles, and percentiles that are useful when the data size is large. among these measures, a change of origin transformation is meaningful to the arithmetic mean only, and a change of scale transformation is applicable to all the three means. these are discussed in subsequent sections."
369,1,"['symmetric distributions', 'binomial distribution', 'symmetric', 'exponential', 'parameter', 'standard', 'statistical', 'distributions', 'gamma', 'parameters', 'distribution', 'asymmetric', 'pareto', 'binomial', 'continuous', 'beta distribution', 'f distribution', 'cauchy', 'gamma distributions', 'normal', 'normal distribution', 'pareto distribution']", MEASURES OF CENTRAL TENDENCY,seg_85,"statistical distributions come in various shapes. some of them are always symmetric around a real number for univariate distributions (or a vector for multivariate distributions), which can be zero or nonzero. examples include the standard normal, standard cauchy, and student’s t distributions (symmetric about 0), general normal distribution n( , 2), which is symmetric about , and general cauchy distributions. examples of asymmetric distributions include the exponential, beta and gamma distributions, f distribution, pareto distribution, and so on. some of these distributions are symmetric for particular parameter values though. for instance, the 2-parameter beta distribution beta-i (a, b) is symmetric when the parameters are equal (a= b), and the binomial distribution bino (n, p) is symmetric when p = 1∕2,∀ n. as mentioned below, a great majority of statistical distributions are asymmetric. most of the symmetric distributions are of continuous type."
370,1,"['symmetric', 'location', 'symmetry', 'asymmetric', 'distributions']", MEASURES OF CENTRAL TENDENCY,seg_85,"the “central tendency” measures the location of symmetry of symmetric distributions, and the center of gravity of asymmetric distributions. we call it a location"
371,1,"['geometric mean', 'change of origin', 'case', 'location', 'transformation', 'geometric', 'sample', 'linear', 'data', 'mean', 'distributions', 'median', 'arithmetic mean', 'nonlinear', 'distribution', 'location measures']", MEASURES OF CENTRAL TENDENCY,seg_85,"measure because they can locate the approximate centering of the distribution along the real line (univariate case). the most commonly used measures of location are the arithmetic mean, median, and the mode. among them, the arithmetic mean is a linear measure as it uses the sum of the data values in the numerator. geometric and hms are nonlinear measures (geometric mean is log-linear as shown below). a change of origin transformation (e.g., using the mean as the pivot) can be used to align the location measures of different distributions. arithmetic, geometric, and hms and the median always lie between the minimum and maximum of the sample values (for n ≥ 2), while the mode may get aligned with the extremes."
372,1,"['sample size', 'sample', 'sample mean', 'mean', 'population']", ARITHMETIC MEAN,seg_87,"the mean of a population is denoted by the greek letter  , and the corresponding sample mean is denoted by x (or xn where n is the sample size). we define it as"
373,1,['discrete'], ARITHMETIC MEAN,seg_87,∞ =−∞ xk pk if x is discrete;   = { ∫x
374,1,['continuous'], ARITHMETIC MEAN,seg_87,∞ =−∞ x f (x)dx if x is continuous.
375,1,"['probabilities', 'weighted average', 'moment', 'range', 'random variable', 'variable', 'summation', 'mean', 'random', 'average']", ARITHMETIC MEAN,seg_87,the summation or integration needs to be carried out only throughout the range of the respective random variable (as the pdf is defined to be zero outside the range). this represents the weighted average of all possible values of a random variable with the corresponding probabilities as weights. the mean is the first moment because it is obtained by putting j = 1 in
376,1,['discrete'], ARITHMETIC MEAN,seg_87,j pk if x is discrete;  j = { ∫−
377,1,['continuous'], ARITHMETIC MEAN,seg_87,∞ ∞ xjf (x)dx if x is continuous.
378,1,"['sample size', 'sample', 'mean']", ARITHMETIC MEAN,seg_87,the simple (arithmetic1) mean of a sample of size n is defined as the sum of the sample values divided by the sample size. symbolically
379,1,"['sample size', 'sample', 'median', 'results', 'data', 'mean']", ARITHMETIC MEAN,seg_87,"where the subscript n on the left hand side (lhs) denotes the sample size, and on xn denotes the nth data value. we write it as x when no ambiguity is present. duplicate values, if any, are counted distinctly in finding the mean. it is evident from equation (2.1) that the mean of a sample need not coincide with one of the sample values for n 1 (median for odd sample size, and mode will always coincide with a sample value). distributing the constant with each of the sample values results in"
380,1,"['sample', 'sample mean', 'data', 'mean']", ARITHMETIC MEAN,seg_87,"this shows that the sample mean gives equal weights or importance to each sample data item. if a sample contains several zeros, all of them are counted in the above definition. subtract xn from both sides of equation (2.1), and write xn on the right-hand side (rhs) as n terms each of which is xn∕n to get"
381,1,"['factor', 'summation']", ARITHMETIC MEAN,seg_87,"take (1∕n) as common factor from rhs, and write the rest of the terms using the summation notation. this gives"
382,1,"['sample', 'deviations', 'mean']", ARITHMETIC MEAN,seg_87,"as (1/n) is a constant, this means that the sum of the deviations of sample values from its mean is always zero. this can also be stated as follows:"
383,1,['sample'], ARITHMETIC MEAN,seg_87,"n =1(xi − c) = 0 for a sample, then c = xn."
384,1,['summation'], ARITHMETIC MEAN,seg_87,proof: apply the summation to each individual term in the bracket to get ∑i
385,1,"['sample size', 'sample']", ARITHMETIC MEAN,seg_87,"n =1 c = n ∗ c. substitute in the above to get n ∗ xn − n ∗ c = 0, or equivalently n ∗ (xn − c) = 0. as n being the sample size is nonzero, the only possibility is that c = xn. this result will be used in subsequent chapters."
386,1,"['statistics', 'observation', 'location', 'sample', 'sample mean', 'data', 'mean', 'population', 'inferential statistics', 'location measure', 'median', 'arithmetic mean', 'variables', 'population mean', 'nominal', 'variable']", ARITHMETIC MEAN,seg_87,"the sample mean is the most extensively used location measure due to its desirable properties in inferential statistics. as the mean utilizes each and every observation in a sample, it rapidly converges to the population mean as n → ∞. the arithmetic mean is not an appropriate measure of central tendency when nominal variables are coded numerically. however, the mean is meaningful in one situation—when a dichotomous nominal (i.e., binary) variable is coded as 0 and 1, the mean gives the proportion of items that are coded as 1. as we cannot compare nominal data, the median p. 54 also is meaningless. the mode p. 58 is the most appropriate measure of central tendency for nominal data."
387,1,"['sample size', 'sample', 'design', 'data', 'mean']", Updating Formula For Sample Mean,seg_89,"as mentioned above, the mean of a sample can be found if the sum of the observations and the sample size are known. all of the sample values may not be readily available in some scientific and industrial applications. as an example, suppose the data come from sensors installed in a large factory. several industries and factories have a multitude of sensors such as temperature (heat), light, pressure, humidity (moisture), gas, and chemical sensors installed at various strategic points. in addition, some specialized industries such as chip design factories, dvd, and floppy disk manufacturing plants measure dust and microparticle suspension in the air to ensure that they do not get deposited into sensitive chip components, circuits, or platters. smoke and radiation sensors are more important in space stations. similarly, some"
388,1,"['combinations', 'variations', 'vary', 'concentration']", Updating Formula For Sample Mean,seg_89,"pharmaceutical companies have microbe sensors on the machine parts that manufacture some medicines. a high concentration of microbes in the ingredients could be lethal to patients if it contaminates just a few of the tablets or capsules manufactured2. each of the sensors can have variations in terms of calibration. for example, there are separate heat sensors for air, water, liquids (different liquids boil at different temperatures; it slightly differs for the same liquid in the presence of various solvents, or combinations of them; the boiling point also depends on the altitude), chemicals, and surface temperatures. this will vary from factory to factory. while air temperature, smoke, and humidity sensors are more important in textile factories, pressure and temperature sensors are more important in robotic factories. as another example, hydroponics farms are closed (air-tight) laboratories in which plants are grown in sand or water tubes or containers. the light, nutrient concentrations, and temperature sensors are the most important, followed by water and microbe concentrations in hydroponics farms. these can be continuously monitored using various sensors."
389,1,"['observation', 'process', 'charts', 'sample', 'sample mean', 'data', 'cases', 'mean', 'statistical', 'quality control', 'control', 'deviations']", Updating Formula For Sample Mean,seg_89,"in all of the above cases, we wish to continuously check process deviations using quality control charts or statistical models that heavily depend on the sample mean. in such situations, we could find the mean of already available data, and iteratively update the mean when new data items are received from various sensors. this is called online updating. suppose we have a sample of size n with mean xn. if an additional observation xn+1 is added to our sample, the new mean becomes"
390,0,['n'], Updating Formula For Sample Mean,seg_89,"multiply numerator and denominator by n, and separate out the last term xn+1 to get"
391,1,['factor'], Updating Formula For Sample Mean,seg_89,"take 1/(n+1) as a common factor, and write this as"
392,1,"['arithmetic mean', 'case', 'algorithm', 'data', 'mean', 'recursive algorithm']", Updating Formula For Sample Mean,seg_89,"each newly received data item is used only once in the updating formula. note that the correction term (xn+1 − xn)∕(n + 1) can be positive or negative depending on whether the new data item xn+1 is   or   xn. in the particular case when xn+1 = xn, the mean is unchanged. this provides a recursive algorithm for arithmetic mean [2]."
393,1,['mean'], Updating Formula For Sample Mean,seg_89,example 2.1 find mean by updating formula
394,1,"['sample', 'locations', 'mean']", Updating Formula For Sample Mean,seg_89,"thickness of paint layer applied on straight locations using nylon brush is dependent on the paint viscosity and smoothness of the surface. paint-layer tends to be thicker on harsh surfaces than smooth ones. a sample surface of size 1"" × 1"" is test-painted, and the layer thickness (in mm) after drying is noted down at 10 random spots. use the updating formula (2.9) to compute the mean paint thickness. x = {0.26, 0.51, 0.39, 0.27, 0.44, 0.58, 0.34, 0.29, 0.4, 0.53}"
395,1,"['mean', 'data']", Updating Formula For Sample Mean,seg_89,"solution 2.1 form a sequence of pairs (xi, xi) where xi is the mean of all data until the current one. we get (0.26, 0.26), (0.51, 0.385), (0.39, 0.38667), (0.27, 0.3575), (0.44, 0.374), (0.58, 0.4083), (0.34, 0.39857), (0.29, 0.385), (0.4, 0.386667), (0.53, 0.401) as the values. the second value in the last pair is the mean xn."
396,1,"['sample', 'plots', 'samples', 'mean', 'average']", Updating Formula For Sample Mean,seg_89,"in some applications, we have the mean of subsamples already available. as examples, the mean marks of two or more classes in the same college, the mean yield of two or more plots in an agricultural experimentation, the mean purchase amount of day-time and night-time customers to an online store, and the average sales amount in two consecutive time periods (days, months, years, etc.) all record multiple means for different samples. these separately computed means could be combined, irrespective of their individual sample sizes, using the following theorem."
397,1,"['sample', 'combined sample', 'samples', 'mean']", Updating Formula For Sample Mean,seg_89,"theorem 2.1 if x1 and x2 are the means of two samples of sizes n1 and n2, respectively, the mean of the combined sample is given by x = (n1x1 + n2x2)∕(n1 + n2)."
398,1,"['sample', 'observations', 'samples', 'grand mean', 'mean']", Updating Formula For Sample Mean,seg_89,"proof: the n1x1 and n2x2 in the rhs represent the sum of the observations of the first and second sample, respectively, so that their sum is the grand total of all observations. by dividing this total by (n1 + n2) gives the grand mean on the lhs. this result can be extended to any number of samples as follows:"
399,1,"['sample', 'combined sample', 'samples', 'mean']", Updating Formula For Sample Mean,seg_89,"corollary 1 if xi, i = 1, 2, ..m are the means of m samples of sizes n1, n2, … , nm, respectively, the mean of the combined sample is given by x = (n1x1 + n2x2 + · · · + nmxm)∕(n1 + n2 + · · · + nm)."
400,1,"['sample', 'unbiased estimator', 'sample mean', 'random samples', 'population mean', 'samples', 'mean', 'probability', 'random', 'population', 'estimator', 'unbiased']", Updating Formula For Sample Mean,seg_89,"as e(xn) =  , the sample mean is used as an unbiased estimator of the unknown population mean  . this has two interpretations. if repeated random samples of small size n are drawn from a population, the mean of these samples will clutter around the population mean  . on the other hand, the mean of a sample of size n converges in probability to the population mean as n → ∞. equivalence of both these statements can be understood from the above lemma, where n = (n1 + n2 + · · · + nm) → ∞with each of the n′"
401,0,[], Updating Formula For Sample Mean,seg_89,"is being equal, and m is large."
402,1,['mean'], Updating Formula For Sample Mean,seg_89,example 2.2 combined mean
403,1,['mean'], Updating Formula For Sample Mean,seg_89,"two trucks work continuously to transport passenger luggage from an airport to a terminal. if the mean weight (in tonnes) transported in 10 trips by truck-1 is 58, and 12 trips of truck-2 is 46, what is the mean weight transported by these two trucks combined?"
404,1,"['sample', 'mean', 'observation']", Updating Formula For Sample Mean,seg_89,"corollary 2 if an existing observation xn is removed from a sample of size n with mean xn, the new mean is given by (nxn − xn)∕(n − 1)."
405,1,"['sample', 'observations', 'mean']", Updating Formula For Sample Mean,seg_89,"corollary 3 if m observations with mean xm are removed from a sample of size n with mean x, the new mean is given by xnew = (nx − mxm)∕(n − m)."
406,1,"['sample', 'transformed', 'variables', 'change of origin', 'case', 'mean']", Sample Mean Using Change of Origin and Scale,seg_91,"the change of origin technique is useful to compute the mean when the sample values are large. if the variables are transformed as yi = xi − c, the means are related as yn = xn − c. in this case, the updating formula becomes yn = [(n − 1) ∗ yn−1 + xn − c]∕n. this can also be written as"
407,1,"['change of origin', 'change of scale', 'data', 'transformation', 'change of origin and scale']", Sample Mean Using Change of Origin and Scale,seg_91,the change of scale transformation y = c ∗ x gives yn = c ∗ xn. we could simultaneously apply the change of origin and scale transformation to the data as zi = (xi − c)∕d. the means are then related as zn = (xn − c)∕d. the updating formula then becomes
408,1,"['range', 'data', 'sphere', 'mean', 'vary', 'variance']", Sample Mean Using Change of Origin and Scale,seg_91,"the above equation is quite useful in iteratively computing the mean when the data values are large and have large variance. as an example, microparticle sensors have limited range (or visibility) to maintain correct accuracy. if the range is 1cm3 (theoretically, it is a sphere of appropriate radius (if they are setup above ground) or a semi-sphere (if they are mounted on walls or flat surfaces) such that there are no empty regions between adjacent sensors) around its sensing point, the number of microparticles in it could be very large, which could vary depending on the air current. similarly, smoke sensors installed in rooms or buildings near the road or highway sides, or inside vehicles on the road have a cutoff threshold for the number of carbon particles. if this number is beyond the threshold, it is flagged as smoke from fire. if it is below the threshold, it is assumed as engine exhausts or cigarette smoke, and so on. the numbers used in all these situations are large in magnitude and have large variance. however, we need to only accumulate the values for a suitable time window. a smoke detector is least concerned with the number of carbon particles it encountered 2 min ago. its window is very small, perhaps 1–3 s. the window size of microparticle"
409,1,['vary'], Sample Mean Using Change of Origin and Scale,seg_91,"sensors could vary depending on the air-current—if air circulates fast, the window is a few milliseconds, and if it circulates slowly, it could be 1 or 2 s. this could also vary among sensors installed in other media such as water, liquids, or chemicals. if the window size is d, the general updating formula given above becomes"
410,1,"['mean', 'data']", Sample Mean Using Change of Origin and Scale,seg_91,this is called the ‘window mean’ as it simply accumulates the mean of the most recently seen d data values.
411,1,['mean'], Sample Mean Using Change of Origin and Scale,seg_91,example 2.3 mean updating
412,1,['mean'], Sample Mean Using Change of Origin and Scale,seg_91,"the mean of the number of particles received in a sensor for 6 s is 1600. if two new counts (970 and 1830) are recorded in subsequent seconds, find the new mean using updating formula (2.9) in page 47."
413,1,"['mean', 'intervals']", Sample Mean Using Change of Origin and Scale,seg_91,"solution 2.3 our updating formula is xn+1 = xn + (xn+1 − xn)∕(n + 1). we are given that n = 6 (as the particles are counted in intervals of 1 s), xn = 1600. for xn+1 = 970, the correction term is  n+1 = (xn+1 − xn)∕(n + 1) = (970 − 1600)∕7 = −90. substitute xn+1 = xn +  n+1 to get the new mean as 1600 − 90 = 1510. the new correction term is  n+2 = (xn+2 − xn+1)∕(n + 2) = (1830 − 1510)∕8 = 40. substitute in xn+2 = xn+1 +  n+2 to get the new mean as 1510 + 40 = 1550."
414,1,"['outliers', 'sample', 'trimmed means', 'arithmetic mean', 'observations', 'data', 'summation', 'mean']", Trimmed Mean,seg_93,"data outliers have a major influence on the arithmetic mean, as they are given equal importance as other data values. a solution is to delete extreme observations from the low and high end of a sample (of sufficiently large size) and compute the mean of the rest of the data. these are called trimmed means. they can be left-trimmed (only low end data are discarded), right-trimmed (only high end data are discarded), or simply trimmed (from both the ends). it is symmetrically trimmed if an equal number of observations are discarded from both the ends. using the summation notation introduced in chapter 1, this becomes"
415,1,"['sample', 'data', 'statistic', 'mean', 'trimmed mean', 'order statistic']", Trimmed Mean,seg_93,"t where xm denotes that this is the trimmed mean of m = n − 2k data values, and x(i) is the ith order statistic. this definition uses a count (k) to truncate data values from both the ends. a cutoff threshold can also be used to discard data values from either or both the ends of a rearranged sample. in fact, an entire sample need not be sorted (arranged in increasing or decreasing order) to find the trimmed mean."
416,1,"['sample', 'observation', 'weighted mean', 'mean']", Weighted Mean,seg_95,each observation (sample value) is weighted by 1∕n in the simple mean (see equation (2.2)). the weighted mean is an extension in which we multiply (or divide)
417,1,"['weighted mean', 'mean', 'observation', 'associated']", Weighted Mean,seg_95,"each observation by an appropriate nonzero weight. if  1,  2, … ,  n are the weights associated with x1, x2, … , xn, respectively, the weighted mean is given by"
418,1,"['sample', 'parameters', 'experiment', 'arithmetic mean', 'observations', 'data', 'weighted mean', 'mean', 'outcome', 'correlated']", Weighted Mean,seg_95,"weighted mean assigns different importance to different sample observations. for example, if the data were collected over a time window (as in supermarket sales), more recent transactions must be highly weighted than distant ones to the past. similarly in some medical studies in which the age of a patient is correlated with the outcome of an experiment, patients in various age groups may be weighted differently. we denote the weighted mean by xn(w) (or xn(wn)) to distinguish it from simple mean, and to indicate that the weights are the parameters. different weightings may be used on the same sample. when all the weights are equal, the weighted mean reduces to the arithmetic mean."
419,1,"['grouped data', 'frequencies', 'mean', 'data']", Mean of Grouped Data,seg_97,the mean of grouped data is obtained from the above by replacing i’s with corresponding class frequencies fi’s as
420,1,"['moment', 'median', 'frequencies', 'results', 'data', 'distribution', 'cases', 'frequency', 'moments', 'mean']", Mean of Grouped Data,seg_97,"here, fi are the frequencies and xi is the middle point of the respective class. it is assumed that there are no open classes (such as x 5 or x 100) at the extremes. in such cases, the median is more appropriate. each of the class widths are assumed to be equal in equation (2.14). a shepperd’s correction may be applied to get more accurate results. this is desirable because the middle value of a class is used to compute the mean (and higher order moments) under the assumption that the entire frequency falling in a class is concentrated at or around the middle value. this warrants a correction to compensate for the distribution of data throughout the class. there is no correction for the first moment 1. for 2, the correction term is h2∕2 so that the corrected term is 2 − h2∕2 where h is the class width. if there are a large number of classes and some of the adjacent classes have relatively very low frequencies, they may be combined to reduce the computation."
421,1,"['weighted mean', 'mean']", Updating Formula for Weighted Sample Mean,seg_99,an updating formula could also be developed for the weighted mean as follows. start with equation (2.14) for n + 1 as
422,0,[], Updating Formula for Weighted Sample Mean,seg_99,multiply and divide the first term on the rhs by ∑i
423,0,[], Updating Formula for Weighted Sample Mean,seg_99,"factor from first two terms, and cancel out ∑i"
424,0,[], Updating Formula for Weighted Sample Mean,seg_99,1  i from numerator and denominator
425,0,[], Updating Formula for Weighted Sample Mean,seg_99,of the first term to get
426,1,"['sample', 'data', 'weighted mean', 'mean']", Updating Formula for Weighted Sample Mean,seg_99,"in terms of the mean of the weights, this becomes xn+1(wn+1) = xn(wn) +  n+1 (xn+1 − xn(wn)). when xn+1 = xn(wn), the weighted mean will remain (n+1) n+1 the same, irrespective of the weight assigned to the new sample data item."
427,0,[], Updating Formula for Weighted Sample Mean,seg_99,example 2.4 calories burned while exercising
428,1,"['weighted mean', 'mean', 'table']", Updating Formula for Weighted Sample Mean,seg_99,"calories burned on a treadmill by a person depends on many things including speed of the belt, age, and physical stature. table 2.1 gives the calories burned and speed on treadmill of 16 visitors to a health club. find the weighted mean using equation (2.17)."
429,1,"['weighted mean', 'mean', 'table']", Updating Formula for Weighted Sample Mean,seg_99,solution 2.4 calculations are shown in table 2.1. weighted mean is computed directly to check the computations. the last entry in the last column gives the weighted mean as 8.0282.
430,1,"['sample', 'ordinal', 'sample mean', 'relative frequency', 'statistics', 'case', 'data', 'nominal', 'statistical', 'hypotheses', 'frequency', 'mean', 'representative']", Advantages of Mean,seg_101,"the am can be computed even if data contain many zeros. in addition, it possesses some desirable statistical properties in other fields of statistics such as testing of hypotheses and inferences. it is meaningful for ordinal or higher scales of measurements that are numerically coded. there is one particular case of nominal data for which the mean is meaningful. if the nominal data are coded as either 0 or 1, the mean will give the relative frequency of sample values that are coded as 1. as a simple example, suppose the sex of patients to a clinic are coded as 0= female, 1=male. if 120 patients visit the clinic on a particular day, we could find the mean of these values to find out what proportion of them were males. this is due to the fact that we have coded males as “1.” what if we want to find out the proportion of females only? one solution is to subtract the males’ proportion from 1 to get the female proportion (as the proportions for males and females add up to 1). the mean also has an interpretation as the balancing point (center of gravity) of a simple or weighted sample (see below). this implies that if one were to use a single number between the minimum and maximum of the sample values as a representative of the sample, the sample mean seems to be the most appropriate value to use."
431,1,['mean'], Advantages of Mean,seg_101,some of the advantages of mean are summarized below:
432,1,['mean'], Advantages of Mean,seg_101,1. the mean is easy to compute.
433,1,['treatment'], Advantages of Mean,seg_101,2. it lends itself to further arithmetic treatment.
434,1,['sample'], Advantages of Mean,seg_101,3. it is always unique (whereas mode of a sample need not be unique).
435,1,['data'], Advantages of Mean,seg_101,4. it can easily be updated (when data are added or deleted).
436,1,"['sample', 'missing values', 'linear', 'observations', 'missing value', 'mean', 'grand mean', 'function']", Advantages of Mean,seg_101,"as the mean is a linear function of the sample values, we could deal with missing values as follows: (i) find the grand mean xg by omitting all missing observations; (ii) replace each missing value by xg and find the new mean x."
437,1,"['observations', 'deviations', 'mean']", Properties of The Mean,seg_103,"the mean satisfies many interesting properties. for example, the mean places itself in-between the extremes of observations in such a way that the sum of the deviations of observations (from it) to its left and to its right are equally balanced in terms of their magnitudes. this is proved in the following theorem."
438,1,"['sample', 'observations', 'deviations']", Properties of The Mean,seg_103,"theorem 2.2 for any sample of size n   1, the sum of the deviations of observations"
439,1,['mean'], Properties of The Mean,seg_103,from the mean ∑j
440,1,"['weighted mean', 'mean']", Properties of The Mean,seg_103,proof: this is already proved in equation (2.4) (p. 46). this can be extended to the weighted mean as follows:
441,1,"['levels', 'table', 'data', 'mean']", Properties of The Mean,seg_103,hexavalent chromium is a toxic chemical found in the metropolitan areas. data in table 2.2 gives the levels in nanogram per cubic meter for 10 different places. compute the mean and verify whether ∑i
442,1,"['table', 'data', 'deviations', 'mean']", Properties of The Mean,seg_103,"solution 2.5 the sum of the numbers is 7.66, from which the mean is found as 0.766. the second row of table 2.2 gives the deviations of data from the mean. the last column is the sum of the deviations, which is obviously zero."
443,1,['data'], Properties of The Mean,seg_103,example 2.6 am coinciding with a data value
444,1,"['arithmetic mean', 'mean', 'data']", Properties of The Mean,seg_103,"if the arithmetic mean of n data values coincide exactly with one of the data values (say xk), then xk must be the am of the other (n − 1) data values."
445,1,"['mean', 'data']", Properties of The Mean,seg_103,solution 2.6 let there be n data values with mean xn. then we have n ∗ xn =
446,1,"['loss', 'data']", Properties of The Mean,seg_103,"n =1 xi. without loss of generality, assume that the coinciding data value is xk so that xn = xk and the lhs becomes nxk. cancel one xk term from lhs and rhs. the multiplier on the lhs becomes (n − 1). what remains on the rhs is the sum of the data values less xk. divide both sides by (n − 1) to get xk = xn−1. as k is arbitrary, the result follows. this result is easy to extend to gm and hm (see exercise 2.10, p. 65)."
447,1,"['variance', 'analysis of variance', 'variances']", Properties of The Mean,seg_103,"in analysis of variance procedures, we encounter within group variances which are measured around the means of each group xi. = 1"
448,1,['variances'], Properties of The Mean,seg_103,"n∑jxij, and between group variances"
449,1,['mean'], Properties of The Mean,seg_103,which are measured around overall mean x.. = n
450,1,"['variable', 'mean']", Properties of The Mean,seg_103,1 k∑i∑jxij. note that a “.” in these expressions fixes a variable. thus x is the mean that is averaged around all values
451,1,['mean'], Properties of The Mean,seg_103,".. of i and j, whereas xi. is the mean that is averaged around all j values."
452,1,"['sample', 'sample median', 'efficient', 'median', 'data', 'distribution', 'set', 'frequency', 'data set', 'population']", MEDIAN,seg_105,"the population median is that value below which 50% of the values fall. in other words, the median divides the total frequency (area under the distribution) into exactly equal parts. analogous definition holds for the sample median. it is most appropriate when all sample values are different. it can be easily found if the sample values are arranged in sorted order (in ascending or descending order). the complexity of sequential data sorting is o(n log n) where n is the size of the data. parallel sorting techniques can improve this to o(n). still, it may be time consuming to sort an entire data set, just to find the median when the data size is too large. however, efficient algorithms are available to locate approximate median without data sorting [2, 17]."
453,1,"['sample size', 'sample', 'median']", MEDIAN,seg_105,"the median of a sample is unique for odd sample size (middle element at [(n + 1)∕2]th position, or x(n+1)∕2). when the sample size is even, we take the"
454,1,"['mean', 'median']", MEDIAN,seg_105,arithmetic mean of the middle values (at (n∕2)th and (n∕2 + 1)th positions) as the median. symbolically:
455,1,['median'], MEDIAN,seg_105,example 2.7 median finding
456,1,['median'], MEDIAN,seg_105,"find the median of (5,2,8,4,7) and (5,2,8,9,4,7)."
457,1,"['median', 'observations', 'case', 'data', 'set', 'data set', 'mean', 'number of observations']", MEDIAN,seg_105,"solution 2.7 here, the number of observations is odd. the sorted data set is (2,4,5,7,8). the middle element is 5, which is the median. in the second case, the number of observations is even. the sorted data set is (2,4,5,7,8,9). the middle elements are 5 and 7. the mean of these middle elements is (5+7)/2= 6, which is the median."
458,1,"['data', 'median']", MEDIAN,seg_105,"trimmed median is meaningful when the trimming occurs at either of the extremes. if data values are discarded at the low end, the trimmed median moves to the right and vice versa. when an equal number of data values are discarded from both ends, the median will remain the same."
459,1,"['grouped data', 'data', 'median']", Median of Grouped Data,seg_107,"finding the median of grouped data is more difficult, as we need to first locate the median class. it is found in two steps as follows:"
460,1,['median'], Median of Grouped Data,seg_107,1. find the class to which the median belongs
461,1,"['sample size', 'sample', 'median', 'cumulative frequency', 'frequency', 'limit']", Median of Grouped Data,seg_107,"2. compute it as median = l + c ∗ (n∕2 − m)∕f where l is the lower limit of the median class, c is the fixed class width, n is the sample size, m is the cumulative frequency up to median class, and f is the frequency in the median class."
462,1,"['median', 'random variable', 'variable', 'random', 'expected values']", Median of Grouped Data,seg_107,"theorem 2.3 the expected absolute departure of a random variable is minimum when it is taken around the median (i.e., e|x − c| is minimum when c is the median (expected values are discussed in chapter 8))."
463,1,['discrete'], Median of Grouped Data,seg_107,"proof: let x be discrete. by definition, e|x − c| = ∑ (c − xi)f (x) + ∑ (xi − c)f (x)."
464,1,['summation'], Median of Grouped Data,seg_107,"xi c xi c perturb the constant c by a small amount  c so that c = c −  c. the net change is then δ = − ∑ ( c)f (x) + ∑ ( c)f (x). taking the constant  c outside the summation, we"
465,1,['median'], Median of Grouped Data,seg_107,"xi c xi c get δ =  c[∑ f (x) − ∑ f (x)]. if c is the median, then the expression in the square"
466,1,"['median', 'frequency', 'continuous']", Median of Grouped Data,seg_107,"brackets is zero (because the median divides the total frequency into equal parts). thus, the result. if x is continuous, we could write"
467,1,"['sample size', 'sample', 'indicator function', 'median', 'observations', 'results', 'data', 'frequency', 'mean', 'function', 'indicator']", Median of Grouped Data,seg_107,"whereas the mean balances the data above and below it in terms of the magnitudes of observations, the median balances the frequency (count) of data above and below it, irrespective of their magnitudes (here we are assuming that the median for even sample size is the mean of the middle (sorted) sample values). thus the median can be found iteratively using an indicator function. define an indicator function i(xj) = 1 if xj  median and i(xj) = 0 otherwise. summing results in ∑xj i(xj) = n∕2 if n is even;"
468,1,['median'], Median of Grouped Data,seg_107,and (n − 1)∕2 if n is odd (because i(xj) is zero at xj =median). then the median can be defined as
469,1,['median'], Median of Grouped Data,seg_107,n∕2 if n is even; median = maximum xj such that ∑ i(xj) = xj {
470,1,"['quartiles', 'median']", Median of Grouped Data,seg_107,"as i(xj) is defined in terms of the median, we start with a guess value (say m0) and evaluate the lhs. if it is less than the rhs, it means that our guess value was short of the true median. we increment our guess value m0 by a small amount, and repeat the above procedure (checking i(xj) = 0 values and changing perhaps some of them to 1) until equality holds. if lhs sum is greater than the rhs value, we keep on decrementing our guess value m0 by a small amount (checking i(xj) = 1 values and changing perhaps some of them to 0) until equality holds. this is easy to parallelize, and can be extended to find quartiles (discussed below)."
471,1,"['sample median', 'sample', 'median', 'observations', 'skewed', 'data', 'skewed data', 'mean', 'ogive']", Median of Grouped Data,seg_107,2.4.1.1 advantages of median the sample median is least influenced by extreme observations (for n 2). it can be approximated graphically using ogive curves. median is better than the mean for skewed data. the median can be found even for open-ended data.
472,1,"['sample size', 'sample', 'sample median', 'median', 'data', 'samples', 'mean']", Median of Grouped Data,seg_107,"finding the median of a sample of size n ≥ 4 is computationally more involved than finding the mean. if the data are unsorted, we may require multiple passes through the data to locate the median. the nature of the sample size n (whether it is odd or even) should be known to compute the sample median, whereas this is immaterial to compute the mean and mode. the theorem 2.1 (p. 48) allows us to use a divide and conquer strategy to find the mean of large samples by finding the mean of subsamples, but such a strategy will not in general work for finding the median."
473,1,"['sample', 'medoid', 'median', 'case', 'algorithm', 'data', 'distance metric', 'mean']", Median of Grouped Data,seg_107,sample median is used as smoothing filters in digital image processing. it is also used in data clustering algorithms (k-median algorithm). the data item nearest to the mean (if mean does not coincide with a sample item) is called the medoid. this nearness can be quantified using a distance metric. the medoid is not unique in the univariate case if it is equally distant from the nearest data points on both the sides of it.
474,1,"['medoid', 'case', 'algorithm', 'location', 'correlation', 'distance metric', 'data', 'location measures']", Median of Grouped Data,seg_107,"this is more of a problem in the multivariate case, in the presence of correlation, for which mahalanobis distance metric is the most appropriate choice. medoid is used in k-medoid algorithm of data clustering. location measures are summarized in figure 2.1."
475,1,"['sample', 'quartiles', 'percentile', 'median', 'percentiles', 'quartile', 'third quartile', 'frequency', 'first quartile']", QUARTILES AND PERCENTILES,seg_109,"quantiles is a common name for quartiles (which divide the total frequency into four equal parts), deciles (which divide the total frequency into 10 equal parts), and percentiles (see figure 2.2). they can be considered as generalizations of the median. as the quartiles divide the total frequency into four equal parts, there are three of them. the first quartile is denoted by q1. it is that value of x below which one-fourth of the frequency lie. the second quartile is the same as the median (q2 =median). the third quartile q3 is that value of x below which three-fourth of the frequency lie (or above which one-fourth of the frequency lie). deciles divide the total frequency into one-tenth parts. percentiles are those values of x that divide the total frequency into units of (1/100). thus, 25th percentile= first quartile. the five parameters [x(1),q1,q2 = m,q3, x(n)] is called the five-number summary of a sample, where x(1) and x(n) are the minimum and maximum of the sample."
476,1,"['quartiles', 'class interval', 'interval', 'cumulative frequency', 'grouped data', 'data', 'percentiles', 'frequency', 'limit']", QUARTILES AND PERCENTILES,seg_109,"the quartiles of grouped data are found using qk = l + c ∗ (kn∕4 − m)∕f where k = 1 for q1, and k = 3 for q3. here, l is the lower limit of the respective class, c is the fixed class interval, n is the total frequency, m is the cumulative frequency up to the respective class, and f is the frequency in the respective class. this formula can be generalized to find the percentiles as pk = l + c ∗ (kn∕100 − m)∕f ,"
477,1,"['quantile', 'probabilities', 'inequality', 'discrete', 'discrete distributions', 'function', 'distributions']", QUARTILES AND PERCENTILES,seg_109,"where k ∈ {1, 2, 3, .. , 99}. a quantile function q(u) is analogously defined as q(u) = inf {x ∶ f(x) ≥ u}, where u ∈ (0, 1). they can easily be found for continuous distributions (find x ∶ x = f−1(u)). the inequality may not strictly hold for discrete distributions due to uneven split of probabilities."
478,1,"['outliers', 'sample', 'median', 'quantiles', 'data']", QUARTILES AND PERCENTILES,seg_109,"trimmed quantiles are obtained by trimming data values from either or both the ends of a sample. they are useful when outliers are present. when an equal number of data values are discarded from both extremes, the median will remain the same, but other quantiles will move uniformly toward the median."
479,1,"['sample', 'observations', 'grouped data', 'data', 'population']", MODE,seg_111,"mode of a sample is that data item which occurs most frequently. the corresponding value is called modal value. if each data item is unique, any of the observations can be taken as the mode. hence, it is most appropriate when some sample values are repeated. a population with two or more modes is called multi-modal. the mode of grouped data can be found using a two-step procedure:"
480,0,[], MODE,seg_111,1. find the class to which the mode belongs.
481,1,"['sample', 'variables', 'conditional', 'samples', 'frequency', 'mean', 'bivariate', 'limit']", MODE,seg_111,"the lower limit of modal class, l is the difference in frequency between modal class and the class below it, and u is the difference in frequency between the next class above it and the modal class. for bivariate and higher samples, we could define conditional mode by fixing (conditioning) some of the variables. however, the existence of unique conditional modes does not necessarily mean that the mode for the entire sample is unique. as an example, in a class of students, there could exist multiple values for height or weight but it is rare to have two or more students with the same height and weight unless the sample is too large."
482,1,"['outliers', 'missing value', 'case', 'observation', 'frequency', 'sample', 'symmetric', 'data', 'mean', 'distributions', 'sample size', 'median', 'skewed', 'skewed distributions']", Advantages of Mode,seg_113,"as the mode is located along the maximum frequency, it is easy to find irrespective of whether the data are symmetric or skewed. mode can be found even when the data are open ended. some other advantages are summarized below: (1) mode can be approximated graphically, which is useful for skewed distributions and in multivariate case. (2) mode is not influenced by outliers. (3) the modal value coincides with a sample observation (whereas the median for even sample size and mean need not coincide with sample values). (4) mode is the most appropriate measure for categorical data. the biggest disadvantage of mode is that it need not be unique. it can coincide with the minimum or maximum of the sample (which is not possible for mean for n ≥ 2, although it could happen for median). when large number of data items are missing or have default values, the mode can wrongly get located at the missing value. mode utilizes only the value of most frequently occurring observation"
483,1,"['sample', 'contrast', 'frequency', 'mean']", Advantages of Mode,seg_113,(max frequency counts) in contrast to the mean that utilizes actual values of every item in a sample.
484,1,"['median', 'inequality', 'samples', 'mean', 'distributions']", Advantages of Mode,seg_113,"an approximate relation exists for samples from bell-shaped distributions as (mean–mode) ≃ 3(mean–median). for right–skewed distributions (mean ≤ median ≤ mode). this is called the mean–median–mode inequality [18, 19]."
485,1,"['geometric mean', 'geometric', 'mean']", GEOMETRIC MEAN,seg_115,the geometric mean (gm) of n nonzero numbers is defined as
486,1,['observations'], GEOMETRIC MEAN,seg_115,"where ∏ denotes the product of the observations. we will denote gm by x̂n to distinguish it from xn. if none of the observations are zeros, we could take the logarithm of both sides of equation (2.18) to get log(gm) = (1∕n)∑i"
487,1,"['arithmetic mean', 'mean', 'observation']", GEOMETRIC MEAN,seg_115,"n =1 log (xi). this shows that log(gm) is the arithmetic mean in the “log-space.” because the logarithm is defined only for positive argument, this summary measure is meaningful only when all observations are positive (if at least one observation is 0, the product will itself be zero. the usual practice in such situations is to omit all zeros, and find the gm of the remaining values)."
488,1,"['sample', 'graphics', 'observations', 'compound', 'population', 'rates']", GEOMETRIC MEAN,seg_115,"the gm coincides with the sample observations when all observations are equal. hence, it is most appropriate when the products of several positive numbers combine together to produce a resulting quantity as in rates of changes, exchange rates, inflation rates, compound interests, population growth, and so on. other examples are successive discounts; price or stock market increases and decreases; successive size changes (enlargements or contractions) of images, graphics; successive volume changes; and so on. it is used in image enhancement applications to smooth low contrast images by taking the gm of the surrounding pixels."
489,1,"['arithmetic mean', 'observations', 'geometric mean', 'change of origin', 'change of scale', 'data', 'mean', 'transformation', 'significance', 'geometric', 'rates']", GEOMETRIC MEAN,seg_115,"some of the rates of changes can be positive or negative. is the geometric mean defined for negative numbers? theoretically no!, because the nth root of a negative number is imaginary. however, if there is an even number of negatives, the product of them will be a positive number. as an example, if x = {− 3,−2, 2, 3}, the product of data values is +36. hence, it looks like we could define the gm when negative numbers occur in pairs. however, this is not true, because it loses the significance as a measure of central tendency, which may wrongly get located toward the positive values). this implies that even in rates of changes involving negative numbers, we should opt for the arithmetic mean. as gm inherently involves the product of individual observations, the change of origin technique is not useful. however, the change of scale transformation y = c ∗ x provides the relationship ŷn = c ∗ x̂n. the gm for"
490,1,['data'], GEOMETRIC MEAN,seg_115,grouped data is given by (
491,1,"['outliers', 'data', 'frequency']", GEOMETRIC MEAN,seg_115,n =1 fj is the total frequency. trimmed gm is meaningful when outliers or zero values are present. a left-trimmed gm is appropriate when data contain several zeros.
492,1,"['sample', 'case', 'data']", Updating Formula For Geometric Mean,seg_117,"as in the case of am, there are situations in which we need to update an already found gm using newly arrived data values. when all sample values are non-negative, an updating formula for gm can easily be derived as"
493,1,['factor'], Updating Formula For Geometric Mean,seg_117,"where the logarithm is to any base. by taking 1∕n as a common factor, and denoting log of gm by ẑ this could also be written as"
494,0,[], Updating Formula For Geometric Mean,seg_117,"the successive values can be evaluated iteratively by starting with log(x̂1) = log(x1), log(x̂2) = 1"
495,1,"['transformed', 'variables', 'observations', 'change of scale', 'transformation', 'recurrence']", Updating Formula For Geometric Mean,seg_117,"2 log(x̂1) + 1 2 log(x2), log(x̂3) = 2 3 log(x̂2) + log(x3)∕3, and so on. the iterations are stopped when log(x̂n) is reached. by taking the anti-logarithm we get the required result. when the variables are transformed using the change of scale transformation y = c ∗ x, these iterations are carried out in yi and at the end, the gm of y is multiplied by c to get the gm of x, as shown below. alternatively, we could add a constant log(c) to the recurrence (2.19) to iteratively update log(x̂i). as the gm involves the product of nonzero observations, weighted gm is meaningless. however, there is one situation where weighting by exponentiation is useful. consider gm  = (x1"
496,0,[], Updating Formula For Geometric Mean,seg_117,′s are scaled up if xj   1 and scaled down if 0   xj   1 (as the powers of a fraction are less than the fraction itself). in
497,1,['case'], Updating Formula For Geometric Mean,seg_117,"the particular case, when fj = −xj, we get gm  = (x1"
498,1,"['transformed', 'change of scale', 'data']", Updating Formula For Geometric Mean,seg_117,"lemma 2 prove that the gm of change of scale transformed data (y-variable) is given by gm(y) = c * gm(x), where yi = cxi, and c is a constant (positive or negative)."
499,1,"['geometric mean', 'geometric', 'mean']", Updating Formula For Geometric Mean,seg_117,example 2.8 geometric mean for shear strength
500,1,"['geometric mean', 'geometric', 'mean']", Updating Formula For Geometric Mean,seg_117,"find the geometric mean for the shear strength x = (32, 80, 56, 75, 69, 26, 44, 50) using equation (2.19)."
501,1,['precision'], Updating Formula For Geometric Mean,seg_117,"solution 2.8 as the numbers are large, we will apply the above lemma to compute the gm (product of the original numbers is 42,435,993,600,000= 4.24e+13, which is too big for single precision) by dividing each number by 10 to get"
502,1,"['data', 'table']", Updating Formula For Geometric Mean,seg_117,"x = {3.2, 8.0, 5.6, 7.5, 6.9, 2.6, 4.4, 5.0}. here, n = 8. the calculations are shown in table 2.3, where the last column contains the successive gm. the gm of scaled data is 5.05204019. hence using lemma 3, the gm of original data is 5.05204019*10 = 50.5204019. as ln(4.24359936e+13)/8= 3.92237725 and exp(3.92237725) = 50.5204019, we get the same result directly."
503,1,"['arithmetic mean', 'observations', 'data', 'mean']", HARMONIC MEAN,seg_119,"if all the observations are nonzero, the reciprocal of the arithmetic mean of the reciprocals of observations is known as hm. for ungrouped data, it is defined as hm = n∕∑i"
504,1,"['grouped data', 'mean', 'data', 'case']", HARMONIC MEAN,seg_119,n =1(1∕xi). the hm is used when (nonzero) numbers combine via reciprocals as in the case of finding the mean speed of vehicles that go the same distance (not for the same duration). the hm for grouped data is given by f∕∑i
505,1,"['sample', 'precision', 'algorithm', 'set', 'inequality']", HARMONIC MEAN,seg_119,"n =1 fj. we will denote it by xn to distinguish it from x and xn. a simple inequality exists n between the three popular means as: (am ≥ gm ≥ hm). when each of the sample values are weighted using the same set of weights, this identity is preserved [20]. hm finds applications in clustering (k-hms algorithm). the f-score used in text mining is the hm of precision and recall [2]."
506,1,['sample'], Updating Formula For Harmonic Mean,seg_121,"if the sample values arrive successively, we may have to update the hm from an already found value. the updating formula for hm is easily derived as"
507,0,['n'], Updating Formula For Harmonic Mean,seg_121,"or in terms of the hm() notation as hm(xn) = n∕[(n − 1)∕hm(xn−1) + 1∕(xn)]. dividing numerator and denominator by n, and rearranging gives"
508,1,"['harmonic', 'harmonic mean', 'mean']", Updating Formula For Harmonic Mean,seg_121,"where hm(xj) denotes the harmonic mean of x1, ...., xj, and hm(x1) = x1. denoting the reciprocal of the hm by rhm, we could rewrite it in the easy-to-remember form"
509,1,"['change of origin', 'case', 'change of scale', 'transformation']", Updating Formula For Harmonic Mean,seg_121,"as in the case of gm, the change of origin transformation is meaningless. the change of scale transformation y = c ∗ x for hm gives hm(y)= c*hm(x)."
510,1,"['transformed', 'data']", Updating Formula For Harmonic Mean,seg_121,"lemma 3 if yi = c ∗ xi, where c is a constant, prove that the hm of transformed data (y-variable) is given by hm(y) = c * hm(x)."
511,1,"['outliers', 'data']", Updating Formula For Harmonic Mean,seg_121,trimmed hm is meaningful when outliers or zero values are present. a left-trimmed hm is appropriate when data contain several zeros.
512,1,"['harmonic', 'harmonic mean', 'mean']", Updating Formula For Harmonic Mean,seg_121,example 2.9 harmonic mean finding
513,1,['data'], Updating Formula For Harmonic Mean,seg_121,find the hm for the data in example 2–32 (pp. 2–24) using equation in page 60.
514,1,"['observations', 'table']", Updating Formula For Harmonic Mean,seg_121,"solution 2.9 the calculations are shown in table 2.4. column 2 gives the reciprocal of observations. these sum to 0.170622041. column 3 gives the rhs of equation (2.22), the reciprocals of which are given in column 4. we could verify our result by direct substitution as n∕(∑ 1∕xi) = 8∕0.170622041 = 46.88726, which agrees with the last entry in column 4."
515,1,['median'], WHICH MEASURE TO USE,seg_123,"three types of means (am, gm, and hm), along with the median and mode, serve"
516,1,"['mean', 'location']", WHICH MEASURE TO USE,seg_123,"as measures of location. in addition, the quadratic mean is defined as √"
517,0,[], WHICH MEASURE TO USE,seg_123,a question that analysts face is “which measure is the most appropriate?”. this
518,1,"['quantitative', 'interval', 'geometric mean', 'observation', 'ordinal data', 'geometric', 'rates', 'sample', 'ordinal', 'data', 'mean', 'graphics', 'median', 'arithmetic mean', 'skewed', 'distribution', 'medoid', 'nominal', 'average']", WHICH MEASURE TO USE,seg_123,"depends both on the nature of the data (qualitative or quantitative, positive or negative) and the application at hand. the mode is the only appropriate measure for numerically coded nominal or ordinal data. for interval and ratio type data, the median is better than the mean and mode if the distribution is skewed. as the medoid coincides with a sample observation, it is preferred when all data values are integers and arithmetic operations involve differences between data values and the medoid (as in clustering). the arithmetic mean is to be preferred when the numbers combine additively to produce a resultant value. examples are consumption of materials or power, quantities measured on a scale such as heights, weights, thickness, and temperatures. the geometric mean is better suited when several nonzero numbers combine multiplicatively to produce a resultant value (or equivalently, the logarithm of several nonzero numbers combine additively). this includes rates of changes such as successive discounts; time-dependent growth; successive size changes (enlargements or contractions) of images, graphics; successive volume changes, power and voltage changes and so on. the hm is preferred when reciprocals of several nonzero numbers combine additively to produce a resultant value. examples are electrical resistance or capacitance in parallel circuits, average speed of vehicles for the same distance, and so on. quadratic mean is better suited when squares of several numbers combine additively as in squared euclidean distances."
519,1,"['outliers', 'median', 'data', 'location', 'weighted mean', 'mean', 'population', 'varying']", SUMMARY,seg_125,"several popular measures of location are introduced and exemplified which are useful to compare several groups. the measures of location portray the central location of varying data values. they are sometimes called “sample statistics” which are substitutes for their population counterparts. the mean is vulnerable to unusually low or high values (which are recognized as outliers) in an uneven manner. in such situations, the median should be used. when the data size is large or there is a need to identify more often repeating value in the data, the mode is preferable over the mean or median. if neither the median nor the mode resolves the issue of uneven influences exerted by the outliers on the measures of location, the weighted mean could be chosen as a remedy."
520,1,"['median', 'location', 'data', 'location measures']", SUMMARY,seg_125,"updating formula for some of them are also presented. these are useful in online computations, where new data arrive continuously. important properties of these location measures are also discussed. this allows an analyst to choose the most appropriate measure of central tendency for the data at their hand [21]. median finding algorithms are discussed in references 17, 22, and 23. see reference 20 for a discussion of am, gm, and hm inequalities for weighted data, and references 24, 25"
521,1,"['variability', 'location measures', 'location']", SUMMARY,seg_125,for new measures of central tendency and variability. a discussion of visualizing of location measures can be found in references 26–28.
522,1,"['quartiles', 'data', 'set', 'data set']", SUMMARY,seg_125,a) quartiles divide a data set into three equal parts
523,1,['quartiles'], SUMMARY,seg_125,b) third quartiles lies between 7th and 8th decile
524,1,['sample'], SUMMARY,seg_125,c) the mode of a sample can never be the minimum of the sample
525,1,"['frequency', 'data', 'median']", SUMMARY,seg_125,d) the median balances the frequency count of data above and below it
526,1,"['geometric mean', 'geometric', 'mean', 'data']", SUMMARY,seg_125,e) geometric mean of data containing at least one zero is zero
527,1,"['sample', 'observation']", SUMMARY,seg_125,f) every sample observation contributes to the mode
528,1,['sample'], SUMMARY,seg_125,g) mode of a sample is always a sample value
529,1,"['mean', 'data']", SUMMARY,seg_125,h) duplicate data values are counted distinctly in finding the mean.
530,1,['table'], SUMMARY,seg_125,"2.2 the tuition fees of 230 graduate schools per semester are given in table 2.5,"
531,1,"['mean', 'median']", SUMMARY,seg_125,where the count column indicates the number of schools charging the fee on the left. find the mean and median of tuition fees.
532,1,['medoid'], SUMMARY,seg_125,2.3 what is a medoid? what are its needed to update the mode using
533,1,['data'], SUMMARY,seg_125,uses? how can it be used to meanew data?
534,1,['data'], SUMMARY,seg_125,sure data spread? 2.7 which of the following is most
535,1,"['location measures', 'location']", SUMMARY,seg_125,2.4 to which of the location measures appropriate as a measure of loca-
536,1,"['average', 'medoid']", SUMMARY,seg_125,does a medoid converge to as the tion in finding the average distance
537,0,[], SUMMARY,seg_125,sample size is increased? of vehicles that travel the same
538,0,[], SUMMARY,seg_125,2.5 for which of the following meaduration?
539,1,"['arithmetic mean', 'geometric', 'change of origin', 'mean']", SUMMARY,seg_125,sures is the change of origin tech(a) arithmetic mean (b) geometric
540,1,"['harmonic mean', 'median', 'arithmetic mean', 'geometric mean', 'mean', 'harmonic', 'geometric']", SUMMARY,seg_125,nique useful? (a) arithmetic mean mean (c) harmonic mean (d) all (b) geometric mean (c) harmonic of them mean (d) median
541,1,"['location measure', 'location']", SUMMARY,seg_125,2.8 which location measure is most 2.6 in what situations is the mode
542,1,['data'], SUMMARY,seg_125,"appropriate for the following data? most appropriate, and most inap-"
543,1,['information'], SUMMARY,seg_125,propriate? what information is (i) growth of visitors to a web site
544,1,"['median', 'grouped data', 'data', 'mean', 'trimmed mean']", SUMMARY,seg_125,(ii) amount of money in a com2.18 describe situations where pound interest account (iii) electrimmed mean and median are tric current in a parallel circuit (iv) useful for grouped data. can you debt of a company. find trimmed mean without complete data sorting? 2.9 if the gm of n data values coincide
545,1,"['harmonic mean', 'data', 'location', 'mean', 'harmonic']", SUMMARY,seg_125,"2.19 give examples of some situations exactly with one of the data values where the harmonic mean is the (say xk), then prove xk is the gm most appropriate location meaof the other (n − 1) data values. sure. when is it most inappropri2.10 if the hm of n data values coincide ate?"
546,1,"['sample', 'standardized', 'data', 'mean', 'standard']", SUMMARY,seg_125,"exactly with one of the data values 2.20 can you always find the gm and (say xk), then xk must be the hm hm for standardized data y = of the other (n − 1) data values. (x − x)/s, where x is the sample 2.11 what is the most commonly used mean and s is the standard devia-"
547,1,"['observations', 'mean']", SUMMARY,seg_125,location measure? what are its tion? advantages over others? 2.21 find the mean of n observations 2.12 what are some uses of samthat are in arithmetic progression
548,1,"['medoid', 'median']", SUMMARY,seg_125,ple median? what is a medoid? with first term k and common difwhere is it used? ference d.
549,0,[], SUMMARY,seg_125,2.13 what is the least stable measure 2.22 what is the first step in comput-
550,1,"['median', 'geometric mean', 'grouped data', 'data', 'mean', 'geometric']", SUMMARY,seg_125,of central tendency? (a) arithmetic ing (i) the mode? (ii) the median mean (b) geometric mean (c) harof raw data and grouped data?
551,1,['mean'], SUMMARY,seg_125,monic mean (d) mode 2.23 what are the two situations in
552,1,['mean'], SUMMARY,seg_125,2.14 prove that the sum of the deviawhich the mean is the same as the
553,1,['sample'], SUMMARY,seg_125,tions of sample values from the sample value?
554,1,"['change of origin', 'mean']", SUMMARY,seg_125,sample mean is zero. what is the 2.24 when is the change of origin use-
555,1,"['mean', 'population']", SUMMARY,seg_125,corresponding population equivaful in computing the mean? lent?
556,1,"['percentage', 'arithmetic mean', 'plots', 'mean']", SUMMARY,seg_125,2.25 the percentage of seeds that ger2.15 what is trimmed arithmetic mean? minate from eight different plots
557,1,['harmonic'], SUMMARY,seg_125,"ric and trimmed harmonic means. 89.3, 94.4, 95.0, 83.1, 90.6, 96.1}."
558,1,"['geometric', 'location measure', 'location']", SUMMARY,seg_125,2.16 in what situations is the geometric which location measure is most
559,0,[], SUMMARY,seg_125,mean most appropriate? what are appropriate? find its value.
560,1,['data'], SUMMARY,seg_125,some data restrictions on comput2.26 the first-order bragg reflection of
561,0,[], SUMMARY,seg_125,ing it? x-ray at different angles through t 2.17 if xn−2 denotes the 1-trimmed a crystal gave the wavelengths (in
562,1,"['change of scale', 'mean', 'observation', 'median']", SUMMARY,seg_125,"mean after deleting the smallest nanometers) as {0.0795, 0.0841, and largest observation in a sam0.0790, 0.0844, 0.0842, 0.0840}. t ple, prove that (1 − 2∕n) ∗ xn−2 = use the change of scale technique xn − (x(1) + x(n))∕n. to find the mean and the median."
563,0,[], SUMMARY,seg_125,2.27 the number of hours that a bat2.32 the expected absolute departure
564,1,"['median', 'random variable', 'variable', 'mean', 'random']", SUMMARY,seg_125,tery can be continuously operof a random variable is minimum ated in different devices after a when it is taken around the—(a) 30 min recharge is given below. mean (b) median (c) mode (d) both find the median and mean. x = (a) and (b).
565,0,[], SUMMARY,seg_125,subjected to an elongation 2.28 the resistance of an electronic
566,1,"['median', 'data', 'mean', 'test']", SUMMARY,seg_125,"stress test to see how much circuit was measured using it can be stretched before it five different components as breaks. let x = {9.2 6.7 15.3 {5.2, 4.9 5.12, 4.95, 5.1}. find the 18.0 11.6 10.8 7.7 16.1 8.5 12.0} mean and median. convert data to denote the break point length in z-scores. cm. (i) find the mean and the 2.29 what is trimmed median? if median."
567,1,"['sample', 'data']", SUMMARY,seg_125,"extreme data values are removed 2.34 soluble dissolvents (in milfrom both ends of a sample, does ligram/liter) in drinking water"
568,1,['median'], SUMMARY,seg_125,the trimmed median differ from are measured at different places
569,1,"['median', 'mean', 'standard']", SUMMARY,seg_125,"the original median? in a city. find the mean 1 n 2.30 prove that (xn −  ) = ∑j=1(xj − and median, and standard-"
570,1,['median'], SUMMARY,seg_125,n  ) = (median −  ) + 1 n ∑n
571,1,"['sample', 'median', 'data', 'mean']", SUMMARY,seg_125,"j=1(xj − ize the data where x = {560, median), where xn is the mean of 458, 490, 525, 482, 554, 499, 538, a sample of size n. 540, 507, 481, 513} is the amount of dissolvent in mg/l. 2.31 if data values are discarded from"
572,1,"['sample', 'mean', 'median']", SUMMARY,seg_125,"the low end of a sorted sample, 2.35 should the complete sample be the trimmed median —(a) moves sorted to compute the trimmed t to the left (b) moves to the right mean using the formula xm ="
573,0,[], SUMMARY,seg_125,"k +1 x(i)? if not, how dictable much sorting is required?"
574,0,[], MEASURES OF SPREAD,seg_127,"after finishing the chapter, students will be able to"
575,0,[], MEASURES OF SPREAD,seg_127,◾ describe popular measures of spread
576,1,['range'], MEASURES OF SPREAD,seg_127,◾ understand range and inter-quartile range
577,1,"['deviation', 'standard deviation', 'variance', 'standard']", MEASURES OF SPREAD,seg_127,◾ understand variance and standard deviation
578,1,"['coefficient of variation', 'coefficient', 'variation']", MEASURES OF SPREAD,seg_127,◾ comprehend the coefficient of variation
579,0,[], MEASURES OF SPREAD,seg_127,◾ apply the above concepts to practical problems
580,1,"['range', 'location', 'sample', 'symmetric', 'data', 'samples', 'mean', 'population', 'statistical', 'distributions', 'unimodal', 'distribution', 'location measures', 'variability', 'bivariate']", NEED FOR A SPREAD MEASURE,seg_129,"the prime task in many statistical analyses is to summarize the location and variability of data. one or more concise measures are used for this purpose. these are real numbers for univariate samples, and a vector or matrix for bivariate and higher dimensional samples. chapter 2 introduced several location measures for this purpose. if repeated samples are drawn from a univariate population, they can lie anywhere within the range (min, max). as shown below, this depends on the shape of the distribution. if the parent population is unimodal (with a clear peak), a great majority of sample values will fall close to the mode. as the mean and mode coincide for symmetric unimodal distributions, we expect most of the data points to fall within the vicinity of the mean for such distributions. on the other hand, if the distribution is uniform, there is an equal chance for any new data item to fall anywhere within"
581,1,"['sample size', 'sample', 'leptokurtic', 'range', 'data', 'mean', 'distributions']", NEED FOR A SPREAD MEASURE,seg_129,"the range. the number of data points that fall in the vicinity of the mean or mode depends more on how fast the distributions tail-off in both directions. we expect less data points around the mean if the tailing-offs are slow, than otherwise. thus, there are likely to be more data points in the close proximity of the mean for leptokurtic distributions (defined in chapter 4) when sample size is large."
582,1,"['location', 'frequency', 'probability', 'sample', 'symmetric', 'data', 'mean', 'standard', 'population', 'confidence', 'location measure', 'distributions', 'median', 'distribution', 'standard deviations', 'variability', 'sampling', 'deviations', 'average', 'dispersion']", NEED FOR A SPREAD MEASURE,seg_129,"this shows that a location measure alone is insufficient to fully understand a data distribution. assume that we have somehow found the mean (average) of a population. in repeated sampling from that population, why do some data points fall above the mean, and some others fall below? can we predict with some confidence how far from a location measure (e.g., the mean) are the new data values likely to lie? what is the probability that a randomly chosen new data value will fall above the mean or two standard deviations away (in both directions) from the mean? these types of queries can be answered using spread measures discussed below. as the median divides the total frequency into two equal halves, we know that there is a 50–50 chance that a new sample value will be above or below the median. hence for symmetric unimodal distributions, we expect that there is an equal chance for new data values to fall above or below the mean too. however, to quantify “how far from a location measure (such as the mean) they are likely to lie,” we need well-defined measures. these are called measures of dispersion or spread (we will use “measures of spread,” “dispersion measure,” or measure of variability synonymously)."
583,1,"['sample', 'variability', 'observations', 'data', 'statistic', 'dispersion measure', 'dispersion']", NEED FOR A SPREAD MEASURE,seg_129,"definition 3.1 a univariate dispersion measure concisely summarizes the extent of spread or variability of data in a sample of size n ≥ 2 using a well-defined statistic, with a minimum value of zero indicating that there is no spread; and an increasingly positive value indicating the extent of spread of observations."
584,1,"['range', 'case', 'location', 'function', 'sample', 'linear', 'ratio measure', 'parameter', 'population', 'density function', 'nonlinear', 'variance', 'location measures', 'dispersion measures', 'dispersion']", NEED FOR A SPREAD MEASURE,seg_129,"as the zero value is well-defined, this is a ratio measure. increasing values of it indicate that the sample values are more spread-out over its range. the extent of this spread depends on whether the measure is linear or nonlinear. as shown below, some of the dispersion measures (such as the variance) are upper-bounded by the square of the range. as in the case of location measures, these are also applicable to sample and population. a population parameter is called a scale-parameter if the density function takes the form (1∕ ) f (x∕ )."
585,1,"['sample', 'case']", NEED FOR A SPREAD MEASURE,seg_129,"there are two situations in which a univariate sample measure of spread can be zero—(i) if the sample contains just one item (n = 1), (ii) if all sample values coincide. in case (i) there is no spread as the sample is a singleton. this is symbolically written as s1"
586,1,"['sample', 'variance', 'sample variance']", NEED FOR A SPREAD MEASURE,seg_129,"2 = 0, where sn2 denotes the sample variance (defined in p. 77). note that"
587,1,"['deviation', 'sample', 'range', 'observations', 'case', 'mean', 'sample variance', 'variance']", NEED FOR A SPREAD MEASURE,seg_129,"if the sample variance uses (n − 1) in the denominator, we get a zero (1 − 1) in the denominator. hence, the sample variance is undefined (it is not zero) if (n − 1) is used in the denominator, and is zero if n is used. in case (ii), all the sample observations are the same (xi = xj = c ∀i, j). this is as equal as having a singleton sample. the mean in this case is c, so that each of the deviation terms in the numerator of s2n is zero. the sample range and mean absolute deviation are both zero as well (as the minimum and maximum are both c)."
588,1,"['range', 'change of origin', 'transformation', 'sample', 'linear', 'data', 'mean', 'standard', 'sample variance', 'standard deviation', 'sample size', 'median', 'nonlinear', 'change of scale', 'variance', 'deviation', 'table', 'deviations']", Categorization of Dispersion Measures,seg_131,"sample range, inter-quartile range (iqr), mean absolute deviations (from the mean or median), sample variance, and standard deviation are the most commonly used measures of spread. while the sample variance additively combines the squared deviations of sample values from its mean, the mean absolute deviation combines the absolute values of deviations additively, and the range-based measures (sample range, iqr, etc.) combine the appropriate extremes of sample values linearly. all dispersion-measures quantify the spread of data into a positive numeric scale. they are not affected by a change of origin transformation (as the entire data are translated linearly by this transformation). all of the measures defined below are affected by a change of scale transformation. there are many ways to categorize the measures of spread (see table 3.1)—(i) linear and nonlinear measures, (ii) pivotal measures and pivot-less measures, (iii) measures that utilize sample size and those that do not use sample size, and (iv) additive and nonadditive measures."
589,1,"['sample', 'functions', 'linear', 'range', 'nonlinear', 'function']", Categorization of Dispersion Measures,seg_131,"1. linear and nonlinear measures linear measures combine sample values as simple linear functions or their deviations from pivotal values. for instance, the sample range is a linear function of the first and last sample values as r = (x(n) − x(1)) (see section 3.2, p. 71)."
590,1,"['deviation', 'mean']", Categorization of Dispersion Measures,seg_131,the mean deviation n
591,1,"['sample', 'linear', 'nonlinear', 'deviations', 'mean', 'function']", Categorization of Dispersion Measures,seg_131,"on the other hand, is a linear function of deviations measured from the mean xn. nonlinear measures combine sample values nonlinearly (as square-roots, squares, or higher powers). nonlinear measures are the preferred choice in some applications because they often inflate (blow-up) the deviations so that the computed value is larger than those obtained from linear measures."
592,1,"['sample', 'location', 'function', 'dispersion measures', 'location measure', 'dispersion']", Categorization of Dispersion Measures,seg_131,2. pivotal measures and pivot-less measures some of the dispersion measures use a location measure as a pivot to quantify the spread (see below). recall from chapter 2 that some of the location measures (such as the means) are expressible as a function of the sample values.
593,1,"['deviation', 'sample', 'range', 'location', 'quartile deviation', 'quartile', 'function', 'location measures', 'location measure']", Categorization of Dispersion Measures,seg_131,"by expanding such location measures as a function of the sample values, it is possible to obtain those spread measures without an explicit location measure. nevertheless, this criterion allows us to distinguish some spread measures from the others. exceptions are the range, iqr, and quartile deviation (qd) that do not use a location measure as a pivot."
594,1,"['sample size', 'sample']", Categorization of Dispersion Measures,seg_131,3. measures that utilize the sample size
595,1,"['range', 'coefficient of variation', 'sample', 'mean', 'coefficient', 'distributions', 'sample size', 'skewed', 'variance', 'skewed distributions', 'deviations', 'variation']", Categorization of Dispersion Measures,seg_131,"the variance, coefficient of variation (cv), and mean deviations discussed below fully utilize each and every sample value (and thus the sample size n). on the contrary, the sample range utilizes only the minimum and maximum; and iqr utilizes only two of the sample values denoted by q1 and q3. the range does not distinguish between multi-modal distributions, skewed distributions, and peaked distributions. hence, the range and iqr are called minimax measures."
596,0,[], Categorization of Dispersion Measures,seg_131,4. additive and nonadditive measures
597,1,"['sample', 'method', 'range', 'case', 'information', 'variance']", Categorization of Dispersion Measures,seg_131,"additive measures are those that can be found by divide-and-conquer (d c) method without further information. in other words, suppose we divide a sample into two subsamples and find the measure values from these subsamples. if we could combine these values obtained independently from the subsamples without additional information to find the corresponding measure for the entire sample, then it is called additive. in the case of sample range, we need extra information to find the range of the original sample. if a sample s is divided into two subsamples s1 and s2, and we find the ranges r1 and r2, we cannot find the range of the original sample s unless the subsample minimums and maximums are both known. sometimes the subsamples may be such that all elements in one of them is less than (or greater than) all elements in the other. if such overlap information about subsamples is known, we could sometimes find the range using the minimum of lower subsample and maximum of the upper subsample. however, the overlap can occur in many ways—(i) s1 completely subsumes s2, (ii) s2 completely subsumes s1, (iii) minimum of s2 lies between minimum and maximum of s1, or vice versa, and (iv) minimum of s2 is greater than maximum of s1 or vice versa. in this case, we could obtain range(s) = max(max(s1), max(s2)) − min(min(s1), min(s2)). suppose the subsamples s1 and s2 are nonoverlapping, and additionally we know that elements in s1 are all less than the elements in s2. in this particular case, we could find the range as range(s) = max(s2) − min(s1). similar arguments hold when the minimum element of s1 is greater than the maximum of s2, in which case the roles of s1 and s2 simply get swapped and we obtain range (s) = max(s1) − min(s2). variance is an additive measure."
598,0,[], Categorization of Dispersion Measures,seg_131,5. absolute and relative measures
599,1,"['deviation', 'range', 'observations', 'dispersion measures', 'deviations', 'mean', 'variance', 'standard', 'standard deviation', 'average', 'dispersion']", Categorization of Dispersion Measures,seg_131,"some of the dispersion measures are absolute. they are expressed in the same unit as that of the observations. examples of absolute dispersion measures are the range, qd, mean deviations, variance, and standard deviation. variance, being the average of the squared deviations of observations from their mean, is expressed in the unit squared. relative measures, on the other hand, do not"
600,1,"['location', 'rates', 'sample', 'sample mean', 'data', 'mean', 'standard', 'coefficient', 'standard deviation', 'median', 'variance', 'location measures', 'deviation', 'percentage', 'variability', 'sample standard deviation', 'deviations', 'dispersion']", Categorization of Dispersion Measures,seg_131,"depend on a unit. examples are the coefficient of dispersion and cv. absolute measures are easy to convert into relative measures. simply find the unit in which they are expressed and divide by another measure (usually one of the location measures) expressed in the same unit. sample standard deviation (s), being the positive square-root of variance (s2), has the same unit as the data. hence, we could divide s by any of the location measures (mean, median, or mode) to get a relative measure. as the standard deviation uses the sample mean as pivotal measure to take the deviations, it is customary to use the mean in the denominator to get a relative measure s∕x. owing to the possibility of x becoming zero (resulting in a very large value) this measure is defined only for x ≠ 0. this measure called the cv (p. 82) can also be expressed as a percentage. as it is dimensionless, it can be used to compare the variability of data measured in different units. for instance, data collected from different geographical regions that have different currencies (dollars, euro, yen, etc.) can be compared without worrying about the currency exchange rates or conversions."
601,1,"['sample', 'range', 'dispersion measures', 'case', 'distance metric', 'sample variance', 'variance', 'dispersion']", Categorization of Dispersion Measures,seg_131,"6. distance-based measures some of the dispersion measures can be cast in distance metric form. as an example, we can interpret the univariate sample range as either the manhattan distance between xn and x1 as |xn − x1|, or as the euclidean distance as [(xn − x1)1∕2]2. the sample variance in the univariate case is the squared"
602,1,['data'], Categorization of Dispersion Measures,seg_131,"1 (x − xn)′ (x − xn), where x is the data vector of size"
603,1,"['sample', 'sample mean', 'mean', 'average']", Categorization of Dispersion Measures,seg_131,"n, and xn is an n-vector in which each element is the sample mean xn (i.e., n values ⏞⏞⏞⏞⏞⏞ ⏞⏞⏞⏞⏞ ′ xn = {xn, xn, … , xn}). here, n denotes the appropriate divisor used (either n = n − 1, or n = n; see discussion below). similarly, the average absolute"
604,0,['n'], Categorization of Dispersion Measures,seg_131,deviation (aad) can be written as aad = n
605,0,[], Categorization of Dispersion Measures,seg_131,written in vector form in which each component is √|xi − xn|. the analogue in
606,1,"['sample', 'case']", Categorization of Dispersion Measures,seg_131,"the multivariate case is the mahalanobis distance (x − xn)′s−1(x − xn), where s is the pooled sample variance–covariance matrix."
607,1,"['sample', 'variability', 'range', 'data', 'location', 'information', 'samples', 'mean', 'population', 'vary']", RANGE,seg_133,"the sample range can throw more insight into the inherent variability in a population. suppose repeated samples are taken from a population and the range is updated each time. if it does not vary very much, it is an indication that we have captured most of the variability into the sample. as an example, if the range of temperatures in 24 h for two cities are the same, we cannot conclude that both cities have the same weather because one city, say on the sea-front, might have cooled faster at night whereas another city in mid-plains might have cooled slower. if we have the additional information that the mean temperature during the 24-h period was almost the same, we could have a better perception regarding the weather at the two cities. thus, a measure of location along with a spread measure can describe the nature of our data in a better way than either"
608,1,"['skewness', 'kurtosis', 'data']", RANGE,seg_133,"of them alone. as temperatures increase and decrease gradually, we can conclude that the weather is more or less the same. as shown below, even this cannot fully describe the data if skewness and kurtosis are also present."
609,1,"['sample', 'range', 'observation']", RANGE,seg_133,"definition 3.2 range of a sample is the difference between the largest and smallest observation of the sample. symbolically, if x = {x1, x2, ..xn} are the “n” sample values that are arranged in increasing order,"
610,1,"['sample', 'ordinal', 'interval', 'range', 'data', 'ordinal data', 'coefficient']", RANGE,seg_133,"the range is zero in only one particular case—when all of the sample values are the same. in all other situations, it is a positive number which is an integer when the sample values are integers. even if all data values are negative, the range is always positive as we are subtracting the minimum from the maximum. for instance, if x = { −11,−5,−3,−2}, the minimum is −11 and maximum is −2, so that the range is max − min = (−2) − (−11) = 11 − 2 = 9, where we have used the fact that maximum of negative numbers max(xi ∶ xi   0∀i) = −min(|xi|). range is defined for interval or ratio data too. it is also meaningful for numerically coded ordinal data, if the codes are equi-spaced. coefficient of range is defined as cr = (x(n) − x(1))∕(x(1) + x(n)), which is unit-less. if each of the sample values are positive, this measure lies in [0,1). if x(1) is negative, this measure could take any positive value. it is assumed that (x(1) + x(n)) is nonzero."
611,1,"['sample', 'range', 'observations', 'case', 'data', 'continuous']", Advantages of Range,seg_135,"the range is easy to compute and easy to interpret. we require only the smallest and largest observations of a sample to compute the range. this can be obtained in a single pass through the data (unless the data are sorted, in which case we can easily pick out the smallest and largest observations in two fetches). range is easy to update if new data arrive continuously. for instance, suppose data are received from a traffic sensor on a continuous basis. the data may indicate either the number of vehicles in a street or locality; or the speed of a passing vehicle. as new data arrive, it is a simple matter to check if it lies above or below the minimum and maximum to decide whether the range needs to be updated. if new data are within the so far accumulated min and max, the range is unaffected. the range can be bulk-updated if old minimum and maximum are known, and several new sample values are received. suppose a sample sk has minimum and maximum xk and xm"
612,1,['sample'], Advantages of Range,seg_135,k ax. if the minimum and maximum of a min new sample are xm
613,1,['range'], Advantages of Range,seg_135,"1 and xm k+a1x, the new range is max(xm k ax, xm"
614,0,[], Advantages of Range,seg_135,"course, we need to save the new minimum and maximum to update for subsequent iterations."
615,1,"['outliers', 'sample', 'range', 'observation']", Disadvantage of Range,seg_137,"the biggest disadvantage of range is that it is extremely sensitive to outliers (on both extremes). as it does not utilize every observation of a sample, it cannot distinguish"
616,1,"['sample size', 'sample', 'sample variance', 'range', 'skewed distributions', 'change of scale', 'skewed', 'data', 'transformation', 'indicator', 'variance', 'distributions']", Disadvantage of Range,seg_137,"between skewed distributions that have the same range. it is not a good indicator of spread when the sample size varies. range is not unit-less. it uses the same unit as that of the data. thus, it is affected by a change of scale transformation. for example, if the family incomes of a sample are measured in dollars and euros, the range will be different. it does not lend itself to further arithmetic operations (as does the sample variance)."
617,1,"['sample', 'range', 'data', 'information', 'variates', 'bivariate']", Disadvantage of Range,seg_137,"range is better suited for univariate data. range of multivariate data contains too little information about the multivariate spread, especially in the presence of correlation. for example, consider a bivariate sample of say height and weight of students, or amount of two different dissolvents in drinking water. the range can measure only the difference between the individual variates x and y."
618,1,"['range', 'transformation', 'process', 'sample', 'ordinal', 'estimate', 'data', 'quality control', 'sample size', 'scales of measurement', 'transformations', 'control', 'plotting', 'average', 'plot', 'measurement']", Applications of Range,seg_139,"the sample range has lot many applications in engineering and applied sciences. it is applicable to ordinal and higher scales of measurement. it is used in quality control and process control systems. some of the data plotting and visualization techniques use the sample range. as an example, the box-plot and range plot use the sample range. the sample range is also used in data transformations. for instance, the min–max transformation in (section 1.9.4) uses the data range in the denominator. if the sample size is small (say 4 or 5 as in quality control applications), the range is a quite good estimate of the spread. thus, we use average of the ranges r = ∑ri∕n in"
619,1,"['range', 'mean', 'control', 'process', 'charts']", Applications of Range,seg_139,"quality control charts as (x ∓ 3r∕(d2√n)). the mean of ranges r can indicate when a process deviates in one direction. for example, suppose a time-dependent process deviates to the “high” (or increasing) side. even if the range remains the same, the mean of ranges will steadily increase. however, if the range deviates from both sides (either inwards or decreasing values or outwards or increasing values), the mean of ranges could remain the same."
620,1,"['outliers', 'sample', 'quartiles', 'range', 'data']", INTERQUARTILE RANGE IQR,seg_141,"the sample range is sensitive to outliers at both ends. this could be diminished by removing possible outliers and then computing the range of remaining data. these are called trimmed range. a generalization of it is called the iqr. we defined quartiles in section 2.5. as the name implies, the iqr is the range of data quartiles."
621,1,"['outliers', 'sample', 'quantile', 'cumulative distribution function', 'quartiles', 'distribution function', 'observations', 'information', 'distribution', 'boxplots', 'function']", INTERQUARTILE RANGE IQR,seg_141,"definition 3.3 the iqr is defined as (q3 − q1), where q3 and q1 are the upper and lower quartiles. (q1 is that value below which one-fourth of the observations fall, and q3 is that value below which three-fourth of the observations fall, after the sample is arranged in ascending order). one half of iqr is called the qd. the unit quantile function is a parametrized version of it defined as q(u) = (f−1(u) − f−1(1 − u))∕2, where 0 ≤ u ≤ 1 and f(x) denotes the cumulative distribution function. this reduces to qd for u = 3∕4, and is negative for u   0.5. it is unaffected by outliers, and provides supplementary information on the spread of observations around the center of the sample. it is used in boxplots to visually detect outliers."
622,1,"['range', 'change of origin', 'case', 'data', 'change of scale', 'transformation']", Change of Origin and Scale Transformation for Range,seg_143,"range is unaffected by a change of origin data transformation. the change of scale transformation y = c ∗ x gives the relationship range(y) = c*range(x), as both extremes are scaled by the same constant. the constant c is chosen as  1 if x values are very large. this is especially useful when large data are expressed in scientific notation. in this case, dividing by 10k is done by adjusting just the index of the number. for example, let x = 3.6524219879e+8. to divide x by 106, simply adjust e+8 to e+2 to get x = 365.24219879, which is the number of days in a year."
623,0,[], Change of Origin and Scale Transformation for Range,seg_143,example 3.1 outstanding amounts on 10 bank loans
624,1,"['range', 'data']", Change of Origin and Scale Transformation for Range,seg_143,"ten outstanding loan amounts in a bank are x = [60,000, 40,000, 85,000, 37,000, 110,000, 280,000, 72,000, 92,000, 154,000, 81,000]. find the range and qd of the data."
625,1,"['transformed', 'range', 'data', 'quartile', 'coefficient', 'first quartile']", Change of Origin and Scale Transformation for Range,seg_143,"solution 3.1 as the data values are all large, we divide them by c = 100,000 to get y = [0.60, 0.4, .85, 0.37, 1.10, 2.80, 0.72, 0.92, 1.54, 0.81]. the maximum and minimum values of transformed data are 2.8 and 0.37. the range of y is 2.80 − 0.37 = 2.43. from this the range of x is obtained by multiplying by c as 2.43 ∗ 100,000 = 243,000. to find the qd, we need to find q3 and q1. the first quartile is that value below which one-fourth of the data values lie. rearranging the data in ascending order gives y = [0.37, 0.4, 0.60, 0.72, 0.81, 0.85, 0.92, 1.10, 1.54, 2.80]. as there are two data values below 0.60, q1 = 0.60. similarly, q3 = 1.10 as there are two values above it. from this we get the qd of y as 1.10 − 0.60 = 0.50. multiply by 100, 000 to get the qd of x as 50,000. the quartile coefficient is (q3 − q1)∕(q3 + q1) = 50,000∕170,000 = 0.294."
626,1,"['sample', 'data', 'sample variance', 'variance', 'dispersion measure', 'dispersion']", THE CONCEPT OF DEGREES OF FREEDOM,seg_145,degrees of freedom concept originated in data analysis. sample variance was the most popular dispersion measure in wide use during the 19th and early 20th centuries.
627,1,"['degrees of freedom', 'independent', 'statistics', 'level']", THE CONCEPT OF DEGREES OF FREEDOM,seg_145,"the concept of degrees of freedom (dof) is used in many branches of applied sciences. in physics and physical chemistry, it indicates the independent mode or free dimensionality in which a particle or system can move, or be oriented wrt fixed coordinate axes. in mechanical and aeronautical engineering, dof denotes the flexibility of motion of a particle or an object in 3d. such a particle has 6 dof—namely: (i) up or down (heaving), (ii) left or right (swaying), (iii) forward or backward (surging), (iv) tilting up or down (pitching), (v) turning left or right wrt a plane (yawing), and (vi) tilting side-to-side (rolling). it has an entirely different interpretation in statistics, where loosely speaking, it denotes the local level"
628,1,"['deviation', 'sample', 'estimated', 'independent', 'estimate', 'deviations', 'statistic', 'mean', 'confidence']", THE CONCEPT OF DEGREES OF FREEDOM,seg_145,"of confidence left in a sample of size n ≥ 2. if nothing has been estimated from a sample, its dof is n. the dof is reduced by one for each statistic (that uses all of the sample values) estimated from it. consider the deviations (x1 − x, x2 − x, … , xn − x). these deviations always sum to zero (p. 53), specifying any of the (n − 1) values automatically determines the nth deviation. this is precisely the reason why we use (n − 1) as the dof of a sample from which the mean has been estimated. it may also be noted that this reduction in dof is not a global phenomena. so, if 10 persons estimate the mean of a sample of size 15, the dof is reduced by one for each one of them (the dof does not become 5, but it is simply 14 for each person) under the assumption that each person’s procedures or actions are independent."
629,1,"['sample size', 'sample', 'standard normal', 't distribution', 'distribution', 'sum of squares', 'samples', 'normal', 'statistic', 'standard', 'standard normal distribution', 'normal distribution']", THE CONCEPT OF DEGREES OF FREEDOM,seg_145,"sampling distribution of the statistic t = (xn −  )∕(s∕√n) follows a student’s t distribution with n dof for normal samples, where n is the sample size. similarly, the sum of squares of n sample values drawn from a standard normal distribution has a central  2 distribution with n dof. these are discussed in chapter 11."
630,1,"['sample', 'process', 'estimated', 'sample mean', 'observations', 'data', 'deviations', 'mean', 'variance']", THE CONCEPT OF DEGREES OF FREEDOM,seg_145,"it is defined in terms of the mean as the pivot as in equation (3.8), which uses the sample mean xn explicitly. assume that the variance is computed in two steps. the first step computes the sample mean. the second step then finds the deviations of observations and finds the variance. as the mean has to be estimated from the data, some “information content” of the obtained sample is lost during this process."
631,1,"['sample', 'estimated', 'sample mean', 'estimation', 'statisticians', 'data', 'information', 'loss', 'mean', 'parameter', 'sample variance', 'variance']", THE CONCEPT OF DEGREES OF FREEDOM,seg_145,"for each parameter estimated from the sample, we quantify it as a unit loss of information. we use (n − 1) in the denominator of sample variance to indicate the loss of 1 “dof” due to the estimation of the sample mean from the data. to compensate for this loss of information, it is logical to use (n − 1) as the divisor for the variance. this lead some statisticians to advocate the formula (3.8) for sample variance. however, there are many expressions for the sample variance that does not explicitly involve the sample mean. some such formulas are given in chapter 1, which are repeated"
632,1,"['statistic', 'summation', 'varying', 'order statistic']", THE CONCEPT OF DEGREES OF FREEDOM,seg_145,where we have used the subscript partially varying summation notation introduced in chapter 1. another formula in terms of order statistic can be found in references 9 and 29.
633,1,"['sample', 'level', 'information', 'associated', 'function', 'confidence']", THE CONCEPT OF DEGREES OF FREEDOM,seg_145,definition 3.4 the dof is a concept associated with the information content of a sample that indicates a local level of confidence left in a sample as a function of the
634,1,"['sample', 'distribution', 'statistic', 'population']", THE CONCEPT OF DEGREES OF FREEDOM,seg_145,sample size. it is also applied to a statistic computed from a sample or the distribution of a parent population.
635,1,"['statistics', 't distribution', 'negative binomial', 'hypergeometric', 'sample', 'parameter', 'statistical', 'distributions', 'parameters', 'gamma', 'distribution', 'binomial', 'noncentral', 'sampling distribution', 'f distribution', 'determinant', 'noncentrality', 'sampling']", THE CONCEPT OF DEGREES OF FREEDOM,seg_145,"the dof of a statistical distribution is actually a parameter. they are so-called due to an analogy with the sampling distribution of some related statistics. as examples, the student’s t distribution has a parameter which is traditionally known as “n,” which is called its dof; and snedecor’s f distribution has two parameters m,n which are called its (numerator and denominator) dof. other distributions that utilize the dof concept are the 2 distribution, fisher’s z distribution, wishart distribution, and noncentral versions of these central distributions (noncentral 2,f,t ,z [4, 5]. many other distributions such as the distribution of the trace of a wishart matrix and the distribution of statistics computed from the sample variance–covariance matrix (such as the distribution of the determinant, or minimum and maximum eigen values) can also have dof parameter. noncentral distributions also exist without the dof concept. as examples, the noncentral gamma, beta, negative binomial, and hypergeometric laws have shape and scale parameters, and one or more noncentrality parameters but no dof."
636,1,"['deviation', 'sample', 'estimated', 'estimate', 'range', 'observation', 'statistic', 'mean', 'variance']", THE CONCEPT OF DEGREES OF FREEDOM,seg_145,"as mentioned above, 1 dof is lost for each statistic computed from a sample. this does not mean that we must lose one dof for each statistic. the rule is that if a statistic involves each and every observation of a sample, it loses 1 dof. thus if the mode or range of a sample is estimated, 1 dof is not lost. but if the am, gm, hm, variance or mean deviation, or some other statistic that utilizes each observation of a sample is estimated, 1 dof is lost for each such estimate."
637,1,"['deviation', 'sample', 'sample mean', 'mean']", AVERAGED ABSOLUTE DEVIATION AAD,seg_147,the aad (also called sample mean absolute deviation (smad)) from the mean is
638,1,"['sample', 'estimated', 'case', 'sample variance', 'variance']", AVERAGED ABSOLUTE DEVIATION AAD,seg_147,"where n = n − 1 if n   1 and xn is estimated from the sample (some authors use n in the denominator; this is why we have kept n which can be interpreted appropriately). as in the case of sample variance (defined below), this quantity is undefined for a sample of size 1, if n − 1 is used as the divisor. this is because the numerator then becomes x1 − x1 = 0, and the expression (3.5) is of 0/0 form. but if n = n, the aad is defined as zero (as the expression (3.5) becomes 0/1 = 0). expand xn in (3.5), and simplify to get the alternate expressions"
639,1,"['mathematical expectation', 'expectation', 'condition', 'population']", AVERAGED ABSOLUTE DEVIATION AAD,seg_147,"where we have used a condition on the second indexvar (p. 1–26). the corresponding population analogue is e|x − |, where e[ ] denotes mathematical expectation"
640,1,"['outliers', 'sample', 'deviation', 'medoid', 'median', 'arithmetic mean', 'range', 'case', 'observation', 'statistic', 'mean']", AVERAGED ABSOLUTE DEVIATION AAD,seg_147,"(chapter 8). it is also called mean (absolute) deviation from the mean. as in the case of arithmetic mean, this measure uses each and every observation of the sample. it is affected by outliers, but not as much as the range. computations can be simplified if medoid is used in place of the mean xn, resulting in aad from the medoid. when all sample values are integers, this will ease the computations because the medoid itself being a sample value will be an integer (whereas the mean need not be an integer) so that the differences are all integers. a related statistic is median absolute deviation (aad around the median) defined as ∑i"
641,1,"['deviation', 'median', 'observations', 'median absolute deviation', 'deviations']", AVERAGED ABSOLUTE DEVIATION AAD,seg_147,"n =1 |xi − median|∕n. median absolute deviation around the median is the middle value of (sorted) absolute deviations of observations from the median. symbolically,"
642,1,"['deviation', 'deviation ', 'median']", AVERAGED ABSOLUTE DEVIATION AAD,seg_147,median absolute deviation = median|xi − m| where m = median(xi). (3.7)
643,1,"['deviation', 'medoid', 'median', 'observations', 'median absolute deviation', 'deviations']", AVERAGED ABSOLUTE DEVIATION AAD,seg_147,"similarly, we could define the median absolute deviation around the medoid as the middle value of (sorted) absolute deviations of observations from the medoid (by replacing m by medoid in (3.7))."
644,1,"['deviation', 'sample', 'sample mean', 'median', 'range', 'deviations', 'mean']", Advantages of Averaged Absolute Deviation,seg_149,"as the deviation from each and every sample value is summed, it contains more information than the range. it is easy to compute as we need only the absolute deviations from the sample mean (or median)."
645,1,"['sample', 'treatment']", Disadvantages of Averaged Absolute Deviation,seg_151,"it does not lend itself to further arithmetic treatment. for example, if a sample s of size n is divided into two subsamples, and the aad of each subsample is found, it is not in general possible to combine the subsample values to find the aad of s."
646,1,"['deviation', 'medoid', 'median', 'change of origin', 'change of scale', 'data', 'summation', 'transformation', 'change of origin and scale']", Change of Origin and Scale Transformation for AAD,seg_153,"the aad can be found easily using the change of origin and scale transformation. consider the change of origin transformation y = x + c. then the aad of y is the same as the aad of x because each term inside the summation becomes |yi − yn| = |xi + c − (xn + c)| = |xi − xn|. this holds true for averaged absolute deviation from medoid and median, as the data are simply translated. next, consider the change of scale transformation y = c ∗ x. each deviation term of y is |yi − yn| = |c ∗ xi − c ∗ (xn)| = |c| ∗ |xi − xn|, so that aad(y ,n)= |c|*aad(x,n)."
647,1,"['sample', 'variance', 'sample variance']", VARIANCE AND STANDARD DEVIATION,seg_155,"using the notation introduced in section 1 (chapter 1), we define sample variance as"
648,1,"['variance', 'population']", VARIANCE AND STANDARD DEVIATION,seg_155,the variance of the population is defined as
649,1,"['sample size', 'sample', 'estimated', 'population mean', 'data', 'sample covariance', 'sum of squares', 'covariance', 'mean', 'population', 'variance']", VARIANCE AND STANDARD DEVIATION,seg_155,"where   is the population mean, n is the sample size, and n is the population size. being a sum of squares, s2 and  2 are always≥ 0. we will keep the divisor n in analogy with the sample covariance (see chapter 8) that uses n in the denominator although x and y are estimated from the data. in addition, the sample covariance should reduce to the variance when y′"
650,0,[], VARIANCE AND STANDARD DEVIATION,seg_155,i s are replaced by xi
651,1,"['recursive algorithm', 'variance', 'algorithm', 'observation']", VARIANCE AND STANDARD DEVIATION,seg_155,"′s (and y is replaced by x). moreover, in the recursive algorithm for variance defined below, we assume that the variance for n = 1 (a single observation) is zero. this assumption is invalid if (n−1) is used in the denominator."
652,0,[], VARIANCE AND STANDARD DEVIATION,seg_155,"by expanding the square, and summing the resulting terms individually, this could also be computed as"
653,1,['frequency'], VARIANCE AND STANDARD DEVIATION,seg_155,where n is to be interpreted appropriately (as n − 1 or n). this has a frequency version given by
654,1,"['deviation', 'standard', 'standard deviation', 'variance']", VARIANCE AND STANDARD DEVIATION,seg_155,"positive square-root of variance is called the standard deviation. many other formula are also available for the variance (see references 9, 30–32, which are more of theoretical interest than from a computational viewpoint."
655,1,"['deviation', 'sample', 'range', 'observations', 'data', 'distribution', 'samples', 'mean', 'population', 'sample variance', 'variance']", Advantages of Variance,seg_157,"the main advantages of variance are that (i) it uses all of the sample observations, (ii) it lends itself to further arithmetic operations, (iii) distribution of sample variance is known when the population distribution is known, and (iv) it can be found without data sorting. it is well defined for univariate as well as multivariate samples (but the range and mean absolute deviation are seldom used for multivariate samples or procedures)."
656,1,"['sample', 'variance', 'sample variance']", Advantages of Variance,seg_157,theorem 3.1 the sample variance can be recursively computed as s2
657,1,"['sample', 'variance', 'sample variance']", Advantages of Variance,seg_157,proof: we give a proof for the more general result for the unscaled variance. let n denote the denominator of the sample variance for n + 1 dof. then
658,1,['factor'], Advantages of Variance,seg_157,"1 (xi − xn+1)2∕n, where n is the scaling factor (either n or n + 1). split n+1 this into two terms to get"
659,1,['variable'], Advantages of Variance,seg_157,"expand as a quadratic and sum term by term. the first term becomes (n − 1)sn2. the product term reduces to zero using equation 2.4 (p. 46) of last chapter. as the second term does not involve the index variable i, summing it n times gives the second square"
660,1,['factor'], Advantages of Variance,seg_157,term as n(xn+1 − xn)2∕(n + 1). combine (1)+(2) and take n∕(n + 1)2(xn+1 − xn)2 as a common factor to get
661,0,['n'], Advantages of Variance,seg_157,divide throughout by n = n to get
662,1,['factor'], Advantages of Variance,seg_157,"n+1) as a scaling factor, the corresponding"
663,1,['variance'], Advantages of Variance,seg_157,recurrence for variance becomes (1 + 1
664,1,['variance'], Advantages of Variance,seg_157,example 3.2 variance of chlorine in drinking water
665,1,"['variance', 'locations']", Advantages of Variance,seg_157,"chlorine in drinking water at eight locations in ml/cc are [8, 17, 12, 13, 10]. find the variance using theorem 3.1."
666,1,"['sample', 'table', 'sample means', 'variance']", Advantages of Variance,seg_157,solution 3.2 table 3.2 gives various steps using both algorithms (that uses n in the denominator and (n − 1) in the denominator). the sample means (second column) need be computed until (n − 1)th row. the last entry in 3rd (resp 4th) column is the variance with n (resp. n − 1) in the denominator.
667,1,"['sample', 'combined sample', 'samples', 'variance', 'variances']", Advantages of Variance,seg_157,"2) are the arithmetic means and variances of two samples s1 and s2 of respective sizes n1 and n2, the variance of the combined sample of size n1 + n2 is given by"
668,1,"['sample', 'sample mean', 'combined sample', 'mean', 'variance']", Advantages of Variance,seg_157,"proof: by theorem 2.1, the combined sample mean is xc = (n1x1 + n2x2)∕(n1 + n2). the variance of the combined sample by definition is"
669,0,[], Advantages of Variance,seg_157,consider the expression ∑n
670,0,[], Advantages of Variance,seg_157,"+n2 (xi − xc)2. split this into two terms t1 and t2 over s1 and s2, respectively. substitute for xc in t1 to get"
671,1,['summation'], Advantages of Variance,seg_157,take (n1 + n2)2 outside the summation from the denominator.
672,1,['summation'], Advantages of Variance,seg_157,"add and subtract n2x1 inside the summation, combine −n1x1 and −n2x1 as −(n1 + 1 n2)x1, to get t1 = (n1+n2)2 ∑xi∈s1 ((n1 + n2)[xi − x1] + n2(x1 − x2))2. expanding the"
673,0,[], Advantages of Variance,seg_157,"as (x1 − x2) is a constant, the second expression becomes n1n2"
674,0,[], Advantages of Variance,seg_157,"2. thus, t1 simplifies to n1s1"
675,0,[], Advantages of Variance,seg_157,a similar reduction is possible for t2 by adding and subtracting n1x2 inside the
676,0,[], Advantages of Variance,seg_157,summation of t2. this gives us t2 = n2s2
677,1,['variance'], Advantages of Variance,seg_157,where n1 and n2 are the divisors used in the respective variance (n or n − 1).
678,1,['covariance'], Advantages of Variance,seg_157,corollary 1 the covariance of two subsamples can be combined using the relationship
679,1,"['covariance', 'mean', 'factors']", Advantages of Variance,seg_157,"where n1 and n2 are the scaling factors, covc1 is the covariance and (x1, y1) is the mean vector of the first subsample, and covc2 is the covariance and (x2, y2) is the mean vector of the second subsample."
680,1,"['transformed', 'data', 'variances']", Change of Origin and Scale Transformation for Variance,seg_159,"theorem 3.3 if data are transformed as y = (x − a)∕c, the variances are related as sy2 = (1∕c2)sx2."
681,0,[], Change of Origin and Scale Transformation for Variance,seg_159,proof: the means are clearly related as y = (x − a)∕c. consider ∑i
682,1,"['range', 'scale variant', 'location', 'sample', 'estimate', 'data', 'information', 'standard', 'standard deviation', 'location measure', 'variance', 'deviation', 'measurement', 'variability', 'variable', 'dispersion measures', 'dispersion']", Change of Origin and Scale Transformation for Variance,seg_159,"dispersion measures concisely quantify the amount of spread inherent in a sample. some dispersion measures use a location measure as a pivot to calculate the deviations (section 3.1.1). both the sample range and iqr do not use a pivot. hence, they can be used to get a preliminary estimate of the spread. because the dispersion measures explain the inherent variability in the sample data, those measures that utilize a location measure are preferred for engineering applications. all absolute spread measures depend on the unit of measurement of the variable. in other words, they are scale variant. however, there are some relative measures such as the cv that does not depend on the unit of measurement. this is summarized as several theorems below (section 3.6) in p. 82. the popular dispersion measures can be arranged according to the inherent information on the amount of spread captured by them from a sample as: range iqr aad variance. thus, the variance and standard deviation (positive square-root of variance) contain maximum spread information in a sample."
683,1,"['ordinal', 'precision', 'observations', 'algorithm', 'data', 'loss', 'standard', 'ordinal data', 'variance']", Disadvantages of Variance,seg_161,"the main disadvantages of variance are that (i) extreme observations on either side has a large influence on variance, (ii) it is inappropriate for numeric coded ordinal data, (iii) it can result in loss of precision due to squaring when large decimal numbers are involved, and (iv) it may require 2 passes through data (in standard algorithm) although it can be computed in a single pass."
684,1,"['deviation', 'sample', 'sample variance', 'range', 'sum of squares', 'standard', 'standard deviation', 'variance']", A Bound for Sample Standard Deviation,seg_163,"as the sample variance is a sum of squares, it is always ≥ 0. several researchers have come up with bounds for variance or standard deviation. mcleod and henderson [33–35] gave a lower bound for the standard deviation. shiffler and harsha [36] provided an upper bound for the standard deviation in terms of the range. for a sample of size ≥3, these bounds can be combined to get a bound for the standard deviation as"
685,1,"['sample', 'range']", A Bound for Sample Standard Deviation,seg_163,"where r is the sample range. note that the upper bound is not tight asymptotically. as n becomes large, the quantity (n∕(n − 1))1∕2 will converge quickly to 1,"
686,1,['variance'], A Bound for Sample Standard Deviation,seg_163,"2, giving s ≤ r/√ 2 = 0.70711 ∗ r. in fact, it can be shown that for n = 2 the equality holds, because the variance is (x1 − x2)2∕2. a sharper upper bound can be found in reference 37 as s ≤ r(1∕2 − (n − 2)∕[n(n − 1)])1∕2 for n ≥ 2. write (n − 2) as (n − 1) − 1 to get"
687,1,"['sample', 'standard', 'variables', 'population']", COEFFICIENT OF VARIATION,seg_165,"definition 3.5 the cv of a sample is a relative ratio-measure defined as (s∕x) × 100, and the corresponding population cv is ( ∕ ) × 100. this form of cv is called the standard form, and it applies to single variables only."
688,1,"['sample size', 'sample', 'confidence intervals', 'variability', 'change of scale', 'data', 'intervals', 'normality', 'samples', 'numerical', 'transformation', 'confidence', 'variance']", Advantages of Coefficient of Variation,seg_167,"the cv is simple to understand. it is a unit-less measure whose numerical value is high when data variance is high. if the variability of two samples measured in different units are to be compared, cv is the most appropriate measure. in other words, cv allows variability of heterogeneous samples to be compared among themselves. a change of scale transformation y = c ∗ x will not change the cv as the c will be canceled out from the numerator and denominator. it can be used to create confidence intervals. it provides caution on the sample size, normality, or departures from it."
689,1,"['sample', 'random', 'factor', 'random sample', 'variance']", Advantages of Coefficient of Variation,seg_167,"the cv of a random sample of size 1 is zero if n is used as a divisor in the variance, and the cv is undefined if n − 1 is used. symbolically, cv(x1) = 0 if n is used as scaling factor. for a sample of size 2, cv(x1, x2) = |x1 − x2|∕(x1 + x2) if n is used,"
690,1,['variance'], Advantages of Coefficient of Variation,seg_167,2|x1 − x2|∕(x1 + x2) if n − 1 is used in variance. when cv is used
691,1,['model'], Advantages of Coefficient of Variation,seg_167,"in the model setting, it can indicate which model better fits the data—the smaller the cv, the better the fit."
692,1,"['outliers', 'sample', 'variability', 'variables', 'arithmetic mean', 'results', 'change of origin', 'case', 'location', 'variable', 'samples', 'transformations', 'mean', 'transformation']", Disadvantages of Coefficient of Variation,seg_169,"as the denominator of cv contains x, the arithmetic mean should be nonzero. if a variable takes both positive and negative values and the mean is very close to zero, we cannot use the cv. thus it is most appropriate when variables are either strictly positive or strictly negative. this is a disadvantage. however, there is a catch. the cv is location variant. this means that a change of origin transformation y = x + c results in the same numerator sy, but the denominator is shifted by c units. when cv is used to compare the variability of two samples, at least one of which has zero mean, we could choose the c carefully in such a way that the resulting means are nonzero. alternatively, we could remove outliers from that sample which had a mean near zero with the hope of shifting it away from zero. it cannot easily be extended to the multivariate case. another disadvantage of cv is that it can result in misleading or conflicting interpretations under some transformations such as y = log(x)."
693,1,"['goodness of fit', 'dependent', 'dependent variable', 'estimate', 'samples', 'mean', 'parameter', 'standard', 'standard deviation', 'error', 'estimation', 'regression', 'variance', 'deviation', 'estimated', 'standard error', 'variable', 'mean square', 'mean square error', 'dispersion']", An Interpretation of Coefficient of Variation,seg_171,"as the cv is a ratio of standard deviation (which being the +ve square-root of variance is always positive) to the mean, its sign depends on the sign of x (if x is positive (negative), cv is positive (negative); if x is zero, cv is undefined). as the numerator and denominator are expressed in the same unit as the variate, the ratio is unit-less. thus, it summarizes the dispersion of a variable as a concise and unit-less real number. hence, it can be used across geographical boundaries, irrespective of the units in use. when two samples are being compared by the cv, a higher value may be due to a lower x. depending on whether x →0 from below or above, the cv will tend to ±∞ for fixed variance. the cv also has an interpretation in terms of the ratio of the root mean square error (rmse) to the mean of the dependent variable in regression models, and as the ratio of the standard error of an estimate to its estimated value [see ( ̂ )∕ ̂ ] in parameter estimation]. the smaller the cv, the better the goodness of fit, or the estimation procedure."
694,1,"['deviation', 'factors', 'factor', 'change of origin', 'case', 'change of scale', 'scores', 'standard', 'transformation', 'standard deviation', 'tests']", Change of Origin and Scale for CV,seg_173,"consider the change of scale transformation first. let y = c ∗ x, where c is nonzero. we know that sy = |c| ∗ sx and y = c ∗ x. as the scaling factor c factors out from both the numerator and denominator, the cv is scale invariant (cvy = cvx). this is a desirable characteristic in some applications such as cross-national comparison of traits or attributes such as income, profits, and expenses or cross comparison of scores in different tests such as gre and gmat. next, consider a change of origin transformation y = x + d. in this case the standard deviation remains the same, but"
695,1,"['coefficient', 'mean', 'variability']", Change of Origin and Scale for CV,seg_173,"the mean is shifted by d units. then, cvy = sy∕(x + d) = cvx ∗ x∕(x + d). thus, the coefficient of variability can be increased or reduced depending on the magnitude and sign of x∕(x + d)."
696,1,"['sample', 'variables', 'data', 'variable', 'statistic', 'mean']", GINI COEFFICIENT,seg_175,"gini’s mean difference is a summary statistic that measures the extent of the distribution of a variable by fixing other variables. for a sample (x1, x2, … , xn), it is defined for raw data as"
697,1,"['frequency', 'distributions']", GINI COEFFICIENT,seg_175,where k = n(n − 1) if j varies from i to n or k = n2 if j varies from 1 to n. for frequency distributions
698,1,"['coefficient', 'concentration', 'data']", GINI COEFFICIENT,seg_175,n =1 fi. this has the same unit as the data. a unit-less measure can be obtained by dividing gmd by x to get the gini coefficient of concentration as
699,1,"['data', 'skewed', 'skewed data', 'mean', 'dispersion']", GINI COEFFICIENT,seg_175,"as it is a mean difference (see equation 3.2), it can be used to measure dispersion of highly skewed data. as the numerator is always positive, gcc can take any positive value."
700,1,"['range', 'outlier', 'dispersion measure', 'sample', 'partitions', 'data', 'information', 'samples', 'mean', 'population', 'standard', 'standard deviation', 'coefficient', 'percentile', 'median', 'variance', 'deviation', 'consistency', 'dispersion measures', 'dispersion']", SUMMARY,seg_177,"popular measures of dispersion are discussed at length in this chapter. in a sense, the dispersion captures and summarizes data information. the lesser dispersion value refers to more consistency and hence reliability (see figure 3.1). the sample dispersion measure is a surrogate of the corresponding unknown population dispersion. commonly used dispersion measures are range, variance, qd, coefficient of variance, and absolute mean deviation (see figure 3.2). the range (xn − x1) is not indicative of the neighborhood of the data as the same range value could occur among smaller or larger values. hence, the sample dispersion is preferred over the sample range. the variance is of second degree, while the mean is of first degree. the cv (which is the ratio of the standard deviation over the mean) is often used if the measure of dispersion has to be compared across samples. when the data contain one or more unusual value (which is technically called outlier), the inter-qd is selected over the variance or range. the 50th percentile (which is technically called the median) partitions the ordered data into two equal segments. the inter-qd portrays the range between the"
701,1,"['deviation', 'dispersion', 'data', 'skewed', 'percentiles', 'tails']", SUMMARY,seg_177,"25th and 75th percentiles. when the data have outlier(s), the inter-qd is preferable over other measures of dispersion. alternatively, the measure of absolute deviation is exercised when the data are skewed or have thick tails."
702,1,"['poisson', 'evaluating', 'data', 'symmetry', 'variance', 'dispersion']", SUMMARY,seg_177,"every data analyst who works with numeric data will find the discussion very illuminating and easy to follow. updating formulas that comes handy when new data arrive are presented and illustrated. a discussion of algorithms for variance can be found in references 30–32, 38–41. a measure of variance for hierarchical data appears in reference 42. evaluating methods of symmetry are discussed in references 43–45, and poisson dispersion in reference 46."
703,1,"['data', 'dispersion']", SUMMARY,seg_177,a) dispersion is a measure of data spread
704,1,"['quartile', 'third quartile']", SUMMARY,seg_177,b) third quartile lies between 7th and 8th decile
705,1,"['sample', 'range', 'skewed distributions', 'skewed', 'distributions']", SUMMARY,seg_177,c) sample range can distinguish between skewed distributions
706,1,"['sample', 'range', 'observation']", SUMMARY,seg_177,d) every sample observation contributes to the range
707,1,"['sample', 'range', 'data']", SUMMARY,seg_177,e) range of a sample can be negative when all data values are negative
708,1,['sample'], SUMMARY,seg_177,f) a sample with large cv is less dispersed than a sample with small cv
709,1,"['measurement', 'data']", SUMMARY,seg_177,g) cv measures the data spread irrespective of the unit of measurement.
710,1,"['sample', 'range']", SUMMARY,seg_177,3.2 what is the sample range when all 3.10 prove that s ≤ r ( 1
711,1,"['sample', 'observations']", SUMMARY,seg_177,"2 − 1 n )1∕2 for sample observations are the same large n, where s is the sample stan-"
712,1,"['range', 'deviation']", SUMMARY,seg_177,(say k)? dard deviation and r is the range.
713,1,"['location measure', 'location']", SUMMARY,seg_177,(a) 0 (b) k (c) ∞ (d) 1 3.11 why is it that a location measure
714,0,[], SUMMARY,seg_177,3.3 which of the measure additively alone is insufficient to fully under-
715,1,"['distribution', 'sample', 'data']", SUMMARY,seg_177,combines the squared sample valstand a data distribution?
716,1,"['deviations', 'mean']", SUMMARY,seg_177,ues? (a) mean absolute deviations 3.12 a city has five weight-loss clinics.
717,1,"['sample', 'range']", SUMMARY,seg_177,(b) sample range (c) inter-quartile each one uses a different diet and
718,1,['variance'], SUMMARY,seg_177,range (d) variance exercise program. (i) an executive
719,0,[], SUMMARY,seg_177,who wishes to shed the maximum 3.4 what are the popular categoriza-
720,1,"['dispersion measures', 'dispersion']", SUMMARY,seg_177,body fat in shortest time should tion of dispersion measures? prefer a weight-loss program hav3.5 what are some desirable qualities ing
721,1,"['variance', 'dispersion']", SUMMARY,seg_177,of a good measure of dispersion? (a) low variance (b) high variance
722,1,['skew'], SUMMARY,seg_177,(c) negative skew (d) platykurtic 3.6 for which of the following mea-
723,1,['change of origin'], SUMMARY,seg_177,(ii) a housewife who has 15 sures is the change of origin techdays available should prefer
724,1,"['sample', 'range', 'mean']", SUMMARY,seg_177,nique useful? (a) mean absolute a weight-loss program havdeviations (b) sample range (c) ing______
725,1,"['range', 'variance', 'mean']", SUMMARY,seg_177,inter-quartile range (d) variance (a) high mean  15 (b) low vari-
726,1,"['sample', 'mean']", SUMMARY,seg_177,3.7 if a sample s is split into two subance (c) low mean and high vari-
727,1,"['range', 'variance', 'mean']", SUMMARY,seg_177,"samples, can you find the range of ance (d) high mean and low s if the range of the subsamples variance."
728,0,[], SUMMARY,seg_177,information is needed?   u   1 denotes the inverse of
729,1,['measurement'], SUMMARY,seg_177,"3.8 identify the unit of measurement the cdf, prove that a sym-"
730,1,['distribution'], SUMMARY,seg_177,in each of the following statismetric distribution can be char-
731,1,"['condition', 'symmetric', 'geometric mean', 'statistic', 'measurements', 'mean', 'geometric']", SUMMARY,seg_177,"tics: (i) s2n∕xn (ii) am/gm, where acterized by the condition = f−1(u) + f−1(1 − u) = 0 for am= xn, gm = geometric mean u ∈ (0, .5). prove that such dis(iii) 4∕ 2 if all measurements are tributions are symmetric around in centimeters. 0 if f−1(1∕2) = 0 and symmetric 3.9 consider a statistic defined as sdn = around if f−1(1∕2) = . prove"
732,1,['skewed'], SUMMARY,seg_177,"1 dj 2, where dj = xj+1 − xj that     0 for positively skewed"
733,1,"['observations', 'skewed', 'frequency', 'distributions']", SUMMARY,seg_177,denotes the difference between and     0 for negatively skewed successive ordered observations. distributions. can you compute when is it minimum? what does this measure for frequency dis-
734,1,['data'], SUMMARY,seg_177,it measure? tributed data?
735,1,['change of origin'], SUMMARY,seg_177,"3.14 can the qd be ever zero? if so, 3.23 how does the change of origin"
736,1,"['sample', 'data', 'transformation']", SUMMARY,seg_177,for what type of data? what is its and scale transformation affect the interpretation? sample aad?
737,1,['quantile'], SUMMARY,seg_177,3.15 describe the unit quantile func3.24 consider a measure defined as
738,1,['variance'], SUMMARY,seg_177,"4 =1(qi − qi−1), where tion. how is it useful to measure q0 = x(1) is the first-order statissample variance?"
739,0,[], SUMMARY,seg_177,"3.16 critically examine the different tic, q4 = x(n) and other qi"
740,1,"['quantile', 'parameter', 'variation', 'weibull', 'dispersion']", SUMMARY,seg_177,"quartiles. can it be used to meameasures for variation, and indisure the dispersion? what does cate their advantages and disadhigh values indicate? vantages. 3.25 prove that the unit quantile func3.17 in what situations is the recurtion of one parameter weibull"
741,1,"['sample', 'algorithm', 'distribution', 'sample variance', 'variance']", SUMMARY,seg_177,"sive algorithm for sample variance distribution with cdf f(x) = helpful over the iterative version? −xa 1 − e , x, a   0 is q(u) = [1 − log (1 − u)]1∕a. show that 3.18 if there are two items (x1, x2) in a"
742,1,"['sample', 'mean', 'median']", SUMMARY,seg_177,"the median is log (2)1∕a. sample, prove that the mean abso-"
743,1,"['deviation', 'sample', 'median', 'range', 'data', 'mean']", SUMMARY,seg_177,lute deviation around the mean 3.26 when is the sample range meanand median are both equal to half ingful for numerically coded ordithe sample range. nal data?
744,1,"['sample', 'sample mean', 'information', 'mean', 'variance']", SUMMARY,seg_177,3.19 if the sample mean and variance of 3.27 what information is needed to
745,1,"['sample', 'independent', 'range', 'data', 'samples']", SUMMARY,seg_177,"two independent samples of sizes update the sample range using new 8 and 10 are (65, 20) and (70, 32), data?"
746,1,"['variance', 'percentage']", SUMMARY,seg_177,find the variance of the combined 3.28 the percentage of seeds that ger-
747,1,['plots'], SUMMARY,seg_177,"sample. minate from eight different plots 3.20 what are the possible ranges of are as follows: {98.2, 92.7, 89.3,"
748,1,"['range', 'coefficient', 'variability']", SUMMARY,seg_177,"values for cv? how is it help94.4, 95.0, 83.1, 90.6, 96.1}. find ful in comparing the variability of the coefficient of range cr."
749,1,"['quartile', 'samples', 'coefficient']", SUMMARY,seg_177,different samples? 3.29 if a quartile coefficient is defined
750,1,"['sample', 'standard']", SUMMARY,seg_177,"3.21 how is the sample standard devias qc = (q3 − q1)∕(q3 + q1),"
751,1,"['asymptotic', 'range', 'inequality']", SUMMARY,seg_177,ation related to the range? what is what are the possible values? is the asymptotic inequality between it absolute or relative measure?
752,1,"['sample size', 'sample']", SUMMARY,seg_177,them when sample size n is large? 3.30 consider a measure defined as
753,1,['median'], SUMMARY,seg_177,3.22 if the origin and scale are trans[(q1 − p − median) − (median −
754,1,['percentile'], SUMMARY,seg_177,"formed as y = c ∗ x + d, where qp)]∕[q1 − p − qp], where p is a c ≠ 0, prove that cvy = cvx ∗ percentile. what does it measure? x∕(x + d∕c). what are the possible values?"
755,0,[], SKEWNESS AND KURTOSIS,seg_179,"after finishing the chapter, students will be able to"
756,1,['skewness'], SKEWNESS AND KURTOSIS,seg_179,◾ describe measures of skewness
757,0,[], SKEWNESS AND KURTOSIS,seg_179,◾ understand absolute and relative measures
758,0,[], SKEWNESS AND KURTOSIS,seg_179,"◾ comprehend galton’s, pearson’s, bowley’s, and kelly’s measures"
759,1,['kurtosis'], SKEWNESS AND KURTOSIS,seg_179,◾ interpret pearson’s and stavig’s kurtosis measures
760,0,[], SKEWNESS AND KURTOSIS,seg_179,◾ describe l-kurtosis
761,1,['kurtosis'], SKEWNESS AND KURTOSIS,seg_179,◾ understand spectral kurtosis
762,1,['bias'], MEANING OF SKEWNESS,seg_181,"the literal meaning of “skew” is a bias, dragging, or distortion toward some particular value, group, subjects, or direction."
763,1,"['unimodal', 'distribution', 'skewness']", MEANING OF SKEWNESS,seg_181,definition 4.1 a measure of skewness is a numeric metric to concisely summarize the degree of asymmetry of a unimodal distribution that can be compared with other similar numbers.
764,1,"['correlation coefficients', 'sample', 'coefficients', 'table', 'cauchy', 'correlation', 'distribution', 'asymmetric', 'normal', 'statistical', 'distributions']", MEANING OF SKEWNESS,seg_181,"a great majority of statistical distributions are asymmetric (see table 4.1). nevertheless, symmetrical laws such as the normal, student’s t, laplace, cauchy distributions, and the distribution of sample correlation coefficients are more popular."
765,1,"['contrast', 'unimodal', 'data', 'location', 'asymmetric', 'tail', 'bimodal', 'distributions']", MEANING OF SKEWNESS,seg_181,asymmetry of unimodal distributions can be due to a flat left tail or right tail. two asymmetric mirror-image distributions (around the mode) could have exactly the same location and spread. a quantified measure of this asymmetry is needed to compare and contrast such distributions (see reference 47 for bimodal data).
766,1,"['outliers', 'change of origin', 'location', 'transformation', 'data', 'information', 'samples', 'skewness', 'distributions', 'skewed', 'distribution', 'asymmetric', 'location measures', 'normality', 'tail']", MEANING OF SKEWNESS,seg_181,"a change of origin transformation can be used to align the location measures of two distributions (or samples). then, we could compare the spreads of the two distributions. these two measures are insufficient to fully understand the data. two distributions could have the same location and spread, but one could tail-off slowly to the right, whereas the other could tail off slowly to the left. in other words, two asymmetric mirror-image distributions (around the mode) could have exactly the same location and spread. this is exactly the reason for studying skewness [48]. this information is useful to fit empirical distributions and in parametric analysis. in addition, possible outliers under the assumption of normality of data may turn out to be nonoutliers when the data distribution is actually skewed."
767,1,"['failures', 'unimodal', 'skewed', 'skewed distribution', 'distribution', 'symmetry', 'set', 'scores', 'population', 'outcome']", MEANING OF SKEWNESS,seg_181,"a skewed distribution may be a desired outcome in some domains. as an example, instructors in several educational institutions set the exam questions in such a way that the resulting scores reflect symmetry (the mark distribution is often unimodal with a great majority of the class having marks in the neighborhood of the mode, and a few students with scores in both the extremes—a few failures and more or less an equal number of distinctions. owing to the heterogeneity of student population and different learning habits, a perfect bell shape is seldom achieved. even if the instructor had a bell-shaped distribution in mind while setting the exam questions, it could end up as a skewed distribution. a mark distribution skewed to the left indicates an easy exam (or a question paper leak or mass copying), whereas a distribution skewed to the right indicates a difficult exam."
768,1,"['uniform distribution', 'unimodal', 'statistics', 'data', 'discrete', 'distribution', 'symmetry', 'continuous', 'distributions']", MEANING OF SKEWNESS,seg_181,"asymmetry is the opposite of symmetry. it is literally applied to physical structures, arrangements, formations, and so on. examples are buildings, walls, gates, paintings and pictures, arrangement of flowers and beads, formation of groups of people or ships, and so on around an axis. in statistics, it is used to describe data distributions around a central value. it is more meaningful for unimodal distributions. the uniform distribution (both discrete and continuous) exhibits a special type of symmetry. note that the symmetry is always measured with respect to the x-axis (most software packages record the variate values along the abscissa and"
769,1,"['class interval', 'interval', 'unimodal', 'case', 'skewed', 'location', 'distribution', 'data', 'discrete', 'symmetry', 'skewness', 'tail', 'location measures']", MEANING OF SKEWNESS,seg_181,"frequencies along the y-axis). the following discussion is more pertinent to continuous data than discrete ones. with this convention, positive skewness (or skewed to the right) indicates a heavy and long-extending tail on the right side of the location alignment. similarly, a negative skewness (or skewed to the left) indicates a heavy and long-extending left tail. in other words, a disproportionate amount of data falls on the left or right side of a unimodal distribution, thereby dragging or dispersing the location measures. this will be apparent only when the data size is large, and the spread is small. consider the systolic bp of some patients. if the number of patients is small (say 12), the values may all be distinct, in which case the mode is not unique. therefore, we may have to use grouping of data (using a class interval) to find symmetry."
770,1,"['sample size', 'sample', 'error', 'median', 'symmetric', 'skewed', 'data', 'sampling', 'distribution', 'skewed distribution', 'errors', 'mean', 'symmetric distribution']", MEANING OF SKEWNESS,seg_181,"the mean, median, and mode coincide for symmetric laws. in addition, the quartiles are equally distant from the median. however, for negatively skewed distribution, the mean is less than the median and mode in general (although exceptions do exist). for positively skewed distribution, the mean is greater than the median and median is greater than the mode in general. thus, the difference between these measures can tell us if the data are symmetric or skewed [12]. it is important to remember that a skewed sample may come from a symmetric distribution and vice versa because of sampling errors. this error is minimal when the sample size is large."
771,1,"['skewed', 'distribution', 'level', 'tests']", MEANING OF SKEWNESS,seg_181,"if the distribution of marks in a series of tests of the same difficulty level moves from a positively skewed shape at the beginning to a negatively skewed shape at the end of the course, it is an indication that the teaching was effective and student participation was good."
772,1,"['poisson', 'table', 'uniform distribution', 'symmetric', 'symmetric distributions', 'discrete', 'distribution', 'symmetry', 'poisson distribution', 'binomial', 'continuous', 'binomial distribution', 'discrete uniform distribution', 'distributions']", MEANING OF SKEWNESS,seg_181,"from table 4.1, it is evident that most of the symmetric distributions are continuous. an exception is the discrete uniform distribution, which exhibits a special type of symmetry. the binomial distribution is symmetric when p = 0.5, and the poisson distribution (chapter 6, p. 6–67) approaches symmetry for very large   values."
773,1,"['sample', 'measurement', 'median', 'arithmetic mean', 'symmetric', 'skewed distributions', 'skewed', 'data', 'samples', 'mean', 'skewness', 'average', 'distributions']", Absolute Versus Relative Measures of Skewness,seg_183,"absolute measures express the skewness in terms of the unit of measurement. as the arithmetic mean is greater than mode for positively skewed distributions, d = (mean-mode) provides a quick check to see if data are skewed. this is an absolute measure because it uses the same unit as the data. similarly (q3 + q1) − 2 ∗ m where m is the median is zero for symmetric laws and 0 for positively skewed distributions. a disadvantage of these measures is that they cannot be compared across samples. the unit can be canceled by dividing the quantity with another quantity computed from the same sample that has the same degree. for instance, bowley’s measure is obtained by dividing (q3 + q1) − 2 ∗ m by (q3 − q1). this measure is zero in two situations:–(i) when data are symmetric, the average (q3 + q1)∕2 coincides with the median m, so that the numerator is zero (ii) when all sample values are the same"
774,1,['numerical'], Absolute Versus Relative Measures of Skewness,seg_183,"(q3,q1), and m are equal. as the denominator is much greater than the numerator, the numerical value of this measure is much less than 1."
775,1,"['kurtosis', 'skewed distribution', 'probability', 'standard normal distribution', 'skew', 'sample', 'symmetric', 'data', 'standard', 'skewness', 'error', 'distributions', 'standard normal', 'skewed', 'distribution', 'absolute value', 'standard error', 'normal', 'tail', 'tails', 'normal distribution']", Absolute Versus Relative Measures of Skewness,seg_183,"how much skewed is a skew distribution? we need a yardstick to measure the amount of departure from an otherwise symmetric law. the original skewness measures used the standard normal distribution n(0, 1) as the yardstick. to quantify the amount of skewness, we could consider the standard error of skewness (ses) and measure the departure from twice the standard error. an approximate ses for a sample of size n is √6∕n (see reference 49. if 2*ses absolute value of skewness, we may reasonably conclude that the data are from a skewed distribution. we could also distinguish if the data are skewed to the left (skewness 0) or to the right (skewness 0). skewness can be categorized based on the shape of both tails. a distribution which is skewed to the right may have a left tail either below a standard normal left tail, above it, partially below and partially above it, or align exactly with the normal. this gives rise to lepto-right-skewed, meso-right-skewed and platy-right-skewed distributions (where lepto-, mesoand platyindicate the behavior of the left tail of a right-skewed distribution). the difference is subtle but important. analogous definition holds for left-skewed distribution whose right tail may be unaligned with a normal right tail. as defined in the following, the kurtosis measures the relative concentration or amassment of probability mass toward the center (peak) of a distribution. hence, these peculiarities are of interest among itself than from a kurtosis view point."
776,1,"['symmetric', 'kurtosis', 'distribution', 'normality', 'normal', 'moments', 'cumulant', 'skewness']", Absolute Versus Relative Measures of Skewness,seg_183,note that the third-order central moments (in the numerator of some skewness measures) vanish not only for the normal law but also for any symmetric distribution. whether the fourth cumulant vanish or not depends on the distribution. both the skewness and kurtosis are measures of shape departures from normality.
777,1,"['sample', 'symmetry', 'skewness']", Absolute Versus Relative Measures of Skewness,seg_183,definition 4.2 skewness is a numeric measure of the degree of departure of a sample of size n   2 from symmetry.
778,1,"['sample', 'unimodal', 'skewed', 'location', 'distribution', 'skewed distribution', 'population', 'tails']", Absolute Versus Relative Measures of Skewness,seg_183,"the above-mentioned definition pertains to the sample, although it is defined for both the sample and the population. if a unimodal skewed distribution is superimposed on a unimodal bell-shaped distribution so as to align the peaked points exactly (location alignment), we could visualize various possible types of asymmetry in the left and right tails. a positively (respectively negatively) skewed distribution has longer tails on the right (left) side of the location alignment."
779,1,['skewness'], Absolute Versus Relative Measures of Skewness,seg_183,example 4.1 check the skewness of marks
780,1,"['distribution', 'skewness', 'skewed']", Absolute Versus Relative Measures of Skewness,seg_183,the mark distribution of 60 students of a class had a skewness of −0.65 at the start of the semester and +0.75 at the end. are the marks significantly skewed?
781,1,"['skewed', 'skewness', 'distributions']", Absolute Versus Relative Measures of Skewness,seg_183,"solution 4.1 here n = 60, so that 6∕n = 0.10, ses = 6∕n1∕2 = 0.316227766, and 2*ses = 0.632455532. as the absolute values of skewness are both greater than 2*ses, we conclude that both distributions are skewed."
782,1,"['median', 'location', 'mean', 'location measures']", Absolute Versus Relative Measures of Skewness,seg_183,"skewness was originally defined as a measure of asymmetry. all three location measures (the mean, median, and mode) together can throw some insight on the"
783,1,"['median', 'skewed distributions', 'symmetric', 'symmetric distributions', 'skewed', 'data', 'distribution', 'mean', 'skewness', 'variance', 'distributions']", Absolute Versus Relative Measures of Skewness,seg_183,"skewness of a data distribution. in general, mean   median   mode for negatively skewed distributions, mean = median = mode for symmetric distributions, mean   median   mode for positively skewed distributions. the variance could be exactly identical for positively and negatively skewed distributions. we need a measure of skewness to distinguish between possible asymmetries. there are many measures for this purpose [50]."
784,1,"['sample', 'range', 'location', 'variance', 'dispersion']", CATEGORIZATION OF SKEWNESS MEASURES,seg_185,"1. location and scale-based measures these measures combine the location and scale measures (in the numerator and denominator, respectively) to get a unit-less measure of dispersion. one popular example is pearson’s   measure [51] for a sample defined as   = (xn − mode)∕sn. as the mode can be on the left or right of xn, the numerator can be positive, negative, or zero. the denominator being the positive square-root of variance is always positive. hence,   can take any real value. as xn and mode both lie in between the minimum and maximum,   is bounded by the sample range."
785,1,"['quartiles', 'median', 'distribution', 'percentiles', 'deviations']", CATEGORIZATION OF SKEWNESS MEASURES,seg_185,"2. quartile-based measures these measures utilize the quartiles of a distribution. the popular examples are bowley’s, hinkley’s, and kelley’s measures discussed in the following text. another measure that utilizes the averaged deviations of percentiles from the median using a cutoff threshold can be found in reference 43."
786,1,"['sample', 'standard', 'standard deviations', 'moment', 'deviations', 'population', 'skewness']", CATEGORIZATION OF SKEWNESS MEASURES,seg_185,"3. moment-based measures these utilize the third central moment as   =  3∕ 3 for the population. they are defined for both the population and the sample. as the denominator is the cube of the standard deviations, they are also unit-less and scale invariant, that is, x and y = |c| ∗ x have the same skewness for nonzero c ∈ r."
787,1,"['plot', 'functions', 'condition', 'unimodal', 'symmetric', 'skewed distributions', 'case', 'skewed', 'distribution', 'function', 'skewness', 'distributions']", CATEGORIZATION OF SKEWNESS MEASURES,seg_185,"4. measures that utilize inverse of distribution functions these measures use the inverse of theoretical distribution functions. one example is the spread function. let f−1(u) = sup(x∶f(x) ≤ u), 0   u   1 denotes the inverse of the cdf. then, a symmetric zero-centered distribution can be characterized by the condition   = f−1(u) + f−1(1 − u) = 0 for u ∈ (0, .5). such distributions are symmetric around 0 if f−1(1∕2) = 0, and symmetric around   if f−1(1∕2) =  , in which case 2  −   = 0. note that     0 for positively skewed and     0 for negatively skewed distributions. balanda and macgillivray [52] used sf(u) = f−1(0.5 + u) − f−1(0.5 − u) for 0 ≤ u ≤ 0.5 as a measure of skewness. if both f and g are unimodal and invertible, one could produce a plot of sf(u)∕sg(u) (or its inverse) to compare the relative skewness."
788,1,"['linear', 'linear combination', 'combination', 'statistics', 'expected value', 'variable', 'random variable', 'order statistics', 'mean', 'random']", CATEGORIZATION OF SKEWNESS MEASURES,seg_185,"5. measures that utilize l-moments for a real-valued random variable x with finite mean  , the l-moment is defined as expected value of linear combination of order statistics."
789,1,"['sample', 'linear', 'linear combination', 'combination', 'range', 'dispersion', 'statistics', 'order statistics', 'statistic', 'mean', 'expected values', 'order statistic']", CATEGORIZATION OF SKEWNESS MEASURES,seg_185,"the first l-moment is the same as the mean, so that  1 = e(x) =  . the second l-moment  2 = 0.5 ∗ [e(x(2∶2) − x(1∶2))] where x(i∶n) for i   n denotes the ith order statistic of a sample of size n. this is e(half range). define  3 = (1∕3) ∗ e(x(3∶3) − 2 ∗ x(2∶3) + x(1∶3)). the l-skewness is then defined as  3 =  3∕ 2 [9, 53]. as  3 and  2 are expected values of linear combination of order statistics,  3 is unit-less. analogously,  2 =  2∕ 1 can be considered as a measure of dispersion."
790,1,"['levels', 'integer part', 'case', 'symmetric distributions', 'frequency', 'sample', 'quartiles', 'symmetric', 'data', 'percentiles', 'quartile', 'distributions', 'sample size', 'percentile', 'median']", CATEGORIZATION OF SKEWNESS MEASURES,seg_185,"the quartiles, deciles, and percentiles are symmetrically located from the median for all symmetric distributions. while deciles divide the total frequency (or area) into 10 equal parts, percentiles (%-tiles) divide it into 100 equal parts. we could go to finer levels into one thousand equal parts, and so on. however, these are seldom popular owing to the simple reason that sample sizes are most often 1000. if the sample size is  100, even the percentiles are not used as several percentiles could coincide. deciles are special %-tiles, as also quartiles are special deciles. for example, q2 = d5 = p50, where q, d, p stand for quartile, decile, and percentile, respectively. in the following discussion, k ∈ {1, 2, 3}, j ∈ {1, 2, .., 9}. the general formula to convert from quartile into decile is qk = d(10∕4)∗k = d(5∕2)∗k, provided (5∕2) ∗ k is an integer. the reverse relationship is dj = q(2∕5)∗j, provided (2∕5) ∗ j is an integer (which happens to be the case for j = 5). similarly dj = p10∗j or its reverse pi = di∕10 if i∕10 is an integer. a similar relation between quartiles and percentiles is qk = p25∗k or pi = q⌈i∕25⌉. as q⌈i∕25⌉ returns the integer part (ceil operator), several percentiles can get mapped to the same quartile when the data size is small."
791,1,"['sample size', 'sample', 'case', 'data', 'population', 'skewness']", MEASURES OF SKEWNESS,seg_187,there are many skewness measures available. they can be applied to the population or sample. sample skewness is more important for data analysts and engineers. most of the measures discussed in the following are sample measures. it is assumed that the sample size n is sufficiently large for the expressions involved to be meaningful. the population analogs are denoted by greek letters and their sample counterparts by lower case english letters by convention.
792,1,"['lack of symmetry', 'median', 'skewness coefficient', 'coefficient', 'symmetric', 'location', 'distribution', 'symmetry', 'percentiles', 'skewness']", Bowleys Skewness Measure,seg_189,"as skewness measures the lack of symmetry, several measures can be defined by utilizing their location relative to the distance from the median. if the distribution is symmetric, (m − q1) and (q3 − m) are equal. in general, (m − pk) and (p100−k − m) are equal where pk denotes the percentiles and p50 = m is the median. this property has been utilized by harremoës [54], macgillivray [55]. the simplest one is due to bowley [56], who defined a skewness coefficient as"
793,1,"['quartiles', 'median', 'symmetric', 'discrete', 'errors', 'discrete distributions', 'percentiles', 'continuous distributions', 'continuous', 'distributions']", Bowleys Skewness Measure,seg_189,"where q1 and q3 are the lower and upper quartiles and m is the median. as the percentiles are equally distant from the median, the above-mentioned measure is zero for symmetric continuous distributions because the numerator is zero (this may not hold for discrete distributions owing to round-off errors)."
794,1,['population'], Bowleys Skewness Measure,seg_189,the corresponding population analog is easily expressed in terms of the cdf as
795,1,"['percentile', 'function', 'skewness']", Bowleys Skewness Measure,seg_189,"this measure was generalized in reference 57, who parametrizes it in terms of an arbitrary percentile u as skewness function"
796,1,"['normalized', 'range', 'discrete', 'percentiles', 'populations', 'continuous']", Bowleys Skewness Measure,seg_189,"where u is normalized to the range [0,1]. this is easy to compute for continuous populations (for discrete populations, the percentiles may not align exactly on variate values). the above-mentioned measure is negative for u   1"
797,0,[], Bowleys Skewness Measure,seg_189,2 (as the denominator
798,1,['skewness'], Bowleys Skewness Measure,seg_189,is −ve). it coincides with galton skewness measure for u = 3∕4. subtract and add f−1(1 − u) in the numerator and simplify to get the alternative form
799,1,['range'], Bowleys Skewness Measure,seg_189,example 4.2 range of values for bowley’s measure
800,1,"['skewness', 'interval']", Bowleys Skewness Measure,seg_189,"prove that bowley’s skewness measure lies in the interval [−1,+1]."
801,1,['skewed'], Bowleys Skewness Measure,seg_189,"solution 4.2 consider bs = (q3 + q1 − 2m)∕(q3 − q1). when m = q1 (highly positively skewed), the numerator simplifies as (q3 + q1 − 2 ∗ q1) = (q3 − q1). this cancels out with the denominator giving bs = 1. similarly when m = q3 (highly negatively skewed), the numerator becomes q1 − q3 = −(q3 − q1), giving bs = −1. add and subtract q1 in the denominator to get bs = (q3 + q1 − 2m)∕(q1 + q3 − 2q1). divide both the numerator and the denominator by 2 to get"
802,1,"['cases', 'absolute value', 'mean']", Bowleys Skewness Measure,seg_189,"as q1 ≤ q2 ≤ q3, the mean (q1 + q3)∕2 must lie in between them. hence, the absolute value of the numerator must be less than absolute value of the denominator. this means that |bs| 1 for all other cases. hence, bs ∈ [−1,+1]."
803,1,['skewness'], Bowleys Skewness Measure,seg_189,example 4.3 bmi skewness calculation
804,1,"['table', 'skewness coefficient', 'coefficient', 'skewness']", Bowleys Skewness Measure,seg_189,the bmi of 30 patients is given in first two columns of table 4.2. compute galton’s skewness coefficient.
805,1,"['quartiles', 'table', 'cumulative frequency', 'skewed', 'data', 'quartile', 'frequency', 'limit']", Bowleys Skewness Measure,seg_189,"solution 4.3 here n = 30, so that n/4 = 7.5. hence, q1 is that value below which one-fourth of the data lie. from the sorted column (3), we find that the seventh and eighth values are both 23.3. hence q1 = 23.30. next, q3 is that value below which three-fourth of the data lie (or equivalently above which one-fourth of the data lie). from the last column, we see that eight patients have bmi ≥ 27.4 and there are seven patients with bmi ≥ 28.4. hence q3 = 27.9. if data are grouped using a class width of 3, we get table 4.3. using the formula for quartiles as qk = l + (n ∗ k∕4 − m) ∗ c∕f where n = 30, l = lower limit of quartile class, k = 1 for q1 and 3 for q3,m is the cumulative frequency up to (but excluding) quartile class, c is class width ( = 3) and f is the frequency of quartile class, we get q1 = 23.5 and q3 = 27.25. as m = 24.45, (q3 + q1 − 2m)∕(q3 − q1) = (50.75 − 48.9)∕3.75 = 0.4933, showing that the data are skewed to the right (table 4.4)."
806,1,"['quantile', 'cumulative frequency', 'data', 'quartile', 'frequency', 'limit']", Bowleys Skewness Measure,seg_189,"as the kth quartile uses the lower limit of the class where qk falls, frequency of that class and cumulative frequency up to the class, there is no need to sort the complete data. fast methods exist to find any quantile when the data size is very large and one of them needs to be computed."
807,1,"['range', 'statistics', 'symmetric distributions', 'location', 'sample', 'symmetric', 'data', 'samples', 'standard', 'populations', 'standard deviation', 'skewness', 'distributions', 'skewed', 'expected value', 'location measures', 'deviation', 'skewed distributions']", Pearsons Skewness Measure,seg_191,"pearson’s measure of sample skewness was introduced in section 1. it is a ratiomeasure defined as = (xn − mode)∕sn. nearly, bell-shaped distributions satisfy an approximate relationship (x-mode) ∼ 3*(x-median). this allows us to express the above as = 3*(x-median)/s. this is more meaningful, as the mode of a sample need not be unique. as the numerator is the difference between two location measures, can be positive or negative. as it is divided by the standard deviation, it is unit-less. this measure returns 0 for symmetric distributions. it is 0 for negatively skewed distributions. the expected value of these statistics tends to zero when samples come from large symmetric populations. for most data, it will lie in the range (−3,+3)."
808,0,[], Pearsons Skewness Measure,seg_191,"pearson’s   = (x − mode)∕s ≃ 3 ∗ (x − median)∕s ∈ (−3,+3). (4.6)"
809,1,"['normalized', 'moment', 'variable', 'random variable', 'random', 'skewness']", Pearsons Skewness Measure,seg_191,pearson also suggested another measure of skewness in terms of third moment of a unit normalized random variable as
810,1,"['standard normal', 'normal', 'moments', 'standard', 'skewness', 'tail']", Pearsons Skewness Measure,seg_191,"this can be expressed in terms of moments as 3∕ 3. as the standard normal distribution has skewness zero, positive values of skewness indicates a flat right tail and vice versa. its square 1 = 1"
811,1,"['unimodal', 'symmetric', 'symmetric distributions', 'expected value', 'moments', 'distributions']", Pearsons Skewness Measure,seg_191,"2 is sometimes used, under the assumption of the existence of finite second and third moments. as the numerator contains a centralized measure (with expected value zero for symmetric distributions), this measure is location invariant for unimodal distributions. as the denominator contains quantities in the same unit, it is unit-less. as the orders of the numerator and the denominator are the same, the measure is scale invariant too."
812,1,['skewness'], Pearsons Skewness Measure,seg_191,example 4.4 pearson’s skewness calculation
813,1,"['table', 'data', 'skewness']", Pearsons Skewness Measure,seg_191,compute pearson’s skewness for the data in table 4.2.
814,1,"['data', 'skewed', 'median']", Pearsons Skewness Measure,seg_191,"solution 4.4 we find x = 25.49667, and s = 3.4455852, median m = 24.45. substitute these values to get   = 3 ∗ (25.49667 − 24.45)∕3.44558 = 3.14∕3.44558 = 0.9113, showing that the data are skewed to the right."
815,1,['distribution'], Pearsons Skewness Measure,seg_191,example 4.5 is marks distribution bell-shaped?
816,1,"['deviation', 'median', 'symmetric', 'skewed', 'distribution', 'mean', 'standard', 'standard deviation']", Pearsons Skewness Measure,seg_191,"the marks obtained by students in an exam have mean 70 and median 72, with a standard deviation of 8. is the distribution of marks symmetric? if not, is it skewed to the left or right?"
817,1,"['skewed', 'distribution', 'asymmetric', 'skewness']", Pearsons Skewness Measure,seg_191,"solution 4.5 we find pearson’s measure of skewness as   = 3*(x-median)/s = 3*(70 − 72)/8 = −6∕8 = −0.75. as this is  0, the distribution is asymmetric and is negatively skewed."
818,1,"['statistics', 'sample statistics', 'vary', 'sample', 'samples', 'populations', 'population', 'skewness', 'coefficient', 'distributions', 'unimodal', 'distribution', 'expected value', 'test', 'sampling']", Pearsons Skewness Measure,seg_191,"a skewness measure can be used to compare two samples drawn from distinct populations. however, as the sample statistics vary in repeated sampling from the same population, these comparisons are often vague. for example, pearson’s coefficient has expected value (chapter 8) zero for unimodal distributions. suppose we take repeated samples from a uniform or u-shaped distribution. the coefficient will vary widely in these situations (because the mode is not well defined for uniform distributions, and there are two modes for u-shaped distributions). see reference 58 for a discussion on a quadratic-mean based skewness test."
819,1,"['densities', 'symmetry', 'asymmetric', 'probability', 'population', 'transformation', 'skewness', 'distributions']", Pearsons Skewness Measure,seg_191,"the concept can be extended to population densities with intent to order them based on a skewness measure. one notable contribution is by van zwet [59], who defines a partial-order among probability laws with cumulative distributions f and g as f≤sg iff g−1(f(x)) is convex for x ≥ k (or equivalently f−1(g(x)) is concave), where k is the common point of symmetry of the distributions. this allows one to compare those distributions, the inverse of at least one of which exists. symmetric distributions can be converted into asymmetric ones using the transformation f (x, ) = 2g(x)g( ) where ∈ ℝ, and g() denotes the cdf [60, 61]."
820,1,"['deviation', 'quartile deviation', 'quartile', 'third quartile', 'coefficient']", Coefficient of Quartile Deviation,seg_193,the coefficient of quartile deviation exclusively uses the first and third quartile
821,1,"['linear', 'median', 'range', 'ratio measure']", Coefficient of Quartile Deviation,seg_193,"while bowley’s measure uses the median, cqd does not depend on the median. hence, it is less informative. as the numerator and denominator are both linear in qi and measured in the same units, cqd is a unit-less ratio measure with finite range. it is always positive as both the numerator and the denominator are positive."
822,1,['range'], Coefficient of Quartile Deviation,seg_193,example 4.6 range of values for cqd
823,1,"['quartile', 'third quartile']", Coefficient of Quartile Deviation,seg_193,solution 4.6 add and subtract q1 in the numerator to get (q3 + q1 − 2q1). combine the first two terms with the denominator and write the third terms separately. then cqd = 1 − 2q1∕(q1 + q3). because q3 (being the third quartile)
824,1,"['range', 'data', 'table']", Coefficient of Quartile Deviation,seg_193,"divides the entire data in 75%:25% ratio, it is always greater than q1. thus, the ratio q1∕(q1 + q3) is always in the range (0,.5). substituting 0 shows that cqd is always less than 1. substituting 0.5 shows that cqd is greater than zero (see table 4.3). hence cqd ∈ (0,1)."
825,1,"['inverse gaussian', 'distribution', 'symmetry', 'moments', 'skewness', 'distributions']", Other Skewness Measures,seg_195,"the concept of symmetry has been defined in terms of density or distribution functions in the above-mentioned discussions. kelly’s measure of skewness uses deciles and is defined as (d1 + d9 − 2d5)∕(d9 − d1). the inverse gaussian (ig)-symmetry is an analog that utilizes equality of positive and negative moments. for the ig( , ) law (chapter 7, pp. 7–64), it is easy to verify that e(x∕ )−r = e(x∕ )r+1, where negative index denotes inverse moments. the negative moments are defined only when f (x) = 0 for x = 0. this property is also satisfied by the log-normal law and scale mixtures of ig distributions [62]."
826,1,"['unbiased estimator', 'biased', 'expected value', 'population', 'skewness', 'estimator', 'unbiased']", Other Skewness Measures,seg_195,the skewness measure defined earlier is biased. an unbiased estimator can be obtained by differently scaling it to have an expected value exactly equal to the population skewness. this is why some software packages use n∕[(n − 1)(n − 2)] ∑j[(xj − x)∕s]3 (for n ≥ 3) as a measure of skewness. see reference 63 for a comparison of skewness measures.
827,1,"['leptokurtic', 'unimodal', 'symmetric', 'standard normal', 'kurtosis', 'skewed', 'data', 'distribution', 'pearson', 'multimodal', 'normal', 'discrete', 'standard', 'distributions']", CONCEPT OF KURTOSIS,seg_197,"kurtosis originated in data analysis. some data distributions are more peaked than the standard normal law, whereas some others are less peaked. this prompted pearson (1905) to classify distributions as leptokurtic, mesokurtic, and platykurtic. kurtosis was originally defined using the standard normal law as a yardstick. a data distribution that has the same kurtosis as n(0, 1) is called mesokurtic. those with higher kurtosis is called leptokurtic and with lower values is called platykurtic. they are applicable to discrete and truncated data, skewed, and symmetric data that are continuous. they are more meaningful to unimodal data than rectangular data. they are less meaningful to u-shaped and other multimodal data."
828,1,"['kurtosis', 'location', 'distribution', 'jointly', 'probability', 'tail', 'location measure']", CONCEPT OF KURTOSIS,seg_197,definition 4.3 kurtosis is a measure of both the peakedness of the distribution in and around the location measure (center of mass) and a measure of the tail weights that jointly characterize the accumulation of probability mass toward the center [64].
829,1,"['sample', 'kurtosis', 'case', 'population']", CONCEPT OF KURTOSIS,seg_197,the population analogs are denoted by greek letters and their sample counterparts by lower case english letters by convention. pearson’s kurtosis measure for the population is denoted by 2 and sample counterpart by b2 (or b2(n)).
830,1,"['unimodal', 'kurtosis', 'distribution', 'frequency', 'distributions']", An Interpretation of Kurtosis,seg_199,pearson’s definition of kurtosis confines itself to unimodal distributions. it emphasizes the overall frequency at or around the central part (mode) of a distribution.
831,1,"['kurtosis', 'probability', 'sample', 'exponential distribution', 'results', 'data', 'exponential', 'parameter', 'standard', 'error', 'distributions', 'parameters', 'leptokurtic', 'distribution', 'poisson distribution', 'poisson', 'absolute value', 'standard error', 'normal', 'tail', 'tails']", An Interpretation of Kurtosis,seg_199,"as theoretical distributions can take a variety of shapes depending on the parameter values, this “central part” may move to the extremes for some parameters. one example is the exponential distribution c ∗ exp (−c x) or the poisson distribution with very small values (say 0.10) that tails off slowly to the right (the left tail of these distributions are either very short or nonexistent). kurtosis is defined for these distributions too. such distributions are not kurtosis comparable with others that tails off in both directions. the classical kurtosis measures how much of the probability mass is moved from the shoulders (say within ± 2 to ± 3 ) of a normal law to the center that results in an identical leptokurtic distribution or vice versa (how much mass is moved from the central part (say from ± ) to the regions beyond, so as to get a platykurtic distribution). to quantify the amount of kurtosis, we could consider the sek and measure the departure from twice the standard error. an approximate sek for a sample of size n is √24∕n (see reference 49. if 2*sek absolute value of kurtosis, we may reasonably conclude that the data are from a non-platykurtic distribution. the above-mentioned interpretation of kurtosis can be refined, resulting in a new interpretation in terms of both the tailing off behavior combined with the peakedness simultaneously [65]–[70]."
832,1,"['kurtosis', 'random', 'function', 'sample', 'moment', 'random variable', 'standard', 'sample variance', 'sample size', 'asymptotic', 'standard normal', 'distribution', 'convergence', 'variance', 'second moment', 'variable', 'normal']", An Interpretation of Kurtosis,seg_199,"as the variance is a quadratic function of the random variable (for a population) or a quadratic function of the sample values (for s2), its second moment has power 4 (it is a biquadratic or quadratic of the random variable or sample values). in other words, the variance of the sample variance must be a function of pearson’s 2 (var(s2)/e[s2]2 = 2 − 1 asymptotically). as mentioned earlier, the variance is measured in the same unit as the sample values, whereas the kurtosis is a unit-less measure. if the sample size is large, it is known that √n(s2 − 2) → n(0, ( 2 − 1) 4), which is interpreted as convergence in distribution. as 2 = 3 for the standard normal law, the asymptotic convergence is to n(0, 2). in addition,√n(log (s2) − log ( 2)) → n(0, ( 2 − 1)) using mann–wald theorem [62]. it is well known that ns2∕ 2 is distributed as n"
833,1,"['kurtosis', 'samples', 'normal', 'population']", An Interpretation of Kurtosis,seg_199,"2 −1 when samples come from a normal population, so that e(ns2∕ 2)2 ≃ 2(n − 1). this result is used in some of the kurtosis measures defined in the following discussion."
834,1,"['distribution', 'binomial distribution', 'binomial']", An Interpretation of Kurtosis,seg_199,example 4.7 when is binomial distribution mesokurtic?
835,1,"['distribution', 'binomial distribution', 'binomial']", An Interpretation of Kurtosis,seg_199,prove that the binomial distribution is mesokurtic when p = 2 1 ( 1 ± √
836,1,"['kurtosis', 'distribution', 'binomial', 'binomial distribution', 'coefficient', 'distributions']", An Interpretation of Kurtosis,seg_199,"solution 4.7 the coefficient of kurtosis of binomial distribution is  2 = 3 + (1 − 6pq)∕npq (chapter 6). for mesokurtic distributions,  2 = 3. this means that (1 − 6pq)∕npq = 0. as the denominator is always positive, this expression is zero when pq = 1∕6. write this as p(1 − p) − 1∕6 = 0 or equiv-"
837,0,[], An Interpretation of Kurtosis,seg_199,3 . consider the
838,0,[], An Interpretation of Kurtosis,seg_199,3 second expression . multiply numerator and denominator by √ 3 and 6 cancel out 3 to get 1∕2√ 3. substitute in the aforementioned and take (1/2)
839,1,"['factor', 'condition']", An Interpretation of Kurtosis,seg_199,as common factor to get the condition as p = 2 1 (1 ± √
840,1,['distribution'], An Interpretation of Kurtosis,seg_199,. the distribution
841,1,['leptokurtic'], An Interpretation of Kurtosis,seg_199,is leptokurtic (respectively platykurtic) if (1 − 6pq)   (respectively  ) 0. in
842,0,[], An Interpretation of Kurtosis,seg_199,terms of p this becomes p   (respectively  ) 2 1 (1 ± √
843,1,['leptokurtic'], An Interpretation of Kurtosis,seg_199,it is leptokurtic if p   2 1 ( 1 − √
844,0,[], An Interpretation of Kurtosis,seg_199,1 3) and platykurtic if
845,1,['kurtosis'], Categorization of Kurtosis Measures,seg_201,the kurtosis can be measured in more than one way [61]. this section gives a categorization of popular kurtosis measures.
846,1,"['kurtosis', 'standard normal distribution', 'sample', 'moment', 'population', 'standard', 'standard normal', 'distribution', 'moments', 'standard deviations', 'deviations', 'normal', 'normal distribution']", Categorization of Kurtosis Measures,seg_201,"1. moment-based measures the classical kurtosis measures are moment-based and assume the existence of finite fourth moment (for the population). most of them utilize the fourth central moment or its scale transforms. they are defined for both the population and the sample. pearson’s kurtosis is expressed for a population in terms of moments as  2 =  4∕ 2 2. as the denominator is the fourth power of the standard deviations, they are also unit-less. because the standard normal distribution has kurtosis 3, the quantity  2 =  2 − 3 is widely used (see the following discussion)."
847,1,"['variables', 'kurtosis', 'standardized', 'population']", Categorization of Kurtosis Measures,seg_201,2. measures that utilize standardized variables (z-scores) the classical measure of pearson’s population kurtosis is defined as  2 = e[z4] where z = (x −  )∕ . stavig’s kurtosis measure [71] is defined as 1 − e[|z|]. seiner bonett used e[g(z)] where
848,1,"['unimodal', 'data']", Categorization of Kurtosis Measures,seg_201,which gives more importance to the peak at the center for unimodal data.
849,1,"['distribution', 'quantiles']", Categorization of Kurtosis Measures,seg_201,"3. quantile-based measures these measures utilize the quantiles of a distribution. the popular ones are due to balanda and macgillivray [52], groeneveld and meeden [72] and groeneveld [73]."
850,1,"['functions', 'distribution', 'function']", Categorization of Kurtosis Measures,seg_201,4. measures that utilize inverse of distribution functions these measures use the inverse of theoretical distribution functions. one example is the spread function of balanda and macgillivray [52]
851,1,"['plot', 'unimodal', 'continuous', 'skewness']", Categorization of Kurtosis Measures,seg_201,"the u is called interquantile distance. if both f and g are continuous unimodal and invertible, one could produce a plot of sf(u)∕sg(u) (or its inverse) to compare the relative skewness [74]."
852,0,[], Categorization of Kurtosis Measures,seg_201,5. measures that utilize density crossing
853,1,"['condition', 'kurtosis', 'samples', 'frequency', 'variance', 'distributions']", Categorization of Kurtosis Measures,seg_201,"the “density crossing” is a sufficient condition to kurtosis-order two samples. finucan [75] showed that if two distributions have the same variance, and if the frequency curves cross twice on each side of the mode, then one of them has higher kurtosis than the other."
854,1,"['range', 'unimodal', 'symmetric', 'kurtosis', 'case', 'standardized', 'asymmetric', 'skewness', 'bivariate', 'distributions']", Categorization of Kurtosis Measures,seg_201,"4.4.2.1 van zwet ordering of kurtosis as in the case of skewness, theoretical distributions can be “kurtosis ordered” [59]. this is more meaningful for symmetric unimodal distributions. a bivariate ordering based on both the skewness and the kurtosis is more meaningful for asymmetric distributions. as various distributions have different range, they are standardized to the same range before they are ordered."
855,1,"['sample', 'leptokurtic', 'standard normal', 'kurtosis', 'case', 'skewed', 'data', 'distribution', 'skewed distribution', 'loss', 'normal', 'population', 'standard', 'standard normal distribution', 'normal distribution']", MEASURES OF KURTOSIS,seg_203,"kurtosis measures are used to numerically evaluate the relative peakedness or flatness of data. the standard normal distribution can be used as a yardstick for bell-shaped data, but the concept is valid for other shapes such as j-shaped, reverse j-shaped, and cusp-shaped data. it is applicable to both the sample and the population. this has important implications in some fields. as examples, suppose that there are many weight-loss programs available. the distribution of actual weight lost, or the time spent in the program by participants can take various shapes. a negatively skewed distribution in the first case will indicate that more persons lost more weight and in the second case will indicate that participants who spent more time in the program lost more weight. a leptokurtic distribution indicates that the program was very effective in weight loss, whereas a platykurtic distribution indicates that the weight loss was gradual. hence, people will be more attracted to a positively skewed or leptokurtic weight-loss program. similarly consider machine servicing by various vendors or repair persons. if there are multiple shops that could do this, a client may be more interested in that service shop with a leptokurtic and positively skewed servicing time distribution. we need a standard scale to measure the amount of kurtosis."
856,1,"['kurtosis', 'pearson', 'moments', 'population']", Pearsons Kurtosis Measure,seg_205,"using the reasoning in page 4–22, pearson defined the population kurtosis in terms of moments as"
857,1,"['mathematical expectation', 'moment', 'standardized', 'expectation']", Pearsons Kurtosis Measure,seg_205,"where e() denotes mathematical expectation (chapter 8) [76]. this is the fourth moment of the standardized variate z = (x −  )∕ . using v(x) = e[x2] − e[x]2 on [(x −  )∕ ]2, we have"
858,1,"['sample', 'kurtosis', 'asymmetric', 'function', 'coefficient', 'variance', 'distributions']", Pearsons Kurtosis Measure,seg_205,"rearranged, we get e{[(x −  )∕ ]4} = v{[(x −  )∕ ]2} + {e[(x −  )∕ ]2}2. the second expression being the square of the variance the rhs becomes v{[(x −  )∕ ]2} + {v[(x −  )∕ ]}2. an interpretation of this result is that the kurtosis and variance (spread) are related through squares. as the numerator is an even function of the variate, this measure allows one to compare the kurtosis of asymmetric distributions. the sample kurtosis coefficient is"
859,1,"['range', 'kurtosis', 'discrete', 'sample', 'estimate', 'biased', 'data', 'standardized', 'population', 'standard', 'standard deviation', 'distributions', 'leptokurtic', 'distribution', 'fisher', 'continuous', 'deviation', 'sample standard deviation']", Pearsons Kurtosis Measure,seg_205,"where sn is the sample standard deviation. because sums of fourth powers is always positive, b2 ≥ 0. as the zero-point is well defined, it is a ratio-measure with range ∈ ℝ. it is shown in the following (next page) that the kurtosis of the standard normal distribution is 3. this means that irrespective of whether the data are discrete or continuous, we could subtract 3 to get 2 = 2 − 3 as a standardized measure of kurtosis as suggested by fisher. then 2 0 indicates leptokurtic and 2 0 indicates platykurtic distributions. replacing the population quantities by the corresponding sample equivalents, we could get a biased estimate as"
860,1,"['sample', 'estimate', 'factor', 'biased', 'kurtosis', 'biased estimator', 'population', 'estimator', 'unbiased']", Pearsons Kurtosis Measure,seg_205,the sample kurtosis is a biased estimator of the population kurtosis. we need to apply a different scaling factor to get the unbiased estimate. this is why some
861,1,"['coefficient', 'kurtosis']", Pearsons Kurtosis Measure,seg_205,example 4.8 classical kurtosis coefficient
862,1,"['coefficient', 'kurtosis', 'mean', 'dispersion']", Pearsons Kurtosis Measure,seg_205,prove that the classical kurtosis coefficient measures the dispersion of [(x −  )∕ ]2 around its mean 1.
863,1,"['standardized', 'normally distributed', 'distribution', 'mean', 'variance']", Pearsons Kurtosis Measure,seg_205,"solution 4.8 replace e[(x −  )∕ ]2 on the rhs of equation (4.12) by v[(x −  )∕ ] + {e[(x −  )∕ ]}2. as z = (x −  )∕  is a standardized variate, it has mean e(z) = 0 and variance v(z) = 1. if x is normally distributed, ((x −  )∕ )2 has a chi-square distribution having 1 dof with mean 1 and variance 2 (chapter 7). substitute these values in equation (4.12) to get"
864,1,"['variance', 'mean']", Pearsons Kurtosis Measure,seg_205,this shows that e{[(x − )∕ ]4} = v{[(x − )∕ ]2} + 1 measures the dispersion of [(x − )∕ ]2 about its mean 1 (which is the variance of z = (x − )∕ ).
865,1,"['normal', 'variance']", Pearsons Kurtosis Measure,seg_205,substitute for v{[(x −  )∕ ]2} = 2 (chi-square variance with 1 dof) shows that the kurtotis is 3 when x is normal.
866,1,"['symmetric', 'kurtosis', 'case', 'symmetric distributions', 'distribution', 'asymmetric', 'variance', 'distributions']", Pearsons Kurtosis Measure,seg_205,"the dependency of kurtosis on variance is more pronounced for symmetric distributions than for others (asymmetric and truncated distributions). the amount by which spread is reduced when kurtosis is increased depends also on whether frequency is moved from both the shoulders of a distribution to the center or only from one side (left or right) to the center, in which case the reduction of variance could be minimal. if one is interested only in the peakedness of distributions, a truncated measure that eliminates the contribution of the tail(s) may be more appropriate. the truncation point can be setup equidistant from the mode for symmetric distributions."
867,1,"['standard normal', 'distribution', 'normal', 'standard', 'standard normal distribution', 'normal distribution']", Pearsons Kurtosis Measure,seg_205,"for the standard normal distribution, 2 is 3. however, there exist many other distributions that also have 2 = 3. for example, the tukey distribution with = 5.2"
868,1,['gamma'], Pearsons Kurtosis Measure,seg_205,and double gamma law with   = 0.5 ∗ (1 +√
869,1,"['kurtosis', 'distribution', 'probability', 'tail', 'tails', 'distributions']", Pearsons Kurtosis Measure,seg_205,"13) all have 2 = 3, although their shapes are different [77, 78]. this is because pearson’s kurtosis measure encapsulates both the peakedness and tail weight(s) of a distribution. this is easy to understand using truncated distributions. a left-truncated distribution tails off slower than their nontruncated counterparts. truncating a distribution at left or right tail also increases the peak probability by f (x)∕(1 − ) where is the probability of truncated part."
870,1,"['kurtosis', 'pearson']", SkewnessKurtosis Bounds,seg_207,"several researchers have studied the d = (skewness−excess kurtosis) quantity from various perspectives. see for example references 79, and 80. pearson obtained the bound  1"
871,1,"['bernoulli', 'bernoulli distributions', 'distributions']", SkewnessKurtosis Bounds,seg_207,2 −  2 ≤ 2 for bernoulli distributions. this was further improved by several
872,1,['distribution'], SkewnessKurtosis Bounds,seg_207,"researchers. if the distribution is infinitely divisible,  1"
873,1,"['range', 'kurtosis', 'symmetric distributions', 'poisson distributions', 'inequality', 'moment', 'symmetric', 'normal and poisson distributions', 'distributions', 'normal distributions', 'distribution', 'test', 'poisson', 'normal', 'normal distribution']", SkewnessKurtosis Bounds,seg_207,"for normal and poisson distributions [81]. this property was used in reference 82 to distinguish poisson or normal distributions from other infinite divisible distributions. a similar quantity is c = skewness/kurtosis (for kurtosis≠ 0), which is well behaved for symmetric distributions in general and the normal distribution in particular. see reference 83 for some inequalities, reference 54 in the context of minimizing information divergence under moment constraints, reference 84 for a studentized range based test and reference 85 for a right and left inequality order, and reference 86 for skewness-invariant measures of kurtosis."
874,1,['kurtosis'], Lkurtosis,seg_209,"this is a generalization of kurtosis, introduced by hosking [53], that uses l-moments (denoted by  ′"
875,1,"['linear', 'linear combination', 'combination', 'variable', 'random variable', 'expectation', 'statistic', 'mean', 'random', 'order statistic']", Lkurtosis,seg_209,"ks). for a real-valued random variable x with finite mean  , we define the l-moment as expectation of linear combination of order statistic as follows (see page 4–10). let x("
876,1,"['sample', 'random', 'statistic', 'random sample', 'order statistic']", Lkurtosis,seg_209,n k) denote the kth order statistic of a random sample of size n.
877,1,['coefficients'], Lkurtosis,seg_209,4 1)). note that the coefficients of x(
878,0,[], Lkurtosis,seg_209,"j k) are the rows of pascal’s triangle with alternating signs. the l-skewness is then defined as  3 =  3∕ 2. similarly, l-kurtosis is defined as  4 =  4∕ 2"
879,1,"['linear', 'linear combination', 'combination', 'statistics', 'order statistics', 'expectation']", Lkurtosis,seg_209,"2. as both are expectation of linear combination of order statistics, they are unit-less measures."
880,1,"['ratio scale', 'independent', 'interval', 'kurtosis', 'case', 'data', 'normal', 'tails', 'limit']", Spectral Kurtosis SK,seg_211,"as mentioned earlier, kurtosis can clearly distinguish between peakedness and flatness in numeric data in the interval or ratio scale. it is a quantified real number (+ve or −ve) whose magnitude represents the amount of departure of a distribution from the shoulders toward the center and tails. this property of kurtosis can be used to predict machine faults (using past data) [87], in fault diagnosis of equipments or independent parts and materials [88, 89] damage assessment of structures [90], crack detection of isotropic plates, machine diagnostics and prognostics [91], modular classification of digital signals, and so on. as an example, they can be used to warn an operator on machine overloads or wear and tear beyond a threshold. consider an aircraft or helicopter with a fixed weight limit on the cargo and passenger compartments, respectively. if either or both of these sections exceed the weight limit, resulting in an overall overweight, the bearings sound during takeoff due to the excess weight acting down can slightly deviate from the normal takeoff sound at the same ground speed (in the case of aircraft). similarly, by analyzing acoustic signals, one can distinguish between human footsteps from background noise (impurities) or identify submarines from whales in deep water. seismic sensors and geophones use such signals to automatically measure the movement of objects or to distinguish between possible objects (such as vehicles, humans, other animals, or objects) and direction of movement (moving toward, away from or along a trajectory around the sensor)."
881,1,"['cumulants', 'deviation', 'kurtosis', 'frequency', 'normal']", Spectral Kurtosis SK,seg_211,definition 4.4 spectral kurtosis (sk) is a ratio-measure defined in the frequency domain of a signal that reveals the deviation from gaussianity of the spectral components with intent to separate randomly occurring signals from normal ones using cumulants.
882,1,"['transformed', 'data', 'discrete', 'frequency', 'fourier transform', 'frequency transforms', 'transform']", Spectral Kurtosis SK,seg_211,"the above-mentioned definition assumes that original data are transformed into a band-delimited frequency domain using one of the popular frequency transforms such as discrete fourier transform (dft), discrete wavelet transform (dwt), and so on. a simpler definition that hides the technical details is as follows:"
883,1,"['frequency', 'kurtosis']", Spectral Kurtosis SK,seg_211,definition 4.5 the spectral kurtosis of a signal is the kurtosis of its sampled frequency components.
884,1,"['transformed', 'random', 'discrete', 'frequency', 'random process', 'process']", Spectral Kurtosis SK,seg_211,"let x[n] denote a real-time discrete random process where the index n denotes the time. let x[m] denote the transformed signal in the frequency domain. then, the sk of x[n] is defined as"
885,1,"['cumulants', 'random', 'case', 'stationary', 'cumulant', 'random process', 'process']", Spectral Kurtosis SK,seg_211,"where  r is the rth order cumulant and x∗[m] denotes the complex conjugate. in the case where x[n] is a stationary random process, nonnull cumulants of x[m] will have as many complex conjugate terms as nonconjugate terms."
886,1,"['outliers', 'independent', 'harmonics', 'discrete']", Detecting Faults Using SK,seg_213,"rotating machines typically exhibit nonstationary vibration signatures that are easy to detect using sk [92]. the popularity of sk comes from its proved effectiveness in real-time signal detection and removal of noise (impurities, harmonics, outliers, or deviants). in time-varying discrete signals, it can distinguish between constant amplitude harmonics, time-varying amplitude harmonics, and noise. in addition, it is conceptually simple and easy to compute. its value is independent of the noise present in the input signal. this is why it has been applied in a variety of fields such as astronomy, industrial robotics, and deep-sea explorations."
887,1,"['precision', 'transformed', 'harmonics', 'data', 'deviations', 'frequency', 'transform', 'fourier transform', 'event', 'harmonic']", Detecting Faults Using SK,seg_213,"consider a healthy induction motor running at a constant speed. the harmonic components of such asynchronous machines are constant amplitude harmonics. the sk of such faulty machines can be compared with healthy ones to identify possible deviations [93]. this means that the data generated by a healthy machine is stored for future use and compared continuously with current data generated while it is presumably operating under fault (such as cracks, defunct components, or lubricant depletion) to detect any possible deviations. owing to the heterogeneity of working conditions, a healthy data vector is used instead of a single data instances. these data are usually bandpass filtered and transformed into the frequency domain (usually using short time fourier transform (stft) or wavelet transforms [90] and processed in a fixed time window. the sk of both these data is found and compared to detect defects. another application is to detect and remove (if present) radio-frequency interference (rfi) in radio astronomy and gps. the precision of such event or object identification can be improved using multiple receivers."
888,1,"['precision', 'frequencies', 'kurtosis', 'frequency band', 'frequency', 'variation', 'function']", Detecting Faults Using SK,seg_213,sk can be used to measure the impulsiveness of signals (variation of frequencies) as a function of frequencies in a band-filtered domain [94]. they can give an indication on the most impulsive part of a vibrating signal. kurtosis of each frequency band can be used to improve the precision in a time or frequency decomposed signal. see references 95–98 for applications to fault diagnosis.
889,1,"['sample', 'moment', 'kurtosis', 'discrete', 'discrete distributions', 'distribution', 'distance metric', 'continuous distributions', 'mean', 'continuous', 'second moment', 'distributions']", Multivariate Kurtosis,seg_215,"the kurtosis concept has been extended to multivariate distributions by many researchers. see references 49, 99–101. the following discussion is on multivariate continuous distributions, although the concept is valid for discrete distributions. let and σ denote the mean vector and variance–covariance matrix of a multivariate distribution in ℝd. then, the classical kurtosis measure [99] is defined as d = e{[(x − )′σ−1(x − )]2}. as e{[(x − )′σ−1(x − )]} is the squared mahalanobis distance metric, this represents the second moment of mahalanobis’ squared distance. the sample analog is obtained by replacing by the mean vector x"
890,1,"['sample', 'kurtosis']", Multivariate Kurtosis,seg_215,and σ by the sample variance–covariance matrix. a discussion of source separation using kurtosis maximization can be found in reference 102.
891,1,"['lack of symmetry', 'symmetric', 'kurtosis', 'data', 'symmetry', 'normal', 'skewness', 'tail', 'statistical']", SUMMARY,seg_217,this chapter discussed several measures of skewness and kurtosis. most of the popular statistical techniques are devised for the symmetric bell-shaped data (which is technically called normal data). the skewness captures the lack of symmetry in the data trend. kurtosis captures the tail thickness in the data trend. financial and health data are also known to exhibit thick tailness.
892,1,"['functions', 'simulation', 'asymptotic', 'statistics', 'kurtosis', 'distribution', 'moments', 'skewness', 'statistical', 'distributions']", SUMMARY,seg_217,"most of the popular measures of skewness and kurtosis are based on the central moments or functions of it. however, it is well known that the moments do not always determine a distribution uniquely. several examples to support this fact are available in the literature (see references 60 and 103. this leads to skewness and kurtosis measures based on other statistics than moments. √ 1 and 2 are routinely used in statistical analysis [104]. asymptotic distributions of skewness and kurtosis coefficients are discussed in reference 105, an application in regenerative simulation in reference 106, and rain-drop diameter distribution in reference 107. an application of fuzzy mean-variance-skewness to portfolio selection models can be found in reference 108. a visualizing discussion can be found in reference 109."
893,1,"['skewness', 'lack of symmetry', 'symmetry']", SUMMARY,seg_217,a) skewness is a measure of the lack of symmetry
894,1,"['data', 'moment']", SUMMARY,seg_217,b) third moment measures the asymmetry of data
895,1,"['skewness', 'symmetry', 'median']", SUMMARY,seg_217,c) zero skewness indicates symmetry around the median
896,1,"['skewness', 'tail', 'long left tail']", SUMMARY,seg_217,d) positive skewness indicates a long left tail
897,1,"['sample', 'observation', 'coefficient', 'skewness', 'coefficient of skewness']", SUMMARY,seg_217,e) every sample observation contributes to the coefficient of skewness
898,1,"['variance', 'kurtosis', 'data']", SUMMARY,seg_217,f) kurtosis measures are useless in providing variance of data
899,1,"['independent', 'coefficient', 'skewness coefficient', 'change of scale', 'transformation', 'skewness']", SUMMARY,seg_217,g) the skewness coefficient is independent of change of scale transformation
900,1,"['kurtosis', 'data']", SUMMARY,seg_217,h) truncating data at left end increases kurtosis
901,1,"['symmetric', 'skewed']", SUMMARY,seg_217,i) a left-truncation of symmetric law makes it positively skewed
902,1,"['transformed', 'kurtosis', 'data', 'frequency']", SUMMARY,seg_217,j) spectral kurtosis uses frequency transformed data.
903,1,['skewness'], SUMMARY,seg_217,"4.2 prove that bowley’s skewness 4.4 if pearson’s   is zero, one can"
904,1,['data'], SUMMARY,seg_217,measure varies between −1 and infer that the__ (a) data distribu-
905,1,"['distribution', 'symmetric']", SUMMARY,seg_217,+1. tion is symmetric (b) distribution
906,1,['distributions'], SUMMARY,seg_217,is mesokurtic (c) x = mode but 4.3 for bell-shaped distributions
907,1,"['distribution', 'skewness']", SUMMARY,seg_217,distribution need not be symmetprove that the skewness measures are zeros. ric (d) distribution is bell-shaped
908,1,"['skewness', 'kurtosis']", SUMMARY,seg_217,4.5 show that the kurtosis of stan4.13 what is the value of skewness for
909,1,"['asymptotic', 'uniform distribution', 'symmetric', 'standard normal', 'distribution', 'normal', 'parameter', 'standard', 'continuous', 'convergence', 'normal distribution', 'distributions']", SUMMARY,seg_217,"dard normal distribution is 3. disthe following distributions? cuss how this helps in asymptotic a) bell-shaped distribution b) convergence of other distributions continuous uniform distribution, that tend to standard normal for c) symmetric triangular distribularge parameter values. tion."
910,1,['range'], SUMMARY,seg_217,4.6 what are some desirable qualities 4.14 what is the range of possible val-
911,1,['skewness'], SUMMARY,seg_217,of a good measure of skewness? ues of pearson’s skewness mea-
912,1,"['sample', 'expected value', 'distributions']", SUMMARY,seg_217,metry using the 5-number suma zero value indicate? what is mary of a sample. its expected value for bell-shaped distributions? what is a disadvan4.8 arrange the following distributage of this measure?
913,0,[], SUMMARY,seg_217,tions according to increasing lev-
914,1,"['standard normal distribution', 'parameters', 'gamma', 'cauchy', 'range', 'standard normal', 'kurtosis', 'skewed', 'cauchy distribution', 'distribution', 'gamma distribution', 'normal', 'standard', 'skewness', 'normal distribution']", SUMMARY,seg_217,"els of kurtosis (called kurtosis 4.15 what is the range of values for ordering) (i) student’s t distrithe standard skewness and kurtobution with n 25, (ii) standard sis measures? what is the reason cauchy distribution, (iii) standard for defining the kurtosis measure normal and (iv) double exponen2 as 2 − 3? derive its value for tial (v) gamma distribution with standard normal distribution. parameters (10, 2) 4.16 what does a distribution of marks 4.9 consider the distribution of marks in an exam skewed to the left indi-"
915,1,['skewness'], SUMMARY,seg_217,"obtained in an exam. what type cate? of skewness is exhibited in the following situations? (i) the exam 4.17 when is the bowley measure was easy for majority of stuand galton measure of skewness dents, (ii) the exam was difficult equal?."
916,1,"['distribution', 'symmetric', 'symmetric distribution']", SUMMARY,seg_217,"for majority of students, and (iii) 4.18 if a symmetric distribution is questions that carry around 50 of left-truncated, will the new distri-"
917,0,[], SUMMARY,seg_217,the marks were easy questions. bution be positively or negatively
918,1,['skewed'], SUMMARY,seg_217,4.10 what is the 5-number summary skewed? will it change disper-
919,1,"['sample', 'gamma', 'kurtosis', 'change of scale', 'moments', 'skewness']", SUMMARY,seg_217,of a sample? can you check the sion?. skewness and kurtosis of the sam4.19 does the skewness and kurtosis ple using the 5-number summary? get affected by the change of scale 4.11 find the moments of gamma distransformation y = c ∗ x? does
920,1,"['skewness', 'quantile', 'kurtosis']", SUMMARY,seg_217,"tribution, and obtain the measures the quantile based measures get of skewness and kurtosis. affected?"
921,1,"['statistic', 'kurtosis']", SUMMARY,seg_217,4.12 if the sign of kurtosis statistic is
922,1,"['leptokurtic', 'skewed', 'data', 'distribution', 'normal', 'skewed data', 'normal distribution']", SUMMARY,seg_217,"positive, it indicates (a) leptokurtic 4.20 what value does cqd = (q3 − distribution (b) mesokurtic distriq1)∕(q3 + q1) take for (i) symmetric data? (ii) for positively bution (c) platykurtic distribution skewed data? (d) normal distribution."
923,1,"['skewness', 'population']", SUMMARY,seg_217,4.21 show that the population skewmeasure the skewness? what does
924,0,[], SUMMARY,seg_217,ness can be expressed as  1(x) = high values indicate?
925,1,['percentile'], SUMMARY,seg_217,"[e(x4) − 4 e(x) + 6 2e(x2) − 4.29 prove that the kth percentile is 3 4]∕d4, where d = std.dev. given by pk = l+(n*k/100−m)*"
926,1,"['range', 'limit']", SUMMARY,seg_217,"4.22 what is the range of moore’s kurc/f, where l = lower limit of per-"
927,1,"['percentile', 'frequency']", SUMMARY,seg_217,"tosis measure? what are the possicentile class, m is the cumulative ble value ranges? frequency up to (but excluding) percentile class, c is class width 4.23 comment on the statement “kur- and f is the frequency of percentile"
928,1,['variance'], SUMMARY,seg_217,tosis and variance (spread) are class. inversely related.” 4.30 which of the following measures 4.24 what is the relation between quar-
929,1,['percentiles'], SUMMARY,seg_217,tiles and percentiles? uses  4∕ 2
930,1,['dispersion'], SUMMARY,seg_217,"2? (a) dispersion, (b)"
931,1,"['kurtosis', 'location']", SUMMARY,seg_217,"skewness, (c) kurtosis, (and d) 4.25 give the mathematical expression location"
932,1,"['quartiles', 'skewness']", SUMMARY,seg_217,to convert deciles into quartiles. 4.31 find skewness for bragg reflec-
933,1,"['continuous', 'data']", SUMMARY,seg_217,"4.26 when is the continuous uniform tion of x-ray data {0.0795,"
934,1,['skewness'], SUMMARY,seg_217,2 ≤  2 + 5/6. 4.32 find skewness for the seeds
935,1,['data'], SUMMARY,seg_217,example data (p. 3–31) given in 4.28 consider a measure defined as chapter 3
936,1,['skewness'], SUMMARY,seg_217,q = (q2 − q0)∕(q4 − q2) where q0 = x(1) is the first-order statis4.33 prove that the skewness can
937,1,['quartiles'], SUMMARY,seg_217,"tic, q4 = x(n) and other q′ is are be increased using one-sided the quartiles. can it be used to truncation."
938,0,[], PROBABILITY,seg_219,"after finishing the chapter, students will be able to"
939,1,['probability'], PROBABILITY,seg_219,◾ comprehend the concept of probability
940,1,['probability'], PROBABILITY,seg_219,◾ explore different ways to express probability
941,1,['probability'], PROBABILITY,seg_219,◾ understand various approaches to probability
942,1,"['events', 'probabilities']", PROBABILITY,seg_219,◾ grasp the meaning of events and how to assign probabilities to them
943,0,[], PROBABILITY,seg_219,◾ apply various counting rules and selection techniques
944,1,"['independent', 'dependent', 'events', 'independent events']", PROBABILITY,seg_219,◾ differentiate between dependent and independent events
945,1,"['bayes theorem', 'conditional', 'probability', 'conditional probability', 'bayes']", PROBABILITY,seg_219,◾ understand conditional probability including bayes theorem
946,1,['probabilities'], PROBABILITY,seg_219,◾ practice computations of probabilities for a variety of problems
947,1,['probability'], INTRODUCTION,seg_221,"probability had its humble beginning in gambling and games of chance. the theoretical foundations of probability were laid by several 17th and 18th century mathematicians. prominent among them are the french mathematicians blaise pascal (1623–1662) and pierre de fermat (1601–1665), dutch astronomer christian huygens (1629–1695), english mathematician and physicist isaac newton (1642–1727), french mathematicians abraham de moivre (1667–1754), pierre simon laplace"
948,1,['poisson'], INTRODUCTION,seg_221,"(1749–1827), simeon-denis poisson (1781–1840), german mathematician leibnitz gottfried (1646–1716), and so on to name a few."
949,1,"['levels', 'quantitative', 'uncertainty', 'probability']", INTRODUCTION,seg_221,definition 5.1 probability is a quantitative ratio capturing the possible levels of uncertainty or chance.
950,1,"['statistics', 'frequency', 'probability', 'random sample', 'random', 'sample', 'population', 'statistical', 'quality control', 'random sampling', 'distributions', 'estimation', 'control', 'sampling']", INTRODUCTION,seg_221,"it is encountered in almost all applied sciences such as statistical physics, quantum mechanics, bioinformatics, and various branches of engineering. the study of probability became an essential part of statistics owing to the obvious reason that probability is deep rooted in a great majority of statistical models and procedures. for example, random sampling, frequency distributions, reliability and gaming models, estimation and inference, statistical quality control, and so on are based on the foundations of probability. chances play a prominent role in characterizing a random sample from an unknown population. there exist many approaches to define and use probability. we begin with the most popular approaches. a thorough understanding of these approaches is essential for students to apply probability to solve real-life problems."
951,1,"['urn models', 'combination', 'results', 'cases', 'permutation', 'set', 'probability', 'set theory']", INTRODUCTION,seg_221,"the greatest challenge in solving a probability problem is that there are usually many ways to solve it but no obvious way to verify the results. suppose that a student is presented with a probability problem. the first thing to decide is which of the approaches is the most appropriate one to solve it. there are several set theoretic laws, rules, permutation and combination, urn models, principle of inclusion and exclusion, and so on that are used to solve probability problems. fundamental laws of set theory give rise to analogous laws of probability. majority of these approaches are classical, as exemplified in the following discussion. the answer obtained in a problem can be verified only in some particular cases where the numbers involved are small."
952,1,"['quantitative', 'uncertainty', 'events', 'probability', 'random', 'experiments']", PROBABILITY,seg_223,definition 5.2 probability is a quantitative measure of uncertainty or chance associated with future events or random experiments.
953,1,"['estimation', 'probability', 'outcomes', 'estimation theory']", PROBABILITY,seg_223,"in gambling or games of uncertain outcomes, it is referred to as “the odds.” for example, it may be mentioned that the odds are three to two that a horse will win a race. in estimation theory, it is called the “likelihood.” the reliability in engineering and plausibility in management refers to probability."
954,1,"['random samples', 'range', 'variates', 'probability', 'random', 'random sample', 'associated', 'process', 'sample', 'events', 'samples', 'population', 'condition', 'outcomes', 'random variables', 'variables']", PROBABILITY,seg_223,"probability is always associated with one or more future events, a happening, an unknown process, or a working condition. probability is also associated with random samples, random variables, and uncertain outcomes. the “likelihood” that is mentioned earlier associates a probability to a random sample drawn from a population with a known functional form. probability associated with random variates is mathematical expressions that return a real number in [0,1] range for each possible value"
955,1,"['simple events', 'range', 'location', 'set', 'probability', 'random', 'numerical', 'experiment', 'data', 'events', 'parameter', 'event', 'level', 'method', 'table', 'likelihood', 'uncertainties', 'bandwidth']", PROBABILITY,seg_223,"of the variate. these can be too low for some x values when the range of the variate is infinite or for particular parameter values. in numerical probability problems that are discussed in the following sections, it represents the chance of a specified event as a real number. this probability is the same irrespective of the method used to arrive at it. to simplify our discussion, we assume the chance mechanism as logically fine grained. the chance mechanism may be a fine-grained event (likelihood of error-free transmission of a data packet, chance of winning a game, likelihood that two political contestants will address the same location or share the same podium, etc.), a random phenomenon (chance that an electronic component will fail in a computer), or an experiment (probability of survival after a surgery, probability that a new drug will be more effective than the existing ones). note that in each of them there exist many levels of uncertainties. for example, transmission of a data packet depends on network bandwidth, transmission media, network congestion, and the proper functioning of other hardware or software components. thus, there are multiple interacting simple events involved in the main event see table 5.1 for a set of symbols). in all probability problems, we will unambiguously identify the events at the root level. probability is a ratio-measure. a “probability of zero” indicates an impossibility. a ‘probability of one’ indicates a complete certainty (in common parlance “in all probability” denotes a very likely or certain event). these occur quite often in theoretical problems but are a rarity in practice."
956,1,"['failure', 'events', 'probability', 'control', 'quality control', 'processes']", PROBABILITY,seg_223,"probabilities encountered in some fields are extremely small. consider manufactured products from a company that has implemented six-sigma. as all processes are streamlined and quality control techniques ensure stringent restrictions, chances of defects in newly manufactured items are extremely small. other examples are survival chances in some terminal diseases, chances of product returns in newly introduced items (like new models of cell phones), chances of natural calamities in some locations, and so on. each of these events has a “complementary event” (defined below) for which the corresponding probability is quite high (close to 1). for instance, probability that an electronics component will work without failure is high. this shows that the magnitude of probability depends on how we define events."
957,1,"['functions', 'percentage', 'tail areas', 'information', 'probability', 'tail']", DIFFERENT WAYS TO EXPRESS PROBABILITY,seg_225,"it was mentioned in section 5.2 (p. 112) that probability is a real number between 0 and 1. the information content in probability statements can be expressed in multiple ways. popular ways to express a probability are (i) fractional form, (ii) decimal form, (iii) scientific form, (iv) percentage form, (v) literal form, (vi) pictorial form, and (vii) as tail areas under empirical curves or functions [2]."
958,1,"['factors', 'case', 'cases', 'probability']", DIFFERENT WAYS TO EXPRESS PROBABILITY,seg_225,"the fractional form represents a probability as a fraction p/q where p and q are assumed to be without common factors (called proper form of a fraction). the decimal form represents a probability in the form 0.dddd where “d” denotes a decimal digit that may or may not repeat. in case of repeating fractions, the digits repeat either individually or as a group. for example 1∕3 = 0.333 is a single digit repeating fraction (here the digits that are underlined denotes the repeating part). consider 5∕11 = 0.4545. this is a double-digit repeating fraction (the repeating part 45 is underlined). such repeating fractions are encountered in several applications. the fractional form has the advantages that it is easy to remember and compact for permanent computer storage. fortunately, the decimal form of probability can be converted into its fractional equivalent by some simple algorithms described in the following. for this, we consider three cases depending on whether any of the trailing digits cyclically repeat or not."
959,1,"['method', 'interval', 'greatest common divisor', 'algorithm']", Converting Nonrepeating Decimals to Fractions,seg_227,"suppose we have a non-repeating decimal number. how do we convert it into the equivalent fractional form p/q? as the trailing decimal digits do not repeat, multiply the decimal number by an appropriate power of 10 (say m = 10k) to remove all decimal places. let the number after multiplication be n. find the greatest common divisor (gcd) of m and n (say p = gcd(m, n)). if p ≠ 1 and p ≠ n, divide both m and n by p to get the answer. this method will work only when the number (n) is divisible by 2, 5, or their multiples (such as 4, 10, and so on.). we can only give approximate result when the trailing decimal digits cyclically repeat over a wide interval. we summarize it as an algorithm for positive fraction in the following. extension to negative fractions is straightforward. line 6 in the listing means that the result is returned in the form p/q."
960,0,[], Converting Nonrepeating Decimals to Fractions,seg_227,example 5.1 decimal to fractional form example-1
961,1,['probabilities'], Converting Nonrepeating Decimals to Fractions,seg_227,"express the following probabilities in fractional form p/q: (i) 0.18, (ii) 0.0015, (iii) 0.125, (iv) 0.29, (v) 0.032"
962,1,['case'], Converting Nonrepeating Decimals to Fractions,seg_227,"solution 5.1 we need to multiply 0.18 by 100 to discard all decimal digits. thus, n = 18,m = 100. the gcd(18,100) is 2. dividing both 18 and 100 by 2 gives the answer as 9/50. (ii) in this case, we have n = 15 and m = 10, 000. the"
963,1,['case'], Converting Nonrepeating Decimals to Fractions,seg_227,"gcd(15,10,000)= 5. dividing both 15 and 10,000 by 5 gives the resulting fractional form as 3/2000. (iii) here n = 125,m = 1000, and gcd(125,1000)= 125. dividing by 125 gives the resulting fractional form as 1/8. (iv) here n = 29, m = 100, and gcd(29,100)= 1 (as 29 is a prime). hence, the resulting fractional form is 29/100. (v) in this case, we have n = 32 and m = 103 = 1000, gcd(32, 1000)= 8 giving the result 4/125."
964,0,[], Converting Nonrepeating Decimals to Fractions,seg_227,algorithm 5.1 convert a non-repeating decimal number into fractional form
965,0,[], Converting Nonrepeating Decimals to Fractions,seg_227,1: input the decimal number into x
966,0,[], Converting Nonrepeating Decimals to Fractions,seg_227,2: count the total number of decimal places k in x
967,1,"['cases', 'set', 'case']", Converting Repeating Decimals to Fractions,seg_229,"this is more challenging than the nonrepeating case. here we consider two cases. in the first case, the repeating block starts as the very first digit. if a set of trailing digits repeat cyclically within a reasonable size, we identify the decimal number as p = 0.dd where d is the cyclically repeating part. as done earlier, we multiply it by m = 10k to move the decimal point to the right position of the last digit of the first repeating block (k is the size of the repeating block). let the resulting value be y = 10k ∗ p. compute z = y − p, which is devoid of fractions. now find r = gcd(z,m − 1). divide both z and m − 1 by r to get the desired fractional representation."
968,0,[], Converting Repeating Decimals to Fractions,seg_229,example 5.2 decimal to fractional form example-2
969,1,['probabilities'], Converting Repeating Decimals to Fractions,seg_229,"express the following probabilities in fractional form p/q: (i) 0.666, (ii) 0.1818, (iii) 0.315 315"
970,0,[], Converting Repeating Decimals to Fractions,seg_229,"solution 5.2 (i) let p = 0.666. here the first digit itself repeats indefinitely. hence d = 6 (repeating block), k = 1 (its size). multiply p by m = 10 to get"
971,1,['algorithm'], Converting Repeating Decimals to Fractions,seg_229,"y = 6.66. subtract p from y to get z = 6. as m − 1 = 9, r = gcd(z,m − 1) = gcd(6, 9) = 3. divide numerator and denominator by 3 to get the fractional equivalent as (6/3)/(9/3) = 2/3. this is of the form p/q without common factors. (ii) let p = 0.1818. here d = 18, k = 2, so that m = 100 (as there are two digits that cyclically repeats) and y = 18.1818. compute z = y − p = 18, and r = gcd(z,m − 1) = gcd(18, 99) = 9. divide both z and m − 1 by r to get p = 2∕11. (iii) here d = 315 repeats indefinitely. hence, we need to multiply by m = 103 = 1000 to move the decimal place. this gives z = 315,m = 1000, r = gcd(315, 999) = 9. the answer is (315/9)/(999/9)= 35/111. we give below an algorithm for this purpose."
972,0,[], Converting Repeating Decimals to Fractions,seg_229,algorithm 5.2 convert a repeating decimal number into fractional form
973,0,[], Converting Repeating Decimals to Fractions,seg_229,1: input the decimal number into x
974,0,[], Converting Repeating Decimals to Fractions,seg_229,{∗ assumption: blocks of digits repeat starting with the first digit ∗}
975,0,[], Converting Repeating Decimals to Fractions,seg_229,2: count the total number of decimal places k in x
976,0,[], Converting Repeating Decimals to Fractions,seg_229,3: find the repeating cycle length k in x
977,0,[], Converting Repeating Decimals to Fractions,seg_229,4: multiply x by 10k to make it an integer followed by a fraction (say y)
978,0,[], Converting Repeating Decimals to Fractions,seg_229,5: subtract x from y to get an integer z
979,1,['case'], Converting TailRepeating Decimals to Fractions,seg_231,"this is a variant of the aforementioned in which the trailing digits repeat cyclically, after a nonrepeating block of digits. this is the hardest case to consider. we identify the decimal number as p = 0. d1dd where d1 is the non-repeating part and d is the cyclically repeating part. note that d1 can be a single digit or zero too (as in 0.633, 0.01515). as done earlier, we multiply p by m = 10n (where n is the number of digits in d1) to move the decimal point to the right position of the last digit of d1. let y = p ∗ 10n. next multiply y by 10k to move the decimal point to the right position of the first block of repeating digits and store it in z. then q = z − y is devoid of fractions. next find r = gcd(q, 10n(10k − 1)). divide both q and 10n(10k − 1) by r to get the desired fractional representation. these are explained in the following sections."
980,1,['tail'], Converting TailRepeating Decimals to Fractions,seg_231,algorithm 5.3 convert tail repeating decimal into a fraction
981,0,[], Converting TailRepeating Decimals to Fractions,seg_231,1: input the decimal number into x
982,0,[], Converting TailRepeating Decimals to Fractions,seg_231,{∗ assumption: blocks of digits repeat after a non-repeating block ∗}
983,0,['n'], Converting TailRepeating Decimals to Fractions,seg_231,"2: find the repeating cycle length k in x, and non-repeating block length n"
984,0,[], Converting TailRepeating Decimals to Fractions,seg_231,3: multiply x by 10n to make it an integer followed by a repeating fraction (say y)
985,0,[], Converting TailRepeating Decimals to Fractions,seg_231,4: multiply y by 10k to make it an integer followed by a fraction (say z)
986,0,[], Converting TailRepeating Decimals to Fractions,seg_231,5: subtract y from z to get an integer q
987,0,[], Converting TailRepeating Decimals to Fractions,seg_231,example 5.3 decimal to fractional form example-3
988,1,['probabilities'], Converting TailRepeating Decimals to Fractions,seg_231,"convert the following probabilities (i) 0.6333 (ii) 0.21515, (iii) 0.0571428 571428 into the form p/q."
989,1,['case'], Converting TailRepeating Decimals to Fractions,seg_231,"solution 5.3 let x = 0.6333. as the nonrepeating block is of size 1, first multiply x by 10 to get y = 6.333, then multiply y by 10 to get z = 63.33. subtract y from z to get q = 63 − 6 = 57. find r = gcd(57, 10 ∗ (10 − 1)) = gcd(57, 90) = 3. divide both 57 and 90 by 3 to get p = (57∕3)∕(90∕3) = 19∕30. in part (ii) p = 0.21515. here repeating cycle length is k= 2 digits, and nonrepeating block size is n = 1 so that 10k = 100, 10k+n = 1000, and [10n(10k − 1)] = 990. this gives q = 215 − 2 = 213. form the fraction p = 213∕990. find the gcd as m = gcd(213, 990) = 3. divide both the numerator and denominator of p by 3 to get the required answer p = 71∕330. in case (iii), we have k = 6 and n = 1, so that 10k = 1000000, and [10n(10k − 1)] = 9999900. this gives q = 571428, and p = 571428∕9999900. next, we need to find the gcd(571428,9999900). we write 571428 = 22 ∗ 33 ∗ 11 ∗ 13 ∗ 37 and 9999900 = 22 ∗ 32 ∗ 52 ∗ 37, from which the gcd is 2857140. divide both the numerator and denominator of p by 2857140 to get the required answer p = 2∕35."
990,0,[], Converting TailRepeating Decimals to Fractions,seg_231,example 5.4 repeating decimals to fractional form
991,1,['probabilities'], Converting TailRepeating Decimals to Fractions,seg_231,"convert the probabilities (i) 0.01515, (ii) 0.006363 into fractional form."
992,1,['case'], Converting TailRepeating Decimals to Fractions,seg_231,"solution 5.4 let x = 0.01515. here the repeating cycle length is k= 2 digits, and n= 1 so that 10k = 100 and 10n(10k − 1) = 990. multiply x by 10n to make it an integer (in this case 0) followed by a fraction as y= 0.1515. next multiply y by 10k to make it an integer (in this case 15) followed by a fraction as z = 10k+n ∗ x = 0.1515 ∗ 100 = 15.1515. now subtract y from z to get an integer q=z−y = 15.1515 − 0.1515 = 15. form the fraction p = q/[10n(10k − 1)] = 15/990. find the gcd of z and 10n(10k − 1) as m = gcd(z, 10n(10k − 1)) = gcd(15,990)= 15. finally, divide both the numerator and denominator of p by 15 to get the required answer p= 1/66. (ii) here also k= 2 digits, so that 10k = 100. proceed as earlier and find the gcd of 63 and 99 as gcd(63,99) = 9. divide both 63 and 99 by 9 to get the answer 7/11."
993,1,"['cases', 'algorithm']", Converting TailRepeating Decimals to Fractions,seg_231,"we could improve upon our gcd in some particular cases. for example, if q is an odd number, gcd(q, [10n(10k − 1)]) is the same as gcd(q, [(10k − 1)]). if the nonzero digits in the nonrepeating block is an exact divisor of the repeating block, we could reduce it to the above-mentioned form. consider x = 0.00021 4242, in which the nonrepeating block has a 21, which divides the repeating block 42 42. this reduces to x = 0.0000102020 in which the nonrepeating block has nonzero digit as a single 1, and the repeating block is “02” of length 2 (or nonrepeating block “10” followed by repeating digits “20”). when there are several leading zeros in the nonrepeating block as in this example, we could consider the nonrepeating block as the nonzero digits (by simply sliding the decimal place over all zeros) and make a final adjustment to the result. this is described in the following algorithm."
994,1,['tail'], Converting TailRepeating Decimals to Fractions,seg_231,algorithm 5.4 tail repeating decimal with many leading zeros into p/q form
995,0,[], Converting TailRepeating Decimals to Fractions,seg_231,1: input the decimal number into x
996,0,[], Converting TailRepeating Decimals to Fractions,seg_231,"{∗ assumption: blocks of digits repeat after a non-repeating block, the first few of"
997,0,[], Converting TailRepeating Decimals to Fractions,seg_231,"2: find the repeating cycle length k in x, and block size of leading zeros of length"
998,0,['n'], Converting TailRepeating Decimals to Fractions,seg_231,"m, and non-repeating nonzero-digit block of length n"
999,0,[], Converting TailRepeating Decimals to Fractions,seg_231,3: multiply x by 10m to move the decimal place over the zeros (say y)
1000,0,[], Converting TailRepeating Decimals to Fractions,seg_231,4: multiply y by 10n to make it an integer followed by a repeating fraction (say z)
1001,0,[], Converting TailRepeating Decimals to Fractions,seg_231,5: multiply z by 10k to make it an integer followed by a fraction (say t)
1002,0,[], Converting TailRepeating Decimals to Fractions,seg_231,6: subtract z from t to get an integer q
1003,1,['tail'], Converting TailRepeating Decimals to Fractions,seg_231,example 5.5 tail repeating decimal to fractional form
1004,0,[], Converting TailRepeating Decimals to Fractions,seg_231,convert the decimal 0.00022 4545 to fractional form.
1005,0,['n'], Converting TailRepeating Decimals to Fractions,seg_231,"solution 5.5 here repeating block is of length k= 2, nonrepeating block is of length n= 2 (5–“3 zeros”), and m is 3 (as there are three leading zeros). first multiply x by 10m. we have 10k = 100, 10k+n = 10, 000, and 10n(10k − 1) = 9900. this gives q = 2245 − 22 = 2223, and p= 2223/9900000. next, find m=gcd(2223, 9900000)= 9. dividing both the numerator and denominator of p by 9 gives p = 247∕1100000, which is the required answer."
1006,0,[], Converting TailRepeating Decimals to Fractions,seg_231,"the repeating block may be too long for some fractions, especially involving ratios of primes. consider 2/17= 0.1176470588235294 117.., which repeats itself after 16 decimal places. similarly, there are many fractions for which the cycle of digits repeats well beyond the calculator display. consider 7/29 = 0.2413 7931 0344 8275 8620 6896 5517 241, and so on, which repeats after 28 decimal places! (they are in general of the form k/(k*n+ 1) with cycle block size k*n). we could either approximate such decimals or employ other algorithms."
1007,1,"['approximation', 'case']", Converting TailRepeating Decimals to Fractions,seg_231,"if we truncate it at the wrong decimal place (say 8th or 16th place), the resulting fraction will not come even close to the true value (7/29 in the above-mentioned example). for example, truncating at second decimal place gives 6/25 and truncating at 12th decimal place gives 30172413793/125000000000. an astute reader will notice that our original digits are repeated after a nonrepeating block of length 4. this property can be used to approximate the fractional value using the second algorithm given earlier. this means that we may sometimes be able to approximate a nonrepeating decimal number (or a repeating decimal with a large cycle length) by dividing it by a small number. in the above-mentioned case, we get the approximation as 3017/12500 = 0.24136. this is correct to the fourth decimal place."
1008,1,"['memory', 'method', 'data', 'locations']", Converting TailRepeating Decimals to Fractions,seg_231,"assuming that all our decimal numbers are positive, we could store any decimal number in just two memory locations (one for storing p and the other for storing q). signed decimals need an extra 1 bit to store the sign as 0 for positive and 1 for negative. as an unsigned int type can store numbers between 0 and 65,535 in just 2 bytes of memory, we could represent a great majority of fractions that we encounter in practice using this method, provided that both the numerator and denominator are less than 65,536. we could use the unsigned long int data type (4 bytes of memory) when larger numbers are involved, as it can store up to 4,294,967,295."
1009,1,"['geometric', 'geometric probability', 'probability']", Converting TailRepeating Decimals to Fractions,seg_231,percentage form of probability is obtained by multiplying the decimal form by 100. these are usually used in conversations and correspondences. scientific form is preferred when a probability is too small with several leading zeros. the pictorial form is used in geometric probability problems.
1010,1,"['experimental', 'statistics', 'probability', 'experiments']", SAMPLE SPACE,seg_233,"random experiments are at the core of experimental probability. here, the word “ex- periment” has a different meaning in statistics than its literal meaning. a simple"
1011,1,"['experiment', 'statistics', 'observation']", SAMPLE SPACE,seg_233,"measurement of a physical or other characteristics of an object, a count of objects that satisfy one or more conditions, an observation of the duration of a phenomena (like the lifetime of a device) can all be considered as an experiment in statistics."
1012,1,"['random', 'experiments', 'outcomes']", SAMPLE SPACE,seg_233,definition 5.3 random experiments are those that are repeated under identical conditions every time and always produce one among several outcomes.
1013,1,"['levels', 'observation', 'measurements', 'random', 'vary', 'sample', 'experiment', 'results', 'trial', 'factors', 'outcomes', 'measurement', 'experiments']", SAMPLE SPACE,seg_233,"here, the clause “under identical conditions” needs some scrutiny. it only means that the conditions are replicable and statistically insignificant. for example, consider the measurement of the storage of water in a reservoir. if measurements are taken over a period of time (say on successive days or weekends), the conditions may not be exactly identical in the strict scientific sense. owing to the pull exerted by celestial bodies on the surface of the earth, the reservoir levels could go up when the moon has just passed overhead. this gravitational pull is more in the equatorial region when the moon and the sun are both oriented in more or less the same direction over the place of observation (this is why very high tides occur on some days), which is maximum during the closest approach of the moon to the earth. similarly, the amount of water evaporated depends on the day-time temperature, wind speed, humidity, and reservoir area among other things. it is our tacit assumption that random experiments are conducted in rapid succession or in short duration of time. extraneous factors, if any, that could affect the measurements should be accurately maintained in highly sensitive and time-dependent scientific experiments. these are often negligible when the sample is collected over a short duration. the purpose of an experiment could also be the identification of such differences (as in agricultural experiments). the qualifier “random” indicates that the outcomes are unpredictable until the results are observed. in other words, the results will vary from trial to trial even when the conditions of the experiment are the same."
1014,1,"['sample', 'experiment', 'set', 'random', 'sample space', 'outcomes']", SAMPLE SPACE,seg_233,definition 5.4 the set of all possible outcomes of a random experiment is called its sample space.
1015,1,"['sample', 'arithmetic mean', 'complement', 'null set', 'set', 'mean', 'probability', 'event', 'complement of an event', 'sample space']", SAMPLE SPACE,seg_233,"the sample space itself is an event because it always occurs. by convention, it is indicated by the greek symbol ω (pronounced capital omega). its complement is denoted by ωc =   (the null set, pronounced small phi). the complement of an event x is denoted as x′ ,x, or xc. as x is used in subsequent chapters to denote the arithmetic mean, we will use xc for complement. the very first step in solving any probability problem is to identify the sample space. these are quite often easy to find. we illustrate it with various examples."
1016,1,"['sample', 'sample space', 'experiment']", SAMPLE SPACE,seg_233,example 5.6 sample space for simple experiment
1017,1,"['sample', 'experiment', 'sample space']", SAMPLE SPACE,seg_233,what is the sample space of an experiment of throwing two fair coins?
1018,1,"['sample', 'tail', 'sample space']", SAMPLE SPACE,seg_233,"solution 5.6 denote a head turning up by h and a tail turning up by t. then, the sample space is {hh, ht, th, tt} where hh denotes that both throws resulted in heads, and so on. here “h” and “t” are simply labels. we could assign any"
1019,1,"['sample', 'tail', 'sample space']", SAMPLE SPACE,seg_233,"label we wish (because the english letter h is a silent syllable in spanish (words) and is pronounced differently in greek, russian, etc.). for example, if head is denoted by a “1” and the tail by a “0,” our sample space becomes {11, 10, 01, 00}."
1020,0,[], SAMPLE SPACE,seg_233,example 5.7 circuits in series in a device
1021,1,"['sample', 'sample space']", SAMPLE SPACE,seg_233,"there are two circuits in series in a device, both of which can be open or closed. identify the sample space when the device is turned on."
1022,0,[], SAMPLE SPACE,seg_233,"solution 5.7 denote the open circuit by a 0 and closed circuit by a 1. then, the possibilities are {00, 01, 10, 11} where 00 indicates that both circuits are open and 11 indicates that both are closed."
1023,0,[], SAMPLE SPACE,seg_233,example 5.8 balls in urns
1024,1,"['sample', 'sample space']", SAMPLE SPACE,seg_233,find the sample space for (i) drawing two balls from an urn containing three red and two blue balls that are indistinguishable except for the color (ii) two throws of a dice that result in a sum of 10.
1025,1,"['sample', 'cases', 'sample space']", SAMPLE SPACE,seg_233,"solution 5.8 (i) denote the red ball by r and blue ball by b. the possible outcomes are {r, r}, {r, b}, {b, r}, {b, b}, (ii) denote the numbers on the die by {1, 2, 3, 4, 5, 6}. then the possible 36 values in the sample space are {1, 1}, {1, 2}, {1, 3}, {1, 4}, {1, 5},{1, 6}, {2, 1}, {2, 2}, {2, 3}, {2, 4}, {2, 5}, {2, 6}, … , {5, 6}, and {6, 6}. here {1, 6} and {6, 1} are considered to be different, even if the two dice are thrown simultaneously. for part (ii), the favorable cases are {(5,5), (4,6), (6,4)}."
1026,1,"['absolute value', 'sample', 'sample spaces', 'experiment', 'random', 'sample space', 'event']", SAMPLE SPACE,seg_233,"the sample space obviously depends on the defined event. if an event u is defined as the sum of the numbers that show up when two dice are thrown, the sample space of u becomes {2, 3, 4, 5, 6, 7, 8, 9, 0, 11, 12}. if another event v is defined as the absolute value of the difference between the numbers that show up, the sample space of v becomes {0, 1, 2, 3, 4, 5}. this shows that multiple sample spaces can be obtained on the same random experiment."
1027,1,"['permutations', 'urn models', 'recurrence relations', 'combinations', 'venn diagrams', 'venn', 'probability', 'recurrence']", MATHEMATICAL BACKGROUND,seg_235,"probability problems are unlike the problems in other sciences. beginning students sometimes find it difficult to solve probability problems because there are either several ways or no obvious way to solve it. different problems may require a different approach, concept, or tool. there are many such tools and techniques needed to solve every problem in probability. examples are venn diagrams, permutations and combinations, principle of inclusion and exclusion, urn models, recurrence relations,"
1028,1,"['event algebra', 'without replacement', 'conditional', 'independence', 'probability', 'geometric', 'probability distributions', 'replacement', 'events', 'event', 'distributions', 'combination', 'bipartite graphs', 'sampling']", MATHEMATICAL BACKGROUND,seg_235,"divide and conquer or decrease and conquer principles, sampling with and without replacement, bipartite graphs, and de’morgan’s laws to name a few. in addition, independence of events, conditional events, and other event algebra discussed in the following sections may be needed individually or in combination in some problems. there are still other problems that can be solved easily by geometric reasoning, properties of probability distributions, and so on. a thorough understanding of these tools and techniques are essential to solve all probability problems with ease. the following section first describes the essential tools and then applies it to individual problems."
1029,1,"['cardinality', 'homogeneous', 'probability theory', 'sets', 'complement', 'set', 'probability', 'set theory']", Sets and Mappings,seg_237,"a set is a collection of distinguishable elements logically considered as a group. the elements may be homogeneous or heterogeneous. for example, consider fruits and vegetables as two separate sets. the fruit set can comprise of apples, oranges, berries, bananas, and the like, whereas the vegetable set may consist of potatoes, tomatoes, carrots, and so on. total number of distinct elements in a set s is called the size of the set or its cardinality. it is denoted as |s|. this is always an integer ≥1. to extend the set theory to various situations involving intersect and complement operations, we will denote an empty set (without any elements in it) by the greek symbol (pronounced “small phi”). the size of the empty set by convention is zero (i.e., | | = 0). the totality of all elements under consideration in a set is called the universal set, super-set, or set space. it is symbolically denoted by ω. any element of ω is called a member or point of the set and is denoted by . multiple elements can be combined to get subsets of the set ω. in probability theory, our main interest is in counting proper subsets of ω."
1030,1,"['set', 'null set', 'set ']", Sets and Mappings,seg_237,definition 5.5 the collection of all subsets of a set s (including the null set   and the set itself) is called the power-set (it is denoted by 2s and has 2|s| elements).
1031,1,['cardinality'], Sets and Mappings,seg_237,example 5.9 cardinality of power-set
1032,1,"['cardinality', 'set', 'power set']", Sets and Mappings,seg_237,use induction to prove that the power set p(s) of a finite set s has cardinality 2|s|.
1033,1,"['cardinality', 'sets', 'set']", Sets and Mappings,seg_237,"solution 5.9 consider a singleton set s (with just one element, say b). its power-set is { , b} of cardinality 2. next consider a set with two elements s = {a, b}. its power-set is { , {a}, {b}, {a, b}} of cardinality 4. thus, the assumption is true for n = 1, 2 where n is the number of elements in the set. assume that it is true for an arbitrary set s of size k   2. obviously, cardinality of s is 2|s| = 2|k| = 2k. label all elements currently in s by a group symbol  . now add a single new element x to s to make it s′ = { , x} of cardinality |s′| = k + 1. the power-set of s′ comprises the power-set of s, plus new subsets formed by adding x to each of them. as sets are unordered collections, adding x to each subset of   produces at most 2|s| new subsets. thus, the total number of subsets in s′ is 2|s| + 2|s| = 2 ∗ 2|s| = 2|s|+1 = 2k+1 = 2|s′|. this shows that if"
1034,0,['n'], Sets and Mappings,seg_237,"the assumption is true for n = k, it is true for n = k + 1. by induction, it is true for all positive integers n ≥ 1."
1035,0,[], Sets and Mappings,seg_237,example 5.10 powerset example-2
1036,1,['set'], Sets and Mappings,seg_237,"find the power-set of the set s = {a, b, c}"
1037,0,[], Sets and Mappings,seg_237,solution 5.10 we will tackle the problem by the divide-and-conquer approach. first consider all one-element subsets. there are three singleton subsets
1038,1,"['set', 'null set', 'set ']", Sets and Mappings,seg_237,"two-element subsets as {a, b}, {a, c}, {b, c} (see section 5.9.5 in page 145). to this add the null set   (with no elements), and the set s itself to get the power-set"
1039,0,[], Sets and Mappings,seg_237,having 2|s| = 23 = 8 elements. this is pictorially shown in figure 5.1.
1040,1,"['probability theory', 'set', 'probability']", Sets and Mappings,seg_237,a special decomposition of a finite set s is of importance in probability theory. this is called the partition of s or set partition.
1041,1,"['set', 'mutually exclusive']", Sets and Mappings,seg_237,"definition 5.6 a partition of a finite set s with at least two distinct elements is a collection of mutually exclusive and collectively exhaustive subsets s1, s2, … , sm such that s = s1 ∪ s2 ∪ · · · ,∪ sm = ∪m"
1042,1,['set'], Sets and Mappings,seg_237,"i=1si, and si ∩ sj =   for all i ≠ j. note that   and s are not counted in a set partition."
1043,1,"['distribution', 'set']", Sets and Mappings,seg_237,"each element of a set partition can be mapped to a real number pi. if this mapped number has the property that they add up to 1 (∑ipi = 1), it is called a distribution defined over s."
1044,1,"['case', 'discrete', 'set', 'venn diagrams', 'probability', 'euler diagrams', 'sample', 'graphical', 'sets', 'events', 'event', 'sample space', 'interactions', 'venn diagram', 'venn', 'continuous', 'probabilities', 'interaction']", Venn Diagrams,seg_239,"the british mathematician and cleric john venn (1834–1923) introduced venn diagrams in 1881 for representing sets and operations on them. they became instantly popular because there are just two symbols used in its graphical representation—a rectangle denotes the universal set u, and one or more labeled circles or ellipses drawn wholly within the rectangle denote subsets of u. event interactions are represented by intersecting labeled circles (see figures 5.2 and 5.3). the area that is common to intersecting circles can map the actual amount of interaction of the events. in most of the problems, this need not be so fine-grained because the venn diagram is not used to compute the probabilities directly; rather it is a visual device simply to ascertain if events interact or not. sets without common elements are drawn as nonintersecting circles. this is useful when the number of events is small (say 2–6). the importance of venn diagrams in probability arises because events (both discrete and continuous) that underlie probabilities are easily represented by sets. they are valuable tools in breaking complex probability problems involving multiple intersecting events into simpler subproblems that are easy to solve. venn diagrams have been extended by many researchers to suit problems in engineering, geology, chemistry, and other sciences. examples are karnaugh maps, euler diagrams, johnston diagrams, edwards’ venn diagrams, and peirce diagrams. euler diagrams are an extension of venn diagrams to represent more than one sample space (see reference 110). venn diagrams may not be easy to comprehend when there are too many intersecting events. in such a case, we could form a hierarchical venn diagram by labeling events with a common denomination to the top of the hierarchy."
1045,1,"['events', 'union']", Venn Diagrams,seg_239,example 5.11 union of events
1046,1,['percentage'], Venn Diagrams,seg_239,"sixty percentage of the people in an office read newspaper “a,” and 50% read newspaper “b.” if 10% of the people read neither “a” nor “b,” what percentage of the people read both newspapers?"
1047,1,"['percentage', 'venn diagram', 'venn', 'event']", Venn Diagrams,seg_239,"solution 5.11 this problem is easily solved using a venn diagram. let “a” denote the event that people read newspaper “a” and “b” denote the event that they read newspaper “b.” then a ∪ b denotes the event that people read either of the newspapers and a ∩ b denotes the event that people read both newspapers. as this problem involves count or percentage, the event and count can be considered as synonymous. as the number of people who reads either of them is given as a ∪ b, the number of people who read neither is u-a ∪ b = 10% (given). from this, we get 10 = 100 − [(60 + 50) − a ∩ b] or a ∩ b = 20%. hence, 20% of the people read both newspapers."
1048,1,"['quantitative', 'discretization', 'case', 'probability', 'mutually exclusive', 'experiment', 'data', 'cases', 'categorical variable', 'disjoint', 'categorical', 'nonlinear', 'continuous', 'outcomes', 'variables', 'variable']", Tree Diagrams,seg_241,"several probability problems involve mutually exclusive subcases or subevents. these are best represented as rooted trees or forests (a collection of disjoint trees is called a forest). a tree in computer science is a nonlinear data structure with a distinguished node called the root. a pictorial representation of trees makes it much easy to comprehend. for this purpose, the root is always drawn either at the top or at the left. a tree is a special case of a graph. although a graph can be directed or undirected, a tree is almost always undirected. the branches (straight lines) drawn from a node represent subproblems, subsets, or subcases. this representation can sometimes decompose a complex probability problem into two or more simple ones or as a hierarchy of subproblems. each such subcase can be further broken down into smaller trees. this subdivision usually uses a categorical variable such as sex and religion or outcomes of an experiment. quantitative variables could also be used to subdivide a node into smaller subtrees if (i) the number of cases are small or (ii) discretization is used to categorize the continuous variable."
1049,1,['experiment'], Tree Diagrams,seg_241,example 5.12 tree-diagram for coin toss experiment
1050,1,"['sample', 'sample space', 'tree diagram']", Tree Diagrams,seg_241,a fair coin is tossed three times. draw the tree diagram and find the sample space.
1051,1,"['sample', 'union', 'sample space', 'outcomes']", Tree Diagrams,seg_241,"solution 5.12 as there are just two possible outcomes in each throw, we denote it by two branches from the nodes. consider the first throw. it could result in either an h or a t. the second and subsequent throws are denoted as further branching as in the figure. the sample space is obtained as the union of labels at the leaf nodes as {hhh, hht, hth, htt, thh, tht, tth, ttt}. this is pictorially shown in figure 5.4."
1052,0,[], Tree Diagrams,seg_241,"in some problems, there exist more than one way to draw a tree. sometimes, the tree is formed by the occurrence of a related happening as in sports tournaments in which the winning team encounters other players or opponents."
1053,1,"['ordinal', 'nominal', 'sets', 'probability', 'bipartite graph']", Bipartite Graphs,seg_243,a bipartite graph consists of two sets of nodes (say v and w) such that each node in v is connected to some node in w and vice versa. this means that nodes in v are not connected to other nodes in v and similarly nodes in w are not connected to other nodes in w. the elements of v and w are nominal or ordinal type in most of the applications. the bipartite graph is extremely useful in simplifying some of the probability problems.
1054,0,[], Bipartite Graphs,seg_243,example 5.13 jobs and applicants
1055,1,"['model', 'bipartite graphs', 'sets']", Bipartite Graphs,seg_243,a company has five vacancies that require different skill sets for which 10 applications are received. describe how bipartite graphs can be used to model the matching of applicants and jobs.
1056,1,"['nominal', 'sets', 'set', 'bipartite graph']", Bipartite Graphs,seg_243,"solution 5.13 here, both sets are nominal type. represent 10 applicants by 10 labeled nodes on the left and five jobs by five nodes on the right. make a link among the two sets of nodes if the skill set of ith applicant fits the job j. for each job j, if there is only a single applicant, remove it from the bipartite graph. if the remaining graph is a forest, we could identify groups of applicants that are clustered in groups of jobs (or a single job). otherwise, the bipartite graph will show the choices for hiring applicants."
1057,1,"['disjoint', 'bipartite graphs', 'cases', 'bipartite graph']", Bipartite Forests,seg_245,"these are special cases of bipartite graphs in which the entire graph can be decomposed into two or more disjoint bipartite graphs. the smallest possible bipartite graph is one in which one node on the left is connected to just one node on the right. in the jobs and applicants example, if one applicant is connected to one job and there are no other links between these two nodes, this can be removed as there are no other choices. hence, bipartite forests decompose a larger problem into smaller subproblems that can be independently solved."
1058,1,"['set', 'events', 'null set', 'associated', 'probability', 'set theory']", EVENTS,seg_247,"there are many types of events encountered in probability and related fields. a good understanding of them can greatly simplify some of the probability problems. in addition, they provide a parallel between the axioms of set theory and probability. we assume in the subsequent discussions that p( ) = 0 (probability associated with the null set is zero), and p(ω) = 1."
1059,1,"['sample', 'experiment', 'event', 'outcome', 'sample space']", EVENTS,seg_247,definition 5.7 an event is a well-defined outcome of an experiment or a subset of a sample space.
1060,1,"['outcomes', 'experiment', 'events', 'set', 'associated', 'probability', 'random', 'outcome', 'trial', 'event']", EVENTS,seg_247,"the literal meaning of an event is a thing that happens. in probability, an event is a well-defined outcome associated with a random experiment or a trial. set theoretically, events are subsets of the set of all possible outcomes."
1061,1,"['sample spaces', 'discrete', 'set', 'probability', 'random', 'sample', 'experiment', 'events', 'sample space', 'condition', 'continuous', 'outcome', 'outcomes', 'independent']", EVENTS,seg_247,"objective probability has two basic building blocks. first, there should be a random experiment that generates uncertain outcomes. these can be discrete or continuous. a unique label is assigned to each outcome of the experiment to distinguish among themselves. secondly, we must have events that are either single outcomes or a collection of outcomes that satisfy a user-specified condition or criterion. the set of all possible outcomes of a random experiment is called sample space. the sample space is specific to each random experiment. it may or may not depend on time. most of the problems that are encountered below are time independent sample spaces."
1062,1,"['experiment', 'discrete', 'events', 'associated', 'probability', 'event', 'trial']", EVENTS,seg_247,"each discrete event is uniquely identified by a label, a symbol, a number, or other identifying mark. there are no hard and fast rules to name events. event labels are usually denoted by capital letters of an alphabet (english, greek, etc.). we can combine events and denote them by other labels or abbreviated letters. one experimenter may label the events resulting in a single toss of a coin as {h, t} while another may label it as {1, 0}. these labels are meant to distinguish the events among themselves. if you conduct such a trial or experiment, every event will eventually occur. some events that have high chance of occurring materializes more. thus, the probability associated with each event can tell us which are more likely to materialize than others."
1063,1,['event'], EVENTS,seg_247,example 5.14 event identification
1064,1,['events'], EVENTS,seg_247,consider contaminants in drinking water sources collected from different parts of a city. describe what are some possible events and how to combine them.
1065,1,"['sample', 'simple event', 'event', 'limit']", EVENTS,seg_247,"solution 5.14 assume that there are a dozen possible contaminants in drinking water. label each contaminant by a unique letter. then a simple event can be defined as “presence of contaminant in drinking water above the prescribed limit.” for instance, let “a” denote the presence of arsenic above its permitted limit and let “c” denote the presence of chromium, and so on. if a sample contains both arsenic and chromium (and not the others), it can be labeled as “ac”"
1066,1,"['combination', 'event']", EVENTS,seg_247,or “ca.” this type of concatenated labeling is inconvenient when there are a large number of possibilities. a solution is to give separate codes of fixed size or numbers to each combination event.
1067,1,"['failures', 'predicted', 'failure', 'data', 'probabilistic', 'events', 'outcomes']", Deterministic and Probabilistic Events,seg_249,"events may be deterministic or probabilistic. outcomes of deterministic events are always predictable using mathematical equations or various laws of physics, chemistry, or logic. for instance, chances of getting six spades in a hand of 10 cards is exactly predictable as the number of spades in any deck of cards is 13. the reservoir capacity example given in page 5–18 is predictable using the position of the sun and path of the moon. probabilistic events can be predicted by past analysis of outcomes. an aircraft’s engine failure can be predicted well in advance if past data on engine failures are available for specific engines of certain age and type."
1068,1,"['discrete', 'events', 'probability', 'continuous']", Discrete Versus Continuous Events,seg_251,"events can be discrete or continuous. in probability problems (especially textbook examples), we seldom encounter continuous events. however, continuous events are encountered in engineering applications."
1069,1,"['continuous', 'events']", Discrete Versus Continuous Events,seg_251,example 5.15 continuous events
1070,1,"['continuous', 'events']", Discrete Versus Continuous Events,seg_251,give examples of continuous events.
1071,1,"['sample', 'level', 'continuous', 'case', 'variable', 'complement', 'probability', 'event', 'complement of an event', 'sample space', 'complementary event']", Discrete Versus Continuous Events,seg_251,"solution 5.15 consider a microchip that has just been manufactured. its lifetime (in hours) is an event that can take any positive value. the sample space in this case is s = {t|t ≥ 0} and the variable involved is time. let f denote the event that it does not fail during the first c = 1000 hours of operation. then p(f) = p(t c). this probability becomes smaller as c is increased. consider a swimming pool with an optimal capacity. if the water inflow and outflow are ignored, the amount of water in the pool (say in cubic feet or inches) can be considered as a continuous sample space. the complementary event in this case is the optimal level less the current capacity. both of them are continuous. in general, if the sample space is continuous, the complement of an event defined on the sample space is also continuous."
1072,1,"['random', 'variables', 'data', 'location', 'events', 'frequency', 'associated', 'variations', 'probability', 'experiments', 'processes']", Discrete Versus Continuous Events,seg_251,"probabilistic events are associated with random experiments, random variations in some processes (like manufacturing), or unknown variations in some variables (probability that an air-bag in a car will fail to inflate upon a collision depends on the speed at impact and various circuitry characteristics. probability of a rain or snow (at an appropriate location) tomorrow depends on hundreds of interacting atmospheric variables). one need not understand the variables that drive a phenomenon to predict the probability. past data collected over a period of time can be used in such situations using the frequency approach. in this chapter, we are more interested in probability mechanisms involved in random experiments."
1073,1,"['experiment', 'results', 'probability', 'random', 'outcome', 'event']", Discrete Versus Continuous Events,seg_251,"each event of a random experiment is mapped to a probability between 0 and 1. this is denoted by p(a) where a is the event label, name, or token; and p() is the notation for probability. an event is said to occur if the outcome of a random experiment results in that event."
1074,1,"['combination', 'event']", Discrete Versus Continuous Events,seg_251,example 5.16 combination event examples
1075,1,"['events', 'failures']", Discrete Versus Continuous Events,seg_251,give examples of events defined on each of the following: (i) software-controlled machine failures and (ii) viscous flow through a pipe.
1076,1,"['combination', 'case', 'events', 'variations', 'average', 'vary']", Discrete Versus Continuous Events,seg_251,"solution 5.16 machines may fail due to (i) mechanical faults, (ii) electrical faults, (iii) software faults, (iv) wrong handling or wrong initial settings, and (vi) other reasons. let these be denoted by events m, e, s, h and o. combination events can then be represented as described earlier. in the case of viscous flow, we could define events using flow-rate or average amount of liquid transported since last overhaul work. this could vary slightly depending on the pressure applied, viscosity of liquid, surface corrosion, and outside temperature variations."
1077,1,"['probabilities', 'interval', 'events', 'probability', 'event']", Discrete Versus Continuous Events,seg_251,"almost certain events have probability one. as examples, the probability that an email message with a correctly specified address will be delivered to an existing person within a reasonable time interval is 1 (unless the recipient’s mailbox is full or the server is down, the probability of which are small), probability that a payment for an online transaction processed and approved through a payment gateway will be credited to the merchant’s account is 1. uncertain or unlikely events have probability near 0. for instance, the probability that an atm machine will eject an amount larger than requested by a customer is zero. a probability of 0.5 implies a “fifty–fifty” chance for the occurrence or nonoccurrence of an event. experimentalists, managers, and practitioners are more interested in probabilities that deviate much away from 0.5 owing to this simple reason."
1078,1,"['events', 'probability']", Event Categories,seg_253,there are many types of events encountered in probability problems.
1079,1,"['simple events', 'compound event', 'compound', 'cases', 'events', 'set', 'simple event', 'event', 'compound events']", Event Categories,seg_253,"1. simple and compound events a simple event (also called elementary event) cannot be decomposed into simpler events. all compound events are built using either simple events or other compound events using event operations. in most cases, a compound event can be sliced up into several fine-grained simple events. consider the working condition of a device. it may be defective (d) or nondefective (n). these are simple events. an event can comprise of a set of items. consider the tossing of two dice, each with six faces marked 1–6. the possible events are {(1,1), (1,2), ..., (6,6)}."
1080,1,"['disjoint', 'mutually exclusive', 'events', 'mutually exclusive events']", Event Categories,seg_253,"2. mutually exclusive events events are mutually exclusive if they are disjoint. symbolically, two events a and b are mutually exclusive or disjoint if a∩b =   or equivalently"
1081,1,"['disjoint', 'mutually exclusive', 'experiment', 'events', 'set', 'probability', 'event', 'tail', 'mutually exclusive events']", Event Categories,seg_253,"p(a∩b) = 0. in the coin-tossing experiment, the mutually exclusive events are head and tail. this definition can be extended to any number of events. let a1,a2, … ,an be n events. if n   2, they are totally mutually exclusive if p(a1 ∩ a2 ∩ · · · ∩ an) =  . if they are totally mutually exclusive, they need not be pairwise mutually exclusive. for instance, events a and b can have a common portion and b,c can have a common portion, but a and c can be disjoint. this implies that p(a ∩ c) =  . as   ∩ x is   where x is any other event, we can simply add events to totally mutually exclusive events. suppose b = a. incorporating b to p(a ∩ c) =   we get p(a ∩ b ∩ c) =   although a ∩ b = a. a set of events is minimally mutually exclusive if there are events a1,a2, … ,ak such that p(a1 ∩ a2 ∩ · · · ∩ ak) =  , but this relationship does not hold for any m   k. the mutually exclusivity property is extremely useful in decomposing some of the complex probability problems into simpler ones."
1082,1,['events'], Event Categories,seg_253,3. equally likely events (eles)
1083,1,"['prime number', 'cardinality', 'probability', 'random', 'sample', 'experiment', 'results', 'cases', 'events', 'event', 'sample space', 'outcomes', 'unbiased', 'independent', 'probabilities', 'experiments']", Event Categories,seg_253,"if every event of an experiment has an equal chance of occurring, they are called equally likely events (eles). probability problems are greatly simplified in such situations. examples are dice and coin tosses. consider the outcomes of an unbiased coin (with two possible events h and t), tosses of a six-faced die, a regular prism with four faces, or a regular pyramid with five faces. the probability of any event occurring in ele is one divided by the total number of events in the sample space. consider a regular five-faced pyramid with faces numbered from 1 to 5 that is tossed to a hard surface. as there is a unique face (at the bottom) on which it will rest, we can define an event as “face number that is hidden at the bottom.” then, each of the faces is equally likely with probability 1/5. two or more independent experiments with ele may be combined. let v denote an event defined on the sample space ω of n equally likely outcomes. then p(v) = |v|/n where |v| denotes the cardinality of v (number of favorable elementary events in v). consider two tosses of our pyramid. define an event that v=“the sum of the numbers at the bottom is even.” there are 5×5 = 25 total possibilities. the 13 favorable cases are (1,1), (1,3), (1,5), (2,2), (2,4), (3,1), (3,3), (3,5), (4,2), (4,4), (5,1), (5,3), and (5,5). hence p(v) = 13/25. next consider the toss of a fair die. let v be the event that the face that shows up is a prime number. the favorable cases are v = {1, 2, 3, 5} with |v| = 4. this gives p(v) = 4∕6 = 2∕3. these results are summarized in the following theorem. equally likely principle (elp): if the sample spaceω of a random experiment consists of a finite number of equally likely outcomes, then any non-null event e defined on ω has probability of occurrence |e|∕|ω|. these probabilities can easily be calculated directly using the count-and-conquer techniques or indirectly calculated using one of the do-little principles (section 5.6.4 (p. 131), section 5.15.4(p. 159))."
1084,1,['events'], Event Categories,seg_253,4. complementary events
1085,1,"['sample', 'events', 'complement', 'event', 'outcomes']", Event Categories,seg_253,complementary events are those that do not include the outcomes of another event. this complement operation is taken with respect to the entire sample
1086,1,"['sample', 'outcome', 'events', 'complement', 'set', 'probability', 'event', 'complement of an event', 'sample space', 'outcomes', 'complementary event']", Event Categories,seg_253,"space ω. this means that complement of an event belongs to the sample space. some probability problems can be substantially simplified by the complementary event principle. one common example is those problems that contain at least one outcome. if the number of outcomes is large, we could take the complement of the event to get a simple favorable set. a related operator is event difference (denoted by −), which is taken with respect to another event. thus, x-y represents an event that contains events in x but not in y. consider the event v defined on the toss of a fair die. the complementary event comprises vc = {4, 6}."
1087,1,"['independent', 'combinations', 'dependent', 'events', 'independent events', 'dependence', 'event']", Event Categories,seg_253,"5. dependent and independent events events may be dependent on one another. this dependence can be due to a shared property or some underlying commonalities. two or more events are independent if the occurrence of one in no way affects the occurrence of others. consider the following examples: (i) let x denote the event that an e-commerce customer at a video store has “red hair” and y denote the event that the order is for “adventure movie.” then, x and y are independent. (ii) consider a school kid with six shirts, five pants, and three ties. wearing any of the combinations are independent events. however, the decision to choose a shirt that matches the color of the pants or a tie that matches the shirt color may preclude some possibilities. thus, the events may be considered as dependent. see 5.15.7 in page 163 for further discussion."
1088,1,"['sample', 'condition', 'experiment', 'geometric distribution', 'conditional', 'dependent', 'distribution', 'events', 'probability', 'event', 'tail', 'sample space', 'geometric']", Event Categories,seg_253,"6. conditional events events may depend conditionally on other events. these are called conditional events. in other words, you have knowledge that some other event has occurred. this filters out a subset of the sample space, thereby reducing the computational burden. consider a simple experiment of throwing a fair coin until you get the first head. as the coin is fair, probability of getting a head in the first throw is 1/2. the event of head occurring in the second throw is conditional on the first throw resulting in a tail. similarly for subsequent events. these are the sequence of events considered in geometric distribution. consider an electronic board with a parallel circuit in each of which there are three components. if any component in one circuit fails, the device will continue operating. however if at least one component in both circuits fails, the device will stop working. thus, the nonworking condition of a device is conditionally dependent on both circuits in the board."
1089,1,"['sample', 'combination', 'discrete', 'events', 'complement', 'event', 'sample space']", DoLittle Principle for Events,seg_255,complementary events are sometimes easy to find when the sample space consists of a large number of discrete events as in the above-mentioned example. these are especially true in “at least k” and “at most k” type problems (or a combination of both) that can be considerably simplified by taking the complement or opposite event. it is also called complement-and-conquer principle. it has two versions—a count version
1090,1,"['events', 'probability']", DoLittle Principle for Events,seg_255,to count the complementary events and a probability version to obtain the probability of complementary events. both of them are related.
1091,1,"['logical operators', 'compound', 'set', 'probability', 'random', 'associated', 'sample', 'experiment', 'events', 'event', 'sample space', 'compound events', 'union', 'outcomes', 'complement', 'combinations']", EVENT ALGEBRA,seg_257,"every random experiment involves two or more events. events are usually combined using the logical operators {and, or, not}. combining events using or operator often increases the probability, while combining events using and operator often decreases the probability. if a and b are two events, p(a or b) = p(a) only when b is a proper subset of a. similarly p(a and b) = p(a) only when a and b are exactly identical. the not operator may or may not increase the probability—this depends on how big a chunk of the sample space is spanned by the defined event. the events resulting from applying a not operator to another event is called its complement. for example, consider the throw of a six-faced die numbered 1–6. if event x is defined as “an even number shows up,” then it has probability 0.5 as the possible outcomes are x = {2, 4, 6}. its complement event also has probability 0.5, as the not operator returns xc = {1, 3, 5}. next consider the event “the number that shows up is a prime.” this has associated probability 4/6 = 2/3 as the possible outcomes are {1, 2, 3, 5}. its complement event consists of {4, 6} with probability 2/6 = 1/3. here, the complementation has reduced the probability. unless otherwise stated, the complement is always taken with respect to the entire sample space ω. in other words, the not operator is to be interpreted as anything that remains in the sample space other than those in the considered event or subspace. sometimes, we seek the probability of occurrence of event combinations. events can be combined using set theoretic operations union (∪), intersections (∩), complements, and differences. any of these can be combined to produce compound events."
1092,1,"['intersection', 'discrete', 'set theory', 'events', 'set', 'joint', 'union', 'probability', 'event']", Laws of Events,seg_259,"there are several laws of events that are direct descendants of corresponding laws of set theory. these laws are helpful to solve some of the discrete probability problems. more importantly, they form the theoretical foundations on which classical probability theory is built up. most of the axioms of probability are direct generalizations of corresponding event axioms. union of events represents the occurrence of either or both of them, whereas intersection of two events represents their joint occurrence (both occur)."
1093,0,[], Laws of Events,seg_259,"consider a commercial flight that is scheduled for departure at a fixed time. a delay in departure can happen due to many reasons:–(i) technical problems with the aircraft, (ii) delay in one or more connecting flights that have passengers for current flight, (iii) delay of flight crew in reporting for duty (iv) delay in security checking, (v) delay due to runway problems or congestion, and (vi) other reasons."
1094,1,"['independent', 'intersection', 'events', 'union']", Laws of Events,seg_259,assume that each of these is independent occurrences. form meaningful events using union and intersection of events.
1095,1,"['independent', 'combination', 'events', 'union', 'event']", Laws of Events,seg_259,"solution 5.17 let t, v, w, x, y, and z denote each of the six events. as each of them are independent, a union represents an “either or” situation. for example, t ∪ v represents the event that the flight is delayed either due to technical problems with the aircraft or connecting flights are delayed. the event t ∪ v ∪ w ∪ x ∪ y denotes the event that there is a likely flight delay due to the occurrence of individual events or a combination of events mentioned. the event t ∩ v denotes that there is technical problem with the aircraft and connecting flights are delayed. if all events are represented by single character labels, we could drop the ∩ operator and represent multiple event occurrences by a concatenated label. for instance, w ∩ x ∩ y can be represented as wxy. this is just a new name or label given to a combination of events."
1096,1,"['bayes theorem', 'sets', 'set', 'law of total probability', 'probability', 'total probability', 'bayes']", Laws of Events,seg_259,"5.7.1.1 law of total probability let x and y be two nonempty sets with common elements. then, we can partition the set x into two parts as x = x ∩ y + x ∩ y , where x ∩ y contains members of x with both traits and x ∩ y contains members of x without the trait of y. in terms of probability, this is written as p(x) = p(x ∩ y) + p(x ∩ y). similarly, y = y ∩ x + y ∩ x gives p(y) = p(y ∩ x) + p(y ∩ x). this result is used in the derivation of bayes theorem."
1097,1,"['disjoint', 'event algebra', 'commutative laws', 'states', 'set theory', 'events', 'set', 'event', 'commutative law']", Laws of Events,seg_259,"5.7.1.2 commutative laws the literal meaning of “commutative” is “unchanged in result by a reordering of operands.” these are meaningful for binary operators that take two operands. these laws are formed in event algebra and set theory using ∪ and ∩ set-theoretic operators. simply put, these laws state that the events on either side of these operators can be swapped. let x and y denote two events. then, the commutative law states that x ∪ y = y ∪ x and x ∩ y = y ∩ x. in example 2.17, t ∪ v and v ∪ t represent the same event. similarly, w ∩ x and x ∩ w represent the same thing. humans can easily conceive the meaning of these expressions by perception. this is especially easy when the events are disjoint. however, the law holds even when two events overlap. they are more useful when more than two events are involved."
1098,0,[], Laws of Events,seg_259,example 5.18 weight-loss program
1099,1,"['combinations', 'combination', 'event', 'average', 'commutative law']", Laws of Events,seg_259,"consider a weight-loss clinic for over fat people that offers three programs:– (i) restricted diet (rd) program that can decrease the weight on the average by 10 pounds in 4 weeks, (ii) a fat-burning exercise regime (er) with a thermal belt that sheds on the average 8.5 pounds in 4 weeks, (iii) a bariatric surgery (bs) program that sheds on the average 16 pounds in 4 weeks. a patient can opt for either individual programs or for a combination. does the commutative law make sense in this example? describe the following event combinations in plain english:– (i) er∩bs and (ii) rd∪bs."
1100,1,"['intersection', 'events', 'union']", Laws of Events,seg_259,"solution 5.18 as events represent various programs offered by the clinic, the intersection of events indicate the programs for which a patient has opted. thus, er∩bs indicates that a patient is registered in both er and bs programs. however, union of events in this problem does not make sense. if the events are defined in terms of counts (total number of people registered for the program), then union of events make sense. for instance, er∪bs indicates the total number of people registered for either of the programs or both."
1101,1,"['disjoint', 'associative laws', 'associative law', 'states', 'case', 'events', 'commutative law']", Laws of Events,seg_259,"5.7.1.3 associative laws let x, y, and z denote three distinct events. then, the associative law states that x ∪ (y ∪ z) = (x ∪ y) ∪ z and x ∩ (y ∩ z) = (x ∩ y) ∩ z. the meaning of each of them is that the flight is delayed due to a delay in security checking or a delay due to runway problems or congestion or due to other reasons. here, the operator that is commuted is the same. as in the case of commutative law, humans can easily conceive the meaning when the events are disjoint."
1102,1,"['distributive laws', 'distributive law', 'combinations', 'states', 'events', 'event']", Laws of Events,seg_259,"5.7.1.4 distributive laws the name distributive comes from the fact that two nonidentical event combinations are simplified by distributing one of the operators. let x, y, and z denote three events. then, the distributive law states that"
1103,1,"['sample', 'intersection', 'compound', 'events', 'sample space', 'compound events']", Laws of Events,seg_259,where the ∪ operator outside the parenthesis in the first expression is distributed and the∩ operator outside the parenthesis in the second expression is distributed. this law is more meaningful when some events have a “combined effect.” set-theoretically this means that the intersection of some of the events is non-null. these rules are extremely useful in reducing the favorable sample space of compound events.
1104,1,"['distributive law', 'case', 'associative law']", Laws of Events,seg_259,"in the case of associative law, we had the same operator (either ∪ or ∩). if the operators are different, we get the distributive law given below discussion."
1105,1,['distributive laws'], Laws of Events,seg_259,example 5.19 distributive laws
1106,1,"['distributive law', 'statistics']", Laws of Events,seg_259,"consider the pre-requisite courses for enrolling in a statistics course. a student who has finished college algebra (x) is eligible, as also those who have finished both of computer science 100 (y) and maths 104 (z). express this using distributive law."
1107,1,"['events', 'condition']", Laws of Events,seg_259,"solution 5.19 label the events as x, y, and z. then, the condition can be expressed as x ∪ (y ∩ z)."
1108,1,"['combinations', 'event']", Laws of Events,seg_259,example 5.20 event combinations
1109,1,"['combinations', 'event']", Laws of Events,seg_259,consider example 5.18 given earlier. describe the following event combinations in plain english:– (i) rd ∩ (bs ∪ er) and (ii) (er ∩ rd) ∪ bs
1110,0,[], Laws of Events,seg_259,"solution 5.20 as the operator inside the bracket in (i) is ∪, the meaning is to select a weight-loss program with rd and either bs or er. similarly, the meaning of (ii) is to opt for a program with either bs alone or both er and rd."
1111,1,"['compound', 'events', 'complement', 'compound events']", DeMorgans Laws,seg_261,"these laws relate the complement of compound events in terms of individual complements. in the following section, we use overline to denote complements."
1112,1,"['complement', 'union', 'intersection']", DeMorgans Laws,seg_261,rule 1 complement of an intersection is the union of their complements.
1113,1,"['events', 'percentage']", DeMorgans Laws,seg_261,"let a and b be two arbitrary events. then a ∩ b = a ∪ b. consider the newspaper readership problem. if there are just two newspapers and the percentage of people who read both of them are known, the percentage of people who read neither of them can be found using the above-mentioned law."
1114,1,"['complement', 'union', 'intersection']", DeMorgans Laws,seg_261,rule 2 complement of a union is the intersection of their complements.
1115,1,['events'], DeMorgans Laws,seg_261,"symbolically, a ∪ b = a ∩ b. these rules can be extended to any number of events"
1116,1,"['venn diagrams', 'venn']", DeMorgans Laws,seg_261,i=1ai. these are proved using induction and venn diagrams.
1117,1,"['sample', 'linear', 'sample spaces', 'discrete', 'observation', 'events', 'probability']", BASIC COUNTING PRINCIPLES,seg_263,"a great majority of probability problems can be solved by a good mastery of a few counting principles. these are more applicable to discrete sample spaces in 1d and 2d than for others. they are intended to count the number of objects, events, possibilities, occurrences, or arrangements that satisfy zero or more properties or constraints. there are a myriad of constraints possible. these may be related to adjacency, occupancy, linear or circular arrangement, observation of some events, and so on."
1118,1,"['disjoint', 'mutually exclusive', 'case', 'cases', 'set', 'events', 'event']", Rule of Sums ROS,seg_265,"this is also known as the principle of disjunctive counting. consider a set s of objects that has been divided into disjoint subsets s1, s2, … , sm so that s = s1 ∪ s2 ∪ · · · ∪ sm. if there are n1 favorable cases for an event in s1, n2 favorable cases for the same event in s2, and so on, nm favorable cases for the same event in sm, then the total number of favorable cases for the event in s is n1 + n2 + · · · + nm. symbolically, this can be written as |s| = |s1 ∪ s2 ∪ · · · ∪ sm| = |s1| ∪ |s2| ∪ · · · ∪ |sm|. another way to state it is as follows:– there are m cases or events with no common options (i.e., they are mutually exclusive). if ith case or event can occur in ni ways, then the total number of options or ways in which one of them can occur is n1 + n2 + · · · + nm. the principle of inclusion and exclusion (p. 158) is an extension when at least two of the subsets have common elements."
1119,0,[], Rule of Sums ROS,seg_265,example 5.21 breakfast choices
1120,0,[], Rule of Sums ROS,seg_265,"the mcdonalds restaurant offers eight varieties of breakfast, whereas burger king offers six varieties. joe has a choice of going either to mcdonalds or to burger king on any day (but not both) for breakfast. how many choices of breakfast are possible?"
1121,1,['rule of sums'], Rule of Sums ROS,seg_265,"solution 5.21 this problem can be cast using rule of sums (ros), where s1= the choices available at mcdonald’s and s2 = the choices available at burger king. total possible choices are |s1| + |s2| = 8 + 6 = 14."
1122,1,"['independent', 'experiment', 'cases', 'probability', 'outcome', 'experiments', 'outcomes']", Principle of Counting POC,seg_267,"this is also called multiplication law of counting (mloc) or multiplication principle. it has direct applications in counting the number of occurrences of outcomes of experiments such that the first experiment can result in n1 possible outcomes, and for each outcome, there exist another independent experiment with n2 possible outcomes, and so on. it is also useful in classical approach to probability in which we need to count the favorable number of cases of an experiment."
1123,0,['n'], Principle of Counting POC,seg_267,"lemma 1 if one thing (or activity) can be done in “m” ways, and another in succession in “n” ways, the two together can be done in m ∗ n different ways."
1124,0,[], Principle of Counting POC,seg_267,example 5.22 computer file types
1125,0,[], Principle of Counting POC,seg_267,"a software allows an image to be saved in three different file types (as .jpeg, .gif, or .tiff) in four different resolutions. how many possible ways are there to save the image?"
1126,1,['independent'], Principle of Counting POC,seg_267,"solution 5.22 as the file types and resolutions are independent, there are 3 * 4= 12 different ways to save the image."
1127,0,[], Principle of Counting POC,seg_267,"a car manufacturer offers eight exterior colors and four interior designs. how many varieties of cars can be produced if (i) each of them can be manufactured in luxury and sedan models? (ii) if each of them can be made as petrol, diesel and hybrid (electric) versions?"
1128,1,"['design', 'events', 'case']", Principle of Counting POC,seg_267,"solution 5.23 we define three events as follows:– (i) e= {exterior color}, (ii) i= {interior design}, and (iii) m= {model}. as the number of possibilities for e is 8, i is 4, and m is 2, by the principle of counting there exist 8× 4× 2= 64 possible choices. for case (ii), there are three types (petrol, diesel, and hybrid) so that the number of ways is 8× 4× 3= 96."
1129,0,[], Principle of Counting POC,seg_267,"in some problems, we may have to combine both ros and poc multiple times to reach a final result. this is illustrated in the following example."
1130,0,[], Principle of Counting POC,seg_267,example 5.24 multiple choice exam
1131,0,[], Principle of Counting POC,seg_267,"a multiple choice exam has 15 questions, each with 4 answer choices (say a, b, c, and d). how many possible ways are there to answer the questions assuming that multiple markings are not allowed, and (i) all questions are to be answered and (ii) questions can be skipped (kept unanswered)."
1132,1,['combination'], Principle of Counting POC,seg_267,"solution 5.24 as the questions are independently answered, any of the questions can be marked in four ways. hence, the total possible answer combination"
1133,1,"['case', 'combinations']", Principle of Counting POC,seg_267,"in (i) is 415. for instance, if there are just 2 questions, the 16 answer choices are (a,a), (a,b), (a,c), (a,d), (b,a), (b,b), (b,c), (b,d), (c,a), (c,b), (c,c), (c,d), (d,a), (d,b), (d,c), and (d,d), where the first letter denotes the answer for question-1 and second letter is the answer for question-2. in case (ii), suppose that k questions are answered and (15-k) are skipped. as any of the questions can be answered in four ways, there are 4k answer combinations for k answered questions. however, the k questions can be any among the 15 questions. a stu-"
1134,0,[], Principle of Counting POC,seg_267,dent could select arbitrary k questions in ( 1 k
1135,1,['combinations'], Principle of Counting POC,seg_267,"145). by the multiplication principle, the total number of possible combinations"
1136,1,['range'], Principle of Counting POC,seg_267,5) ∗ 4k. by summing this expression over the possible range of k gives
1137,0,[], Principle of Counting POC,seg_267,"5) ∗ 4k. here, k = 0 means that none of the questions"
1138,0,[], Principle of Counting POC,seg_267,"are answered. this can be done in just one way. similarly, k= 15 means that all"
1139,0,[], Principle of Counting POC,seg_267,questions are answered (in one way only). this is of the form ∑k
1140,1,"['binomial', 'combinations']", Principle of Counting POC,seg_267,"n =0 ( n k) ∗ xk, which is the binomial expansion of (x + 1)n. thus, the above-mentioned sum is (4 + 1)15 = 515. if there are just two questions, we have nine new combinations in addition to the 16 listed earlier as (*,a), (*,b), (*,c), (*,d), (a,*), (b,*), (c,*), (d,*), and (*,*), where “*” indicates an unanswered question and (*,*) means that both questions are skipped. this gives a total of 16 + 9 = 25 = 52 combinations."
1141,0,[], Principle of Counting POC,seg_267,example 5.25 cloth washing
1142,0,[], Principle of Counting POC,seg_267,"a schoolchild has 10 colorless and 6 colored dresses to be washed on a weekend. colored dresses are of two types—red and blue. both of them cannot be loaded into the same washing cycle due to color dissolving. the color-less dresses can be washed in any of the three settings: (i) hot, (ii) lukewarm, and (iii) cold and rinsed after the wash cycle in two settings (lukewarm-rinse or cold-rinse); whereas the colored dresses of same color can all be washed and rinsed in a cold or lukewarm wash only. how many ways are there to wash all the clothes?"
1143,1,['cases'], Principle of Counting POC,seg_267,"solution 5.25 this problem is most easily solved using a tree. there are two branches at the top for colored and colorless. the colored branch is further broken down as red and blue. first consider colorless dresses. they cannot be mixed with colored ones due to color staining. thus, there exist three ways to wash them and two ways to rinse them. by the poc, there exist 3× 2= 6 ways to wash them. next consider colored clothes. how many of the colored ones are red or blue is not known. let c of them be red and 6-c blue. the c red ones can be washed in four ways {c-c, c-l, l-c, l-l}, where c indicates a cold and l indicates a lukewarm wash or rinse in the first and second place. similarly 6-c blue clothes have four washing choices. thus, there are eight choices for the colored clothes. adding both cases together, we get the answer as 6+ 8= 14 choices."
1144,1,"['probabilities', 'table', 'experiments', 'outcomes']", Principle of Counting POC,seg_267,"in some experiments, each of the outcomes has an equal number of occurrences (see table 5.2). in other words, the probabilities are equally likely. these are much easier to solve as illustrated in the following."
1145,0,[], Principle of Counting POC,seg_267,example 5.26 car license plates
1146,0,[], Principle of Counting POC,seg_267,a car license plate comprises of two english capital letters followed by four digits. how many license plates are possible if (i) each of the letters and digits can be repeated and (ii) only digits can be repeated.
1147,1,['case'], Principle of Counting POC,seg_267,"solution 5.26 there are 26 capital english letters and 10 digits (between 0 and 9). as repetitions are allowed for (i) there are (26 × 26) × (10 × 10 × 10 × 10) = 676 × 104 possible ways. our assumption is that lower case letters are not used on license plates (which need not be true in some countries). as letter repetitions are not allowed for (ii), there are (26 × 25) × (10 × 10 × 10 × 10) = 650 × 104 possible ways."
1148,0,[], Principle of Counting POC,seg_267,example 5.27 cylindrical number lock
1149,1,['set'], Principle of Counting POC,seg_267,"consider a cylindrical number lock on a briefcase with three wheels or rings. assume that each of the wheels is marked with the digits 0–6 (total seven digits or rollings possible). using a lever, a user can set the lock to any desired number (formed by the three digits chosen in succession from the wheels, say from left to right). what is the total number of possible lock codes?"
1150,1,"['independent', 'combinations']", Principle of Counting POC,seg_267,"solution 5.27 as there are three independent wheels, each with seven possibilities, the total number of combinations is 7 × 7 × 7 = 343. thus, the briefcase can be locked in 343 possible ways."
1151,1,"['method', 'discrete', 'sets', 'probability']", Complete Enumeration,seg_269,"as the name implies, this method enumerates (count one by one) all possibilities. this is more relevant in discrete probability problems involving throws of a coin or dice, arrangement of digits, alphabets, assignments of elements in two finite sets, spin of a numbered wheel, and so on. sometimes, we need to enumerate only a small subset by eliminating commonalities as in the following problem."
1152,0,[], Complete Enumeration,seg_269,"how many ways are there for a leap year (with 366 days) to have (i) 53 sundays?, (ii) 53 saturdays and 53 sundays?, (iii) exactly 52 saturdays and 52 sundays?, (iv) exactly 53 fridays or 53 sundays?, and (v) exactly 52 tuesdays and 52 thursdays?"
1153,1,"['complete enumeration', 'cases', 'complement', 'combinations']", Complete Enumeration,seg_269,"solution 5.28 as 52 × 7 = 364, every year will have 52 weekdays each for sure. there is an extra day (strictly speaking 1.24219879 days) for nonleap years, and there are two extra days in leap years (the 0.24 days add up to approximately 1 day in 4 years and is counted as february 29 to get a leap-year). as these two extra days are consecutive, we can do a complete enumeration of these days as (sunday, monday), (monday, tuesday), (tuesday, wednesday), (wednesday, thursday), (thursday, friday), (friday, saturday), and (saturday, sunday). these are the only seven possible combinations for the extra 2 days. (i) as sunday occurs in two of the seven combinations, the number of ways a leap year will have 53 sundays is 2. (ii) as (saturday, sunday) occurs once, the desired number of favorable cases is 1. (iii) neither saturday nor sunday occurs in four out of the seven possible pairs. (iv) there exist four pairs containing either a friday or a sunday. (v) there are three favorable cases, namely, (sunday, monday), (friday, saturday), and (saturday, sunday) using the complement rule."
1154,0,[], Complete Enumeration,seg_269,example 5.29 roots of quadratic equation
1155,1,['coefficients'], Complete Enumeration,seg_269,"consider a quadratic equation px2 + qx + r = 0, whose nonzero coefficients (p, q, r) are determined by the number that turns up when a die with six faces numbered 1–6 is thrown. find the number of ways in which (i) the equation will have real roots, (ii) equal roots, (iii) imaginary roots, (iv) both integer roots, and (vi) exactly one integer root."
1156,1,"['condition', 'complete enumeration', 'case', 'cases', 'coefficients']", Complete Enumeration,seg_269,"solution 5.29 as each of the coefficients p, q, r is determined by the number that shows up in the throw of a die, we need three throws to decide them (say choose p first, then q, and finally r). we do a complete enumeration as follows. as each of them can be in {1, 2, 3, 4, 5, 6}, there exist a total number of 6 × 6 × 6 = 63 = 216 equations by the poc. (i) we know that the condition for real roots is q2 − 4pr ≥ 0. as repetitions are allowed, the least value of 4pr is 4 × 1 × 1 = 4. however, q2 is greater than 4 when q = 3, 4, 5, 6. this means that there exist four favorable cases when p= r= 1 (and five cases if q = 2 is also counted in which case we have equal roots). next consider (p= 2 and r= 1) or (p= 1 and r= 2). in both cases 4pr= 8, and q2 is greater than 8 when q = (3, 4, 5, 6). proceed similarly with (p= 3 and r = 1), (p= 1 and r= 3), or (p= 2 and r= 2). in the first two cases 4pr = 12 and q2 is greater than 12 for q= (4, 5, 6). for (p= 2 and r= 2), (p= 1 and r= 4), or (p= 4 and r= 1) 4pr= 16 and q= (5,6). for (p= 1 and r= 5) or (p= 5 and r= 1) 4pr= 20 and q= (5,6). similarly for (p= 1 and r= 6), (p= 6 and r= 1), (p= 2 and r= 3) and (p= 3 and r= 2), 4pr= 24 and q= (5,6). finally"
1157,1,['cases'], Complete Enumeration,seg_269,"for (p= 2 and r= 4) or (p= 4 and r= 2), 4pr= 32 and q= (6). for (p= 3 and r= 3), real roots are not possible as 4pr= 36. summing the counts, we get the total number of cases as 38. in addition, there are five cases (given the following discussion) for the roots to be equal."
1158,1,"['cases', 'complement', 'case', 'table']", Complete Enumeration,seg_269,"there exists (38 + 5) = 43 ways (see table 5.3). in case (ii), the five favorable cases are (1,2,1), (1,4,4), (2,4,2), (3,6,3), and (4,4,1). a quadratic equation can have either real roots or equal roots or imaginary roots only. hence, the favorable cases for (iii) can be directly obtained using complement rule as 216 − 43 = 173. consider case (iv). both roots are integers in two cases: (a) both q and q2 − 4pr are odd and (b) both are even. the 10 favorable cases are (1,2,1),(1,3,2),(1,4,3),(1,4,4),(1,5,4),(1,5,6),(1,6,5),(2,4,2), (2,6,4),(3,6,3). (v) the eight favorable cases are (2,3,1), (2,5,2), (2,5,3), (3,4,1), (3,5,2), (4,5,1), (4,6,2), and (5,6,1)."
1159,1,"['permutations', 'linear', 'permutation', 'set', 'probability']", PERMUTATIONS AND COMBINATIONS,seg_271,"the literal meaning of permutation is an ordering or arrangement. mathematically, a permutation of a set s is a one-to-one mapping of s onto itself. in other words, it is the total number of arrangements of a set of elements. the elements being arranged are all uniquely distinguishable to the human eye. this arrangement can be linear (in 1d) or circular (in 2d space). an ordered subset of a larger set is also called a permutation. as a great majority of probability problems are valid in 1d or 2d only, we will not discuss higher dimensional permutations. consider a set of three students {amy, john, and mary}. denoting each of them by their first letter, there are six ways to arrange them linearly as {a,j,m}, {a,m,j}, {j,a,m}, {j,m,a}, {m,a,j}, and {m,j,a}. these are the only possible linear permutations."
1160,1,"['permutation', 'set']", PERMUTATIONS AND COMBINATIONS,seg_271,"definition 5.8 a permutation is an arrangement of the whole or part (with at least two elements) of a finite set of distinguishable elements without repetition, where the order is considered as important."
1161,1,"['permutation', 'permutations', 'replacement']", PERMUTATIONS AND COMBINATIONS,seg_271,"there is no universally accepted notation for permutations. the four most widely used notations are npr,pr,n,p(r, n) and pnr , where both n and r are integers such that r ≤ n. permutation can also be interpreted as selection of elements from a group without replacement."
1162,1,['permutations'], PERMUTATIONS AND COMBINATIONS,seg_271,theorem 5.1 prove that the total number of permutations of r objects from among n distinguishable objects is npr where r ≤ n.
1163,1,['set'], PERMUTATIONS AND COMBINATIONS,seg_271,"proof: as there are n elements initially, there exist n ways to choose the first element. now there are (n − 1) elements remaining, as one element is already removed from the set. thus, there are (n − 1) ways to select the second object. continuing in like fashion r times, we see that there are"
1164,1,"['table', 'without replacement', 'replacement', 'factorial', 'samples', 'pochhammer notation', 'falling factorial']", PERMUTATIONS AND COMBINATIONS,seg_271,ways to choose r objects from n objects ((n)r is the pochhammer notation for falling factorial). this is the same as the number of samples of size r without replacement from n distinguishable objects (see table 5.4).
1165,1,['set'], PERMUTATIONS AND COMBINATIONS,seg_271,lemma 2 a set of n distinguishable objects can be linearly arranged among themselves in n! ways.
1166,0,['n'], PERMUTATIONS AND COMBINATIONS,seg_271,"proof: mark the positions of n objects. the first position can be filled by any of them (in n possible ways). once this position is filled, there are (n − 1) objects remaining and (n−1) positions to put them into. next we fix second of the (n−1) possible positions. there are (n-1) ways to choose an object to this position. thus, the first two positions can be filled in n*(n−1) ways. continuing this way we find that for filling the last position, we have only one choice. hence, the total number of ways to fill all the positions is npn = n ∗ (n − 1) ∗ (n − 2) ∗ · · · ∗ 2 ∗ 1 = n!∕(n − n)! = n! ways (as 0! = 1 by convention)."
1167,0,[], PERMUTATIONS AND COMBINATIONS,seg_271,example 5.30 national flags
1168,0,[], PERMUTATIONS AND COMBINATIONS,seg_271,a political summit is attended by delegates from five countries. all five national flags are to be arranged in a row at the entrance. in how many ways can this be done?
1169,0,[], Permutations with Restrictions,seg_273,"in most of the practical applications, we have restrictions on the elements. the most common restriction is duplicates (property restriction) discussed in the following. other types of restrictions include adjacency restrictions, locational restrictions (such as fixed positions), and end point (extreme position) restrictions."
1170,0,[], Permutations with Restrictions,seg_273,example 5.31 book arrangement
1171,0,[], Permutations with Restrictions,seg_273,"a schoolchild has five books, one each on mathematics, gaming, english, physics, and biology. how many ways are there to arrange the books on a rectangular rack if (i) no order is maintained among them?, (ii) the leftmost book must be gaming book?, (iii) the left-most and right-most places are occupied by mathematics and physics books?, (iv) physics and mathematics books are always adjacent, and (v) english and biology books cannot be next to each other?"
1172,1,"['complement', 'case', 'event']", Permutations with Restrictions,seg_273,"solution 5.31 (i) if no order is maintained among them, the possible number of ways is 5! = 120. in case (ii), the gaming book occupies a fixed position. there are four books to be arranged. this can be done in 4!= 24 ways. in case (iii), two places are preoccupied. the remaining three books can be arranged in 3! = 6 ways. in case (iv), we consider physics and mathematics as a single logical bundle. then, it reduces to arranging four books among themselves. this can be done in 4! = 24 ways. the easiest way to tackle (v) is using the do-little principle (complement-and-conquer). we consider the complement event that english and biology books are together. as it is similar to case (iv), there are 24 possibilities. the required answer is then found using the complement as 5! − 4! = 120 − 24 = 96."
1173,0,['n'], Permutation of Alike Objects,seg_275,"if there are n things of which n1 of them are of one kind, n2 of them are of another kind, … , nk of them are of kth kind, then there are"
1174,1,"['coefficient', 'permutations', 'multinomial']", Permutation of Alike Objects,seg_275,"different permutations, where n = n1 + n2 + · · · + nk. this is called the multinomial coefficient. it can alternately be written as"
1175,0,[], Permutation of Alike Objects,seg_275,"where the missing operator is a “*.” some of the ni in this theorem can be one. when all of the ni’s are 1’s, the denominator reduces to 1 and we get the number"
1176,1,['permutations'], Permutation of Alike Objects,seg_275,"of permutations as n!. this result can be stated alternately as follows. the number of possible divisions of n distinct objects into r groups of respective sizes n1, n2, … , nr"
1177,1,"['multinomial', 'coefficients']", Permutation of Alike Objects,seg_275,"n is ( n1,n2,… nr ) = n!∕[n1! ∗ n2! ∗ · · · ∗ nr!]. multinomial coefficients are further discussed in chapter 6."
1178,0,[], Permutation of Alike Objects,seg_275,example 5.32 shelving of books
1179,1,['statistics'], Permutation of Alike Objects,seg_275,"a library has received a shipment of 12 books of which 2 are duplicate copies of a statistics book, 4 are duplicate copies of a database book, and 3 each are duplicate copies of c programming book and java programming book. these need to be kept on a reserve shelf. in how many ways, can this be arranged?"
1180,0,[], Permutation of Alike Objects,seg_275,example 5.33 train coaches
1181,0,[], Permutation of Alike Objects,seg_275,a train has five ordinary coaches and three ac coaches in addition to an engine. how many ways are there to connect the coaches if the engine is always at the front?
1182,1,['permutations'], Permutation of Alike Objects,seg_275,"solution 5.33 this problem is most easily solved using permutations of alike objects. the eight coaches can be considered as objects of which five are of one kind and three are of another kind. hence using the above-mentioned theorem, the total number of possibilities is 8!/(5!*3!) = 336/6 = 56 ways."
1183,1,"['permutation', 'permutations', 'table']", Cyclic Permutations,seg_277,"if the permutations of distinguishable objects occur along a “logical circle,” it is called cyclic or circular permutation. here, logical circle means that the objects can be thought of as forming an imaginary circle (although physically it can be any closed shape including a triangle, square, rectangle, or pentagon). the circle can be rotated by fixing the objects in place. hence, it is not the circle, but the order of occurrence of objects that is more important. as an example, if several people sit around a rectangular table, we could consider it as a logical circle as long as the sides of the table are not distinguished or considered with respect to the persons."
1184,1,['permutations'], Cyclic Permutations,seg_277,lemma 3 the number of permutations of n distinguishable objects along a circle is (n − 1)!.
1185,0,['n'], Cyclic Permutations,seg_277,proof: keep any one of the objects as fixed. there are (n − 1) others remaining. they can be arranged among themselves in (n − 1)! ways.
1186,0,[], Cyclic Permutations,seg_277,example 5.34 roundtable seating
1187,1,['table'], Cyclic Permutations,seg_277,"how many ways are there to seat four people w,x,y,z around a circular table?"
1188,0,[], Cyclic Permutations,seg_277,example 5.35 circular arrangement
1189,1,['table'], Cyclic Permutations,seg_277,"suppose that n boys and n girls are to be seated around a circular table. (i) how many ways can this be done if no two of the same sex are seated next to each other, (ii) there are no restrictions on males and females?, and (iii) three or more pairs of boys cannot be together, but at most two pairs of boys are allowed?"
1190,1,"['permutation', 'complement', 'case']", Cyclic Permutations,seg_277,"solution 5.35 first fix the n boys around the circle with an empty chair between them. this can be done in (n − 1)! ways, as they can be rearranged among themselves using circular permutation formula given earlier. as there are n empty chairs, the n girls can be circularly arranged in (n − 1)! ways. this gives a total of (n − 1)!2 possible ways. (ii) if there are no restrictions, we need to arrange 2n persons along a circle. this can be done in (2n − 1)! ways. case (iii) is most easily solved using the do-little principle. the complement of the problem is to find the number of ways in which any three males are together. mark the group of three males by m. then, there are n − 3 remaining males (plus one m). they can be arranged among themselves in (n − 3 + 1 − 1)! = (n − 3)!"
1191,0,[], Cyclic Permutations,seg_277,ways. as the three males can be fixed in ( 3
1192,0,['n'], Cyclic Permutations,seg_277,"n) ways, the total number of ways is"
1193,1,['complement'], Cyclic Permutations,seg_277,n) ∗ (n − 3)! ways. take complement from (n − 1)!2 ways to get the desired
1194,0,['n'], Cyclic Permutations of Subsets,seg_279,"consider n distinct objects. if r is an integer between 1 and n, there are (r − 1)! ( n"
1195,0,[], Cyclic Permutations of Subsets,seg_279,r) different ways to circularly permute the r objects.
1196,0,[], Cyclic Permutations of Subsets,seg_279,example 5.36 train coaches
1197,0,[], Cyclic Permutations of Subsets,seg_279,"a train has five ordinary coaches and three ac coaches in addition to the engine. how many ways are there to connect the coaches if (i) the coach immediately behind the engine and the rear-end coach are both ordinary?, (ii) if all ac coaches cannot be together?, (iii) at most three ordinary coaches can be together?"
1198,1,"['permutations', 'case']", Cyclic Permutations of Subsets,seg_279,"solution 5.36 denote ordinary coach by o and ac coach by c. this problem has restrictions. in case (i), two of the five “o coaches” are fixed. this leaves three o and three c coaches remaining to be connected. using permutations"
1199,1,"['complementary event', 'event']", Cyclic Permutations of Subsets,seg_279,"of alike objects, the answer is 6!/(3!*3!)= 20 ways. (ii) fix the five o coaches with an empty space in-between them. there are two extra empty spaces at the beginning and end (extremes behind the engine). this gives a total of six empty spaces where we could place three c coaches together. total number of ways in which five o coaches and three c coaches can be connected together is 8!/(5!*3!)= 56 ways. by subtracting the number of ways in which all the ac coaches are together, we get the answer to part (ii) as 56 − 6 = 50. (iii) the complementary event of “at most three ordinary coaches can be together” is either four coaches are together or all five coaches are together. these are more easier to count. number of ways in which four coaches are together is found as follows: fix the three c coaches with a space in between them (including beginning and end). we can place four o coaches in four ways. the remaining one o coach can be placed in three ways. this gives a total of 4 × 3 = 12 ways. next consider all five coaches together. these can be placed in four different ways. by the ros principle, total number of ways for the complementary event is 12+ 4= 16 ways. hence, the desired number of ways is 8!/(5!*3!)−16= 56−16= 40 ways."
1200,1,"['permutations', 'combination', 'permutation', 'combinations']", Combinations,seg_281,"permutation is an arrangement technique in which the order of elements matters, but order of elements does not matter in combinations. this means that if x and y are two elements, xy and yx are considered the same in combination but not in permutations. the combination of n things taken r at a time was introduced in chapter 1. it is"
1201,0,['n'], Combinations,seg_281,denoted by ( n
1202,1,"['permutations', 'combination', 'without replacement', 'replacement']", Combinations,seg_281,"this denotes the number of ways in which r objects can be selected from n distinguishable objects without regard to order and without replacement. for a fixed r, there exist r! permutations that give the same combination. hence,"
1203,1,['cases'], Combinations,seg_281,particular cases are ( n
1204,0,[], Combinations,seg_281,example 5.37 pilot choices
1205,0,[], Combinations,seg_281,"a flight has to be scheduled using a pilot. there are 12 persons in the pool for the pilot among whom 8 are males, 5 of the 8 speak english and spanish, and the rest 3 speak english only. two of the four females speak english and spanish, and the rest of them speak english only. how many ways are there to select a pilot"
1206,0,[], Combinations,seg_281,"and a copilot such that (i) there is one male and one female, both speak spanish? (ii) two males, at least one of whom speak spanish?"
1207,1,"['events', 'event']", Combinations,seg_281,"solution 5.37 define events a, b as follows:–a: = event that the pilot candidate is male and b: = event that the candidate speaks spanish. we seek the number of possibilities of the event a ∩ b. there are five out of eight males who speak"
1208,0,[], Combinations,seg_281,spanish and english. the number of ways to choose a bilingual male is ( 1
1209,1,"['cases', 'product rule']", Combinations,seg_281,"similarly, number of ways to choose a bilingual female is 2. by the product rule, total number of ways is 5*2 = 10. (ii) as there are five males who speak spanish, we consider the two cases: (i) both chosen persons speak spanish. (ii) only one"
1210,1,['cases'], Combinations,seg_281,of them speak spanish. the favorable cases for (i) is ( 2
1211,1,['cases'], Combinations,seg_281,"3) = 5 ∗ 3 = 15. by the ros principle, the total favorable cases are"
1212,0,[], Combinations,seg_281,find the number of ways of obtaining a hand of cards in a poker game.
1213,1,['combination'], Combinations,seg_281,solution 5.38 this problem is easy to solve using combination law. as a hand
1214,0,[], Combinations,seg_281,"contains five cards in a poker game, the number of ways is ( 5 5"
1215,1,['plot'], Combinations,seg_281,example 5.39 irrigation plot
1216,1,"['plot', 'sample', 'random']", Combinations,seg_281,"an irrigation plot is divided into 6 × 6 blocks of equal size (with 36 subplots). a sample of four subplots is to be selected at random. what is the number of ways in which the four subplots will (i) lie along any row or column, (ii) lie along the main diagonal or parallel to the main diagonal (from top left to bottom right), (iii) they stick together as a 2 × 2 subplot anywhere, and (iv) if nine subplots are selected, find the number of ways they stick together as 3 × 3 subplots."
1217,1,['plots'], Combinations,seg_281,solution 5.39 total number of ways to select four subplots from 36 plots is
1218,1,['case'], Combinations,seg_281,"6). in case (i), there are two possibilities to consider (1) they lie along the rows"
1219,1,['case'], Combinations,seg_281,"and (2) they lie along the columns. in the first case, there are ( 4"
1220,1,"['symmetry', 'case']", Combinations,seg_281,"all four to lie along any fixed row. as there are six rows, the total number of ways is 6 × 15 = 90 ways. owing to symmetry, there are 90 ways for the columns too. this gives 180 total possibilities. in case (ii), the main diagonal (with six slots), its immediate above and below diagonals with five slots and those at distance 2"
1221,0,[], Combinations,seg_281,from it (with four slots) are the only favorable positions. there exist ( 4
1222,0,[], Combinations,seg_281,"ways for the main diagonal, ( 4"
1223,0,[], Combinations,seg_281,5) = 5 ways for its immediate above and below
1224,0,[], Combinations,seg_281,"diagonals, and ( 4"
1225,0,[], Combinations,seg_281,4) = 1 way each for distance 2 diagonals. by the ros princi-
1226,1,['case'], Combinations,seg_281,"ple, total number of ways is 15 + 2*5 + 2*1 = 27 ways. for case (iii), we fix the 2 × 2 subplot as a square and use cell (2,2) (second column in second row) as an “anchor” for alignment. this anchor can be aligned in a 5 × 5 subplots giving 25 possible ways. similarly for case (iv), consider cell (3,3) as the anchor. this can be anchored along a 4 × 4 matrix of subplots giving a total of 16 possible ways."
1227,0,[], Combinations,seg_281,example 5.40 plant operators
1228,0,[], Combinations,seg_281,"a small production plant needs eight operators, two shipping and handling persons, two clerks, and one supervisor for a day. if there are 10 operators, four shipping and handling persons, three clerks, and two supervisors available for work, how many ways are there to staff the plant?"
1229,1,['disjoint'], Combinations,seg_281,"solution 5.40 as each of the jobs are disjoint, we could apply the"
1230,0,[], Combinations,seg_281,above-mentioned principle and get the answer as ( 1 8
1231,0,[], Combinations,seg_281,example 5.41 chess players
1232,0,[], Combinations,seg_281,"a college has 10 chess players of which 6 are males and 4 are females. two students are to be sent for an inter-collegiate festival. how many ways are there to send a team of 2 if: – (i) the gender is not considered, (ii) exactly one is a male, (iii) at least one must be female, and (iv) both are females?"
1233,1,['case'], Combinations,seg_281,"solution 5.41 for case (i), the total number of possibilities is ( 1 2"
1234,1,"['cases', 'case']", Combinations,seg_281,"9∕2 = 45. in case (ii), the total favorable cases is ( 1 6) ∗ ( 1"
1235,1,['case'], Combinations,seg_281,"4) = 24. in case (iii),"
1236,0,[], Combinations,seg_281,there are two possibilities {fm and ff}. the possible ways for fm is found
1237,0,[], Combinations,seg_281,above as 24. possible ways for ff is ( 2
1238,0,[], Combinations,seg_281,4) = 6. adding these two gives the
1239,1,['case'], Combinations,seg_281,"answer as 24 + 6 = 30. in case (iv), answer to this is found above as ( 2"
1240,1,"['probabilities', 'sets', 'events', 'probability', 'level']", PRINCIPLE OF INCLUSION AND EXCLUSION PIE,seg_283,this is one of the most widely used principles when events or sets interact (have subsets or subevents in common). it has two interpretations:– in terms of counts and in terms of probabilities. both are analogous at the conceptual level. we discuss the count version below. the extension to probability is given in a later section. the count version provides an answer to the query “how many elements or objects are there in
1241,1,"['sets', 'union']", PRINCIPLE OF INCLUSION AND EXCLUSION PIE,seg_283,"the union of a finite number of sets, some of which have elements or properties in common?.”"
1242,1,['sets'], PRINCIPLE OF INCLUSION AND EXCLUSION PIE,seg_283,"theorem 5.2 if a1, a2, … , an are finite sets, some of which have common elements, then"
1243,1,"['sets', 'summation', 'cardinalities']", PRINCIPLE OF INCLUSION AND EXCLUSION PIE,seg_283,"where vertical bars denote the cardinalities (number of elements) of respective sets, and i   j, and so on, on the summation sign denotes that the sum is carried out only for those values of indices satisfying respective conditions."
1244,1,"['sets', 'results', 'case']", PRINCIPLE OF INCLUSION AND EXCLUSION PIE,seg_283,"proof: consider the special case with just two sets say x and y. the above-mentioned theorem takes the form |x ∪ y| = |x| + |y| − |x ∩ y|. if x and y do not overlap, then x ∩ y =   so that |x ∩ y| = 0, and the results follow. if x = y , then x ∩ y = x = y , so that the negative term cancels out with one of the x or y giving the result. next, suppose that x and y overlap (with c common elements where c ≥ 1) and x ≠ y . in counting |x| + |y|, the c common elements are counted twice. hence, we need to subtract one of the c counts to get the number of elements in x ∪ y . this gives |x ∪ y| = |x| + |y| − |x ∩ y|. assume that the theorem is true for an arbitrary m. consider"
1245,1,"['union', 'intersection', 'case']", PRINCIPLE OF INCLUSION AND EXCLUSION PIE,seg_283,write a = a1 ∪ a2 ∪ · · ·ai ∪ · · ·am. then (5.9) becomes |a ∪ am+1|. expand it using the special case to obtain |a ∪ am+1| = |a| + |am+1| − |a ∩ am+1|. substitute for a = a1 ∪ a2 ∪ · · ·ai ∪ · · ·am and use the fact that intersection distributes over union operator to get the rhs as
1246,1,['sets'], PRINCIPLE OF INCLUSION AND EXCLUSION PIE,seg_283,"corollary 1 if a1,a2, … an are finite sets, some of which have common elements, then |a1 ∩ a2 ∩ · · ·ai ∩ · · ·an| = | u| −∑i"
1247,0,[], PRINCIPLE OF INCLUSION AND EXCLUSION PIE,seg_283,example 5.42 divisible integers
1248,0,[], PRINCIPLE OF INCLUSION AND EXCLUSION PIE,seg_283,"how many integers between 1 and 100 are divisible by 3, 5 or 7?"
1249,1,"['table', 'results', 'set', 'events']", PRINCIPLE OF INCLUSION AND EXCLUSION PIE,seg_283,"solution 5.42 let s denote the set of 100 integers s = {1, 2, 3, … , 100}. define three events as follows:– (i) e1 = count of all integers in s that are divisible by 3, (ii) e2 = count of all integers in s that are divisible by 5, and (iii) e3 = count of all integers in s that are divisible by 7. then, e1 ∩ e2 is the count of all integers in s that are divisible by both 3 and 5, and so on. using the pie principle e1 ∪ e2 ∪ e3 is the set of integers divisible by 3, 5, or 7. the results needed to compute this are given in table 5.5. using (5.11),"
1250,1,"['probabilities', 'parameters', 'recurrence relations', 'case', 'discrete', 'discrete distributions', 'distributions', 'recurrence', 'recurrence relation']", RECURRENCE RELATIONS,seg_285,"a recurrence relation is a recursive relationship that relates the nth term of a sequence or task in terms of lower order terms. if the nth term is related to the (n − 1)th term, it is called first-order recurrence. most of the recurrence relations encountered in this book are first order recurrences. if the nth term is related to two prior terms, it is called second-order recurrence. a special case is the recurrence relation connecting successive probabilities of discrete distributions. these are formed by reducing one of the integer parameters. these are explained in subsequent sections."
1251,1,"['sets', 'table']", Derangements and Matching Problems,seg_287,"matching problems comprise two sets of objects (such as husband and wife, person and hat, person and overcoat, and letter and envelope) that have a one-to-one correspondence among themselves. these types of problems seem to have fascinated mathematicians for centuries. the first “person and hat” problem was documented by de montfort in reference 111. they arise in many situations. for example, consider 13 cards numbered 1–13 without duplicates that are kept face down on a table."
1252,1,"['bipartite graph', 'probability']", Derangements and Matching Problems,seg_287,"a person utters a number between 1 and 13 and then picks up a card and notes the number. this is repeated 13 times, such that each time a different number is uttered. obviously, after 12 tries, the last number can easily be guessed. in how many ways, can the person get k(  13) correct matches? as another example, suppose that there are n books kept on a book rack in some specific order (say in alphabetical order of first author name, increasing order of accession numbers, or using call numbers). during the “library-hour,” kids take out all the books and return it arbitrarily back to the rack. what is the chance that exactly k of the books are returned back to their original position? what is the probability that none of the books are in their proper position? these problems can easily be modeled by the bipartite graph described in p. 126. in such a mapping, the first pair of the n objects are represented as n nodes on the left (say s), and the second pair is represented by n nodes on the right (say t). each node in s can be connected to at most 1 node in t. an undirected arc from node i in s to node j in t denotes a new order or assignment. a perfect match (original order is maintained) is indicated by a forest in which each node in s is connected to the matching node in t. these are easy to solve when n is small. in the following discussion, it is assumed that n is fairly large. a few of the situations where such problems arise are listed in the following:"
1253,1,['paired'], Derangements and Matching Problems,seg_287,assume that the men and women are randomly paired for a dance. a complete match occurs if each couple happens to be paired together.
1254,0,['n'], Derangements and Matching Problems,seg_287,2. suppose that n letters are to be sent to n different people in n envelopes. the
1255,0,[], Derangements and Matching Problems,seg_287,"addresses are already printed on the envelope, and the letters are shuffled. an absent-minded clerk randomly puts the letters, one each, into the n envelopes. a complete match occurs if each letter is put in its correct envelope."
1256,0,['n'], Derangements and Matching Problems,seg_287,3. suppose n people with overcoats go for a party. they give the coat to the waiter
1257,0,[], Derangements and Matching Problems,seg_287,"for safe keeping. while leaving the party, the waiter randomly grabs a coat and gives it to the people. a complete match occurs if each person gets his or her own coat."
1258,0,['n'], Derangements and Matching Problems,seg_287,4. a defective electronic device has n exactly looking parts. a repair person
1259,1,['tests'], Derangements and Matching Problems,seg_287,"removes each of them without labeling them, tests it individually, and returns them back to the original positions arbitrarily. a complete match occurs if each part ends up in its correct slot."
1260,1,['derangement'], Derangements and Matching Problems,seg_287,"each of these problems is mathematically equivalent. if none of them match, it is called a derangement. they can be modeled by different techniques such as recurrence relations and using inclusion–exclusion principle."
1261,1,['derangements'], Derangements and Matching Problems,seg_287,theorem 5.3 total number of derangements of n elements is
1262,1,['event'], Derangements and Matching Problems,seg_287,proof: this can be proved in many ways. we give the following two simple proofs. the first one uses the pie principle. let ai denote the event that ith object is cor-
1263,1,['paired'], Derangements and Matching Problems,seg_287,"rectly paired with matching pair (ith letter is put in its correct envelope, etc.). let ai"
1264,1,"['complement', 'event', 'derangement', 'complementary event', 'paired']", Derangements and Matching Problems,seg_287,"denote the complementary event. then a1a2 denotes that the first two objects are not paired with their matches. the event a1a2 … an denotes that none of the objects are matched with their peers. this is what is meant by a derangement. by generalized demorgan’s law, we have a1 ∩ a2 ∩ … ∩ an = [a1 ∪ a2 ∪ · · ·ai ∪ · · ·an]c, where the superscript denotes the complement. using the “do-little” principle (section 5.6.4 (p. 131)), the complementary event a1 ∪ a2 ∪ … ∪ an on the rhs is much easier to evaluate. using the pie principle, this can be expanded as"
1265,1,['paired'], Derangements and Matching Problems,seg_287,"there are n ways in which ith object can be paired with its match. for any two arbitrary pairs (i,j), there are n(n − 1) ways to pair them using the multiplication law."
1266,0,[], Derangements and Matching Problems,seg_287,"owing to the restriction on i and j, there are ( 2"
1267,1,['paired'], Derangements and Matching Problems,seg_287,the number of ways in which k items are paired with their peers is (n − k)!. substitute in equation (5.13) to get
1268,1,['factor'], Derangements and Matching Problems,seg_287,"expand ( n i ) = n!∕(i!(n − i)!), take n! as common factor, subtract from n! and"
1269,1,['approximation'], Derangements and Matching Problems,seg_287,"n =0 (−1)i∕i!. if n is large, n!/e is a good approximation to equation (5.14) because e−1 = ∑k"
1270,1,"['derangements', 'condition', 'recurrence relations', 'cases', 'set', 'derangement', 'paired', 'recurrence']", Derangements and Matching Problems,seg_287,"∞ =0 (−1)k∕k!. the derangement problem is easy to solve using recurrence relations. let un denote the number of derangements of n objects and sn denote the corresponding set. fix any two objects say p and q. there are four possibilities:– (i) both p and q are paired with their own match, (ii) either p or q is paired with own partner, (iii) p is paired with q’s match and q is paired with p’s, and (iv) only one of p or q is paired with the other’s match. obviously, options 1 and 2 do not belong to sn because they violate the derangement condition. only favorable cases are options 3 and 4. consider option 3 first. as they are not matched to their peers, the remaining (n − 2) objects can be deranged in un−2 ways. obviously, sn−2 is a subset of sn. object p can be matched to (n− 1) other objects j (excluding its peer say (i). this automatically determines one such match for q as i. thus, there are (n − 1) ways in which pairs of objects that occupy each others’ place can be formed. if one among the fixed objects occupy another place but not vice versa, there exist un−1 ways in which others can go wrong. this gives the recurrence un = (n− 1)*(un−1 + un−2). write the rhs as"
1271,1,['results'], Derangements and Matching Problems,seg_287,repeated application of equation (5.16) results in un − n ∗ un−1 = (−1)n−2(u2 − 2u1). substitute u1 = 0 and u2 = 1 to get un − n ∗ un−1 = (−1)n. divide throughout by n! and cancel out n from the second term to get
1272,1,"['derangements', 'derangement']", Derangements and Matching Problems,seg_287,"replace n successively by (n − 1), (n − 2), ..., 2 and add them together to get (un∕n!) = {1∕2! − 1∕3! + · · · (−1)n∕n!}. add and subtract 1 in the rhs and write it as 1−1/1! to get the final result. for two objects (a,b), there is only one derangement (namely (b,a)) with d2 = 1. for three objects say (a,b,c), there exist two derangements (b,c,a) and (c,a,b) so that d3 = 2. similarly d4 = 6 and so on."
1273,0,[], Derangements and Matching Problems,seg_287,example 5.43 hat-check problem
1274,0,[], Derangements and Matching Problems,seg_287,"there are n customers at a club, each of whom wears a cap. each member puts his cap in a basket while entering the club in the evening. while going out, each one picks a cap randomly and walks out. what is the possible number of ways that (i) all of them picks their own cap, (ii) no one picks their own cap, and (iii) exactly half of them gets back their own hats (where n is even)?"
1275,1,['derangement'], Derangements and Matching Problems,seg_287,"solution 5.43 number the caps from 1 to n. the total number of possible ways to arrange the n caps is n!. out of this, there is only one way in which everyone can get their own caps, so that the answer to (i) is 1 out of n!, (ii) number of ways in which no one picks their own hat is the derangement dn, and (iii) if exactly half of them gets back their own hats, the other half do not get their hat. this can"
1276,1,"['model', 'set', 'urn model']", URN MODELS,seg_289,an urn model is a conceptual framework for representing a set of problems that satisfy the following conditions:–
1277,0,[], URN MODELS,seg_289,"• problem involves a collection of (preferably three or more) items, where each item belongs to a group or has a type, and elements of the same group or type are indistinguishable;"
1278,1,['random'], URN MODELS,seg_289,"• all items are put together or assigned as a whole such that any subset of them can be selected at random, without looking at their type;"
1279,0,[], URN MODELS,seg_289,"• each of the groups are distinguishable, but their order or arrangement is unimportant."
1280,1,"['model', 'urn model']", URN MODELS,seg_289,there are a large number of problems that can be cast as urn model. these are also called occupancy problems.
1281,0,[], URN MODELS,seg_289,example 5.44 unique id numbers
1282,0,[], URN MODELS,seg_289,a university wishes to assign a unique 4 digit id number to each of the enrolled students with the following restrictions that the student number cannot start with digit “0.” what is the maximum number of ids that can be generated if (i) the digits can be repeated any number of times and (ii) digits cannot be repeated?.
1283,1,"['multiplication rule', 'case']", URN MODELS,seg_289,"solution 5.44 consider the four positions as four numbered urns arranged along a line. we can fill these urns from left to right. as the student number cannot start with digit “0,” the first urn can be filled in nine ways (with digits one through nine). in case (i), we are allowed to repeat the already used digit. this means that the second, third, and fourth urns can be filled with any of the 10 digits. this gives 9 ∗ 103 = 9000 possible numbers. in case (ii), the first urn can be filled in nine ways as before. the second urn can be filled in nine ways as digits cannot be repeated. similarly, the third and fourth urns can be filled in eight and seven ways, respectively. by the multiplication rule, we get the answer as 9*9*8*7 = 4536 ways. thus, up to 4536 student, id numbers can be generated if digits are not repeated."
1284,1,['table'], URN MODELS,seg_289,"theorem 5.4 total number of ways in which n indistinguishable balls can be put in k distinguishable urns (see table 5.6) where none of the urns can be empty, and maximum capacity of each urn is n is kn."
1285,1,['linear'], URN MODELS,seg_289,"proof: as the urns are distinguishable, arrange them in a linear order. start with the leftmost urn. we can put any of the balls there. thus, there are n ways. this is true for each of the k urns. by the multiplication law, the total number of ways is kn."
1286,0,[], URN MODELS,seg_289,example 5.45 common birthday
1287,1,"['events', 'probability']", URN MODELS,seg_289,"suppose that there are n − 1 ( 365) other persons along with you in a room, none of whom are twins. how many ways are there for each of the following events to realize assuming that leap-years are not accounted for? (i) none of the people shares a common birthday, (ii) at least one other person in the room shares a birthday with you, (iii) at most three people share a common birthday, and (iv) find the value of n such that the probability for at least two persons to share a common birthday is 0.6?"
1288,1,['table'], URN MODELS,seg_289,solution 5.45 this can be cast in the urn-model framework by assuming days of the year consecutively numbered as urns and people as balls (table 5.6). then
1289,1,"['case', 'event', 'complementary event']", URN MODELS,seg_289,"case (i): “none have a common birthday” means that all birthdays are different. this is the same as the number of ways to choose n different days from 365 days, which is (365)n. (ii) the complementary event of “at least one other person” is that discussed in case (i). as the total number of ways for the birthdays of n persons is 365n, the required answer is 365n − (365)n. (iii) at most three people will have common birthdays if either two or three people have the same birthday."
1290,1,['partitions'], PARTITIONS,seg_291,we saw in section 5.9.5 (p. 5–63) that the number of partitions of n things into two
1291,1,"['partitions', 'set', 'stirling number', 'recurrence']", PARTITIONS,seg_291,"divide a finite set s of size n elements into all possible subsets. this is not to be confused with set partitions defined in section 5.5.1 in page 122. the trivial subsets are s itself and one-element subsets. for simplicity, consider a set with three elements s = {1, 2, 3}. then, the seven possible partitions are {{1},{2},{3},{1,2},{1,3},{2,3}}, and {1,2,3}. when s has four elements, there are m = 15 partitions. in general, when there are n elements, there exist s(n, k) partitions, where s(n, k) is called the stirling number of second kind. these numbers satisfy the recurrence s(n, k) = s(n − 1, k − 1) + k ∗ s(n − 1, k), where s(n, 1) = s(n, n) = 1, s(n, 2) = 2n−1 − 1."
1292,1,"['mutually exclusive', 'set', 'events', 'probability', 'axiomatic approach']", AXIOMATIC APPROACH,seg_293,"with the solid mathematical footing given earlier, we are ready to define the axiomatic approach to probability. consider a finite set of mutually exclusive and collectively exhaustive set of events ai such that ∪n"
1293,1,"['sample', 'experiment', 'events', 'probability', 'random', 'function', 'sample space', 'axiomatic approach']", AXIOMATIC APPROACH,seg_293,"i=1ai = ω. the literal meaning of an axiom is “a statement that is always true or obviously true.” events lie at the core of axiomatic approach. we break the sample space of a random experiment into events that do not occur together. then, we define probability as a real valued function that obeys certain conditions."
1294,1,"['sample', 'experiment', 'probability measure', 'probability', 'random', 'function', 'outcome', 'sample space']", Probability Measure,seg_295,a probability measure has two fundamental ingredients. a sample space ω of outcomes of a random experiment and a function that maps each elementary outcome ai to a real number between 0 and 1 such that they add up to 1. these are stated as three axioms:–
1295,1,"['disjoint events', 'events', 'disjoint']", Probability Measure,seg_295,"3. if ai is a sequence of disjoint events, then p(∪n i=1ai) = ∑i"
1296,1,"['mutually exclusive', 'probability measure', 'events', 'probability', 'function', 'mutually exclusive events']", Probability Measure,seg_295,the third axiom can be extended to countably infinite collective mutually exclusive events. such a function is called a probability measure.
1297,1,['set'], Probability Measure,seg_295,the postulation p( ) = 0 follows because ω ∪ = ω and p(ω) = 1. a direct consequence of these axioms is the following set of properties that are stated in set theoretic symbols and operators.
1298,1,"['sample', 'events', 'sample space']", Probability Measure,seg_295,"theorem 5.5 if x and y two arbitrary events defined on a sample space, then (i) 0 ≤ p(x) ≤ 1, (ii) 0 ≤ p(y) ≤ 1, (iii) if x ⊂ y → p(x) ≤ p(y), and (iv) p(x ∪ y) = p(x) + p(y) − p(x ∩ y)."
1299,1,['results'], Probability Measure,seg_295,"proof: the first two results follows from the above-mentioned theorem. to prove the third result, we write x + (y − x) = y . then apply the third axiom to get p(x) + p(y − x) = p(y). as p(y − x) ≥ 0, it follows that p(x) ≤ p(y)."
1300,1,['disjoint'], Probability Measure,seg_295,"to prove (iv), write x ∪ y as the disjoint unions as x ∪ y = (x ∩ yc) ∪ (y ∩ xc) ∪ (x ∩ y). as the subsets on the rhs are disjoint, axiom 3 can be applied to get"
1301,1,"['addition rule', 'probability']", Probability Measure,seg_295,"add and subtract p(x ∩ y) on the rhs and combine p(x ∩ yc) + p(x ∩ y) as p(x). similarly, write p(x ∩ y) as p(y ∩ x) and combine p(y ∩ xc) + p(y ∩ x) as p(y). substitute the values on the rhs to get p(x) + p(y) − p(x ∩ y). this is known as the addition rule of probability."
1302,1,"['sample', 'probabilities', 'range', 'probability measure', 'events', 'set', 'probability', 'sample space']", Probability Space,seg_297,"a probability space is a triplet {ω, ,ℙ}, where ω is the sample space,   the set of events defined on ω, and ℙ the probability measure that maps events in   → [0, 1] such that p(ω) = 1,p(a) ∈ [0, 1]∀a ∈  , which is countably additive. this forms the foundation for several theoretical studies in probability. note that all three are related, but second and third components are more related than others. this is because the third component is a mapping from elementary events of ω to the real line [0,1]. in other words, ℙ has domain   and range [0,1] (it is assumed here that the probabilities"
1303,1,"['experiment', 'random', 'percentages']", Probability Space,seg_297,are represented as decimals and not as percentages). it is used to mathematically represent a random phenomenon or an unknown experiment.
1304,1,"['sample', 'model', 'experiment', 'events', 'set', 'associated', 'probability', 'event', 'sample space']", Probability Space,seg_297,"a probability model is a triplet ℙ = (ω, s, p(x)), where ω is the sample space, s a set of events associated with an experiment, and p(x) the probability associated with each event in s such that ∑ip(xi) = 1."
1305,1,"['sample', 'mutually exclusive', 'experiment', 'states', 'conditional', 'events', 'probability', 'random', 'sample space', 'outcomes', 'event']", THE CLASSICAL APPROACH,seg_299,"the sample space ω is well-defined and often enumerable in the classical approach. in addition, there are no conditional events involved. assume that there are n equally likely, mutually exclusive, and collectively exhaustive outcomes of a random experiment. if m of them are favorable to an event e, the classical approach states that the desired probability is m upon n (i.e., p = m∕n). symbolically, this can be written as"
1306,1,['outcomes'], THE CLASSICAL APPROACH,seg_299,p(e) = number of outcomes favorable to e∕ total number of outcomes in ω.
1307,1,"['sample', 'sample space']", THE CLASSICAL APPROACH,seg_299,this definition holds only when the sample space is finite.
1308,1,"['probabilities', 'events', 'union', 'probability']", Counting Techniques in Classical Probability,seg_301,"several counting techniques were discussed in section 5.8 (starting p. 135). these form the foundation of the classical approach. some of the counting techniques developed there have direct analogs in probability. consider for example, the principle of inclusion and exclusion discussed in page 147. it was mentioned there that the pie has two variants in terms of counts and probabilities. the “probability version” given below has direct application in finding the probability of a union of events, at least some of which have common elements."
1309,1,"['events', 'probabilities', 'probability']", Assigning Probabilities to Events,seg_303,it is fairly straightforward to find probabilities of events by the classical approach. first find the total number of possible events say n. then find the number of favorable events say m. then divide m by n to get the probability.
1310,1,"['cases', 'probability']", Assigning Probabilities to Events,seg_303,"lemma 4 classical probability = number of favorable cases/total number of cases (p = m∕n, where m = # favorable cases, n = total number of cases)."
1311,1,['estimated'], Assigning Probabilities to Events,seg_303,"numerator can be either enumerated, estimated by other means, or evaluated recursively in most of the problems. these are exemplified in the following."
1312,1,"['prime number', 'without replacement', 'replacement', 'probability', 'random']", Assigning Probabilities to Events,seg_303,a box contains cards marked with numbers 1–10. (a) what is the probability that a number drawn at random is (i) prime number and (ii) divisible by 3. (b) what is the probability that the sum of two numbers drawn at random without replacement is (i) even and (ii) odd integer greater than or equal to 15.
1313,1,"['sample', 'without replacement', 'case', 'conditional', 'replacement', 'cases', 'prime numbers', 'probability', 'sample space']", Assigning Probabilities to Events,seg_303,"solution 5.46 the sample space is well defined. there are no conditional probabilities involved. total favorable cases are easy to enumerate. hence, we could easily find the probability by dividing the total favorable cases by the number of points in the sample space. for case (a), we need to enumerate all prime numbers. there are 5 of them as {1,2,3,5,7} are all primes. hence, required probability by lemma 8 is 5/10 = 1/2. in case (ii), the favorable cases are {3,6,9}. from this, the required probability follows easily as 3/10. in part (b), we are drawing the cards without replacement. total number of ways to draw two num-"
1314,0,[], Assigning Probabilities to Events,seg_303,0). as the sum of two numbers is even when both are even or
1315,1,"['sample', 'cases', 'probability', 'sample space']", Assigning Probabilities to Events,seg_303,"both are odd, we can easily enumerate the 20 favorable cases in the sample space as s= {(1,3),(1,5),(1,7),(1,9),(3,5),(3,7),(3,9),(5,7),(5,9),(7,9),(2,4),(2,6),(2,8), (2,10), (4,6),(4,8),(4,10),(6,8),(6,10),(8,10)}. the required probability is then 20∕( 1 2"
1316,1,"['sample', 'case', 'cases', 'sample space']", Assigning Probabilities to Events,seg_303,"0) = 20∕45 = 4∕9. in case (ii), the favorable cases in the sample space"
1317,1,"['sample', 'probability', 'sample space']", Assigning Probabilities to Events,seg_303,"are (5,10),(7,10),(9,10),(6,9),(8,9),(7,8),(9,8). hence, the required probability by lemma 8 is p = total favorable cases/number of points in the sample space = 7/( 1 2"
1318,0,[], Rules of Probability,seg_305,this section refreshes some of the rules that are necessary for laying a foundation for subsequent discussions.
1319,1,['probability'], Rules of Probability,seg_305,rule 3 probability is always between 0 and 1 (0 ≤ p(a) ≤ 1).
1320,1,"['percentage', 'interval', 'method', 'probability']", Rules of Probability,seg_305,"in section 5.2, we have seen various ways to express probability. all of the methods described there (except the percentage method) map the probability into the interval [0,1]."
1321,1,"['sample', 'sample space', 'probability']", Rules of Probability,seg_305,rule 4 probability of the entire sample space is 1. that is p(ω) = 1.
1322,1,"['events', 'probability']", Rules of Probability,seg_305,the proof follows trivially because the probability of all the events occurring is certainty.
1323,1,"['disjoint', 'probabilities', 'disjoint events', 'events', 'probability']", Rules of Probability,seg_305,"rule 5 probability of occurrence of either of two disjoint events is the sum of their individual probabilities (i.e., p(a ∪ b) = p(a) + p(b))."
1324,0,[], Rules of Probability,seg_305,example 5.47 playing card problem
1325,1,['probability'], Rules of Probability,seg_305,what is the probability that a card selected from a deck of playing cards will be either an ace or a queen?
1326,1,"['disjoint', 'disjoint events', 'events', 'event']", Rules of Probability,seg_305,"solution 5.47 let “a” denote the event that it is an ace and “b” denote the event that it is a queen. these two are disjoint events. hence, the required probability by rule 5 is p(a) + p(b). but p(a) = 4∕52 = 1∕13 = p(b). the answer follows as 2/13. this rule can be extended to any number of disjoint events. let a1,a2, … ,an be disjoint events. then (p(a1 ∪ a2 · · · ∪ an) = ∑i"
1327,1,['product rule'], Rules of Probability,seg_305,rule 6 product rule
1328,1,"['independent', 'events', 'independent events', 'probability']", Rules of Probability,seg_305,"if a and b are two independent events, the probability of occurrence of both of these events is the product of their individual probabilities:– p(a ∩ b) = p(a)p(b)."
1329,1,"['independent', 'probabilities', 'events', 'independent events', 'probability']", Rules of Probability,seg_305,"proof: as the events are independent, the occurrence of a has nothing to do with the occurrence of b. the probability of occurrences of a and b is the product of their individual probabilities. this rule can be generalized to any number of independent events as p(a1a2 · · ·an) = p(a1)p(a2)...p(an)."
1330,0,[], Rules of Probability,seg_305,example 5.48 furniture making
1331,1,"['process', 'probabilities']", Rules of Probability,seg_305,"a furniture is made through three processes:–(i) cutting process, (ii) drilling process, and (iii) assembly and finishing process. the respective probabilities of a defect in each of the stages are 1/60, 1/20, and 1/80. find the probability that a finished furniture is (i) defective and (ii) has no cutting or drilling defect."
1332,1,"['probability', 'independent', 'processes']", Rules of Probability,seg_305,"solution 5.48 assume that the processes are independent. the probability that it is defective is 1/60*1/20*1/80 = 1/96,000. (ii) probability that it has no cutting or drilling defect is (1 − 1∕60) ∗ (1 − 1∕20) = 0.93416."
1333,0,[], Rules of Probability,seg_305,rule 7 sum rule
1334,1,"['events', 'independent', 'probability']", Rules of Probability,seg_305,the probability of occurrence of either of two events (not necessarily independent) is p(a ∪ b) = p(a) + p(b) − p(a ∩ b).
1335,1,"['events', 'intersection']", Rules of Probability,seg_305,"proof follows trivially using the principle of inclusion and exclusion. let “x” denote the common intersection of events a and b(x = a ∩ b). then p(a) + p(b)will contain the “x” portion twice. therefore, we need to subtract it once to get p(a ∪ b). another proof appears in theorem 5.5 (p. 155)."
1336,1,"['complement', 'probability', 'event']", Rules of Probability,seg_305,"the probability of non-occurrence of an event is the complement of the probability of occurrence. symbolically, p(a) = 1 − p(a)."
1337,1,"['sample', 'probability of the event', 'events', 'complement', 'union', 'probability', 'event', 'complement of an event', 'sample space']", Rules of Probability,seg_305,"the complement of an event comprises all events in the sample space ω except the event. as the probability of the sample space is 1, it follows that the probability of the event union the probability of its complement is 1. symbolically p(a) + p(a) = 1, from which the result follows."
1338,1,"['sample', 'events', 'sample space']", Rules of Probability,seg_305,"theorem 5.6 if a1,a2, … ,an are events defined on a sample space, at least some of which have common elements, then"
1339,1,"['events', 'case']", Rules of Probability,seg_305,proof: consider a special case with just two events say x and y. we know that
1340,1,"['sample', 'disjoint', 'results', 'disjoint events', 'events', 'sample space']", Rules of Probability,seg_305,"divide both sides by the total number of points in the sample space and swap the lhs and rhs. then, the above-mentioned expression takes the form p( x ∪ y) = p(x) + p(y) − p(x ∩ y). if x and y are disjoint events, then x ∩ y =   so that p(x ∩ y) = 0, and the results follow. next assume that the result is true for an arbitrary m   2. then p(∪iam"
1341,1,['probabilities'], Rules of Probability,seg_305,"as done earlier, divide by |ω| to get the probabilities as p(∪m"
1342,0,[], Rules of Probability,seg_305,"i=1(ai ∩ am+1). now apply the above-mentioned equation to get the rhs in desired form. thus, the result follows by induction."
1343,1,['sets'], Rules of Probability,seg_305,"corollary 2 if a1,a2, … an are finite sets, some of which have common elements, then |a1 ∩ a2 ∩ · · ·ai ∩ · · ·an| = |u| −∑i"
1344,1,"['sample', 'discrete', 'events', 'probability', 'sample space', 'numerical']", DoLittle Principle of Probability,seg_307,it was mentioned in section 5.6.4 (p. 131) that complementary events are sometimes easy to find when the sample space consists of a large number of discrete events as in the above-mentioned example. these are especially true in “at least k” and “at most k” type problems. these are called the do-little (or complement-and-conquer) principle of probability. see page 162 and 166 for numerical examples.
1345,0,[], DoLittle Principle of Probability,seg_307,example 5.49 multiple choice exam
1346,1,['probability'], DoLittle Principle of Probability,seg_307,"a multiple choice exam has 15 questions, each with 4 answer choices (say a,b,c,d). if a student guesses the answer to every question, what is the probability of getting at least two questions correct?"
1347,1,"['independent', 'complement', 'probability', 'event']", DoLittle Principle of Probability,seg_307,"solution 5.49 here, the keyword is “at least 2.” the complement event is 0 or 1 correct answers that are much easier to find. as there are four choices, probability of guessing the answer is 1/4 (so that the probability of incorrect answer is 3/4). we can consider the 15 questions as independent. this gives the probability of 0 correct answers as (3∕4)15 = 0.0133635. now consider getting at"
1348,0,[], DoLittle Principle of Probability,seg_307,"least one correct answer. this correct answer may correspond to any of the questions 1 through 15 (in other words, there exist 15 possibilities). hence, this has"
1349,0,[], DoLittle Principle of Probability,seg_307,probability ( 1 1
1350,0,[], DoLittle Principle of Probability,seg_307,abilities from 1 gives the required answer of getting at least two questions correct as 1 − 0.0668173 − 0.0133635 = 1 − 0.080181 = 0.919819.
1351,0,[], DoLittle Principle of Probability,seg_307,example 5.50 at least type problem
1352,1,['probability'], DoLittle Principle of Probability,seg_307,there are 10 students in a class. what is the probability that at least two of them have a common birthday if none were born in a leap-year?
1353,1,"['cases', 'probability', 'event', 'complementary event']", DoLittle Principle of Probability,seg_307,"solution 5.50 here, the keyword is again “at least 2.” the complementary event is “none of them have a common birthday.” this means that each student has a different birthday. arrange the students in an arbitrary order. the first student has 365 choices. having fixed the birthday of first student, there are 364 choices for the second student and so on. thus, by the multiplication principle, total number of ways (favorable cases) in which all birthdays are different is m = 365*364*363*356. total number of ways in which the birthdays can be distributed (including those counted above) is n = 36510. the required probability (in which all birthdays are different) is obtained by dividing m by n. note that one of the 365’s cancel out from the numerator and denominator giving the answer as (364)9∕3659 = 0.88305. subtract this from 1 to get the probability that at least two of them have a common birthday. in general, if there are m students, the probability for all birthdays to be different is (364)m−1∕365m−1."
1354,0,[], DoLittle Principle of Probability,seg_307,example 5.51 shipping container
1355,1,['probability'], DoLittle Principle of Probability,seg_307,a shipping container is loaded with 50 food cartons. the probability that any of the cartons will get damaged during transshipment is 0.003 = 3/1000. what is the probability of finding at least one defective carton when the container reaches its destination?
1356,1,['probability'], DoLittle Principle of Probability,seg_307,"solution 5.51 as the probability that it will get damaged during shipment is 0.003, the probability that it will not be damaged is 0.997. hence, the probability that at least one of them gets damaged = 1-probability that none of them is damaged = 1 − (0.997)50 = 1 − 0.8605 = 0.1395."
1357,0,[], DoLittle Principle of Probability,seg_307,example 5.52 defective circuits
1358,1,['probability'], DoLittle Principle of Probability,seg_307,"an electronic board has three parallel circuits, each of which contains three, eight, and five components. the probability for each component to malfunction is 0.0015. the board will stop working when at least one of the parallel circuits has a defect. what is the probability that the board does not work?"
1359,1,"['probabilities', 'probability']", DoLittle Principle of Probability,seg_307,"solution 5.52 probability that at least one of the circuits does not work = 1-probability that none of them are defective. probability that the first circuit is not defective = 0.99853. similarly, the corresponding probabilities for second and third circuits can be found."
1360,1,"['probabilities', 'experiment', 'case', 'cases', 'events', 'permutation', 'probability', 'random', 'experiments', 'event', 'axiomatic approach']", Permutation and Combination in Classical Approach,seg_309,"permutation is useful to solve a variety of probability problems involving placement of objects (such as books, people, and electronic components). probabilities can be assigned to the events that make up a random experiment using the axiomatic approach. this is easily done in the case of equally likely experiments using the classical approach. when the favorable cases for an event involve counting several arrangements, we can use the techniques developed in the permutation and combination theorems."
1361,1,['probability'], Permutation and Combination in Classical Approach,seg_309,"consider example 5.28 in page 139. what is the probability for a leap year with 366 days to have (i) 53 sundays?, (ii) 53 saturdays and 53 sundays?, (iii) exactly 52 saturdays and 52 sundays?, (iv) exactly 53 fridays or 53 sundays?, and (v) exactly 52 tuesdays and 52 thursdays?"
1362,1,"['cases', 'complement', 'probability', 'combinations']", Permutation and Combination in Classical Approach,seg_309,"solution 5.53 as mentioned before, there are only seven possible combinations for the extra 2 days. (i) as sunday occurs in two of the seven combinations, the probability that a leap year will have 53 sundays is 2/7. (ii) as (saturday, sunday) occurs once, the desired probability is 1/7. (iii) as neither saturday nor sunday occurs in four out of the seven possible pairs, the desired probability is 4/7. (iv) there exist four pairs containing either a friday or a sunday required probability is 4/7. (v) there are three favorable cases, namely, (sunday, monday), (friday, saturday), and (saturday, sunday) using the complement rule. hence, the answer is 3/7."
1363,0,[], Permutation and Combination in Classical Approach,seg_309,example 5.54 roots of quadratic equation
1364,1,['probability'], Permutation and Combination in Classical Approach,seg_309,"consider the quadratic equation px2 + qx + r = 0 considered in example 5.29 (p. 139). find the probability that (i) the equation will have real roots, (ii) equal roots, (iii) imaginary roots, (iv) both integer roots, and (v) exactly one integer root."
1365,1,"['cases', 'coefficients']", Permutation and Combination in Classical Approach,seg_309,"solution 5.54 as each of the coefficients p,q,r is determined by the number that shows up in the throw of a die, we need three throws to decide them (say choose p first, then q, and finally r). in example 5.29, we found that there exists (38 + 5) = 43 ways for the equation to have real roots. hence, the answer to (i) is 43/216. (ii) as the five favorable cases are (1,2,1), (1,4,4), (2,4,2), (3,6,3), and (4,4,1), answer to (ii) is 5/216. a quadratic equation can have real roots, equal roots, or imaginary"
1366,1,"['cases', 'complement', 'case', 'probability']", Permutation and Combination in Classical Approach,seg_309,"roots only. hence, the favorable cases for case (iii) can be directly obtained using complement rule as 216 − 43 = 173. this gives the probability for case (iii) as 173/216. consider case (iv). both roots are integers in 10 favorable cases, so that the required probability is 10/216 = 5/108. (v) there are eight favorable cases so that the probability is 8/216 = 1/27."
1367,1,['tails'], Permutation and Combination in Classical Approach,seg_309,example 5.55 equal number of heads and tails in coin toss
1368,1,"['probability', 'tails', 'unbiased']", Permutation and Combination in Classical Approach,seg_309,an unbiased coin is tossed 2n times where n ≥ 1. what is the probability of observing an equal number of heads and tails?
1369,1,"['trials', 'tails']", Permutation and Combination in Classical Approach,seg_309,"solution 5.55 as we are interested in “an equal number of heads and tails,” this can be considered as an arrangement of n heads and n tails in 2n trials."
1370,1,['tails'], Permutation and Combination in Classical Approach,seg_309,n) ways in which n heads and n tails can occur. each of them
1371,1,"['probability', 'associated']", Permutation and Combination in Classical Approach,seg_309,"has the associated probability pnqn where q = 1 − p. by the ros principle, the"
1372,1,['unbiased'], Permutation and Combination in Classical Approach,seg_309,"n) pnqn. as we are given that the coin is unbiased, p = q = 0.5."
1373,0,['n'], Permutation and Combination in Classical Approach,seg_309,substitute in the above-mentioned equation to get the answer as ( 2 n
1374,0,[], Permutation and Combination in Classical Approach,seg_309,example 5.56 common birthday
1375,1,['probability'], Permutation and Combination in Classical Approach,seg_309,suppose that there are n ( 365) passengers in a plane. what is the probability that at least two people have a common birthday? what is the minimum value of n such that the probability that (i) none will have a common birthday is 0.4313? (ii) two or more people will share a common birthday is at least 0.9?
1376,1,"['sample', 'complement', 'probability', 'sample space', 'event']", Permutation and Combination in Classical Approach,seg_309,"solution 5.56 assume that the birthdays are randomly distributed, and none were born on february 29 of a leap year. then we could consider the 365 days as the equivalent of numbered urns. a person whose birthday is january 10 is assigned to 10th urn, and one whose birthday is december 26 is assigned to urn 360. the desired probability is found by enumerating the number of ways in which these urns can be filled by people such that at least two people are assigned to an urn. this sample space is not easy to enumerate. next apply the “complement-and-conquer” principle. consider the complement event. as a common birthday occurs with at least two people, the complement event is that none of the passengers have a common birthday. this is equivalent to counting the number of ways in which people can be assigned to urns such that each urn is either empty or has at most one assigned person. this event is greatly simplified. the total number of ways in which birthdays of n passengers may fall among the 365 days is 365n. order the persons arbitrarily from 1 to n. there are 365 possibilities for the first person’s birthday. as that day is taken, the second person’s birthday can fall in 364 days, and so on. as our assumption is that n 365,"
1377,1,"['probability', 'pochhammer notation']", Permutation and Combination in Classical Approach,seg_309,"the last person’s birthday can be chosen in 365*364*363*...*(365 − n + 1)ways. this can be denoted using factorials as 365!∕(365 − n)! or using pochhammer notation as (365)n. thus, the number of ways in which each person’s birthday is different is 365!/(365 − n)! because we do not care which person’s birthday is on a particular date. from this, the probability that each of the birthdays is different is obtained as p = 365!∕((365 − n)!365n) = (365)n∕365n. hence, the probability that at least two people have a common birthday is 1 − p = 1 − (365)n∕365n. for part (i), we need to find n such that (365)n∕365n ≃ 0.4313. take log of both sides and try successive values to get n = 25. in part (ii), we have to find that value of n for which 1 − p = 1 − (365)n∕365n ≤ 0.1. for n = 40, the probability that all birthdays are different is 0.108768, and for n = 41, it is 0.0968. hence n = 41."
1378,0,[], Permutation and Combination in Classical Approach,seg_309,example 5.57 no common birthday
1379,1,['probability'], Permutation and Combination in Classical Approach,seg_309,consider the above-mentioned example where n   365. what is the probability that (i) none have a birthday on sundays?. (ii) exactly k persons have a common birthday on the x’mas day?.
1380,1,"['case', 'probability']", Permutation and Combination in Classical Approach,seg_309,"solution 5.57 (i) a year has either 52 or 53 sundays (if 1 january is a sunday, then that year will have 53 sundays, as 31 december is also sunday). in the former case, there are 365 − 52 = 313 days that are not sundays. hence, the total number of possibilities is 313n. in the later case, there are 312 days that are not sundays with 312n possibilities. thus, the probability is either (312∕365)n or (313∕365)n depending on whether january 1st is a sunday or not. (ii) as k persons birthday fall on an x’mas day, there are n − k persons whose birthday"
1381,0,['n'], Permutation and Combination in Classical Approach,seg_309,falls on other 364 days. there are ( n k) ways for k persons to have birthday on
1382,1,['probability'], Permutation and Combination in Classical Approach,seg_309,"x’mas day and (364)n−k ways for other birthdays. hence, the required probability"
1383,1,['events'], Sequentially Dependent Events,seg_311,"events cyclically repeat in some applications. consider a working traffic light. in each cycle, the signal changes color from green (g) to yellow (y) to red (r) and then to green. hence, the events are {g, y, r}. these are not equally likely because the duration of these signals are preset based on the traffic density in different directions. assume that green signal is shown for 50 seconds, yellow for 5 seconds, and red for 35 seconds in one direction. then p(green) = 50/90 = 5/9, p(yellow) = 5/90 = 1/18, and p(red) = 35/90 = 7/18. this may differ in other directions."
1384,1,"['condition', 'independence', 'events', 'probability', 'random', 'experiments']", Independence of Events,seg_313,"independence of events is an important condition to check, as this can considerably simplify probability calculations. this is most often intuitively clear to humans but not to machines. independence of events is often assumed in random experiments."
1385,1,"['events', 'independent', 'event']", Independence of Events,seg_313,definition 5.9 two events “a” and “b” are independent if the occurrence of either of them is not influenced by prior knowledge about the occurrence of the other event.
1386,1,"['independent', 'independence', 'events', 'probability']", Independence of Events,seg_313,"symbolically, we denote it as p(a and b) = p(a) ∗ p(b). note that independence is a “logical relation” among events, but it is mathematically cast using the probability notation. this can also be expressed as p(a) = p(a|b) (or p(b) = p(b|a)), where the vertical bar denotes conditioning (it is read as “probability of a equals probability of a given b,” etc.). the first notation is easier than others to generalize the concept to n events. symbolically events e1,e2, … ,en are independent if p(e1 ∩ e2 · · · ∩ en) = p(e1) ∗ p(e2) ∗ · · · ∗ p(en)."
1387,0,[], Independence of Events,seg_313,example 5.58 student selection
1388,1,['probability'], Independence of Events,seg_313,"one class has 5 girls and 10 boys. another class has 8 girls and 7 boys. if one student each is selected from both classes, what is the probability that (i) both are boys, (ii) both are girls, and (iii) one boy and one girl?"
1389,1,"['independent', 'events', 'probability', 'event']", Independence of Events,seg_313,"solution 5.58 let “ai” denote the event of selecting a boy and “bi” denote the event of selecting a girl from ith class. p(a1) = 10∕15 = 2∕3 and p(a2) = 7∕15. the probability that both are boys = 2/3*7/15 = 14/45, (ii) probability that both are girls= 1/3*8/15 = 8/45, and (iii) one boy and one girl can come in two ways (boy from first class or second class). thus, the probability that one is a boy and other is a girl is (2/3)*(8/15) + (1/3)*(7/15) = 23/45 as the events “a” and “b” are independent."
1390,0,[], Independence of Events,seg_313,example 5.59 restaurant menu
1391,1,['probability'], Independence of Events,seg_313,"a restaurant offers 6 varieties of soup; of which 4 are vegetarian and 2 are nonveg soups; 10 varieties of the main course meal; of which 8 are nonveg and the rest 2 are vegetarian meals. if 80% of the customers take vegetarian soup, and among those 90% orders nonveg main meal, what is the probability that a randomly chosen customer will order a veg soup followed by a vegetarian main meal? if 95% of the people who orders nonveg soup also orders nonveg meals, what is the probability that a randomly chosen customer will eat vegetarian meal?"
1392,1,"['table', 'events', 'probability']", Independence of Events,seg_313,"solution 5.59 as there are interacting events, this problem is easy to crack using a table. probability that a randomly chosen customer will order a nonveg soup is 80%. probability that this is followed by vegetarian main meal is 0.80*0.10 = 0.08 or 8%. from table 5.7, we see that 20% of the customer’s order nonveg soup, among which 95% (or 19 customers) order nonveg main meal. this means that only 1% of the customers who order nonveg soup also orders veg meal. hence, the probability that a randomly chosen customer will eat vegetarian meal (irrespective of soup type) is 8 + 1 = 9%."
1393,1,"['conditional', 'discrete', 'independence', 'probability', 'random', 'conditional probabilities', 'events', 'continuous random variables', 'continuous', 'random variables', 'probabilities', 'variables', 'joint']", Independent Random Variables,seg_315,independence of events discussed earlier can be expressed in terms of conditional probabilities as p(x|y = y) = p(x). this immediately leads to independence of random variables. let x and y be discrete or continuous random variables. we define the independence in terms of probability of joint occurrence and individual occurrences as follows:
1394,1,"['random variables', 'independent', 'variables', 'random']", Independent Random Variables,seg_315,definition 5.10 two random variables x and y are independent if p(xy) = p(x) ∗ p(y).
1395,1,"['random variables', 'variables', 'random']", Independent Random Variables,seg_315,this definition can be extended to any number of random variables.
1396,1,"['functions', 'random variables', 'variables', 'distribution', 'independence', 'probability', 'random']", Independent Random Variables,seg_315,"as random variables have probability and distribution functions, we have several choices to define independence. two random variables x and y are independent if any of the following conditions is satisfied:– (i) f (x|y = y) = f (x), (ii) f (y|x = x) = f (y), (iii) f(x,y) = f(x) ∗ f(y), (iv) f(x|y = y) = f(x), and (v) f(y|x = x) = f(y). in addition, generating functions can also be used."
1397,1,"['probability', 'random', 'simulation', 'experiment', 'data', 'trials', 'ratio measure', 'simulated', 'event', 'trial', 'experimental', 'frequencies', 'outcomes', 'estimated', 'likelihood']", Independent Random Variables,seg_315,"an “empirical” probability is estimated after an experimental trial using known or observed frequencies of outcomes. here, the assumption is that the trials are independent. it may also be estimated using a computer simulation. experimental probability is derived numerically through the use of existing or simulated data. in the coin-tossing example, if we toss the coin 100 times and observe the number of heads that turn up, we could find the experimental probability of observing a head. objective probability is a ratio measure that expresses the likelihood of an event occurring in many repeated and identical trials of a random experiment."
1398,0,[], Independent Random Variables,seg_315,example 5.60 birthday sharing
1399,1,['probability'], Independent Random Variables,seg_315,"a class has 60 students, of which 20 are males. find (i) probability that the birthday of at least one student falls on a sunday. (ii) probability that at least three female students will share the same birthday on wednesday. (iii) probability that at most two male students will have their birthday on a weekend."
1400,1,['probability'], Independent Random Variables,seg_315,"solution 5.60 we assume that there are 52 weeks in a year (52 × 7 = 364 days). as the extra day in a year can be a sunday (for nonleap years) with probability 1/7 and other days with probability 6/7, we get the exact probability as follows. the"
1401,1,['probability'], Independent Random Variables,seg_315,"probability that the birthday of an arbitrary student falls on a sunday is p = 1∕7 and the probability that it does not fall on a sunday is q = 6∕7. to answer (i), we use the complement-and-conquer principle (p. 131), which is the probability that none of the student birthdays fall on a sunday. hence, the required probability is 1 − q60 = 1 − (6∕7)60. this answer is not exact. for non-leap years, the extra day can be a sunday with probability 1/7, we get the exact result as (6∕7)[1 − (6∕7)60] + (1∕7)[1 − (53∕365)60]. for leap years, the multipliers are 5/7 and 2/7. (ii) using do-little principle, the answer is one-probability that less than two female students share a birthday on wednesday. as there are 40 female"
1402,0,[], Independent Random Variables,seg_315,"other possibilities (only one student’s birthday is on wednesday or none have their birthday on wednesday) are irrelevant. (iii) the answer can be broken into three groups: (a) none have their birthday on a weekend, (b) only one male student has birthday on a weekend, and (c) exactly two male students have birth-"
1403,0,[], Independent Random Variables,seg_315,example 5.61 chessboard squares
1404,1,"['probability', 'random']", Independent Random Variables,seg_315,"if two squares are chosen at random on a chessboard, what is the probability that they will form a rectangle?"
1405,1,"['cases', 'events']", Independent Random Variables,seg_315,"solution 5.61 there are 64 squares in total on the chessboard. the chosen squares will form a rectangle when they are adjacent and either horizontally or vertically aligned (but not diagonally). let these be denoted by events x and y. the total favorable cases for x to materialize on any row are seven (as this could happen in (1,2),(2,3), ..., (7,8)) squares. as there are eight rows, the total number of favorable cases for x is 8 × 7 = 56. similarly, there are 56 cases for vertical alignment along any of the columns. thus, the total number of favorable cases is 56 + 56 = 112. total number of ways to choose"
1406,0,[], Independent Random Variables,seg_315,two squares on a chessboard of 64 squares is ( 6 2
1407,0,[], Independent Random Variables,seg_315,"4). hence, the required"
1408,1,"['sample', 'sample space']", Independent Random Variables,seg_315,probability = total favorable cases/number of points in the sample space = 112∕( 6 2
1409,1,"['outcomes', 'experiment', 'trials', 'probability', 'random', 'event']", FREQUENCY APPROACH,seg_317,"consider a random experiment that is conducted n times under identical conditions. if an event x occurs r times out of the n equally likely outcomes, the ratio r/n can be considered as the probability of occurrence of the event x. this probability may fluctuate for small values of n but will stabilize for large values. symbolically, p(x) = r∕n = number of favorable cases/total number of trials."
1410,1,"['sample', 'probabilities', 'trials', 'frequency', 'probability', 'random', 'sample space', 'distributions']", FREQUENCY APPROACH,seg_317,"definition 5.11 probabilities computed using frequency distributions or random trials that are repeated under identical conditions are called empirical probability. this approach can be used when the sample space is fuzzy, uncountable, or even unknown."
1411,1,"['range', 'probability', 'table']", FREQUENCY APPROACH,seg_317,"the bmi values of 200 patients are given in table 5.8. find the probability that a new patient will have a bmi in the range (i) “24–27,” (ii) between 21 and 27, and (iii) at least 30?"
1412,1,"['sample size', 'sample', 'probabilities', 'estimates', 'range', 'table', 'relative frequency', 'frequency']", FREQUENCY APPROACH,seg_317,"solution 5.62 from the table, the relative frequency of patients with bmi in the range “24–27” is p = 67∕200, which is the required answer. for part (ii) by the frequency approach, we get p = (56 + 67)∕200 = 123∕200; for part (iii) by the frequency approach, we get p = (11 + 2)∕200 = 0.065. the probabilities obtained are only estimates of the towards true value. if the sample size is increased from 200 to 2000, some of these probabilities may improve slightly toward true value."
1413,0,[], FREQUENCY APPROACH,seg_317,example 5.63 newspaper readership
1414,1,['probability'], FREQUENCY APPROACH,seg_317,consider the example 11 in page 5–26. what is the probability that a randomly chosen person reads either of the newspapers?
1415,1,"['events', 'probability']", FREQUENCY APPROACH,seg_317,"solution 5.63 define the events a and b as before. the required probability is p(a ∪ b) = p(a) + p(b) − p(a ∩ b). as we are given that p(a ∩ b) = 0.10, we could directly obtain p(a ∪ b) as 1 − p(a ∩ b) = 1 − 0.10 = 0.90."
1416,0,[], FREQUENCY APPROACH,seg_317,example 5.64 human blood groups
1417,1,"['percentage', 'probability']", FREQUENCY APPROACH,seg_317,"the human blood is categorized into four groups called “a,” “b,” “o,” and “ab” using the presence of an antigen on the cell marker. suppose that the percentage of people with these blood groups is 40, 12, 43, and 5, respectively. find the probability that (i) two persons getting married are of blood group “a,” (ii) two persons getting married are of the same blood group, and (iii) a child will be born with blood group “o.”"
1418,1,"['probabilities', 'table', 'cases', 'product rule', 'frequency', 'probability']", FREQUENCY APPROACH,seg_317,"solution 5.64 using the frequency approach, we expect the probability of any person with blood group “a” as 0.40. denote this as p(a) = 0.40. thus, the probability that both couples are of type “a” is 0.4*0.4 = 0.16 by the product rule. (ii) we need to add the probabilities for each couple to be of the same type. this gives p = 0.4 ∗ 0.4 + 0.12 ∗ 0.12 + 0.43 ∗ 0.43 + 0.05 ∗ 0.05 = 0.3618. (iii) assuming that all possible blood types are present among the parents, there are 16 possibilities. from the table 5.9 page 173, we see that an “o” occurs in nine cases. hence, the required probability is 9/16."
1419,1,"['data', 'entropy', 'probability']", Entropy Versus Probability,seg_319,"entropy is a term that originated in data communication. it is a measure of the uncertainty in a system. small entropy values indicate the presence of structure and large entropy values indicate randomness. probability and entropy are inversely related. this means that the probability of certainty is 1 while entropy of certainty is 0. while the probability quantifies the degree of belief, the entropy quantifies the lack of pattern or organization. it is used in data communications, decision tree induction, and many other fields [2]."
1420,1,"['sample', 'random variables', 'random', 'condition', 'variables', 'bayes theorem', 'conditional probability', 'conditional', 'events', 'probability', 'event', 'sample space', 'bayes']", BAYES THEOREM,seg_321,"this theorem was invented by the english mathematician and cleric thomas bayes (1702–1761) but was published posthumously in 1763. the basic ingredient of bayes theorem is conditional probability. here, the word “conditional” implies that an event depends on one or more conditions being fulfilled. usually, the condition is the occurrence of another event. conditional probability concept is always based on two or more events (in the same sample space) or random variables."
1421,1,"['sample', 'conditional probability', 'conditional', 'probability', 'event', 'sample space']", BAYES THEOREM,seg_321,definition 5.12 conditional probability is the probability of occurrence of an event with prior knowledge or assumption about another event defined on the same sample space.
1422,0,[], BAYES THEOREM,seg_321,example 5.65 soxes and colors
1423,1,"['probability', 'conditional', 'conditional probability']", BAYES THEOREM,seg_321,"suppose that a drawer contains n pairs of soxes. all soxes are exactly alike except for the color. in utter darkness, a boy wishes to grab just enough number of soxes so that at least two of them are of the same color (he need not have to go and grab another one). what is the minimum number of soxes to grab if (i) there are only two possible colors ((black and white), (ii) there are three possible colors?, (iii) what is the probability of obtaining two whites in a grab of size 3?, and (iv) a kid grabs four soxes. one of them is found to be a black. what is the conditional probability that the other three together will make two matching pairs?."
1424,1,['cases'], BAYES THEOREM,seg_321,solution 5.65 let the two colors be black (b) and white (w). minimum grabs cannot be 2 as they could be of opposite color. let it be three. the possible cases
1425,1,"['permutations', 'combination', 'case', 'cases', 'probability']", BAYES THEOREM,seg_321,"are {b,b,b}, {b,b,w}, {b,w,b}, {w,b,b}, {b,w,w}, {w,b,w}, {w,w,b}, and {w,w,w}. because the order is unimportant, some of these are exactly identical. (i) as every combination should include either two blacks or two whites, the minimum number of soxes to grab is 3. for case (ii), let the three colors be black (b), red (r), and white (w). if the minimum number of soxes grabbed is 3, there is only one case {b,r,w} (or its permutations) where a match cannot occur. however, if the minimum number of soxes grabbed is 4, a match will always occur. (iii) the answer is easily seen to be 0.5 from above. (iv) as one of them is black, the other three should contain two whites and one black to make two matching pairs or all three blacks. the favorable cases are {b,w,w}, {w,b,w},{w,w,b}, and {b,b,b}. the required probability is 4/8 = 1/2."
1426,1,"['conditional', 'observation', 'probability', 'conditional probability', 'hypothesis']", BAYES THEOREM,seg_321,"bayes theorem is a convenient way to compute the conditional probability of a hypothesis h given that an observation (evidence) e has occurred using the probability of an observation, given that a hypothesis has occurred."
1427,1,"['joint', 'probability', 'hypothesis']", BAYES THEOREM,seg_321,lemma 5 probability of hypothesis given evidence is the ratio of joint occurrence of hypothesis and evidence over probability of evidence. p(h|e) = p(h ∩ e)∕p(e).
1428,1,"['probabilities', 'probability', 'hypothesis']", BAYES THEOREM,seg_321,corollary 3 the unconditional probability of hypothesis is the sum of the products of the probabilities of hypothesis given evidence and probability of evidence; and probability of hypothesis given no evidence and probability of no-evidence. p(h) = p(h|e).p(e) + p(h|e).p(e).
1429,1,"['probabilities', 'prior probabilities', 'bayes theorem', 'likelihood', 'probability', 'posterior probability', 'bayes', 'posterior']", BAYES THEOREM,seg_321,"here, p(h|e) is the posterior probability. these can be obtained from each other with the help of prior probabilities and likelihood as given by bayes theorem."
1430,1,"['sample', 'estimated', 'probabilities', 'bayes theorem', 'likelihood', 'data', 'information', 'associated', 'probability', 'risks', 'posterior probability', 'bayes', 'posterior']", Bayes Theorem for Conditional Probability,seg_323,"this theorem is also known as the law of inverse probability. bayes theorem is used to calculate posterior probability in terms of priors. in other words, bayes theorem analyzes the root causes and associated risks of alternatives using empirical data to come up with the best plausible aposteriori probability or probability of occurrence of hypothetical causes. conceptually, posterior = likelihood × prior/evidence where likelihood is estimated from sample data or found by other means. it expresses aposteriori probability in terms of apriori probabilities using newly acquired information."
1431,1,"['events', 'probability', 'outcomes']", Bayes Theorem for Conditional Probability,seg_323,"let x and y be two arbitrary events. suppose that y has already occurred. if x and y have some outcomes in common, x will occur iff x ∩ y occurs. this is symbolically denoted as p(x|y) = p(x ∩ y)∕p(y). cross-multiply to get p(x ∩ y) = p(y) ∗ p(x|y). as x and y are arbitrary, this can also be expressed as p(x ∩ y) = p(x) ∗ p(y|x). this is the multiplicative law of probability discussed earlier. in p(x|y) = p(x ∩ y)∕p(y), replace the numerator p(x ∩ y) by p(x).p(y|x). substitute in the aforementioned to get p(x|y) = p(x).p(y|x)∕p(y). using the law of total probability (page 133), we have p(y) = p(y ∩ x) + p(y ∩ x). reorder the events to get p(y) = p(x ∩ y) + p(x ∩ y). now write p(x ∩ y) = p(x) ∗ p(y|x) and p(x ∩ y) = p(x) ∗ p(y|x)."
1432,1,"['bayes theorem', 'bayes', 'hypothesis']", Bayes Theorem for Conditional Probability,seg_323,"this is the simplest form of bayes theorem. using the hypothesis and evidence notation used earlier, if the nonoccurrence of the hypothesis is denoted by p(h), we get"
1433,1,"['events', 'event']", Bayes Theorem for Conditional Probability,seg_323,"next consider n events a1,a2, … ,an. let b be an event that spans at least two of the a′"
1434,1,"['probabilities', 'estimate', 'bayes theorem', 'information', 'probability', 'bayes']", Bayes Theorem for Conditional Probability,seg_323,"is. if the apriori probabilities of occurrence of p(b|ai) are known, we could utilize the information in obtaining an estimate of the aposteriori probability using bayes theorem. symbolically, it can be written as p(ai|b) ="
1435,1,"['case', 'data', 'set', 'probability']", Bayes Theorem for Conditional Probability,seg_323,"proof: let ai denote possible explanations for a given set of data b. as the data size increases, the probability p(b|ai)p(ai) increases. if a is decomposed as a = a1 ∪ a2 ∪ · · ·an, then b can be represented as b = ba1 ∪ ba2 ∪ · · · ∪ ban. thus p(b) = ∑ip(bai) = ∑ip(ai).p(b|ai). as p(ai|b) = p(ai).p(b|ai)∕p(b), the proof follows by substituting the value for p(b). this proves the theorem for the general case."
1436,1,"['estimate', 'bayes theorem', 'case', 'hypotheses', 'probability', 'event', 'bayes', 'hypothesis']", Bayes Theorem for Conditional Probability,seg_323,"in multiple hypotheses situations, bayes theorem provides a “best” estimate for the probability of evidence under the assumption that each hypothesis is true. a generalization to the three event case easily follows as p(ab|c) = p(a|bc) ∗ p(b|c) = p(b|ac) ∗ p(a|c)."
1437,0,[], Bayes Theorem for Conditional Probability,seg_323,example 5.66 atm cash withdrawal
1438,1,"['probability', 'information']", Bayes Theorem for Conditional Probability,seg_323,"consider cash withdrawals at an atm booth. from analysis of prior fraudulent transaction, a bank has found that the probability of any transaction to be fraudulent is one in thousand (p(fraud)= 0.001), 90% of fraudulent transactions are for amounts above 2000 (i.e., p(amount 2000|fraud) = 0.90), and 99% of cash withdrawals for amounts 2000 are genuine. using this information, what is the probability that a transaction is fraudulent, given that the withdrawal amount is 4000?"
1439,1,"['bayes theorem', 'bayes']", Bayes Theorem for Conditional Probability,seg_323,"solution 5.66 we have p(fraud) = 0.001, p(amount   2000|fraud) = 0.90, p(amount   2000|not fraud) = 0.99 by bayes theorem, p(fraud| amount 2000) = p(fraud) * p(amount   2000|fraud)/[p(fraud) * p(amount   2000|fraud) + p(not fraud) * p(amount   2000| not fraud)] = 0.001 * 0.90 / [0.001 * 0.90 + 0.999 * 0.99] = 0.0009/(0.0009 + 0.98901) = 0.90917 e-3 = 0.000909."
1440,1,"['bayes theorem', 'bayes']", Bayes Theorem for Conditional Probability,seg_323,"5.17.1.1 odds-likelihood ratio form of bayes theorem in some applications, we are interested in finding the ratio of the likelihoods"
1441,0,[], Bayes Theorem for Conditional Probability,seg_323,p(hypothesis1| evidence) p(hypothesis1)p(evidence/hypothesis1) = . (5.23) p(hypothesis2| evidence) p(hypothesis2)p(evidence/hypothesis2)
1442,0,[], Bayes Theorem for Conditional Probability,seg_323,example 5.67 blood type of parents
1443,1,"['combination', 'probabilities', 'probability']", Bayes Theorem for Conditional Probability,seg_323,"table 5.10 gives the break-down of the actual count of patients who visited a clinic, with the combination blood type of parents, where columns denote father’s and rows denote mother’s blood type1. a newly admitted patient only knows that her father was “o” blood type. find the probabilities that (i) her mother had blood type ab and (ii) mother was also “o” blood type. (iii) if another patient knows only that mother’s blood type is ab, what is the probability that the father’s blood type is a or o?"
1444,1,"['table', 'event']", Bayes Theorem for Conditional Probability,seg_323,"solution 5.67 let x denote the event that father was “o” blood type. let yi denote the event that mother’s blood type is as given on the ith row of table 5.10. from the table 5.9 below, we find that if father is of type “o” and mother is of type “ab,” there are two possibilities for the child to have blood types a or b."
1445,1,"['bayes theorem', 'bayes', 'table']", Bayes Theorem for Conditional Probability,seg_323,"from the above-mentioned table, we get p(y1) = 183∕456,p(y2) = 52∕456, p(y3) = 200∕456, and p(y4) = 21∕456. similarly p(x|y1) = 68∕183,p(x|y2) = 23∕52,p(x|y3) = 72∕200,p(x|y4) = 9∕21. for question (i), we need to find p(yi|x) for i = 4 and 3. using bayes theorem p(yi|x)="
1446,1,['probability'], Bayes Theorem for Conditional Probability,seg_323,"is 21/456 * 9/21= 9/456. substitute the values to get the answer to p(y4|x) = 9∕456∕[172∕456] = 9∕172. for part (ii), the numerator is p(y3).p(x|y3) = 200∕456 ∗ 72∕200 = 72∕456, so that the required probability is 72/172. as the gcd(72,172)= 4, divide both numerator and denominator by the gcd to get the answer as 18/43."
1447,1,"['table', 'bayes theorem', 'event', 'bayes']", Bayes Theorem for Conditional Probability,seg_323,"(ii) let y denote the event that mother’s blood type is ab. let xi denote the event that father’s blood type is as given on the ith column of table 5.10. as done earlier, we get p(x1) = 200∕456,p(x2) = 59∕456,p(x3) = 172∕456, and p(x4) = 25∕456. similarly p(y|x1) = 8∕200,p(y|x2) = 2∕59,p(y|x3) = 9∕172, and p(y|x4) = 2∕25. for part (iii), we need to find p(xi|y) for i = 1 and 3 (blood group “a” or “o”). using bayes theorem, this is p(xi|y)="
1448,1,['probabilities'], Bayes Theorem for Conditional Probability,seg_323,"the denominator is 200/456 * 8/200 + 59/456 * 2/59 + 172/456 * 9/172 + 25/456 * 2/25 = 21/456. the numerator is 200/456 * 8/200 = 8/456. substitute the above-mentioned values to get the answer for blood group of father = “a” as (8/456)/(21/456) = 8/21. answer for father’s blood group “o” differs only in the numerator. as the numerator is 172/456 * 9/172 = 9/456, we get the answer for subpart as (9/456)/(21/456) = 9/21. add these two probabilities to get the answer that father is of type a or o as 8/21 + 9/21 = 17/21 = 0.809523 809523."
1449,1,"['conditional', 'product rule', 'probability', 'conditional probability']", Bayes Theorem for Conditional Probability,seg_323,"5.17.1.2 product rule for conditional probability p(ab|c) = p(a|c) ⋅ p(b|ac) = p(b|c).p(a|bc), where ab denotes a ∩ b, and so on."
1450,1,"['probabilities', 'decision theory', 'data', 'statistical']", Bayes Classification Rule,seg_325,"consider two propositions a and b whose apriori probabilities are known. let u(a) and u(b) denote the utilities of propositions a and b, respectively. then, a is preferred over b if u(a) u(b). in data mining applications, bayes’ rule is used to specify how the learning system updates its beliefs as new data instances arrive. this is the basic principle of statistical decision theory."
1451,1,['utility'], Bayes Classification Rule,seg_325,"5.17.2.1 rule of expected utility assuming a as the action and b as the consequence, this rule gives the utility of a as"
1452,1,"['estimated', 'probabilities', 'prior probabilities']", Bayes Classification Rule,seg_325,"a disadvantage of this approach is that they depend on prior probabilities of propositions explicitly. if these are unknown, they need to be estimated (using point esti-"
1453,1,"['algorithm', 'sampling', 'process']", Bayes Classification Rule,seg_325,"2 mation, em algorithm , stochastic sampling, or parametric approximations) before starting the decision process."
1454,1,"['probabilities', 'data', 'entropy']", Bayes Classification Rule,seg_325,"now consider the problem of classifying a dichotomous attribute using data instances. if the two attribute values are “yes” and “no,” we could obtain a measure of entropy using the probabilities of yes and no responses as"
1455,1,['range'], Bayes Classification Rule,seg_325,"as the logarithm of a number in the range (0,1) is negative, it combines with the minus sign to return a positive number."
1456,1,"['urn models', 'probability', 'confidence', 'average']", SUMMARY,seg_327,"this chapter introduced the concepts, tools, and techniques of probability in an intuitive way. several examples drawn from different fields help the readers in honing the problem-solving skills, and applying it with confidence to practical problems. several self-understanding and concrete examples make the book accessible to even average students. see reference 112 for a historical review, references 113 and 114 for theoretical aspects, reference 115 for urn models, and references 116–120 for further examples."
1457,1,"['prediction', 'outcome', 'uncertainty', 'dependent', 'conditional', 'data', 'probability', 'conditional probability', 'level', 'dependence', 'experiments', 'outcomes']", SUMMARY,seg_327,"data uncertainty prevail in most experiments. with scientific rules and regulations of probability, the exactitude and their remedies in the data could be understood and interpreted. two or more outcomes in an application might be dependent. as explained in this chapter, the level of their dependence could be calculated and utilized. when two outcomes are dependent, the prediction of one outcome becomes more precise based on the occurrence of a connected outcome using conditional probability. this type of prediction is the basic foundation of decision making in engineering and applied sciences."
1458,1,"['discrete', 'probability', 'event']", SUMMARY,seg_327,"a) a probability of 0.5 is realized only for discrete event,"
1459,1,['probabilities'], SUMMARY,seg_327,"b) addition and subtraction are the only operations on probabilities,"
1460,1,"['probability of an event', 'venn diagram', 'venn', 'event', 'probability']", SUMMARY,seg_327,"c) a venn diagram can quantify the probability of an event,"
1461,1,"['events', 'probability']", SUMMARY,seg_327,"e) combining events using or operator often increases the probability,"
1462,1,"['discrete', 'events', 'venn diagrams', 'venn']", SUMMARY,seg_327,"f) venn diagrams can represent only discrete events,"
1463,0,[], SUMMARY,seg_327,5.2 the mechanism that generates 5.9 convert to fractional form p/q (i)
1464,1,"['sample', 'combination', 'experiment', 'random', 'sample space', 'outcomes', 'event']", SUMMARY,seg_327,"uncertain outcomes is called 0.2727 (ii) 0.428571 428571, (iii) 0.285714 285714, (iv) 0.809523 (a) event (b) random experiment 809523 (c) sample space (d) combination"
1465,1,"['permutations', 'mutually exclusive', 'event']", SUMMARY,seg_327,5.10 an event e1 can happen in m 5.3 permutations of objects in which ways and a mutually exclusive
1466,1,['event'], SUMMARY,seg_327,nothing is in its original position is event e2 can happen in n ways. in
1467,1,"['derangement', 'combination']", SUMMARY,seg_327,"(a) power-set (b) circular perhow many ways, can e1 and e2 mutation (c) combination (d) happen? derangement."
1468,1,['outcomes'], SUMMARY,seg_327,5.11 an elevator starts with six girls 5.4 the identifiable outcomes of a ranfrom the ground floor to all other
1469,1,"['experiment', 'events', 'probability', 'event']", SUMMARY,seg_327,"dom experiment is called floors of a four storeyed building. (a) event (b) probability (c) samif all disembarks, define appropriple space (d) power-set ate events."
1470,1,"['sample', 'sets', 'probability', 'outcomes']", SUMMARY,seg_327,5.5 decimal form of probability 5.12 sets of outcomes of a sample
1471,1,"['range', 'cardinality', 'event']", SUMMARY,seg_327,always take values in the range space meeting some specifications (a) −1 to +1 (b) 0 to 1 (c) −0.5 is (a) subspace (b) partition (c) to +0.5 (d) any positive value cardinality (d) event
1472,1,['random'], SUMMARY,seg_327,5.6 a pharmacist has six medicine 5.13 if 4 squares are chosen at random
1473,0,[], SUMMARY,seg_327,"packs, three of which are of one on a chessboard, what is the probkind, two of another kind and a last ability that they will form a bigger one of a single kind. how many square?"
1474,0,[], SUMMARY,seg_327,ways are there to arrange them on 5.14 a man has five pairs of soxes of
1475,0,[], SUMMARY,seg_327,a shelf? different styles and colors. one
1476,0,[], SUMMARY,seg_327,"5.7 a video store has nine cassettes, of night when the power was off, he"
1477,1,"['probability', 'random']", SUMMARY,seg_327,"selects two soxes at random. find which four are of one kind, three the probability that they form a are of a second kind, and two are matching pair. of a third kind to be arranged on a"
1478,1,"['events', 'set']", SUMMARY,seg_327,"rack. in how many ways, can this 5.15 consider a set of ordered events"
1479,1,"['experiment', 'random']", SUMMARY,seg_327,be done? of a random experiment. express
1480,1,"['events', 'multinomial', 'probability']", SUMMARY,seg_327,following events using probability 5.8 evaluate the multinomial coeffi-
1481,0,[], SUMMARY,seg_327,5.16 prove or disprove the following: crust is fixed as thick how many
1482,0,[], SUMMARY,seg_327,6 × 8 pieces of equal size. the 5.18 there are 60 students in a class. bar can be broken only along a
1483,1,['probability'], SUMMARY,seg_327,what is the probability that (i) two straight line horizontally or verti-
1484,0,[], SUMMARY,seg_327,or more students share the same cally but not diagonally. if the bars
1485,1,['probability'], SUMMARY,seg_327,"birthday? (ii) there are exactly k are broken one at a time, what is days in which no one’s birthday the probability of obtaining eight falls? 2 × 3 pieces in seven tries?. 5.19 how many people are there in a"
1486,1,['probability'], SUMMARY,seg_327,"5.26 a school kid has 10 varieties of room to have the probability that shirt, 7 varieties of pants, and 5 two or more people will have the same birthday is (i) greater than varieties of tie. in how many ways, 0.5? and (ii) less than 0.75? can the kid dress up?"
1487,0,['n'], SUMMARY,seg_327,5.20 a diabetes medicine comes as 5.27 consider n tosses of a die with
1488,0,[], SUMMARY,seg_327,"a tablet, capsule, nasal spray, or faces numbered 1–6. what is the injection. if each of them is availprobability that the top face numable in regular and generic variber is greater than the bottom face eties, in how many ways, can it be number?."
1489,1,['sets'], SUMMARY,seg_327,prescribed? 5.28 an electronic circuit has n2 com5.21 let x and y be two finite sets. ponents that look identical. a
1490,1,['set'], SUMMARY,seg_327,define x ⊕ y as the set of all eletechnician has time to inspect all ments in x or y but not in both. except n of the components in any
1491,0,[], SUMMARY,seg_327,"verify whether ⊕ is commutative trip. if the components are chosen,"
1492,0,[], SUMMARY,seg_327,and associative. one after another find the number
1493,1,['events'], SUMMARY,seg_327,"of ways to choose the components 5.22 if x and y are nondisjoint events,"
1494,0,[], SUMMARY,seg_327,"prove (i) p(x ∪ y) + p(x ∩ y) = in any trip. using stirling’s forp(x) + p(y), (ii) p[(x ∩ y) ∪ mula for factorials obtain a simpli(y ∩ x)] = p(x) + p(y) − 2 ∗ fied expression for it."
1495,1,"['sample', 'sample space']", SUMMARY,seg_327,p(x ∩ y) 5.29 describe a suitable sample space
1496,1,['events'], SUMMARY,seg_327,"5.23 if x and y are nondisjoint events, ω for the following experi-"
1497,1,"['absolute value', 'probabilities']", SUMMARY,seg_327,"arrange the following probabilities ments:–(i) absolute value of the in increasing order of magnitude: difference between the numbers at p(x),p(x ∩ y),p(x ∪ y),p(x) + the top and bottom when a die is"
1498,1,['event'], SUMMARY,seg_327,"taining m white and n red balls; eties of crust (thin crust, medium, (iii) two dice are thrown and an and thick crust), 4 varieties of event is specified as “the number cheese, and 12 varieties of toppings. how many different variat top of second die is greater than eties of pizzas can be ordered?. if the one on the first die.”"
1499,0,[], SUMMARY,seg_327,5.30 a committee consists of five memeach winning person gets an
1500,1,"['expected value', 'trials', 'probability']", SUMMARY,seg_327,"bers of whom the three males are amount of 1 from the loser. find (x, y, z) and the females are (u,v). the expected value of the amount if a meeting is attended by only owned by the winner at nth game. three members, what is the probwhat is the probability of (i) p ability that (i) at least one female winning the game?, (ii) the game is present; (ii) if v was present in being over in 120 plays?, and (iii) both have 50:50 in n trials. the meeting, what is the probabil-"
1501,0,[], SUMMARY,seg_327,"ity that y and z were also present? 5.35 there are 160 customers who buy how many ways are there to form electronics, and 120 customers"
1502,0,[], SUMMARY,seg_327,"a subcommittee of size 3 compriswho buy other items. among the ing (i) at least two males, (ii) at 160 customers, 30 also buy other most one female?, and (iii) exactly items. if nine customers are ran-"
1503,0,[], SUMMARY,seg_327,"two males? domly chosen, what is the prob-"
1504,0,[], SUMMARY,seg_327,ability that (i) a customer who 5.31 a furniture shop makes a vari-
1505,1,['process'], SUMMARY,seg_327,"bought only other items will get ety of furnitures. each piece goes selected? and (ii) a customer through three processes:–(i) cutwho bought both items will get ting process, (ii) drilling process, selected? and (iii) assembly and finish-"
1506,1,['process'], SUMMARY,seg_327,ing process. a quality inspector 5.36 a safe locker has two locks. an inspects each furniture before it is intruder has gained access to “n”
1507,1,"['probability', 'random']", SUMMARY,seg_327,"shipped. the respective probabilkeys. if two keys are chosen at ity of a defect in each of the stages random each time, (i) what is the is 1/60, 1/20 and 1/80. if a finished probability that the intruder will"
1508,1,['probability'], SUMMARY,seg_327,"furniture is found to be defective, be able to open the locker in first what is the probability that (i) it is try and (ii) the locker in third try?"
1509,0,[], SUMMARY,seg_327,a cutting defect and (ii) it is due 5.37 if x ∪ y is translated into words
1510,1,['event'], SUMMARY,seg_327,"cess? and y occurs,” x − y as “x but 5.32 is the event x ∪ y defined when not y occurs,” x as “x does"
1511,1,"['continuous', 'discrete']", SUMMARY,seg_327,"either of them is discrete and the not occur,” translate the following other is continuous? is the concept expressions into words:"
1512,1,"['independence', 'events']", SUMMARY,seg_327,"of independence defined for con(i) x ∩ y (ii) x − x ∩ y , and (iii) tinuous events? x ∪ y − x ∩ y ."
1513,0,['n'], SUMMARY,seg_327,5.33 there are n different tasks to be 5.38 a train has 3 general (unreserved)
1514,0,['n'], SUMMARY,seg_327,"assigned to m employees where compartments, 12 reserved n   m. how many ways are there coaches, and a pantry car in if every employee is assigned at addition to the engine. out of"
1515,0,[], SUMMARY,seg_327,"least one task? 12 reserved coaches, 3 are ac"
1516,0,[], SUMMARY,seg_327,5.34 two persons p and q play a game coaches and the rest are ordi-
1517,1,['probability'], SUMMARY,seg_327,"with respective initial amounts of nary coaches. how many ways are 70 and 30. probability of p winthere to connect the coaches if (i) ning is p, and for q it is 1−p. the pantry car can never be the first"
1518,0,[], SUMMARY,seg_327,or last and (ii) the coach immedihow many ways are there for seatately behind the engine and the ing arrangement in the aircraft?
1519,0,[], SUMMARY,seg_327,rear-end coach are both ordinary?. 5.43 there are 22 people in a hospi(iii) if all ac coaches cannot be tal including 2 twins who were
1520,0,[], SUMMARY,seg_327,together? born on the same day. what is the
1521,0,[], SUMMARY,seg_327,mathematical expression to find 5.39 a university wishes to assign a
1522,1,['probability'], SUMMARY,seg_327,"the probability that at least three unique 6 digit number to each persons have the same birthday? of the enrolled students with the at most two people have the same following restrictions that :– (i) birthday? no-one except the twins the student number cannot start have a common birthday? with digit “0,” (ii) digits cannot be"
1523,0,[], SUMMARY,seg_327,"repeated in the first three places, 5.44 a customer needs change for a 10 but repetition is allowed in subdollar bill in 5 dollar, 2 dollar, and sequent digits. how many student 1 dollar bills. how many ways are numbers can be generated? there to make the change?"
1524,0,[], SUMMARY,seg_327,5.40 a fruit merchant has five bas5.45 use the pie principle to find
1525,0,[], SUMMARY,seg_327,"kets of apples (all of one kind), how many integer solutions exist"
1526,0,[], SUMMARY,seg_327,"three baskets of oranges (all of one for the equation x1 + x2 + x3 = 11 kind), and two baskets of bananas where x1 ≤ 3, x2 ≤ 4, and x3 ≤ 6."
1527,0,[], SUMMARY,seg_327,to be displayed in front of the 5.46 a street has 10 houses on one shop. (i) how many ways can this side and 12 houses on the other be put if all of them are placed side. each of the houses should in one straight line (ii) if they are be numbered sequentially by start-
1528,0,[], SUMMARY,seg_327,arranged as a circle? ing from either end of the road
1529,0,[], SUMMARY,seg_327,5.41 a restaurant offers 5 varieties of with three digits. how many ways
1530,0,[], SUMMARY,seg_327,"soup, 10 varieties of the main can this be done if only the digcourse meal, and 5 varieties of ice its {0,1,2,3,4,5} are used? if all cream or cake after meal. how houses on one side get even house many choices are possible for a numbers and all houses on the person who will take any of the other side get odd numbers? if 0 choices? how many choices are cannot be used as a first digit for possible if a person does not take numbering."
1531,0,[], SUMMARY,seg_327,"2 of the 10 varieties of the main 5.47 a rocket can fail independently course and 3 varieties of ice-cream due to navigation error(ne),"
1532,0,[], SUMMARY,seg_327,"choices? software error(se), or hardware"
1533,1,['probability'], SUMMARY,seg_327,5.42 the passenger area of a jumbo-jet fault(hf). the probability of ne
1534,1,"['errors', 'probability']", SUMMARY,seg_327,"can be divided into an execuis twice as large as that of se, and tive section (xs) and an economy the probability of se is three times section (es). there are three difas large as hf. assume that it ferent ways in which xs can be failed. (i) if there were no navigaarranged and five different ways in tion errors, what is the probability which es can be arranged. total that it was due to one of the other"
1535,1,"['probability', 'coefficients']", SUMMARY,seg_327,"faults? (ii) what is the probability px2 − qx + r = 0 is formed, whose that it was due to hardware faults nonzero coefficients (p, q, r) are given that there was no software determined by the number that"
1536,1,['probability'], SUMMARY,seg_327,fault? turns up when this die is thrown. what is the probability that the 5.48 there are four blood group types roots are real if the numbers that
1537,0,[], SUMMARY,seg_327,"of rh-factors {+, −}. assume ing are 3 and 4?. what is the probthat all of them are equally likely. ability that the roots are equal? among a group of 50 students,"
1538,1,['probability'], SUMMARY,seg_327,"what is the probability that (i) 5.52 a class comprises 35 males and 25 there are at least 5 students with females. if five students each have o blood group.(ii) probability for to give a seminar randomly each at least 10 students with +ve day, find the probability that (i) all rh-factor and blood group a or b. five of them on a day are males"
1539,0,[], SUMMARY,seg_327,and (ii) three are males and rest 5.49 consider a quadratic equation
1540,1,['coefficients'], SUMMARY,seg_327,"px2 + qx + r = 0, whose nonzero females. how many ways are there if at least two boys are to give the coefficients are determined by the seminar each day? number that turns up when a die"
1541,1,['probability'], SUMMARY,seg_327,"is thrown. find the probability that 5.53 a trailer truck has 10 identi(i) the discriminant b2 − 4 ∗ a ∗ c cal looking wheels. a mechanic is an integer, (ii) the roots are interemoves the brake pedals for gers, (iii) at least one integer root, cleaning and returns them back to and (iv) there are no real roots. the wheels after some time. what"
1542,1,['probability'], SUMMARY,seg_327,is the probability that (i) all brake 5.50 a family has n friends. they
1543,0,[], SUMMARY,seg_327,pedals are returned to their correct invite m (1 m n∕7) friends wheels? and (ii) none of the brake randomly on each day from sunpedals match their corresponding day to saturday to their house wheels? where some of the invited guests
1544,1,['sets'], SUMMARY,seg_327,"may overlap on different days, but 5.54 an online examination has n questhe group as a whole are different tions, which are taken together by on each day (no identical groups m (≥2) students sitting in a cominvited twice). for instance, if puter laboratory. to avoid copym = 2 and {x,y} are invited on ing, the instructor sets it up in such sunday, {x,z} or {y ,z} may be a way that the questions for each invited on another day. what is the student are generated using unique"
1545,1,"['random numbers', 'random']", SUMMARY,seg_327,minimum and maximum number random numbers between 1 and of friends who visit the house in n (so that the same question is
1546,0,[], SUMMARY,seg_327,"a week? not displayed twice, and adjacent"
1547,0,[], SUMMARY,seg_327,5.51 consider a die with six faces. they students may get different ques-
1548,1,['probability'], SUMMARY,seg_327,"are not numbered from 1 to 6, but tion orders). find the probability it is known that two of the numthat (i) all questions are generbers repeat once (resulting in four ated in exactly the same order to numbers). a quadratic equation two students sitting next to each"
1549,1,['probability'], SUMMARY,seg_327,"other?. (ii) exactly k of the ques5.58 two exactly identical deck of tions are generated the same order cards is shuffled. then, two cards to two students sitting next to each each are drawn from the pool other?. (iii) all questions are genand kept face down. a player is erated differently for two or more allowed to take one pair of face students? down cards at a time until k identical pairs are obtained. what is the 5.55 assume that n pairs of husbands probability of obtaining k match-"
1550,1,"['paired', 'probability', 'random']", SUMMARY,seg_327,"and wives enter a club, each one ing pairs? wearing a hat. the hats are handed 5.59 an urn contains m blue and n over to a waitress for keeping. red balls. a second urn contains after a short while, all n pairs of a blue and b red balls. two balls people assemble for a dance each are drawn at random from the first one wearing a hat. if the waiter urn and put into the second urn. distributes the hats at random, and then, a ball is drawn from the secthe dancing pairs are formed at ond urn. find the probability that random, find the probability that (i) it is blue and (ii) it is red. (i) no couple are properly paired,"
1551,1,"['probability', 'random']", SUMMARY,seg_327,"and nobody gets their own hat; 5.60 there are n pairs of shoes in a box (ii) exactly k of the couple are (total 2n shoes). if m( n) shoes matched, but no one get their are chosen at random from the matching hat; (iii) exactly k of box without looking at the shoe, the hats are matched, but no coufind the probability that (i) none ple are matched; (iv) exactly k of of the m shoes have a matching the couple and m of the hats are pair, (ii) at most two of them have matched; and (v) all husbands and a matching pair, and (iii) in how wives are matched but none of the many ways can you choose m pairs"
1552,0,['n'], SUMMARY,seg_327,hats are matched. (  n) such that at least one match-
1553,1,"['biased', 'probability']", SUMMARY,seg_327,ing pair is obtained. 5.56 a biased coin has probability p of
1554,1,"['without replacement', 'conditional', 'replacement', 'events', 'probability', 'outcomes']", SUMMARY,seg_327,"heads showing up. if it is tossed 12 5.61 a lottery selects the winner by times, find the conditional probadrawing 5 numbers between 1 and bility that for each of the follow39 randomly. find probability of ing if it is known that a total of six the following events if (i) none of heads have been obtained: (i) the the numbers can repeat (all numfirst four outcomes are htht and bers are unique or it is like sam(ii) they are ttth. pling without replacement) and"
1555,0,[], SUMMARY,seg_327,"(ii) numbers can repeat: (a) all 5 5.57 a telephone number has eight dignumbers are odd, (b) all numbers"
1556,1,"['independent', 'error', 'probability']", SUMMARY,seg_327,"its. what is the probability for are below 25, (c) at least 2 numeach of the following, if startbers are above 30, and (d) none of ing (leftmost) digits cannot be the numbers are primes. zeros? (i) four or more digits are repeated?, (ii) at most six digits 5.62 a device is manufactured in m are repeated?, and (iii) none of the independent successive steps. digits are repeated? probability of making an error at"
1557,1,['probability'], SUMMARY,seg_327,"step k is pk. find the probability and 7? and (ii) at least by 3 and 7 that out of n manufactured items, but not by 5?"
1558,1,"['events', 'independent']", SUMMARY,seg_327,"(ii) at most two are defectives, p(b|a) = p(a ∩ b) or (ii) p(ac) ∗ (iii) a lot contains between two p(bc) = p(ac ∩ bc) when events and five defectives, and (iv) none a and b are independent. are defectives. 5.68 a faulty electronics appliance has 5.63 consider a quadratic equation eight exactly looking components."
1559,1,['samples'], SUMMARY,seg_327,"px2 − qx + r = 0, whose nonzero bob samples four components"
1560,1,"['samples', 'probability', 'tests']", SUMMARY,seg_327,"coefficients (p, q, r) are deterarbitrarily, tests each of them indimined by the number that turns vidually, writes his initial “b” up when a regular pyramid with on each of them and puts them five faces numbered 1–5 is thrown. back. after he is finished, peter find the number of ways in which comes and samples three compo(i) the equation will have real nents arbitrarily and does the same roots, (ii) equal roots, (iii) imagitesting, writes his initial “p” on nary roots, (iv) both integer roots, each of the 3 and puts them back. and (vi) exactly one integer root. what is the probability that (i)"
1561,0,[], SUMMARY,seg_327,5.64 a software company has 8 vb none of the components have both
1562,0,[], SUMMARY,seg_327,"experts, 5 c++ experts, 10 java marks, (ii)exactly two of the comexperts, and 4 c# expert programponents will have marks “p” and mers. a new project that requires “b”? and (iii) at least two of them 3 vb, 2 java, and 3 c++ experts is have both the marks?."
1563,0,[], SUMMARY,seg_327,"to be initiated. in how many ways 5.69 a group of 12 school kids are on can the team be formed? if another a sightseeing trip. the instructor project requires two each of vb wants to stock enough drinks of and c# experts, three each of c++ each kind. there are seven stuand java experts, how many ways dents who drink coffee, four stuare there to form the team? dents who drink tea, nine students"
1564,1,['coefficient'], SUMMARY,seg_327,"5.65 the simple matching coefficient who drink fruit juice, three stu-"
1565,1,['coefficient'], SUMMARY,seg_327,"(smc) used in cluster analysis dents who drink coffee and tea, is a similarity coefficient defined four who drinks coffee and juice, on binary strings as smc(x, y) = and two who drink tea and juice."
1566,1,"['probability', 'data', 'random']", SUMMARY,seg_327,"1 (number of positions in which x (i) how many students drink all and y match), where d= total numthree beverages? (ii) a student is ber of bits or the size of the data. selected at random and is found to if x and y are d bits long, what is drink fruit juice. what is the probthe probability that (i) smc takes ability that student does not drink the value 1, (ii) smc takes the coffee."
1567,0,[], SUMMARY,seg_327,(d even). with respective male and female
1568,1,['test'], SUMMARY,seg_327,"and 200 are divisible by (i) 3, 5, (32,12). an aptitude test is given"
1569,1,['probability'], SUMMARY,seg_327,"in all the three classes. if a girl total ocean area. the probable hit scored the highest marks followed point is 85% in ocean. what is the by two boys overall, what is the probability that it will (i) fall in the probability that (i) all came from atlantic ocean and (ii) it will hit y?. (ii) female topper came from land mass or atlantic?"
1570,1,['tests'], SUMMARY,seg_327,z and males came from x and y? 5.75 two medical tests are being devel-
1571,0,[], SUMMARY,seg_327,5.71 the underground water supply oped for a new virus infection.
1572,1,"['test', 'probability']", SUMMARY,seg_327,system in a city in northern latthe first test t1 has probability itude has 5 major pipes and 50 of identifying the presence of the
1573,1,"['cases', 'test']", SUMMARY,seg_327,minor pipes with respective probavirus in 99% of the cases but is bilities of cracks in a year as pois expensive. the second test t2 is
1574,1,"['false positive', 'rate', 'cases', 'probability', 'test']", SUMMARY,seg_327,"(1/1000) and pois (1/200). what cheap, but it can detect the disis the probability of one major and ease in 96% of the cases. the first two minor cracks in a year? if a test has a false positive rate of"
1575,1,['test'], SUMMARY,seg_327,"crack has indeed occurred, what is 0.05, whereas the second test has"
1576,1,"['false positive', 'rate', 'probability']", SUMMARY,seg_327,the probability that both of them a false positive rate of 0.06. if a
1577,0,[], SUMMARY,seg_327,"have cracked? person is tested positive using t2,"
1578,1,['probability'], SUMMARY,seg_327,5.72 a tourist has to visit n tourist-spots what is the probability that the dis-
1579,1,['tests'], SUMMARY,seg_327,in a city. in how many possible ease is truly present? if both tests order can this be visited if (i) all of show positive what is the proba-
1580,0,['n'], SUMMARY,seg_327,"them are visited on a single day?, bility that the person truly has the (ii) n∕2 each in two days (for n disease. even)?, and (iii) m of them on first day and rest on second day. 5.76 there are nine rings, all of which"
1581,1,['probability'], SUMMARY,seg_327,have exactly identical look. three 5.73 if the probability of head showof them are gold and the rest are
1582,1,"['trials', 'trial', 'probability']", SUMMARY,seg_327,"the probability that (i) the second what is the probability of identihead is obtained in an odd numfying the three gold rings in (i) bered trial and (ii) the third head is two weightings?, (ii) three weightobtained in at least 10 and at most ings?, and (iii) four weightings? 15 trials."
1583,0,[], SUMMARY,seg_327,5.74 a satellite that failed to reach orbit 5.77 a container has 12 machinery
1584,1,"['probability theory', 'probability', 'random']", SUMMARY,seg_327,"is falling down to the earth. some parts, all looking alike. seven of internal parts made of steel, titathem are known to be good, three nium, and beryllium that have a of them have mild defect, and the high melting point are likely to rest have severe defect. two parts make through the descent withare selected at random. find probout burning up. the exact locaability that (i) both of them are tion where it will hit the surface good and (ii) one is good and other is unknown owing to its eccentric has mild defect? path. assume that 70% of earth 5.78 a course on probability theory surface is covered with water. the atlantic ocean is 16.67% of the is attended by 28 students with"
1585,1,['probability'], SUMMARY,seg_327,"statistics major or computer scibirthday with you?. if m among ence major. if there are 18 statisthe n people are males, find the tics majors and 14 computer probability that at least one male science majors, how many are shares the birthday with you."
1586,1,['table'], SUMMARY,seg_327,"double majors? 5.80 using table 5.9 (p. 5.9), com-"
1587,1,"['probability', 'conditional', 'conditional probability']", SUMMARY,seg_327,5.79 suppose you are in a room with n pute the conditional probability
1588,0,['o'], SUMMARY,seg_327,other people. what is the probathat parent blood types are a or b bility that no one else shares the given that child’s type is o.
1589,1,['independent'], SUMMARY,seg_327,"5.81 if x and y are independent, which of the following are also independent? (a) x"
1590,1,"['probability', 'table']", SUMMARY,seg_327,5.82 the customer breakdown to a store is given in table 5.11. find the probability
1591,1,"['conditional', 'probability', 'conditional probability']", SUMMARY,seg_327,"that (i) a randomly chosen customer to the store is a female and (ii) conditional probability that a customer will visit the store on friday, given that the customer is male."
1592,0,[], SUMMARY,seg_327,5.83 cancer incidence among first-hand smokers and second-hand smokers (who
1593,1,"['probability', 'table']", SUMMARY,seg_327,are exposed to smokers inside enclosed areas) are given in table 5.12. a new patient is found to be a nonsmoker. what is the probability that he has benign cancer? a retiree is having malignant tumor. find the probability that he is a smoker.
1594,1,['events'], SUMMARY,seg_327,"5.84 if a1,a2, … an are finite events, some of which have overlaps, prove that"
1595,0,[], SUMMARY,seg_327,where summations are carried out using conditions specified.
1596,1,['processes'], SUMMARY,seg_327,5.85 a mineral is extracted using two processes p1 and p2. two types of impurities
1597,1,"['table', 'data', 'probability', 'process']", SUMMARY,seg_327,"in the mineral are examined by a quality inspector. data appear in table 5.13. a lot produced by process p1 is randomly selected. find the probability that it contains impurity b. if a lot is known to contain impurity a, what is the chance that it was produced by process p2?"
1598,0,[], DISCRETE DISTRIBUTIONS,seg_329,"after finishing the chapter, students will be able to"
1599,1,"['binomial theorem', 'binomial']", DISCRETE DISTRIBUTIONS,seg_329,◾ understand binomial theorem and its forms
1600,1,"['distribution', 'bernoulli', 'trials', 'bernoulli trials', 'bernoulli distribution']", DISCRETE DISTRIBUTIONS,seg_329,◾ explain bernoulli trials and bernoulli distribution
1601,1,"['distribution', 'binomial distribution', 'binomial']", DISCRETE DISTRIBUTIONS,seg_329,◾ describe binomial distribution and its properties
1602,1,"['poisson', 'distribution', 'poisson distribution']", DISCRETE DISTRIBUTIONS,seg_329,◾ apply poisson distribution in practical situations
1603,1,"['distribution', 'hypergeometric distribution', 'geometric', 'hypergeometric']", DISCRETE DISTRIBUTIONS,seg_329,"◾ understand geometric, hypergeometric distribution, and its properties"
1604,1,"['negative binomial distribution', 'distribution', 'negative binomial', 'binomial', 'binomial distribution']", DISCRETE DISTRIBUTIONS,seg_329,◾ describe negative binomial distribution and its properties
1605,1,"['distribution', 'multinomial distribution', 'multinomial']", DISCRETE DISTRIBUTIONS,seg_329,◾ describe logarithmic and multinomial distribution and its properties
1606,1,"['power method', 'method', 'discrete', 'discrete distributions', 'distributions']", DISCRETE DISTRIBUTIONS,seg_329,◾ apply the power method to find the md of discrete distributions
1607,1,"['sample', 'discrete random variable', 'random variables', 'interval', 'continuous', 'variables', 'experiment', 'discrete', 'variable', 'random variable', 'random', 'function', 'sample space']", DISCRETE RANDOM VARIABLES,seg_331,"a real-valued function defined on the sample space of a random experiment is called a random variable. we denote the random variables by capital english letters (x, y, etc.) and particular values by lowercase letters (x, y, etc.). random variables can be discrete or continuous. a random variable that can take a countable number of possible values in a finite or infinite interval is called a discrete random variable. in most of the applications, the values assumed are positive (x ≥1) or nonnegative (x ≥ 0) integers that are equispaced. theoretically, this is not a restriction. consider, for example,"
1608,1,"['table', 'random variable', 'variable', 'random']", DISCRETE RANDOM VARIABLES,seg_331,"the portion of a fruit (say apples) taken by a person at a dining table per day. if it is cut evenly and eaten by different family members, the random variable of interest takes values 0, 1"
1609,1,"['change of origin', 'discrete', 'discrete distributions', 'variable', 'displaced distributions', 'continuous distributions', 'continuous', 'transformation', 'distributions']", DISCRETE RANDOM VARIABLES,seg_331,"2 , 1, 1 1 2 , 2, and so on. similarly, if an employer allows an employee to take either a half-day leave or a full-day leave only, the variable of interest takes integer or half-integer values. however, the majority of discrete distributions discussed in the following are defined on “counts” or “occurrences” that can take nonnegative integer values (0, 1, 2, … ). displaced distributions are those obtained by a change of origin transformation (y = x ± c). the constant c is assumed to be a nonzero integer for discrete distributions and a real number for continuous distributions. left-truncated distributions are exceptions in which the starting value is offset by a positive integer."
1610,1,"['range', 'statistics', 'case', 'discrete', 'negative binomial', 'hypergeometric', 'random', 'binomial distributions', 'probability', 'poisson distributions', 'geometric', 'negative binomial distributions', 'hypergeometric distributions', 'statistical', 'distributions', 'functions', 'gamma', 'discrete distributions', 'distribution', 'binomial', 'continuous', 'poisson', 'incomplete beta', 'random variables', 'probabilities', 'variables', 'discrete random variables']", DISCRETE RANDOM VARIABLES,seg_331,"this chapter discusses popular discrete distributions. the x values are assumed to be equispaced integers, unless otherwise noted. the domain of x can be finite (as in binomial, discrete uniform, and hypergeometric distributions (hgds)) or infinite (as in poisson, geometric, and negative binomial distributions). in the case of infinite range, we naturally expect the probabilities to tail-off to zero beyond a cutoff. discrete distributions with finite range are more popular in practical applications, whereas those with infinite range are more important theoretically. this is because some statistical distributions with finite range asymptotically converge to discrete distributions with infinite range as shown below. the cumulative distribution functions (cdfs) of discrete random variables are step functions. the cdf of binomial, negative binomial, and poisson distributions can be expressed as continuous functions such as the incomplete beta and gamma functions as shown below. it may be noted that there exist many more statistical distributions than those mentioned below (see references 121–123. here, we discuss only those that are widely used in the applications of probability and statistics in everyday life."
1611,1,"['binomial theorem', 'distribution', 'binomial', 'statistical']", BINOMIAL THEOREM,seg_333,"the binomial theorem with positive and negative exponents has many applications in statistical distribution theory. this section provides an overview of this theorem, which will be used in the sequel. we first consider an expansion for integer powers of a sum or difference of two quantities. more specifically, if n is a positive integer, and x and y are nonzero real numbers, the power (x + y)n can be expressed as a sum of n + 1 quantities in either of two ways as follows:"
1612,0,['n'], BINOMIAL THEOREM,seg_333,k) denotes n!∕(k!(n − k)!. this is most easily proved by induction on n (see
1613,1,['function'], BINOMIAL THEOREM,seg_333,"exercise). here, the indexvar k is used as an exponent and a function (in ( n"
1614,1,"['binomial coefficients', 'binomial', 'coefficients']", BINOMIAL THEOREM,seg_333,"k) (also denoted as nck, see page 1–16) are called binomial coefficients,"
1615,1,['case'], BINOMIAL THEOREM,seg_333,which are always integers when n and k are integers. the special case ( 0
1616,1,['case'], BINOMIAL THEOREM,seg_333,"to be 1 by convention. in the particular case when x = y = 1, the aforementioned"
1617,1,['coefficients'], BINOMIAL THEOREM,seg_333,"n k), the coefficients in the above-mentioned"
1618,1,['symmetric'], BINOMIAL THEOREM,seg_333,expansion are symmetric (hence 2n = ∑k
1619,0,['n'], BINOMIAL THEOREM,seg_333,"n k), which follows by summing in"
1620,1,['coefficients'], BINOMIAL THEOREM,seg_333,"reverse). if n is odd, there are (n + 1) terms with (n + 1)∕2 coefficients symmetrically"
1621,1,['coefficients'], BINOMIAL THEOREM,seg_333,"placed. for instance, if n = 5, there are three coefficients ( 0"
1622,1,"['symmetric', 'coefficients']", BINOMIAL THEOREM,seg_333,"5) = 10. if n is even, there are n∕2 symmetric coefficients with a"
1623,1,['coefficient'], BINOMIAL THEOREM,seg_333,unique middle coefficient ( n∕
1624,0,['n'], BINOMIAL THEOREM,seg_333,"n 2). if y is negative, we write x − y as x + (−y) and the above-mentioned expansion gives"
1625,0,[], BINOMIAL THEOREM,seg_333,"when the index n in the above-mentioned expansion is negative, we get an infinite series as given below:"
1626,1,['case'], BINOMIAL THEOREM,seg_333,"in the particular case when y = 1, we get"
1627,1,"['probabilities', 'interval', 'distribution', 'events', 'associated', 'probability', 'total probability', 'statistical', 'distributions']", BINOMIAL THEOREM,seg_333,"we have not placed any restrictions on x and y values in the above-mentioned expansions, other than that they are nonzero real numbers. as the total probability of statistical distributions must sum to unity, we make the restriction that x + y = 1. these are usually denoted by p and q (or   and 1 −  ) instead of x and y in statistical applications. this implies that q = 1 − p, so that both p and q lie in the interval [0, 1]. as shown in the following, p and q are the probabilities associated with the occurrence or nonoccurrence of well-defined events in distribution theory."
1628,1,['coefficients'], Recurrence Relation for Binomial Coefficients,seg_335,binomial coefficients satisfy many recurrences. these are most often proved by
1629,0,[], Recurrence Relation for Binomial Coefficients,seg_335,r) denotes the number of ways of choosing r
1630,1,"['without replacement', 'replacement']", Recurrence Relation for Binomial Coefficients,seg_335,objects from among n objects without replacement. we give only the simplest and most popular recurrences in the following:
1631,0,[], Recurrence Relation for Binomial Coefficients,seg_335,"this is known as pascal’s identity. as the only arithmetic operation involved is addition, this always returns an integer result."
1632,1,"['recurrence', 'error', 'coefficients']", Recurrence Relation for Binomial Coefficients,seg_335,"this recurrence simultaneously decrements both the arguments and is useful in computing the coefficients for small r values. it is used in chapter 8, example 8.35 (p. 8–11). as shown in the following, this could result in approximations owing to truncation error resulting from (n/r). a remedy is suggested below."
1633,0,['n'], Recurrence Relation for Binomial Coefficients,seg_335,n 1) this form is useful when n is large and r is small.
1634,1,"['distribution', 'binomial distribution', 'binomial']", Recurrence Relation for Binomial Coefficients,seg_335,"this form is useful when n is very large and r is close to n, so that the decrementing of n is continued until n becomes r. it is used to simplify the md of binomial distribution (p. 6–23)."
1635,1,"['negative binomial distribution', 'distribution', 'negative binomial', 'binomial', 'binomial distribution']", Recurrence Relation for Binomial Coefficients,seg_335,this is used in negative binomial distribution.
1636,0,['n'], Recurrence Relation for Binomial Coefficients,seg_335,this form is useful when n and m are large and close-by.
1637,1,"['combinations', 'combination', 'factorial', 'moments', 'factorial moments']", Recurrence Relation for Binomial Coefficients,seg_335,this combines multiple summations of combinations into a single combination. it is used in finding factorial moments (see page 6–45).
1638,1,"['discrete', 'factorial', 'discrete distributions', 'convolution', 'moments', 'factorial moments', 'distributions']", Recurrence Relation for Binomial Coefficients,seg_335,this is called vandermonde convolution. it is used in deriving factorial moments of some discrete distributions (see page 6–82).
1639,1,['coefficients'], Recurrence Relation for Binomial Coefficients,seg_335,binomial coefficients evaluated by a computer can sometimes result in approx-
1640,0,[], Recurrence Relation for Binomial Coefficients,seg_335,"imations. for instance, ( 3"
1641,1,['error'], Recurrence Relation for Binomial Coefficients,seg_335,1 ) = 1.6666666 ∗ 2 ∗ 3 = 9.999999999999) owing to truncation error. this is because the expression inside the bracket is forcibly evaluated. if the order of evalu-
1642,0,['n'], Recurrence Relation for Binomial Coefficients,seg_335,ation is modified as ( n
1643,0,[], Recurrence Relation for Binomial Coefficients,seg_335,"1) ∕r without parenthesis, we will get the correct"
1644,0,[], Recurrence Relation for Binomial Coefficients,seg_335,"integer result. alternatively, use pascal’s identity (1). it always returns an integer as it involves only additions (see reference 22)."
1645,1,"['binomial theorem', 'distribution', 'bernoulli', 'binomial', 'bernoulli distribution', 'statistical', 'distributions']", Distributions Obtainable from Binomial Theorem,seg_337,"there are many statistical distributions that are derived from the above-mentioned form of the binomial theorem. taking n = 1, x = p, and y = q = 1 − p, we get the bernoulli distribution (section 6.4). setting n  1 to be an integer, x = p and y = q"
1646,1,"['binomial theorem', 'range', 'results', 'geometric distribution', 'case', 'negative binomial distribution', 'discrete', 'distribution', 'negative binomial', 'binomial', 'binomial distribution', 'geometric', 'coefficients']", Distributions Obtainable from Binomial Theorem,seg_337,"= 1 − p, results in the binomial distribution (section 6.5). putting n = −1, p = −p, q = q, we get f (x) = 1∕(q − p), which is a special case of discrete uniform distribution. setting n = −m, x = p, and y = q = 1 − p results in the negative binomial distribution (section 6.8). writing (x + y)n as xn(1 + y∕x)n, putting y/x = −q, 1/x = p, and n = −1 we get the geometric distribution (which has infinite range). put n = −1 and write (x − y)−1 as 1∕x (1 − y∕x)−1. setting y∕x = , taking logarithm and expanding using − log(1 − x) = x + x2∕2 + x3∕3 + · · · results in logarithmic series distribution. write (1 + y)n = (1 + y)n1 ∗ (1 + y)n2 where n1 + n2 = n. expand each one using binomial theorem, equate identical coefficients on both sides and divide rhs by lhs constant to get the hgd."
1647,1,"['integer part', 'discrete', 'negative binomial', 'probability', 'associated', 'binomial distribution', 'geometric', 'lorenz curve', 'mean', 'distributions', 'curve', 'discrete distributions', 'distribution', 'binomial', 'poisson', 'deviation', 'geometric distributions']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,"finding the md of discrete distributions is a laborious task, as it requires a lot of arithmetical work. it is also called the mean absolute deviation or l1-norm. the md is closely associated with the lorenz curve used in econometrics, gini index and pietra ratio used in economics and finance, and in reliability engineering. in 1730, the french mathematician abraham de moivre (1667–1754) gave a surprisingly simple and computationally appealing closed-form expression for the md of a binomial distribution (which is given in p. 201). this is perhaps the very first published work on md. this was followed by several interesting investigations, which are given in the summary section (p. 201). johnson [125] surmised that the md of some discrete distributions can be put in the form 2 2 fm, where 2 = 2 and fm is the probability mass evaluated at the integer part of the mean m = ⌊ ⌋. this holds good for poisson, binomial, negative binomial, and geometric distributions. kamat [126] generalized johnson’s result to several discrete distributions."
1648,1,"['deviation', 'medoid', 'median', 'case', 'discrete', 'discrete distributions', 'deviations', 'mean', 'distributions']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,the following theorem greatly simplifies the work and is very helpful to find the md of a variety of discrete distributions. it can easily be extended to the multivariate case and for other types of mean deviations such as mean deviation from the median and medoid.
1649,1,"['discrete', 'distribution', 'discrete distribution', 'tails']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,theorem 6.1 the md of any discrete distribution that tails off to the left is expressed
1650,1,"['distribution', 'arithmetic mean', 'mean', 'limit']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,"where ll is the lower limit of the distribution,   the arithmetic mean, and f(x) the cdf."
1651,0,[], MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,proof: by definition ul
1652,1,"['distribution', 'limit']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,where ll is the lower and ul the upper limit of the distribution.
1653,1,"['range', 'summation']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,split the range of summation from ll to   − 1 and   to ul and note that |x −  | =   − x for x    . this gives
1654,1,"['expectation operator', 'expectation']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,"as e(x) =  , we can write e(x −  ) = 0, where e() is the expectation operator. expanding e(x −  ) as"
1655,1,"['range', 'summation']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,"as done earlier, split the range of summation from ll to   − 1 and   to ul to get"
1656,0,[], MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,split this into two sums to get
1657,1,"['random variables', 'variables', 'random']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,"as the md is always positive, the first term in (6.11) is greater than the second for positive random variables."
1658,1,['summation'], MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,expand the summation inside the bracket in reverse order of indexvar as
1659,0,[], MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,collect the first term from each expression on the rhs to get
1660,1,['expectations'], MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,"where f(  − 1) = p(  − 1) + p(  − 2) + · · · + p(ll) so that both partial expectations are bounded, for finite  . now substitute in (6.11). the  f(  − 1) term cancels out, leaving behind"
1661,0,[], MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,which is same expression obtained above.
1662,0,[], MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,write (6.14) as two summations
1663,1,"['distribution', 'symmetric', 'mean']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,"if the mean   is a half-integer, a correction term f(⌊ ⌋) must be added to get the correct md. if the distribution of x is symmetric, we can write the aforementioned"
1664,1,"['distribution', 'function', 'tails']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,"where s(x) is the survival function. if the distribution tails off to the right extreme, the aforementioned is evaluated as"
1665,1,"['case', 'discrete', 'probability', 'function', 'results', 'mean', 'coefficient', 'distributions', 'functions', 'discrete distributions', 'distribution', 'summation', 'deviation', 'method', 'bivariate']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,"if the mean   is neither an integer nor a half-integer, the summation is carried out to the nearest integer. in this case, the results are only approximate (see example 6.40 in p. 6–78). nevertheless, the above-mentioned theorem is of enormous use, as it can be easily extended to find the md of bivariate and multivariate discrete distributions. there are two other novel methods to find the mean deviation. the first one uses generating functions (chapter 9, section 9.4, p. 9–11) to fetch a single coefficient of t −1 in the power series expansion of (1 − t)−2px(t), where px(t) is the probability generating function. this works best for discrete distributions. the second method is using the inverse of distribution functions (chapter 10, section 10.10, p. 10–9), the discrete analog of which is obtained by replacing integration by summation."
1666,1,"['tail probability', 'discrete', 'distribution', 'probability', 'discrete distribution', 'tail', 'variance']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,example 6.1 variance of discrete distribution as tail probability
1667,1,"['probabilities', 'discrete', 'discrete distributions', 'tail probabilities', 'mean', 'tail', 'variance', 'distributions']", MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,prove that the variance of discrete distributions can be expressed in terms of tail probabilities when the mean is an integer or a half-integer.
1668,1,['probability'], MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,"l   s(x). equating johnson’s result that md = 2 2fm, where  2 =  2 and fm is the probability mass evaluated at the"
1669,1,['mean'], MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,integer part of the mean m = ⌊ ⌋ we get  2 ∗ fm = ∑  x=
1670,1,['table'], MEAN DEVIATION OF DISCRETE DISTRIBUTIONS,seg_339,"when m = ⌊ ⌋ is a half-integer, the correction term mentioned earlier must be applied (table 6.1)."
1671,1,"['discrete', 'negative binomial', 'hypergeometric', 'binomial distributions', 'function', 'negative binomial distributions', 'hypergeometric functions', 'distributions', 'functions', 'gamma', 'distribution', 'binomial', 'continuous', 'beta function', 'incomplete beta', 'deviation', 'normal', 'normal distribution', 'continuous distributions']", Recurrence Relation for Mean Deviation,seg_341,"mean deviation of some distributions involves complicated terms. in the above-mentioned theorem, we have obtained an expression for md in terms of cdf. it is possible to develop recurrences for md in those situations where the cdf has closed-form expressions in terms of incomplete beta or gamma functions, normal distribution, confluent hypergeometric functions, or orthogonal polynomials. this argument applies to both discrete and continuous distributions. as examples, the cdf of binomial and negative binomial distributions are expressed in terms of incomplete beta functions. however, the beta function satisfies several recurrences like"
1672,1,"['beta function', 'results', 'function', 'recurrence']", Recurrence Relation for Mean Deviation,seg_341,"equation (6.20) allows one to successively reduce the first argument of beta function, which in turn results in a recurrence for the md. similarly"
1673,1,"['parameters', 'results', 'recurrence']", Recurrence Relation for Mean Deviation,seg_341,"ix(a + 1, b − 1) = [1 + bx∕(a(1 − x))] ix(a, b) − bx∕[a(1 − x)] ix(a − 1, b + 1). (6.22) equation (6.21) allows one to successively reduce both parameters, which results in another recurrence for md."
1674,1,"['experiment', 'bernoulli distribution', 'results', 'failure', 'distribution', 'bernoulli', 'success', 'probability', 'random', 'outcome']", BERNOULLI DISTRIBUTION,seg_343,the bernoulli distribution results from a random experiment in which each outcome is either a success (denoted by 1) with probability p or a failure (denoted by 0) with a probability q so that p + q = 1. this means that fixing the value of p automatically
1675,1,"['probability', 'random', 'function', 'experiment', 'random variable', 'success', 'bernoulli trial', 'bernoulli distribution', 'trial', 'bernoulli random variable', 'density function', 'probability density function', 'condition', 'failure', 'distribution', 'outcomes', 'hypothesis', 'bernoulli', 'variable']", BERNOULLI DISTRIBUTION,seg_343,"fixes the value of q. a question that naturally arises is what should be chosen as p?. this is not an issue because p and q are simply place holders for probabilities. it depends more on the research hypothesis. here, the meaning of the word success and failure should not be taken literally – it simply means two dichotomous outcomes of an experiment. in engineering, it can denote faulty or nonfaulty, working or defunct, closed or open (as in electrical circuits), and detected or undetected (radioactivity, smoke, abnormality, etc.). if we wish to check if something is faulty, we choose p as the probability of a fault. in medical sciences, p is chosen as the probability of the presence of a symptom or condition. thus, this distribution finds applications in a variety of fields. it is named after the swiss mathematician jacques bernoulli (1654–1705). such an experiment is known as a bernoulli trial. the probability density function (pdf) of a bernoulli random variable is given by f (x; p) = pxq(1−x), x = 0 or 1, and 0 ≤ q = 1 − p ≤ 1. we will denote the bernoulli distribution by ber(p). this could also be expressed in the following convenient form:"
1676,1,"['distribution', 'bernoulli', 'mean', 'bernoulli distribution', 'variance']", BERNOULLI DISTRIBUTION,seg_343,"the mean and variance of a bernoulli distribution are   = p,  2 = pq. as the only values of x are 0 and 1, we get the mean as e(x) = 0∗q + 1∗p = p. similarly, e(x2) = 02 ∗ q + 12 ∗ p = p, so that the variance becomes e(x2) − e(x)2 = p − p2 = p(1 − p) = pq.  1 = (1 − 2p)∕√(pq),  2 = 3 + (1 − 6p)∕pq."
1677,1,"['estimated', 'estimate', 'successes', 'distribution', 'bernoulli', 'moments', 'bernoulli trials', 'trials', 'probability', 'function', 'parameter', 'experiments']", BERNOULLI DISTRIBUTION,seg_343,"bernoulli distribution has only one unknown parameter p. this unknown probability is usually estimated either from past experiments or from empirical studies. if we observe k successes in n bernoulli trials, an estimate of p is obtained as p = k∕n. the probability generating function is easily obtained as px(t) = (q + pt), and characteristic function is (t) = q + peit. hence, all moments about zero are p."
1678,1,"['independent', 'probability distributions', 'geometric distributions', 'negative binomial', 'bernoulli', 'bernoulli trials', 'binomial', 'probability', 'trials', 'geometric', 'distributions']", BERNOULLI DISTRIBUTION,seg_343,"there are many other probability distributions based on bernoulli trials. for example, the binomial, negative binomial, and geometric distributions mentioned earlier; success-run distributions are all defined in terms of independent bernoulli trials."
1679,1,"['distribution', 'bernoulli', 'bernoulli distribution']", BERNOULLI DISTRIBUTION,seg_343,example 6.2 cdf of bernoulli distribution
1680,1,"['variable', 'probability']", BERNOULLI DISTRIBUTION,seg_343,suppose p denotes the probability of a trait in a group of persons. define a random variable x that takes the value 1 if trait is present and is 0 otherwise. find the pdf and cdf of x.
1681,1,"['outcomes', 'random variable', 'variable', 'random']", BERNOULLI DISTRIBUTION,seg_343,solution 6.2 assign a random variable x to the two possible outcomes as p(trait) = p and p(trait not present) = q = 1 − p. the pdf is expressed as
1682,0,[], BERNOULLI DISTRIBUTION,seg_343,"as there are only two possible values, the cdf is obtained as"
1683,1,"['cases', 'table']", BERNOULLI DISTRIBUTION,seg_343,"the p = 0 or p = 1 cases are called degenerate cases, as there is no randomness involved. see table 6.2 for summary of properties."
1684,1,"['random variables', 'variables', 'bernoulli', 'random']", BERNOULLI DISTRIBUTION,seg_343,example 6.3 product of two bernoulli random variables
1685,1,['distribution'], BERNOULLI DISTRIBUTION,seg_343,"if x and y are iid ber(p), find the distribution of u = x ∗ y"
1686,1,"['probabilities', 'bernoulli', 'success', 'probability of success', 'probability']", BERNOULLI DISTRIBUTION,seg_343,"solution 6.3 x and y both takes the values 1 with probability p, and 0 with probability q = 1 − p. hence, xy takes the value 0 when either or both of x and y take the value 0 with probability q2 + qp + pq. here, q2 is the probability that both of them takes the value 0, and qp and pq are the probabilities that either of them takes value 0 and other takes the value 1. write q2 + qp = q[q + p] and use q + p = 1 to get q. next combine q with pq to get q + pq = q(1 + p). write q as (1 − p) to get (1 − p) ∗ (1 + p) = 1 − p2. probability that xy takes the value 1 is p2. hence, xy is bernoulli with probability of success p2. this can be extended to"
1687,1,['case'], BERNOULLI DISTRIBUTION,seg_343,the case ∏i
1688,1,"['independent', 'distribution', 'bernoulli', 'trials', 'bernoulli distribution']", BINOMIAL DISTRIBUTION,seg_345,binomial distribution is a natural extension of bernoulli distribution for two or more independent trials (n   1). it was first derived in its present form by the swiss
1689,1,"['random variables', 'independent', 'variables', 'successes', 'bernoulli', 'bernoulli trials', 'binomial', 'random', 'trials']", BINOMIAL DISTRIBUTION,seg_345,"mathematician jacques bernoulli (1654–1705), which was published posthumously in 1713 [127], although the binomial expansion (for arbitrary n) was studied by blaise pascal [128]. it can be interpreted in terms of random trials or in terms of random variables. consider n independent bernoulli trials. we assume that the trials have already occurred. we are interested in knowing how many successes have taken place among the n trials. this number is any integer from 0 to n inclusive. assuming"
1690,1,['successes'], BINOMIAL DISTRIBUTION,seg_345,"that there are x successes, this can happen at any of the n positions in ( n"
1691,1,"['failures', 'successes', 'success', 'probability of success', 'probability', 'trial']", BINOMIAL DISTRIBUTION,seg_345,"x) ways. since the probability of success remains the same from trial to trial, the probability of x successes and n − x failures is given by"
1692,1,"['distribution', 'exponential', 'binomial', 'binomial distribution']", BINOMIAL DISTRIBUTION,seg_345,it is called binomial distribution because it is the xth term in the binomial expansion of (p + q)n. it belongs to the exponential family.
1693,1,"['random variables', 'parameters', 'variables', 'trials', 'distribution', 'random variable', 'variable', 'bernoulli', 'bernoulli trials', 'binomial', 'random', 'binomial distribution', 'parameter']", BINOMIAL DISTRIBUTION,seg_345,"the random variable interpretation of binomial distribution is based on independent bernoulli trials. let x1,x2, … ,xn be a sequence of iid bernoulli random variables with the same parameter p. then, the sum x = x1 + x2 + · · · + xn has a binomial distribution with parameters n and p. we denote this as bino(n, p)."
1694,1,"['parameters', 'table', 'distribution', 'trials', 'success', 'probability of success', 'probability', 'trial']", Properties of Binomial Distribution,seg_347,"there are two parameters for this distribution (see figure 6.1 and table 6.3), namely the number of trials (n   1; an integer), and the probability of success in each trial"
1695,1,"['sample', 'independent', 'with replacement', 'replacement', 'sampling', 'distribution', 'binomial', 'probability', 'populations', 'binomial distribution', 'population', 'trial']", Properties of Binomial Distribution,seg_347,"(p), which is a real number between 0 and 1. this probability remains the same from trial to trial, which are independent. this distribution is encountered in sampling with replacement from large populations. if p denotes the probability of observing some characteristic (there are x individuals that have the characteristic in the population, so that p = x∕n where n is the population size), the number of individuals in a sample of size n from that population has the characteristic is given by a binomial distribution bino(n, p)."
1696,1,"['independent', 'table', 'results', 'distribution', 'trials', 'moments', 'mean', 'binomial', 'binomial distribution', 'variance']", Properties of Binomial Distribution,seg_347,"6.5.1.1 moments as the trials are independent, and x = x1 + x2 + … + xn, the mean is e(x) = p + p + .. + p = np. similarly, the variance of x is v(x) = v(x1) + v(x2) + … + v(xn) = pq + pq + … + pq = npq. hence  1 = e(x) = np,var(x) = npq =  1 ∗ q. note that when p → 0 from above, q → 1 from below and the variance →  1. this results in a distribution with the same mean and variance (table 6.3). if np =   is a constant, we could reparametrize the binomial distribution by putting   = np to get"
1697,1,"['symmetry', 'variance', 'mean']", Properties of Binomial Distribution,seg_347,"with mean   and variance  (n −  )∕n. the symmetry of variance for fixed n indicates that the variance of bino(n, p) and bino(n, q) are the same."
1698,1,"['distribution', 'binomial', 'binomial distribution', 'variance']", Properties of Binomial Distribution,seg_347,example 6.4 maximum variance of a binomial distribution
1699,1,"['distribution', 'binomial', 'function', 'binomial distribution', 'variance']", Properties of Binomial Distribution,seg_347,"prove that the maximum variance of a binomial distribution bino(n, p) as a function of p is n∕4."
1700,1,['variance'], Properties of Binomial Distribution,seg_347,"solution 6.4 we know that the variance is given by v(n, p) = npq = np − np2."
1701,0,[], Properties of Binomial Distribution,seg_347,"differentiating with respect to p, we get  "
1702,1,['limit'], Properties of Binomial Distribution,seg_347,"and solving for p gives p = 1/2. as the second derivative is −2n, this indeed gives the maxima. substitute in the above to get v(n, p) = n*(1/2)*(1/2) = n/4. this can be increased without limit by letting n → ∞ (see discussion in page 6–54)."
1703,1,"['ordinal', 'probabilities', 'moment', 'likelihood', 'discrete', 'distribution', 'random variable', 'variable', 'random', 'level', 'discrete distribution']", Properties of Binomial Distribution,seg_347,"the ratio of two probabilities of a discrete distribution at distinct ordinal (x) values provides the relative likelihood of the random variable taking a value at one level versus the other. this is useful to calculate the probabilities recursively, locate the mode, and develop moment recurrences [129]."
1704,1,"['distribution', 'binomial distribution', 'binomial']", Properties of Binomial Distribution,seg_347,example 6.5 mode of binomial distribution
1705,1,"['distribution', 'binomial distribution', 'binomial']", Properties of Binomial Distribution,seg_347,find the mode of the binomial distribution.
1706,0,[], Properties of Binomial Distribution,seg_347,solution 6.5 consider the ratio
1707,1,"['unimodal', 'distribution', 'binomial', 'binomial distribution']", Properties of Binomial Distribution,seg_347,"add and subtract xq in the numerator and combine −xp − xq to get −x. then the ratio becomes 1 + [(n + 1)p − x]∕(xq). the bracketed expression in the numerator is positive when (n + 1)p x and negative otherwise. if (n + 1)p = x, the bracketed expression vanishes giving two modes at x − 1 and x. otherwise, the mode is ⌊(n + 1)p⌋. this shows that the binomial distribution is unimodal for all values of p and n, except when (n + 1)p is an integer."
1708,1,['moments'], Moment Recurrences,seg_349,"low order moments could be obtained using the density recurrences fx(n, p)∕fx(n − 1, p) = (n∕(n − x))q, which upon cross multiplication and rearrangement becomes"
1709,1,"['range', 'moments']", Moment Recurrences,seg_349,"multiply both sides of (6.25) by xk, denote the moments by  k(n, p), and sum over the range of x to get"
1710,1,['moments'], Moment Recurrences,seg_349,"put k = 1, to get  2(n, p) = n[ 1(n, p) − q 1(n − 1, p). substituting  1(n, p) = np,  1(n − 1, p) = (n − 1)p, the rhs becomes n[np − q(n − 1)p] = np[np + q]. higher order moments are obtained similarly."
1711,1,"['moment', 'binomial', 'moment recurrence', 'recurrence']", Moment Recurrences,seg_349,example 6.6 binomial ordinary moment recurrence
1712,1,"['distribution', 'moments', 'binomial', 'binomial distribution', 'recurrence']", Moment Recurrences,seg_349,prove that the ordinary moments of a binomial distribution satisfy the recurrence
1713,0,[], Moment Recurrences,seg_349,solution 6.6 write
1714,0,[], Moment Recurrences,seg_349,  multiply throughout by pq and rearrange to get  k
1715,1,"['moment', 'binomial', 'moment recurrence', 'recurrence']", Moment Recurrences,seg_349,example 6.7 binomial central moment recurrence
1716,1,"['distribution', 'moments', 'binomial', 'binomial distribution', 'recurrence']", Moment Recurrences,seg_349,prove that the central moments of a binomial distribution satisfy the recurrence
1717,0,[], Moment Recurrences,seg_349,solution 6.7 consider
1718,1,['functions'], Moment Recurrences,seg_349,note that there are three functions of p on the rhs (as q = 1 − p). differentiate both sides with respect to p using chain rule to get
1719,1,"['moment', 'binomial', 'moment recurrence', 'recurrence']", Moment Recurrences,seg_349,example 6.8 binomial central moment recurrence
1720,1,"['distribution', 'moments', 'binomial', 'binomial distribution', 'recurrence']", Moment Recurrences,seg_349,prove that the central moments of a binomial distribution satisfy the recurrence
1721,0,[], Moment Recurrences,seg_349,solution 6.8 write
1722,1,"['binomial theorem', 'range', 'binomial']", Moment Recurrences,seg_349,"consider equation (6.25) as (n − x)fx(n, p) = nqfx(n − 1, p). write (n − x) as n(p + q) − x = nq − (x − np) on the lhs. multiply throughout by (x − np)k, write (x − np) = [(x − (n − 1)p) − p] on the rhs, expand using binomial theorem and sum over the proper range to get"
1723,1,"['moment', 'factorial', 'moment ', 'factorial moment']", Moment Recurrences,seg_349,theorem 6.2 the factorial moment  (r) is given by  (r) = n(r)pr.
1724,1,"['moment', 'factorial', 'factorial moment']", Moment Recurrences,seg_349,proof: consider the expression for factorial moment as
1725,1,['range'], Moment Recurrences,seg_349,substitute the pdf and sum over the proper range of x to get the rhs as
1726,1,"['factorial', 'moments', 'stirling number', 'factorial moments']", Moment Recurrences,seg_349,"where s(r, i) is the stirling number of first kind. the factorial moments are found"
1727,1,['stirling numbers'], Moment Recurrences,seg_349,using stirling numbers as r
1728,1,['moment'], Moment Recurrences,seg_349,′ denotes the ith ordinary moment. the reverse relationship is
1729,1,['stirling number'], Moment Recurrences,seg_349,"where s(r, i) is the stirling number of second kind. this allows us to write"
1730,1,"['deviation', 'distribution', 'binomial', 'mean', 'binomial distribution']", Moment Recurrences,seg_349,theorem 6.3 prove that the mean deviation from the mean of a binomial distribution
1731,0,[], Moment Recurrences,seg_349,where   is the largest integer less than np (symbolically   = ⌊np⌋).
1732,0,['n'], Moment Recurrences,seg_349,"proof: by definition, n n"
1733,0,[], Moment Recurrences,seg_349,"split the rhs into two sums for x ≤ np and x   np, respectively:"
1734,0,[], Moment Recurrences,seg_349,in the aforementioned to get
1735,0,[], Moment Recurrences,seg_349,x) and cancel out common terms to get
1736,1,['factor'], Moment Recurrences,seg_349,"taking npq as common factor, we notice that the alternate terms in the left and right sums of 6.50 cancel out giving"
1737,1,"['summation', 'case']", Moment Recurrences,seg_349,"when np is an integer. if np is noninteger, we carry out the summation up to   = ⌊np⌋ so that the last term of the lhs remains (all others are canceled out). in this case, we get"
1738,1,"['power method', 'method']", Moment Recurrences,seg_349,"where   = ⌊np⌋ is the greatest integer ≤np. this is the form obtained originally by de moivre and subsequently by bertrand [131]. johnson [125], diaconis and zabell [132] among others have discussed other equivalent forms and approximations. in section 6.5.6, page 208, we provide a new method to find the md using the power method introduced in section 6.3, page 189."
1739,1,"['functions', 'results']", Moment Recurrences,seg_349,"6.5.2.1 generating functions generating functions are extensively discussed in chapter 9. here we give the main results, which are proved in that chapter."
1740,1,"['moment', 'moment generating function', 'probability', 'function']", Moment Recurrences,seg_349,"theorem 6.4 the probability generating function is px(t) = (q + pt)n, and the moment generating function is mx(t) = (q + pet)n."
1741,0,[], Moment Recurrences,seg_349,proof: the pgf is
1742,1,"['expectation operator', 'expectation']", Moment Recurrences,seg_349,"where e() is the expectation operator. the mgf is found similarly by replacing tx by etx. see also chapter 9, page 379."
1743,1,"['independent', 'table', 'success', 'probability of success', 'binomial', 'probability', 'random']", Additivity Property,seg_351,"if x1 ∼ bino(n1, p) and x2 ∼ bino(n2, p) are independent binomial random variables with the same probability of success p, the sum x = x1 + x2 is distributed as bino(n1 + n2, p) (table 6.4)."
1744,1,"['independent', 'distribution', 'bernoulli', 'success', 'trials', 'bernoulli trials', 'binomial', 'probability', 'binomial distribution', 'probability of success']", Additivity Property,seg_351,"proof: the easiest way to prove the above-mentioned result is using the mgf. as x1 and x2 are independent, mx1+x2 (t) = mx1 (t) ∗ mx2 (t). substituting mx(t) = (q + pet)n, the rhs becomes (q + pet)n1 ∗ (q + pet)n2 = (q + pet)n1+n2 , which is the mgf of bino(n1 + n2, p). an interpretation of this result in terms of bernoulli trials is the following – “if there are n1 independent bernoulli trials with the same probability of success p and another n2 independent bernoulli trials with the same probability of success, they can be combined in any desired order to produce a binomial distribution of size n1 + n2.”"
1745,1,"['random variable', 'variable', 'random']", Additivity Property,seg_351,"another way of stating the above-mentioned theorem is that if x + y is distributed as bino(n1 + n2, p), and either of x or of y is distributed as bino(n1, p), the other random variable must be bino(n2, p) (or ber(p) is n2 = 1). this"
1746,1,"['independent', 'success', 'probability of success', 'binomial', 'binomial distributions', 'probability', 'distributions']", Additivity Property,seg_351,"result can be extended to any number of independent binomial distributions with the same probability of success. symbolically, if xi ∼ bino(ni, p), then"
1747,0,[], Additivity Property,seg_351,"∑ixi ∼ bino(∑ini, p)."
1748,1,['distribution'], Additivity Property,seg_351,example 6.9 distribution of y = n − x
1749,1,"['parameters', 'distribution', 'binomial', 'mean', 'binomial distribution', 'variance']", Additivity Property,seg_351,"if x has a binomial distribution with parameters n and p, derive the distribution of y = n − x and obtain its pgf and mgf. obtain the mean and variance. what is the additive property for y?"
1750,1,['range'], Additivity Property,seg_351,"solution 6.9 as x takes the values 0, 1, …, n; y also takes the same values in reverse. thus, the range of x and y are the same."
1751,1,['binomial'], Additivity Property,seg_351,y) qypn−y. this is the pdf of a binomial
1752,0,[], Additivity Property,seg_351,"distribution with p and q reversed. hence y = n − x ∼ bino(n, q), so that all properties are obtained by swapping the roles of p and q in the corresponding property of bino(n,p). the pgf is"
1753,1,"['variates', 'cumulant', 'mean', 'binomial', 'random', 'function', 'variance']", Additivity Property,seg_351,"similarly, the mgf is my (t) = e(ety) = (p + qet)n. from this, the cumulant generating function follows as n ∗ ln (q + pet). the mean and variance are nq and npq, respectively. this shows that x and y have the same variance, but the mean is nq = n(1 − p) = n − np. as it is binomial distributed, the additive property remains the same. this means that if y and z are bino(n1, q) and bino(n2, q) random variates, then y + z is distributed as bino(n1 + n2, q), provided q (or equivalently p = 1 − q) is the same."
1754,1,"['failures', 'successes', 'distribution', 'trials', 'binomial', 'binomial distribution']", Distribution of the Difference of Successes and Failures,seg_353,"the number of successes and failures in a binomial distribution is related through n. if there are x successes, there exist n − x failures and vice versa. in other words, they must add up to the total number of trials. the following example derives the distribution of the absolute difference of them."
1755,1,['distribution'], Distribution of the Difference of Successes and Failures,seg_353,example 6.10 distribution of u = |x − y|∕2.
1756,1,"['independent', 'failures', 'successes', 'distribution', 'bernoulli', 'success', 'trials', 'bernoulli trials', 'probability of success', 'probability', 'tails']", Distribution of the Difference of Successes and Failures,seg_353,let x denotes the number of successes (or heads) and y denotes the number of failures (or tails) in n independent bernoulli trials with the same probability of success p. find the distribution of u = |x − y|∕2 for n even.
1757,1,['probability'], Distribution of the Difference of Successes and Failures,seg_353,"solution 6.10 obviously, u takes the values (0, 1, … , n∕2). as n is even, it can take the value 0 in just one way – when both x and y are n∕2. the probability of"
1758,1,['case'], Distribution of the Difference of Successes and Failures,seg_353,this case is (n 2
1759,1,"['failures', 'successes', 'probability']", Distribution of the Difference of Successes and Failures,seg_353,"other values are materialized. first consider the number of successes exceeding the number of failures by x. let t be the number of failures (so that the number of successes is t + x). then t = (n − x)∕2, and t + x = (n + x)∕2. probability of this happening is"
1760,1,"['failures', 'successes', 'probability']", Distribution of the Difference of Successes and Failures,seg_353,"for x = 2, 4, .., n. next consider the number of failures exceeding the number of successes by x. let t be the number of failures. then t = (n + x)∕2 and t − x = (n − x)∕2. probability of this happening is"
1761,0,['n'], Distribution of the Difference of Successes and Failures,seg_353,"n x), this becomes"
1762,1,['probability'], Distribution of the Difference of Successes and Failures,seg_353,"for x = 2, 4, …, n. adding (6.55) and (6.57) gives the probability of u assuming the value u as"
1763,1,['factor'], Distribution of the Difference of Successes and Failures,seg_353,take qn as a common factor and write (q∕p)t = (p∕q)−t to get the alternate form
1764,1,"['sample', 'table']", Distribution of the Difference of Successes and Failures,seg_353,"t ) ∕2n−1. a similar result could be derived when n is odd. see table 6.5 for some sample values, and exercise 6.11 (p. 247)."
1765,1,"['failures', 'table', 'successes', 'distribution', 'trials']", Distribution of the Difference of Successes and Failures,seg_353,"there is another way to derive the above-mentioned distribution using a result in chapter 10 (p. 401). let there be x successes and (n − x) failures in n trials (table 6.6). then s − f = x − (n − x) = 2x − n = y(say), where s denotes the successes and f"
1766,1,['failures'], Distribution of the Difference of Successes and Failures,seg_353,"denotes the failures. clearly, y takes the values −n,−n + 2, … , 0, …, n − 2, n."
1767,1,['distribution'], Distribution of the Difference of Successes and Failures,seg_353,n p(y = y) = p(2x − n = y) = p(x = (n + y)∕2) = ( (n + y)∕2) p(n+y)∕2q(n−y)∕2. (6.59) the distribution of |y| is given in section 10.4.1 as f (y) + f (−y). put y = −y in equation (6.59) and add to get
1768,1,['factors'], Distribution of the Difference of Successes and Failures,seg_353,n x) and take common factors outside to get the above-mentioned form
1769,1,"['probabilities', 'distribution', 'binomial', 'binomial distribution', 'recurrence']", Algorithm for Binomial Distribution,seg_355,successive probabilities of the binomial distribution are found using the recurrence
1770,0,[], Algorithm for Binomial Distribution,seg_355,qn. this could also be written as
1771,0,['n'], Algorithm for Binomial Distribution,seg_355,reduce the number of iterations by starting with f (n) = (
1772,0,['n'], Algorithm for Binomial Distribution,seg_355,n) pn = pn and recurring
1773,0,[], Algorithm for Binomial Distribution,seg_355,backward using the relationship f (x − 1) = q
1774,0,[], Algorithm for Binomial Distribution,seg_355,example 6.11 winning group
1775,1,"['independent', 'probability']", Algorithm for Binomial Distribution,seg_355,"a class has b boys and g girls, both ≥ 2. a competition is conducted between the boys (who form group g1) and the girls (who form group g2), where each competition is independent of others and it is between the groups. if there are n prizes to be distributed to the winning groups, find (i) probability that girls bag more prizes than boys, (ii) number of prizes bagged by boys is odd, (iii) number of prizes bagged by girls is even number, and (iv) boys get no prizes."
1776,1,"['distribution', 'probability']", Algorithm for Binomial Distribution,seg_355,"solution 6.11 as there are b boys and g girls, the proportion of boys is b∕(b + g) and that of girls is g∕(b + g). as there are n prizes, the distribution of the prizes in favor of the boys is a bino(n, b∕(b + g)), where we have assumed that a “suc- cess” corresponds to the boys winning a prize. we assume that this probability remains the same because the prizes are distributed to the groups independently. (i) probability that girls bag more prizes than boys = probability that boys get less prizes than girls = pr[x n−x] = pr[2x   n] = pr[x   n/2]"
1777,1,['summation'], Algorithm for Binomial Distribution,seg_355,"= ∑ piqn−i, where p = b∕(b + g) and the summation is from 0 to i=0"
1778,1,"['binomial theorem', 'binomial']", Algorithm for Binomial Distribution,seg_355,"consider the expression (p + q)n − (p − q)n. expanding using binomial theorem and canceling out all even terms, we get"
1779,1,['probability'], Algorithm for Binomial Distribution,seg_355,"hence, the required probability is 1 2 [(p + q)n − (p − q)n]. however, p + q = 1 and p − q = b∕(b + g) − g∕(b + g) = (b − g)∕(b + g). substitute in the aforementioned to get the required probability as 1 2 [1 − [(b − g)∕(b + g)]n]. when the number of boys is less than that of girls, the second term can be negative for"
1780,1,"['binomial theorem', 'binomial']", Algorithm for Binomial Distribution,seg_355,"where we have swapped the roles of p and q. to evaluate this sum, consider the expression (p + q)n + (q − p)n. expanding using binomial theorem, all"
1781,0,[], Algorithm for Binomial Distribution,seg_355,". hence, the required prob-"
1782,1,['probability'], Algorithm for Binomial Distribution,seg_355,ability is 1 2 [1 + [(g − b)∕(b + g)]n]. (iv) probability that boys get no prizes = qn = [g∕(b + g)]n.
1783,1,['probability'], Algorithm for Binomial Distribution,seg_355,consider rolling a die 20 times. what is the probability of getting at least 10 sixes?
1784,1,"['distribution', 'probability']", Algorithm for Binomial Distribution,seg_355,"solution 6.12 the probability p of getting a six on any roll is 1/6, and the count x of sixes has a b(20, 1/6) distribution. hence, the required probability is obtained"
1785,1,['probabilities'], Algorithm for Binomial Distribution,seg_355,by summing the individual probabilities as∑x
1786,1,"['distribution', 'binomial distribution', 'binomial']", Tail Probabilities,seg_357,"the cdf of a binomial distribution bino(n, p) is fx(n, p) = ∑k"
1787,1,"['incomplete beta', 'method', 'beta function', 'incomplete beta function', 'distribution', 'binomial', 'function', 'binomial distribution']", Tail Probabilities,seg_357,"k) pkq(n−k). we could compute this by the straightforward method of adding the successive probabilities. however, for large n and k, this method is very inefficient. a better approach is to use the relationship between the binomial distribution and the incomplete beta function as follows."
1788,1,"['function', 'discrete', 'continuous']", Tail Probabilities,seg_357,"(see chapter 7, section 7.6, p. 7–36). the lhs of equation (6.63) is a discrete sum, whereas the rhs is a continuous function of p. when k   np∕2, this is computed as"
1789,1,"['tail', 'tail probabilities', 'probabilities']", Tail Probabilities,seg_357,"fx(k) = 1 − ip(k + 1, n − k). alternatively, the tail probabilities (sf) can be expressed"
1790,1,"['functions', 'gamma']", Tail Probabilities,seg_357,"replacing the factorials by gamma functions, this is seen to be equivalent to"
1791,1,"['incomplete beta', 'beta function', 'incomplete beta function', 'function']", Tail Probabilities,seg_357,"as the incomplete beta function is widely tabulated, it is far easier to evaluate the rhs. this is especially useful when n is large and k is not near n."
1792,1,"['power method', 'method', 'distribution', 'binomial', 'binomial distribution']", Tail Probabilities,seg_357,find the md of binomial distribution using the power method in section 6.1 (p. 189).
1793,1,"['distribution', 'binomial', 'mean', 'binomial distribution', 'limit']", Tail Probabilities,seg_357,"solution 6.13 we know that the mean of binomial distribution is np. the lower limit ll for the bino(n, p) is x = 0, so that xf(x) = 0. hence using theorem 6.1, the md is given by"
1794,1,"['results', 'table']", Tail Probabilities,seg_357,"the results obtained by equation (6.66) and theorem 6.1 are given in the table 6.7 (see also table 6.1). both results totally tally when np is an integer or half-integer (with the correction term). otherwise, the results are only approximate when np is small, but the accuracy increases for large np values."
1795,1,['limit'], Tail Probabilities,seg_357,"if n is large and p is small, the first few f(x) terms in equation (6.66) could be nearly zeros. a solution is to start the iterations at x =   − 1 and recur backward or start the iterations at a higher integer from lower limit ll and recur forward."
1796,1,"['normal approximation', 'probability', 'tail areas', 'approximation', 'symmetric', 'success', 'limit', 'curve', 'parameters', 'distribution', 'binomial', 'central limit theorem', 'variance', 'probabilities', 'tables', 'normal', 'probability of success', 'tail']", Approximations,seg_359,"as there are two parameters for this distribution, the binomial tables are lengthy and cumbersome. when the probability of success p is very close to 0.5, the distribution is nearly symmetric (see figure 6.6, p. 6–61). from figure 6.6, it is evident that the normal approximation is not good for x values away from the modal value (10) when n is small due to the disparity in the variance. if we reduce the variance of the approximating normal, the peak probabilities will increase. when n is large, the central limit theorem can be used to approximate binomial by a normal curve. the accuracy depends both on n and whether the value of p is close to 0.5. not only the probabilities but also the cumulative probabilities can also be approximated using normal tail areas. this approximation is quite good when p is near 0.5 rather than near 0 or 1 (use the normal approximation when np 10 or np(1 − p) 10 or both np and n(1 − p) are 5). symbolically, p[x ≤ k] = z(√"
1797,1,['standard'], Approximations,seg_359,"p q ), where z() is the standard"
1798,1,"['precision', 'approximation', 'discrete', 'distribution', 'discrete distribution', 'continuity correction']", Approximations,seg_359,"normal distribution. as this is an approximation of a discrete distribution by a continuous one, a continuity correction could improve the precision for small n. this gives us"
1799,1,['normal'], Approximations,seg_359,"(6.67) see reference 133 for normal approximations, references 123, 133–135 for further discussions."
1800,1,"['poisson', 'distribution', 'binomial', 'poisson distribution', 'binomial distribution']", Limiting Form of Binomial Distribution,seg_361,"the binomial distribution tends to the poisson law (p. 6–67) when n → ∞, p → 0 such that np remains a constant. this result was known to s.d. poisson (1837), which is why the limiting distribution is called poisson distribution. this is easily derived from the pdf as follows (see figures 6.3 and 6.7). write the pdf as"
1801,0,[], Limiting Form of Binomial Distribution,seg_361,"multiply the numerator and denominator by nk, combine it in the numerator with pk and write qn−k as (1 − p)n ∗ (1 − p)−k to obtain:"
1802,0,['n'], Limiting Form of Binomial Distribution,seg_361,"according to our assumption, np is a constant (say  ) so that p =  ∕n. substitute in the aforementioned and let n → ∞"
1803,0,[], Limiting Form of Binomial Distribution,seg_361,"if k is finite, the multipliers all tend to 1 and (1 −  ∕n)−k also tends to 1. the last term"
1804,1,['limit'], Limiting Form of Binomial Distribution,seg_361,"lt tends to e−  using the result that (1 − x∕n)n = e−x. hence in the limit, the rhs"
1805,1,"['poisson', 'distribution', 'poisson distribution']", Limiting Form of Binomial Distribution,seg_361,n → ∞ tends to the poisson distribution e−  k∕k!. we could write this in more meaningful form as
1806,1,"['poisson', 'linear', 'rate', 'probabilities', 'asymptotic', 'distribution', 'binomial', 'binomial distribution', 'convergence']", Limiting Form of Binomial Distribution,seg_361,"where o(np2∕2) is the asymptotic notation [8, 22]. an interpretation of this result is that the binomial distribution tends to the poisson law when p → 0 faster than n → ∞. in other words, the convergence rate is quadratic in p and linear in n. this allows us to approximate the binomial probabilities by the poisson probabilities even for very small n values (say n   10), provided that p is comparatively small."
1807,0,[], Limiting Form of Binomial Distribution,seg_361,"the above-mentioned result can also be proved using the pgf. we know that px(t; n, p) = (q + pt)n. write q = 1 − p and take logarithm of both sides to get log(px(t; n, p)) = n log(1 − p(1 − t)). write n = −(−n) on the rhs and expand as an infinite series using − log(1 − x) = x + x2∕2 + x3∕3 + · · · to get"
1808,1,"['poisson', 'distribution', 'poisson distribution']", Limiting Form of Binomial Distribution,seg_361,"write np =   and take negative sign inside the bracket. then, the rhs becomes log(px(t; n, p)) =  (t − 1) − np2(t − 1)2∕2 + · · ·. when exponentiated, the first term becomes the pgf of a poisson distribution (p. 6–69). the rest of the terms contain higher order powers of the form npr∕r for r ≥ 2."
1809,1,"['poisson', 'probabilities', 'approximation', 'distribution', 'random variable', 'variable', 'success', 'probability of success', 'binomial', 'probability', 'random', 'binomial distribution', 'parameter', 'tails']", Limiting Form of Binomial Distribution,seg_361,"we have assumed that p → 0 in the above-mentioned proof. this limiting behavior of p is used only to fix the poisson parameter. this has the implication that we could approximate both the left-tail and right-tail areas, as well as individual probabilities using the above-mentioned approximation. if p is not quite small, we use the random variable y = n − x, which was shown to have a binomial distribution (see example in p. 6–27) with probability of success q = 1 − p, so that we could still approximate probabilities in both tails when p is very close to 1."
1810,1,"['normal approximation', 'sample', 'approximation', 'symmetry', 'mean', 'limit', 'distributions', 'precision', 'distribution', 'central limit theorem', 'variance', 'poisson', 'probabilities', 'normal', 'normal distribution']", Limiting Form of Binomial Distribution,seg_361,"when p is near 0.5, a normal approximation is better than the poisson approximation due to the symmetry. however, a correction to the poisson probabilities could improve the precision. for large values of n, the distributions of the count x and the sample proportion are approximately normal. this result follows from the central limit theorem. the mean and variance for the approximately normal distribution of x are np and np(1 − p), identical to the mean and variance of the binomial(n, p) distribution. similarly, the mean and variance for the approximately normal distribution of the sample proportion are p and (p(1 − p)∕n)."
1811,0,[], Limiting Form of Binomial Distribution,seg_361,example 6.14 political parties
1812,1,['probability'], Limiting Form of Binomial Distribution,seg_361,"consider a group of n individuals who support one of two political parties say p1 and p2. assuming that none of the votes are invalid, what is the probability that a candidate of party p1 wins over the other candidate?"
1813,0,[], Limiting Form of Binomial Distribution,seg_361,"solution 6.14 if the voting decision of an individual is not influenced by the decision of another (for example, husband’s and wife’s decision or decision"
1814,1,"['random variable', 'variable', 'binomial', 'random']", Limiting Form of Binomial Distribution,seg_361,"among friends), the proportion of individuals who support one of two political parties can be regarded as a binomial distributed random variable with probability p. to find winning chances, we need to consider whether n is odd or even. if n is odd, p1 will win if the number of votes received is ≥n+1 . thus, the required 2"
1815,1,"['summation', 'probability']", Limiting Form of Binomial Distribution,seg_361,"x) pxqn−x, where p = probability that the vote is in favor 2 of the candidate of p1. if n is even, the summation needs to be carried out from"
1816,0,[], Limiting Form of Binomial Distribution,seg_361,example 6.15 malfunctioning electronic device
1817,1,"['observations', 'probability']", Limiting Form of Binomial Distribution,seg_361,consider an electronic device containing n transistors from the same manufacturer. the probability of each transistor malfunctioning is known from previous observations over a long period of time to be p. find the probability that (i) at most three transistors malfunction and (ii) none of the transistors malfunction.
1818,1,"['independent', 'binomial']", Limiting Form of Binomial Distribution,seg_361,"solution 6.15 we assume that the transistors malfunction independent of each other. then, the number of transistors that malfunction has a binomial distribu-"
1819,1,['probability'], Limiting Form of Binomial Distribution,seg_361,"tion. hence, the required probability is p[x ≤ 3] = ∑x"
1820,1,['probability'], Limiting Form of Binomial Distribution,seg_361,x) pxqn−x. probability
1821,0,[], Limiting Form of Binomial Distribution,seg_361,that none of them malfunction is ( 0
1822,1,"['dependent', 'bernoulli', 'trials', 'bernoulli trials']", Limiting Form of Binomial Distribution,seg_361,"see reference 136 for dependent bernoulli trials and references 135, 137, and 138 for further examples."
1823,1,"['outcomes', 'uniform distribution', 'discrete', 'distribution', 'random variable', 'variable', 'probability', 'random', 'discrete uniform distribution']", DISCRETE UNIFORM DISTRIBUTION,seg_363,a random variable that takes equal probability for each of the outcomes has a discrete uniform distribution (duni[n]). the pdf is given by
1824,1,"['range', 'discrete', 'distribution', 'bernoulli', 'bernoulli distribution']", DISCRETE UNIFORM DISTRIBUTION,seg_363,"for n = 2, we get the bernoulli distribution with p = 1∕2. as each of the probabilities is equal, f (x)∕f (x + k) = 1 for all k in the range. it is also called discrete"
1825,1,"['distribution', 'uniform distribution', 'discrete uniform distribution', 'discrete']", DISCRETE UNIFORM DISTRIBUTION,seg_363,"rectangular distribution. a displaced discrete uniform distribution duni[a, b] (where b   a) can be defined as f (x) = pr[x = a + k] = 1∕n, for k = 0, 1, 2, … ,b−a (or f (x) = pr[x = k] = 1∕n for k = a, a + 1, a + 2, … , b − 1). choosing a = 0, b = n gives another form of the distribution as f (x) = 1∕(n + 1), for x = 0, 1, 2, … , n. in general, we could shift the origin by c (positive or negative) to get the generalized duni(n) as f (x) = pr[x = k] = 1∕n, for k = c, c + 1, …, c + n − 1."
1826,1,['mean'], DISCRETE UNIFORM DISTRIBUTION,seg_363,find the cdf of duni[n] and obtain the mean using e(x) = ∑kp(x ≥ k).
1827,1,"['distribution', 'power method', 'method']", DISCRETE UNIFORM DISTRIBUTION,seg_363,find the md of duni[n] distribution using the power method in section 6.1 (p. 6–7) when n is odd.
1828,1,"['distribution', 'mean']", DISCRETE UNIFORM DISTRIBUTION,seg_363,"solution 6.17 we know that the mean of duni[n] distribution is c = (n + 1)∕2. using theorem 6.1, the md is given by"
1829,1,['summation'], DISCRETE UNIFORM DISTRIBUTION,seg_363,take (1∕n) outside the summation and evaluate∑x
1830,1,"['distribution', 'parameter']", Properties of Discrete Uniform Distribution,seg_365,this distribution has a single parameter. the mgf is easy to find as
1831,0,[], Properties of Discrete Uniform Distribution,seg_365,for t ≠ 0 and = 1 for t = 0. the pgf is obtained by replacing et in the above by t as
1832,1,"['function', 'characteristic function']", Properties of Discrete Uniform Distribution,seg_365,t 1−tn px(t) = . the characteristic function is written as  x(t) = (1 − eitn)∕[n(e−it −
1833,1,['mean'], Properties of Discrete Uniform Distribution,seg_365,"n 1−t 1)], where we have divided both numerator and denominator by et. the mean and"
1834,1,"['coefficient', 'kurtosis']", Properties of Discrete Uniform Distribution,seg_365,variance are easily seen to be e(x) = (n + 1)∕2 and v(x) = (n2 − 1)∕12. the coefficient of kurtosis is  2 = 3
1835,1,"['probabilities', 'table', 'discrete', 'probability', 'distributions']", Properties of Discrete Uniform Distribution,seg_365,4 −1 ). this shows that it is always platykurtic. truncated discrete uniform distributions are of the same type with the probabilities simply enlarged (because dividing by the truncated sum of probabilities simply enlarges each individual probability. see table 6.5).
1836,1,"['function', 'variance']", Properties of Discrete Uniform Distribution,seg_365,example 6.18 variance as a function of   for the duni(n)
1837,1,"['function', 'variance']", Properties of Discrete Uniform Distribution,seg_365,express the variance of duni(n) as a function of   alone
1838,1,"['estimated', 'estimate', 'data', 'distribution', 'mean', 'variance']", Properties of Discrete Uniform Distribution,seg_365,"solution 6.18 we know that = (n + 1)∕2 and 2 = (n2 − 1)∕12. from = (n + 1)∕2, we get n = 2 ∗ − 1. write 2 = (n2 − 1)∕12 = (n − 1)(n + 1)∕12. substitute for (n + 1)∕2 = to get 2 = [(2 ∗ − 1) − 1] ∗ ∕6 = ( − 1) ∗ ∕3. if the variance of a duni(n) distribution is estimated from data, we can obtain an estimate of the mean as follows. write the aforementioned as a quadratic equation x2 − x − 3k = 0, where x = and k = 2."
1839,1,['estimate'], Properties of Discrete Uniform Distribution,seg_365,"  = (1 +√ 1 + 12 ∗  2)∕2. alternatively, we could first estimate n from vari-"
1840,1,"['mean', 'table']", Properties of Discrete Uniform Distribution,seg_365,ance as n = √ 1 + 12 ∗  2 and obtain the mean as (n + 1)∕2 (see table 6.8).
1841,1,"['factorial', 'moments', 'factorial moments']", Properties of Discrete Uniform Distribution,seg_365,example 6.19 factorial moments of duni(n)
1842,1,"['factorial', 'moments', 'mean', 'factorial moments']", Properties of Discrete Uniform Distribution,seg_365,"find the factorial moments of duni[n], and obtain the mean."
1843,0,[], Properties of Discrete Uniform Distribution,seg_365,solution 6.19 by definition
1844,1,"['summation', 'vary']", Properties of Discrete Uniform Distribution,seg_365,"as (1∕n) is a constant while summing with respect to x, take it outside the summation and adjust the indexvar to vary from k to n. this gives  (k) ="
1845,1,['distribution'], Properties of Discrete Uniform Distribution,seg_365,example 6.20 distribution of u = x+y.
1846,1,"['distribution', 'range']", Properties of Discrete Uniform Distribution,seg_365,"if x and y are iid duni(n) with the same range, find the distribution of u = x + y ."
1847,1,"['independence', 'loss']", Properties of Discrete Uniform Distribution,seg_365,"solution 6.20 without loss of generality, we assume that x and y take values 1, 2, … ,n. then pr[x + y = k] = pr[x = t ∩ y = k − t] = pr[x = t] ∩ pr[y = k − t] due to independence. as t is arbitrary, this becomes ∑t"
1848,1,"['sample', 'with replacement', 'replacement', 'sampling', 'distribution', 'probability distribution', 'population', 'probability', 'random', 'random sampling']", An Application,seg_367,"the duni(n) is used in lotteries and random sampling. let there be m prizes in a lottery. if n tickets are sold, the chance that an arbitrary ticket will win a prize is m∕n. if each ticket is printed with the same number of digits (say the width is 6), and each of the digits (0, 1, ..., 9) is equally likely, the pdf of kth digit is duni(10). similarly, in random sampling with replacement, if the population has n elements, the probability distribution of the kth item in the sample is 1∕n."
1849,1,"['bernoulli trials', 'probability', 'random', 'geometric', 'results', 'trials', 'random variable', 'success', 'trial', 'geometric distribution', 'distribution', 'outcome', 'independent', 'failures', 'bernoulli', 'variable', 'probability of success', 'experiments']", GEOMETRIC DISTRIBUTION,seg_369,"consider a sequence of independent bernoulli trials with the same probability of success p. we observe the outcome of each trial, and either continues it if it is not a success or stop it if it is a success. this means that if the first trial results in a success with probability p, we stop further trials. if not, we continue observing failures until the first success is observed. let x denotes the number of trials needed to get the first success. naturally x is a random variable that can theoretically take any value from 0 to ∞. in summary, practical experiments that result in a geometric distribution can be characterized by the following properties:"
1850,1,"['experiment', 'bernoulli', 'trials', 'bernoulli trials']", GEOMETRIC DISTRIBUTION,seg_369,1. the experiment consists of a series of iid bernoulli trials
1851,1,"['trials', 'limit']", GEOMETRIC DISTRIBUTION,seg_369,2. the trials can be repeated independently without limit (as many times as neces-
1852,1,"['outcome', 'trial']", GEOMETRIC DISTRIBUTION,seg_369,"sary) under identical conditions. the outcome of one trial has no effect on the outcome of any other, including next trial"
1853,1,"['success', 'probability of success', 'trial', 'probability']", GEOMETRIC DISTRIBUTION,seg_369,"3. the probability of success, p, remains the same from trial to trial until the exper-"
1854,0,[], GEOMETRIC DISTRIBUTION,seg_369,iment is over.
1855,1,"['trials', 'random variable', 'variable', 'random']", GEOMETRIC DISTRIBUTION,seg_369,4. the random variable x denotes the number of trials needed to obtain the first
1856,1,"['probabilities', 'failures', 'geometric distribution', 'distribution', 'random variable', 'variable', 'success', 'probability of success', 'probability', 'random', 'independence', 'trials', 'geometric']", GEOMETRIC DISTRIBUTION,seg_369,"if the probability of success is reasonably high, we expect the number of trials to get the first success to be a small number. this means that if p = 0.9, the number of trials needed is much less than if p = 0.5 in general. let xk denote the random variable for observing the first success. if a success is obtained after getting x failures, the probability is qxp by the independence of the trials. this is called the geometric distribution (see figure 6.3). it gets its name from the fact that the pdf is a geometric progression with first term p and common difference q with closed form (p∕(1 − q)). in other words, the individual probabilities (divided by the first probability p) form a geometric progression. some authors define the pdf of a geometric"
1857,1,['cases'], GEOMETRIC DISTRIBUTION,seg_369,"distribution as f (x; p) = qx−1p, where x = 1, 2, …,∞. we could combine the above-mentioned two cases and write the pdf as"
1858,0,[], GEOMETRIC DISTRIBUTION,seg_369,"⎧qx−1p if x ranges from 1, 2, 3, … ,∞ ⎪ f (x; p) = qxp if x ranges from 0, 1, 2, … ,∞ ⎨⎪0 elsewhere. ⎩"
1859,1,"['polya distribution', 'change of origin', 'case', 'location', 'distribution', 'mean', 'transformation', 'variance', 'location measures']", GEOMETRIC DISTRIBUTION,seg_369,"the second form follows easily by a change of origin transformation y = x − 1 in the first form. the mean, mode, and other location measures are simply displaced in this case (see the following discussion). the variance remains the same because v(x − 1) = v(x). the polya distribution"
1860,1,['mean'], GEOMETRIC DISTRIBUTION,seg_369,"is obtained by setting the mean q∕p =   so that 1∕p = (1 +  ) or p = 1∕(1 +  ). now substitute in qx−1p, multiply numerator and denominator by   to get the above-mentioned form (figure 6.4)."
1861,1,"['interval', 'distribution', 'success', 'probability of success', 'probability', 'parameter', 'trial']", GEOMETRIC DISTRIBUTION,seg_369,"this can be considered as the distribution of waiting time until the occurrence of first success. consider a sequence of customers in a service queue. assume that either a new customer joins the queue (with probability p) or none arrives (with probability q = 1 − p) in a short-enough time interval. the time t until the next arrival is distributed as geo(p). it has a single parameter p, the probability of success in each trial."
1862,1,"['case', 'negative binomial distribution', 'variates', 'distribution', 'negative binomial', 'binomial', 'parameter', 'binomial distribution', 'geometric', 'distributions']", GEOMETRIC DISTRIBUTION,seg_369,"6.7.0.1 relationship with other distributions it is a special case of the negative binomial distribution when r = 1. if x1,x2, … ,xn are iid geometric variates with parameter p, then y = x1 + x2 + … + xn has a negative binomial distribution with"
1863,1,"['distribution', 'exponential', 'exponential distribution', 'discrete']", GEOMETRIC DISTRIBUTION,seg_369,"parameters n,p. this is easily proved using the mgf. it can also be considered as the discrete analog of the exponential distribution."
1864,1,"['functions', 'moments', 'mean', 'variance']", GEOMETRIC DISTRIBUTION,seg_369,"6.7.0.2 moments and generating functions the mean and variance are   = 1∕p,  2 = q∕p2 if f (x; p) = qx−1p, with support x = 1, 2, … ; and q∕p, q∕p2 if f (y; p) = qyp with support y = 0, 1, 2, … . the mean is easily obtained"
1865,1,"['distribution', 'moments', 'mean']", GEOMETRIC DISTRIBUTION,seg_369,"if the pdf is taken as f (x; p) = qx−1p, the mean is 1∕p. the ordinary moments of this distribution are easy to find using pgf or mgf. we find higher order moments using the mgf technique."
1866,1,"['characteristic function', 'range', 'function']", GEOMETRIC DISTRIBUTION,seg_369,(the mgf for range 1 to ∞ is pet∕(1 − qet)). the characteristic function is obtained from the mgf as φx(t) = p∕(1 − qeit).
1867,1,"['geometric distribution', 'distribution', 'moments', 'geometric']", GEOMETRIC DISTRIBUTION,seg_369,example 6.21 moments of geometric distribution
1868,1,"['distribution', 'moments']", GEOMETRIC DISTRIBUTION,seg_369,obtain the moments of geo(p) distribution using mgf
1869,1,['variance'], GEOMETRIC DISTRIBUTION,seg_369,"from which mx ′′(0) = q∕p2 + q2∕p2, so that the variance is q∕p2. the cdf is"
1870,1,['probability'], GEOMETRIC DISTRIBUTION,seg_369,"x =0 qkp = p[1 + q + q2 + · · · + qx]. as q is a probability, each power of q is between 0 and 1. hence, the above-mentioned series converges for all values of q, giving the summed value p[1 − qx+1]∕[1 − q]. as (1 − q) = p, the p in the numerator and denominator cancels out giving the cdf as f(x) = [1 − qx+1]."
1871,1,"['deviation', 'geometric distribution', 'distribution', 'mean', 'geometric']", GEOMETRIC DISTRIBUTION,seg_369,example 6.22 mean deviation of geometric distribution
1872,1,"['distribution', 'geometric distribution', 'geometric']", GEOMETRIC DISTRIBUTION,seg_369,find the md of geometric distribution using theorem 6.1 (p. 189).
1873,1,"['distribution', 'mean']", GEOMETRIC DISTRIBUTION,seg_369,"solution 6.22 we know that the mean of geo(p) distribution is   = q∕p. using theorem 6.1, the md is given by"
1874,0,[], GEOMETRIC DISTRIBUTION,seg_369,combine with the first term to get
1875,0,[], GEOMETRIC DISTRIBUTION,seg_369,write q = 1 − p in the exponent to get an alternate expression
1876,1,"['distribution', 'exponential', 'exponential distribution']", GEOMETRIC DISTRIBUTION,seg_369,example 6.23 y = ⌊x⌋ of an exponential distribution
1877,1,"['distribution', 'exponential', 'exponential distribution']", GEOMETRIC DISTRIBUTION,seg_369,"if x has an exponential distribution, find the distribution of y = ⌊x⌋."
1878,1,['continuous'], GEOMETRIC DISTRIBUTION,seg_369,"solution 6.23 as x is continuous, pr[y = y] = pr[y ≤ x y + 1]. now consider"
1879,1,"['success', 'geometric', 'probability of success', 'probability']", GEOMETRIC DISTRIBUTION,seg_369,"write exp(− y) as [exp(− )]y. then, equation (6.89) is of the form qyp = (1 − q)qy, where q = exp(− ). this is the pdf of a geometric distribution with probability of success p = 1 − q = [1 − exp(− )]. hence, y = ⌊x⌋ is geo([1 − exp(− )])."
1880,1,"['geometric distribution', 'distribution', 'moments', 'geometric']", GEOMETRIC DISTRIBUTION,seg_369,example 6.24 moments of geometric distribution qx∕2p
1881,1,"['distribution', 'mean']", GEOMETRIC DISTRIBUTION,seg_369,find the mean of a distribution defined as
1882,0,[], GEOMETRIC DISTRIBUTION,seg_369,"q p if x ranges from 0, 2, 4, 6, …,∞ f (x; p) = { 0"
1883,1,['factor'], GEOMETRIC DISTRIBUTION,seg_369,solution 6.24 by definition e(x) = ∑xxqx∕2p = p[2q + 4q2 + 6q3 + · · ·]. take 2q as common factor and simplify using
1884,1,"['geometric', 'geometric probability', 'probability']", GEOMETRIC DISTRIBUTION,seg_369,example 6.25 geometric probability exceeding 1∕p
1885,1,"['mean', 'probability']", GEOMETRIC DISTRIBUTION,seg_369,if x ∼ geo(p) find the probability that x takes values larger than the mean.
1886,1,"['integer part', 'probability']", GEOMETRIC DISTRIBUTION,seg_369,"solution 6.25 let ⌊1∕p⌋ denote the integer part. then, the required probability is"
1887,1,"['geometric distribution', 'factorial', 'distribution', 'moments', 'factorial moments', 'geometric']", GEOMETRIC DISTRIBUTION,seg_369,example 6.26 factorial moments of geometric distribution
1888,1,"['factorial', 'distribution', 'moments', 'factorial moments']", GEOMETRIC DISTRIBUTION,seg_369,obtain the factorial moments of geo(p) distribution.
1889,0,[], GEOMETRIC DISTRIBUTION,seg_369,solution 6.26 differentiate the identify ∑x
1890,1,"['factorial', 'moments', 'factorial moments']", GEOMETRIC DISTRIBUTION,seg_369,"∞ =0 qx = 1∕(1 − q) with respect to q multiple times to obtain the factorial moments. differentiating it once, we get"
1891,0,[], GEOMETRIC DISTRIBUTION,seg_369,"multiply both sides by pq. then, the lhs becomes ∑x"
1892,1,['results'], GEOMETRIC DISTRIBUTION,seg_369,∞ =0 xqxp = e(x). the rhs is pq∕p2 = q∕p. differentiating it again results in
1893,0,[], GEOMETRIC DISTRIBUTION,seg_369,multiply both sides by q2p and simplify to get e[x(x − 1)] = 2q2∕p2. differentiating k times gives
1894,0,[], GEOMETRIC DISTRIBUTION,seg_369,multiply both sides by qkp to get
1895,1,"['distribution', 'geometric distribution', 'geometric']", GEOMETRIC DISTRIBUTION,seg_369,we could reparameterize the geometric distribution by putting   = q∕p to get
1896,1,"['geometric distribution', 'distribution', 'mean', 'variance', 'geometric']", GEOMETRIC DISTRIBUTION,seg_369,with mean   and variance  (1 +  ). left truncated geometric distribution is obtained by truncating at a positive integer k. the resulting pdf is f (x; p) = qx+kp∕[1 −∑y
1897,1,"['densities', 'independent', 'failures', 'geometric distribution', 'change of origin', 'successes', 'distribution', 'bernoulli', 'trials', 'bernoulli trials', 'transformation', 'geometric']", Properties of Geometric Distribution,seg_371,"both the above-mentioned densities in equation 6.7 are related through a change of origin transformation y = x−1. this simply displaces the distribution to the left or right. using e(y) = e(x) − 1, we get e(y) = (1∕p − 1) = (1 − p)∕p = q∕p. variance remains the same because v(y) = v(x). as 2 = ∕p , the distribution is over-dispersed. similarly, z = min(x1,x2, … ,xn) has the same geometric distribution. a geometric distribution of order k is an extension:–in a sequence of independent bernoulli trials, we look for the first consecutive block of k successes (either in the beginning itself or surrounded by failures). for example, in ssffsfssf sssf, an sss occurs at position 10."
1898,1,"['characteristic function', 'leptokurtic', 'table', 'geometric distribution', 'symmetric', 'kurtosis', 'distribution', 'probability', 'function', 'skewness', 'geometric']", Properties of Geometric Distribution,seg_371,"coefficient of skewness is  1 = (2 − p)∕√q. as the numerator never vanishes for valid values of 0 ≤ p ≤ 1, the geometric distribution is never symmetric (in fact, it is always positively queued). the kurtosis is  2 = (p2 + 6q)∕q = 6 + p2∕q. as p2∕q can never be negative, the distribution is always leptokurtic. probability generating function is p∕(1 − qs) and the characteristic function is p∕(1 − qeit) (see table 6.9)."
1899,1,"['population variance', 'sample', 'sample mean', 'observations', 'expected value', 'statistical', 'mean', 'population', 'sample variance', 'variance', 'distributions']", Properties of Geometric Distribution,seg_371,"in chapter 3, it was mentioned that the sample variance is a measure of the spread of observations around the sample mean. as the expected value of a constant multiple of the sample variance is the population variance (e(s2) = ((n−1)/n) 2), we categorize statistical distributions using the boundedness property of population variance."
1900,1,"['cumulants', 'correlation', 'negative binomial', 'binomial distributions', 'function', 'geometric', 'negative binomial distributions', 'population variance', 'linear', 'logistic', 'uniform distribution', 'results', 'double exponential', 'samples', 'extreme value', 'exponential', 'mean', 'parameter', 'populations', 'population', 'coefficient', 'statistical', 'limit', 'distributions', 'functions', 'normal distributions', 'parameters', 'combination', 'asymptotic', 'geometric distribution', 'nonlinear', 'distribution', 'moments', 'binomial', 'correlation coefficient', 'variance', 'noncentral', 'poisson', 'linear combination', 'extreme value distributions', 'geometric distributions', 'bernoulli', 'normal']", Properties of Geometric Distribution,seg_371,"several statistical distributions have a single unknown parameter. examples are bernoulli, geometric, poisson, 2, exponential, rayleigh, and t distributions. the population variance is a linear function of this parameter for poisson ( = ), 2(= 2n), and other distributions. it is a nonlinear function for exponential (1∕ 2), t (n∕(n − 2)) and geometric distributions ((1 − p)∕p2). for normal distributions, it is 2. it is the square of a parameter for double exponential and logistic distributions, constant multiple of the square for rayleigh and extreme value distributions, and square of the difference of the parameters for uniform distribution. it is a quadratic for binomial (np(1 − p)) distribution. it is a linear combination of parameters for noncentral 2(2(n + 2 )). this discussion shows that the variance can be increased without limit by increasing the respective parameter(s) in the numerator or decreasing the parameters in the denominator for some distributions. however, there are some statistical distributions with strictly bounded parameter values, whose variance is either a ratio of parameters or a function of two or more unknown parameters (e.g., as transcendental functions), and cannot be increased without limit. examples are the beta-i(a, b) with variance ab∕[(a + b)2(a + b + 1)], student’s t (n∕(n − 2) for n 2), and the distribution of the correlation coefficient. this has interesting implications in the asymptotics of statistical distributions with respect to a subset of the parameter space. for example, the variance of both geometric and negative binomial distributions can be increased without limit by letting p → 0. we could reparametrize these distributions appropriately to have this asymptotic behavior at the extreme right end of the parameter space. for instance, let = q∕p for the geometric distribution, so that p = 1∕(1 + ). then p → 0 is equivalent to → ∞. these distributions have a characteristic property that the variance is greater than the mean. similar results could be obtained for higher order moments and cumulants. naturally, we expect this property to hold in samples drawn from such populations. such samples are called over-dispersion samples (s2 ≥ x)."
1901,1,"['characteristic function', 'parameters', 'random number', 'geometric distribution', 'negative binomial distribution', 'variates', 'distribution', 'negative binomial', 'binomial', 'parameter', 'random', 'function', 'binomial distribution', 'random numbers', 'geometric']", Properties of Geometric Distribution,seg_371,"let x1,x2, … ,xn be iid geometric random variates with common parameter p. then, y = x1 + x2 + … + xn has a negative binomial distribution with parameters n,p. this is easily proved using the characteristic function. this property can be used to generate random numbers from negative binomial distribution using a random number generator for geometric distribution. similarly, z = min(x1,x2, …,xn) has the same geometric distribution. see reference 139 for characterizations and reference 140 for applications."
1902,1,"['distribution', 'geometric distribution', 'variance', 'geometric']", Properties of Geometric Distribution,seg_371,example 6.27 variance of geometric distribution
1903,1,"['asymptotic', 'geometric distribution', 'distribution', 'mean', 'function', 'variance', 'geometric']", Properties of Geometric Distribution,seg_371,prove that the ratio of variance to the mean of a geometric distribution is 1∕p. express the variance as a function of   and discuss the asymptotic behavior.
1904,1,"['trials', 'success', 'mean', 'function', 'variance', 'limit']", Properties of Geometric Distribution,seg_371,"solution 6.27 we know that the variance is q∕p2 = (1 − p)∕p2 and mean is q∕p. as p→ 0, numerator of variance→ 1 and the denominator→ 0. the ratio  2∕  = (q∕p2)∕(q∕p) = 1∕p, which is obviously   1 as 0   p   1. thus, the ratio tends to ∞. this has the interpretation that as p→ 0, the number of trials needed to get the first success increases without limit. the variance is expressed as a function of the mean as  2 =  (1 +  )."
1905,1,"['conditional', 'distribution', 'geometric', 'conditional distribution']", Properties of Geometric Distribution,seg_371,example 6.28 conditional distribution of geometric laws
1906,1,"['distribution', 'conditional distribution', 'conditional']", Properties of Geometric Distribution,seg_371,"if x and y are iid geo(p), find the conditional distribution of (x|x + y = n)."
1907,1,['independent'], Properties of Geometric Distribution,seg_371,"solution 6.28 as x and y are independent,"
1908,1,['limit'], Properties of Geometric Distribution,seg_371,n =0 p[x = k]p[y = n − k] (here we have terminated the upper limit at n because y is positive) =∑k
1909,1,"['distribution', 'uniform distribution', 'discrete uniform distribution', 'discrete']", Properties of Geometric Distribution,seg_371,which is the pdf of a discrete uniform distribution duni((n+1)).
1910,1,"['geometric', 'probabilities']", Properties of Geometric Distribution,seg_371,example 6.29 geometric probabilities
1911,0,[], Properties of Geometric Distribution,seg_371,"if x∼ geo(p), find the following probabilities:– (i) x takes even values and (ii) x takes odd values."
1912,1,"['distribution', 'geometric distribution', 'geometric', 'probabilities']", Properties of Geometric Distribution,seg_371,"solution 6.29 as the geometric distribution takes x = 0, 1, 2, … ∞ values, both the above-mentioned probabilities are evaluated as infinite sums. (i) p[x is even] = q0p + q2p + · · · = p[1 + q2 + q4 + · · · ] = p∕(1 − q2) = 1∕(1 + q). (ii) p[x is odd] = q1p + q3p + · · · = qp[1 + q2 + q4 + · · · ] = qp∕(1 − q2) = q/(1+q), which could also be obtained from (i) because p[x is even] = 1−p[x is odd] = 1−[1/(1+q)] = q/(1+q)."
1913,1,"['density function', 'function', 'geometric']", Memoryless Property,seg_373,the geometric density function possesses an interesting property called memory-less property.
1914,0,[], Memoryless Property,seg_373,"substituting the pdf, this becomes ∑x"
1915,0,['n'], Memoryless Property,seg_373,"∞ =m+1 qxp = qn (see the following), which is pr(x n). the above-mentioned result holds even if the operator is replaced by ≥."
1916,1,['probabilities'], Tail Probabilities,seg_375,the survival probabilities from x = c is
1917,1,['probabilities'], Tail Probabilities,seg_375,"as q   1, this goes down to zero for large c. the left-tail probabilities can be found from complementation as pr[0 ≤ x ≤ c] = 1 − pr[x   c] = 1 − qc."
1918,1,"['random number', 'geometric distribution', 'distribution', 'samples', 'random', 'geometric']", Random Samples,seg_377,"random samples from this distribution can be generated using a uniform random number u in (0,1) by first finding a c such that 1 − qc−1   u   1 − qc. subtract 1 from each term and change the sign to get qc+1   1 − u   qc−1. now consider qc   1 − u. as 1−u and u have the same distribution, taking log we get c ∗ log(q)   log(u) from which c   log(u)∕ log(q). similarly, taking log of both sides of 1 − u   qc−1, we get (c − 1) log(q)   log(1 − u) or equivalently c   1 + log(u)∕ log(q). combine both the conditions to get c = ⌊1 + log(u)∕ log(q)⌋. this value being an integer is returned as the random variate from the geometric distribution."
1919,1,"['distribution', 'probabilities']", NEGATIVE BINOMIAL DISTRIBUTION,seg_379,"this distribution gets its name from the fact that the successive probabilities are obtained from the infinite series expansion of the expression pk(1 − q)−k, where q = 1 − p and p   0 (see also exercise 6.20, p. 6–97)."
1920,1,"['independent', 'failures', 'successes', 'bernoulli', 'trials', 'success', 'bernoulli trials', 'probability', 'trial']", NEGATIVE BINOMIAL DISTRIBUTION,seg_379,"consider a sequence of independent bernoulli trials. instead of counting the number of trials needed to get the first success, we count the number of trials needed to get the kth success, where k is a fixed constant integer greater than 1 known in advance (we are actually counting the number of failures, as the number of successes is fixed at k). hence, in x + k − 1 trials, we have observed k − 1 successes, and the (x + k)th trial must result in the kth success. the probability of occurrence is thus"
1921,0,['n'], NEGATIVE BINOMIAL DISTRIBUTION,seg_379,"n x), the pdf becomes"
1922,1,['geometric'], NEGATIVE BINOMIAL DISTRIBUTION,seg_379,"for x = 0, 1, 2, … and k = 1, 2, … . for k = 1, this reduces to the geometric distri-"
1923,0,[], NEGATIVE BINOMIAL DISTRIBUTION,seg_379,is not restricted to be an integer. put y = x + k in equation (6.99) to get an alternate
1924,0,[], Properties of Negative Binomial Distribution,seg_381,"x−1), the pdf can be written alternatively as"
1925,0,[], Properties of Negative Binomial Distribution,seg_381,differentiating with respect to t and putting t = 0 gives e(x) = kq∕p.
1926,0,[], Properties of Negative Binomial Distribution,seg_381,example 6.30 candidate interviews
1927,1,"['set', 'probability']", Properties of Negative Binomial Distribution,seg_381,"a company requires k candidates with a rare skill set, see figure 6.6 for p = 0.2, 0.8. as there is a scarcity of local candidates perfectly matching the required skill set, the company decides to conduct a walk-in interview until all k candidates have been found. if the probability of a candidate who matches perfectly is p, find the expected number of candidates interviewed, assuming that several candidates whose skill set is not completely matching also walks-in."
1928,1,"['parameters', 'distribution', 'negative binomial', 'set', 'binomial', 'probability']", Properties of Negative Binomial Distribution,seg_381,"solution 6.30 we are given that the probability of perfect match is p. each interviewed candidate is either rejected if the skill set is not 100% match or hired. as the company needs k such candidates, the distribution of finding all k candidates is negative binomial with parameters (k,p). the expected number of candidates is kq∕p. owing to the rarity of the sought skill set, p is small so that q∕p is large. for instance, if p = 0.1, q∕p = 9 and if p = 0.005, q∕p = 199."
1929,1,"['moment', 'factorial', 'moments', 'factorial moment', 'factorial moments', 'falling factorial']", Properties of Negative Binomial Distribution,seg_381,6.8.1.1 factorial moments the falling factorial moments are easier to find than ordinary moments. let  (r) denote the rth factorial moment.
1930,1,"['moment', 'factorial', 'moment ', 'factorial moment']", Properties of Negative Binomial Distribution,seg_381,theorem 6.6 the factorial moment  (r) is given by  (r) = k(r) (q∕p)r.
1931,1,['range'], Properties of Negative Binomial Distribution,seg_381,k+x−1 over the proper range of x to get the rhs as ∑xx(x − 1) … (x − r + 1)( x ) pkqx.
1932,0,[], Properties of Negative Binomial Distribution,seg_381,put y = x − r in equation (6.103) and rearrange the indexvar. this gives
1933,1,"['function', 'gamma', 'gamma function']", Properties of Negative Binomial Distribution,seg_381,"the infinite sum in equation (6.104) is easily seen to be (1 − q)−(k+r). as (1 − q) = p, the pk cancels out giving  (r) = e[x(r)] = k(r)(q∕p)r. this can be written in terms of gamma function as  (r) = [γ(k + r)∕γ(k)] (q∕p)r."
1934,1,"['negative binomial', 'binomial distributions', 'tail areas', 'function', 'binomial distribution', 'parameter', 'distributions', 'distribution', 'binomial', 'beta function', 'poisson', 'f distribution', 'incomplete beta', 'incomplete beta function', 'negative binomial distribution', 'tail']", Properties of Negative Binomial Distribution,seg_381,"6.8.1.2 relationship with other distributions tail areas of binomial and negative binomial distributions are related. consider equation (6.101). then pr[y ≥ n − c] = pr[x ≤ c], where x is distributed as bino(n, p) and y as nbino(c, p). as k → ∞ and p →1 such that k(1 − p) is a constant, the negative binomial distribution approaches a poisson law with parameter = k(1 − p). similarly, nbino(k, p∕k) as k → ∞ tends to the poisson law exp(− ) x∕x! [121] with complexity o(kq2∕2). this means that “the negative binomial distribution tends to the poisson law when p →1 faster than k → ∞.” if k is an integer or a half-integer, the sf can be written as pr[y y] = i1−p(y, k), where i(a, b) denotes the incomplete beta function. this can also be written in terms of an f distribution as pr[y y] = ft(2k, 2y) where t = p ∗ y∕(q ∗ k)."
1935,1,"['independent', 'geometric distributions', 'negative binomial distribution', 'distribution', 'negative binomial', 'binomial', 'parameter', 'binomial distribution', 'geometric', 'distributions']", Properties of Negative Binomial Distribution,seg_381,"proposition 1 the negative binomial distribution can be regarded as a sum of k independent geometric distributions with the same parameter p. we know from section 6.7.0.2 in p. 216 that the mgf of a geo(p) distribution is given by mx(t) = p∕(1 − qet). hence, the mgf of k iid geo(p) is given by my (t) = [p∕(1 − qet)]k."
1936,1,"['poisson', 'gamma mixture of the poisson parameter', 'parameter', 'gamma']", Properties of Negative Binomial Distribution,seg_381,example 6.31 gamma mixture of the poisson parameter
1937,1,"['poisson', 'parameter', 'distribution']", Properties of Negative Binomial Distribution,seg_381,"prove that a gamma(m, p) mixture of the poisson parameter ( ) gives rise to a nbino(p,m∕(m + 1)) distribution."
1938,1,"['poisson', 'gamma', 'variates']", Properties of Negative Binomial Distribution,seg_381,"solution 6.31 the pdf of poisson and gamma variates are, respectively,"
1939,1,['distribution'], Properties of Negative Binomial Distribution,seg_381,the unconditional distribution is obtained as f (x) =
1940,0,[], Properties of Negative Binomial Distribution,seg_381,this upon rearrangement becomes
1941,1,"['negative binomial', 'variance', 'binomial']", Properties of Negative Binomial Distribution,seg_381,example 6.32 variance of negative binomial
1942,1,"['negative binomial', 'binomial', 'mean', 'random', 'variance']", Properties of Negative Binomial Distribution,seg_381,prove that the ratio of variance to the mean of a negative binomial random variable is 1∕p.
1943,1,"['variance', 'mean', 'parameter', 'limit']", Properties of Negative Binomial Distribution,seg_381,"solution 6.32 we know that the variance is given by v(r, p) = kq∕p2. obviously, this can be increased by increasing the parameter k without limit. as the mean is kq∕p, the ratio of variance to the mean is (kq∕p2)/(kq∕p) = 1∕p, which is obviously greater than 1 (as 0 p 1). this gives p ∗ 2 = ."
1944,1,"['negative binomial distribution', 'distribution', 'negative binomial', 'binomial', 'binomial distribution']", Properties of Negative Binomial Distribution,seg_381,find the md of negative binomial distribution using theorem 6.1 (p. 189).
1945,1,"['negative binomial distribution', 'distribution', 'negative binomial', 'binomial', 'mean', 'binomial distribution']", Properties of Negative Binomial Distribution,seg_381,"solution 6.33 we know that the mean of negative binomial distribution is   = kq/p. using theorem 6.1, the md is given by"
1946,1,"['incomplete beta', 'beta function', 'incomplete beta function', 'function']", Properties of Negative Binomial Distribution,seg_381,"k+c−1 where i(a, b) is the incomplete beta function. this simplifies to 2c( c )"
1947,1,"['integer part', 'mean', 'probability']", Properties of Negative Binomial Distribution,seg_381,"qcpk−1 = 2 2 ∗ fc, where fc is the probability mass evaluated at the integer part of the mean."
1948,1,"['moments', 'recurrence']", Moment Recurrence,seg_383,the central moments satisfy the recurrence
1949,1,['independent'], Moment Recurrence,seg_383,"k+x−1 as ( x ) is independent of p, and q = 1 − p, write the aforementioned as"
1950,1,['function'], Moment Recurrence,seg_383,differentiate the expression within the curly brackets with respect to p using the function of a function rule to get
1951,0,[], Moment Recurrence,seg_383,combine the last two terms as [k(1 − p) − px] = −p(x − k(1 − p)∕p) = −p(x + k − k∕p) to get −p (x + k − k∕p)r+1pk−1(1 − p)x−1. multiply and divide by pq and combine the terms as −(1∕q) (x + k − k∕p)r+1pk(1 − p)x. this gives
1952,0,[], Moment Recurrence,seg_383,cross-multiply and rearrange the expressions to get the result.
1953,1,"['random variables', 'variables', 'random']", Moment Recurrence,seg_383,"theorem 6.7 additivity theorem: if x1 ∼ nb(n1, p) and x2 ∼ nb(n2, p) are independent nb random variables, then x1 + x2 ∼ nb(n1 + n2, p)"
1954,1,['method'], Moment Recurrence,seg_383,"proof: this is most easily proved by the mgf method. we have seen in equation 6.103 that mgf is [p∕(1 − qet)]k. as p is the same, replace k by n1 and n2 and take the product to get the result. this result can be extended to any number of nbin(ri, p) as follows: if xi ∼ nbin(ri, p), then ∑ixi ∼ nbin(∑iri, p). the pgf is obtained by replacing et by t."
1955,1,"['incomplete beta', 'probabilities', 'beta function', 'table', 'incomplete beta function', 'distribution', 'tail probabilities', 'binomial', 'random', 'function', 'binomial distribution', 'tail']", Tail Probabilities,seg_385,"as the random variate extends to ∞, the right-tail probabilities are more challenging to evaluate (table 6.10 and figure 6.6). the left-tail probabilities of nbin(r, p) are related to the right-tail probabilities of binomial distribution as fk(r, p) = p(x ≤ k) = p(y ≥ r) = 1-bino(k + r, p). the upper tail probabilities of an nb distribution can be expressed in terms of the incomplete beta function as"
1956,1,"['tail', 'complement', 'tail probabilities', 'probabilities']", Tail Probabilities,seg_385,the lower tail probabilities can be found from the complement rule as
1957,1,"['distribution', 'tail', 'f distribution', 'tail areas']", Tail Probabilities,seg_385,this can also be expressed as tail areas of an f distribution (a. meyer [141]; see also guenther [142].
1958,1,"['negative binomial', 'probabilities', 'binomial']", Tail Probabilities,seg_385,example 6.34 negative binomial probabilities
1959,0,[], Tail Probabilities,seg_385,"if x∼ nbin (r, p), find the following probabilities:– (i) x takes even values and (ii) x takes odd values"
1960,1,['coefficients'], Tail Probabilities,seg_385,this can be simplified and expanded into an even polynomial in t with the corresponding coefficients giving the desired sum. (ii) the pgf for x taking odd values is 1 2 [px(t) − px(−t)]. substitute for px(t) to get
1961,1,"['probabilities', 'coefficients']", Tail Probabilities,seg_385,proceed as above and expand as an odd polynomial in t whose coefficients give desired probabilities.
1962,0,[], Tail Probabilities,seg_385,see references 143 and 144 for a generalizations and reference 145 for mle.
1963,1,"['poisson', 'model', 'random', 'interval', 'discrete', 'intervals', 'distribution', 'events', 'poisson distribution']", POISSON DISTRIBUTION,seg_387,"the poisson distribution was invented by s.d. poisson (1781–1840) in 1838 as counts (arrivals) of random discrete occurrences in a fixed time interval. it can be used to model temporal, spatial or spatiotemporal rare events that are open-ended. for example, it is used to predict the number of telephone calls received in a small time interval, number of accidents in a time period, number of automobiles coming at a gas station, number of natural disasters (like earthquakes) in a year, and so on. these are all temporal models with different time intervals. examples of spatial frame of reference include predicting defects in newly manufactured items such as clothing sheets, paper rolls or newsprints, cables and wires, and micro-chips. spatiotemporal applications include predicting earthquakes and tsunamis in a particular region over a time period, outbreak of epidemics in a geographical region over a time period, and so on. it is also used in many engineering fields. the unit of the time period in these"
1964,1,['poisson'], POISSON DISTRIBUTION,seg_387,cases is implicitly assumed by the modeler. a wrong choice of the time period may lead to convoluted poisson models.
1965,0,[], POISSON DISTRIBUTION,seg_387,the pdf is given by
1966,1,['range'], POISSON DISTRIBUTION,seg_387,"where e is the natural logarithm. obviously, summing over the range of x values"
1967,1,"['function', 'exponential']", POISSON DISTRIBUTION,seg_387,"∞ =0 e−  x∕x! = e− (1 +   +  2∕2! + · · · ) = e− e  = 1, where the indexvar is varied as an exponent and a function. it belongs to the exponential family."
1968,1,"['poisson', 'condition', 'approximation', 'case', 'data', 'distribution', 'cases', 'success', 'trials', 'probability of success', 'binomial', 'probability', 'binomial distribution']", POISSON DISTRIBUTION,seg_387,"it can be considered as the limiting case of a binomial distribution as shown in section 6.5.8 in page 6–37. most of the textbooks give this limiting behavior as follows: “when n, the number of trials is large, and p, the probability of success is small, such that np remains a constant , then bino(n, p) → pois( ).” johnson et al. [123, 306] mentions in page 152 that “it is the largeness of n and smallness of p that are important.” the product = np can remain a constant in two limiting cases: (i) n → ∞ faster than p → 0 and (ii) p → 0 faster than n → ∞. as shown in section 6.5.8, this limiting property is valid only when np remains finite, and np2∕2 and higher order terms are negligible. we give the revised rule that “the binomial distribution tends to the poisson law when p tends to zero faster than n tends to infinity.” thus, the poisson approximation is valid even for low values of n, provided that p is comparatively very small. in most practical applications, the value of n is at the hands of a researcher, and the value of p is observed from the data. when p is near 0.5, the above-mentioned condition may not hold. in such cases, a correction term is needed to get higher accuracy for the approximation. consider equation (6.72) in page 6–38, which is reproduced in the following:"
1969,0,[], POISSON DISTRIBUTION,seg_387,"keeping first term intact, and collecting constant terms from the rest, we get the rhs as"
1970,1,"['poisson', 'distribution', 'poisson distribution']", POISSON DISTRIBUTION,seg_387,"exponentiating lhs and rhs, we see that the first term becomes the pgf of the poisson distribution."
1971,1,"['distribution', 'mean', 'variance', 'parameter', 'noncentral', 'distributions']", Properties of Poisson Distribution,seg_389,"this distribution has a single parameter  , which is both the mean and variance of the distribution. it is easy to compute for small   values. it is an excellent choice for forming mixture distributions (like noncentral  2 distribution)."
1972,1,"['poisson', 'random variables', 'independent', 'variables', 'random']", Properties of Poisson Distribution,seg_389,the difference of two independent poisson random variables has the skellam distribution with pdf (see figure 6.5)
1973,1,"['function', 'bessel function', 'modified bessel function']", Properties of Poisson Distribution,seg_389,where ix() is the modified bessel function of the first kind.
1974,1,"['moments', 'moment']", Properties of Poisson Distribution,seg_389,6.9.1.1 moments and mgf the first moment is readily obtained as
1975,1,"['second moment', 'moment']", Properties of Poisson Distribution,seg_389,"using em ∗ en = em+n, the aforementioned reduces to  . to find the second moment e(x2), write x2 as x ∗ (x − 1) + x to get"
1976,1,"['poisson', 'moment', 'factorial', 'distribution', 'moments', 'factorial moment', 'poisson distribution', 'factorial moments', 'variance']", Properties of Poisson Distribution,seg_389,"from this, the variance is found as v(x) = e(x2) − e(x)2 = . factorial moments of a poisson distribution are easier to find because of the presence of x! in the denominator of the pdf. the rth factorial moment is"
1977,1,['moments'], Properties of Poisson Distribution,seg_389,"′ ′ higher order moments can be obtained from this as  2 =  2 +  ,  3 =  3 + 3 2 +  ."
1978,1,"['moment', 'moment generating function', 'function']", Properties of Poisson Distribution,seg_389,6.9.1.2 moment generating function the moment generating function is
1979,1,"['poisson', 'distribution', 'poisson distribution']", Properties of Poisson Distribution,seg_389,example 6.35 mode of poisson distribution
1980,1,"['poisson', 'distribution', 'bimodal', 'poisson distribution']", Properties of Poisson Distribution,seg_389,"prove that the mode of the poisson distribution is ⌊ ⌋ if   is noninteger and is bimodal with the modes located at [  − 1,  ] otherwise."
1981,1,"['integer part', 'prime number']", Properties of Poisson Distribution,seg_389,"solution 6.35 consider the ratio fx(k,  )∕fx(k − 1,  ) =  ∕k. if k ≤  , the lhs is strictly increasing. otherwise, it is strictly decreasing. if   is integer,  ∕k will assume the last integer value at k =   (if   is a prime number, this occurs only once, but if it is composite, the ratio could be integer for more than one value of k). thus, if   is an integer, the rhs becomes 1 when k =   giving fx( ,  ) = fx(  − 1,  ) (we have simply substituted k =  ). thus, the maximum occurs at k =   − 1 and  . otherwise, there is a single mode at [ ], the integer part."
1982,0,[], Properties of Poisson Distribution,seg_389,example 6.36 defectives in shipment
1983,1,"['poisson', 'tables', 'parameter', 'probability']", Properties of Poisson Distribution,seg_389,"consider a collection of items such as light bulbs and transistors, of which some are known to be defective with probability p = 0.001. let the number of defectives in a shipment follow a poisson law with parameter (tables 6.11 and 6.12). how is p and related? what is the probability of finding (i) no defectives and (ii) at least two defective items in a shipment containing 20 items?"
1984,1,"['complement', 'probability', 'event']", Properties of Poisson Distribution,seg_389,"solution 6.36 if n is the number of items in the shipment, p and   are related as np =  . to find the probability of at least two defectives, we use the complement-and-conquer rule. the complement event is that of finding"
1985,1,"['complement', 'probabilities', 'probability']", Properties of Poisson Distribution,seg_389,"either 0 or 1 defective. the corresponding probabilities are e− and e− . as n = 20, n ∗ p = 20 ∗ 0.001 = 0.02. (i) the probability of finding no defectives = e−0.02 = 0.98019867 and (ii) substitute for to get e−0.02 + 0.02 ∗ e−0.02 = 0.9801986 + 0.0196039 = 0.9998 as the complement probability. from this, the required answer follows as 1−0.9998 = 0.0002"
1986,1,['additivity property'], Properties of Poisson Distribution,seg_389,"6.9.1.3 additivity property if x1 ∼ pois( 1) and x2 ∼ pois( 2) are independent, then x1 + x2 ∼ pois( 1 + 2)."
1987,1,"['random variables', 'table', 'variables', 'random']", Properties of Poisson Distribution,seg_389,we get mx1+x2 (t) = e( 1+ 2)(et−1). this result can be extended to an arbitrary number of random variables (see table 6.11).
1988,1,['distribution'], Properties of Poisson Distribution,seg_389,example 6.37 distribution of x1|(x1 + x2 = n)
1989,1,"['distribution', 'independent']", Properties of Poisson Distribution,seg_389,"if x1 ∼ pois( 1) and x2 ∼ pois( 2) are independent, then the distribution of x1|(x1 + x2 = n) is bino(n,  1∕( 1 +  2))."
1990,1,"['probability', 'conditional', 'conditional probability']", Properties of Poisson Distribution,seg_389,solution 6.37 consider the conditional probability p[x1|(x1 + x2 = n)] = p[x1 = x1] ∩ p[x2 = n − x1]∕p(x1 + x2 = n). substitute the density to get e− 1 1
1991,0,[], Properties of Poisson Distribution,seg_389,mon terms from the numerator and denominator and writing ( 1 +  2)n in the denominator as ( 1 +  2)x1 ∗ ( 1 +  2)n−x1 this becomes
1992,1,"['success', 'probability of success', 'binomial', 'probability']", Properties of Poisson Distribution,seg_389,which is the binomial pdf with probability of success p =  1∕( 1 +  2).
1993,1,"['poisson', 'probabilities']", Properties of Poisson Distribution,seg_389,example 6.38 poisson probabilities
1994,0,['e'], Properties of Poisson Distribution,seg_389,above-mentioned sum in the square bracket is then cosh( ) = 1 2 (e  + e− ). from
1995,1,['probability'], Properties of Poisson Distribution,seg_389,"this, we get the required probability as e−  ∗ 1 2 (e  + e− ) = 1 2 (1 + e−2 ) (ii) p[x"
1996,1,"['distribution', 'conditional distribution', 'conditional']", Properties of Poisson Distribution,seg_389,example 6.39 conditional distribution
1997,1,"['conditional', 'distribution', 'random variable', 'variable', 'random', 'conditional distribution']", Properties of Poisson Distribution,seg_389,"if x ∼ pois( ), find the conditional distribution of the random variable (i) x|x is even and (ii) x|x is odd."
1998,1,"['poisson', 'random variable', 'variable', 'random']", Properties of Poisson Distribution,seg_389,"solution 6.39 let y denote the random variable obtained by conditioning x to even values and z denote the random variable obtained by conditioning x to odd values. as the poisson variate takes values x = 0, 1, 2, … the variate y takes the values y = 0, 2, 4, 6, … ∞, and z takes the values y = 1, 3, 5, … ∞. using above-mentioned example,"
1999,1,"['probability', 'conditional', 'conditional probability']", Properties of Poisson Distribution,seg_389,"from conditional probability, p[x = k|x is even] = p[x = k ∩ x is even]/p[x is even] = f(y) = 2e−  y∕[y!(1 + e−2 )]. this gives"
2000,1,"['poisson', 'probabilities', 'gamma', 'distribution', 'poisson distribution', 'tail probabilities', 'function', 'tail', 'gamma function', 'distributions']", Properties of Poisson Distribution,seg_389,6.9.1.4 relationship with other distributions the tail probabilities of a poisson distribution is related to the incomplete gamma function as follows:
2001,1,"['function', 'gamma', 'gamma function']", Properties of Poisson Distribution,seg_389,theorem 6.8 prove that the survival function of pois( ) is related to incomplete gamma function as
2002,1,['range'], Properties of Poisson Distribution,seg_389,"∞ e−xxrdx. put y = x −  , so that the range becomes 0 to ∞, and we get"
2003,1,['independent'], Properties of Poisson Distribution,seg_389,take constants independent of y outside the integral to get e   ∫0
2004,1,"['poisson', 'distribution', 'probability', 'function', 'poisson distribution']", Properties of Poisson Distribution,seg_389,1)amma function is the survival probability of poisson distribution. this proves
2005,1,"['poisson', 'degrees of freedom', 'distribution', 'fisher', 'poisson sum', 'function']", Properties of Poisson Distribution,seg_389,theorem 6.9 prove that the survival function of a central chi-square distribution with even degrees of freedom is a poisson sum (fisher [146]) as
2006,1,"['gamma', 'gamma distributions', 'distributions']", Properties of Poisson Distribution,seg_389,"proof: the proof follows easily because the  2 and gamma distributions are related as  2n ≡ gamma(n∕2, 1∕2)."
2007,1,"['poisson', 'probabilities']", Properties of Poisson Distribution,seg_389,"putting n = 2m, we find that the cdf of central  2 with even df can be expressed as a sum of poisson probabilities."
2008,1,"['probabilities', 'recurrence']", Algorithms for Poisson Distribution,seg_391,individual probabilities can be calculated using the forward recurrence
2009,1,"['poisson', 'memory', 'precision', 'probabilities', 'algorithm', 'distribution', 'loss', 'poisson distribution', 'poisson distributions', 'error', 'distributions']", Algorithms for Poisson Distribution,seg_391,"when   is large, e−  is too small. this may result in loss of precision or even underflow (in computer memory). as subsequent terms are calculated using the first term, error may propagate throughout the subsequent computation steps. a solution is to use the log-recursive algorithm suggested in reference 4. another possibility is an algorithm that starts with the mode of the poisson distribution, which then iteratively calculates subsequent values leftward (reverse) and rightward (forward). this may be combined with the log-recursive algorithm to provide a reliable and robust algorithm for poisson distributions, and other mixture distributions that use poisson weighting [4, 5]. the left-tail probabilities (cdf) fc( ) = ∑j"
2010,0,[], Algorithms for Poisson Distribution,seg_391,"c =0 p(j) converge rapidly for small   values. cdf can be evaluated efficiently using fc( ) =  (c + 1,  )∕γ(c + 1), where  (c + 1,  ) = ∫ "
2011,1,['gamma'], Algorithms for Poisson Distribution,seg_391,∞ e−ttcdt is the incomplete gamma integral.
2012,1,"['poisson', 'power method', 'method', 'distribution', 'poisson distribution']", Algorithms for Poisson Distribution,seg_391,find the md of poisson distribution using the power method section 6.1 (p. 189). see figure 6.5.
2013,1,"['poisson', 'distribution', 'mean', 'poisson distribution', 'limit']", Algorithms for Poisson Distribution,seg_391,"solution 6.40 we know that the mean of poisson distribution is  . the lower limit ll is x = 0, so that xf(x) = 0. hence using theorem 6.1, the md is given by  −1 c"
2014,1,['table'], Algorithms for Poisson Distribution,seg_391,where c = ⌊  − 1⌋. see table 6.12.
2015,1,"['poisson', 'approximation', 'distribution', 'binomial', 'poisson distribution', 'binomial distribution']", Algorithms for Poisson Distribution,seg_391,"6.9.2.1 approximations the poisson distribution provides a good approximation to the binomial distribution b(n, p) when p is small, provided   = np   10, and n is large enough. the accuracy of this approximation increases as p tends to zero. as"
2016,1,"['rate', 'dependent']", Algorithms for Poisson Distribution,seg_391,"mentioned earlier, this limiting behavior is more dependent on the rate at which p → 0 faster than n → ∞ (see figure 6.7)."
2017,1,"['distribution', 'normal', 'variance']", Algorithms for Poisson Distribution,seg_391,"as the variance of the distribution is  , normal approximations are not appli-"
2018,1,"['normal', 'continuity correction']", Algorithms for Poisson Distribution,seg_391,  is approximately normal. the continuity correction can be applied as before to get
2019,1,"['transformation', 'variance']", Algorithms for Poisson Distribution,seg_391,p[(x;  )≤k] = pr[z≤ k− ±0.5 ]. the square root transformation is a variance stabiliz-
2020,1,"['observation', 'distribution', 'transformation']", Algorithms for Poisson Distribution,seg_391,"≥ ≥ √   ing transformation for this distribution. many approximations have appeared in the literature based on this observation. for example, the anscombe [147] approxima-"
2021,1,"['poisson', 'probabilities', 'normality', 'transformation']", Algorithms for Poisson Distribution,seg_391,"x +√ x + 1 transformation to normality suggested by freeman and tukey [148] . as the poisson left-tail areas are related to the  2 right-tail areas, the individual poisson probabilities can be approximated using the  2 probabilities."
2022,1,"['poisson', 'interval', 'uncertainty', 'distribution', 'probability', 'poisson distribution']", Algorithms for Poisson Distribution,seg_391,"6.9.2.2 applications the poisson distribution has been applied to various problems involving high uncertainty (low probability of occurrence). examples are the number of false fire alarms in a building, number of flaws in a sheet roll of newly manufactured fabric, number of phone calls received by a telephone operator in a fixed time interval, number of natural calamities such as earthquakes and tsunamis in a fixed time interval (say 1 month), number of epidemics in a locality, number of deaths due to a rare disease, and so on."
2023,0,[], Algorithms for Poisson Distribution,seg_391,example 6.41 structural damage
2024,1,"['probabilities', 'probability']", Algorithms for Poisson Distribution,seg_391,"a dam is built to withstand water pressure and mild tremors. let x denote the number of damages resulting from a major quake. if x is distributed as pois(0.008), find the following probabilities: (i) probability of no damage, (ii) probability of at least two damages, and (iii) probability of at most four damages."
2025,1,['probability'], Algorithms for Poisson Distribution,seg_391,"solution 6.41 the pdf is f (x,  ) = e−0.008(0.008)x∕x!, for x = 0, 1, 2 … . answer to (i) is p0 = e−0.008(0.008)0∕0! = e−0.008 = 0.9920. answer to (ii) is 1 − p(0) − p(1) = 1 − 0.992032 − 0.007936 = 1 − 0.99996817 = 3.18298e − 05. (iii) probability of at most four damages =∑x"
2026,1,"['poisson', 'discrete', 'distribution', 'poisson distribution']", Truncated Poisson Distribution,seg_393,"a useful distribution in epidemiological studies is a poisson distribution truncated at 0. it is also used in search engine optimization. assume that a user query returns a large number of matches that are displayed by a search engine in discrete screenfuls of say 10 matches each. then, the number of pages viewed by a surfer can be modeled as a zero-truncated poisson law or a zipf law [2]. the pdf is given by"
2027,1,"['poisson', 'rate', 'distribution', 'mean', 'poisson distribution', 'variance', 'dispersion']", Truncated Poisson Distribution,seg_393,"where the second expression is obtained from the first by multiplying the numerator and denominator by e . the mean and variance are  ∕(1 − e− ). see shanmugam [149]–[151] for incidence rate restricted poisson distribution, reference 152 for spinned poisson distribution, and reference 46 for a discussion on poisson dispersion."
2028,1,"['sample', 'without replacement', 'replacement']", HYPERGEOMETRIC DISTRIBUTION,seg_395,"consider a “lot” containing n items of which k are of one kind, and the rest (n − k) are of another kind. we assume that the two kinds are indistinguishable. suppose we sample n items without replacement from the lot. the number of items x of first kind is then given by"
2029,1,['parameters'], HYPERGEOMETRIC DISTRIBUTION,seg_395,"this is called the hgd, which has three parameters k,n and n. this can be derived using the following argument. as there are k items of one kind, we can choose x"
2030,0,['n'], HYPERGEOMETRIC DISTRIBUTION,seg_395,"x) ways. to make the count to n, we need to select further n − x"
2031,0,['n'], HYPERGEOMETRIC DISTRIBUTION,seg_395,"items. however, these can be selected from (n − k) items of second kind in (n"
2032,1,['product rule'], HYPERGEOMETRIC DISTRIBUTION,seg_395,"ways. using the product rule for selection (chapter 5), the total number of ways is the expression in the numerator of equation (6.136). to make it a pdf, we need to divide"
2033,0,['n'], HYPERGEOMETRIC DISTRIBUTION,seg_395,"it by the total number of ways to select n items, namely (n"
2034,1,"['independent', 'failures', 'successes', 'distribution', 'bernoulli', 'trials', 'bernoulli trials']", HYPERGEOMETRIC DISTRIBUTION,seg_395,"n ). we have not made any assumptions on the items being sampled. in practical applications of this distribution, it could be defective and nondefective items, marked and unmarked items, successes and failures in independent bernoulli trials, and so on."
2035,1,"['binomial coefficients', 'sampling', 'symmetry', 'binomial', 'coefficients']", HYPERGEOMETRIC DISTRIBUTION,seg_395,"as the expression involves binomial coefficients, there is a natural symmetry involved in the above-mentioned pdf. instead of sampling x items from the first kind, we could take x items from the second kind and (n − x) items from the first kind. this gives us the alternate pdf:"
2036,1,"['range', 'combination']", HYPERGEOMETRIC DISTRIBUTION,seg_395,"to impose the range for both these forms, we modify the range of x values as 0, 1, 2, … ,min(m,n − m, n). as all combination terms must exist, x varies between max(0, n + m − n) and min(m,n)."
2037,1,"['distribution', 'recurrence', 'parameters']", Properties of Hypergeometric Distribution,seg_397,"this distribution has three parameters, all of which are integers. the recurrence relation for the pdf is"
2038,1,['parameters'], Properties of Hypergeometric Distribution,seg_397,"as the parameters are all related, the following symmetries follow easily (i) h(x; k, n,n) = h(x; n, k,n), (ii) h(x; k, n,n) = h(k − x; k,n − n,n), and (iii) h(x; k, n,n) = h(n − x;n − k, n,n). replace x by x − 1 in equation (6.138) to"
2039,1,"['moment', 'factorial', 'moments', 'factorial moment', 'falling factorial']", Moments of Hypergeometric Distribution,seg_399,factorial moments are easier to find due to the x! in the denominator (of both the forms (6.136) and (6.137)). the rth falling factorial moment
2040,1,['summation'], Moments of Hypergeometric Distribution,seg_399,cancel out x(x − 1) … (x − r + 1) from x! in the denominator and write (k)x = k(k − 1) · · · (k − r + 1)(k)x−r and take it outside the summation. this
2041,0,[], Moments of Hypergeometric Distribution,seg_399,k). change the indexvar using u = x − r to
2042,0,[], Moments of Hypergeometric Distribution,seg_399,"using vandermonde’s identity (p. 6–6), this becomes"
2043,1,"['functions', 'hypergeometric functions', 'distribution', 'mean', 'hypergeometric', 'variance']", Moments of Hypergeometric Distribution,seg_399,"the mean is easily obtained from the above by putting r = 1 as nk∕n. the variance is (nk∕n)(1 − k∕n)(n − n)∕(n − 1). replace nk∕n on the rhs by   and write the multiplier as (1 − m∕n) ∗ [(n − n)∕(n − 1)]. this shows that  2     as both (1 − m∕n) and (n − n)∕(n − 1) are fractions. the mode of the distribution is ⌊(k + 1)(n + 1)∕(n + 2)⌋, which is greater than the mean. the mgf does not have simple form but is expressed in terms of hypergeometric functions as"
2044,1,"['covariance', 'coefficient', 'skewness', 'coefficient of skewness']", Moments of Hypergeometric Distribution,seg_399,"n−n covariance is given by cov(xi, xj) = npipj n−1 . the coefficient of skewness is as follows:"
2045,1,['table'], Moments of Hypergeometric Distribution,seg_399,see table 6.13 for more properties.
2046,1,"['independent', 'distribution', 'hypergeometric', 'random']", Moments of Hypergeometric Distribution,seg_399,"theorem 6.10 if x and y are independent bino(m, p) and bino(n, p) random variables, then the distribution of x|x + y = n is hypergeometric and is independent of p."
2047,1,"['conditional', 'distribution', 'random variable', 'variable', 'probability', 'random', 'conditional distribution']", Moments of Hypergeometric Distribution,seg_399,"proof: consider the random variable z = x + y . as the probability p is the same, this is distributed as bino(n + m, p). the conditional distribution of x given z = k is p[x = x|z = k] ="
2048,1,['independent'], Moments of Hypergeometric Distribution,seg_399,"as x and y are independent, x + y ∼ bino(m + n, p). hence, we get"
2049,0,[], Moments of Hypergeometric Distribution,seg_399,this reduces to (m
2050,1,['independent'], Moments of Hypergeometric Distribution,seg_399,n). this obviously is independent of p.
2051,1,"['poisson', 'probabilities', 'parameters', 'distribution', 'tail probabilities', 'binomial', 'probability', 'poisson distribution', 'binomial distribution', 'tail']", Approximations for Hypergeometric Distribution,seg_401,"hypergeometric probabilities can be approximated by the binomial distribution. when n and k are large, p = k∕n is not near 0 or 1, and n is small with respect to both k and n − k, the hgd is approximately a bino(n, k∕n). if k∕n is small and n is large, the probability can be approximated using a poisson distribution. closed-form expressions for tail probabilities do not exist, except for particular values of the parameters. however in general"
2052,1,['distribution'], NEGATIVE HYPERGEOMETRIC DISTRIBUTION,seg_403,this distribution is also called markov–polya distribution. the pdf is given by
2053,1,"['case', 'distribution', 'binomial', 'mean', 'binomial distribution', 'variance']", NEGATIVE HYPERGEOMETRIC DISTRIBUTION,seg_403,"for x = 0, 1, 2, … , n where a, b, n are integers. the mean and variance are given by   = e(x) = an∕(a + b), variance =  2 = abn(a + b + n)∕[(a + b)2(a + b + 1)]. replace an/(a + b) on the rhs by  , we get  2 =   ∗ b(a + b + n)∕[(a + b)(a + b + 1)]. write the rhs as   ∗[b/(a+b)*(a+b+n)/(a+b+1)]. beta binomial distribution discussed in the following is a special case when k and (n − k) are integers."
2054,1,"['conditional', 'distribution', 'random variable', 'variable', 'success', 'probability of success', 'binomial', 'probability', 'random', 'binomial distribution', 'conditional distribution']", BETA BINOMIAL DISTRIBUTION,seg_405,"this distribution can be obtained as the conditional distribution of binomial distribution in which the probability of success is distributed according to the beta law. consider the binomial distributed random variable with pdf bx(n, p) ="
2055,1,"['range', 'continuous random variable', 'random variable', 'variable', 'random', 'continuous']", BETA BINOMIAL DISTRIBUTION,seg_405,"xr)0 ≤ p ≤ 1. as p is a continuous random variable in the range (0,1), we obtain"
2056,1,"['distribution', 'joint', 'probability', 'joint probability']", BETA BINOMIAL DISTRIBUTION,seg_405,the unconditional distribution of x by integrating out p from the joint probability
2057,1,['distribution'], BETA BINOMIAL DISTRIBUTION,seg_405,"γ(a)γ(b) distribution and using the expansion b(a, b) = as"
2058,1,"['bayesian', 'conditional expectation', 'conditional', 'expectation', 'mean']", BETA BINOMIAL DISTRIBUTION,seg_405,"this form is widely used in bayesian analysis. the mean   is most easily obtained from the conditional expectation as e(x) = e[e(x|p)] = ne(p) = na∕(a + b) = np,"
2059,1,"['moment ', 'moment']", BETA BINOMIAL DISTRIBUTION,seg_405,a where p = (a+b) . the second raw moment  2
2060,1,['variance'], BETA BINOMIAL DISTRIBUTION,seg_405,which the variance follows as  2 = npq + n(n−1)pq where q = 1 − p. this can also
2061,1,"['bayesian', 'table', 'trials', 'parameter']", BETA BINOMIAL DISTRIBUTION,seg_405,"a+b+1 be written as npq(a + b + n)∕(a + b + 1). in bayesian analysis, this is written as n (1 − )[1 + (n − 1) ], where = p and = 1∕(a + b + 1) is the pairwise correlation between the trials called overdispersion parameter. this form is obtained from the previous one by writing a + b + n as a + b + 1 + (n − 1) and dividing by the denominator a + b + 1 (see table 6.14)."
2062,0,[], BETA BINOMIAL DISTRIBUTION,seg_405,see reference 153 for properties and generalizations.
2063,1,"['table', 'case', 'negative binomial distribution', 'distribution', 'negative binomial', 'binomial', 'parameter', 'binomial distribution']", LOGARITHMIC SERIES DISTRIBUTION,seg_407,this is a special case of the left-truncated negative binomial distribution where the zero class has been omitted and the parameter k tends to one (table 6.14).
2064,1,"['continuous', 'discrete', 'distribution', 'function', 'discrete distribution']", LOGARITHMIC SERIES DISTRIBUTION,seg_407,"although log() is a continuous function, this is a discrete distribution with infinite support. it has pdf"
2065,0,[], LOGARITHMIC SERIES DISTRIBUTION,seg_407,an alternate parametrization is as
2066,1,"['model', 'logarithmic distribution', 'moment', 'factorial', 'distribution', 'moments', 'mean', 'parameter', 'factorial moment', 'factorial moments', 'variance']", Properties of Logarithmic Distribution,seg_409,"the logarithmic distribution has a single parameter p. the mean is = q∕[−plog(p)]. as 0 p 1, log(p) is negative, thereby canceling out negative sign. variance is 2 = −q(q + log(p))∕[(p log(p))2]. in terms of , this is 2 = (1∕p − ) or equivalently 2 + 2 = ∕p. cross-multiply to get p = ∕( 2 + 2). this shows that the variance is less than the mean for (1∕p − ) 1 or equivalently p 1∕(1 + ). for the alternate representation (6.13), the mean is = a ∕(1 − ) and variance is (1 − a )∕(1 − ). to fit the model, compute x and s2 and find p̂ = x∕(x2 + s2). as the variance is +ve, q − log(p). the factorial moments are easier to find than central moments. the kth factorial moment is given by"
2067,1,"['distribution', 'table']", Properties of Logarithmic Distribution,seg_409,"the chf is given by  (t) = ln (1 − qeit)∕ ln (1 − q). as the values assumed by x are integers, it is used in those modeling situations involving counts. for instance, the number of items of a product purchased by a customer in a given period of time can be modeled by this distribution. see table 6.15 for more properties."
2068,1,"['probabilities', 'distribution', 'binomial', 'binomial distribution', 'categories']", MULTINOMIAL DISTRIBUTION,seg_411,this distribution can be considered as a generalization of the binomial distribution with n( 2) categories. the corresponding probabilities are denoted as pi for the ith
2069,1,"['distribution', 'multinomial distribution', 'multinomial']", MULTINOMIAL DISTRIBUTION,seg_411,"n =1 pi = 1. we denote it by mn(n, p1, p2, … , pn). the pdf of a general multinomial distribution with k classes is"
2070,1,['product notation'], MULTINOMIAL DISTRIBUTION,seg_411,"where x1 + x2 + … + xk = n and p1 + p2 + … + pk = 1. using the product notation introduced in chapter 1, this can be written as (n!∕∏i"
2071,0,['n'], MULTINOMIAL DISTRIBUTION,seg_411,i . this can n also be written as (
2072,1,"['coefficient', 'multinomial']", MULTINOMIAL DISTRIBUTION,seg_411,"x1, x2, …, xk ), which is called the multinomial coefficient. as the"
2073,1,['parameters'], MULTINOMIAL DISTRIBUTION,seg_411,"k =1 pi = 1, there are k parameters. for k = 2, this reduces to bino(n, p)."
2074,1,"['poisson', 'case', 'distribution', 'binomial', 'poisson distribution', 'binomial distribution']", MULTINOMIAL DISTRIBUTION,seg_411,"as in the case of binomial distribution, we could show that this distribution tends to the multivariate poisson distribution:"
2075,1,"['distribution', 'multinomial distribution', 'multinomial']", MULTINOMIAL DISTRIBUTION,seg_411,"theorem 6.11 if n is large and pi is small such that npi =  i remains a constant, the multinomial distribution approaches e−( 1+ 2+···+ k) n"
2076,1,"['multinomial', 'case', 'distribution', 'binomial', 'binomial distribution']", MULTINOMIAL DISTRIBUTION,seg_411,"proof: the easiest way to prove this result is using pgf. as in the case of binomial distribution, it is easy to derive the pgf of multinomial as (p1t1 + p2t2 + · · · + pktk)n. now proceed as done in section 6.5.8."
2077,1,"['correlation', 'distribution', 'covariance', 'binomial', 'binomial distribution']", Properties of Multinomial Distribution,seg_413,"for each class, the means can be obtained using binomial distribution as e(xi) = npi, var(xi) = npiqi, and cov(xi,xj) = −npipj. as the covariance is negative, so is the correlation. this is because when one of them increases, the other must decrease due to the sum constraint on the xi"
2078,1,['table'], Properties of Multinomial Distribution,seg_413,′s. the chf is given by  (t) = [1 +∑m j=1 pj(eitj − 1)]n. see table 6.16 for more properties.
2079,1,"['probabilities', 'marginal', 'conditional', 'conditional distributions', 'distribution', 'marginal distributions', 'observation', 'binomial', 'marginal distribution', 'distributions']", Properties of Multinomial Distribution,seg_413,6.14.1.1 marginal and conditional distributions the marginal distributions are binomial that follows easily from the observation that the probabilities are obtained as terms in the expansion of (p1 + p2 + · · · + pn)n . if marginal distribution of xj is
2080,1,"['missing data', 'multinomial', 'parameters', 'multinomial distribution', 'conditional', 'distribution', 'data', 'variates', 'binomial', 'probability', 'conditional distribution', 'distributions']", Properties of Multinomial Distribution,seg_413,"conditional distributions of multinomials are more important as these are used in the expectation–maximization algorithms (emas) [22]. let xn be a multinomial distribution with k classes defined earlier. suppose we have missing data in an experiment. for convenience, we assume that the first j components are observed, and j + 1 through k classes have missing data (unobserved). to derive the ema for this type of problems, one needs to find the conditional distribution of x|observed variates. the conditional distribution of xi given xj = nj is binomial with parameters n − nj and probability pi∕(1 − pj)."
2081,1,['probabilities'], Properties of Multinomial Distribution,seg_413,"as xj+1,xj+2, … ,xk are unobserved with respective probabilities pj+1, pj+2, … , pk, we write it using p(a|b) = p(a∩b)/p(b) as"
2082,1,"['independence', 'trials']", Properties of Multinomial Distribution,seg_413,"owing to the independence of the trials, this becomes"
2083,1,"['distribution', 'multinomial distribution', 'multinomial']", Properties of Multinomial Distribution,seg_413,"canceling out common terms, this can be simplified to a multinomial distribution. see reference 154 for the mode of multinomial distribution,"
2084,0,[], Properties of Multinomial Distribution,seg_413,example 6.42 human blood groups
2085,1,['probability'], Properties of Multinomial Distribution,seg_413,"consider the human blood groups example in chapter 5. suppose that the percentage of people with the blood groups {a, b, o, and ab} are 40, 12, 5, and 43, respectively. find the probability that (i) in a group of 60 students, 30 or more are of blood group “a” and (ii) at least 4 persons have blood group o."
2086,1,"['frequency', 'probability']", Properties of Multinomial Distribution,seg_413,"solution 6.42 using the frequency approach, we expect the probability of any person with blood group “a” as 0.40. denote this as p1 = 0.40. similarly p2 = 0.12, p3 = 0.05, and p4 = 0.43. this gives the pdf as"
2087,1,"['marginal', 'distribution', 'binomial', 'probability', 'marginal distribution']", Properties of Multinomial Distribution,seg_413,"0 30 bino(60, 0.40), as the marginal distribution is binomial. (ii) probability of “o” blood group is 5/100 = 1/20. thus, the answer is 1−∑i"
2088,0,[], Properties of Multinomial Distribution,seg_413,"3 =0 bino(60, 1∕20) ="
2089,1,"['poisson', 'data', 'discrete', 'discrete distributions', 'parameter', 'probabilistic', 'continuous', 'geometric', 'probabilistic modeling', 'distributions']", SUMMARY,seg_415,"the collected data are either count or continuous number type. several important discrete distributions encountered in probabilistic modeling are discussed and summarized in this chapter. some of these are used in subsequent chapters. sometimes, there are competing models (such as poisson with small , geometric or logarithmic distributions that have striking similarities for some parameter values)."
2090,1,"['negative binomial', 'probability', 'random sample', 'random', 'binomial distribution', 'geometric', 'process', 'sample', 'bernoulli distribution', 'quality control', 'distribution', 'binomial', 'control', 'outcomes', 'negative binomial distribution', 'bernoulli', 'probability distribution', 'geometric probability']", SUMMARY,seg_415,"a manufactured item might meet engineering specifications (to be a quality item) or is a defective item otherwise. in a quality control inspection of sample items, the outcomes with respect to a specific item follow a bernoulli distribution. the number of quality items in a random sample of n inspected items follows a binomial distribution. if the inspection of items is done until a defective item is encountered, then the number of items inspected until the termination of the inspection follows a geometric probability distribution. if a modification in the inspection process is made such that the inspection of items is continued until an accumulation of a specified number r of defective items, then the number of inspected items follows a negative binomial distribution."
2091,1,"['deviation', 'power method', 'probabilities', 'method', 'distribution', 'tail probabilities', 'mean', 'binomial', 'binomial distribution', 'tail', 'distributions']", SUMMARY,seg_415,students and professionals are often interested in the tail probabilities of these distributions and approximations of it for power calculations. these tail probabilities can be obtained in closed form for some of the distributions. this in turn provides an alternative method to compute the mean deviation using the power method introduced in section 6.3 (p. 6–6). several researchers have extended abraham de moivre’s 1730 [130] result on the md of a binomial distribution. the notable ones being by bertrand
2092,1,"['deviation', 'sample', 'probabilities', 'sample mean', 'distribution', 'mean']", SUMMARY,seg_415,"[131], frisch [155], kamat [156], winkler [157], diaconis and zabell [132], jogesh babu and rao [158], pham-gia et al. [159], pham-gia and hung [160] (who also derives the distribution of sample mean absolute deviation), egorychev et al. [161], and so on. in section 6.3 (p. 6–6), we have provided a greatly simplified expression involving either the cdf (left-tail probabilities) or the sf (right-tail probabilities) when the mean   is an integer or half integer."
2093,1,"['distribution', 'binomial', 'mean', 'binomial distribution', 'variance']", SUMMARY,seg_415,a) for the binomial distribution mean is   variance
2094,1,"['distribution', 'variance', 'bernoulli', 'bernoulli distribution']", SUMMARY,seg_415,b) variance of a bernoulli distribution lies between 0 and 1/4
2095,1,"['poisson', 'distribution', 'bimodal', 'poisson distribution']", SUMMARY,seg_415,c) poisson distribution is bimodal if   is noninteger
2096,1,"['poisson', 'distribution', 'poisson distribution']", SUMMARY,seg_415,d) poisson distribution satisfies the memory-less property
2097,1,"['distribution', 'range', 'geometric', 'geometric distribution']", SUMMARY,seg_415,e) geometric distribution has infinite range
2098,1,"['distribution', 'variance', 'mean']", SUMMARY,seg_415,f) mean of a negative-binomial distribution is always greater than the variance
2099,1,['variance'], SUMMARY,seg_415,"g) the variance of bino(n, p) and bino(n, q) are the same"
2100,1,"['discrete', 'distributions']", SUMMARY,seg_415,h) truncated discrete uniform distributions are of the same type.
2101,1,"['range', 'discrete', 'negative binomial', 'binomial', 'hypergeometric', 'distributions']", SUMMARY,seg_415,6.2 which of the following distributions have infinite range? (a) binomial (b) negative binomial (c) discrete uniform (d) hypergeometric
2102,1,"['variables', 'mean', 'variance']", SUMMARY,seg_415,"6.3 if the mean and variance of a binorandom variables, find the prob-"
2103,1,['distribution'], SUMMARY,seg_415,"mial distribution are equal, what is ability of each of the following:"
2104,1,"['negative binomial distribution', 'distribution', 'negative binomial', 'mean', 'binomial', 'binomial distribution', 'variance']", SUMMARY,seg_415,p[x   y] (d) p[x|x + y = (n + 6.4 if the mean and variance of a 1)] = 1∕n. negative binomial distribution are
2105,1,['binomial'], SUMMARY,seg_415,"equal, what is the value of p? 6.9 using the binomial expan-"
2106,1,['distribution'], SUMMARY,seg_415,6.6 for which distribution is
2107,0,[], SUMMARY,seg_415,"f (x)∕f (x + k) = 1 for all k in the 6.10 if x is bino(n, p) prove that"
2108,1,['binomial'], SUMMARY,seg_415,"range?. e(x∕n) = p, and var(x∕n) = 6.7 prove that the binomial distribupq∕n."
2109,1,"['distribution', 'variance', 'mean']", SUMMARY,seg_415,"tion attains its maximum at k = 6.11 find the mean and variance of the ⌊(n + 1)p⌋. if k is an integer, then distribution discussed in section there are two maxima at k = (n + 6.5.4, p. 204. 1)p and k = (n + 1)p − 1 = np +"
2110,1,"['distribution', 'geometric']", SUMMARY,seg_415,"binomial distribution nb(k, p) is 6.8 if x and y are independently and identically distributed geometric ⌊(q∕p)(k − 1)⌋."
2111,1,"['independent', 'probabilities', 'tail probabilities', 'binomial', 'tail']", SUMMARY,seg_415,"6.13 if x1,x2, … ,xn are independent 6.23 find tail probabilities of binomial"
2112,1,"['incomplete beta', 'random variables', 'beta function', 'variables', 'incomplete beta function', 'distribution', 'random', 'function', 'distributions']", SUMMARY,seg_415,"ber(p) random variables, find distributions using incomplete beta function. (i) pr[b(10, 0.4) ≤ the distribution of u = ∏i"
2113,1,['variance'], SUMMARY,seg_415,"n =1(1 + 6], (ii) pr[b(22, 0.7) ≥ 18], (iii) kxi), where k is a constant. pr[b(40, 0.2) ≥ 35]. 6.14 prove that variance of duni(n) is"
2114,1,"['transformation', 'change of origin', 'discrete']", SUMMARY,seg_415, 2 =  (  − 1)∕3 where   = (n + 6.24 prove that a change of origin transformation simply displaces 1)∕2. hence or otherwise show the discrete uniform distribu-
2115,1,['variance'], SUMMARY,seg_415,that the variance is greater than the tion to the left or right with
2116,1,['coefficient'], SUMMARY,seg_415,"6.15 show that the coefficient of varia + 1, a + 2, … , a + n."
2117,1,"['discrete', 'binomial', 'distributions']", SUMMARY,seg_415,ation (cv) of a binomial distribu6.25 which of the following discrete tion is cv =√q∕(np). distributions is always leptokur-
2118,1,"['moment ', 'moment']", SUMMARY,seg_415,6.16 show that the third moment  3 tic?
2119,1,"['poisson', 'distribution', 'binomial', 'binomial distribution']", SUMMARY,seg_415,of a binomial distribution is npq (a) binomial (b) poisson (c)
2120,1,"['geometric', 'discrete']", SUMMARY,seg_415,(q − p). geometric (d) discrete uniform.
2121,1,"['discrete', 'distribution', 'discrete distribution', 'variance']", SUMMARY,seg_415,6.17 prove that the variance of a bino6.26 for which discrete distribution is
2122,1,"['discrete', 'distribution', 'covariance', 'binomial', 'mean', 'variance', 'geometric']", SUMMARY,seg_415,"mial distribution is always less the variance always greater than than the mean. the mean? (a) binomial (b) poisson (c) geometric (d) discrete 6.18 find the covariance cov(xi,xj) uniform."
2123,1,"['poisson', 'multinomial', 'approximation', 'multinomial distribution', 'distribution', 'binomial', 'skewness']", SUMMARY,seg_415,for multinomial distribution. why 6.27 truncation never changes the is it negative?. skewness of which distribution? 6.19 can the poisson approximation to (a) binomial (b) poisson (c)
2124,1,"['geometric', 'discrete']", SUMMARY,seg_415,"bino(n, p) be used when n is geometric (d) discrete uniform."
2125,1,['discrete'], SUMMARY,seg_415,"small? (say n   15)?. if so, under 6.28 which of the following discrete"
2126,1,"['symmetric', 'distributions']", SUMMARY,seg_415,what conditions?. distributions is never symmetric?
2127,1,"['poisson', 'binomial']", SUMMARY,seg_415,"6.20 prove that the nb(k, p) with pdf (a) binomial (b) poisson (c) geo-"
2128,1,['hypergeometric'], SUMMARY,seg_415,−1) pkqx can be obtained metric (d) hypergeometric.
2129,1,"['poisson', 'discrete', 'discrete distributions', 'mean', 'binomial', 'variance', 'distributions']", SUMMARY,seg_415,following discrete distributions? 6.21 the mean and variance can never (a) binomial (b) poisson (c) geo-
2130,1,['distributions'], SUMMARY,seg_415,be equal for which of the folmetric (d) uniform. lowing distributions? (a) bino-
2131,1,"['poisson', 'probabilities', 'distribution', 'negative binomial', 'binomial', 'geometric']", SUMMARY,seg_415,mial (b) poisson (c) geometric 6.30 for which distribution does the (d) negative binomial. individual probabilities (divided
2132,1,"['geometric', 'probability']", SUMMARY,seg_415,by the first probability p) form a 6.22 what is the probability that a fair geometric progression?.
2133,0,[], SUMMARY,seg_415,coin need to be flipped ten times to
2134,1,['normal'], SUMMARY,seg_415,"get the 5th head on 10th flip? 9th 6.31 if x∼ pois( ), prove that log(x) head on 10th flip?. is approximately normal with"
2135,1,"['poisson', 'variance', 'probability']", SUMMARY,seg_415,"mean log( ) as → ∞. what is 6.39 if x ∼ poisson( ), find the probathe variance?. bility that (i) x takes even values, (ii) x takes odd values. 6.32 prove that the probability generat-"
2136,1,['function'], SUMMARY,seg_415,"ing function (pgf) of duni(n) is 6.40 if x ∼ geometric(p) find the probpx(t) = (1 − tn)∕[n(1 − t)]. ability that (i) x takes even values,"
2137,0,[], SUMMARY,seg_415,"(ii) x takes odd values. 6.33 prove that bino(n,  ∕n) →"
2138,0,[], SUMMARY,seg_415,"pois( ) as n → ∞. what about 6.41 let x ∼ binomial(n, p) and yk ∼ limiting behavior of bino(n, 1 − negative binomial(k, p). prove"
2139,1,['binomial'], SUMMARY,seg_415,6.34 describe how you can approxi6.42 find x such that the binomial
2140,1,"['poisson', 'probabilities', 'distribution', 'binomial', 'poisson distribution']", SUMMARY,seg_415,"mate binomial probabilities using left-tail probabilities (say ) are (i) a poisson distribution when p is b(20, 0.8) with = 0.41145 (ii) not so small, but is near 1 (say b(10, 0.7) with = 0.38278?. 0.98). 6.43 if x ∼ binomial(n, p), find the 6.35 find the mode of negative binocovariance of (x∕n, (n − x)∕n)."
2141,1,"['discrete', 'distribution', 'mean', 'discrete distribution', 'variance', 'geometric']", SUMMARY,seg_415,"mial nbino(k, p). show that 2 = (1 + ∕k). 6.44 obtain the mean and variance of zero-truncated geometric distribu6.36 if x is a discrete distribution such tion."
2142,1,"['uniform distribution', 'discrete', 'distribution', 'skewness']", SUMMARY,seg_415,"that p(x = a) = p and p(x = b) = 1 − p, find distribution of (x − 6.45 what is the skewness of a discrete a)∕(b − a). uniform distribution? show that it"
2143,0,[], SUMMARY,seg_415,is always platykurtic. 6.37 if x ∼ geometric(p) find the dis-
2144,0,[], SUMMARY,seg_415,tribution of y = exp(x). 6.46 if x ∼ geometric(p) with pdf
2145,1,['discrete'], SUMMARY,seg_415,"f (x;  ) = for x = 0, 1, 6.38 if x is discrete with support 1,2,..."
2146,1,['variance'], SUMMARY,seg_415,and variance.
2147,1,"['distribution', 'results']", SUMMARY,seg_415,6.47 prove that truncating a duni(n) distribution results in another distribution
2148,1,"['distribution', 'tails']", SUMMARY,seg_415,"of the same type as follows. if the truncation is at a single point on either extremes, the new distribution is duni(n − 1). if the truncation is at both tails (one point each), the resulting distribution is duni(n − 2). if k points are truncated at both tails, the new distribution is duni(n − 2k)."
2149,1,['mean'], SUMMARY,seg_415,"6.48 if x ∼ bino(n, p) where p → 1 for x = 0, 1, 2, … find the mean"
2150,1,['variance'], SUMMARY,seg_415,"from below, find the limiting disand variance."
2151,1,"['variance', 'mean']", SUMMARY,seg_415,tribution of x when n → ∞ and nq 6.51 find the mean and variance of
2152,0,[], SUMMARY,seg_415,"remains a constant. truncated nbino(k, p) distribu-"
2153,1,"['skewness coefficient', 'skewness', 'coefficient']", SUMMARY,seg_415,6.49 obtain the skewness coefficient of
2154,1,"['distribution', 'symmetric']", SUMMARY,seg_415,"geometric distribution and argue ( q p)x, x = 1, 2, 3 … . that it is never symmetric."
2155,1,"['distribution', 'binomial distribution', 'binomial']", SUMMARY,seg_415,with pdf f (x;  ) = e− (1 − e− )x binomial distribution satisfies the
2156,1,"['independent', 'distribution', 'bernoulli', 'success', 'trials', 'bernoulli trials', 'probability of success', 'probability']", SUMMARY,seg_415,"recurrence (1 + p)(k + 1)fk+1(x) = independent bernoulli trials with p(n + k)fk(x), k = 1, 2, … the same probability of success p. find the distribution of u = (|x − 6.53 prove that the hypergeomety| + 1)∕2 for n odd."
2157,1,['distribution'], SUMMARY,seg_415,ric distribution (k x)(n n−
2158,1,"['sample', 'binomial', 'random']", SUMMARY,seg_415,"k) ∕(n n ) 6.55 an urn contains 10 red and 6 blue tends to the binomial distribuballs. a sample of five balls is tion bino(n, p) where p = k∕n selected at random. let y denote"
2159,0,['n'], SUMMARY,seg_415,as n → ∞. the number of red balls in the sam-
2160,1,"['function', 'density function']", SUMMARY,seg_415,6.54 let x denote the number of sucple. find the density function of
2161,1,"['failures', 'with replacement', 'without replacement', 'replacement', 'sampling', 'tails']", SUMMARY,seg_415,cesses (or heads) and y denote the y if sampling is with replacement number of failures (or tails) in n and without replacement.
2162,0,[], SUMMARY,seg_415,"6.56 a company does tele marketing to sell its products. three tele-operators x, y,"
2163,1,"['rate', 'standard deviations', 'deviations', 'success', 'probability', 'standard', 'average']", SUMMARY,seg_415,"z contact customers over the telephone and explains the company’s products to them to get possible orders. the average success rate in selling at least one item out of 100 tele-contacts is 12 for x, 7 for y, and 3 for z with respective standard deviations 5, 2, and 1. find the probability for each of the following: (a) x is able to get 20 or more sales orders out of 300 customers and (b) y and z together gets 30–60 orders."
2164,1,"['discrete', 'discrete distributions', 'random', 'distributions']", SUMMARY,seg_415,6.57 describe how to generate random 6.60 for which discrete distributions is
2165,1,"['negative binomial', 'binomial', 'mean', 'variance']", SUMMARY,seg_415,the variance always less than the numbers from negative binomial mean?
2166,1,['random'], SUMMARY,seg_415,distribution using a random num-
2167,1,"['geometric', 'independent']", SUMMARY,seg_415,"ber generator for geometric distri6.61 if x1,x2, … ,xm are independent bution. ber(pi), where each pi is either equal to p or equal to q, find the"
2168,1,"['random variables', 'independent', 'variables', 'random']", SUMMARY,seg_415,"geometric random variables, find 6.62 if x and y are independent"
2169,1,"['distribution', 'variance', 'mean']", SUMMARY,seg_415,"bino(n, p) find the distribution of the distribution of zi = min(x1, x|x + y = n. find its mean and x2, … ,xn). variance."
2170,1,"['variance', 'discrete']", SUMMARY,seg_415,6.59 which of the discrete distribu6.63 prove that the variance of discrete
2171,1,"['distribution', 'uniform distribution']", SUMMARY,seg_415,tions satisfy: p(x ≥ s + t)|p(x ≥ uniform distribution is greater
2172,1,['mean'], SUMMARY,seg_415,s) = p(x ≥ t). than the mean for n   7.
2173,1,"['variance', 'mean']", SUMMARY,seg_415,6.64 find mean and variance of
2174,1,"['deviation', 'mean']", SUMMARY,seg_415,6.65 find the pdf and cdf of the 6.66 find the mean deviation from the
2175,1,"['uniform distribution', 'geometric distribution', 'distribution', 'mean', 'geometric']", SUMMARY,seg_415,discrete uniform distribution when mean of a geometric distribution
2176,1,['variance'], SUMMARY,seg_415,"obtain the pgf, and the variance. x = 1, … ."
2177,1,['multinomial'], SUMMARY,seg_415,"6.67 if x is distributed as bino(n, p) 6.78 prove that in a multinomial dis-"
2178,1,"['variance', 'standard']", SUMMARY,seg_415,"the variance of (i) standard corr(xi, xj) = −( 1−"
2179,1,"['distribution', 'independent', 'bernoulli']", SUMMARY,seg_415,uniform distribution? (ii) trun6.79 describe independent bernoulli
2180,1,"['distribution', 'uniform distribution', 'trials']", SUMMARY,seg_415,cated uniform distribution? trials. if u = |x − y|∕2 where x
2181,1,"['failures', 'distribution', 'success', 'variance']", SUMMARY,seg_415,"and y are the number of success 6.69 prove that the maximum variance and failures, what is distribution of"
2182,1,"['bernoulli', 'random variable', 'variable', 'binomial', 'random', 'bernoulli random variable']", SUMMARY,seg_415,of a bernoulli random variable is u if p = 1∕2? 1/4 and that of a binomial distri-
2183,1,"['negative binomial distribution', 'distribution', 'negative binomial', 'binomial', 'binomial distribution']", SUMMARY,seg_415,bution is n∕4. 6.80 if a negative binomial distribution
2184,1,"['trials', 'successes']", SUMMARY,seg_415,is defined as the number of trials 6.70 for which of the following valneeded to produce k successes in n
2185,1,"['moment', 'moment recurrence', 'trials', 'mean', 'tails', 'geometric', 'recurrence']", SUMMARY,seg_415,"ues of p does the geometric distribution qxp tails off slowly? (a) trials, obtain a moment recurrence and find the mean. p = 0.1 (b) p = 0.5 (c) p = 0.8 (d)"
2186,0,[], SUMMARY,seg_415,p = 0.9 6.81 if x ∼ geometric(p) find the prob-
2187,1,['mean'], SUMMARY,seg_415,ability that x   ⌈| log(p)|⌉ 6.71 obtain an expression for the mean
2188,1,"['incomplete beta', 'distribution', 'negative binomial', 'binomial', 'function']", SUMMARY,seg_415,"deviation of a negative binomial 6.82 if x and y are iid geo(p), show distribution using incomplete beta p[x − y = 0] = p2∕(1 − q2) = function. p∕(1 + q)."
2189,1,['mean'], SUMMARY,seg_415,"6.72 prove that the mean devia6.83 if x ∼ bino(n, p) find the distri-"
2190,1,"['poisson', 'distribution', 'mean', 'poisson distribution']", SUMMARY,seg_415,tion from the mean of the bution of y = (x − np)2. find its poisson distribution is 2 ∗ mean and mode. exp(− ) ⌊ ⌋+1∕⌊ ⌋!.
2191,1,"['moments', 'probability']", SUMMARY,seg_415,"6.84 if x ∼ nbino(k, p) find a recur6.73 what is the probability distrirence relation for moments using"
2192,1,"['variance', 'failures']", SUMMARY,seg_415,"bution of n − x failures in a f (x + 1, k, p) = q ∗ f (x, k, p) ∗ bino(n, p)? what is its variance? (x + k)∕(x + 1)."
2193,1,['probability'], SUMMARY,seg_415,"6.74 if xi, i = 1, 2, … , r are iid 6.85 probability that high-rise struc-"
2194,1,"['random variables', 'variables', 'random']", SUMMARY,seg_415,"geo(p) random variables, prove r tures in a city center will damage that ∑i=1 xi ∼ nbin(r, p). x other adjacent buildings after an"
2195,1,['probability'], SUMMARY,seg_415,"distribution of (i) x|(x + y = 2n), the probability that it will dam(ii) x|(x − y = n). age three or more buildings in the"
2196,1,['variance'], SUMMARY,seg_415,vicinity. 6.76 what is the variance of limiting
2197,1,"['distribution', 'variance', 'logarithmic distribution', 'mean']", SUMMARY,seg_415,binomial variate with fixed n when 6.86 if the mean and variance of a p → 0 from above or q → 1 from logarithmic distribution are   =
2198,1,"['distribution', 'mean', 'data']", SUMMARY,seg_415," ∕( 2 +  2). how can this be used distribution of u = |x − y|, and to fit the distribution to data? its mean."
2199,0,[], SUMMARY,seg_415,6.87 wavelength w (in nanometer) and stopping potential v of a photo-electric sur-
2200,1,"['distribution', 'logarithmic distribution', 'mean', 'table']", SUMMARY,seg_415,face are given in table 6.17. fit a logarithmic distribution and obtain the mean.
2201,1,['mean'], SUMMARY,seg_415,6.88 it is given that the mean of a binox (n−
2202,1,"['power method', 'method', 'distribution', 'success', 'probability', 'variance']", SUMMARY,seg_415,"x) 6.96 check if f (x) = k ∗ (−1) 2 mial distribution is 6 and variance cos (c)n−2x is a pdf for x = is 4.8. what is the probability of 0, 1, … ⌊n∕2⌋. success p? what is n? 6.97 use the power method introduced 6.89 for which distribution does the in section 6.3, page 6–6 to find the"
2203,1,"['deviation', 'distribution', 'mean', 'transformation']", SUMMARY,seg_415,change of origin transformation mean deviation of truncated binoy = x + 1 and left truncation at mial distribution (truncated at x = x = 0 result in the same law? 0).
2204,1,"['power method', 'method']", SUMMARY,seg_415,"6.90 prove that bino(n, p) → pois( ) 6.98 use the power method introduced"
2205,1,"['deviation', 'approximation', 'distribution', 'mean']", SUMMARY,seg_415,"when n → ∞ and p2 → 0 and np in section 6.3, page 6–6 to find the remains constant. explain how to mean deviation of truncated poisson distribution (truncated at x = use this approximation when p → 0). 1."
2206,1,['probability'], SUMMARY,seg_415,"high-rise building uses pipes from for x = 1, 2, … is a pdf for k = two sources. the probability of"
2207,0,['n'], SUMMARY,seg_415,1∕  cot( n) becoming defective in 2 years
2208,1,['poisson'], SUMMARY,seg_415,6.92 is f (x) = k∕(n + x)2 for x = after installation is poisson dis-
2209,1,"['geometric', 'probability']", SUMMARY,seg_415,"integer ∈ (−∞,∞), a pdf where tributed with = 0.003 for both. k = 2∕sin2( n)? if 120 pipes from source-one and 185 pipes from source-two are 6.93 prove that the geometric disused, what is the probability that"
2210,1,['probability'], SUMMARY,seg_415,tribution f (x; p) = qx−1p can be at least one pipe will leak in 2 obtained from f (x; p) = qxp by years? what is the probability that truncation at x = 0. how are the at most 2 will leak?
2211,1,['variances'], SUMMARY,seg_415,means and variances related? 6.100 a dyeing plant uses six steam
2212,1,"['success', 'probability of success', 'probability']", SUMMARY,seg_415,6.94 the probability of success p of boilers. the probability of any
2213,1,"['poisson', 'poisson distributed']", SUMMARY,seg_415,"boiler exploding in 6 years time a bino(n, p) is given by a root of the quadratic equation x2 − x + is poisson distributed with = cd 1∕10, 000. what is the probabil= 0. find . what are the ity that none of the boilers will c2o"
2214,1,['probability'], SUMMARY,seg_415,x) is a boilers with probability of explod-
2215,1,['probability'], SUMMARY,seg_415,"the probability that at most two 6.102 a newly manufactured micro-chip boilers explode during 6 years is known to have 0.0001 probabilfrom the time of installation. ity of failing on any whole day (24 hours). find the probability that (i) 6.101 assume that a dam is built to it will last at least 60 days, (ii) it"
2216,1,['independent'], SUMMARY,seg_415,"hold 50,000 cubic feet of water. will last between 50 and 100 days, the exceedence in any year has a and (iii) it will last at most 90 days pois(0.018). if the peak flow is (hint: use geo(0.0001)). independent yearly, find the probability of (i) at least three exceedences in a year and (ii) exactly two exceedences in a year."
2217,0,[], SUMMARY,seg_415,6.103 a highway-patrolman is looking for speeding vehicles. it is known from past
2218,1,"['set', 'limit', 'random']", SUMMARY,seg_415,"data that the total number of vehicles y going beyond the set speed limit on a stretch of a highway during busy hours is n ∗ f (x; p), where n = total number of vehicles passed during the time period and x has a logarithmic distribution with f (x; p) = qx∕[−x log p] with p = 0.5 and x = 1, 2, … represents the difference in speed in miles per hour beyond the permitted limit (x = (actual speed-speed-limit) in miles per hour to nearest int). if 200 vehicles go above the speed limit in 1 hour, approximately how many vehicles are in the grace bracket x ≤ 5? approximately how many vehicles are ≥ 10 mph above the speed limit?. if an over-speed vehicle is stopped at random, what is the probability that it exceeds 10% of the speed limit?"
2219,1,['test'], SUMMARY,seg_415,6.104 a civil engineer wishes to test if adding a heated tri-chloride of aluminum to
2220,1,"['poisson', 'mean', 'probability', 'tests']", SUMMARY,seg_415,"cement mixtures can improve the strength of high-rise structures to withstand powerful earthquakes and aftershocks. from laboratory tests, it is found that the probability of a crack developing in this type of cement is poisson distributed with mean one in 6 thousand. if a building portfolio comprising 10 buildings in a city neighborhood is built using aluminum hardened cement, what is the probability that at least one building will develop cracks after an earthquake? what is the probability that at most two buildings develop cracks?"
2221,0,[], SUMMARY,seg_415,6.105 a variety of seed is experimented in a laboratory and is found to have a ger-
2222,1,"['rate', 'probability']", SUMMARY,seg_415,"mination rate of 95% (95 out of 100 seeds will germinate). if 10,000 seeds are sawed in identical conditions at a field, what is the probability that (i) at least 99% will germinate and (ii) between 90% and 98% will germinate."
2223,0,[], CONTINUOUS DISTRIBUTIONS,seg_417,"after finishing the chapter, students will be able to"
2224,1,"['continuous', 'distributions', 'continuous distributions']", CONTINUOUS DISTRIBUTIONS,seg_417,◾ understand various continuous distributions
2225,1,"['continuous', 'distributions', 'continuous distributions']", CONTINUOUS DISTRIBUTIONS,seg_417,◾ describe basic properties of common continuous distributions
2226,1,"['exponential distribution', 'distribution', 'exponential']", CONTINUOUS DISTRIBUTIONS,seg_417,◾ explain memory-less property of exponential distribution
2227,1,['distributions'], CONTINUOUS DISTRIBUTIONS,seg_417,◾ utilize the limiting behavior of some distributions
2228,1,"['functions', 'incomplete beta', 'gamma']", CONTINUOUS DISTRIBUTIONS,seg_417,◾ comprehend the incomplete beta and gamma functions
2229,1,"['continuous', 'distributions', 'continuous distributions']", CONTINUOUS DISTRIBUTIONS,seg_417,◾ apply continuous distributions in practical problems
2230,1,"['power method', 'method', 'continuous distributions', 'continuous', 'distributions']", CONTINUOUS DISTRIBUTIONS,seg_417,◾ use the power method to find the md of continuous distributions
2231,1,"['power method', 'method']", CONTINUOUS DISTRIBUTIONS,seg_417,◾ explore the power method in other applications
2232,1,"['ratio scale', 'measurement', 'quantitative', 'data', 'continuous', 'experiments', 'distributions']", INTRODUCTION,seg_419,"continuous distributions are encountered in many industrial experiments and research studies. for example, measurement of quantities (such as height, weight, length, temperature, conductivity, and resistance) on the ratio scale is continuous or quantitative data."
2233,1,"['quantitative', 'interval', 'continuous random variable', 'data', 'random variable', 'variable', 'random', 'continuous']", INTRODUCTION,seg_419,"definition 7.1 the variable that underlies quantitative data is called a continuous random variable, as they can take a continuum of possible values in a finite or infinite interval."
2234,1,"['range', 'discrete', 'probability', 'random', 'function', 'vary', 'random variable', 'statistical', 'limit', 'level', 'continuous', 'continuous random variable', 'probability function', 'discrete random variable', 'variable']", INTRODUCTION,seg_419,"this can be thought of as the limiting form of a point probability function, as the possible values of the underlying discrete random variable become more and more of fine granularity. thus, the mark in an exam (say between 0 and 100) is assumed to be a continuous random variable, even if fractional marks are not permitted. in other words, even though marks are not measured at the finest possible granularity level of fractions, it can be modeled by a continuous law. if all students scored between say 50 and 100 in an exam, the observed range for that exam is of course 50≤ x ≤100. this range may vary from exam to exam, so that the lower limit could differ from 50, and the upper limit of 100 is never realized (nobody got a perfect 100). as shown below, this range is in fact immaterial in several statistical procedures."
2235,1,"['measurement', 'variables', 'treatment', 'errors', 'normally distributed', 'normal', 'measurements', 'mean', 'variations', 'continuous', 'statistical', 'vary', 'distributions']", INTRODUCTION,seg_419,"all continuous variables need not follow a statistical law. however, there are many phenomena that can be approximated by one of the statistical distributions such as the normal law, if not exact. for instance, errors in various measurements are assumed to be normally distributed with zero mean. similarly, measurement variations in physical properties such as diameter, size of manufactured products, and exceedences of dams and reservoirs are assumed to follow a continuous statistical law. this is because they can vary in both directions from an ideal measurement or value called its central value. this chapter introduces the most common continuous univariate distributions. an extensive treatment requires entire volumes by itself. our aim is to summarize the basic properties that are needed in subsequent chapters."
2236,1,"['deviation', 'method', 'continuous distributions', 'mean', 'continuous', 'distributions']", INTRODUCTION,seg_419,"before we proceed to discuss the popular distributions, we first derive a general method to find the mean deviation (md) of continuous distributions. this result will be used extensively throughout the chapter to derive the md of various distributions."
2237,1,"['deviation', 'model', 'curve', 'lorenz curve', 'continuous distributions', 'sampling', 'distribution', 'mean', 'continuous', 'sampling distribution', 'distributions']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"finding the md of continuous distributions is a laborious task, as it requires a lot of meticulous arithmetic work. it is also called the mean absolute deviation or l1-norm from the mean. the md is closely related to the lorenz curve used in econometrics, gini index and pietra ratio used in economics and finance, and in reliability engineering. it is also used as an optimization model for hedging portfolio selection problems [162, 163], fuzzy multisensor object recognition [164], and minimizing job completion times on computer systems [165]. see also jogesh babu and rao [158] for expansions involving the md and pham-gia and hung [160] for the sampling distribution of md."
2238,1,"['deviation', 'medoid', 'median', 'table', 'integer part', 'case', 'continuous distributions', 'distribution', 'normal', 'exponential', 'mean', 'probability', 'continuous', 'distributions']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"johnson [125] surmised that the md of some continuous distributions can be put in the form 2 2 fm where  2 =  2 and fm is the probability density expression evaluated at the integer part of the mean m = ⌊ ⌋. this holds good for exponential, normal, and  2 distributions. kamat [156] generalized johnson’s result to several continuous distributions, (see table 7.1). the multiplier is distribution specific (see the following discussion). the following theorem greatly simplifies the work and is very helpful to find the md of a variety of univariate continuous distributions. it can easily be extended to the multivariate case and for other types of mds such as mean deviation from the median and medoid."
2239,1,"['deviation', 'power method', 'mean', 'method']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,theorem 7.1 power method to find the mean deviation
2240,1,"['continuous distribution', 'distribution', 'continuous', 'tails']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,the md of any continuous distribution that tails off to the left can be expressed in
2241,0,[], MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,terms of the cdf as  
2242,1,"['distribution', 'arithmetic mean', 'mean', 'limit']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"where ll is the lower limit of the distribution,   the arithmetic mean, and f(x) the cdf."
2243,0,[], MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,proof: by definition ul
2244,1,"['distribution', 'range', 'limit']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"where ll is the lower, and ul is the upper limit of the distribution. split the range of integration from ll to  , and   to ul, and note that |x −  | =   − x for x    . this"
2245,1,"['expectation operator', 'expectation']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"as e(x) =  , we can write e(x −  ) = 0, where e() is the expectation operator. expand e(x −  ) as"
2246,1,['range'], MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"as done earlier, split the range of integration from ll to   and   to ul to get"
2247,0,[], MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,split this into two integrals and integrate each of them to get
2248,0,[], MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,use integration-by-parts to evaluate the second expression. {xf(x) ∣ 
2249,0,[], MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,  f(x)dx. the   ∗ f( ) terms cancel out leaving behind
2250,1,"['range', 'symmetric', 'distribution', 'tails', 'limit', 'distributions']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"here x f(x)|ll = ll ∗ f(ll) means that we are to evaluate the limiting value of x ∗ f(x) at the lower limit of the distributions. for those distributions that extend to −∞, this limit is obviously zero. if the lower limit of the distribution is either zero or it tails off to the limit, the first term in equation (7.7) is zero. if f(x) contains expressions of the form (x−ll), then also this term is zero. similarly for distributions with range x ≥ 0 for which the mode is not ll, f(0) → 0 as x → 0. if the distribution is symmetric, equation (7.7) becomes"
2251,1,"['distribution', 'function', 'tails']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,where s(x) = 1 − f(x) is the survival function (sf). if the distribution tails off to the
2252,0,[], MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"right, we evaluate this as ul"
2253,1,"['tails', 'distributions']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"otherwise, we need to evaluate both the terms in equation (7.7). these situations are illustrated in the numerous md examples throughout the chapter. thus, we could write the aforementioned for distributions that tails off to one of the extremes as"
2254,1,"['incomplete beta', 'beta function', 'incomplete beta function', 'function']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"this representation of md in terms of cdf is extremely helpful when one needs to evaluate the md using the cdf or sf. see the example on the md of beta-i distribution, which is represented in terms of incomplete beta function in page 271."
2255,1,"['continuous distributions', 'central limit theorems', 'continuous', 'convergence', 'limit', 'distributions']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"in addition to finding the md of continuous distributions, this formulation has other important applications in proving convergence of distributions and central limit theorems. replace x on the lhs by s = x1 + x2 + · · · + xn. if xi"
2256,1,"['continuous random variables', 'random variables', 'variables', 'mean', 'random', 'continuous']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"distributed continuous random variables, this has mean n  so that the relationship becomes"
2257,1,"['functions', 'random variables', 'independent', 'continuous', 'variables', 'arithmetic mean', 'asymptotic', 'cases', 'independent random variables', 'mean', 'random', 'function', 'convergence', 'limit']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"where f(x) and s(x) are cdf and sf of s. dividing both sides by n, we see that the lhs is the arithmetic mean (x1 + x2 + · · · + xn)/n and rhs has an “n” in the denominator and f(x), s(x) are the cdf and sf of the mean rather than x. taking the limit as n → ∞, we see that the rhs tends to zero (because both the integrals are bounded for finite mean (see equation 7.13 given below) and the lhs converges to  . this provides a simple and elegant proof for the asymptotic convergence of independent random variables, which can be extended to other cases. for example, if g(xi) is a continuous function of xi with finite mean   = g( ), replacing xi by g(xi) in equation (7.11) provides a simple proof on the convergence of g(xi) to its mean asymptotically. similarly, md of functions of random variables can be easily obtained from equation (7.1) by replacing x with y = g(x),   with   = g( ), and f(x) with the cdf of y."
2258,1,"['functions', 'method', 'continuous', 'discrete', 'discrete distributions', 'distribution', 'continuous distributions', 'probability', 'function', 'coefficient', 'distributions']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"there are two other novel methods to find the md. the first one uses generating functions (chapter 9, section 9.4, p. 381) to fetch a single coefficient of t −1 in the power series expansion of (1 − t)−2px(t), where px(t) is the probability generating function. this works best for discrete distributions. the second method is using the inverse of distribution functions (chapter 10, section 10.4, p. 402). this works best for continuous distributions."
2259,1,['median'], MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,similar expressions are available for the md around the median as
2260,1,['median'], MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,1 1 2 2 e|x − median| = (f−1(1 − x) − f−1(x))dx = (s−1(x) − s−1(1 − x))dx. ∫0 ∫0 (7.12)
2261,1,"['tail areas', 'continuous distributions', 'continuous', 'tail', 'variance', 'distributions']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,theorem 7.2 variance of continuous distributions as tail areas
2262,1,"['continuous distribution', 'continuous', 'distribution', 'tail areas', 'tail', 'variance']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,prove that the variance of a continuous distribution can be expressed in terms of tail areas.
2263,0,[], MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,proof: we found earlier that md = 2 ∫x
2264,1,"['integer part', 'mean', 'probability']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,cdf and s(x) is the sf. equating johnson’s result that md = c  2 fm where  2 =  2 and fm is the probability mass evaluated at the integer part of the mean m = ⌊ ⌋ we get  2 ∗ cfm = 2 ∫x
2265,0,[], MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,an alternate expression given by jones  ; balakrishnan is
2266,1,"['table', 'distribution', 'laplace distribution', 'distributions']", MEAN DEVIATION OF CONTINUOUS DISTRIBUTIONS,seg_421,"where ll and ul are the lower and upper limits [166]. see also chapter 8 (p. 363). the constant multiplier (c = 2) proposed by johnson may be different for some continuous distributions (e.g., for laplace distribution c = 1) (see table 7.1). even in those situations, the above-mentioned result holds in general because the rhs of equation (7.13) simply get scaled by the constant (c)."
2267,1,"['range', 'case', 'discrete', 'negative binomial', 'binomial distribution', 'standard normal distribution', 'geometric', 'standard', 'distributions', 'standard normal', 'discrete distributions', 'distribution', 'binomial', 'continuous', 'noncentral', 'poisson', 'probabilities', 'negative binomial distribution', 'normal', 'normal distribution', 'continuous distributions']", Notion of Infinity,seg_423,"another important point to remember in the study of continuous distributions is the notion of infinity (∞). chapter 6 introduced several discrete distributions that extend to infinity. examples are the poisson, geometric, negative binomial, and logarithmic laws. all of them extend to +∞. here, ∞ is assumed to be a large integer (because discrete distributions take integer values; usually nonnegative). in this chapter, ∞ is assumed to be a large real number (because continuous distributions take real values). this difference is subtle but important because the majority of continuous distributions extend to infinity either at the positive end or both ends. in the discrete case, we write x = 0, 1, … ,∞ (x actually assumes the value ∞), whereas in the continuous case we write it as x ∞ (we seldom write x ≤ ∞). for example, the range for standard normal distribution is written as −∞ z ∞. in the case of mixture distributions that have a discrete part and a continuous part like the noncentral 2 distribution, which is a poisson-weighted sum of central 2 distributions, the continuous part takes precedence. this means that for such distributions the rule reverts to the continuous component, so that the discrete part (i.e., poisson probabilities) assumes all values ∞. if both components of a mixture distribution are discrete (e.g., noncentral negative binomial distribution), the rule reverts to the discrete case."
2268,1,"['range', 'case', 'discrete', 'discrete distribution', 'data', 'mean', 'parameter', 'population', 'populations', 'distributions', 'distribution', 'continuous', 'poisson distribution', 'variance', 'poisson', 'sampling']", Notion of Infinity,seg_423,"this notion pertains only to the population variate values. if the parameter(s) of a discrete distribution takes any value on the real line, we write it as in the continuous case. for example, the parameter of poisson distribution has range −∞ ∞. sampling from such populations always gives us data values in a finite range. as shown below, this range can be fixed in terms of the mean and variance of the distributions. this is especially suitable for bell-shaped distributions."
2269,1,"['interval', 'continuous', 'range', 'uniform distribution', 'distribution', 'rectangular distribution', 'probability']", CONTINUOUS UNIFORM DISTRIBUTION,seg_425,"as the name implies, this distribution assigns a constant probability to each point in a continuous interval. thus, the range is always finite (and quite often small in practical applications). it is also called continuous rectangular or simply rectangular distribution. the pdf of continuous uniform distribution (cuni(a, b)) is given by"
2270,1,"['sample', 'interval', 'range', 'case', 'intercept', 'distribution', 'slope']", CONTINUOUS UNIFORM DISTRIBUTION,seg_425,"considered as an algebraic equation, y = (x − a)∕(b − a) represents a straight line with slope 1∕(b − a) and intercept a∕(a − b). this line is defined only within the interval (a, b) (theoretically, a straight line extends to infinity in both directions). the slope is small when the range (b − a) is large. the slope is large (line is steep) in the limiting case b → a. only the extremes of a sample x(1) and x(n) are sufficient to fit this distribution (figures 7.1)."
2271,1,"['interval', 'range', 'change of origin', 'probability', 'function', 'transformation', 'change of origin and scale', 'uniform distribution', 'results', 'symmetry', 'mean', 'standard', 'median', 'distribution', 'moments', 'probability function']", Properties of Continuous Uniform Distribution,seg_427,"this distribution has a special type of symmetry called flat-symmetry. hence, all odd central moments except the first one are zeros. the median always coincides with the mean, and the mode can be any value within the range. as the probability is constant throughout the interval, the range is always finite (and quite often small). from equation (7.3), we see that a change of origin and scale transformation y = (x − a)∕(b − a) results in the standard uniform distribution. a uniform distribution defined in an interval (c, c +  ) has pdf f (x;  ) = 1∕  for c ≤ x ≤ c +  . take c = 0 to get the standard form f (x;  ) = 1∕ , 0   x    . this is the analog of the duni(n) with probability function f (x;n) = 1∕n, x = 0, 1, 2, · · · ,n − 1 discussed in page 6-41 of chapter 6."
2272,1,"['functions', 'moments', 'mean']", Properties of Continuous Uniform Distribution,seg_427,7.3.1.1 moments and generating functions the moments are easy to find using the mgf. the mean is directly obtained as   = [1∕(b − a)] ∫a
2273,0,[], Properties of Continuous Uniform Distribution,seg_427,moments using the mgf. by definition
2274,1,['moments'], Properties of Continuous Uniform Distribution,seg_427,"to find the moments, we proceed as follows. consider ebt∕t = 1∕t + b + b2t∕2! + · · · + bktk−1∕k! + .. as (1∕t) is common in both ebt∕t and eat∕t, it cancels out. the second term is (b − a)∕(b − a) = 1. thus"
2275,1,"['independent', 'limit']", Properties of Continuous Uniform Distribution,seg_427,"if we differentiate equation (7.15) (k − 1) times with respect to t, all terms below the (k − 1)th term will vanish (as they are derivatives of constants independent of t’s) and all terms beyond the kth term will contain powers of t. only the (k − 1)th term is a constant with a (k − 1)! in the numerator, which cancels out with the k! giving a k in the denominator. by taking the limit as t → 0, we get"
2276,1,"['second moment', 'moment']", Properties of Continuous Uniform Distribution,seg_427,putting k = 2 gives  1 = (b + a)∕2. the second moment is obtained by putting k = 3 as  2
2277,1,"['table', 'moment']", Properties of Continuous Uniform Distribution,seg_427,"tral moment as  2 = (b2 − ab + a2)∕3 − (a + b)2∕4. taking 12 as the lcm of 3 and 4, this simplifies to  2 =  2 = (b − a)2∕12. see table 7.2 for further properties."
2278,0,[], Properties of Continuous Uniform Distribution,seg_427,these two equations to get b =   +√
2279,0,[], Properties of Continuous Uniform Distribution,seg_427,"3  . subtracting gives a =   −√ 3  , from"
2280,0,[], Properties of Continuous Uniform Distribution,seg_427,3) . this allows us to write the pdf in the alternative form as
2281,1,"['distribution', 'moments', 'even moments', 'rectangular distribution']", Properties of Continuous Uniform Distribution,seg_427,example 7.1 even moments of rectangular distribution
2282,1,['moment'], Properties of Continuous Uniform Distribution,seg_427,prove that the kth central moment is zero for k odd and is given by  k = (b − a)k∕[2k(k + 1)] for k even.
2283,0,[], Properties of Continuous Uniform Distribution,seg_427,solution 7.1 by definition  k = b−
2284,1,['jacobian'], Properties of Continuous Uniform Distribution,seg_427,"1 jacobian is  y∕ x = 1, the integral becomes  k = b−a ∫−"
2285,1,"['range', 'symmetric', 'function']", Properties of Continuous Uniform Distribution,seg_427,"odd, this is an integral of an odd function in symmetric range, which is identically"
2286,1,"['deviation', 'distribution', 'rectangular distribution', 'mean']", Properties of Continuous Uniform Distribution,seg_427,example 7.2 mean deviation of rectangular distribution
2287,1,"['distribution', 'rectangular distribution']", Properties of Continuous Uniform Distribution,seg_427,find the md of rectangular distribution.
2288,1,['range'], Properties of Continuous Uniform Distribution,seg_427,b |x −  |∕(b − a)dx. split the range of integration from “a” to   and   to “b” and note that |x −  | =   − x for x   . this gives
2289,0,[], Properties of Continuous Uniform Distribution,seg_427,"   dx − ∫  b  dx. as   = (a + b)∕2, this integral vanishes. what remains is"
2290,1,"['tail', 'limit']", Properties of Continuous Uniform Distribution,seg_427,"next we apply theorem 7.1 (p. 257) to verify our result. as the rectangular distribution does not tail off to zero at the extremes, equation (7.1) seems to be not applicable. however, we know the cdf is (x − a)∕(b − a). if we substitute the lower limit is “a,” in (x − a)∕(b − a), we get zero. hence, theorem 7.1 is applicable. this gives"
2291,1,['limit'], Properties of Continuous Uniform Distribution,seg_427,"c(x − a)dx is (x − a)2∕2|ac . the integral evaluated at the lower limit is obviously zero. as c = (a + b)∕2, the upper limit evaluates to (b − a)2∕8. substitute in equation (7.20). one (b − a) cancels out and we get the md as (b − a)∕4. this tallies with the above-mentioned result."
2292,1,"['random', 'transformation', 'variable transformation', 'results', 'random variable', 'exponential', 'standard', 'random numbers', 'distributions', 'functions', 'change of variable', 'distribution', 'continuous', 'continuous random variable', 'random variables', 'variables', 'variable']", Relationships with Other Distributions,seg_429,"owing to its relationship with many other distributions, it is extensively used in computer generation of random variables. as mentioned earlier, a simple change of variable transformation y = (x − a)∕(b − a) results in the standard uniform distribution u(0, 1), usually denoted as u(0, 1). if x is any continuous random variable with cdf f(x), then u = f(x) ∼ u[0, 1]. this property is utilized to generate random numbers from a distribution if the expression for its cdf involves simple or invertible arithmetic or transcendental functions. for example, the cdf of an exponential"
2293,1,"['random number', 'range', 'case', 'distribution', 'random']", Relationships with Other Distributions,seg_429,"− x distribution (given below) is f(x) = 1 − e . equating to a random number u in the range [0,1] and solving for x, we get 1 − e− x = u or x = − log(1 − u)∕ . u(0, 1) is a special case of beta-i(a, b) when a = b = 1."
2294,1,"['interval', 'factor', 'random', 'transformation', 'correction factor', 'stratified sampling', 'data', 'tests', 'random numbers', 'distributions', 'distribution', 'nonparametric tests', 'test', 'random number', 'errors', 'sampling']", Applications,seg_431,"this distribution finds applications in many fields. it is used in nonparametric tests like kolmogorov–smirnov test. the rounding errors resulting from grouping data into classes uses a u(0, 1) to obtain a correction factor known as sheppard’s correction. quantization errors in audio coding use this distribution. it is also used in stratified sampling, nonrandom clustering, and so on. random numbers for other distributions are easy to generate using u[0, 1]. suppose we have a uniform random number generator between 0 and 1. the transformation y = a + (b − a)x gives a random number in the interval [a, b], where x is in [0,1] (if the random number generated is in [0,32767), we could use the mapping a + (b − a)x∕32767 to get a random number in [a, b])."
2295,0,[], Applications,seg_431,example 7.3 estimating proportions
2296,1,['probability'], Applications,seg_431,a jar contains a mixture of two liquids l1 and l2 that mixes well in each other (as water and wine or acid and water). all that is known is that “there is at most three times as much of one as the other.” find the probability that (i) l1∕l2 ≤ 2 and (ii) l1∕l2 ≥ 1.
2297,1,"['uniformly distributed', 'condition']", Applications,seg_431,"solution 7.3 the given condition is 1 3 ≤ l1∕l2 ≤ 3. let u = l1∕l2. assume that u is uniformly distributed in [1/3, 3]. as 3 − 1∕3 = 8∕3, we take the density"
2298,1,"['distribution', 'continuous']", EXPONENTIAL DISTRIBUTION,seg_433,exponential distribution (exp( )) can be regarded as the continuous analog of geometric distribution. the pdf is given by
2299,1,"['exponential distribution', 'distribution', 'exponential', 'standard']", EXPONENTIAL DISTRIBUTION,seg_433,"−x when   = 1, we get the standard exponential distribution f (x) = e . setting   = 1∕ "
2300,0,['e'], EXPONENTIAL DISTRIBUTION,seg_433,"1 −x∕  gives an alternative representation as f (x,  ) =   e . the cdf is given by"
2301,1,"['distribution', 'variance', 'mean', 'parameter']", Properties of Exponential Distribution,seg_435,"this distribution has a single parameter, which is positive (figure 7.2). variance of this distribution is the square of the mean, as shown in the following. this means that"
2302,1,"['variance', 'kurtosis', 'limit', 'table']", Properties of Exponential Distribution,seg_435,"when  → 0, the variance and kurtosis increases without limit. the sf is 1 − cdf = − x e (see table 7.3)."
2303,1,"['random', 'gamma distribution', 'data', 'information', 'populations', 'statistical', 'distributions', 'additivity property', 'parameters', 'gamma', 'distribution', 'random variables', 'independent', 'variables', 'exponentially distributed', 'exponentially']", Additivity Property,seg_437,"several statistical distributions obey the additivity property. this information is useful while modeling data from two or more identical populations. the sum of k independent exponentially distributed random variables exp( ) has a gamma distribution with parameters k and  . symbolically, if xi are exp( ), then"
2304,1,['table'], Additivity Property,seg_437,"=1 xi ∼ gamma(k,  ). this is most easily proved using the mgf (see table 7.4)."
2305,1,"['functions', 'characteristic function', 'moments', 'function']", Additivity Property,seg_437,7.4.2.1 moments and generating functions the characteristic function is readily obtained by integration as x(t; ) =
2306,0,[], Additivity Property,seg_437,expand as an infinite series using (1 − x)−1 = 1 + x + x2 + x3 + · · · to get
2307,1,"['variance', 'mean']", Additivity Property,seg_437,"from this, the mean and variance follows as   = 1∕  and  2 = 1∕ 2. alternately, the mean is given by   =   ∫0"
2308,0,[], Additivity Property,seg_437,"gamma integral, this becomes   =  γ(2)∕ 2. one   cancels out and we get   = 1∕ "
2309,1,['mean'], Additivity Property,seg_437,"1 −x∕  as γ(2) = 1. for the alternate parametrization f (x,  ) =   e , the mean   =   and"
2310,0,[], Additivity Property,seg_437,variance  2 =  2.
2311,1,"['exponential distribution', 'leptokurtic', 'table', 'results', 'kurtosis', 'distribution', 'asymmetric', 'exponential', 'skewness', 'coefficients']", Additivity Property,seg_437,"the coefficients of skewness and kurtosis are 2 and 9, respectively. hence, the distribution is always asymmetric and leptokurtic. putting y = 1∕x results in the inverse exponential distribution with pdf f (y) = ( ∕y2)e− ∕y. see table 7.3 for further properties."
2312,1,"['distribution', 'exponential', 'exponential distribution', 'median']", Additivity Property,seg_437,example 7.4 median of exponential distribution
2313,1,"['distribution', 'exponential', 'exponential distribution', 'median']", Additivity Property,seg_437,"find the median of exponential distribution with pdf f (x,  ) =  e− x."
2314,1,['median'], Additivity Property,seg_437,solution 7.4 let m be the median. then ∫m
2315,1,['distribution'], Additivity Property,seg_437,"example 7.5 pr(x    ∕2), pr(x   1∕ ) for exp( ) distribution"
2316,1,"['distribution', 'exponential', 'exponential distribution']", Additivity Property,seg_437,− 2∕2 show that pr[x    ∕2] of the exponential distribution is e . what is the pr[x   1∕ ]?
2317,1,"['frequency', 'exponential', 'mean', 'function']", Additivity Property,seg_437,"− x solution 7.5 as the sf is e , pr(x ∕2) is easily seen to be the survival − 2∕2 function evaluated for x = ∕2. this upon substitution becomes e . putting x = 1∕ in the sf, we get e−1 = 1∕e. thus, the mean 1∕ of an exponential distribution divides the total frequency in (1 − 1 e )∶1"
2318,1,"['distribution', 'exponential', 'exponential distribution']", Additivity Property,seg_437,e ratio. this is a characteristic property of exponential distribution.
2319,0,[], Additivity Property,seg_437,example 7.6 lifetime of components
2320,1,"['condition', 'mean', 'probability', 'exponentially distributed', 'exponentially']", Additivity Property,seg_437,"the lifetime of a component is known to be exponentially distributed with mean   = 320 hours. find the probability that the component has failed in 340 hours, if it is known that it was in good working condition when time of operation was 325 hours."
2321,1,"['probability', 'conditional', 'conditional probability']", Additivity Property,seg_437,"solution 7.6 let x denote the lifetime. then x ∼ exp(1/320). symbolically, this problem can be stated as p[x 340|x 325]. using conditional probability, this is equivalent to p[325 x 340]∕p[x 325]. in terms"
2322,0,[], Additivity Property,seg_437,"of the pdf, this becomes ∫3"
2323,1,"['gamma', 'case', 'discrete', 'variates', 'distribution', 'exponential', 'distributions']", Additivity Property,seg_437,"7.4.2.2 relationship with other distributions it is a special case of gamma distribution with m = 1 (p. 283). if x ∼ exp( ) and b is a constant, then y = x1∕b ∼ weib( , b) (p. 320). the difference of two iid exponential variates is laplace distributed. it is also related to the u(0, 1) distribution [167, 168] and power-law distribution, which is a discrete analog of this distribution [169]."
2324,0,[], Additivity Property,seg_437,example 7.7 memory-less property
2325,1,"['exponential distribution', 'distribution', 'exponential']", Additivity Property,seg_437,"prove that the exponential distribution has memory-less property p(x ≥ s + t)|p(x ≥ s) = p(x ≥ t) for s, t ≥ 0."
2326,1,"['model', 'exponential distribution', 'distribution', 'exponential', 'parameter', 'random']", Additivity Property,seg_437,"7.4.2.3 applications this distribution is used to model random proportions and life-time of devices and structures. it has applications in reliability theory and waiting times in queuing theory. for example, the expected life length of a new light bulb can be assumed to follow an exponential distribution with parameter   = 1∕500 hours so that the life time is given by f (x) = (1∕500)(e−x∕500)."
2327,1,['event'], Additivity Property,seg_437,"other examples include modeling: (i) lifetime of destructive devices that are (more or less) continuously or regularly in use, such as light bulbs and tubes, electronic chips. (ii) lifetime of nondestructive or reusable devices until next repair work, electronic devices such as computer monitors and lcd screens, microwaves, electrical appliances such as refrigerators, and lifetime of automobile tires. time until the arrival of the next event (such as next telephone call and emergency call) or time until next customer to an office or business."
2328,1,"['deviation', 'exponential distribution', 'distribution', 'exponential', 'mean']", Additivity Property,seg_437,example 7.8 mean deviation of exponential distribution
2329,1,"['deviation', 'exponential distribution', 'distribution', 'exponential', 'mean']", Additivity Property,seg_437,"find the mean deviation of the exponential distribution f (x,  ) =  e− x."
2330,1,"['exponential distribution', 'distribution', 'exponential', 'tail', 'limit']", Additivity Property,seg_437,"solution 7.8 we apply theorem 7.1 (page 267) to find the md. as the exponential distribution does not tail off to zero at the lower limit (i.e., at 0), − x equation (7.1) seems like not applicable.we know that the cdf is 1 − e . if we apply l’hospital’s rule once on x ∗ f(x) we get x exp(− x) + (1 − exp(− x)). as both terms → 0 as x → 0, the limx→0 x ∗ f(x) = 0, and the theorem 7.1 becomes applicable. this gives"
2331,0,[], Additivity Property,seg_437,split this into two integrals and evaluate each to get
2332,1,"['exponential distribution', 'table', 'distribution', 'exponential', 'tails', 'limit']", Additivity Property,seg_437,"where fm =  e−1 =  ∕e. alternatively, use the sf() version as the exponential distribution tails off to the upper limit (table 7.3)."
2333,1,"['distribution', 'exponential', 'exponential distribution']", Additivity Property,seg_437,7.4.2.4 general form the general form of the exponential distribution is given by
2334,1,"['function', 'characteristic function']", Additivity Property,seg_437,the corresponding characteristic function is
2335,1,"['bayesian', 'statistics', 'type ii', 'distributions', 'model', 'beta distributions', 'distribution', 'binomial', 'continuous', 'beta distribution', 'percentage', 'order statistics', 'prior distribution', 'continuous distributions']", BETA DISTRIBUTION,seg_439,"the beta distribution is widely used in statistics owing to its close relationship with other continuous distributions. it is also used in bayesian models with unknown probabilities, in order statistics and reliability analysis. it is used to model the proportion of fat (by weight) in processed or canned food and percentage of impurities in some manufactured products such as food items, cosmetics, and laboratory chemicals. in bayesian analysis, the prior distribution is assumed to be the beta for binomial proportions. important distributions belonging to the beta family are discussed in the following. these include type i and type ii beta distributions. we will use the respective notations beta-i(a,b) and beta-ii(a,b). beta distributions with three or more parameters are also briefly mentioned."
2336,1,"['distribution', 'beta distribution', 'standard']", TypeI Beta Distribution,seg_441,"this is also called the standard beta distribution. the pdf of beta-i(a, b) is given by"
2337,1,"['beta function', 'results', 'complete beta function', 'function']", TypeI Beta Distribution,seg_441,"where 0   x   1, and b(a, b) is the complete beta function (cbf). particular values for a and b results in a variety of distributional shapes."
2338,1,"['tail areas', 'parameters', 'range', 'symmetric', 'distribution', 'symmetry', 'parameter', 'tail', 'variance']", Properties of TypeI Beta Distribution,seg_443,"this distribution has two parameters, both of which are positive real numbers. the range of x is between 0 and 1. the variance is always bounded, irrespective of the parameter values. put y = 1 − x in the above to get the well-known symmetry relationship fx(a, b) = fy(b, a) or in terms of tail areas ix(a, b) = 1 − i1−x(b, a), where ix(a, b) is described below (p. 277). if a = b, the distribution is symmetric about x = 1∕2. if a = b = 1, it reduces to uniform (rectangular) distribution. when a = b = 1∕2, this distribution reduces to the arc-sine distribution of first kind (section 7.8, p. 279). if b = 1 and a ≠ 1, it reduces to power-series distribution f (x; a) = axa−1 using the resultγ(a + 1) = a ∗ γ(a). put a = + 1, b = + 1 to get an alternate form"
2339,1,"['functions', 'moment', 'moments']", Properties of TypeI Beta Distribution,seg_443,7.5.2.1 moments and generating functions the moments are easy to find using beta integral. the kth moment can be obtained as
2340,0,[], Properties of TypeI Beta Distribution,seg_443,"in terms of rising factorials, this becomes  k"
2341,1,"['characteristic function', 'parameters', 'moment', 'symmetric', 'mean', 'parameter', 'function', 'variance', 'second moment']", Properties of TypeI Beta Distribution,seg_443,"′ = a[k]∕(a + b)[k]. the mean is obtained by putting k = 1 as   = a∕(a + b) = 1 − b∕(a + b). this has the interpretation that increasing the parameter “a” by keeping “b” fixed moves the mean to the right (toward 1). put k = 2 to get the second moment as a(a + 1)∕[(a + b)(a + b + 1)]. the variance is  2 = ab∕[(a + b)2(a + b + 1)]. this is symmetric in the parameters and increasing both “a” and “b” together decreases the variance. if a   1 and b   1, there exist a single mode at (a − 1)∕(a + b − 2). the characteristic function is"
2342,1,"['moment', 'hypergeometric function', 'hypergeometric', 'function']", Properties of TypeI Beta Distribution,seg_443,"where 1f1(a, a + b; it) is the confluent hypergeometric function. the kth central moment can be obtained as follows:"
2343,1,"['coefficient', 'hypergeometric function', 'hypergeometric', 'function', 'skewness', 'coefficient of skewness']", Properties of TypeI Beta Distribution,seg_443,"where 2f1(a, b, c; x) is the hypergeometric function. the coefficient of skewness is"
2344,1,"['deviation', 'mean']", Properties of TypeI Beta Distribution,seg_443,a + b + 1∕[√ ab(a + b + 2)]. mean deviation about the mean is given by
2345,1,['table'], Properties of TypeI Beta Distribution,seg_443,see table 7.5 for further properties (figures 7.3 and 7.4).
2346,1,"['deviation', 'distribution', 'mean', 'beta distribution']", Properties of TypeI Beta Distribution,seg_443,example 7.9 mean deviation of beta distribution
2347,1,"['deviation', 'distribution', 'mean', 'beta distribution']", Properties of TypeI Beta Distribution,seg_443,find the mean deviation of the beta distribution using theorem 7.1.
2348,1,"['case', 'distribution', 'parameter', 'tail', 'beta distribution']", Properties of TypeI Beta Distribution,seg_443,"solution 7.9 as the beta distribution does not tail off to the lower or upper limits for some parameter values (e.g., a = b = 0.25), equation (7.1) seems like not applicable. we know that the cdf is ix(a, b). as done in the case of exponential distribution, using l’hospital’s rule, it is easy to show that x ∗ f(x) → 0, so that the theorem 7.1 is applicable. this gives md = 2 ∫0"
2349,1,['mean'], Properties of TypeI Beta Distribution,seg_443,"c = a∕(a + b) is the mean. taking u = ix(a, b) and dv = dx, this becomes"
2350,0,[], Properties of TypeI Beta Distribution,seg_443,this can be simplified using a result in reference 4 as
2351,1,"['results', 'table']", Properties of TypeI Beta Distribution,seg_443,"to get md = 2bc∕[(a + b)(a + b + 1)]gc(x; a, b), where c = a∕(a + b). results are compared in table 7.6."
2352,1,"['variance', 'mean']", Properties of TypeI Beta Distribution,seg_443,example 7.10 mean versus variance of beta-i
2353,1,"['distribution', 'variance', 'mean']", Properties of TypeI Beta Distribution,seg_443,prove that the variance can never equal the mean of a beta-i distribution.
2354,1,"['variance', 'mean']", Properties of TypeI Beta Distribution,seg_443,"solution 7.10 we know that the variance of beta-i can be represented in terms of the mean as  (1 −  )∕(a + b + 1). assume the contrary that the variance can be equal to the mean. put   = x in the above to get x(1 − x)∕(a + b + 1) = x. this simplifies to −x2 = (a + b)x. as the mean cannot be zero (as “a” cannot be zero), there is no solution possible. hence, the variance of beta-i is always less than the mean. alternatively, divide the variance by the mean and argue that as   ∈ (0, 1), 1 −   is always less than 1, showing that the ratio is  1, which implies that  2    ."
2355,1,"['range', 'distribution', 'transformation', 'beta distribution']", TypeII Beta Distribution,seg_445,"beta distribution of the second kind (also called type-ii beta distribution or inverted beta distribution (ibd)) is obtained from the above by the transformation y = x∕(1 − x) or equivalently x = y∕(1 + y). when x → 0, y → 0, and when x → 1, y → ∞. hence, the range of y is from 0 to ∞. the pdf is given by"
2356,1,"['model', 'range', 'distribution', 'random', 'experiments', 'vary', 'limit']", TypeII Beta Distribution,seg_445,"the beta-i distribution is used to model random experiments or occurrences that vary between two finite limits, which are mapped to the (0,1) range, whereas beta-ii is used when upper limit is infinite."
2357,1,"['degrees of freedom', 'f distribution', 'case', 'distribution', 'beta distributed']", Properties of TypeII Beta Distribution,seg_447,"this is a special case of the unscaled f distribution (distribution of 2m∕ 2n) or an f with the same degrees of freedom. in other words, put y = (m∕n) ∗ x in f distribution to get beta-ii distribution. if y is beta-ii(a,b) then 1∕y is beta-ii(b, a). this means that 1∕[x∕(1 − x)] = (1 − x)∕x is also beta distributed (see the following discussion)."
2358,1,"['functions', 'moments', 'mean', 'variance']", Properties of TypeII Beta Distribution,seg_447,7.5.4.1 moments and generating functions the mean and variance are   = a∕(b − 1) and  2 = a(a + b − 1)∕[(b − 1)2(b − 2)] for b   2. consider e(yk)
2359,1,['range'], Properties of TypeII Beta Distribution,seg_447,"put x = y∕(1 + y) so that y = x∕(1 − x), (1 + y) = 1∕(1 − x), and dy∕dx = [(1 − x) − x(−1)]∕(1 − x)2. this simplifies to 1∕(1 − x)2. the range of x is [0,1]. hence, equation (7.36) becomes"
2360,1,"['factors', 'moment', 'factor', 'mean', 'variance', 'second moment']", Properties of TypeII Beta Distribution,seg_447,"this is b(a + k, b − k)∕b(a, b). put k = 1 to get the mean as γ(a + 1)γ(b − 1) γ(a + b)∕[γ(a)γ(b)γ(a + b)]. write γ(a + 1) = aγ(a) in the numerator and γ(b) = (b − 1)γ(b − 1) in the denominator and cancel out common factors to get   = a∕(b − 1). put k = 2 to get the second moment as b(a + 2, b − 2)∕b(a, b) = γ(a + 2)γ(b − 2)γ(a + b)∕[γ(a)γ(b)γ(a + b)] = a(a + 1)∕[(b − 1)(b − 2)]. from this, the variance is obtained as a(a + 1)∕[(b − 1)(b − 2)] − a2∕(b − 1)2. take   = a∕(b − 1) as a common factor. this can now be written as  ( a"
2361,1,"['distribution', 'variance']", Properties of TypeII Beta Distribution,seg_447,"1 2 − ). substitute for inside the bracket and take (b − 1)(b − 2) as common denominator. the numerator simplifies to b − a + 2a − 1 = (a + b − 1). hence, the variance becomes 2 = a(a + b − 1)∕[(b − 1)2(b − 2)]. as (a + 1)∕(b − 2) − = (a + b)∕[(b − 1)(b − 2)], this expression is valid for b 2. unlike the beta-i distribution whose variance is always bounded, the variance of beta-ii can be"
2362,1,"['gamma', 'expectation', 'complete gamma function', 'function', 'gamma function']", Properties of TypeII Beta Distribution,seg_447,increased arbitrarily by keeping b constant (say near 2+) and letting a → ∞. it can also be decreased arbitrarily when (a + 1)∕(b − 2) tends to   = a∕(b − 1). the expectation of [x∕(1 − x)]k is easy to compute in terms of complete gamma function
2363,1,['table'], Properties of TypeII Beta Distribution,seg_447,γ(a+k)γ(b−k) as e[x∕(1 − x)]k = . see table 7.7 for further properties.
2364,1,['distribution'], Properties of TypeII Beta Distribution,seg_447,example 7.11 the mode of beta-ii distribution
2365,1,['distribution'], Properties of TypeII Beta Distribution,seg_447,prove that the mode of beta-ii distribution is (a − 1)∕(b + 1).
2366,0,[], Properties of TypeII Beta Distribution,seg_447,solution 7.11 differentiate equation (7.35) (without constant multiplier) with respect to y to get
2367,1,"['random variables', 'independent', 'gamma', 'variables', 'variates', 'normal', 'gamma random variables', 'random', 'beta distributed']", Relationship with Other Distributions,seg_449,"put a = b = 1 to get beta(1,1), which is identical to u(0, 1). if x is beta1(a, b) then (1 − x)∕x is beta2(b, a), and x∕(1 − x) is beta2(a, b). if x and y are independent gamma random variables γ(a,  ) and γ(b,  ), then x∕(x + y) is beta(a, b) (see exercise 7.26). as gamma and  2 are related, this result can also be stated in terms of normal variates as follows. if x and y are independent normal variates, then z = x2∕(x2 + y2) is beta distributed. in addition, if x1,x2, · · · ,xk are iid n(0, 1) and z1 = x1"
2368,0,[], Relationship with Other Distributions,seg_449,"2, then each of them are beta-i distributed, as also the product"
2369,1,['set'], Relationship with Other Distributions,seg_449,of any consecutive set of zj
2370,1,"['distribution', 'beta distributed', 'logistic', 'logistic distribution']", Relationship with Other Distributions,seg_449,"′s are beta distributed [167, 171]. the logistic distribution"
2371,1,"['order statistic', 'logistic distribution', 'logistic', 'exponential distribution', 'uniform distribution', 'distribution', 'statistic', 'normal', 'exponential', 'type ii', 'beta distributed', 'statistical', 'beta distribution', 'distributions']", Relationship with Other Distributions,seg_449,"and type ii beta distribution are related as y = − ln(x). if x is beta-i(a, b) then y = ln(x∕(1 − x)) has a generalized logistic distribution [172, 173]. dirichlet distribution is a generalization of beta distribution. order statistic from uniform distribution is beta distributed. in general, jth highest order statistic from a uniform distribution is beta-i(j, n − j + 1). see reference 174 for the beta-generalized exponential distribution, reference 167 for relationships among various statistical distributions, reference 133 for normal approximations, and reference 175 for new properties of this distribution."
2372,1,"['distribution', 'random variable', 'variable', 'normal', 'random', 'distributions']", Relationship with Other Distributions,seg_449,"as the random variable takes values in [0,1], any cdf can be substituted for x to get a variety of new distributions [22]. for instance, put x = φ(x), the cdf of a normal variate to get the beta-normal distribution with pdf"
2373,1,"['range', 'distribution', 'normal', 'normal distribution']", Relationship with Other Distributions,seg_449,"where b(a, b) is the cbf,  (x) is the pdf and φ(x) is the cdf of normal distribution, so that the range is now extended to −∞   x   ∞."
2374,1,"['outliers', 'statistics', 'statistic', 'function', 'linear', 'homogeneity', 'samples', 'likelihood ratio', 'populations', 'tests', 'model', 'linear model', 'distribution', 'beta function', 'incomplete beta', 'incomplete beta function', 'likelihood', 'normal', 'variances']", THE INCOMPLETE BETA FUNCTION,seg_451,"the incomplete beta function (ibf) denoted by ix(a, b) or i(x; a, b) has several applications in statistics and engineering. it is used in wind velocity modeling [176], flood water modeling, and soil erosion modeling. it is used to compute bartlett’s statistic for testing homogeneity of variances when unequal samples are drawn from normal populations [177] and in several tests involving likelihood ratio criterion [178]. it is also used in computing the power function of nested tests in linear models [179], approximating the distribution of largest roots in multivariate inference, and detecting two outliers in the same direction in a linear model [180]. its applications to traffic accident proneness are discussed by haight [181]."
2375,1,"['distribution', 'binomial', 'function', 'binomial distribution', 'statistical', 'distributions']", Tail Areas Using IBF,seg_453,"tail areas of several statistical distributions are related to the beta cdf as discussed below. the survival function of a binomial distribution bino(n, p) is related to the left-tail areas of beta-i distribution as"
2376,0,[], Tail Areas Using IBF,seg_453,"using the relationship (7.47), the cdf becomes"
2377,0,[], Tail Areas Using IBF,seg_453,"when both a and b are integers, this has a compact representation as"
2378,1,"['negative binomial distribution', 'distribution', 'negative binomial', 'binomial', 'function', 'binomial distribution']", Tail Areas Using IBF,seg_453,the survival function of negative binomial distribution is related as follows
2379,1,"['distribution', 'f distribution']", Tail Areas Using IBF,seg_453,the relationship between the cdf of central f distribution and the ibf is
2380,1,['degrees of freedom'], Tail Areas Using IBF,seg_453,"where (m, n) are the numerator and denominator degrees of freedom (dof) and y = mx∕(n + mx). similarly, student’s t cdf is evaluated as"
2381,1,"['logistic distribution', 'statistics', 'correlation', 'negative binomial', 'statistic', 'hypergeometric', 'function', 'tail areas', 'noncentral f', 'sample', 'logistic', 'noncentral beta', 'populations', 'coefficient', 'distributions', 'distribution', 'binomial', 'correlation coefficient', 'noncentral', 'hypergeometric function', 'order statistics', 'tail']", Tail Areas Using IBF,seg_453,"the ibf is related to the tail areas of binomial, negative binomial, student’s t, and central f distributions [182]. it is also related to the confluent hypergeometric function, generalized logistic distribution, the distribution of order statistics from uniform populations, and the hotelling’s t2 statistic. the hypergeometric function can be approximated using the ibf also [183]. the dirichlet (and its inverse) distribution can be expressed in terms of the ibf [184]. it is related to the cumulative distribution function (cdf) of noncentral distributions [7, 185–192] and the sample multiple correlation coefficient [193, 194]. for instance, the cdf of singly noncentral beta [179, 195], singly type-ii noncentral beta, doubly noncentral beta [4], noncentral t [188, 196], noncentral f [5, 197, 198], and the sample multiple correlation coefficient [199, 200] could all be evaluated as infinite mixtures of the ibf."
2382,1,"['distribution', 'beta distribution']", Tail Areas Using IBF,seg_453,definition 7.2 the ibf is the left-tail area of the beta distribution
2383,1,['symmetric'], Tail Areas Using IBF,seg_453,"where b(a, b) is the cbf. obviously, i0(a, b) = 0 and i1(a, b) = 1. replace x by (1 − x) and swap a and b to get a symmetric relationship."
2384,1,"['noncentral beta', 'symmetry', 'tail areas', 'tail', 'noncentral', 'distributions']", Tail Areas Using IBF,seg_453,"this symmetry among the tail areas was extended by chattamvelli [196] to noncentral beta, noncentral fisher’s z, and doubly noncentral distributions."
2385,1,"['range', 'statistics', 'test statistics', 'representations', 'tail areas', 'evaluating', 'noncentral beta', 'symmetric', 'distributions', 'parameters', 'distribution', 'noncentral', 'test', 'beta distribution', 'tail']", Tail Areas Using IBF,seg_453,"if the cdf (left-tail area) of a type-ii noncentral beta distribution is denoted as jx(a, b,  ), then the tail areas are related as jx(a, b,  ) = 1 − i1−x(b, a,  ), where iy(b, a,  ) is the cdf of a type-i noncentral beta distribution. we write ix(b) or i(x; b) for the symmetric ibf ix(b, b) [201, 202]. the parameters of an ibf can be any positive real number. simplified expressions exist when either of the parameters is an integer or a half-integer. these representations have a broad range of applications to evaluating or approximating other related distributions and test statistics mentioned earlier."
2386,1,"['functions', 'representations']", Tail Areas Using IBF,seg_453,"the ibf has representations in terms of other special functions and orthogonal polynomials [183, 202–205]. for example, it could be expressed in terms of hypergeometric series in the following form:"
2387,1,['hypergeometric'], Tail Areas Using IBF,seg_453,where 2f1 denotes the hypergeometric series.
2388,1,"['pearson', 'tables']", Tables,seg_455,"many tables for the ibf are available. see, for example, soper [206], pearson [207], aroian [203], majumder and bhattacharjee [208], and boston and battiste [209]."
2389,1,"['gamma', 'distribution', 'gamma distributions', 'random', 'random numbers', 'beta distribution', 'distributions']", Tables,seg_455,"random variate generation from beta distribution is accomplished using the relationship between the beta and gamma distributions. hence, if two random numbers are generated from γ(1, a) and γ(1, b), where a b, then the beta variate is given by b(a, b) = γ(1, a)∕[γ(1, a) + γ(1, b)] [22]."
2390,1,"['distribution', 'beta distribution', 'parameter']", GENERAL BETA DISTRIBUTION,seg_457,general three parameter beta distribution is given by
2391,1,"['distribution', 'transformation', 'beta distribution']", GENERAL BETA DISTRIBUTION,seg_457,the four-parameter beta distribution is obtained from the above-mentioned representation by the transformation y = (x − a)∕(b − a) to get the pdf
2392,1,"['pochhammer notation', 'parameters', 'coefficient', 'location', 'mean', 'skewness', 'variance', 'coefficient of skewness']", GENERAL BETA DISTRIBUTION,seg_457,"this has mean (ad + bc)∕(c + d) and variance  2 = cd(b − a)2∕[(c + d + 1)(c + d)2]. the location parameters are “a,”“b” and scale parameters are c and d. coefficient of skewness is 2cd(d − c)∕[(c + d)2(c + d)(3)[cd∕((c + d)(c + d)(2))]], where (c + d)(k) is raising pochhammer notation with (c + d)(3) = (c + d)(c + d + 1)(c + d + 2)."
2393,1,"['symmetric', 'case', 'distribution', 'standard', 'beta distribution']", ARCSINE DISTRIBUTION,seg_459,"this is a special case of the beta distribution when a = 1∕2, b = 1∕2. the standard arc-sine distribution (sasd) of first kind has support 0   x   1, is u-shaped and symmetric around x = 1∕2 (see figure 7.5). its pdf is given by"
2394,1,['distribution'], ARCSINE DISTRIBUTION,seg_459,"this shows that the above is indeed a pdf. another form of the distribution called arc-sine distribution of second kind has support −1   x   1 with pdf given by (see exercise 7.74, p. 331)"
2395,1,['range'], ARCSINE DISTRIBUTION,seg_459,"to prove that this is a pdf, integrate over the range to get ∫−"
2396,1,"['distribution', 'moments']", ARCSINE DISTRIBUTION,seg_459,example 7.12 moments of arc-sine distribution
2397,1,"['distribution', 'moment']", ARCSINE DISTRIBUTION,seg_459,find the kth moment of arc-sine distribution of second kind
2398,1,['moment'], ARCSINE DISTRIBUTION,seg_459,solution 7.12 the kth moment is ( 
2399,1,['function'], ARCSINE DISTRIBUTION,seg_459,"2 2 sink d . as sin( ) is an odd function, the integral vanishes"
2400,0,[], ARCSINE DISTRIBUTION,seg_459,"when k is odd. when k is even, the integral becomes ( "
2401,0,[], ARCSINE DISTRIBUTION,seg_459,"∫0  ∕2 sinp d  where p = 2k. using integration by parts, this reduces to"
2402,1,['distribution'], ARCSINE DISTRIBUTION,seg_459,the two-parameter arc-sine distribution (asd) is obtained by putting y = (x − a)∕b in the above as
2403,1,"['median', 'kurtosis', 'mean', 'skewness', 'variance']", ARCSINE DISTRIBUTION,seg_459,"it has mean = median = (a + b)∕2, variance (b − a)2∕8, skewness = 0, and excess kurtosis −3∕2."
2404,1,"['distribution', 'case']", Properties of ArcSine Distribution,seg_461,1 the sasd-i is a special case of beta type-i distribution. put y = x − to get
2405,1,"['coefficient', 'kurtosis', 'distribution', 'bimodal', 'mean', 'skewness', 'variance', 'coefficient of skewness']", Properties of ArcSine Distribution,seg_461,"the mean is 0.5 and variance is 0.125 for the sasd. as the distribution is symmetric, coefficient of skewness is zero. the kurtosis coefficient is 2 = 3∕2. hence, it is always platykurtic. note that the density is maximum when x is near 0 or 1 with the center as a cusp (u-shaped). hence, there are two modes (bimodal) that are sym-"
2406,1,['tails'], Properties of ArcSine Distribution,seg_461,metrically placed in the tails. this is the reason why it is platykurtic. the central
2407,1,['distribution'], Properties of ArcSine Distribution,seg_461,moments of arc-sine distribution of second kind is  2k = ( 2
2408,1,"['table', 'bessel function', 'modified bessel function', 'parameter', 'function']", Properties of ArcSine Distribution,seg_461,"is mx(t) = et∕2i0(t∕2), where i0(x) is the modified bessel function of first kind. the two parameter asd satisfies an interesting property:–if x ∼ asd(a, b) then cx + d ∼ asd(c ∗ a + d, c ∗ b + d). see references 121 and 134 for other relationships and applications. see table 7.8 for further properties."
2409,1,"['mean', 'distributions']", Properties of ArcSine Distribution,seg_461,example 7.13 mean of arc-sine distributions
2410,1,"['distribution', 'mean']", Properties of ArcSine Distribution,seg_461,find the mean of arc-sine distribution of first kind.
2411,1,"['distribution', 'beta distribution']", Properties of ArcSine Distribution,seg_461,"this distribution is related to the beta distribution when a = 1∕2, b = 1∕2."
2412,1,"['distribution', 'deviation', 'mean']", Properties of ArcSine Distribution,seg_461,example 7.14 mean deviation of arc-sine distribution
2413,1,"['distribution', 'deviation', 'mean']", Properties of ArcSine Distribution,seg_461,find the mean deviation of the arc-sine distribution using theorem 7.1.
2414,1,['mean'], Properties of ArcSine Distribution,seg_461,"2 solution 7.14 we have seen earlier that the cdf is sin−1(√x). as the mean is   0.5, we get the md using theorem 7.1 as"
2415,1,['limit'], Properties of ArcSine Distribution,seg_461,put √x = t so that dx = 2tdt. adjust the upper limit of integration as
2416,1,"['exponential distribution', 'gamma', 'distribution', 'exponential', 'parameter', 'gamma distribution']", GAMMA DISTRIBUTION,seg_463,the two parameter gamma distribution can be considered as a generalization of the exponential distribution. its pdf is given by
2417,1,"['distribution', 'exponential', 'exponential distribution']", GAMMA DISTRIBUTION,seg_463,"when m = 1, this reduces to the exponential distribution. hence, it is considered a generalization of the exponential distribution. for m = 1∕2, we get"
2418,1,"['shape parameter', 'parameter']", GAMMA DISTRIBUTION,seg_463,"− x fx( ,m) =√ ∕ x e . the parameter   is called scale parameter and m is the shape parameter (see figure 7.8). a reparametrization as"
2419,1,"['transformation', 'change of scale']", GAMMA DISTRIBUTION,seg_463,"also exist. a change of scale transformation y =  x (so that dy =  dx) in equation (7.55) gives fy(m) = ym−1e−y∕γ(m) given below. as this form is easier to work with, it is extensively tabulated."
2420,1,"['parameters', 'gamma', 'distribution', 'gamma distribution']", Properties of Gamma Distribution,seg_465,"there are two parameters, both of which are real numbers. for   = 1, we get the one-parameter gamma distribution with pdf"
2421,1,"['erlang distribution', 'independent', 'leptokurtic', 'coefficient', 'kurtosis', 'distribution', 'skewness', 'coefficient of skewness']", Properties of Gamma Distribution,seg_465,"for m an integer, this is called erlang distribution. the coefficient of skewness and kurtosis are 1∕√m and 3(1 + 2∕m), which are both independent of . this distribution is always leptokurtic."
2422,1,"['random variables', 'parameters', 'shape parameters', 'gamma', 'variables', 'independent', 'additivity property', 'table', 'variates', 'distribution', 'shape parameter', 'exponential', 'parameter', 'random', 'gamma random variables']", Properties of Gamma Distribution,seg_465,"7.9.1.1 additivity property this distribution can be obtained as the sum of m independent exponential variates with parameter , resulting in gamma(m, ). if x and y are two independent gamma random variables with the same scale parameter and shape parameters m1 and m2, respectively, their sum x + y is distributed as gamma with the same scale parameter and m1 + m2 as shape parameter. this result can be generalized to any number of independent gamma variates as “the sum of m independent gamma variates with shape parameters mi and the same scale parameter is distributed as gamma(∑imi, ), see table 7.4 in page 269.”"
2423,1,"['functions', 'gamma', 'moments']", Properties of Gamma Distribution,seg_465,7.9.1.2 moments and generating functions the raw moments are easy to find using gamma integral. consider
2424,1,"['characteristic function', 'gamma', 'mean', 'function', 'variance']", Properties of Gamma Distribution,seg_465,"using gamma integral, this becomes γ(k + m)∕ k+m. from this, we get the mean as   = m∕  and variance  2 = m∕ 2 =  ∕ . this shows that the variance is more than the mean for     1 and vice versa. the characteristic function is"
2425,1,['range'], Properties of Gamma Distribution,seg_465,"put y = (  − it)x, so that dy = (  − it)dx. the range of integration remains the same and we get"
2426,0,[], Properties of Gamma Distribution,seg_465,"for t    . by expanding this as an infinite series (see chapter 6), we get (1 − it∕ )−m ="
2427,1,"['independent', 'standard', 'gamma', 'standard normal', 'case', 'distribution', 'normal', 'random', 'gamma distribution']", Relationships with Other Distributions,seg_467,"the 2 distribution is a special case of gamma distribution as n2 =gamma(n∕2,1∕2). symbolically, if x1,x2, · · · ,xn are independent standard normal random variables, y = x1"
2428,1,"['independent', 'gamma', 'change of variable', 'distribution', 'variable', 'gamma distribution']", Relationships with Other Distributions,seg_467,"2 + · · · + xn2 ∼ gamma(n∕2, 1∕2). if x1 ∼ gamma(a, b) and x2 ∼ gamma(c, d), are independent, then x1∕(x1 + x2) is distributed as beta-i. inverse gamma distribution is obtained by a simple change of variable y = 1∕x as"
2429,1,"['distribution', 'case', 'gamma']", Relationships with Other Distributions,seg_467,log-gamma distribution is the analog of lnd in the gamma case. the pdf is given by
2430,1,"['distribution', 'gamma distributed', 'gamma', 'probability']", Relationships with Other Distributions,seg_467,"boltzmann distribution in engineering is related to the gamma law. if the quantized energies of a molecule in an ensemble are e1,e2, · · · ,en, the probability that a molecule has energy ei is given by c exp(−ei∕(kt), where k is the boltzmann constant (gas constant divided by avogadro number) and t is the absolute temperature. the sum of the energies is gamma distributed when ei"
2431,1,"['independent', 'table']", Relationships with Other Distributions,seg_467,′s are independent. see table 7.9
2432,1,"['gamma', 'median', 'gamma distributions', 'distributions']", Relationships with Other Distributions,seg_467,for further properties. estimating the median of gamma distributions is discussed in reference 211.
2433,1,"['gamma', 'distribution', 'function', 'gamma distribution', 'gamma function']", Incomplete Gamma Function IGF,seg_469,definition 7.3 the left-tail area of gamma distribution is called the incomplete gamma function. it is given by
2434,1,"['gamma', 'parameters', 'change of scale', 'distribution', 'function', 'transformation', 'gamma distribution']", Incomplete Gamma Function IGF,seg_469,"as there are two parameters, a simple change of scale transformation mentioned earlier can be made. this gives one-parameter gamma distribution. the function γ(m) = ∫0"
2435,1,"['gamma', 'complete gamma function', 'function', 'recurrence', 'gamma function']", Incomplete Gamma Function IGF,seg_469,"∞ xm−1e−xdx is called the complete gamma function. when m is an integer, the above-mentioned integral becomes γ(m) = (m − 1)! when m is not an integer, we get the recurrence γ(m) = (m − 1) ∗ γ(m − 1) using integration by parts. the integral with and without the normalizing constant is denoted as  (x;m) ="
2436,1,['recurrence'], Incomplete Gamma Function IGF,seg_469,these satisfy the recurrence
2437,1,"['poisson', 'probabilities', 'gamma', 'shape parameter', 'parameter']", Incomplete Gamma Function IGF,seg_469,"the cdf of gamma is a sum of poisson probabilities when the shape parameter is p−1 an integer: –fx(m, p) = 1 −∑k=0 e− x( x)k∕k!. the corresponding right-tail area is"
2438,1,"['function', 'pearson']", Incomplete Gamma Function IGF,seg_469,"for example, pearson [207] tabulated the function i(x, p) = γ(p"
2439,1,"['functions', 'hypergeometric functions', 'hypergeometric', 'function', 'error']", Incomplete Gamma Function IGF,seg_469,"it has a representation in terms of confluent hypergeometric functions as  (x; 1, p) = (xp∕p)e−x 1f1(1, p + 1; x) = (xp∕p) 1f1(p, p + 1; −x) and error function as  (x2; 1, 1∕2) = erf(x). see references 121, 213–216 for other properties and relationships."
2440,0,[], COSINE DISTRIBUTION,seg_471,the pdf is given by
2441,1,"['sample', 'location parameter', 'median', 'table', 'symmetric', 'random sample', 'location', 'distribution', 'transform', 'mean', 'parameter', 'random', 'skewness', 'variance']", COSINE DISTRIBUTION,seg_471,"as this distribution is symmetric, the mean, median, and mode coincide at x = a, which is the location parameter; and skewness is zero (table 7.10). the variance depends only on b and is given by 2 = b2[ 2 − 8]. random sample generation: generate u(−1, 1), then transform x = a + 2b sin−1(2u − 1) if u = [0, 1]."
2442,1,"['parameters', 'statistics', 'gaussian distribution', 'location', 'distribution', 'information', 'normal', 'mean', 'normal distribution']", THE NORMAL DISTRIBUTION,seg_473,"the normal distribution is perhaps the most widely studied distribution in statistics. it is known by the name gaussian distribution in engineering in honor of the german mathematician carl friedrich gauss (1777–1855). it has two parameters, which are by convention denoted as   and   to indicate that they capture the location (mean) and scale information. the pdf is"
2443,1,"['population variance', 'deviation', 'standard normal distribution', 'population standard deviation', 'standard normal', 'population mean', 'distribution', 'normal', 'mean', 'population', 'standard', 'parameter', 'standard deviation', 'variance', 'normal distribution']", THE NORMAL DISTRIBUTION,seg_473,"it is denoted by n( ,  2), where the first parameter is always the population mean and second parameter is the population variance. some authors use the notation n( ,  ), where the second parameter is the population standard deviation and z(0, 1) for a standard normal distribution. even if the mean is zero, the first parameter should be specified. thus, n(0,  2) denotes a normal distribution with zero mean."
2444,1,"['sample size', 'sample', 'tail areas', 'table', 'standard normal', 'approximation', 'distribution', 'central limit theorem', 'normal', 'standard', 'transformation', 'standard normal distribution', 'tail', 'limit', 'normal distribution']", THE NORMAL DISTRIBUTION,seg_473,"any normal distribution (with arbitrary and ) can be converted into the standard normal form n(0, 1) using the transformation z = (x − )∕ . this is called “standard normalization”. it is applicable for approximation due to central limit theorem, even in nonnormal situations when the sample size is quite large. the reverse transformation is x = z + . this shows that from the table of standard normal distribution, we could obtain the tail areas of any other normal distribution."
2445,1,"['normal', 'probability']", THE NORMAL DISTRIBUTION,seg_473,example 7.15 probability of normal deviates
2446,1,"['standard deviations', 'normally distributed', 'deviations', 'mean', 'standard', 'variance']", THE NORMAL DISTRIBUTION,seg_473,the radius of a batch of pipes is known to be normally distributed with mean 0.5 inch and variance 0.009. what proportions of a batch of 132 pipes have radius more than 2 standard deviations in the higher side?
2447,1,"['deviation', 'standard deviations', 'deviations', 'standard', 'standard deviation']", THE NORMAL DISTRIBUTION,seg_473,"solution 7.15 as radius ∼ n(0.5, 0.009), standard deviation is 0.0948683. standard normalize it to get z = (x − 0.5)∕0.0948683. area above 2 standard deviations for n(0, 1) is 1 − 0.9772 = 0.0228. thus, in a batch of 132 pipes, we expect 132*0.0228 = ⌊3.0096⌋ = 3 pipes to have radius more than two standard deviations."
2448,1,"['parameters', 'cauchy', 'symmetric', 'tails', 'distribution', 'symmetry', 'normal', 'mean', 'parameter', 'standard', 'variance', 'normal distribution', 'distributions']", Properties of Normal Distribution,seg_475,"the general normal distribution has two parameters, the second of which is positive (the first parameter by convention is the mean ). the distribution is symmetric about the mean with relatively shorter tails than cauchy and t distributions (see figures 7.6 and 7.7). when the mean = 0 and the variance 2 = 1, it is called the standard normal distribution, which is denoted by z(0, 1) or simply by z. the corresponding pdf and cdf are denoted by (x) and φ(x). owing to symmetry φ(−c) = 1 − φ(c) for"
2449,1,"['mean', 'median']", Properties of Normal Distribution,seg_475,"c   0 and φ(0) = 1∕2, so that median = mean = mode with modal value 1/[ √"
2450,1,"['functions', 'symmetric', 'distribution', 'moments', 'even moments']", Properties of Normal Distribution,seg_475,"7.11.1.1 moments and generating functions as the distribution is symmetric, all odd central moments are zeros. the even moments are given by"
2451,1,"['deviation', 'mean', 'gamma']", Properties of Normal Distribution,seg_475,this can easily be proved using gamma integrals (see exercise 7.34 in p. 285). the mean deviation is  √2∕ . the mgf is easily obtained as
2452,0,[], Properties of Normal Distribution,seg_475,write the exponent as − 2
2453,0,[], Properties of Normal Distribution,seg_475,"as the integral evaluates to one, we get the desired result mx(t) = et + 2"
2454,0,[], Properties of Normal Distribution,seg_475,table 7.11 for further properties.
2455,1,"['confidence intervals', 'prediction intervals', 'prediction', 'observations', 'random sample', 'random', 'sample', 'intervals', 'mean', 'population', 'tests', 'statistical', 'confidence', 'parameters', 'statistical inference', 'estimation', 'distribution', 'normal', 'hypotheses', 'normal distribution']", Properties of Normal Distribution,seg_475,"the normal distribution is the basis of many procedures in statistical inference. these include confidence intervals for unknown parameters, prediction intervals for future observations, tests of various hypotheses, and estimation of parameters. given below is a theorem about the mean of a random sample from a normal population, which is used in ci construction and tests about the mean."
2456,1,"['sample', 'sample mean', 'normally distributed', 'normal', 'mean', 'population']", Properties of Normal Distribution,seg_475,"theorem 7.3 the sample mean xn of any sample of size n ≥ 2 from a normal population n( ,  2) is itself normally distributed as n( ,  2∕n)."
2457,1,"['distribution', 'normal', 'normal distribution']", Properties of Normal Distribution,seg_475,proof: the easiest way to prove this result is using the mgf (or chf) as follows. let mx(t) be the mgf of a normal distribution (chapter 8). then mx(t) = [mx(t∕n)]n = [e t∕n+ 1
2458,1,['normal'], Properties of Normal Distribution,seg_475,"2 (t∕n)2 2 ]n = en t∕n+ 1 2 n 2(t∕n)2 = e t+ 1 2 t2( 2∕n), which is the mgf of a normal"
2459,1,"['variance', 'mean']", Properties of Normal Distribution,seg_475,distribution with mean   and variance  2∕n. this proves the result.
2460,1,"['deviation', 'distribution', 'normal', 'mean', 'normal distribution']", Properties of Normal Distribution,seg_475,example 7.16 mean deviation of normal distribution
2461,1,"['deviation', 'distribution', 'normal', 'mean', 'normal distribution']", Properties of Normal Distribution,seg_475,find the mean deviation of the normal distribution using theorem 7.1.
2462,1,"['distribution', 'normal', 'tails', 'normal distribution']", Properties of Normal Distribution,seg_475,"solution 7.16 let x ∼ n( ,  2). as the normal distribution tails off to zero at the lower and upper limits, equation (7.1) is applicable. this gives"
2463,1,['limit'], Properties of Normal Distribution,seg_475,"put z = (x −  )∕ , so that dx =  dz. the lower limit remains the same, but the upper limit in equation (7.73) becomes 0. thus, we get"
2464,0,[], Properties of Normal Distribution,seg_475,"this integral can readily be evaluated using integration by parts. put u = φ(z) and dv = dz, so that du =  (z). this gives"
2465,0,[], Properties of Normal Distribution,seg_475,n!2n. then the integral becomes 1∕√ 2 . apply the constant 2  to get the
2466,1,"['normally distributed', 'transformation', 'sample', 'logistic', 'data', 'multiple regression', 'mean', 'population', 'tests', 'statistical', 'error', 'transformed', 'regression', 'distribution', 'normality', 'normal', 'transform', 'normal distribution']", Transformations to Normality,seg_477,"normality of parent population is a fundamental assumption in many statistical procedures. for example, error terms in logistic and multiple regression are assumed to be normally distributed with zero mean. normality tests of the sample can reveal whether the data came from a normal distribution or not. when the data are not normally distributed, a simple transformation may sometimes transform it to nearly normal form. for instance, count data are usually transformed using the square root transformation,"
2467,1,"['transformed', 'logit transformation', 'logit', 'transformation']", Transformations to Normality,seg_477,1 and proportions are transformed using logit transformation as y = log(p∕q) where
2468,1,"['linear', 'variates', 'normally distributed', 'linear combinations', 'normal', 'combinations']", Functions of Normal Variates,seg_479,"any linear combinations of normal variates are normally distributed. symbolically, if x1,x2, · · · ,xk are iid n( i,  i"
2469,1,['normally distributed'], Functions of Normal Variates,seg_479,k =1 ∓cixi is normally distributed with mean∑i
2470,0,[], Functions of Normal Variates,seg_479,k =1 ∓ci i and variance∑i
2471,1,"['normal', 'variates', 'normally distributed']", Functions of Normal Variates,seg_479,"if x and y are iid normal variates with zero means, then u = xy∕√ x2 + y2 is normally distributed [217, 218]. in addition, if  x2 =  y2, then (x2 − y2)∕(x2 + y2) is"
2472,1,"['gamma distributed', 'independent', 'gamma', 'case', 'variates', 'normally distributed', 'distribution', 'normal']", Functions of Normal Variates,seg_479,"also normally distributed. product of independent normal variates has a bessel-type iii distribution. the square of a normal variate is gamma distributed (of which  2 is a special case). in general, if x1,x2, · · · ,xk are iid n(0,  2), then∑i"
2473,0,[], Functions of Normal Variates,seg_479," 2 k distributed. as shown in page 276, if z1 = x1"
2474,1,['set'], Functions of Normal Variates,seg_479,"tributed, as also the product of any consecutive set of zj"
2475,1,['beta distributed'], Functions of Normal Variates,seg_479,′s are beta distributed [171].
2476,1,['distribution'], Functions of Normal Variates,seg_479,"normal distribution is also related to student’s t, snedecor’s f, and fisher’s z distributions [219]."
2477,1,"['probability', 'populations']", Functions of Normal Variates,seg_479,"example 7.17 probability p(x ≤ y) for two populations n( ,  i"
2478,0,[], Functions of Normal Variates,seg_479,"if x and y are independently distributed as n( , i 2), for i = 1, 2 find the probability p[x ≤ y]."
2479,1,['distribution'], Functions of Normal Variates,seg_479,"solution 7.17 as x and y are iid, the distribution of x − y is n(0,  1"
2480,1,['variances'], Functions of Normal Variates,seg_479,"hence p[x ≤ y] = p[x − y ≤ 0] = φ(0) = 0.5, irrespective of the variances."
2481,1,"['random variables', 'independent', 'variables', 'cauchy', 'cauchy distributed', 'normal', 'random']", Relation to Other Distributions,seg_481,"if x and y are independent normal random variables, then x∕y is cauchy distributed."
2482,0,[], Relation to Other Distributions,seg_481,"this is proved in chapter 10. if x is chi distributed (i.e.,√"
2483,1,"['sample size', 'sample', 'normal distributions', 'curve', 'lognormal', 'normally distributed', 'distribution', 'normal', 'binomial', 'binomial distribution', 'convergence', 'normal distribution', 'distributions']", Relation to Other Distributions,seg_481,"m2 ) and y is independently distributed as beta((b − 1)∕2, (b − 1)∕2), then the product (2 ∗ y − 1) ∗ x is distributed as n(0, 1) [220]. there are many other distributions that tend to the normal distribution under appropriate limits. for example, the binomial distribution tends to the normal curve when the sample size n becomes large. the convergence is more rapid when p → 1∕2 and n is large. the lognormal and normal distributions (section 7.14, p. 297) are related as follows: if y = log(x) is normally distributed, then x has lnd. the lnd can be obtained from a normal law using the transformation x = ey (p. 298)."
2484,1,"['standard normal', 'distribution', 'normal', 'standard', 'tail areas', 'standard normal distribution', 'tail', 'normal distribution']", Relation to Other Distributions,seg_481,7.11.4.1 tail areas the cdf of standard normal distribution is given by
2485,1,"['curve', 'symmetry', 'normal', 'function', 'error']", Relation to Other Distributions,seg_481,"k=0 (−1)kz2k+1∕[(2k + 1)k!2k]. because of the symmetry of the normal curve, the cdf is usually tabulated from 0 to some specified value x. the area of the normal curve from 0 to z is called the error function"
2486,1,"['functions', 'hypergeometric functions', 'gamma', 'change of variable', 'variable', 'complement', 'hypergeometric']", Relation to Other Distributions,seg_481,"using a simple change of variable, this can be expressed in terms of the incomplete gamma integral as√  (1∕2, z2). the complement of the above-mentioned integral is denoted by erfc(z). the erf(z) can be expressed in terms of confluent hypergeometric functions as"
2487,1,"['approximation', 'tails']", Relation to Other Distributions,seg_481,"when the ordinate is in the extreme tails, another approximation as φ(−x) ="
2488,0,[], Relation to Other Distributions,seg_481,can be used. replace −x by +x to get an analogous expression for right-tail areas.
2489,1,"['functions', 'probabilities', 'tail probabilities', 'function', 'tail', 'error']", Relation to Other Distributions,seg_481,−dx2 scaled functions of the form c ∗ e are quite accurate to approximate the tail probabilities. the error function erf(z) has an infinite series expansion as
2490,0,[], Relation to Other Distributions,seg_481,this series is rapidly convergent for small z values. see references 134 and 221.
2491,1,"['linear', 'additivity property', 'variates', 'normally distributed', 'linear combinations', 'normal', 'combinations']", Relation to Other Distributions,seg_481,"7.11.4.2 additivity property as mentioned in section 7.11.3, linear combinations of iid normal variates are normally distributed. if x ∼ n( 1,  1"
2492,1,['independent'], Relation to Other Distributions,seg_481,"are independent, then x ± y ∼ n( 1 ±  2,  1"
2493,1,"['independent', 'results', 'normally distributed', 'experiments', 'processes']", Relation to Other Distributions,seg_481,"2 +  2 2). this is an important point in practical experiments, where results from two or more processes that are independent and normally distributed need to be combined."
2494,1,"['linear', 'linear combination', 'combination', 'variates', 'normal']", Relation to Other Distributions,seg_481,example 7.18 linear combination of iid normal variates
2495,1,"['functions', 'variance', 'mean']", Relation to Other Distributions,seg_481,"x ∼ n(10, 3),y ∼ n(15, 6), and z ∼ n(9, 2.5), find the mean and variance of the following functions: (i) u = x − 2y + 3z and (ii) v = 2x − 1.2y − z."
2496,1,"['combination', 'linear', 'linear combination', 'case']", Relation to Other Distributions,seg_481,"solution 7.18 use the linear combination property to get e(u) = 10 − 2 ∗ 15 + 3 ∗ 9 = 37 − 30 = 7, var(u) = 3 + 6 + 2.5 = 11.5 so that u ∼ n(7, 49.5) in the second case, e(v) = 2 ∗ 10 − 1.2 ∗ 15 − 9 = −7 and var(v) = 4 ∗ 3 + 4 ∗ 6 + 9 ∗ 3.5 = 23.14."
2497,1,"['interval', 'method', 'standard normal', 'normal', 'standard']", Algorithms,seg_483,"random variate generation: if x is uniform in the interval [0,1] then z = √−2 log x cos(2 x) is distributed as standard normal. this is known as box–muller method [22, 222]."
2498,1,"['distribution', 'normal', 'normal distribution']", Algorithms,seg_483,example 7.19 truncated normal distribution pdf
2499,1,"['distribution', 'normal distribution', 'normal']", Algorithms,seg_483,prove that the pdf of an asymmetrically truncated normal distribution with truncation points a and b is
2500,1,['asymmetric'], Algorithms,seg_483,"solution 7.19 as the truncation point is asymmetric, the area enclosed is ∫a b  ((x −  )∕ )dx. put z = (x −  )∕ , so that dz = dx∕ . the limits are changed as (a −  )∕  and (b −  )∕ . thus"
2501,0,[], Algorithms,seg_483,dividing by this quantity gives the pdf as desired.
2502,1,"['cauchy', 'symmetric', 'unimodal', 'cauchy distribution', 'distribution']", CAUCHY DISTRIBUTION,seg_485,"the cauchy distribution is named after the french mathematician a. l. cauchy(1789–1857), although it was known to fermat and newton much earlier. it is symmetric, unimodal and has the general pdf"
2503,1,"['cauchy', 'location', 'parameter', 'standard', 'location parameter']", CAUCHY DISTRIBUTION,seg_485,the location parameter is “a” and scale parameter is “b.” the standard cauchy distribution (scd) is obtained from the above by putting a = 0 and b = 1:–
2504,0,[], CAUCHY DISTRIBUTION,seg_485,the cdf of scd is
2505,1,"['distribution', 'cauchy', 'cauchy distribution']", CAUCHY DISTRIBUTION,seg_485,"and that of general cauchy distribution is given by fx(a, b) ="
2506,1,['mean'], Properties of Cauchy Distribution,seg_487,"∞ ∞ x∕(1 + x2)dx does not exist, the mean is undefined. the limiting lt value 1 ∫−"
2507,1,"['function', 'characteristic function']", Properties of Cauchy Distribution,seg_487,x 2 dx is zero. the characteristic function is  r → ∞ 1+x
2508,1,"['cauchy', 'cauchy distribution', 'distribution', 'tails']", Properties of Cauchy Distribution,seg_487,"median and mode of the general cauchy distribution coincide at x = a, with modal value 1∕(b ). if the distribution is truncated in both tails at x = c, the resulting pdf"
2509,1,"['variance', 'mean']", Properties of Cauchy Distribution,seg_487,1 1 is for −c ≤ x ≤ +c. this has mean 0 and variance c∕tan−1(c) − 1. see
2510,0,[], Properties of Cauchy Distribution,seg_487,(c3 ) f1o+rx2 further properties.
2511,1,"['random variables', 'independent', 'variables', 'cauchy', 'cauchy distributed', 'random']", Functions of Cauchy Variate,seg_489,"if x1,x2, · · ·xn are independent cauchy distributed random variables, then"
2512,1,"['cauchy', 'states', 'law of large numbers', 'distribution', 'mean', 'probability', 'central limit theorem', 'limit']", Functions of Cauchy Variate,seg_489,"x = (x1 + x2 + · · · + xn)∕n is also distributed as cauchy. an implication of this result is that the cauchy mean does not obey the central limit theorem or the law of large numbers (the law of large numbers states that sn∕n for scd converges to   in probability, and the clt states that the distribution of sn∕n tends to n(0, 1) as n → ∞)."
2513,1,"['functions', 'independent', 'additivity property', 'cauchy', 'cauchy distributed', 'uniform distribution', 'variates', 'distribution', 'function']", Functions of Cauchy Variate,seg_489,"cauchy distribution is related to the uniform distribution u(0, 1) through the tangent function tan( ). more precisely, if u ∼ u(0, 1), then tan( u) has a cauchy distribution. this allows us to find tangent functions of cauchy distributed variates like 2x∕(1 − x2), (3x − x3)∕(1 − 3x2), (4x − x4)∕(1 − 6x2 + x4), and so on, which are, respectively, tan(2x), tan(3x), tan(4x), and so on. these are all cauchy distributed [223–225]. if x and y are iid cauchy variates, (x − y)∕(1 − xy) and (x − 1)∕(x + 1) are identically distributed. it satisfies an additivity property: if x1 ∼ cauchy(0, b1) and x2 ∼ cauchy(0, b2) are independent, then x1 + x2 ∼ cauchy(0, b1 + b2)."
2514,1,"['cauchy', 'cauchy distributed', 't distribution', 'cauchy distribution', 'distribution']", Relation to Other Distributions,seg_491,"the student’s t distribution with 1 dof is identically cauchy distribution. if z is cauchy distributed, then (a + bz) is cauchy distributed."
2515,0,[], Relation to Other Distributions,seg_491,many other similar relationships can be found in arnold [224].
2516,1,['distribution'], INVERSE GAUSSIAN DISTRIBUTION,seg_493,this is also called wald’s distribution (see figure 7.9). the pdf takes many forms
2517,0,[], INVERSE GAUSSIAN DISTRIBUTION,seg_493,expand (x −  )2 as a quadratic and divide each term by  x to get
2518,0,[], INVERSE GAUSSIAN DISTRIBUTION,seg_493,take the constant in the exponent as a separate multiplier and put  ∕  =   to get another form
2519,1,"['standard normal', 'normal', 'standard']", INVERSE GAUSSIAN DISTRIBUTION,seg_493,the cdf is expressible in terms of the standard normal cdf as
2520,1,['range'], INVERSE GAUSSIAN DISTRIBUTION,seg_493,7.13.0.1 properties of igd multiply equation (7.90) by eitx and integrate over the range to get
2521,1,"['deviation', 'coefficient', 'mean', 'skewness', 'variance', 'coefficient of skewness']", INVERSE GAUSSIAN DISTRIBUTION,seg_493,"the mean is  , but the mode depends on both  ,   as  ((1 + 9∕(4 2))1∕2 − 3∕(2 )) where   =  ∕ . the variance also depends on both  ,   as  2 =  3∕  =  2∕ . the coefficient of skewness is 3√ ∕  so that a practitioner can choose between a variety of distributional shapes. the mean deviation is given by"
2522,1,"['linear', 'table', 'linear combinations', 'combinations']", INVERSE GAUSSIAN DISTRIBUTION,seg_493,"this can be simplified as given in table 7.14. linear combinations of igd are igd distributed (see summary table 7.4 (p. 297)). in particular, if xi’s are igd( i,  i) then"
2523,1,"['kurtosis', 'leptokurtic']", INVERSE GAUSSIAN DISTRIBUTION,seg_493,n =1 i∕ i)2). the kurtosis is 3 + 15( ∕ ) showing that it is always leptokurtic. writing the exponent as ( x−
2524,1,['distribution'], INVERSE GAUSSIAN DISTRIBUTION,seg_493,  → ∞ this becomes (−1)2 = 1. the resulting distribution is called one-parameter igd:–
2525,1,"['gamma', 'standard normal', 'distribution', 'normal', 'standard', 'distribution gamma', 'gamma distribution']", Relation to Other Distributions,seg_495,"if x ∼ igd( ,  ) then y =  (x −  )2∕( 2x) has chi-square distribution. if   is held constant and   → ∞, igd( ,  ) approaches a gamma distribution gamma( ∕2, 1∕2). when   = 1, the cdf can be expressed in terms of standard normal cdf as"
2526,0,[], Relation to Other Distributions,seg_495,see reference 226 for other approximations.
2527,1,"['table', 'moments']", Relation to Other Distributions,seg_495,"the moments and inverse moments are related as e(x∕ )−r = e(x∕ )r+1, where negative index denotes inverse moments. see table 7.14 for further properties."
2528,1,"['distribution', 'model']", LOGNORMAL DISTRIBUTION,seg_497,"lnd arises in a variety of applications. for example, rare-earth elements and radioactivity, micro-organisms in closed boundary regions, solute mobility in plant cuticles, pesticide distribution in farm lands, time between infection and appearance of symptoms in certain diseases, file sizes on hard disks, and so on follow approximately the lnd. it also has applications in insurance and economics. it is the widely used parametric model in mining engineering for low-concentration mineral deposits."
2529,1,"['distribution', 'normal', 'transformation', 'normal distribution']", LOGNORMAL DISTRIBUTION,seg_497,1 −x2∕2 it is obtained from the normal distribution √ 2  e using the transformation y =
2530,1,['transformed'], LOGNORMAL DISTRIBUTION,seg_497,ex or equivalently x = log(y). this means that the transformed variate is lognormally
2531,1,"['lognormal', 'lognormal distribution', 'normally distributed', 'distribution', 'normal', 'standard']", LOGNORMAL DISTRIBUTION,seg_497,"distributed. it is important to remember that if x is normally distributed, log(x) is not lognormal (a normal variate extends from −∞ to ∞, but logarithm is undefined for negative argument). this gives  x∕ y = 1∕y, so that the pdf of standard lognormal distribution becomes"
2532,1,"['distribution', 'lognormal distribution', 'lognormal']", LOGNORMAL DISTRIBUTION,seg_497,the general form of the lognormal distribution is easily obtained as
2533,1,"['lognormal', 'probabilities', 'lognormal distribution', 'distribution', 'normal', 'tail probabilities', 'mean', 'transformation', 'tail', 'variance', 'normal distribution']", LOGNORMAL DISTRIBUTION,seg_497,"here,   and  2 are not the mean and variance of lognormal distribution but that of the underlying normal law (from which lnd is obtained by the transformation y = ex). tail probabilities can be easily evaluated using the cdf of a normal distribution. for instance, if y ∼ lognormal(0,1) then p[y   y0] = pr[z   ln(y0)] = 1 − φ(ln(y0))."
2534,1,"['distribution', 'function', 'parameter']", Properties of Lognormal Distribution,seg_499,this distribution and ig distribution are somewhat similar shaped for small parameter values (figure 7.10). the cdf can be expressed in terms of erf() function as
2535,1,"['lognormal', 'lognormal distributions', 'standard normal', 'normal', 'mean', 'standard', 'distributions']", Properties of Lognormal Distribution,seg_499,"from this, it is easy to show that the area from the mode to the mean of an lnd is φ( ) − φ(− ), where φ() denotes the cdf of standard normal. this result can be used to characterize lognormal distributions."
2536,1,"['lognormal', 'quantile', 'standard normal', 'quantiles', 'normal', 'standard']", Properties of Lognormal Distribution,seg_499,"the quantiles of standard normal and lognormal are related as qp(x) = exp(  +  zp(z)), where zp denotes the corresponding quantile of standard normal variate. replace p by p + 1 and divide by the above-mentioned expression to get"
2537,1,"['lognormal', 'linear', 'cumulants', 'independent', 'approximation', 'lognormal distribution', 'distribution', 'moments', 'noncentral']", Properties of Lognormal Distribution,seg_499,"the sum of several independent lnds can be approximated by a scaled lnd. a first-order approximation can be obtained by equating the moments of linear combination with target lognormal distribution as done by patnaik [198] for noncentral 2 distribution. as the cumulants of lnd are more tractable, we could equate the cumulants and obtain a reasonable approximation. there are many other approaches for this purpose. see, for example, references 227–229, and so on."
2538,1,"['distribution', 'lognormal distribution', 'lognormal']", Properties of Lognormal Distribution,seg_499,example 7.20 mode of lognormal distribution
2539,1,['unimodal'], Properties of Lognormal Distribution,seg_499,prove that lnd is unimodal with the mode at exp(  −  2). what is the modal value?
2540,1,['mean'], Moments,seg_501,the mean is   = e + 2
2541,1,['variance'], Moments,seg_501,1  2 and variance  2 = e2 + 2 (e 2 − 1) = e2  (  − 1) where
2542,1,"['lognormal', 'location', 'random sample', 'random', 'sample', 'mean', 'parameter', 'limit', 'parameters', 'distribution', 'central limit theorem', 'variance', 'random variables', 'independent', 'variables', 'normality', 'normal', 'normal distribution']", Moments,seg_501,"= e 2 . the ratio ∕ 2 simplifies to (e 2 − 1). while the variance of the general normal distribution is given by a single-scale parameter 2, the variance of lognormal distribution depends on both the location and scale parameters and 2. as this distribution in the “logarithmic scale” reduces to the normal law, many of the additive properties of the normal distribution have multiplicative analogs for the lnd. for example, the additive form of the central limit theorem that asserts that the mean of a random sample tends to normality for increasing values of n can be stated for ln() as follows: if x1,x2, · · · ,xn are independent lognormal random variables with finite e(log(xi)), then z = (log(sn) − n ∗ e[log(xi)])∕( n ∗ var(log(xi)))1∕2 asymptotically approaches normality, where sn is the product of the xi"
2543,1,['table'], Moments,seg_501,′s. see table 7.15 for
2544,1,"['lognormal', 'deviation', 'lognormal distribution', 'distribution', 'mean']", Moments,seg_501,example 7.21 mean deviation of lognormal distribution
2545,1,"['deviation', 'mean']", Moments,seg_501,find the mean deviation of lnd using theorem 7.1.
2546,1,"['lognormal', 'lognormal distribution', 'distribution', 'tails']", Moments,seg_501,"solution 7.21 let x∼ln( ,  2). as the lognormal distribution tails off to zero at the lower and upper limits, equation (7.1) is applicable. this gives"
2547,1,['limit'], Moments,seg_501,"put z = ((ln(x) −   −  2∕2), so that dx = ez+ + 2∕2dz. the lower limit in equation (7.100) becomes −∞, and the upper limit is 0 because ln(c) =   + 1 2 2. thus, we get md ="
2548,1,['limit'], Moments,seg_501,"now put (z∕  −  ∕2) =   so that dz =  dv. the upper limit remains the same, but the lower limit becomes − ∕2. upon substitution, the   cancels out from the numerator and denominator and equation (7.103) becomes"
2549,0,[], Moments,seg_501,substitute in equation (7.102) to get the md as
2550,1,"['curve', 'normal']", Moments,seg_501,"divide the area under the normal curve from −∞ to − ∕2,− ∕2 to + ∕2, and from + ∕2 to +∞. we notice that as the total area is unity, the expression (7.105) is simply the middle area from− ∕2 to+ ∕2. hence, it becomes 2∗ φ( ∕2) − 1. substitute for the bracketed expression to get the md as"
2551,1,"['lognormal', 'geometric mean', 'variates', 'mean', 'geometric']", Moments,seg_501,example 7.22 geometric mean of iid lognormal variates
2552,1,"['lognormal', 'random variables', 'independent', 'variables', 'distribution', 'random']", Moments,seg_501,"if x1,x2, · · · ,xn are independent lognormal random variables ln( ,  2), find the distribution of the gm = (x1 ∗ · · · ∗ xn)1∕n."
2553,1,"['lognormal', 'normally distributed', 'normal', 'transformation']", Moments,seg_501,"solution 7.22 as xi is lnd, log(xi) are normally distributed. taking log gives y = log(gm) = (log(x1) + · · · + log(xn))∕n. each component in this expression is normal n( ,  2), so that y is n( ,  2∕n). taking the inverse transformation x = ey shows that gm is lognormal ln( ,  2∕n)."
2554,1,"['distribution', 'expectation', 'lognormal distribution', 'lognormal']", Moments,seg_501,"7.14.2.1 partial expectation of lognormal distribution the partial expectation of lnd has applications in economics, finance, and insurance. it is defined as"
2555,1,['function'], Moments,seg_501,consider the survival function form (7.9) of md as
2556,0,[], Moments,seg_501,"using l’hospital’s rule, the first expression inside the bracket reduces to − s( ). divide throughout by 2 and rearrange equation (7.109) to get"
2557,1,['expectation'], Moments,seg_501,"depending on whether k     or k    , the integral between them can be expressed in terms of φ(). this shows that the partial expectation of lnd is related to the md through the sf value at  ."
2558,1,['mean'], Fitting Lognormal Distribution,seg_503,we have seen earlier that the mean e(x) = e + 2
2559,1,['variance'], Fitting Lognormal Distribution,seg_503,1  2 and variance v(x) = e2 + 2 (e 2 −
2560,1,"['sample size', 'sample', 'parameters', 'sample mean', 'estimates', 'mean']", Fitting Lognormal Distribution,seg_503,"1). take log and solve for   and  2 to get   = ln(e(x)) − 0.5*ln(1+ (var(x)∕e(x)2)), and  2 = ln(1+(var(x)∕e(x)2)). if the sample size is sufficiently large, we could replace e(x) by the sample mean xn, and var(x) by s2n, and obtain estimates of the unknown parameters."
2561,1,"['distribution', 'pareto']", PARETO DISTRIBUTION,seg_505,"this distribution is named after the italian economist vilfredo pareto (1848–1923), who studied the income distribution of populace during his lifetime. the pdf is ckcx−(c+1) where x ≥ k, c 0 are constants [230]. for income and wealth distributions, the constant c is greater than 1 (and near 2.0 in developed countries)."
2562,1,"['coefficient of variation', 'median', 'distribution', 'variation', 'function', 'coefficient']", Properties of Pareto Distribution,seg_507,"the survival function of this distribution takes the simple form s(x) = (k∕x)c. the median is given by k21∕c and coefficient of variation is 1∕√c(c − 2), which is independent of k."
2563,1,"['functions', 'moments']", Properties of Pareto Distribution,seg_507,7.15.1.1 moments and generating functions the ordinary and inverse moments are easy to find. moments higher than c do not exist.
2564,1,"['distribution', 'moments', 'pareto', 'pareto distribution']", Properties of Pareto Distribution,seg_507,example 7.23 moments of pareto distribution
2565,1,"['moment', 'distribution', 'pareto', 'pareto distribution']", Properties of Pareto Distribution,seg_507,prove that the rth moment of pareto distribution is  r = c ∗ kr∕(c − r) for r   c.
2566,1,['range'], Properties of Pareto Distribution,seg_507,"solution 7.23 as the range of x is from k to ∞, we have"
2567,1,"['median', 'factor', 'pareto distributions', 'change of origin', 'recurrence relation', 'pareto', 'mean', 'transformation', 'change of origin and scale', 'variance', 'recurrence', 'distributions']", Properties of Pareto Distribution,seg_507,"as the integrand is a power of x, it converges for r   c to get ckc∕(r − c)(0 − kr−c). the kc cancels out giving  r = c ∗ kr∕(c − r). take c as a common factor from denominator to get  r = kr(1 − r∕c)−1. this gives the recurrence relation  r+1 =  r ∗ k ∗ (c − r)∕(c − r − 1). from this, the mean and variance are easily obtained as   = kc∕(c − 1),  2 = k2c∕[(c − 1)2(c − 2)]. the generalized pareto distributions are obtained by change of origin and scale transformation. this has cdf 1 − (k∕(x + b))c and mode k. median is k ∗ 21∕c."
2568,1,"['deviation', 'distribution', 'pareto', 'mean', 'pareto distribution']", Properties of Pareto Distribution,seg_507,example 7.24 mean deviation of pareto distribution
2569,1,"['deviation', 'distribution', 'pareto', 'mean', 'pareto distribution']", Properties of Pareto Distribution,seg_507,"find the mean deviation of the pareto distribution f (x, k, c) = ckcx−(c+1) where x ≥ k."
2570,1,"['distribution', 'pareto', 'tail', 'limit', 'pareto distribution']", Properties of Pareto Distribution,seg_507,"solution 7.24 we apply theorem 7.1 (p. 256) to find the md. note that the pareto distribution is defined for x ≥ k. at x = k, the functional value is 1∕k. as the pdf does not tail off to zero at the lower limit (i.e., at k), equation (7.1) seems like inapplicable. we know that the cdf is 1 − (k∕x)c. if we apply l’hospital’s rule once on x ∗ f(x), we get 1 [1 − (k∕x)c]+x[ckc x−c−1]. the first term → 0 as x → k, whereas the second term tends to c. we need to use the equation (7.7). however, the term x f(x)∣ll=k → 0, so that it reduces to"
2571,0,[], Properties of Pareto Distribution,seg_507,d x−cdx}. the expression inside the square bracket simplifies to
2572,1,['table'], Properties of Pareto Distribution,seg_507,k∕(c − 1) and the integral simplifies to (k∕(1 − c))(c − 1)c−1∕cc−1 + k∕(1 − c). the term k∕(1 − c) cancels with first term k∕(c − 1). take c outside from the second expression to get md = 2k(1 − (1∕c)c−1)/(c-1). see table 7.16 for further properties.
2573,1,"['exponentially', 'independent', 'exponential distribution', 'gamma', 'pareto distribution', 'results', 'pareto distributions', 'discrete', 'distribution', 'exponential', 'pareto', 'function', 'exponentially distributed', 'gamma distribution', 'distributions']", Relation to Other Distributions,seg_509,"this distribution is related to exponential distribution as follows: if y is exponentially distributed, then x = k ∗ exp(y∕c) has pareto distribution. the zipf distribution is the discrete analog of pareto distribution. as c → ∞, the pdf approaches dirac’s   function. left truncation results in pareto distributions. the sum of the logarithm of several independent scaled pareto distributions has a gamma distribution. see references 122, 230–233 for further properties."
2574,1,"['random number', 'random numbers', 'method', 'random']", Algorithms,seg_511,"as the cdf is 1 − (k∕x)c, it is easy to generate random numbers using the inverse-cdf method. let u be a uniform random number. equate u = 1 − (k∕x)c and solve for x to get (1 − u)1∕c = k∕x or x = k∕(1 − u)1∕c."
2575,1,"['distribution', 'laplace distribution', 'control', 'quality control', 'error']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,"this distribution, invented by pierre laplace in 1774, has many applications in quality control, error modeling (called laplacian noise), and inventory control (especially of slow moving items). it is also called laplace distribution."
2576,1,"['standard', 'location parameter', 'parameter', 'location']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,"where “a” is the location parameter and b is the scale parameter. the standard form is obtained by putting a = 0, b = 1 as f (z) = e−|z|."
2577,1,"['exponential distribution', 'distribution', 'double exponential', 'moments', 'exponential', 'mean', 'double exponential distribution', 'variance', 'distributions']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,"7.16.0.1 properties of double exponential distribution this distribution is symmetric around x = a (see figure 7.11). hence 1 and all odd moments are zeros. the mean and variance are = a, and 2 = 2b2. when a = 0, the resulting distribution and that of two iid exponential(b) distributions are the same."
2578,1,"['double exponential', 'moments', 'exponential', 'even moments']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,example 7.25 even moments of double exponential
2579,1,"['double exponential', 'moments', 'exponential', 'even moments']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,prove that the even moments of double exponential are given by  2k = (2k)!b2k.
2580,1,"['moment', 'symmetric', 'distribution', 'moments']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,"solution 7.25 as the distribution is symmetric, the odd moments are all zeros. the even moment is given by  2k ="
2581,0,[], DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,"as   = a, put y = (x − a) and change the limits accordingly."
2582,1,['gamma'], DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,"put −y = t in the first integral. then, it becomes the second integral. hence  2k = 2 ∫0 ∞ y2k exp(−y∕b)dy. write y2k as y(2k+1)−1. using gamma integral, this"
2583,0,[], DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,"by integrating with respect to x, we get the cdf as"
2584,1,"['distribution', 'laplace distribution', 'standard']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,the standard form of the laplace distribution is obtained by putting a = 0 and b = 1 in the above.
2585,1,"['function', 'characteristic function']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,the characteristic function is obtained easily as follows:
2586,1,"['variable', 'range']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,"by splitting the range of integration from −∞ to a and from a to ∞ and changing the variable as y = (x − a)∕b, we get"
2587,1,"['functions', 'table', 'cauchy', 'laplace and cauchy distributions', 'characteristic functions', 'mean', 'variance', 'distributions']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,"from which the mean and variance can be obtained easily as   = a and  2 = 2b2. for a = 0, b = ±1, this becomes (t) = 1∕(1 + t2), which shows that laplace and cauchy distributions are related through characteristic functions. see table 7.17 for further properties."
2588,1,"['deviation', 'distribution', 'mean', 'laplace distribution']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,example 7.26 mean deviation of laplace distribution
2589,1,"['deviation', 'distribution', 'mean', 'laplace distribution']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,find the mean deviation of the laplace distribution using theorem 7.1.
2590,1,"['distribution', 'laplace distribution', 'tails']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,"solution 7.26 let x∼laplace(a, b). as the laplace distribution tails off to zero at the lower and upper limits, equation (7.1) is applicable. this gives"
2591,1,['table'], DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,"(see table 7.17, the cdf line). put z = (a − x)∕b, so that dx = −bdz. when x = a, z = 0; but when x = −∞, z becomes +∞. cancel out the 2 to get"
2592,1,"['deviation', 'mean']", DOUBLE EXPONENTIAL DISTRIBUTION,seg_513,this shows that the mean deviation is b.
2593,1,"['independent', 'variates', 'distribution', 'exponential', 'standard', 'laplace distribution']", Relation to Other Distributions,seg_515,"the standard laplace distribution is the distribution of the difference of two independent exponential variates. in general, if xi are two independent exp( ) variates, then y = x1 − x2 has a laplace distribution. the ratio of two iid laplace variates l(0, b1) and l(0, b2) is a central f(2, 2) variate."
2594,1,"['confidence intervals', 'multinomial', 'contingency table', 'predicted', 'sample', 'multinomial distribution', 'data', 'intervals', 'sample variance', 'confidence', 'model', 'distribution', 'pearson', 'variance', 'table']", CENTRAL   DISTRIBUTION,seg_517,this distribution has a long history dating back to 1838 when bienayme obtained it as the limiting form of multinomial distribution (see figure 7.12). it was used by karl pearson for contingency table analysis during 1900. it is also used in testing goodness of fit between observed data and predicted model and in constructing confidence intervals for sample variance.
2595,1,"['random variables', 'random', 'independent', 'standard normal random variables', 'variables', 'standard normal', 'normal', 'standard']", CENTRAL   DISTRIBUTION,seg_517,"if x1,x2, … ,xn are independent standard normal random variables, the distribution of y = x1"
2596,1,"['distribution', 'parameter']", CENTRAL   DISTRIBUTION,seg_517,2 + · · · + xn2 is called the central  2 distribution. it has only one parameter called dof (n) with pdf
2597,1,"['gamma', 'case', 'distribution', 'gamma distribution']", CENTRAL   DISTRIBUTION,seg_517,"it is a special case of the gamma distribution gamma(n∕2, 1∕2)."
2598,1,['distribution'], CENTRAL   DISTRIBUTION,seg_517,the distribution of√ 2 is known as chi distribution and has pdf
2599,0,[], Properties of Central   Distribution,seg_519,"integrating equation (7.121) with respect to x, we get the cdf as"
2600,1,"['gamma', 'change of variable', 'variable', 'function', 'recurrence', 'gamma function']", Properties of Central   Distribution,seg_519,"make the change of variable u = y∕2 to get the above in terms of incomplete gamma function as fn(x) =  (n∕2, x∕2)∕γ(n∕2) = p(n∕2, x∕2). it satisfies the recurrence fn(x) − fn−2(x) = −2 ∗ fn(x)."
2601,1,"['functions', 'moments']", Properties of Central   Distribution,seg_519,7.17.1.1 moments and generating functions the mgf is easily obtained as
2602,1,['moment'], Properties of Central   Distribution,seg_519,"from this, the kth moment is easily obtained as  k"
2603,1,"['variance', 'mean', 'table']", Properties of Central   Distribution,seg_519,′ = 2kγ(n∕2 + k)∕γ(n∕2). the mean is n and variance is 2n. see table 7.18 for further properties.
2604,1,"['distribution', 'deviation', 'mean']", Properties of Central   Distribution,seg_519,example 7.27 mean deviation of central  2 distribution
2605,1,"['distribution', 'deviation', 'mean']", Properties of Central   Distribution,seg_519,find the mean deviation of the central  2 distribution with pdf fx(n) = xn∕2−1e−x∕2∕[2n∕2γ(n∕2)].
2606,1,"['distribution', 'tail', 'limit']", Properties of Central   Distribution,seg_519,"solution 7.27 we apply theorem 7.1 (p. 257) to find the md. as the  2 distribution does not tail off to zero at the lower limit (i.e., at 0) for n   3, equation (7.1) seems like not applicable. we know that the cdf is p(n∕2, x∕2). if we apply l’hospital’s rule once on x ∗ f(x), we find that it → 0 as x → 0. as the lim x ∗ f(x) = 0, and the theorem 7.1 becomes applicable. x→0 this gives md = 2 ∫0"
2607,1,"['critical values', 'median']", Properties of Central   Distribution,seg_519,see references 234 and 235 for properties including the median of 2 and reference 236 for critical values.
2608,1,"['additivity property', 'independent', 'table', 'distribution', 'noncentral']", Properties of Central   Distribution,seg_519,"7.17.1.2 additivity property this distribution in particular and its noncentral version in general satisfies a reproductive property given below. if x ∼ 2m and y ∼ 2n are independent, then x + y ∼ 2m+n. this result was proved by helmert [238]. see table 7.4."
2609,1,"['standard normal', 'normal', 'mean', 'standard', 'variance']", Properties of Central   Distribution,seg_519,7.17.1.3 approximations wilson and hilferty proved that ( n2∕n)1∕3 is approximately normal with mean (1 − 2∕(9n)) and variance 2∕(9n). this allows the cdf to be expressed in terms of standard normal cdf as
2610,0,[], Properties of Central   Distribution,seg_519,"this has been extended by many researchers [239, 240]."
2611,0,[], Relationships with Other Distributions,seg_521,"if x ∼  2n, then c ∗ x for c   0 is gamma(n∕2, 2c). it is related to the u = u(0, 1) as −2 log(u) ∼  2"
2612,1,"['poisson', 'distribution', 'normal', 'distributions']", Relationships with Other Distributions,seg_521,"2. the sf of a chi-square distribution is related to the poisson cdf (section 6.9 in p. 229). it is also related to the normal, beta, t, and f distributions (see references 239, 241–245)."
2613,1,"['confidence intervals', 'correlation', 'significance', 'coefficients', 'correlation coefficients', 'sample', 'population variance', 'intervals', 'population', 'standard', 'tests', 'statistical', 'confidence', 'statistical inference', 'standard normal', 'distribution', 'variance', 'normal']", STUDENTS T DISTRIBUTION,seg_523,"this distribution was obtained by william gosset [246] as the distribution of the ratio z∕√ 2(n)∕n where z is a standard normal variate and z and 2(n) are independent. a derivation is given in chapter 11. it is frequently encountered in small sample statistical inference when population variance is unknown. it is used in tests for the means, in testing the significance of correlation coefficients, in constructing confidence intervals, and so on."
2614,1,"['sample size', 'sample', 'statistical inference', 'adjusted', 'parameter', 'statistical']", STUDENTS T DISTRIBUTION,seg_523,"it has a single parameter n called the dof (df), which was described in chapter 3. theoretically, n need not be an integer. as n represents the sample size adjusted for “loss of information,” it is always an integer in statistical inference. the pdf is given by"
2615,1,"['symmetric', 'change of origin', 'function', 'transformation', 'change of origin and scale']", STUDENTS T DISTRIBUTION,seg_523,"where k = γ((n + 1)∕2)∕[√n  γ(n∕2)]. as it is an even function of t, it is always symmetric around t = 0. the more general form is obtained by a change of origin and scale transformation t = (y −  )∕c."
2616,1,"['standard', 'unimodal', 'symmetric', 'standard normal', 'tails', 'distribution', 'normal', 'mean', 'parameter', 'standard normal distribution', 'variance', 'normal distribution']", Properties of Students T Distribution,seg_525,"it is always symmetric, unimodal with mode t = 0. the modal value is (γ(n + 1)∕2)∕[√n γ(n∕2)]. the mean = 0 if n 1 and does not exist otherwise. it has a single parameter n, which controls both the spread and peakedness. for higher values of n, the flatness in the tails decreases, and the peakedness increases. eventually, it coincides with the standard normal distribution for large n. the variance is n∕(n − 2) if n 2."
2617,1,"['distribution', 'coefficient', 'kurtosis']", Properties of Students T Distribution,seg_525,"the distribution is concave upward for |t|   −√(n∕(n + 2)) and concave 6 downward otherwise. for n   4, the kurtosis coefficient is given by  2 = 3 + n−4 ,"
2618,1,['leptokurtic'], Properties of Students T Distribution,seg_525,showing that it is always leptokurtic. write the pdf in equation (7.132) as
2619,1,"['standard normal', 'distribution', 'normal', 'asymmetric', 'standard', 'distributions']", Properties of Students T Distribution,seg_525,"2 as n → ∞. this shows that the limiting distribution is a standard normal. by factoring the pdf into two asymmetric products, jones ; faddy obtained the skew-t distributions"
2620,1,"['tests', 'statistical tests', 'distribution', 'statistical', 'tail areas', 'tail']", Properties of Students T Distribution,seg_525,"where k is given by 1∕k = c1∕22c−1b(a, b) and c = (a + b) [247]. see references 146, 248–251 for tail areas, reference 252 for the distribution of the difference of two t-variables, and reference 253 for applications to statistical tests."
2621,1,"['deviation', 't distribution', 'distribution', 'mean']", Properties of Students T Distribution,seg_525,example 7.28 mean deviation of student’s t distribution
2622,1,"['deviation', 't distribution', 'distribution', 'mean']", Properties of Students T Distribution,seg_525,find the mean deviation of student’s t distribution using theorem 7.1.
2623,1,['symmetric'], Properties of Students T Distribution,seg_525,"solution 7.28 let k = γ((n + 1)∕2)∕[√n γ(n∕2)]. as the student’s t distribution is symmetric around zero, the md is given by"
2624,0,[], Properties of Students T Distribution,seg_525,"put t2 = n tan2( ) so that t =√n tan( ), and dt =√n sec2( )d . the limits of integration become 0 to  ∕2 and we get"
2625,0,[], Properties of Students T Distribution,seg_525,"put cos( ) = t so that sin( )d  = −dt, and the limits are changed as 1 to 0; and"
2626,1,"['function', 'complete beta function', 'beta function']", Properties of Students T Distribution,seg_525,"as the integral evaluates to 1∕(n − 1), md = 2√n∕[(n − 1)b(1∕2, n∕2)]. expand the complete beta function b(1∕2, n∕2) = γ(1∕2)γ(n∕2)∕γ((n + 1)∕2) and write γ((n + 1)∕2) = ((n − 1)∕2)γ((n − 1)∕2). one (n − 1) cancels out from numerator and denominator giving the alternative expression"
2627,1,"['t distribution', 'distribution', 'tails']", Properties of Students T Distribution,seg_525,"√n∕ γ((n − 1)∕2)∕γ(n∕2). next we apply the theorem 7.1 to find the md. as the student’s t distribution tails off to zero at the lower and upper limits, equation (7.1) is applicable. this"
2628,0,[], Properties of Students T Distribution,seg_525,as the sf can be expressed in terms of the ibf as
2629,1,['table'], Properties of Students T Distribution,seg_525,"where y = t2∕(n + t2), and sign(t) = −1 for t   0 (see table 7.19, the cdf line), equation (7.139) becomes"
2630,0,[], Properties of Students T Distribution,seg_525,the first term is zero using l’hospital’s rule. take 2n outside the integral to get
2631,1,"['distribution', 'cauchy', 'cauchy distribution']", Relation to Other Distributions,seg_527,"for n = 1, it reduces to the cauchy distribution. if x and y are iid  2-distributed"
2632,1,['variables'], Relation to Other Distributions,seg_527,"random variables with the same dof, then (√n∕2)(x − y)∕√"
2633,1,['variates'], Relation to Other Distributions,seg_527,"tributed [254]. if x is an f variates with ndf , then t = √"
2634,1,"['distribution', 'normal', 'normal distribution']", Relation to Other Distributions,seg_527,"n (√ x − 1∕√ x) is student’s t(n). the analog of log normal to normal distribution is the log-student’s t distribution as f (y, n) ="
2635,1,"['distribution', 't distribution']", Relation to Other Distributions,seg_527,where y = log(t) has a student’s t distribution (p. 311).
2636,1,"['confidence intervals', 'correlation', 'random', 'tail areas', 'significance', 'coefficients', 'correlation coefficients', 'sample', 'intervals', 'random variable', 'confidence', 'statistical', 'tests', 'statistical inference', 'distribution', 'hypothesis', 'variable', 'tail']", Relation to Other Distributions,seg_527,"7.18.2.1 tail areas the cdf of a student’s t random variable is encountered frequently in small sample statistical inference. for example, it is used in tests for the means, testing the significance of correlation coefficients, and constructing confidence intervals for means. the area under student’s distribution from −t to +t is of special interest in finding two-sided confidence intervals and tests of hypothesis."
2637,1,['transformation'], Relation to Other Distributions,seg_527,this integral can be converted into an ibf by the transformation y = n∕(n + x2)
2638,1,['symmetry'], Relation to Other Distributions,seg_527,"owing to the symmetry, the area from ±t to the mode (x = 0) is"
2639,1,['symmetry'], Relation to Other Distributions,seg_527,"owing to symmetry, the first integral evaluates to 1/2. represent the second integral using equation (7.150) to get"
2640,1,"['distribution', 'degrees of freedom', 't distribution']", Relation to Other Distributions,seg_527,"2), and i(x; a, b) is the ibf. for even degrees of freedom, the cdf of student’s t distribution can be obtained as fn(t) =:"
2641,1,['cases'], Relation to Other Distributions,seg_527,the special cases n = 2 and n = 4 are f2(t) = 1
2642,1,['cauchy'], Relation to Other Distributions,seg_527,"(1 + 2∕(4 + t2))t∕√(4 + t2)]. as mentioned earlier, this reduces to cauchy cdf for n = 1 as 1"
2643,1,"['functions', 'symmetric', 'distribution', 'moments', 'even moments']", Relation to Other Distributions,seg_527,"7.18.2.2 moments and generating functions as this distribution is symmetric, all odd moments vanish. the even moments are given by"
2644,1,['recurrence'], Relation to Other Distributions,seg_527,this satisfies the first-order recurrence
2645,1,"['function', 'characteristic function', 'table']", Relation to Other Distributions,seg_527,repeated application of which gives a closed-form expression (see table 7.19). the characteristic function is given by
2646,1,"['dummy variable', 'variable']", Relation to Other Distributions,seg_527,"where we have used the variable x instead of t owing to the dummy variable in the chf [255]. upon putting x2∕n = y2, this becomes"
2647,1,"['recurrence', 'table']", Relation to Other Distributions,seg_527,"if n is odd (=2m + 1), this reduces to exp(−|t√n|)sn(|t√n|) where s is a polynomial of degree n − 1 that satisfies the recurrence sm+3(t) = sm+1(t) + t2∕(m2 − 1)sm−1(t). see table 7.19 for further properties."
2648,1,"['tests', 'sample', 'f distribution', 'test statistic', 'statistical tests', 'distribution', 'normality', 'statistic', 'statistical', 'population', 'null distribution', 'sample variance', 'test', 'variance', 'anova']", SNEDECORS F DISTRIBUTION,seg_529,"this distribution, named after g.w.snedecor [256], is used extensively in anova and related procedures. this is due to the normality assumption of the population from which sample came, so that the null distribution of the test statistic has an f distribution. it is also used in computing the power of various statistical tests that employ the sample variance."
2649,1,"['distribution', 'independent', 'variates']", SNEDECORS F DISTRIBUTION,seg_529,this is the distribution of the ratio of two independent scaled  2 variates f =
2650,1,"['distribution', 'f distribution']", SNEDECORS F DISTRIBUTION,seg_529,"a derivation is given in chapter 11. the unscaled f distribution is the distribution of the ratio  2(m)∕ 2(n), which is beta-ii (m∕2, n∕2)."
2651,1,"['case', 'variates', 'gamma distribution', 'long right tail', 'symmetry', 'parameter', 'functions', 'parameters', 'gamma', 'skewed', 'distribution', 'moments', 'f distribution', 'independent', 'normality', 'tail']", Properties of F Distribution,seg_531,"as both the numerator and denominator variates in the definition are 2, this distribution is defined for x 0. owing to symmetry, 1∕f has exactly identical distribution with the dof reversed. the parameters m and n are integers in practical applications. theoretically, the distribution is defined for noninteger dof values as well. the distribution of z = (1∕2) log(f) is more tractable, as it converges to normality faster than f itself. as the 2 distribution is a special case of gamma distribution, the ratio of two properly scaled independent gamma variates has an f distribution [257]. the f distribution has a long right tail and is skewed to the right for small parameter values. several recurrences satisfied by the density, distribution functions, and moments can be found in reference 129."
2652,1,"['functions', 'range', 'distribution', 'moments', 'mean', 'parameter']", Properties of F Distribution,seg_531,"7.19.1.1 moments and generating functions the mean is undefined when n 2, but it is n∕(n − 2) = 1 + 2∕(n − 2) for n 2. this does not depend on the numerator dof parameter m. although the distribution has infinite range, the mean (center of mass) is bounded by 3, and rapidly approaches 1 as n becomes large. the vari-"
2653,1,['mean'], Properties of F Distribution,seg_531,"2n2(m+n−2) ance is  2 = m(n−2)2(n−4) , which is defined for n   4. this in terms of the mean is"
2654,1,"['coefficient', 'skewness coefficient', 'mean', 'skewness']", Properties of F Distribution,seg_531,"2 2∕(n − 4) ∗ ((m + n − 2)∕m). the mode is [(m − 2)∕m] ∗ [n∕(n + 2)]. as n∕(n − 2) is  1 and n∕(n + 2) is  1, the mode is less than the mean. as n becomes large, the mean tends to 1 but the mode tends to (m − 2)∕m. similarly, the skewness coefficient is undefined for n ≤ 6 (all of these conditions are on n and not on m). for n   6, the"
2655,1,"['characteristic function', 'coefficient', 'skewness coefficient', 'function', 'skewness']", Properties of F Distribution,seg_531,2(2m+n−2)√2(n−4) skewness coefficient is  1 = . the characteristic function of f variate
2656,1,"['function', 'hypergeometric function', 'hypergeometric']", Properties of F Distribution,seg_531,"where  (m∕2, 1 − n∕2,−itn∕m) is the confluent hypergeometric function of type-2. a double infinite sum for it is as follows (see references 259 and 260)"
2657,1,['table'], Properties of F Distribution,seg_531,which is valid for n even. see table 7.20 for further properties.
2658,1,"['independent', 'standard', 'standard normal', 't distribution', 'distribution', 'normal', 'random']", Relation to Other Distributions,seg_533,"as mentioned earlier, if x ∼ f(m, n), then y = 1∕x is f(n,m). as the t distribution is the ratio of a standard normal to the square root of an independent scaled  2n random variate, the square of t is f distributed with 1 and ndf . if x and y are independent f"
2659,0,[], Relation to Other Distributions,seg_533,"variates with the same df , then t = √"
2660,1,"['f distribution', 'distribution', 'binomial', 'binomial distribution', 'tail']", Relation to Other Distributions,seg_533,n (√ x −√ y) is student’s t(n) [254]. tail area of binomial distribution is related to the f distribution as
2661,1,['distribution'], Relation to Other Distributions,seg_533,"(7.160) as the denominator dof n → ∞, the variate m ∗ x approaches a  2m distribution."
2662,1,"['distribution', 'tail', 'f distribution', 'tail areas']", Relation to Other Distributions,seg_533,"7.19.2.1 tail areas integrating from 0 to +x gives the cdf of a snedecor’s f distribution with (m, n) dof using equation (7.47) as"
2663,1,"['cases', 'tail', 'tail areas']", Relation to Other Distributions,seg_533,"the tail areas are related as f(x;m, n) = 1∕f(1 − x; n,m). the special cases are"
2664,1,['percentile'], Relation to Other Distributions,seg_533,"√ n+mx) , and f(x; 2, 2) = mx∕(n + mx). see references 261–264 for further properties and references 236 and 265 for percentile points."
2665,1,"['distribution', 'transformation', 'f distribution']", FISHERS Z DISTRIBUTION,seg_535,this distribution is obtained as a transformation from the f distribution as
2666,1,"['distribution', 'f distribution']", FISHERS Z DISTRIBUTION,seg_535,1 z = log(f). it is also called logarithmic f distribution. a derivation is given in
2667,1,['range'], FISHERS Z DISTRIBUTION,seg_535,"2 chapter 11. as the range of f is from 0 to ∞, the range of z is −∞ to ∞ and the pdf is obtained directly from the previous pdf as"
2668,1,"['distribution', 'noncentral', 'results']", FISHERS Z DISTRIBUTION,seg_535,"where b(m∕2, n∕2) is the cbf. the unnormalized z distribution results when f is replaced by the unnormalized f (which is beta-ii). if the f-distribution is noncentral, the corresponding z is singly noncentral. if both chi squares in the f-distribution are noncentral, the corresponding z is called doubly noncentral [4, 266]."
2669,1,"['tail areas', 'tail']", Properties of Fishers Z Distribution,seg_537,left tail areas can be expressed in terms of ibf as follows
2670,1,"['symmetry', 'table']", Properties of Fishers Z Distribution,seg_537,"where c = e2x∕(n + me2x). the cdf satisfies the symmetry relationship zc(m, n) = 1 − z−c(n,m). see table 7.21 for further properties (figures 7.13 and 7.14)."
2671,1,"['cumulants', 'characteristic function', 'gamma', 'moments', 'function', 'gamma function']", Properties of Fishers Z Distribution,seg_537,"7.20.1.1 moments the characteristic function is (n∕m)it∕2γ((n − it)∕2)γ((m + it)∕2)∕[γ(m∕2)γ(n∕2)]. using the derivatives of gamma function, the first two moments are   = (m − n)∕[2mn] = (1∕n − 1∕m)∕2 and  2 = (m + n)∕[2mn] = (1∕n + 1∕m)∕2 approximately. the cumulants are easier to find in terms of digamma function [60, 267, 268]."
2672,1,"['parameters', 'distributions']", Properties of Fishers Z Distribution,seg_537,7.20.1.2 relationship with other distributions when both the parameters
2673,1,"['f distribution', 'distribution', 'normality', 'convergence']", Properties of Fishers Z Distribution,seg_537,"n ). convergence of z to normality is faster than the convergence of f distribution. if x ∼ z(m, n) then exp(2z) ∼ f(m, n). the transformation v = (n∕(n + 1))1∕2(z∕b) is approximately distributed as tn , where n = m + n − 1, b2 = 1"
2674,1,"['distribution', 't distribution']", Properties of Fishers Z Distribution,seg_537,2 (1∕m + 1∕n) and tn is student’s t distribution.
2675,1,"['lognormal', 'model', 'design', 'distribution', 'weibull distribution', 'weibull', 'distributions']", WEIBULL DISTRIBUTION,seg_539,"this distribution is named after the swedish physicist w. weibull (1887–1979), who invented it in 1939 in connection with strength of materials, although it was known to rosen and rammler [270]. it finds applications in reliability theory, quality control, strength of materials, and so on [271, 272]. it is used in the design of wind turbines, model wind speed distributions, fading channels in wireless communications, describe the size of particles in motion (such as raindrops), or those being grinded, milled, crushed, or subjected to external pressure (for which another choice is the lognormal law). the one-parameter weibull distribution has pdf"
2676,1,"['distribution', 'weibull distribution', 'weibull']", WEIBULL DISTRIBUTION,seg_539,"the two-parameter weibull distribution is obtained from the above by a simple transformation x = y∕b as f (y, a, b) = b"
2677,1,"['parameter', 'median']", WEIBULL DISTRIBUTION,seg_539,"shape parameter. mode is b( a−1 )1∕a, median is b(log 2)1∕a. this reduces to the above"
2678,1,"['exponential distribution', 'distribution', 'exponential', 'weibull distribution', 'weibull']", WEIBULL DISTRIBUTION,seg_539,"a form when b = 1. we denote it as weib(a, b). it is easy to see that if [(x − a)∕b]c has an exponential distribution, then x has a general weibull distribution. the corre-"
2679,1,"['function', 'quantile']", WEIBULL DISTRIBUTION,seg_539,"−x  sponding cdf is easily found as f(x) = 1 − e , from which the quantile function can be obtained as q(u) = [− log(1 − u)]1∕  ."
2680,1,"['distribution', 'exponential', 'exponential distribution', 'rayleigh distribution']", Properties of Weibull Distribution,seg_541,"the mode of the distribution is 0 for a   1. when a = 1, we get the exponential distribution. it is also related to the rayleigh distribution. if x ∼ weib(a, b) then"
2681,1,"['distribution', 'extreme value', 'extreme value distribution']", Properties of Weibull Distribution,seg_541,"log(x) has extreme value distribution. for a   1, the mode is at (1 − 1"
2682,1,"['model', 'limit', 'weibull']", Properties of Weibull Distribution,seg_541,a )1∕a. this tends to the limit 1 as a → ∞. the three-parameter weibull model has cdf
2683,0,[], Properties of Weibull Distribution,seg_541,put c = 0 to get two-parameter version earlier.
2684,1,"['moments', 'moment']", Properties of Weibull Distribution,seg_541,7.21.1.1 moments the kth moment is given by e(xk) = ∫0
2685,1,['mean'], Properties of Weibull Distribution,seg_541,"1). from this, we get the mean as   = bγ( 1"
2686,1,"['moment', 'table', 'standard', 'variance', 'second moment']", Properties of Weibull Distribution,seg_541,"a + 1). the second moment is b2γ( 2 a + 1), from this we get the variance as  2 = b2[γ(2∕a + 1) − γ(1∕a + 1)2]. put b = 1 to get the variance of standard form (7.164). see table 7.22 for further properties."
2687,1,"['deviation', 'distribution', 'mean', 'weibull distribution', 'weibull']", Properties of Weibull Distribution,seg_541,example 7.29 mean deviation of weibull distribution
2688,1,"['deviation', 'distribution', 'mean', 'weibull distribution', 'weibull']", Properties of Weibull Distribution,seg_541,find the mean deviation of the weibull distribution using theorem 7.1.
2689,1,"['exponential distribution', 'weibull distribution', 'case', 'distribution', 'exponential', 'parameter', 'tail', 'limit', 'weibull']", Properties of Weibull Distribution,seg_541,"solution 7.29 we apply theorem 7.1 (page 7-5) to find the md. as the weibull distribution does not tail off to the lower limit for some parameter values (e.g., b   1), equation (7.1) seems like not applicable. we know that the cdf is 1 − exp(−( x−c )b). as done in the case of exponential distribution, using a l’hospital’s rule, it is easy to show that x ∗ f(x) → 0, so that the theorem 7.1 is applicable. this gives"
2690,1,['mean'], Properties of Weibull Distribution,seg_541,where m = bγ(1 + 1 a ) is the mean. split this into two integrals and integrate
2691,0,[], Properties of Weibull Distribution,seg_541,expand exp(−( x−c )b) as an infinite series and integrate term by term to get a −2∑k
2692,1,"['uniform distribution', 'discrete', 'distribution', 'weibull distribution', 'weibull', 'distributions']", Properties of Weibull Distribution,seg_541,"7.21.1.2 relationship with other distributions if [(x − a)∕b]c has an exponential distribution, then x has a general weibull distribution. it is also related to the uniform distribution u(0, 1) as follows: –if x ∼u(0, 1) then y = (− ln(x)∕a)1∕b ∼ weib(a, b). see reference 273 discrete weibull distribution."
2693,1,"['random numbers', 'random']", Random Numbers,seg_543,"−(x∕b)a as f(x) = 1 − e , we could generate random numbers using uniform pseudo-"
2694,1,"['interval', 'random', 'function', 'random numbers']", Random Numbers,seg_543,"−(x∕b)a random numbers in [0,1] as u = 1 − e , which on rearrangement becomes x = b(− log(u))1∕a. notice that the log function takes negative values for the argument in [0, 1]. hence, the − log(u) maps it into the positive interval."
2695,1,"['random variables', 'variables', 'distribution', 'normal', 'rayleigh distribution', 'random', 'bivariate']", RAYLEIGH DISTRIBUTION,seg_545,"this distribution is named after the british physicist rayleigh (1842–1919). it finds applications in reliability theory and communication systems. see reference 274 for an application of rayleigh distribution to wind turbine modeling. stability of the rayleigh distribution is discussed in reference 275. see references 276 and 277 for the distribution of the product of two iid rayleigh random variables. this distribution can be considered as the distribution of the radial distance of a point on the bivariate normal surface (with zero means) from the origin. in other words, it is the distribution"
2696,1,"['bivariate normal distribution', 'case', 'distribution', 'joint', 'normal', 'bivariate', 'normal distribution']", RAYLEIGH DISTRIBUTION,seg_545,"x2 + y2, where (x,y) have a joint bivariate normal distribution. it is a special case of  -distribution. the pdf is"
2697,1,"['linear', 'normal', 'linear transformation', 'transformation']", RAYLEIGH DISTRIBUTION,seg_545,the corresponding cdf can be expressed in terms of scaled normal pdf as given below. an alternate parametrization can be obtained by the linear transformation y =
2698,1,"['random variables', 'independent', 'variables', 'distribution', 'normal', 'random']", RAYLEIGH DISTRIBUTION,seg_545,"in general, if x1,x2, · · · ,xn are independent normal random variables n(0,  2), the distribution of x = (x1"
2699,1,['table'], RAYLEIGH DISTRIBUTION,seg_545,see table 7.23 for further properties. the cdf is given by
2700,1,"['deviation', 'distribution', 'mean', 'rayleigh distribution']", RAYLEIGH DISTRIBUTION,seg_545,example 7.30 mean deviation of rayleigh distribution
2701,1,"['deviation', 'distribution', 'mean', 'rayleigh distribution']", RAYLEIGH DISTRIBUTION,seg_545,find the mean deviation of the rayleigh distribution with pdf fx(x; a) =
2702,1,"['distribution', 'tail', 'limit', 'rayleigh distribution']", RAYLEIGH DISTRIBUTION,seg_545,"solution 7.30 we apply theorem 7.1 (p. 256) to find the md. as the rayleigh distribution does not tail off to zero at the lower limit (i.e., at 0), equation (7.1) seems like not applicable. we know that the cdf is 1 − exp(−x2∕2a2). if we apply l’hospital’s rule once on x ∗ f(x), we find that it → 0 as x → 0. as the limx→0x ∗ f(x) = 0, theorem 7.1 becomes applicable. this gives"
2703,1,['limit'], RAYLEIGH DISTRIBUTION,seg_545,split the integral into two parts. the first one integrates to 2m. the second one is −2 ∫0 m exp(−x2∕2a2)dx. put y = x2∕(2a2) so that dy = x∕a2dx. the upper limit of integration becomes m2∕(2a2) =  ∕4. we get
2704,1,['gamma'], RAYLEIGH DISTRIBUTION,seg_545,where p() is the incomplete gamma integral. put the value of m to get the md.
2705,1,"['skewed', 'distribution', 'rayleigh distribution', 'standard', 'skewness', 'variance']", Properties of Rayleigh Distribution,seg_547,"the standard rayleigh distribution is obtained by putting a = 1. as the skewness is 0.63111, it is always positively skewed. variance  2 = 0.27324 2 shows that  2    2 or equivalently  ∕  = 0.5227232. see reference 278 for an application to the distance between pairs of points in wireless networks."
2706,1,"['functions', 'gamma', 'moments', 'function', 'gamma function']", Properties of Rayleigh Distribution,seg_547,7.22.1.1 moments and generating functions ordinary moments can be obtained in terms of gamma function as e[xk] =
2707,1,['mean'], Properties of Rayleigh Distribution,seg_547,from which we get the mean as   =  √
2708,1,['distributions'], Properties of Rayleigh Distribution,seg_547,"7.22.1.2 relationship with other distributions if x is rayleigh(1), then x2 is  2"
2709,1,"['random variables', 'random', 'independent', 'standard normal random variables', 'variables', 'standard normal', 'distribution', 'normal', 'standard']", CHIDISTRIBUTION,seg_549,"if x1,x2, · · · ,xn are independent standard normal random variables, the distribution"
2710,0,[], CHIDISTRIBUTION,seg_549,"2 + x2 2 + · · · + xn2)1∕2, that is,√  n2 is called a chi-distribution with ndf . the pdf is easily obtained as"
2711,1,"['distribution', 'normal', 'normal distribution', 'rayleigh distribution']", Properties of ChiDistribution,seg_551,"this distribution reduces to the half-normal or folded normal distribution for n = 1, the rayleigh distribution for n = 2, and the maxwell distribution for n = 3. the rth"
2712,1,['mean'], Properties of ChiDistribution,seg_551,moment is  r = 2r∕2γ((n + r)∕2)∕γ(n∕2). the mean is√
2713,0,[], Properties of ChiDistribution,seg_551,example 7.31 mode of chi-distribution
2714,0,[], Properties of ChiDistribution,seg_551,find the mode of chi-distribution with n   1df .
2715,1,['results'], Properties of ChiDistribution,seg_551,"(n − 1)∕x − x. equating to zero results in (n − 1) = x2 so that x =√ n − 1 is the solution. as the second derivative is −(n − 1)∕x2 − 1, which is negative for n   1, this is indeed the mode."
2716,1,['distribution'], MAXWELL DISTRIBUTION,seg_553,this distribution is frequently encountered in engineering. it is named after the scottish physicist james c. maxwell (1831–1879). the pdf is given by
2717,1,"['distribution', 'parameter', 'standard']", MAXWELL DISTRIBUTION,seg_553,"here, “a” is a scale parameter. put y = x∕a to get the standard maxwell distribution."
2718,1,"['distribution', 'parameter', 'distributions']", Properties of Maxwell Distribution,seg_555,the maxwell and rayleigh distributions are surprisingly similar shaped for small parameter values (figures 7.15 and 7.16). it has an alternate parametrization known as maxwell’s velocity distribution that represents the velocity of a gas molecule as
2719,1,"['variance', 'estimated', 'mean']", Properties of Maxwell Distribution,seg_555,"where a = molecular weight, t = absolute temperature, and k is the boltzmann constant. mean =√8∕( a) and variance 2 = (3 − 8∕ )∕a. the mean velocity of a gaseous molecule at room temperature can then be estimated as x = [8kt∕(a )]1∕2. integration of equation (7.175) allows us to write the cdf in either of the following formats:–"
2720,1,"['power method', 'method']", Properties of Maxwell Distribution,seg_555,the md is easily obtained using the power method as
2721,1,"['distribution', 'case']", Properties of Maxwell Distribution,seg_555,this distribution is a special case of the chi-distribution that has pdf
2722,1,['table'], Properties of Maxwell Distribution,seg_555,put n = 3 in equation (7.179) to get equation (7.175). see table 7.24 for further properties.
2723,1,"['plot', 'model', 'linear', 'linear combination', 'combination', 'homogeneous', 'data', 'distribution', 'distributions']", SUMMARY,seg_557,"statistical distributions play an important role in data modeling in various fields, including psychology, education, various branches of engineering, medical sciences, management, and the worldwide web. a single distribution suffices for most modeling situations. a linear combination of homogeneous models (such as normals with different means) is sometimes used. a researcher has to choose the most appropriate model depending on the data to be modeled at hand. a simple data plot can quite often reveal the most appropriate distribution that fits it well."
2724,1,"['lognormal', 'continuous distribution', 'memory', 'probability', 'function', 'exponential distribution', 'exponential', 'mean', 'distributions', 'gamma', 'median', 'distribution', 'level', 'continuous', 'variance', 'probability function', 'weibull', 'deviation', 'normal', 'continuous distributions']", SUMMARY,seg_557,"statistical properties such as the mean, variance, cumulative probability function, median, and mean deviation are obtained in summary format for some commonly employed continuous distribution such as uniform, normal (gaussian), exponential, gamma, weibull, and lognormal among others. the exponential distribution has no memory, whereas other continuous distributions in the above-mentioned list do retain a level of memory."
2725,1,"['deviation', 'method', 'continuous', 'distribution function', 'continuous distributions', 'distribution', 'moments', 'mean', 'function', 'distributions']", SUMMARY,seg_557,"ever since the landmark paper of abraham de moivre in 1730 [130], numerous research work had gone into finding the mean deviation of common distributions. see, for example, references 134 and 221, and so on. in reference 166, the authors gave several expressions involving the integral of distribution function that pertains to higher order moments and moments of spacings. in section 7.2, we gave an easy method to find the md of continuous distributions and demonstrated its use throughout this chapter."
2726,1,"['distribution', 'results', 'distributions']", SUMMARY,seg_557,"extensive bibliographies exist for each distribution. see balakrishnan and nevzorov [121], evans et al. [122], johnson, kotz and balakrishnan [60], and hazewinkel [279] for theoretical discussions and properties of these distributions. this chapter gave a bird’s eye view of the main results. separate volumes are"
2727,1,"['pareto distributions', 'normal', 'pareto', 'distributions', 'weibull']", SUMMARY,seg_557,"available exclusively for some of these distributions. for example, see reference 230 for pareto distributions, reference 272 for weibull distributions, references 221 and 280 for normal and related distributions, and reference 281 for lnd. numerous application papers have also appeared recently."
2728,1,"['distribution', 'range']", SUMMARY,seg_557,"c) the md of a distribution in the range [a, b] is always within the range"
2729,1,"['cauchy', 'central limit theorem', 'mean', 'limit']", SUMMARY,seg_557,d) the central limit theorem is applicable to the cauchy mean
2730,1,"['lognormal', 'lognormal distributed', 'lognormal distributions', 'geometric mean', 'mean', 'geometric', 'distributions']", SUMMARY,seg_557,e) geometric mean of lognormal distributions is lognormal distributed
2731,1,"['distribution', 'case']", SUMMARY,seg_557,f) maxwell distribution is a special case of chi-distribution
2732,1,"['distribution', 'skewness']", SUMMARY,seg_557,"g) truncated u(a, b) distribution has the same skewness"
2733,1,"['exponential distribution', 'distribution', 'exponential', 'mean', 'variance']", SUMMARY,seg_557,h) variance of exponential distribution is square of the mean.
2734,1,"['function', 'beta function']", SUMMARY,seg_557,"7.3 prove that b(a + 1, b) = [a∕(a + b)] b(a, b), where b(a, b) denotes the complete beta function (cbf). what is the value of b(0.5, 0.5)?"
2735,1,"['distribution', 'beta distribution']", SUMMARY,seg_557,prove that p(x   y) =  ∕(  +  ). ters p and q is the beta distribution
2736,1,['probability'], SUMMARY,seg_557,"beta-i(p, q) u-shaped? 7.5 if x ∼ exp( ) find the probability"
2737,1,"['distribution', 'normal', 'normal distribution']", SUMMARY,seg_557,that pr(x − 1∕ )   1. 7.12 prove that the normal distribution
2738,1,"['transformation', 'change of scale']", SUMMARY,seg_557,a pdf. the change of scale transformation
2739,1,"['variance', 'mean', 'median']", SUMMARY,seg_557,7.8 prove that mode  median mean butions is the variance the square
2740,1,"['lognormal', 'lognormal distribution', 'distribution', 'exponential', 'mean']", SUMMARY,seg_557,for lognormal distribution. of the mean?. (a) exponential (b)
2741,1,"['lognormal', 'normal']", SUMMARY,seg_557,beta (c) normal (d) student’s t 7.9 prove that for the lognormal distri-
2742,0,[], SUMMARY,seg_557,bution mean/median = exp( 2∕2). 7.14 prove that area from 1∕  to   of an
2743,1,"['lognormal', 'exponential distribution', 'range', 'distribution', 'exponential']", SUMMARY,seg_557,7.10 what is the range of lognormal exponential distribution f (x;  ) =
2744,1,"['distribution', 'normal', 'lognormal distributed', 'lognormal']", SUMMARY,seg_557," e− x is 1 − e− 2 if     1 and distribution? if x is normal, is e − 2 1 log(x) lognormal distributed?. e − otherwise. e"
2745,1,"['independent', 'gamma']", SUMMARY,seg_557,7.15 find the area from 0 to   and from 7.26 if x and y are independent gamma
2746,1,"['variables', 'exponential']", SUMMARY,seg_557,"to ∞ of the exponential distrirandom variables γ(a, ) and bution e− x. hence or otherwise γ(b, ), then prove that x∕(x + y) obtain the area from 1∕ to . is beta(a, b)."
2747,1,"['transformation', 'standard']", SUMMARY,seg_557,7.16 express the areas of standard nor7.27 what transformation to use to
2748,1,"['function', 'error', 'standard']", SUMMARY,seg_557,mal in terms of error function obtain standard arc-sine distribuerf(z) (i) from −c to +c and (ii) −c tion from b∕[ √(x−a)(a+b−x)]
2749,1,"['variance', 'mean', 'moments']", SUMMARY,seg_557,"tribution of y = (1 − x)∕x and 2 = a(a + b − 1)∕[(b − 1)2(b − obtain its mean and variance. find 2)] (b 2) for the beta-ii disthe ordinary moments. tribution with pdf fy(a, b) ="
2750,1,['distribution'], SUMMARY,seg_557,bution is ((m − 2)∕m)(n∕(n + 2)) 7.29 consider a distribution defined as
2751,0,[], SUMMARY,seg_557,"p 0, q 0. find the normaliz7.19 if xi, i = 1, 2,...,n are iid u(0, 1),"
2752,1,"['distribution', 'mean']", SUMMARY,seg_557,"ing constant, the mean and varifind the distribution of s ="
2753,1,"['mean', 'moment']", SUMMARY,seg_557,7.20 prove that the mean of an expo7.30 find the kth moment of arc-sine
2754,1,['distribution'], SUMMARY,seg_557,"nential distribution divides the distribution of first kind given in area in (1 − 1 e ): 1 e ratio. equation (7.8), page 7-40, and obtain the   and  2. 7.21 prove that the characteristic func-"
2755,1,"['moment', 'cauchy', 'distribution', 'parameter']", SUMMARY,seg_557,tion of general cauchy distribu7.31 find the kth moment of the two tion is exp(itb-|t|a). parameter arc-sine distribution
2756,1,['moments'], SUMMARY,seg_557,b∕ √(x − a)(a + b − x). 7.22 what are the central moments
2757,1,"['distribution', 'moments', 'rectangular distribution']", SUMMARY,seg_557,"of a rectangular distribution 7.32 find first two moments of the discuni(a, b)?. obtain 2. tribution fx( ,m) =√ ∕ x e− x"
2758,1,['range'], SUMMARY,seg_557,with range x ≥ 0. 7.23 prove that a = k − c and b = k + c
2759,1,['results'], SUMMARY,seg_557,"in cuni(a, b) results in f (x; c) = 7.33 find the constant c of the distri1/(2c) for a−c≤ x ≤ a + c. find bution f (y) = c∕√(1 − 4y 2), for"
2760,1,['mean'], SUMMARY,seg_557,its mean. −1∕2 ≤ y ≤ 1∕2. find   and  2.
2761,1,['moment'], SUMMARY,seg_557,"7.24 which of the following distribu7.34 prove that the rth moment, for r"
2762,1,"['independent', 'leptokurtic', 'cauchy', 'distribution', 'asymmetric', 'exponential', 'normal']", SUMMARY,seg_557,"tions is always asymmetric and even, of n(0, 1) is  r = 2r∕2γ((r + leptokurtic. (a) exponential (b) 1)∕2)∕√ . beta (c) normal (d) student’s t. 7.35 prove that the distribution of the 7.25 if x ∼u(0, 1), find the distribution sum of n independent cauchy"
2763,1,"['normal', 'variates']", SUMMARY,seg_557,"of y = − log(1 − x). hence prove 1 n variates is f (x) = . that p[y|x x0] = 1 − log(1 − n2+x2 x0) characterizes the u(0, 1) dis7.36 show that the pdf of square root tribution. of a half-standard normal variate"
2764,1,['distribution'], SUMMARY,seg_557,"−x4∕2 7.48 consider the lindley distribution (for z   0) is f (x) =√2∕ e , with pdf f (x) =  2 1"
2765,1,['mean'], SUMMARY,seg_557,"x e− xmx, for x   0. x   0,     0. prove that the mgf is 7.37 prove that if x ∼ weib(p, q), then  2  +t+1 . find the mean and vari-"
2766,1,"['distribution', 'exponential']", SUMMARY,seg_557,y = (x∕q)p has an exponential  +1 ( +t)2 ance. distribution.
2767,1,['cauchy'], SUMMARY,seg_557,"7.49 if x ∼ cauchy( ,  ) with pdf 7.38 if x, y are iid ∼ u[0, 1], find the f (x; ,  ) = 1∕[   (1 + ( x−"
2768,1,['distribution'], SUMMARY,seg_557,  )2)] distribution of −2 log(xy). prove that (i) 2x∕(1 − x2) is 7.39 if x ∼cauchy find the distribution identically distributed (ii) 1∕x ∼
2769,1,['cauchy'], SUMMARY,seg_557,"of y = (x − (1∕x))∕2. cauchy( ∕( 2 +  2),  ∕( 2 +"
2770,1,['exponential'], SUMMARY,seg_557,7.40 a left-truncated exponential dis2)).
2771,1,"['variance', 'pareto', 'mean', 'median']", SUMMARY,seg_557,tribution with truncation point c has pdf f (x; ) = e− x∕[1 − 7.50 prove that the mean and median e−c] for x c. obtain the mean of pareto law f (x; c) = c∕xc+1 are c∕(c − 1) and 21∕c. find the corand variance. responding mean of power-law 7.41 obtain the pdf for a symmetriusing y = 1∕x.
2772,1,"['distribution', 'cauchy', 'cauchy distribution']", SUMMARY,seg_557,"cally both-side truncated cauchy distribution with the truncation 7.51 if x ∼ cauchy( ,  ) with pdf"
2773,1,['variance'], SUMMARY,seg_557,"point  . does the variance exist? f (x; ,  ) = 1∕[   (1 + ( x−  "
2774,1,"['harmonic', 'harmonic mean', 'mean']", SUMMARY,seg_557,find the pdf of each of (i) 1∕(1 + 7.42 prove that the harmonic mean of
2775,1,"['cauchy', 'variates']", SUMMARY,seg_557,x2) and (ii) x2∕(1 + x2). n iid cauchy variates is cauchy
2776,1,"['distribution', 'lognormal distribution', 'lognormal']", SUMMARY,seg_557,"distributed. 7.52 if x has a lognormal distribution,"
2777,1,['distribution'], SUMMARY,seg_557,"7.43 if x ∼ beta − i(a, b), find the find the distribution of y = xn for"
2778,1,['distribution'], SUMMARY,seg_557,the distribution of y = exp(x). c]∕  
2779,1,"['lognormal', 'median']", SUMMARY,seg_557,7.45 prove that the median and mode of 7.54 show that for a lognormal dis-
2780,1,['distributions'], SUMMARY,seg_557,"rmal distributions are e  tribution,  k ′ +1∕ k"
2781,1,['distribution'], SUMMARY,seg_557,"7.46 for n( ,  2) distribution, 68.26 of 7.55 verify whether the log-"
2782,1,"['distribution', 'interval', 'normal', 'normal distribution']", SUMMARY,seg_557,the area lies in the interval (  − normal distribution satisfies
2783,1,"['geometric', 'intervals']", SUMMARY,seg_557,"corresponding intervals for logand hm are the geometric,"
2784,1,"['distribution', 'harmonic']", SUMMARY,seg_557,"normal distribution? arithmetic and harmonic means, 7.47 verify whether f (x; c, d) = respectively."
2785,1,"['distribution', 'beta distribution', 'moment']", SUMMARY,seg_557,"(1 + x)c−1(1 − x)d−1∕[2c+d−1b(c, d)] 7.56 prove that the third moment of is a pdf for −1 x 1, where beta distribution is 3 = 2(b − b(c, d) is the complete beta function. a) 2∕[(a + b)(a + b + 2)]."
2786,1,"['harmonic', 'harmonic mean', 'mean']", SUMMARY,seg_557,7.57 prove that the harmonic mean of n ∑i
2787,1,"['random variables', 'variables', 'cauchy', 'random']", SUMMARY,seg_557,"n =1 c(i, j, n) i exp(− ix), where iid cauchy random variables has c(i, j, n) =∏j"
2788,1,"['deviation', 'mean']", SUMMARY,seg_557,pdf f (x) = n∕[ (n2 + x2)]. 7.68 prove that the mean deviation of
2789,1,['lognormal'], SUMMARY,seg_557,"7.58 prove that the lognormal districuni(a, b) is√"
2790,1,"['deviation', 'unimodal', 'standard', 'standard deviation']", SUMMARY,seg_557,"3 ∕2, where   is bution is unimodal with mode the standard deviation."
2791,1,"['variables', 'variance']", SUMMARY,seg_557,"exp( − 2). what is the modal 7.69 if x and y are iid ranvalue? dom variables with f (x) = 7.59 prove that the variance of stu1∕[ √(2∕b2) − x2], where |x|"
2792,1,['distribution'], SUMMARY,seg_557,"dent’s t with n dof is 1 + √ 2∕|b|, prove that (x + y)∕b has (1∕(n∕2 − 1)) if n   2, which → 1 the same distribution as xy . as n → ∞."
2793,0,[], SUMMARY,seg_557,"7.70 the weight of baggage com7.60 if x is cuni(a, b) find the distripartment of a small aircraft is"
2794,1,"['mean', 'normally distributed']", SUMMARY,seg_557,bution of y = (2x − (a + b))∕(b − normally distributed with mean
2795,1,['variance'], SUMMARY,seg_557,"a) 1800∼kg and variance 20,64∼kg."
2796,1,['probability'], SUMMARY,seg_557,"7.61 prove that beta-i(a, b) can be find the probability that the bag-"
2797,1,"['distribution', 'pareto', 'independent', 'pareto distribution']", SUMMARY,seg_557,has pareto distribution. 7.71 if x and y are independent f vari-
2798,0,[], SUMMARY,seg_557,ates with the same numerator and 7.63 prove that difference of two iid
2799,1,['variates'], SUMMARY,seg_557,"exp( ) variates is laplace(0,  "
2800,1,['degrees of freedom'], SUMMARY,seg_557,1 ). denominator degrees of freedom
2801,1,"['distribution', 'cauchy', 'cauchy distribution']", SUMMARY,seg_557,generalized cauchy distribution √
2802,0,[], SUMMARY,seg_557,y) is student’s t distributed
2803,1,['degrees of freedom'], SUMMARY,seg_557,with pdf f (x;m) = c∕(1 + x2)m with n degrees of freedom.
2804,1,['variances'], SUMMARY,seg_557,is c = γ(m)∕[√ γ(m − 1∕2)]. 7.72 prove that the ratio of variances
2805,1,['exponential'], SUMMARY,seg_557,"constant for the exponential distrin dof is (1 + 1∕n) ∗ (1 − 1∕(n − bution, irrespective of its paramet1)) if n 2."
2806,0,[], SUMMARY,seg_557,ric forms. 7.73 prove that the ratio of modal val-
2807,1,['mean'], SUMMARY,seg_557,7.66 prove that the mean tends to the ues of student’s t with n and
2808,1,"['distribution', 'moments', 'mean']", SUMMARY,seg_557,"mode from above for a beta-ii (n + 2) dof is (1 − 1∕(n + 1)) ∗ distribution by taking the ratio of √(1 + 2∕n) if n   2. mode to the mean as (1 − 1 a )(1 − 2 7.74 find first two moments of the b+1 ). show that they coincide as arc-sine distribution of second both a, b becomes large. 1 kind with pdf f (y) = for  √ (1−y2) 7.67 if xi for i = 1, 2, · · · , n are −1   y   1."
2809,1,['variates'], SUMMARY,seg_557,"independent exp( i) variates 7.75 what does b(m∕2, n∕2) − where  i ≠  j for i ≠ j, the sum"
2810,1,"['distribution', 'lognormal distribution', 'lognormal']", SUMMARY,seg_557,"nx), b() is the complete beta func7.85 prove that the area up to the mode tion and iy(a, b) is the unskilled of lognormal distribution isφ(− ) ibf represent. where φ() is the cdf of underly-"
2811,1,['normal'], SUMMARY,seg_557,ing normal. 7.76 show that as n → ∞k = γ((n +
2812,0,[], SUMMARY,seg_557,1)∕2)∕[√n  γ(n∕2)] → 1∕√ 2 . 7.86 prove that the mean-median-mode
2813,1,"['lognormal', 'mean', 'median']", SUMMARY,seg_557,inequality of lognormal distribu7.77 prove that the modal value tion is mode median mean.
2814,1,"['lognormal', 'median', 'lognormal distribution', 'distribution', 'mean', 'inequality']", SUMMARY,seg_557,of lognormal distribution is 1 − 2∕2 e . 7.87 prove that the mean–median-mode √ 2 exp( − 2) inequality of central 2 distribu7.78 let x be distributed as lognormal tion is mode median mean.
2815,1,"['moment', 'distribution', 'mean', 'transformation', 'variance']", SUMMARY,seg_557,"ln( , ). prove that the mean is = exp( + 1 2 2). if a change of 7.88 find the mean and variance scale transformation y = x∕ is of laplacian distribution with applied, prove that the (r + 1)th pdf f (x; ) = (1∕ √ 2) exp mean of y is exp( 1 2 2r(r + 1)) = √ (− 2|x|∕ ),−∞ x ∞. e(y−r), the rth inverse moment. 7.89 prove that the md of lognor7.79 the fraction (by weight) of impu2"
2816,1,"['parameters', 'standard normal', 'distribution', 'normal', 'asymmetric', 'mean', 'variance', 'standard', 'parameter', 'average']", SUMMARY,seg_557,"mal distribution is 2e + ∕2[2φ rities in a kitchen cleaning liq( ∕2) − 1], where φ(x) is the cdf uid has a beta-i distribution with of standard normal. known parameter a = 2. if the average fraction of impurities is 7.90 for which distribution is the mean 0.18, find the parameter b. what is asymmetric and the variance symthe variance for the impurities?. metric in the parameters? (a) neg-"
2817,1,"['shape parameter', 'binomial', 'parameter', 'hypergeometric']", SUMMARY,seg_557,"ative binomial (b) beta-i(a, b) (c) 7.80 if the shape parameter of a binomial (d) hypergeometric"
2818,1,"['poisson', 'deviation', 'gamma', 'distribution', 'mean', 'function', 'poisson sum']", SUMMARY,seg_557,prove that the survival function 7.91 find the mean deviation of gamma can be expressed as a poisson sum. distribution using the incom-
2819,1,"['gamma', 'moments', 'function', 'gamma function']", SUMMARY,seg_557,plete gamma function (igf) 7.81 show that the ordinary moments
2820,1,['lognormal'], SUMMARY,seg_557,and theorem 7.1 as md = of lognormal can be found using  r
2821,1,['quartiles'], SUMMARY,seg_557,"7.82 prove that the quartiles q1 and q3 7.92 if x is beta-i(a, b), find the dis-"
2822,1,"['distribution', 'cauchy', 'cauchy distribution']", SUMMARY,seg_557,"of generalized cauchy distribution tribution of x∕(1 − x) and (1 − are a − b and a + b, respectively. x)∕x."
2823,1,['quartiles'], SUMMARY,seg_557,"7.83 prove that the quartiles q1 and q3 7.93 if x is beta-ii(a, b), find the dis-"
2824,1,['distribution'], SUMMARY,seg_557,"and 0.75 for q3. 7.94 if y ∼ tn, find the distribution of"
2825,1,['median'], SUMMARY,seg_557,7.84 prove that area from the median (1/2)+(1/2)*y/√n + y
2826,1,"['lognormal', 'normal']", SUMMARY,seg_557,"2. to the mode of lognormal distribu7.95 if x ∼ n(0, 1) prove that the distion is φ(0) − φ(− ) where φ() is the normal cdf. tribution of x2 is (√x)∕√x."
2827,1,"['distribution', 'frequency']", SUMMARY,seg_557,7.96 the distribution of age (x) of patients to a clinic is given by the frequency
2828,1,"['deviation', 'standard deviations', 'skewed', 'distribution', 'deviations', 'mean', 'standard', 'standard deviation']", SUMMARY,seg_557,"function f (x) = 12x(100 − x)2∕1004 for 0 ≤ x ≤ 100. prove that the modal age is 33.33, and the mean age is 40. show that the standard deviation is 20 approximately. find the approximate number of patients between two standard deviations from the mean age. is the age distribution positively or negatively skewed?"
2829,1,['quantiles'], SUMMARY,seg_557,7.97 prove that the quantiles of expo(in inches) from the surface of
2830,1,"['rate', 'distribution', 'rate function', 'function', 'coefficient', 'inequality']", SUMMARY,seg_557,"nential distribution is given by thick concrete floors is distributed xp = − ln(1 − p)∕ . find an upper as f (x) = k[1 − erf (x∕(2√ td))], where t is the time in years and bound to p|x − | ≥ k using d is the diffusion coefficient chebychev’s inequality. find (cm2/sec). determine k if time the hazard rate function (t) = is taken as 20 years. find total f (t)∕sf(t), where sf() is the surchloride up to 3 inches from the vival function. surface."
2831,1,['continuous'], SUMMARY,seg_557,7.98 if f(x) is the cdf of a continuous 7.100 the annual maintenance cost in
2832,1,['variable'], SUMMARY,seg_557,"random variable defined on [0,1], 1000 of a high-rise office complex prove the following: –(i) f(x) is is beta-i(1.2, 0.8) distributed"
2833,1,"['exponential', 'standard']", SUMMARY,seg_557,"u(0, 1) distributed, (ii) − ln(f(x)) where x = (1 − 1∕t), t being the has standard exponential distriage (in years) of the building. find bution, and (iii) −2 ln(f(x)) has the (i) maintenance cost (in000)"
2834,1,['distribution'], SUMMARY,seg_557,"a chi-square distribution with for the fifth year, (ii) total main-"
2835,0,[], SUMMARY,seg_557,"two dof. tenance costs for 10 years, and"
2836,0,[], SUMMARY,seg_557,7.99 chloride content in kilogram (iii) maintenance cost for first 3
2837,0,[], SUMMARY,seg_557,per cubic meter at a distance x years.
2838,0,[], MATHEMATICAL EXPECTATION,seg_559,"after finishing the chapter, students will be able to"
2839,1,"['mathematical expectation', 'expectation']", MATHEMATICAL EXPECTATION,seg_559,◾ understand the meaning of mathematical expectation
2840,1,"['functions', 'random variables', 'variables', 'expectation', 'random']", MATHEMATICAL EXPECTATION,seg_559,◾ find the expectation of sums and functions of random variables
2841,1,"['factorial', 'moments', 'expected values']", MATHEMATICAL EXPECTATION,seg_559,"◾ derive moments (ordinary, central, factorial) as expected values"
2842,1,"['covariance', 'variance', 'expected values']", MATHEMATICAL EXPECTATION,seg_559,◾ interpret variance and covariance as expected values
2843,1,"['conditional expectation', 'conditional', 'independence', 'expectation']", MATHEMATICAL EXPECTATION,seg_559,◾ explain conditional expectation and independence
2844,0,[], MATHEMATICAL EXPECTATION,seg_559,◾ apply the concepts learned to practical problems
2845,1,"['sample', 'data', 'location', 'information', 'statistical', 'population', 'populations', 'sample space', 'location measures']", MEANING OF EXPECTATION,seg_561,"many location measures were introduced in chapter 2. these measures concisely summarize the information in a sample as a single number (for univariate data). analogous measures are needed to succinctly summarize the characteristics of statistical populations. the population and sample space were defined in chapter 5. in most of the discussions later, the functional form of the population is known precisely. however, theoretically the concept is valid even in those situations where the"
2846,1,"['mathematical expectation', 'random variable', 'variable', 'expectation', 'random']", MEANING OF EXPECTATION,seg_561,exact form is either unknown or is partially known. the concept of the mathematical expectation (or simply called expectation) relies on a random variable defined below.
2847,1,"['mathematical expectation', 'random variables', 'variables', 'discrete random variables', 'discrete', 'random variable', 'variable', 'expectation', 'random']", RANDOM VARIABLE,seg_563,the concept of random variable is of prime importance in mathematical expectation. discrete random variables was defined in section 6.1 of chapter 6
2848,1,"['sample', 'probabilities', 'experiment', 'variable', 'random variable', 'associated', 'random', 'function', 'outcome', 'sample space']", RANDOM VARIABLE,seg_563,definition 8.1 a random variable is a function defined on the sample space of a random experiment that maps each possible outcome of the sample space to real numbers such that the associated probabilities sum to one.
2849,1,"['complete enumeration', 'discrete', 'probability', 'random', 'function', 'numerical', 'recurrence relation', 'sample', 'experiment', 'random variable', 'sample space', 'recurrence', 'outcome', 'outcomes', 'discrete random variable', 'random variables', 'probabilities', 'independent', 'variables', 'discrete random variables', 'variable', 'bivariate']", RANDOM VARIABLE,seg_563,"this concept is easy to understand for discrete random variables as the number of points in the sample space are countably finite. any number of random variables can be defined on a given sample space. these may be related or independent (see figure 8.3). in every random experiment, there are some numerical values of interest. for example, consider the toss of a die. the possible outcomes are the faces numbered {1,2,3,4,5,6}, each with probability 1/6. if x denotes the face that turns up, we express it mathematically as f (x) = 1∕6 for x = 1, 2, … 6. what we have done is to simply assign a mathematical function to each outcome of a random experiment. this is the most common way to define a discrete random variable. for example, f (x) = qxp is a mathematically defined random variable. there is one more way to define discrete random variables. it is called complete enumeration. consider the random variable p(1) = 0.2, p(2) = 0.6, p(3) = 0.2. here, x takes three values {1,2,3}. as the probabilities add up to one, it is a well-defined random variable. this can also be written as p(x = 1) = 0.2, p(x = 2) = 0.6, p(x = 3) = 0.2 for a univariate random variable. this notation can be extended to bivariate and higher-dimensional random variables. it is better suited when the sample space is of small size. the individual probabilities can also be defined using a recurrence relation."
2850,1,"['sample', 'ordered pair', 'experiment', 'mass function', 'probability mass function', 'discrete', 'probability', 'random', 'function', 'outcome', 'sample space']", RANDOM VARIABLE,seg_563,"definition 8.2 a probability mass function (pmf) defined on the discrete sample space of a random experiment is a mapping that can be represented as an ordered pair {x, f (x)} if for each possible outcome x of the sample space, the following three conditions are satisfied: (i) f (x) ≥ 0∀x values, (ii)∑x f (x) = 1, and (iii) p(x = x) = f (x) unambiguously."
2851,1,"['continuous random variables', 'random variables', 'interval', 'variables', 'case', 'random', 'continuous']", RANDOM VARIABLE,seg_563,"note that in the case of continuous random variables, it is the area in an infinitesimal interval ∫x"
2852,1,['variable'], RANDOM VARIABLE,seg_563,"  f (x)dx, where   = dx∕2 that represents the value of the variable at x (figure 8.4). this means that pr(x = c) = 0 for a fixed c. thus, we have the following definition."
2853,1,"['density function', 'probability density function', 'continuous', 'probability', 'function']", RANDOM VARIABLE,seg_563,definition 8.3 a probability density function (pdf) defined on the continuous
2854,1,"['experiment', 'random']", RANDOM VARIABLE,seg_563,sample space of a random experiment is a mapping that can be represented
2855,0,[], RANDOM VARIABLE,seg_563,"2 2 f (x)dx} satisfying the following conditions: (i) f (x) ≥ 0∀x values,"
2856,1,['function'], RANDOM VARIABLE,seg_563,"verify whether a function defined as f (x; c) = 2cx exp(−cx2) over [0,∞) is a pdf."
2857,1,['range'], RANDOM VARIABLE,seg_563,"solution 8.1 as exp(−cx2) takes nonnegative values, the above is positive for c   0, proving (i). integrate over the range of x to get 2c ∫0"
2858,1,['range'], RANDOM VARIABLE,seg_563,put x2 = t. the range of integration is the same and we get c ∫0
2859,1,"['cumulative distribution function', 'condition', 'distribution function', 'distribution', 'function']", RANDOM VARIABLE,seg_563,"c[ exp(−ct) |∞ 0 ] = 1, proving condition (ii). integrate from 0 to y to get the cdf −c (cumulative distribution function) as c[ exp(−ct) |0 y2 ] = 1 − exp(−cy2). now −c consider p(a   x   b) = ∫a"
2860,1,"['transformation', 'condition']", RANDOM VARIABLE,seg_563,"exp(−ct)dt using the transformation x2 = t. this integrates to c[ exp(−ct) |b2 ] = −c a2 exp(−ca2) − exp(−cb2). putting x = b2 and x = a2 in the cdf, we get (1 − exp(−cb2)) − (1 − exp(−ca2)) = exp(−ca2) − exp(−cb2), which is the same as above. this proves condition (iii). hence it is a pdf for c   0."
2861,1,"['right tail probabilities', 'range', 'distribution function', 'case', 'discrete', 'probability', 'random', 'function', 'cumulative distribution function', 'random variable', 'tail probabilities', 'distribution', 'summation', 'continuous', 'outcomes', 'random variables', 'probabilities', 'variables', 'discrete random variables', 'variable', 'tail']", Cumulative Distribution Function,seg_565,"the cumulative probabilities are computed by summing individual probabilities from the lowest possible x value to a higher number (see figure 8.2). the implicit assumption here is that the random variable is arranged in increasing order of possible values of the outcomes. symbolically fx(x) = pr[x ≤ x]. using the summation notation introduced in chapter 1, this can be written as fx(x) = ∑k≤xp(k). for x = 1,f(x) = p(x) = p(1). for x = 2,f(x) = p(1) + p(2), and so on. the cdf is a jump (or step) function if x is discrete (figure 8.2). in this case, we call it a cumulative probability function. analogously the right tail probabilities is called the survival function s(x). they are related as f(x) = 1 − s(x). if x is continuous, we use integration instead of summation. thus, irrespective of the nature of the random variable, we can define the cumulative distribution function as the probability that the random variable x takes values less than or equal to x, where x is a specified number within the range. obviously the cdf is an increasing function of x. this means that f(x) − f(x − 1) = p(x) for discrete random variables. in the case of continuous random variables, we have (i) f(x) is a nondecreasing function of x, (ii) limx→llf(x) = 0"
2862,1,"['sample', 'probabilities', 'continuous', 'experiment', 'random', 'function', 'tail', 'outcomes']", Cumulative Distribution Function,seg_565,"ul are the lower and upper limits, and (v) f(x) is continuous function of x on the right, with countable number of discontinuities, if any. property (i) follows trivially due to our implicit assumption that the outcomes of the random experiment are arranged in ascending order of their values. this means that if x1 is strictly less than x2, then all sample points that are part of the left tail up to x1 are automatically part of the left tail up to x2. thus, the sum of probabilities up to x1 is strictly less than that up to x2. other properties follow easily due to the increasing nature of f(x), which must eventually equal 1."
2863,1,"['tail', 'probabilities']", Cumulative Distribution Function,seg_565,"as the cdf accumulates probabilities from the left tail, it easily follows that for b   a,"
2864,1,['discrete'], Cumulative Distribution Function,seg_565,b ⎧⎪f(a) + p(k) if x is discrete; ∑ ⎪ k=a+1 f(b) = ⎨⎪ b
2865,1,['continuous'], Cumulative Distribution Function,seg_565,⎪f(a) + f (x)dx if x is continuous. ⎩ ∫a
2866,1,"['table', 'discrete', 'distribution', 'random variable', 'variable', 'random', 'continuous', 'discrete distribution']", Cumulative Distribution Function,seg_565,"this gives pr[a x ≤ b] = f(b) − f(a) (table 8.1). the pdf of a continuous distribution can be obtained from the cdf by differentiation with respect to the random variable, and that of a discrete distribution can be obtained from the cdf by differencing. symbolically,"
2867,1,['limit'], Cumulative Distribution Function,seg_565,provided the limit exists.
2868,1,['function'], Cumulative Distribution Function,seg_565,"verify whether a function defined as f(x; c, d) = (1∕(d − c))[dxc − cxd] over [0, 1] is a cdf."
2869,1,"['function', 'factor']", Cumulative Distribution Function,seg_565,"solution 8.2 differentiate with respect to x to get f (x; c, d) = (1∕(d − c))[cdxc−1 − cdxd−1]. take cd as a common factor to get f (x; c, d) = (cd∕(d − c))[xc−1 − xd−1]. as c   d and 0 ≤ x ≤ 1, the expression inside the square bracket is always positive. this shows that f(x) is a nondecreasing function of x. as ll = 0,f(ll) = f(0) = (1∕(d − c))[0 − 0] = 0, showing that limx→llf(x) = 0. as ul = 1,f(ul) = f(1) = (1∕(d − c))[d − c] = 1, showing that"
2870,1,['interval'], Cumulative Distribution Function,seg_565,"(cd∕(d − c))[1∕c − 1∕d] = 1. here, f(x) does not have discontinuities in the interval [0, 1]. hence all the conditions mentioned above are satisfied, proving that f(x) is indeed a cdf."
2871,1,"['sample', 'random variables', 'probabilities', 'weighted average', 'variables', 'arithmetic mean', 'populations', 'location', 'expected value', 'associated', 'mean', 'random', 'average']", Expected Value,seg_567,"measures of location introduced in chapter 2 refer to a sample. the expected value can be thought of as the arithmetic mean (weighted average) of random variables or populations, where the weights are the probabilities. the mean has a much more broader meaning, as it could be associated with any numeric quantity which may or may not be random. the concept of “expected value” appeared for the first time in the works of christian huygens around 1657."
2872,1,"['mathematical expectation', 'experiment', 'random variable', 'variable', 'expectation', 'mean', 'population', 'random', 'function', 'event']", Expected Value,seg_567,"definition 8.4 mathematical expectation is used to concisely quantify the mean value of an event, an experiment, a random variable, or a function of it in a population."
2873,1,"['random variables', 'variables', 'range', 'expected value', 'population', 'random', 'average']", Expected Value,seg_567,"it is also called the expected or average value of the argument, or the center of mass in physical sciences. it can be any real number within the range for real-valued random variables. the expected value of integer-valued random variables need not be an integer. it is so called because (i) population may be unknown, (ii) population"
2874,1,"['case', 'conditional', 'discrete', 'random', 'function', 'conditional distributions', 'mean', 'populations', 'population', 'distributions', 'functions', 'continuous random variables', 'parameters', 'expected value', 'expectation operator', 'expectation', 'continuous', 'random variables', 'variables', 'population mean']", Expected Value,seg_567,"could be un-enumerable, (iii) it could be a function of the unknown parameters of the population, and (iv) it is just the expected value (and not the exact value) of the population under study. it is defined for discrete as well as continuous random variables, and any well-defined functions of them. it is a scalar for univariate populations, and a vector for multivariate random variables. it is denoted as e(x),e[x], or e x, where e is the expectation operator, followed by the argument. the argument (also called the operand, which is usually an expression or a function in capital letters) of “e” can be any well-defined function of x, including integer and fractional powers of x, or conditional distributions. it can also be another expectation expression. thus, the “e” operator can be recursively nested (section 8.4, p. 358). in the rest of the chapter, we will simply call it the expected value. in the particular case when the argument is x itself, it is called the population mean."
2875,1,"['expected value', 'random variable', 'variable', 'random']", Expected Value,seg_567,definition 8.5 the expected value of a univariate random variable is defined as:
2876,1,['discrete'], Expected Value,seg_567,⎧ ∞ ⎪ ∑ xkpk if x is discrete; ⎪⎨k=−∞ e(x) = ∞ ∞
2877,1,['continuous'], Expected Value,seg_567,⎪ xf (x)dx = xdf(x) if x is continuous. ⎪⎩∫−∞ ∫−∞
2878,1,"['range', 'case', 'discrete', 'random', 'binomial distribution', 'geometric', 'random variable', 'expected values', 'expected value', 'distribution', 'expectation', 'summation', 'binomial', 'continuous', 'poisson', 'random variables', 'variables', 'variable']", Expected Value,seg_567,"whenever this sum is absolutely convergent, we call it the expected value of x. this means that ∫ |x|df(x) ∞ in the continuous case, and ∑x|x|f (x) ∞ in the discrete case. although we have indicated the range as −∞ to ∞, the actual range depends on the random variable. for example, as the binomial distribution takes values between 0 and a positive integer n, all expected values of binomial random variables use the summation range from 0 to n. similarly, as the poisson and geometric distributions take values between 0 and ∞, the summation is carried out in this range only. hence the very first step in finding the expectation is to figure out the exact range (figures 8.3, 8.4)."
2879,1,['probabilities'], Expected Value,seg_567,"as p′ ks are probabilities that sum to 1 (∑kpk = 1, ∫ f (x)dx = 1), the above sum will almost always converge when∑k"
2880,1,"['random variables', 'variables', 'range', 'case', 'expected value', 'random']", Expected Value,seg_567,"∞ ∞ |x|f (x)dx   ∞. on occasion this sum may diverge, in which case we say that the expected value does not exist. this seldom happens for random variables defined over a finite range."
2881,1,"['probabilities', 'weighted average', 'arithmetic mean', 'range', 'expected value', 'random variable', 'variable', 'expectation', 'expectation operator', 'mean', 'random', 'function', 'average']", Expected Value,seg_567,"as mentioned earlier, e(x) is a weighted average of all possible values that the variable can take with corresponding probabilities as the weights. in other words, the notation e(x) can be considered as an expectation operator, operating upon the entire range of its argument (which is not shown, but is understood from context). the argument of this expectation operator is quite often a random variable, with the implied meaning that it is the centroid of the argument. as discussed below, the argument could also be any well-defined function of the random variable (e.g., e(x2),e(x−k)). if all weights (pk or f (xk)) are equal (and the range of x is finite), the expected value reduces to the ordinary average (arithmetic mean) of all possible values. for instance, let x take values in the range 1–n and let pk = 1∕n (so that ∑kpk = 1) then"
2882,1,['expected value'], Expected Value,seg_567,"example 8.3 expected value of c/x2,x ≥1"
2883,1,"['expected value', 'random variable', 'variable', 'random']", Expected Value,seg_567,"find the expected value of the random variable x with pdf f (x) = c∕x2, x ≥ 1, where c is the normalizing constant."
2884,1,"['total probability', 'probability']", Expected Value,seg_567,"solution 8.3 as the total probability must be one, c ∫1"
2885,1,['expected value'], Expected Value,seg_567,−c[1∕x|∞ 1 ] = 1 giving c = 1. the expected value is e(x) = ∫1
2886,1,"['experiment', 'expected value', 'random', 'experiments', 'average']", Expected Value,seg_567,"in many of the examples given below, we are interested in the expected value of random experiments. in such situations, e(x) can be considered as the average value of the experiment, if it is repeated under identical conditions a large number of times."
2887,0,[], Expected Value,seg_567,example 8.4 promotional coupons in cereals pack
2888,1,['probability'], Expected Value,seg_567,"a cereals manufacturer offers a promotional coupon with a new brand of cereals pack. two types of coupons (that carry either 1 point or 2 points) are printed, and exactly one of them is put in each pack. probability that a customer will find a 1-point coupon is p, and a 2-points coupon is q = 1 − p. if a customer purchases n packs of the cereal, what is the expected number of points earned?"
2889,1,"['probabilities', 'scores', 'probability']", Expected Value,seg_567,"solution 8.4 as each pack contains a coupon, the minimum score is n and the maximum score is 2n. these two scores can happen in one way each with respective probabilities pn and qn. the customer can score (n + 1) points in n ways (exactly one cereal pack contains a 2-points coupon, and the rest (n − 1) packs contain 1-point coupons) with probability pn−1q, so that the probability of score"
2890,0,['n'], Expected Value,seg_567,"n) pn−1q, and so on. hence the expected score is found by summing the"
2891,1,['probabilities'], Expected Value,seg_567,points earned multiplied by the corresponding probabilities as e(x) = (n ∗ pn +
2892,0,['n'], Expected Value,seg_567,binomial expansion for (p + q)n = 1 to simplify the above as n + ( 1
2893,0,[], Expected Value,seg_567,example 8.5 coin tossing game
2894,1,"['expected value', 'loss', 'trials', 'tail']", Expected Value,seg_567,"consider a simple game in which a fair coin is tossed. you win $100 if the head turns up. if it is a tail that turns up, you lose $90. what is your expected loss or gain in one toss? what is the expected value in n ( 2) tosses? does the expected value converge when a sufficiently large number of trials are conducted?"
2895,1,['expected value'], Expected Value,seg_567,"solution 8.5 as the coin is fair, p(head) = p(tail) = 1/2. thus, the expected value in one toss = (1∕2) ∗ 100 − (1∕2) ∗ 90 = 50 − 45 = 5, which is a gain. if this game is repeated n times, our expected gain is 5 ∗ n. this expected value is divergent as n → ∞. the expected value can also be a negative number. in the above example, if the winning amount is 90 and losing amount is 100, the expected value is negative. another example is given below:"
2896,1,"['loss', 'experiment']", Expected Value,seg_567,"consider the experiment of rolling a fair die once. if the number that comes on top is an even integer, you win c units of money. if the number on top is odd, you lose 2*c units of money. what is your expected gain or loss?"
2897,1,['loss'], Expected Value,seg_567,"solution 8.6 if the number on top is 2, 4, or 6, the winning amount is c. if it is 1, 3, or 5, the losing amount is 2c. as the die is fair, each of them have equal probability 1/6. let x denote the loss or gain. then e(x) = −2c ∗ p(x = 1) + c ∗ p(x = 2) − 2c ∗ p(x = 3) + c ∗ p(x = 4) − 2c ∗ p(x = 5) + c ∗ p(x = 6) = (1∕6)[−6c + 3c] = −(3∕6)c = −c∕2, which is a negative number for c 0."
2898,0,[], Expected Value,seg_567,example 8.7 egg hatching
2899,1,['table'], Expected Value,seg_567,a farmer hatches between 40 and 50 eggs every week. total number of chickens hatched are given in table 8.2. find the expected number of chickens that will be hatched next week if n eggs are kept for hatching.
2900,1,"['probabilities', 'table', 'probability', 'population', 'expected values']", Expected Value,seg_567,"solution 8.7 we assume that the eggs are uniformly sampled from a hypothetical population with a constant probability p of hatching. from table 8.2, we get the probabilities of hatching as p1 = 37∕42 = 0.881, p2 = 43∕50 = 0.86, p3 = 0.9592, p4 = 0.88, p5 = 0.8889, p6 = 0.9575, p7 = 0.9167, p8 = 0.8571, p9 = 48∕50 = 0.96, and p10 = 45∕48 = 0.9375. the expected probability of hatching is found as e(p) = [0.881 + 0.86 + 0.9592 + .. + 0.9375]∕10 = 9.09778∕10 = 0.909778. if n eggs are kept for hatching, the expected number of chickens hatched is 0.909778*n. the expected values for n = 40–50 are [36.39,37.30,38.21,39.12,40.03,40.94,41.85, 42.76, 43.67,44.58,45.49]. as the answer must be a whole integer, we could round-off the above values to the nearest integer."
2901,1,"['poisson', 'random', 'range', 'geometric distributions', 'distribution', 'random variable', 'variable', 'summation', 'binomial', 'event', 'geometric', 'distributions']", Range for Summation or Integration,seg_569,"we have taken the general range for x anywhere on the real line in the above definition. range of x depends upon the random variable. for example, poisson and geometric distributions assume integer values in 0 to ∞, whereas binomial distribution has range 0–n. thus, the range for summation (or integration) collapses to the range of the random variable or event involved (see figure 8.4)."
2902,1,['expected value'], Range for Summation or Integration,seg_569,example 8.8 expected value of marks
2903,0,['n'], Range for Summation or Integration,seg_569,"a multiple choice exam comprises of 50 questions, each with 5 answers. all correctly marked answers score 1 mark, and all wrong answers get a penalty of 1/4 mark. what is the expected number of marks obtained by a student who guesses n questions?"
2904,1,['expected value'], Range for Summation or Integration,seg_569,"solution 8.8 assume that the student has actually obtained x correct and (n − x) wrong answers. then the marks obtained is x ∗ 1 − (n − x) ∗ (1∕4) = (5∕4) ∗ x − n∕4 = (5x − n)∕4 = y (say). as xi can take the possible values 0, 1, … , n we get the expected value as e(x) = 1 ∗ 1∕5 + 0 ∗ 4∕5 = 1∕5. now e(y) = (5e(x) − n)∕4 = (5x(n∕5) − n)∕4 = 0. hence the answer is zero. as it is an expected value, the actual score earned when all n questions are attempted could fluctuate around the expected value."
2905,1,"['functions', 'gumbel', 'logistic', 'distribution', 'expected value', 'extreme value', 'exponential', 'statistical', 'distributions']", Expectation Using Distribution Functions,seg_571,"some of the statistical distributions have simple expressions for the cdf. examples are the exponential, logistic, and extreme value (gumbel) distributions. the expected value could be found in terms of the distribution functions as follows:"
2906,1,['discrete'], Expectation Using Distribution Functions,seg_571,"theorem 8.1 if x is discrete, then e(x) = ∑kp(x ≥ k)."
2907,1,['loss'], Expectation Using Distribution Functions,seg_571,"proof: without loss of generality, assume that x takes the values 1,2,.. then"
2908,1,['range'], Expectation Using Distribution Functions,seg_571,"as e(x ± c) = e(x) ± c (section 8.3 in page 348), the result follows for arbitrary range of x."
2909,1,"['incomplete beta', 'beta function', 'incomplete beta function', 'summation', 'function']", Expectation Using Distribution Functions,seg_571,example 8.9 closed form summation of incomplete beta function
2910,1,"['incomplete beta', 'beta function', 'incomplete beta function', 'function']", Expectation Using Distribution Functions,seg_571,is the incomplete beta function (ibf) defined in chapter 7 (p. 277).
2911,1,"['continuous', 'symmetry', 'mean', 'function', 'error']", Expectation Using Distribution Functions,seg_571,"because from chapter 6, we know that the mean of bino(n, p) is np. note that the ibf is a continuous function of p in equation (8.6), which is being summed. the lhs gives exact result only for small n values, as error could propagate for large n values. for example, if n = 8 and p = 0.9 (equation 8.6 ) gives 7.2, which is exact. for n = 12 and p = 0.5, equation (8.6) gives 5.999756, whereas np = 6. use symmetry relation to get a similar expression"
2912,0,['n'], Expectation Using Distribution Functions,seg_571,"which is better suited for p   0.5. hence the closed form expression (on the rhs) in equations (8.6) and (8.7) is extremely useful to evaluate sums on the lhs, especially when n is large."
2913,1,['summation'], Expectation Using Distribution Functions,seg_571,example 8.10 closed form for infinite summation of ibf
2914,1,"['tail', 'tail probabilities', 'probabilities']", Expectation Using Distribution Functions,seg_571,"solution 8.10 we have seen in chapter 6 (p. 228) that if x ∼ nbino(k, p), the lower tail probabilities are found as"
2915,1,"['tail', 'tail probability', 'probability']", Expectation Using Distribution Functions,seg_571,upper tail probability (sf) is obtained by subtraction as ∑x
2916,1,"['symmetry', 'mean']", Expectation Using Distribution Functions,seg_571,"because the mean of nbino(k, p) is kq/p. using symmetry relation, a similar expression follows easily as"
2917,1,"['convergence', 'tail']", Expectation Using Distribution Functions,seg_571,"although an infinite sum, rapid convergence of equation (8.10) occurs for p values in the right tail ( 0.5, especially for p above 0.80 or equivalently q 0.20) so that it can be truncated at j = 2k, giving the lhs as ∑j"
2918,0,[], Expectation Using Distribution Functions,seg_571,"k 0 ip(k, j + 1) for p   0.5. however, the rhs provides a simple and elegant expression, which avoids the expensive evaluation of ibf."
2919,1,"['continuous', 'variates']", Expectation Using Distribution Functions,seg_571,example 8.11 e(x) in terms of f(x) for continuous variates
2920,1,"['continuous random variable', 'random variable', 'variable', 'random', 'continuous']", Expectation Using Distribution Functions,seg_571,"if x is a continuous random variable with cdf f(x), then"
2921,1,['function'], Expectation Using Distribution Functions,seg_571,solution 8.11 consider ∫0 ∞[1 − f(x)]dx. as [1 − f(x)] is the survival function
2922,1,['tail'], Expectation Using Distribution Functions,seg_571,"(right tail area), it can be written as [1 − f(x)] = ∫x"
2923,0,[], Expectation Using Distribution Functions,seg_571,"t =0 f (t)dxdt. as f (t) is a constant while integrating with respect to x, the inner integral simplifies to t ∗ f (t). thus, the rhs becomes ∫t="
2924,1,['results'], Expectation Using Distribution Functions,seg_571,0 ∞ t ∗ f (t)dt. combine both results to get the desired answer.
2925,1,"['quartiles', 'quantile', 'quantiles', 'case', 'distribution', 'percentiles']", Expectation Using Distribution Functions,seg_571,"the quantiles of a distribution was defined in chapter 3. this includes the quartiles, deciles, and percentiles. we can express the quantiles using the cdf as follows: the kth quantile is that value of x for which f(x) = k∕100. in the case of quartiles, the divisor is 4 so that we get 3 quartiles that divide the total area into 25, 50, and 75 points."
2926,0,[], Expectation Using Distribution Functions,seg_571,"proof: by definition,"
2927,1,['results'], Expectation Using Distribution Functions,seg_571,t[1 − f(x)]dx. now let t tend to infinity to get the final results as
2928,1,"['poisson', 'distribution', 'poisson distribution']", Expectation Using Distribution Functions,seg_571,example 8.12 e(x) and e(x2) of a poisson distribution
2929,1,"['poisson', 'poisson random variable', 'random variable', 'variable', 'random']", Expectation Using Distribution Functions,seg_571,find e(x) and e(x2) for a poisson random variable.
2930,1,['summation'], Expectation Using Distribution Functions,seg_571,"∞ =2  x−2∕(x − 2)!. putting x − 2 = y, the summation reduces"
2931,0,['e'], Expectation Using Distribution Functions,seg_571,to e  giving  2e− e  =  2. substitute for the second sum from above to get
2932,1,"['poisson', 'probabilities', 'gamma', 'distribution', 'tail probabilities', 'function', 'tail', 'gamma function', 'poisson distribution']", Expectation Using Distribution Functions,seg_571,"e(x2) =  2 +  . from chapter 6, page 234, we know that the tail probabilities of a poisson distribution is related to the incomplete gamma function as follows:"
2933,1,"['poisson', 'random', 'poisson random variable', 'random variable', 'events', 'variable', 'expectation', 'associated', 'event']", Expectation Using Distribution Functions,seg_571,mathematical expectation can also be defined on events associated with a random variable. consider the event y that a poisson random variable x takes even values. the
2934,0,[], Expectation Using Distribution Functions,seg_571,y even so that u takes all integer values starting with 0. as before e(u) = ∑u
2935,1,"['continuous random variable', 'random variable', 'variable', 'random', 'continuous']", Expectation Using Distribution Functions,seg_571,"if x is a continuous random variable, and d is a constant, find the unknowns a,b,c such that e[x − d|x   d] = ∫a"
2936,1,"['continuous', 'conditional', 'expectation', 'probability', 'conditional probability', 'function']", Expectation Using Distribution Functions,seg_571,"solution 8.13 consider the probability y = p[x − d|x d]. then p[y ≤ y] = p[x − d|x d ≤ y]. this probability is the same as p[x − d ≤ y|x d]. as x is continuous, this can be written as p[d x y + d|x d]. using the conditional probability formula p[a|b] = p[a ∩ b]∕p[b] this becomes p[d x y + d]∕p[x d]. the numerator can be written in terms of unconditional cdf of x as fx(y + d) − fx(d) if d is positive, and fx(d) − fx(y − d) if d is negative. the denominator being the survival function can be written as 1 − fx(d). now apply equation 8.10 on the expectation of y as e[y] = ∫0 ∞[1 − gy (y)]dy. substitute for gy (y) from the above to get e[y] ="
2937,1,"['change of variable', 'variable transformation', 'variable', 'transformation']", Expectation Using Distribution Functions,seg_571,∫0 ∞[1 − [fx(y + d)]∕[1 − fx(d)]dy. apply a change of variable transformation
2938,1,['variance'], Expectation Using Distribution Functions,seg_571,example 8.14 variance in terms of f(x)
2939,1,"['continuous random variable', 'random variable', 'variable', 'random', 'continuous', 'variance']", Expectation Using Distribution Functions,seg_571,"if x is continuous random variable, the variance of x can be expressed as  x2 = ∫0"
2940,1,"['integer part', 'expectation', 'exponential']", Expectation Using Distribution Functions,seg_571,example 8.15 expectation of integer part of exponential
2941,1,['integer part'], Expectation Using Distribution Functions,seg_571,"if x ∼ exp ( ) find e[⌊x⌋], where ⌊x⌋ denotes the integer part of x."
2942,1,"['method', 'geometric distribution', 'integer part', 'distribution', 'mean', 'geometric']", Expectation Using Distribution Functions,seg_571,"solution 8.15 we have seen in chapter 6, page 218, that the integer part of x has a geometric distribution with q = exp (− ). using the above method, the problem reduces to finding the mean of geo(p), where p = 1 − q = [1 − exp(− )]. hence e(y) = e(⌊x⌋) is exp(− )∕[1 − exp(− )]."
2943,1,"['functions', 'random variables', 'method', 'variables', 'distribution', 'expected value', 'random', 'expected values', 'distributions']", EXPECTATION OF FUNCTIONS OF RANDOM VARIABLES,seg_573,"in many practical applications, we have to work with simple mathematical functions of random variables. a possible method is to first find the distribution of these functions and then find its expected value. however, the following theorem gives us a simple method to find expected values of functions of random variables without either deriving their distributions or knowing about the exact distributions."
2944,1,"['distribution', 'summation']", EXPECTATION OF FUNCTIONS OF RANDOM VARIABLES,seg_573,"there is another way to find e[g(x)] if g(x) has a well-defined distribution. instead of using the original pdf inside the summation or integration, we could find the distribution of y = g(x), and find e[y] of this distribution. as an example, let x ∼ n(0, 1) and y = x2. we wish to find 2"
2945,1,"['standard normal', 'distribution', 'expected value', 'degree of freedom', 'normal', 'standard']", EXPECTATION OF FUNCTIONS OF RANDOM VARIABLES,seg_573,"∞ ∞ x2 (x)dx, where  ( ) is the standard normal pdf. we know that y = x2 has a central  2 distribution with 1 degree of freedom, whose expected value is 1. hence e[x2] is also 1. this technique may not always work. in the above example, if we wanted e[x2 − 2x + 3], we need to resort to the first approach because x2 − 2x + 3 does not have a simple distribution."
2946,1,"['random variables', 'variables', 'discrete', 'random', 'continuous']", Properties of Expectations,seg_575,"let x and y be any two random variables, discrete or continuous, univariate or multivariate. in the following discussion, it is assumed that e(x) and e(y) exist (they are finite)."
2947,1,['expected value'], Properties of Expectations,seg_575,theorem 8.2 the expected value of a constant is constant.
2948,1,"['probabilities', 'case', 'discrete', 'summation', 'continuous', 'distributions']", Properties of Expectations,seg_575,"proof: the proof follows trivially because the constant can be taken outside the summation (discrete case) or integration (continuous case) and what remains is either the summation or integration of probabilities that evaluates to a 1.0. symbolically, e(c) = c. here, c is a scalar constant for univariate distributions, and a constant vector for multivariate distributions. symbolically, e(c) = ∑kcpx=k = c∑kpx=k = c, for the discrete case. if x is continuous, e(c) = ∫xcp(x)dx = c∫xp(x)dx = c."
2949,1,"['expected value', 'function', 'linear']", Properties of Expectations,seg_575,"theorem 8.3 the expected value of linear function c ∗ x is c times the expected value of x, where c is a nonzero constant and the expected value exists."
2950,1,"['continuous', 'summation', 'case', 'discrete']", Properties of Expectations,seg_575,"proof: as above, the constant can be taken outside the summation (discrete case) or integration (continuous case) and what remains is either the summation or integration of x that evaluates to e(x). applying the multiplier c gives the result as c ∗ e(x)."
2951,1,"['linear', 'linear combination', 'combination', 'expected value', 'random variable', 'variable', 'random']", Properties of Expectations,seg_575,"theorem 8.4 prove that expected value of linear combination e(a ∗ x + b) = a ∗ e(x) + b for any random variable x, and nonzero constant a."
2952,1,"['expected values', 'discrete', 'continuous']", Properties of Expectations,seg_575,"proof: let x be discrete, and take values x1, x2, … , x∞. from the definition of expected values, e(ax + b) = ∑k(axk + b)pxk = a∑kxkpxk + b∑kpxk = ae(x) + b because∑kpxk = 1. if x is continuous, e(ax + b) = ∫ (ax + b)p(x)dx = a ∫ xp(x)dx +"
2953,1,"['moment', 'distribution', 'random variable', 'variable', 'random']", Properties of Expectations,seg_575,"b ∫ p(x)dx = ae(x) + b. we have not made any assumption on the distribution of the random variable x in this theorem, but only the existence of the first moment."
2954,1,['binomial'], Properties of Expectations,seg_575,example 8.16 e(n − x) of a binomial
2955,1,"['parameters', 'distribution', 'binomial', 'binomial distribution']", Properties of Expectations,seg_575,"if x has a binomial distribution with parameters n and p (bino(n, p)), find e(n − x), of a binomial distribution."
2956,1,"['random variables', 'variables', 'random']", Properties of Expectations,seg_575,"theorem 8.5 if x and y are two random variables, e(x ± y) = e(x) ± e(y) = e(y ± x) and e(ax ± by) = ae(x) ± be(y)."
2957,1,"['random variables', 'sum of two random variables', 'variables', 'expectation', 'random']", Properties of Expectations,seg_575,"proof: the sum of two random variables x and y makes sense only when they are compatible random variables. the first result follows trivially by distributing the summation or integration over the individual components (connected by + or −). the second result can be proved using the fact that e(cx) = ce(x), twice. this is called the linearity property of expectation."
2958,1,"['random variables', 'independent', 'variables', 'expected value', 'independent random variables', 'random']", Properties of Expectations,seg_575,8.3.1.1 expected value of independent random variables we defined independent random variables in chapter 5 as p(xy) = p(x) ∗ p(y).
2959,1,"['functions', 'random variables', 'random', 'probabilities', 'independent', 'variables', 'results', 'events', 'probability', 'outcome', 'expected values', 'outcomes']", Properties of Expectations,seg_575,"this result is defined in terms of probabilities. as expected values can be considered as functions of random variables with probabilities as weights, we could get analogous results in terms of expected values. two outcomes are independent if knowing the outcome of one does not change the probabilities of the outcomes of the other. when two events are independent, we find the probability of both events happening by multiplying their individual probabilities."
2960,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random']", Properties of Expectations,seg_575,"definition 8.6 if x and y are two independent random variables, then e(xy) = e(x) ∗ e(y)."
2961,1,"['independent', 'probabilities', 'discrete', 'independence']", Properties of Expectations,seg_575,"let x and y be discrete. then e(xy) = ∑k(xkyk)pxk ,yk = ∑kxkykpxk pyk using the above theorem on the independence of probabilities. pairing xk with pxk , this becomes ∑k(xkpxk )(ykpyk ). hence, if x and y are independent, then e[xy] = e[x]e[y]."
2962,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random']", Properties of Expectations,seg_575,"theorem 8.6 if x and y are two independent random variables, then p(x ≤ x,y ≤ y) = p(x ≤ x) ∗ pr(y ≤ y)."
2963,1,['discrete'], Properties of Expectations,seg_575,proof: let both be discrete. write the lhs as
2964,1,['summation'], Properties of Expectations,seg_575,"using the properties of summation mentioned in chapter 1, split this into two products as"
2965,1,['independence'], Properties of Expectations,seg_575,due to the independence of x and y.
2966,1,"['functions', 'linear', 'linear combination', 'factors', 'combination', 'expected value']", Properties of Expectations,seg_575,"theorem 8.7 expected value of a linear combination of scaled functions is the linear combination of expected value of the functions with respective scaling factors. symbolically, e(c1g1(x) + c2g2(x) + · · · +) = c1e(g1(x)) + c2e(g2(x)) + · · · +, where the constants ci’s are any real numbers."
2967,1,"['functions', 'random variables', 'independent', 'continuous', 'variables', 'range', 'independent random variables', 'random', 'expectations']", Properties of Expectations,seg_575,"definition 8.7 if x and y are two independent random variables, and g(x), h(y) are everywhere continuous functions of x and y defined on the range of x and y, then e(g(x) ∗ h(y)) = e(g(x)) ∗ e(h(y)) if the expectations on the rhs exist."
2968,1,"['continuous', 'summation', 'case', 'discrete']", Properties of Expectations,seg_575,the proof follows exactly as done above using summation in the discrete case and integration in the continuous case.
2969,1,"['poisson', 'method', 'poisson random variable', 'random variable', 'variable', 'random', 'expected values']", Properties of Expectations,seg_575,prove that e[x2] can be found easily using e[x(x − 1)] and e[x] when the denominator of the random variable involves factorials. find a similar method to find e[x3]. use this technique to find the expected values of a poisson random variable.
2970,1,"['poisson', 'poisson random variable', 'expected value', 'random variable', 'variable', 'expectation', 'random']", Properties of Expectations,seg_575,"solution 8.17 write x2 = x(x − 1) + x. use the above result to break the expected value of rhs into two terms to get e[x2] = e[x(x − 1)] + e[x]. write x3 = x(x − 1)(x − 2) + 3x(x − 1) + x. take expectation to get e(x3) = e[x(x − 1)(x − 2)] + 3e[x(x − 1)] + e[x]. for a poisson random variable, e[x(x − 1)(x −"
2971,1,"['random variables', 'independent', 'variables', 'random']", Properties of Expectations,seg_575,"theorem 8.8 if x1,x2, … ,xm are independent compatible random variables, then e(c1x1 + c2x2 + · · · + cmxm) = ∑icie(xi)."
2972,1,"['random variables', 'variables', 'random', 'expectations']", Properties of Expectations,seg_575,"proof: this follows easily by repeated application of the above result. if x1,x2, … ,xm are m compatible random variables, then e(x1 + x2 + · · · + xm) = e(x1) + e(x2) + · · · + e(xm), provided each of the expectations exist."
2973,1,"['random variables', 'variables', 'bernoulli', 'random']", Properties of Expectations,seg_575,example 8.18 sum of bernoulli random variables
2974,1,"['random variables', 'variables', 'random']", Properties of Expectations,seg_575,"solution 8.18 as x and y are compatible, we apply the above theorem to get the result. this theorem can be extended to any number of random variables."
2975,1,"['success', 'expectation', 'probability']", Properties of Expectations,seg_575,"proof: the proof follows easily by taking the expectation term by term. if the probability of success is equal (the same probability p for each of them), then e(x1 + x2 + · · · + xm) = mp."
2976,1,"['random variables', 'variables', 'mean', 'random']", Properties of Expectations,seg_575,"theorem 8.10 if x1,x2, … ,xn are random variables, each with mean  , the mean"
2977,1,['range'], Properties of Expectations,seg_575,"proof: for simplicity assume that x and y have the same range. consider z = y − x. as x ≤ y , z is always positive. hence e[z] ≥ 0. this means that e(y − x) ≥ 0, or equivalently e[y] ≥ e[x]."
2978,1,"['random variables', 'variables', 'random']", Properties of Expectations,seg_575,"as noted above, the sum on the left makes sense only when the random variables are compatible. any of the “+” can also be replaced by a “−” with the corresponding sign changed on the rhs accordingly."
2979,1,"['functions', 'expected value', 'binomial']", Properties of Expectations,seg_575,example 8.19 expected value of functions of binomial
2980,1,"['parameters', 'distribution', 'binomial', 'binomial distribution']", Properties of Expectations,seg_575,"if x has a binomial distribution with parameters n and p (bino(n, p)), then (i) e(x∕n) = p and (ii) e(n − x)2 = nq[n − p(n − 1)] = nq + n(n − 1)q2."
2981,1,"['moment', 'observation', 'expectation operator', 'expectation', 'second moment']", Properties of Expectations,seg_575,"solution 8.19 the first result follows trivially from the above by replacing c with 1∕n, and taking 1∕n as a constant outside the expectation operator. for (ii) expand the quadratic as e(n − x)2 = e(n2 − 2nx + x2). take term by term expectation to get e(n2) − 2ne(x) + e(x2)). now apply above theorems to get n2 − 2n ∗ np + (np + n(n − 1)p2). this simplifies to nq[n − p(n − 1)]. write p as 1 − q so that [n − p(n − 1)] = n − (1 − q)(n − 1). the n cancels out giving 1 + q(n − 1). substitute in the above to get e(n − x)2 = nq[1 + (n − 1)q] = nq + n(n − 1)q2. this result can also be obtained directly from the observation that n − x has bino(n, q) (example 6.11 1, p. 203) so that its second moment is nq + n(n − 1)q2."
2982,0,[], Properties of Expectations,seg_575,example 8.20 expected heads in flips of four coins
2983,0,[], Properties of Expectations,seg_575,what is the expected number of heads in four flips of a fair coin?
2984,0,[], Properties of Expectations,seg_575,required expected number is 2
2985,1,['expectation'], Properties of Expectations,seg_575,"proof: consider the expression (ax + y∕a)2 = a2x2 + y2∕a2 + 2 ∗ a ∗ (1∕a) ∗ xy = a2x2 + y2∕a2 + 2 ∗ xy . as the lhs being a square is always nonnegative, this can be written as ±2xy ≤ a2x2 + y2∕a2. take expectation of both sides to get"
2986,0,[], Properties of Expectations,seg_575,this follows easily by taking y = x.
2987,1,"['functions', 'random variables', 'variables', 'exponential', 'random']", Expectation of Continuous Functions,seg_577,"some applications involve functions of random variables. examples are fractional powers of x, integer powers of x, exponential, logarithmic and trigonometric functions, and other transcendental functions."
2988,1,['expected value'], Expectation of Continuous Functions,seg_577,example 8.21 expected value of exp( x)
2989,1,['expected value'], Expectation of Continuous Functions,seg_577,"if x is bino(n, p) find expected value of exp( x), where   is a nonzero constant."
2990,1,"['moment', 'random']", Expectation of Continuous Functions,seg_577,"corollary 3 if g(x) is undefined for at least one value of x, then e(g(x)) does not exist. for instance, the first inverse moment e(1∕x) is undefined for all random variables that assume a nonzero value for x = 0 (i.e., f (x = 0) ≠ 0)."
2991,1,"['random variables', 'variables', 'random']", Expectation of Continuous Functions,seg_577,prove that e(1∕x) ≥ 1∕e(x) for positively defined random variables x.
2992,1,['mean'], Expectation of Continuous Functions,seg_577,solution 8.22 let   be the mean of x. then e[(x −  )(1∕x) − 1∕ ] = e[(x −  )(  − x)∕ x = −e[(x −  )2∕( x) ≤ 0.
2993,1,"['continuous', 'range', 'function']", Expectation of Continuous Functions,seg_577,"definition 8.8 if the function g(x) is everywhere continuous in the range of x, then"
2994,1,['discrete'], Expectation of Continuous Functions,seg_577,⎧ ∞ ⎪ ∑ g(x)f (x) if x is discrete; ⎪⎨x=−∞ e(g(x)) = ∞
2995,1,['continuous'], Expectation of Continuous Functions,seg_577,⎪ g(x)f (x)dx if x is continuous. ⎪⎩∫−∞
2996,1,"['expected value', 'function']", Expectation of Continuous Functions,seg_577,example 8.23 expected value of a function
2997,1,"['poisson', 'distribution', 'poisson distribution']", Expectation of Continuous Functions,seg_577,example 8.24 e((− 1)x) of a poisson distribution
2998,1,['summation'], Expectation of Continuous Functions,seg_577,"∞ =0 (−1)x exp(− ) x∕x!. using the above theorem, we take the constant outside the summation to get"
2999,1,"['expected value', 'function', 'factor']", Expectation of Continuous Functions,seg_577,"corollary 4 expected value of a scaled function is the scaling factor times the expected value of the function. symbolically, e(c ∗ g(x)) = c ∗ e(g(x))."
3000,1,"['geometric distribution', 'distribution', 'moments', 'geometric']", Expectation of Continuous Functions,seg_577,example 8.25 moments of geometric distribution qx/2p
3001,1,"['distribution', 'variance', 'mean']", Expectation of Continuous Functions,seg_577,find the mean and variance of a distribution defined as
3002,0,[], Expectation of Continuous Functions,seg_577,"q p if x ranges from 0, 2, 4, 6, … ,∞ f (x; p) = { 0"
3003,1,"['expectation', 'standard']", Expectation of Continuous Functions,seg_577,"solution 8.25 put y = x∕2 to get the standard form. take expectation of both sides to get e(y) = e(x)∕2 = q∕p, so that e(x) = 2q∕p. similarly, v(x) = 4 v(y) = 4q∕p2."
3004,1,"['gumbel', 'parameters', 'normal', 'parameter', 'population', 'variance', 'distributions']", Variance as Expected Value,seg_579,"the variance is a measure of spread in the population. this is captured in a single parameter for normal, laplace, gumbel, and some other distributions, but is a function of two or more parameters for others."
3005,1,"['random variable', 'variable', 'random', 'variance']", Variance as Expected Value,seg_579,definition 8.9 variance of a random variable is  x2 = e[(x − e(x)]2 = e(x2) − e(x)2.
3006,1,"['population variance', 'population mean', 'discrete', 'mean', 'population', 'continuous', 'variance']", Variance as Expected Value,seg_579,"here, e(x) is the population mean, which we denote by  x. if  x = 0, the population variance takes the simple form  x2 = e(x2). the above can be expressed as e[(x − e(x)]2 = ∑k(xk −  )2px(xk) when x is discrete, and ∫x(x −  )2f (x)dx = ∫x(x −  )2df(x) when x is continuous."
3007,1,['variance'], Variance as Expected Value,seg_579,8.3.3.1 properties of variance
3008,1,"['expectation', 'table']", Variance as Expected Value,seg_579,"the proof follows trivially by expanding (x −  )2 = x2 − 2 x +  2, then taking expectation term by term, and using e[x] =   in the middle term (table 8.4)."
3009,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random', 'variance']", Variance as Expected Value,seg_579,2. the variance of independent random variables are additive. symbolically v(x +
3010,1,"['random variables', 'independent', 'additivity property', 'variables', 'variates', 'independent random variables', 'expectation', 'random']", Variance as Expected Value,seg_579,"y) = v(x) + v(y). this is known as the additivity property, which is valid for any number of independent random variables. we prove it for two variates x and y. by definition, var(x + y) = e[(x + y) − e[x + y]]2. use e[x + y] = e[x] + e[y] in the inner expectation and combine with (x + y) to get rhs as e[(x − e[x]) + (y − e[y])]2. expand as a quadratic to get e[(x − e[x])2 + (y − e[y])2 + 2(x − e[x])(y − e[y])]. now take term by term expectation and use e(x − e[x])(y − e[y]) = 0 (as x and y are independent, e(xy) = e(x) ∗ e(y) so that e(x − e[x])(y − e[y]) = 0) to get the result."
3011,1,['expectation'], Variance as Expected Value,seg_579,"by definition var(c ∗ x) = e[cx − e(cx)]2. as c is a constant, it can be taken outside the expectation to get c2 ∗ e[x − e(x)]2."
3012,1,"['linear', 'random variables', 'variables', 'results', 'random', 'variance']", Variance as Expected Value,seg_579,"by definition var(c ∗ x + b) = e[cx + b − e(cx + b)]2. the +b and −b cancels out giving e[cx − e(cx)]2 = c2 ∗ var(x). replace b by −b to get a similar result. the above two results allow us to find the variance of any linear combination of random variables by finding the var(x) just once, and doing simple arithmetic with the constants to get the desired result."
3013,1,"['transformation', 'variance', 'change of origin']", Variance as Expected Value,seg_579,by definition var(x + b) = e[x + b − e(x + b)]2. the +b and −b cancels out giving e[x − e(x)]2 = var(x). replace b by −b to get a similar result. this result shows that a change of origin transformation does not affect the variance.
3014,1,"['independent', 'table']", Variance as Expected Value,seg_579,m =1 var(xj) if xj’s are independent. this can be proved by induction using the above result 1 (see table 8.4).
3015,1,"['random variables', 'variables', 'random']", Variance as Expected Value,seg_579,"theorem 8.14 if x1,x2, … ,xn are random variables, each of which are pair-wise"
3016,1,"['variance', 'mean']", Variance as Expected Value,seg_579,"uncorrelated with the same mean   and variance  2, then the variance of xn =  2∕n."
3017,1,"['distribution', 'binomial', 'binomial distribution', 'variance']", Variance as Expected Value,seg_579,example 8.26 variance of y = n − x of binomial distribution
3018,1,"['parameters', 'distribution', 'binomial', 'binomial distribution', 'variance']", Variance as Expected Value,seg_579,"if x has a binomial distribution with parameters n and p, derive the variance of y = n − x."
3019,1,['variance'], Variance as Expected Value,seg_579,"solution 8.26 this is already derived in chapter 6 (p. 203). here we use the above property to derive it. var(y) = var(n − x) = var(n) + (−1)2var(x). as the variance of a constant is zero, the rhs simplifies to var(x). hence v(x) = var(y) = npq. this is obtainable directly because n − x ∼ bino(n, q)."
3020,1,['variance'], Variance as Expected Value,seg_579,example 8.27 variance of points earned in cereals coupon
3021,1,['variance'], Variance as Expected Value,seg_579,in the cereals coupon example 8.4 in page 342 find the variance on the number of points earned.
3022,1,"['expected value', 'probability', 'associated', 'event']", Variance as Expected Value,seg_579,"solution 8.27 let xi denote the event associated with ith packet. then xi takes the value 1 with probability p and 2 with probability 1 − p so that the expected value is 1.p + 2.(1 − p) = 2 − p. write this as 1 + (1 − p) = 1 + q. if x ≥ 1 packets are bought, e(x) = x1 + x2 + · · · + xn = n(1 + q). e(x2) = 12 ∗ p + 22 ∗ q = p + 4q = 1 + 3q using p + q = 1. from this v(xi) = e(xi"
3023,1,"['variables', 'symmetric']", Covariance as Expected Value,seg_581,"covariance is a nonstandardized measure of the dependency between the variables involved. we denote it by cov(x,y). the order of the variables x and y is unimportant, as it is symmetric in the variables involved."
3024,1,"['random variables', 'variables', 'covariance', 'random']", Covariance as Expected Value,seg_581,"definition 8.10 covariance of two random variables x and y is cov(x,y) = e(xy) − e(x)e(y) = e(x − e[x])(y − e[y]) = e(y − e[y])(x − e[x])."
3025,1,"['covariance', 'quantitative']", Covariance as Expected Value,seg_581,8.3.4.1 properties of covariance covariance satisfies several interesting properties listed below. it is assumed that both x and y are quantitative.
3026,1,"['random variables', 'independent', 'variables', 'independent random variables', 'covariance', 'random']", Covariance as Expected Value,seg_581,1. the covariance of two independent random variables is zero this follows from the above definition because e(xy) = e(x) ∗ e(y) when x and y are independent.
3027,1,"['linear', 'independent', 'linear combination', 'combination', 'random variable', 'variable', 'covariance', 'random']", Covariance as Expected Value,seg_581,"2. the covariance of a random variable with an independent linear combination is additive. symbolically, cov(x + y ,z) = cov(x,z) + cov(y ,z). by definition"
3028,1,"['results', 'independent']", Covariance as Expected Value,seg_581,"as z is independent of x + y ,e(x + y)z = e(xz) + e(yz). also, e(x + y) = e(x) + e(y). substitute in rhs to get e(xz) + e(yz) − [e(x) + e(y)]e(z). rewrite this as [e(xz) − e(x)e(z)] + [e(yz) − e(y)e(z)] = cov(x,z) + cov(y ,z). similar results can be derived for cov(x − y ,z) = cov(x,z) − cov(y ,z) (if z is independent of x − y), cov(x,y + z) = cov(x,y) + cov(x,z) (if x is independent of y + z), and cov(x,y − z) = cov(x,y) − cov(x,z) (if x is independent of y − z)."
3029,1,"['transformation', 'covariance', 'change of origin']", Covariance as Expected Value,seg_581,"6. cov((x − a)∕c, (y − b)∕d) = cov(x,y)∕(cd). as the change of origin transformation does not affect the covariance, the lhs is equal to cov(x∕c,y∕d). now apply above result with a = 1∕c, b = 1∕d to get the result."
3030,1,"['covariance', 'linear']", Covariance as Expected Value,seg_581,"7. if u = a ∗ x + b ∗ y and v = c ∗ x + d ∗ y , where a, b, c, and d are nonzero constants, then cov(u,v) = a ∗ c x2 + b ∗ d y2 + (a ∗ d + b ∗ c)cov(x,y). this result allows us to find the covariance of two arbitrary linear combinations. the proof follows exactly in the same way as above."
3031,1,"['random variables', 'variables', 'covariance', 'random']", Covariance as Expected Value,seg_581,"n =1 cov(xj,yk) if the x’s and y’s are independent. this allows us to find the covariance of sums of random variables."
3032,1,['independent'], Covariance as Expected Value,seg_581,"theorem 8.15 if xi and yi are pair-wise independent, and u = c1 ∗ x1 + c2 ∗ x2 + · · · + cn ∗ xn and v = d1 ∗ y1 + d2 ∗ y2 + · · · + dn ∗ yn, then cov(u,v) ="
3033,0,['n'], Covariance as Expected Value,seg_581,n =1 cidjxiyj. separate the indexvar into two groups as i = j and i ≠ j and write this as ∑k
3034,1,['covariance'], Covariance as Expected Value,seg_581,"n ≠i=1 cidjxiyj. take covariance of both sides to get cov(u,v) = cov[∑k"
3035,1,"['independent', 'covariance', 'summation']", Covariance as Expected Value,seg_581,"n ≠i=1 cidjxiyj]. as xi and yi are pair-wise independent, the second sum is zero (by using the theorem 8.8). taking covariance inside the summation in the first term gives the result."
3036,1,"['binomial theorem', 'moment', 'arithmetic mean', 'expected value', 'variable', 'random variable', 'moments', 'moment ', 'mean', 'binomial', 'random']", Moments as Expected Values,seg_583,"the arithmetic mean of a random variable is the first raw or uncentered moment, which is denoted as   = e(x). we call e(xk) as the kth raw moment and denote it as mk; and e((x −  )k) as the kth central moment  k. here,   is called the pivot. theoretically, the pivot can be any nonzero constant, so long as the expected value exists. by expanding (x −  )k using binomial theorem, it is possible to express the central moments in terms of raw moments as follows:"
3037,1,"['transformation', 'change of origin and scale', 'change of origin']", Moments as Expected Values,seg_583,lemma 2 the change of origin and scale transformation yields  r(c ∗ x + b) = cr ∗  r(x).
3038,0,[], Moments as Expected Values,seg_583,where r is a positive integer.
3039,1,['moments'], Moments as Expected Values,seg_583,"here,  r ′ denotes the raw moments given by e[cx + b]r. as r is a positive integer,"
3040,0,[], Moments as Expected Values,seg_583,we could expand this as a power series to get e[∑k
3041,1,['expectation'], Moments as Expected Values,seg_583,constant outside and operate the expectation on each term to get the result.
3042,1,"['conditional density', 'dependent', 'conditional', 'random', 'function', 'random variable', 'functions', 'conditional expectation', 'density functions', 'expected value', 'expectation', 'random variables', 'variables', 'dependent variables', 'variable']", CONDITIONAL EXPECTATIONS,seg_585,conditional expectation is a useful concept that defines the expected value of a random variable or function thereof by conditioning one or more dependent variables. conditional expectation can also be defined in terms of conditional density functions. the conditional expectation considers a non-null subset of random variables by fixing
3043,1,"['random variables', 'variables', 'random']", CONDITIONAL EXPECTATIONS,seg_585,some other random variables as constant.
3044,1,['discrete'], CONDITIONAL EXPECTATIONS,seg_585,⎧ ∞ ⎪ ∑ yfy(y|x = x) if y is discrete; ⎪⎨x=−∞ e[y|x = x] = ∞
3045,1,['continuous'], CONDITIONAL EXPECTATIONS,seg_585,⎪⎪ yfy(y|x = x)dy if y is continuous. ⎩∫y=−∞
3046,1,"['conditional', 'distribution', 'expected value', 'mean', 'function', 'conditional distribution']", CONDITIONAL EXPECTATIONS,seg_585,"thus, the conditional expected value of y for a given value of x = x is the mean of y computed relative to the conditional distribution, which is a function of x."
3047,1,['independent'], CONDITIONAL EXPECTATIONS,seg_585,theorem 8.16 show that e(y) = e[e(y|x)] if x and y are independent.
3048,1,['continuous'], CONDITIONAL EXPECTATIONS,seg_585,proof: for simplicity assume that x and y are continuous. consider the rhs. e[e(y|x)] = ∫x
3049,1,"['expectation operator', 'expectation']", CONDITIONAL EXPECTATIONS,seg_585,"∞ =−∞ e[y|x = x] fx(x)dx. here we have expanded the outer expectation operator. next, expand the inner expectation operator to get"
3050,0,[], CONDITIONAL EXPECTATIONS,seg_585,"as fx(x) inside the inner integral is a constant while integrating with respect to y, this cancels out from the numerator and denominator to get"
3051,1,"['density function', 'independent', 'range', 'function']", CONDITIONAL EXPECTATIONS,seg_585,"as x and y are independent, the density function fx,y (x, y) factorizes into fx(x) ∗ fy (y). integrate out f (x) over its entire range to unity, and the remaining expression becomes ∫y"
3052,1,['continuous'], CONDITIONAL EXPECTATIONS,seg_585,"proof: as before, assume that x and y are continuous. consider the rhs."
3053,0,[], CONDITIONAL EXPECTATIONS,seg_585,"as x is a constant inside the integral over y, this becomes"
3054,1,['expectation'], CONDITIONAL EXPECTATIONS,seg_585,this integral was shown above as e[y|x = x]. expand the outer expectation operator on the rhs as e[e(xy|x)] = ∫x
3055,0,[], CONDITIONAL EXPECTATIONS,seg_585,"as fx(x) inside the inner integral is a constant while integrating with respect to y, this cancels out from the numerator and denominator to get"
3056,1,"['continuous', 'case']", CONDITIONAL EXPECTATIONS,seg_585,"proof: we will prove the result for the continuous case. by the above definition, e(y + z|x) = ∫y"
3057,1,['condition'], CONDITIONAL EXPECTATIONS,seg_585,example 8.28 expected number of devices in working condition
3058,1,"['probability', 'condition']", CONDITIONAL EXPECTATIONS,seg_585,"the lifetime of a device in years is distributed as exp( ), where   = 1∕8. if n such devices are put together in a satellite, find the following: (i) probability that half or more of the devices are in good working condition after 5 years. (ii) expected number of devices in working condition after 8 years."
3059,1,['probability'], CONDITIONAL EXPECTATIONS,seg_585,"solution 8.28 put t = 5 to get the probability that any device is working after 5 years as   exp(−5 ∗  ) = 0.0669. probability that it is not working is 1 −   exp(−5 ) = 0.9331. as there are n such devices, probability that at least half of"
3060,0,[], CONDITIONAL EXPECTATIONS,seg_585,the devices are working is∑k
3061,1,"['condition', 'case', 'expected value', 'binomial', 'probability']", CONDITIONAL EXPECTATIONS,seg_585,"ship between binomial sf and ibf, this can be written as ic(n∕2, n∕2 + 1), where c = 0.0669. for case (ii), we need to find the expected value after 8 years. the probability of good working condition is   exp(−8 ). the number of devices in working condition is a binomial variate, so that the expected value is np = n  ∗ exp(−8 ) = 0.04598493 ∗ n."
3062,1,"['noncentral beta', 'distribution', 'mean', 'noncentral', 'beta distribution']", CONDITIONAL EXPECTATIONS,seg_585,example 8.29 mean of noncentral beta distribution
3063,1,"['conditional expectation', 'noncentral beta', 'conditional', 'distribution', 'expectation', 'mean', 'noncentral', 'beta distribution']", CONDITIONAL EXPECTATIONS,seg_585,find the mean of noncentral beta distribution using conditional expectation.
3064,1,"['poisson', 'noncentral beta', 'beta distributions', 'conditional', 'distribution', 'random variable', 'variable', 'beta distribution', 'mean', 'random', 'poisson distribution', 'noncentral', 'distributions']", CONDITIONAL EXPECTATIONS,seg_585,"solution 8.29 the noncentral beta distribution is an infinite sum of poisson weighted central beta distributions [7]. depending on whether the central beta distribution is of first or second kind, there exist noncentral beta distribution (ncb) of two kinds [4]. symbolically, ncb of first kind has cdf ix(a, b;  ) ≡ ix(a + n, b), where n ∼ p(   2 ) has a poisson distribution. hence conditional on n, the random variable x has a central beta distribution of first kind. from this, an expression for the mean is easily obtained as follows:"
3065,1,"['distribution', 'beta distribution', 'mean']", CONDITIONAL EXPECTATIONS,seg_585,where we have used the fact that the mean of a beta distribution of first kind is a∕(a + b). write the numerator (a + n) as (a + b + n) − b and simplify to get the
3066,1,"['poisson', 'displaced poisson distribution', 'moment', 'distribution', 'poisson distributed', 'poisson distribution']", CONDITIONAL EXPECTATIONS,seg_585,"rhs as 1 − b ∗ e[1∕(a + b + n)]. as n is poisson distributed, the expression in the bracket is the first inverse moment of displaced poisson distribution, which −  1 e is given as e (a+n) = a 1f1[a,a + 1;  ], where a = a + b, and 1f1[a,a +"
3067,1,"['function', 'hypergeometric function', 'hypergeometric']", CONDITIONAL EXPECTATIONS,seg_585,1;  ] is the confluent hypergeometric function.
3068,1,['dependent'], CONDITIONAL EXPECTATIONS,seg_585,"as the numerator and the denominator are dependent, we use the following formula:"
3069,1,['mean'], CONDITIONAL EXPECTATIONS,seg_585,"where  x and  y denote the mean of x and y, respectively. here,  x = e[b] = b,  y = e(a + b + n) = (a + b +  ∕2),cov(x,y) = 0,var(x) = 0,var(y) = var(a + b + n) = var(n) (using section 5, p. 354) =  ∕2. hence equation (8.21) becomes"
3070,1,"['mean', 'table']", CONDITIONAL EXPECTATIONS,seg_585,"2 . some approximations are given in table 8.3, where the actual mean is an infinite sum as"
3071,1,"['results', 'noncentrality', 'moments', 'representations', 'parameter', 'noncentrality parameter']", CONDITIONAL EXPECTATIONS,seg_585,"the difference between actual and approximate values are given in the last column. the results are quite good for increasing noncentrality parameter ( ) values. the biggest advantage of equation (8.22) is that it takes only 10 arithmetic operations (including the computation of c once), whereas equation (8.23) takes a large number of operations when is large. see reference 282 for integral representations of moments."
3072,1,"['distribution', 'noncentral', 'mean']", CONDITIONAL EXPECTATIONS,seg_585,example 8.30 mean of noncentral  2 distribution
3073,1,"['conditional expectation', 'conditional', 'distribution', 'expectation', 'mean', 'noncentral']", CONDITIONAL EXPECTATIONS,seg_585,find the mean of noncentral chi-square distribution using conditional expectation.
3074,1,"['poisson', 'distribution', 'parameter', 'poisson distribution', 'noncentral']", CONDITIONAL EXPECTATIONS,seg_585,"solution 8.30 let y be distributed as noncentral chi-square. as this is a poisson weighted central chi-square distribution we write y ∼ 2 n+2n , where n has poisson distribution with parameter ∕2. from this we get e(n) = ∕2."
3075,1,"['f distribution', 'distribution', 'mean', 'noncentral', 'noncentral f']", CONDITIONAL EXPECTATIONS,seg_585,example 8.31 mean of noncentral f distribution
3076,1,"['f distribution', 'distribution', 'mean', 'noncentral', 'noncentral f']", CONDITIONAL EXPECTATIONS,seg_585,find the mean of noncentral f distribution.
3077,1,"['f distribution', 'independent', 'distribution', 'noncentral', 'noncentral f']", CONDITIONAL EXPECTATIONS,seg_585,"solution 8.31 the noncentral f distribution is the distribution of the scaled ratio of a noncentral 2( ) over an independent central 2 distribution. symbolically f(p, q, ) = (q∕p) 2 p+2n∕ q2, where p and q are the dof of numerator and"
3078,1,"['parameter', 'noncentrality', 'noncentrality parameter']", CONDITIONAL EXPECTATIONS,seg_585,"denominator  2, and   is the noncentrality parameter."
3079,1,"['f distribution', 'conditional', 'distribution', 'moments', 'noncentral', 'noncentral f']", CONDITIONAL EXPECTATIONS,seg_585,"(p + 2n) 2p + 2n∕(p+2n) this may be written as z f(p, q,  ) ∼ . conditional on n, the p 2q∕q noncentral f distribution is a multiple of central f distribution [129, 283]. we write this as f(p, q,  ) ∼ p + 2n fp+2n,q. the moments follow by the same argument as"
3080,1,['mean'], CONDITIONAL EXPECTATIONS,seg_585,"where we have used the fact that the mean of central f(p, q) is q∕(q − 2)."
3081,1,"['random variables', 'independent', 'table', 'variables', 'results', 'conditional', 'random variable', 'variable', 'expectation', 'conditional variance', 'random', 'variance']", Conditional Variances,seg_587,"the variance of a random variable, conditionally on another variable or on the count of iid (independent identically distributed) random variables occur in several applications. the conditional variance of y for a given x is var(y|x) = e{[y − e(y|x)]2|x}. expanding the quadratic and taking term by term expectation results in var(y|x) = e(y2|x) − [e(y|x)]2 (see table 8.4)."
3082,1,"['random variables', 'variables', 'random variable', 'variable', 'random', 'expectations']", Conditional Variances,seg_587,"theorem 8.18 let n be an integer valued random variable that takes values ≥ 1. let x1,x2, … xn be n iid random variables. define sn = x1 + x2 + · · · + xn . then (i) e(s) = e(x)e(n), provided the expectations exist, (ii) psn (t) = pn(px(t)), and"
3083,1,['independence'], Conditional Variances,seg_587,msn (t) = mn(kx(t)) = (kx(t))n due to independence.
3084,1,"['range', 'expectation', 'vary']", Conditional Variances,seg_587,"proof: assume that n is fixed. then e(s) = e(x1 + x2 + · · · + xn) = e(x1) + e(x2) + · · · + e(xn). as each of the xi’s are iid, the above becomes e(s) = ne(x). now allow n to vary in its range and take the expectation of both sides to get e(s) = e(n)e(x) (because e(e(s)) = e(s) and e(e(x)) = e(x))."
3085,1,['expectation'], Conditional Variances,seg_587,"e(tx)n = [px(t)]n . taking expectation of both sides, we get the desired result. next consider msn (t) = e(etsn ) = e(et[x1+x2+···+xn ]) = mx1 (t) ∗ mx2 (t) ∗ · · · ∗"
3086,1,['vary'], Conditional Variances,seg_587,"mxn (t) = [mx(t)]n . taking log of both sides, we get ksn (t) = log (msn (t)) = n ∗ log (mx(t)) = nkx(t). now allow n to vary to get the result."
3087,1,"['conditional', 'conditional variances', 'variance', 'variances']", Law of Conditional Variances,seg_589,"theorem 8.19 the unconditional variance can be expressed in terms of conditional variances as v(x) = e[v(x|y)] + v[e[x|y]] where v(x) = variance(x), assuming that the variances exist."
3088,1,['expectation'], Law of Conditional Variances,seg_589,"proof: subtract and add e[x|y], and write x − e[x] = (x − e[x|y]) + (e[x|y] − e[x]). square both sides and take expectation of each term to get"
3089,1,"['deviation', 'expectation', 'mean']", Law of Conditional Variances,seg_589,"as e(e[x|y]) = e(x), the last term (3) is zero. substitute e(x) = e(e[x|y]) in the second term e(e[x ∣ y] − e[x])2 to get (2) = e(e[x ∣ y] − e(e[x|y]))2. as this is the expectation of the squared deviation of e[x ∣ y] from its mean, it is var(e[x|y]). symbolically, (2) = var(e[x|y])."
3090,1,"['expected value', 'expectation']", Law of Conditional Variances,seg_589,"using the law of total expectation we have v(x|y) = e[x2|y] − e[x|y]2. take expectation of both sides to get e[v(x|y)] = e{e[x2|y]} − e{e[x|y]2}. write the first term e(x − e[x ∣ y])2 in equation (8.25) as e{e(x − e[x ∣ y])2|y}. expand the quadratic and take term by term expectation to get e{e[x2|y]} − 2e{e(x)e[x|y]} + e{e[x|y]2}. substitute e(x) = e(e[x|y]) in the second term, and cancel out the third term. this reduces to e{e[x2|y]} − e{e[x|y]2} showing that it is the expected value of v[x|y]. symbolically, (1) = e[v(x|y)]. substitute for (1) and (2) in equation (8.25) to get the result."
3091,1,"['noncentral', 'variance']", Law of Conditional Variances,seg_589,example 8.32 variance of noncentral chi-square
3092,1,"['conditional', 'distribution', 'noncentral', 'variance']", Law of Conditional Variances,seg_589,find the variance of noncentral chi-square distribution using conditional expectation.
3093,1,"['poisson', 'linear', 'independent', 'linear combination', 'combination', 'conditional', 'distribution', 'noncentral', 'distributions']", Law of Conditional Variances,seg_589,"solution 8.32 we know that the noncentral chi-square distribution is a poisson weighted linear combination of independent central chi-square distributions. this allows us to write it as y ∼ x + 2n, where conditional on n, x is a central chi-square distribution. for convenience let the dof of central chi-square be denoted by p and n has pois( /2). then"
3094,1,"['poisson', 'distribution', 'table', 'poisson distribution']", Law of Conditional Variances,seg_589,"where we have used the facts that v(c + b ∗ x) = b2v(x) and e(x) = v(x) =   for a poisson distribution [7, 283] (table 8.4)."
3095,1,"['moments', 'case']", INVERSE MOMENTS,seg_591,the definition of ordinary moments can be extended to the case where the order is a negative integer as follows:
3096,1,"['continuous', 'discrete']", INVERSE MOMENTS,seg_591,∞ ⎧⎪ ∑ xj −kpj if x is discrete; ⎪⎨j=−∞ e(1∕xk) = ∞ ⎪ x−kf (x)dx if x is continuous. ⎪⎩∫−∞
3097,1,"['poisson', 'condition', 'moment', 'gamma', 'distribution', 'moments', 'poisson distribution', 'weibull', 'distributions']", INVERSE MOMENTS,seg_591,"a necessary condition for the existence of the first inverse moment is that f (0) = 0. for instance, the poisson distribution has p(x = 0) = e−  0∕0! = e− , which is nonzero ∀ . hence the first inverse moment does not exist. however, there are a large number of distributions that satisfy the necessary condition. examples are chi-square (and gamma), snedecor’s f, beta, and weibull distributions. the exponent k is an integer in most of the applications of inverse moments. however, inverse moments could also be defined for fractional k (called fractional inverse moments)."
3098,1,"['distribution', 'moment']", INVERSE MOMENTS,seg_591,example 8.33 inverse moment of central  2 distribution
3099,1,"['distribution', 'moment']", INVERSE MOMENTS,seg_591,find first inverse moment of central  2 distribution.
3100,1,"['moment', 'range', 'random variable', 'variable', 'moments', 'summation', 'random', 'tail']", INCOMPLETE MOMENTS,seg_593,ordinary and central moments discussed above are defined for the entire range of the random variable x. there are several applications when the summation or integration is carried out partially over the range of x. the omitted range can either be in the left tail or in the right tail. we define the first incomplete moment as ei(x) = ∑x
3101,1,"['random variables', 'variables', 'random', 'expected values']", DISTANCES AS EXPECTED VALUES,seg_595,"statistical distances can be expressed as expected values. consider two real-valued random variables x and y. the k-norm distance between them is dk(x,y) = ‖x − y||k = [e(|y − x|k)]1∕k. this is also called the k-metric. it satisfies the following properties:"
3102,1,['inequality'], DISTANCES AS EXPECTED VALUES,seg_595,"(i) dk(x,y) ≥ 0, (ii) dk(x,y) = 0 iff x = y , (iii) dk(x,y) + dk(y ,z) ≥ dk(x,z) (triangle inequality)."
3103,1,"['sample', 'dissimilarity metrics', 'random variables', 'variables', 'results', 'random', 'expectations']", DISTANCES AS EXPECTED VALUES,seg_595,"particular values of k give various distances such as euclidean metric, manhattan metric, and so on [22]. the sample analogs of these distances are used in cluster analysis as dissimilarity metrics. the above definition can be extended from scalar random variables to vectors and matrices. for instance, if x is an m × n matrix of real-valued random variables, where xij denotes the (i, j)th entry, we define e(x) as that matrix whose (i, j) entry is e[xij], provided the individual expectations exist. using matrix commutativity, associativity, and so on with respect to addition, we could obtain the following results:"
3104,1,"['independent', 'table']", DISTANCES AS EXPECTED VALUES,seg_595,"(i) e(x + y) = e(x) + e(y) if x and y are compatible matrices, (ii) e(ax) = ae(x) if a is a scalar m × n matrix and x has as many rows as columns of a matrix (i.e., a is n × p), and (iii) if x and y are independent, then e(xy) = e(x)e(y) (table 8.5)."
3105,1,"['expected value', 'random variable', 'variable', 'random', 'function', 'tail']", Chebychev Inequality,seg_597,this is a useful result connecting the expected value of a function of a random variable and the tail area of it. let x be a random variable and g(x) be a nonnegative function of it. then the right tail area of g(x) is related to its expected value as p[g(x) ≥ c] ≤ e[g(x)]∕c.
3106,1,"['mathematical expectation', 'conditional expectation', 'table', 'conditional', 'expectation', 'random', 'continuous', 'average']", SUMMARY,seg_599,"this chapter introduced the basic ideas and rules of both the mathematical expectation and conditional expectation, see table 8.5. mathematical expectation plays an important role in digital signal processing, actuarial sciences, astronomy, and many other fields. for example, the average energy (t) of a periodic or random signal in the time domain is represented for continuous signals as (t) = ∫−"
3107,1,['average'], SUMMARY,seg_599,"∞ ∞ f (t)dt, from which the average power of the signal over a time period t1 to t2 is given by"
3108,1,"['frequency', 'fourier transform', 'frequency transforms', 'function', 'transform']", SUMMARY,seg_599,"if f1(t) = f2(t), where f (t) represents the signal value as a time-varying function. as the spectra of periodic signals are more revealing in the frequency domain, most dsp applications use one of the frequency transforms such as fourier transform, cosine transform, wavelet transform, and so on under the assumption that ∫−"
3109,1,"['expected value', 'random variable', 'variable', 'frequency', 'random', 'average']", SUMMARY,seg_599,∞ ∞ |xt (t)|dt   ∞ where |xt (t)| emphasizes that it is a random variable in the time-domain. the average power in the frequency domain can then be represented by expected value
3110,1,"['stationary', 'processes']", SUMMARY,seg_599,"∞ ∞ |xt(f )|2df . as t → ∞,e[|xt (f )|] will stabilize for stationary processes and signals, resulting in power spectral density of the signal. see references 285, 286 for further examples."
3111,1,"['expected value', 'random variable', 'variable', 'random']", SUMMARY,seg_599,a) expected value of a random variable always exist
3112,1,"['expected value', 'transformation', 'change of scale']", SUMMARY,seg_599,b) expected value is unchanged by a change of scale transformation
3113,1,"['chebychev inequality', 'expected values', 'inequality']", SUMMARY,seg_599,c) chebychev inequality can provide an upper bound on expected values
3114,1,"['distribution', 'variance']", SUMMARY,seg_599,"d) variance of a distribution defined in [0, 1] can be   1"
3115,1,"['random variables', 'variables', 'random']", SUMMARY,seg_599,e) e(1∕x) ≥ 1∕e(x) for all random variables x.
3116,1,"['random variables', 'variables', 'random']", SUMMARY,seg_599,f) |e[x]| ≤ √e[x2] for all random variables x
3117,1,"['linear', 'expectation']", SUMMARY,seg_599,h) expectation “e” is a linear and monotone operator.
3118,1,"['sample', 'sample space']", SUMMARY,seg_599,8.2 a——assigns a value to each element of the sample space: (a) generating func-
3119,1,"['expected value', 'random variable', 'variable', 'cumulant', 'random']", SUMMARY,seg_599,tion (b) random variable (c) cumulant (d) expected value.
3120,1,['uniformly distributed'], SUMMARY,seg_599,"8.3 if x1,x2, … ,xn are iid, and 8.11 if x is uniformly distributed in"
3121,1,['expected value'], SUMMARY,seg_599,"y = ∑ixi prove that my (t) = [a, b] find the expected value of"
3122,1,['discrete'], SUMMARY,seg_599,"8.4 if p(x= 0) = q2, p(x= 1) = 2pq, p 8.12 if x is a nonnegative discrete ran-"
3123,1,['variable'], SUMMARY,seg_599,"(x= 2) = p2, find the cdf and dom variable, prove that e(x2) ="
3124,1,['expectation'], SUMMARY,seg_599,"bell-shaped curves?. 1∕e[x], if each expectation exists."
3125,1,"['expected value', 'moments']", SUMMARY,seg_599,"tor fluctuates according to arcsin that c = a∕ba. find the cdf and law. find the expected value of the power = r ∗ i2, where r is the mgf, and obtain the first two moments where x   0. resistance (given)."
3126,1,"['discrete', 'random']", SUMMARY,seg_599,8.16 consider a discrete random vari8.7 prove that c = e(x) minimizes the
3127,0,[], SUMMARY,seg_599,"expression e(x − c)2 able p(x = k) = 4∕( 2k2) for k = 1, 2, … . verify whether the 8.8 prove that c = median(x) miniexpected value exists."
3128,1,['cumulants'], SUMMARY,seg_599,"mizes the expression e|x − c|. 8.17 if x ∼ beta(a, b), where a   b, 8.9 show that all cumulants except the find e[[x(1 − x)]k]"
3129,1,"['symmetric', 'distribution', 'random variable', 'variable', 'random']", SUMMARY,seg_599,"first one vanish for a symmetric 8.18 prove that cov(x,y − z) = distribution. cov(x,y) − cov(x,z). 8.10 if x is a negative random variable"
3130,1,"['distribution', 'geometric distribution', 'geometric']", SUMMARY,seg_599,"(values of x are always  0), prove 8.19 for the geometric distribution that e(x) = ∫0"
3131,0,[], SUMMARY,seg_599,die (marked 1–6) is thrown and the max+b(t) = ebtmx(at). deduce that player losses k dollars if the top m(x− )∕ (t) = e− t∕ mx(t∕ ).
3132,1,"['distribution', 'continuous distribution', 'continuous', 'expected value']", SUMMARY,seg_599,"point k is odd, and gains 2k dollars 8.28 prove that the expected value of if it is even. find e(x),v(x). 1 md is e(md) =  √ 1 − , where 8.21 for any continuous distribution, n"
3133,1,"['sample size', 'sample', 'median', 'population']", SUMMARY,seg_599,"n is the sample size and   is the prove that e|x − c| is minimum population md. when c is the median. 8.29 if f (x) = kx exp(−x) for x ≥ 0, 8.22 if f (x, y) = x + y for 0   x   find k,e(x), and the mgf."
3134,1,"['random variable', 'variable', 'random']", SUMMARY,seg_599,"e(x|y). 8.30 if f (x) = k exp(−|x|) for −∞   x   ∞, find k, e(x), and the mgf. 8.23 let x be a random variable that"
3135,1,['expected value'], SUMMARY,seg_599,"thrown. define the pdf of x and 8.32 if x ∼ exp ( ), find e[√ x] find its expected value e(x). does"
3136,0,[], SUMMARY,seg_599,"1, … , find the mgf and derive the 8.24 find the normalizing constant k"
3137,1,"['function', 'moments']", SUMMARY,seg_599,"in f (x) = k∕(x + c)n+1, where n is first two moments. what is the suran integer and c is a real constant. vival function?"
3138,1,['moments'], SUMMARY,seg_599,prove that all ordinary moments of 8.34 prove that  2 = e(x(x − 1)) + order up to n − 1 are non-existent e(x) − [e(x)]2. apply it to find
3139,1,"['distribution', 'variance', 'geometric']", SUMMARY,seg_599,for this distribution. the variance of geometric and
3140,1,"['poisson', 'poisson distributions', 'distributions']", SUMMARY,seg_599,"8.25 when is cov(x,y) = − x y? poisson distributions."
3141,1,"['poisson', 'distribution', 'poisson distribution']", SUMMARY,seg_599,"8.35 for the poisson distribution (pois( )), prove that"
3142,1,['moment'], SUMMARY,seg_599,| | use this result to derive the first incomplete moment ∑x=0 xe−  x∕x!.
3143,1,['binomial'], SUMMARY,seg_599,x )n) for the binomial
3144,1,"['successes', 'distribution', 'trials']", SUMMARY,seg_599,"[e(x) − k]2 distribution with n trials, where x is the number of successes. what 8.37 find e( 1"
3145,1,"['expectation', 'limit']", SUMMARY,seg_599,2 ) for the is the limit of this expectation as
3146,1,['variable'], SUMMARY,seg_599,random variable in q 8.22. n → ∞?
3147,1,"['discrete', 'random']", SUMMARY,seg_599,8.39 find the mgf of logarith8.51 the pdf of a discrete random
3148,1,"['distribution', 'variable', 'mean', 'variance']", SUMMARY,seg_599,"mic series distribution f (x) = variable is given by f (x) = cx2 for x = {1, 2, 3}. find the mean and c x∕x, x = 1, 2, …where c = −1∕ variance. log (1 −  ), and 0       1. prove"
3149,1,"['moment', 'factorial', 'moment ', 'factorial moment']", SUMMARY,seg_599,that rth factorial moment  (r) = 8.52 verify if f (x) = 1 e−|x|∕( ∕√ 2) is c(r − 1)![ (1 −  )]r. √
3150,1,['expected value'], SUMMARY,seg_599,"8.40 find the expected value for a bino8.53 if f (x) = kx(x + 1) for x = 1, 2,"
3151,1,['distribution'], SUMMARY,seg_599,"mial distribution bino(n, p). 3, 4; find the e[x] and p[x ≥ 2]."
3152,1,['probability'], SUMMARY,seg_599,"e(x) = a∕(a + b). find k, e[x] and the probability"
3153,1,"['variance', 'mean']", SUMMARY,seg_599,"γ(b) mx( t)= 1f1(a, b; x) = γ(a)γ(b−a) 8.55 find the mean and variance for"
3154,1,"['distribution', 'hypergeometric']", SUMMARY,seg_599,∫0 1 eytta−1(1 − t)b−a−1dt is the the distribution f (x) = (1 − n )n confluent hypergeometric func( n x) n−
3155,1,['independent'], SUMMARY,seg_599,"functions a pdf. then find e(x) e(x2 + y2)? and e(x2) (a) f (x) = kx2(1 − x) for 0   x   1, (b) k∕xa+1, a   8.57 if x and y are independent ran-"
3156,1,['variables'], SUMMARY,seg_599,"1, x   0, (c) f (x) = k(x2 + 1) for dom variables with e(x) = −3 and e(y) = 5, find e(2x − 3)(y + x ∈ { − 2,−1, 0, 1, 2}. 5)."
3157,1,['monotonic'], SUMMARY,seg_599,"8.44 if f (x) = 1∕  for 0   x    , show 8.58 if  (x) is a real-valued, monotonic"
3158,1,"['function', 'random']", SUMMARY,seg_599,that e[sin(x)] = 2∕ . function of a positive random
3159,1,['variable'], SUMMARY,seg_599,"8.45 if x   0, find constants a,b,c such variable x, prove that e[ (x)] ="
3160,0,[], SUMMARY,seg_599,1)p[x   k] 8.59 what are the conditions for
3161,1,"['moment', 'expected value', 'function']", SUMMARY,seg_599,a function to be a moment 8.47 what is the expected value of an generating function? are
3162,1,"['variable', 'functions']", SUMMARY,seg_599,indicator variable? the following functions true
3163,0,[], SUMMARY,seg_599,"8.48 if x ∼ bino(n, p) find e( x"
3164,1,"['discrete', 'random']", SUMMARY,seg_599,8.50 the pdf of a discrete random 8.60 suppose an urn contains m red
3165,1,"['with replacement', 'replacement']", SUMMARY,seg_599,"variable is given by f (x) = k(|x| + balls and n blue balls. if r balls 1) for x = −3,−2,−1, 0, 1, 2, 3. are drawn with replacement, what find k and the cdf. evaluate f(2) is expected number of blue balls and p[x ≥ 0]. drawn?"
3166,1,['expected values'], SUMMARY,seg_599,8.61 suppose an urn contains n coins find the expected values of a pois-
3167,1,"['without replacement', 'replacement', 'random variable', 'variable', 'random']", SUMMARY,seg_599,"numbered 1 – n. if r coins are son random variable. what is the drawn without replacement, what expression to find e[x4]?"
3168,1,"['variance', 'mean']", SUMMARY,seg_599,is the expected sum of the num8.67 find the mean and variance of the
3169,1,['distributions'], SUMMARY,seg_599,"bers? distributions: (i) f (x, n) = (n∕2) sin 8.62 if x and y are iid distributed as (nx), 0 ≤ x ≤  ∕n, n   0 is real;"
3170,1,"['symmetric', 'discrete']", SUMMARY,seg_599,k f (y|x) − 8.63 if x is a discrete symmetric ran-
3171,1,['variable'], SUMMARY,seg_599,dom variable (p[x = k] = p[x =  y
3172,1,['expected value'], SUMMARY,seg_599,"−k]), find expected value of 8.69 if the second derivative of h(x) is"
3173,1,['continuous'], SUMMARY,seg_599,"8.64 prove that cov(x,xi − x) = 0 8.70 if x is continuous, prove"
3174,1,"['sample', 'random sample', 'random']", SUMMARY,seg_599,for any random sample. that  r ′ = ∫0 ∞ rxr−1[1 − fx(x) +
3175,1,"['sample', 'random', 'random sample', 'expected values']", SUMMARY,seg_599,any random sample. 8.71 find the expected values of x2 and
3176,1,"['experiment', 'random']", SUMMARY,seg_599,8.66 use x2 = x(x − 1) + x and x3 = x3 in a random experiment of toss-
3177,1,"['factorial', 'moments', 'factorial moments', 'distributions']", SUMMARY,seg_599,8.72 prove that the factorial moments for the following distributions are as given:
3178,1,"['random variables', 'independent', 'variables', 'random']", SUMMARY,seg_599,"8.73 if x1,x2, … ,xn are independent identical random variables with the same"
3179,1,"['expected value', 'arithmetic mean', 'variance', 'mean']", SUMMARY,seg_599,"mean  , and same variance  2, find the expected value and variance of the arithmetic mean of xi"
3180,1,"['continuous random variable', 'random variable', 'variable', 'random', 'continuous']", SUMMARY,seg_599,"8.74 if x is a nonnegative continuous random variable, and y = exp (x2) find the"
3181,1,"['geometric distribution', 'distribution', 'moments', 'geometric']", SUMMARY,seg_599,8.75 find the mgf and first three moments of the geometric distribution f (x) =
3182,1,"['continuous random variable', 'random variable', 'variable', 'random', 'continuous']", SUMMARY,seg_599,"8.76 if x is a real-valued continuous random variable, prove that e[x2] ="
3183,0,[], SUMMARY,seg_599,8.77 the number of mms messages arriving in emily’s cell phone between 9 am
3184,1,"['poisson', 'poisson distributed', 'interval']", SUMMARY,seg_599,and 5 pm is poisson distributed with   = 1 for a 10-min interval. what is the expected number of mms messages received in 1 h? what is the total expected number of messages she will receive between 1 pm and 5 pm?
3185,1,['function'], SUMMARY,seg_599,8.78 the mean-excess function of a variate is defined as ∆[x] = (1∕s(x))∑∞
3186,1,['discrete'], SUMMARY,seg_599,"u (1 − f(x)) if x is discrete, and ∆[x] = (1∕s(x)) ∫u"
3187,1,"['poisson', 'variates', 'exponential', 'function']", SUMMARY,seg_599,"ous, where s(x) = 1 − f(x) is the survival function. find ∆[x] of poisson and exponential variates."
3188,1,"['random variables', 'variables', 'random']", SUMMARY,seg_599,"8.79 suppose x1,x2, … ,xn are iid random variables, with e(xi) =   and"
3189,1,"['independent', 'random variable', 'variable', 'random', 'variance']", SUMMARY,seg_599,"var(xi) =  2. define y = x1 + x2 + · · · + xn , where n is another random variable independent of x. prove that e(y|n) = n , and var(y|n) = n 2. use e(y) = e[e(y|n)] to show that e(y) =   , where   = e(n). show that the unconditional variance of y is   2 +  2 2 where  2 is variance of n."
3190,0,[], SUMMARY,seg_599,"8.80 when a cell phone is powered on, it is registered with a base station. each"
3191,1,"['poisson', 'percentage', 'distribution', 'poisson distribution']", SUMMARY,seg_599,"base station has a “cell” which is the coverage region (say a circular or square region) around it. when the caller moves from place to place, they may move out of one region and into an adjacent region. the phone company automatically detects it and “hands over” the phone identity to the new base station. a phone company has noticed that the majority of subscribers do not change their base station during their call, but the proportion of subscribers who change their base station is an upper truncated poisson distribution with = 0.04, and truncation point 4. find the expected percentage of subscribers who change their base station, and pr[x ≥ 2], where x denotes the number of hand overs."
3192,1,"['discrete random variable', 'discrete', 'random variable', 'variable', 'random']", SUMMARY,seg_599,8.81 if the cdf of a discrete random variable is
3193,1,['mean'], SUMMARY,seg_599,find the pdf and the mean.
3194,1,"['continuous random variable', 'random variable', 'variable', 'random', 'continuous']", SUMMARY,seg_599,8.82 if the cdf of a continuous random variable is
3195,1,['mean'], SUMMARY,seg_599,find the pdf and the mean.
3196,1,"['discrete random variable', 'discrete', 'random variable', 'variable', 'random']", SUMMARY,seg_599,8.83 if the cdf of a discrete random variable is
3197,1,['mean'], SUMMARY,seg_599,how is the mean related to p[x = 1].
3198,1,"['continuous random variable', 'random variable', 'variable', 'random', 'continuous']", SUMMARY,seg_599,8.84 if the cdf of a continuous random variable is
3199,1,['mean'], SUMMARY,seg_599,find the pdf and the mean.
3200,1,"['discrete random variable', 'discrete', 'random variable', 'variable', 'random']", SUMMARY,seg_599,8.85 if the pdf of a discrete random variable is
3201,1,['mean'], SUMMARY,seg_599,prove that the mean is  ∕q. find the pgf.
3202,1,"['continuous random variable', 'random variable', 'variable', 'random', 'continuous']", SUMMARY,seg_599,8.86 if the cdf of a continuous random variable is
3203,1,['mean'], SUMMARY,seg_599,prove that the mean is  ∕4. find the mgf.
3204,0,[], SUMMARY,seg_599,8.87 prove that the memory-less propn
3205,1,"['distribution', 'exponential', 'exponential distribution']", SUMMARY,seg_599,"8.89 if f (x; n,  ) = ( x) (1 −  ∕n)n x∕ erty of exponential distribution is (n −  )x where x = 0, 1, … , n"
3206,1,"['normal', 'independent']", SUMMARY,seg_599,"8.90 if x, y are independent normal 8.88 if both x and y are independent"
3207,1,"['gamma distributed', 'gamma', 'variables', 'variance']", SUMMARY,seg_599,"random variables with the same gamma distributed, prove that (i) variance, find e[(x + y)4|(x − e(y|x) = cx + b, (ii) var(y|x) ="
3208,1,"['distribution', 'exponential', 'exponential distribution']", SUMMARY,seg_599,8.91 suppose you toss a fair die once exponential distribution with
3209,1,['variance'], SUMMARY,seg_599,and note down the number n that = 1∕2400 s and truncation point shows up (1 ≤ n ≤ 6). you then 20 s. what is the expected percenttoss a fair coin n times. let x age of phone calls that take more denote the number of heads that than 5 min? what is the variance you get in n tosses of the coin. of duration of all phone calls?
3210,1,['independent'], SUMMARY,seg_599,8.92 if x and y are independent nections. the number of new cus-
3211,1,['variables'], SUMMARY,seg_599,"random variables, prove that tomers who connect to http p(y ≤ x) = ∫−"
3212,1,"['poisson', 'poisson distributed']", SUMMARY,seg_599,∞ ∞ fy (x)fx(x)dx = server is poisson distributed with 1 − ∫−
3213,1,"['poisson', 'population variance', 'interval', 'population', 'variance']", SUMMARY,seg_599,∞ ∞ fx(y)fy (y)dy. = 20 for a time interval of 1 min. on the ftp server is poisson dis8.93 prove that the population variance tributed with = 3 for same time
3214,1,['events'], SUMMARY,seg_599,"can be expressed as the values period. if both events are indepenof cdf (fx(x)) and its first two dent, what is the expected number derivatives evaluated at x = 1. of customers connecting to the site"
3215,1,"['population variance', 'variance', 'population']", SUMMARY,seg_599,8.94 prove that the population variance in 4 min?.
3216,0,[], SUMMARY,seg_599,can be expressed using the second derivative of kx(t) as kx
3217,1,"['poisson', 'poisson distributed']", SUMMARY,seg_599,′′(x) evalu8.98 the number of cars that arrive at a gas station between 7 am ated at x = 0. and 9 am is poisson distributed
3218,1,['mean'], SUMMARY,seg_599,8.95 an electronic circuit has n2 comwith mean 3 in 5 min. what is
3219,1,['expected value'], SUMMARY,seg_599,"ponents that look identical. a the expected number of minutes technician has time to inspect just a person has to wait if there are n of the components in any trip. no others in the queue? what is what is the expected number of the expected number of cars that trips needed to inspect every comarrive in 27 s? ponent if the components are cho8.99 if x is cuni(a, b) find the distrisen arbitrarily in each repair trip, bution and expected value of y = and inspected components are not (2x − (a + b))∕(b − a). marked."
3220,0,[], SUMMARY,seg_599,8.100 what is the expression to find 8.96 a telephone carrier notices
3221,1,['average'], SUMMARY,seg_599,that the average duration of e[x4] in terms of x(x − 1)(x − cell-phone calls among teenagers 2)(x − 3) and lower order prodis distributed as a left-truncated ucts?
3222,0,[], SUMMARY,seg_599,8.102 high-rise structures at earth-quake-prone areas are designed to withstand
3223,1,"['probability', 'data']", SUMMARY,seg_599,"powerful earthquakes. from past data, it is found that the probability of an earthquake in a year is 0.091, and the probability of a building collapse after"
3224,1,"['expected value', 'information']", SUMMARY,seg_599,"the earthquake is 0.004. the cost of constructing a high-rise building is c0 and the cost of repair after damage is cr. if a building portfolio comprises of n (  2) buildings in a city neighborhood, find the expected value of the cost incurred in 10 years: (i) if no information on the earthquakes are available and (ii) if it is assumed that at least two earthquakes are likely to occur."
3225,0,[], SUMMARY,seg_599,8.103 a pop (post-office protocol) based mail server sends each message and then
3226,1,"['successful', 'mean', 'exponentially distributed', 'exponentially']", SUMMARY,seg_599,"waits for an ack from the receiver. only after the receipt of the ack will the mail server send the next message in the queue. it is known that the delay in receipt of the ack is exponentially distributed with mean 1/2 s. if three messages, each of size 1 k are to be sent, what is the expected number of seconds elapsed for successful transmission if the sending of each message itself takes half-second?."
3227,0,[], SUMMARY,seg_599,8.104 an automated robot controlled inventory warehouse has racks of length 120 m
3228,0,[], SUMMARY,seg_599,on both sides of an alley. the robot is equally likely to break down anywhere on the stretch of 120 m. where should a spare robot be located so that it can immediately take over the task of the broken down robot in minimal waste of time?
3229,1,['error'], SUMMARY,seg_599,8.105 a computer virus can infect a laprun-time exception (kind of error)
3230,1,"['poisson', 'probabilities', 'probability']", SUMMARY,seg_599,"top independently through (i) an in 8 h of use is pois (0.03), email, (ii) an http, or (iii) a muland independently the last three timedia with respective poisson subsystems is pois (0.05). if the probabilities 2%, 0.09%, and 5% software is used for 80 h, (i) in 1 day. if you use both email and what is the expected number of multimedia connection for 30 h, exceptions? (ii) probability that no what is the expected number of exceptions occurred."
3231,1,"['test', 'average']", SUMMARY,seg_599,"computer virus infections? if you 8.108 an auto-emission test center has use both http and multimedia for found that on an average one in 20 h and email for 10 h, what is the eight automobiles fail in the emis-"
3232,1,['test'], SUMMARY,seg_599,"expected number of infections? sion test, and needs tune-up. the"
3233,1,['distribution'], SUMMARY,seg_599,"8.106 if xi – idd beta -2 (a, bi) find distribution of tune-up time in"
3234,1,"['harmonic', 'harmonic mean', 'mean']", SUMMARY,seg_599,expected value of harmonic mean. hours is exp(2.5). if 100 vehi-
3235,0,[], SUMMARY,seg_599,"cles are tested per month, find the 8.107 a software comprises of eight"
3236,1,['probability'], SUMMARY,seg_599,expected number of hours spent on subsystems. probability that the servicing of failed vehicles. first five subsystems will throw a
3237,0,['n'], SUMMARY,seg_599,8.109 suppose n letters are to be sent in n envelops. if the letters and envelops are
3238,0,[], SUMMARY,seg_599,"shuffled and each letter is randomly assigned to an envelope, find the expected number of matches (letters that get into correct envelops)."
3239,0,[], SUMMARY,seg_599,8.110 a cereals manufacturer offers a promotional coupon with a new brand of cereal
3240,1,['probability'], SUMMARY,seg_599,"pack. two types of coupons (that carry either 1 point or 2 points) are printed, and either of them is put in selected packs so that some packs do not contain a coupon. probability that a customer will find a 1-point coupon is p, and a 2"
3241,0,['n'], SUMMARY,seg_599,"points coupon is q. if a customer purchases n packs of the cereal, what is the expected number of points earned?"
3242,1,['distribution'], SUMMARY,seg_599,square distribution e(1∕x)=1∕(n − 2). with equality when relationship is
3243,1,['linear'], SUMMARY,seg_599,"perfectly linear. 8.112 if x ∼ exp ( ) find e[cex], where"
3244,1,"['distribution', 'geometric distribution', 'geometric']", SUMMARY,seg_599,"c is constant. 8.117 if x has geometric distribution,"
3245,0,[], SUMMARY,seg_599,"8.118 if x ∼ bino(n, p) prove that 8.114 prove kax+b(t) = kx(at) + bt. e(n − x)2 = nq[n − p(n − 1)]."
3246,1,"['function', 'convex function']", SUMMARY,seg_599,"8.115 if g(x) is a convex function, prove"
3247,0,['n'], SUMMARY,seg_599,8.119 a consignment of 2n missiles contain ( n
3248,1,['range'], SUMMARY,seg_599,k) that have a range of 100 + k2 miles.
3249,0,[], SUMMARY,seg_599,"if a group of m missiles are randomly picked up and fired, what is the expected miles covered from the firing point to hitting point?"
3250,1,"['variance', 'mean']", SUMMARY,seg_599,"8.121 if x1,x2, … ,xn are iid each with the same mean   and same variance  2, find"
3251,1,"['moment', 'random', 'variance', 'second moment']", SUMMARY,seg_599,the second moment and variance of a random sum sn = x1 + x2 + · · · + xn .
3252,0,[], SUMMARY,seg_599,8.122 what is the limiting value of limh ↓ 0(e[ehx − 1)]/h.
3253,0,[], SUMMARY,seg_599,8.123 an audio signal s is corrupted 8.124 suppose two fair dice are tossed.
3254,1,"['density function', 'random variables', 'variables', 'range', 'independent', 'expected value', 'scores', 'random', 'function', 'uniformly distributed']", SUMMARY,seg_599,"with background noise b. if s find the density function of is uniformly distributed in the (x1,x2), where x1 and x2 range −c to +c, but the noise are the scores that show up. b is uniformly distributed in the three random variables u,v,y range 0 to 2d where d c, what are defined as follows: u = is the expected value of signal min{x1,x2}, the minimum score, plus noice? what is the covariv = max{x1,x2}, the maxiance cov(s,b) assuming that sigmum score, and y = x1 + x2, nal and noise are coming from the sum of the scores. find independent sources? e(u),e(v),e(y), and e(y|x1)."
3255,0,[], GENERATING FUNCTIONS,seg_601,"after finishing the chapter, students will be able to"
3256,1,['functions'], GENERATING FUNCTIONS,seg_601,◾ understand generating functions and their properties
3257,1,"['function', 'functions', 'characteristic function']", GENERATING FUNCTIONS,seg_601,◾ comprehend generating functions and characteristic function
3258,1,"['functions', 'cumulants', 'moments']", GENERATING FUNCTIONS,seg_601,◾ interpret moments and cumulants from generating functions
3259,1,"['functions', 'discrete']", GENERATING FUNCTIONS,seg_601,◾ explore new type of generating functions for discrete cdf
3260,0,[], GENERATING FUNCTIONS,seg_601,◾ apply the concepts to practical problems
3261,0,[],,seg_603,"after finishing the chapter, students will be able to"
3262,1,['functions'],,seg_603,◾ understand generating functions and their properties
3263,1,"['function', 'functions', 'characteristic function']",,seg_603,◾ comprehend generating functions and characteristic function
3264,1,"['functions', 'cumulants', 'moments']",,seg_603,◾ interpret moments and cumulants from generating functions
3265,1,"['functions', 'discrete']",,seg_603,◾ explore new type of generating functions for discrete cdf
3266,0,[],,seg_603,◾ apply the concepts to practical problems
3267,1,['functions'],,seg_603,9.1 types of generating functions
3268,1,['functions'],,seg_603,"generating functions find a variety of applications in engineering and applied sciences. as the name implies, generating functions are used to generate different quantities with minimal work."
3269,1,"['variables', 'function', 'dummy variables', 'coefficients']",,seg_603,"definition 9.1 a generating function is a simple and concise expression in one or more dummy variables that captures the coefficients of a finite or infinite power series expansion, and generates a quantity of interest using calculus or algebraic operations, or simple substitutions."
3270,1,"['functions', 'linear', 'probabilities', 'moment', 'moment generating function', 'distribution', 'random variable', 'variable', 'moments', 'population', 'probability', 'random', 'function']",,seg_603,"depending on what we wish to generate, there are different generating functions. for example, moment generating function (mgf) generates moments of a population and probability generating function generates corresponding probabilities. these are specific to each distribution. an advantage is that if the mgf of an arbitrary random variable x is known, we can mathematically derive the mgf of any linear combination of the form a ∗ x + b. this reasoning holds for other generating functions too."
3271,1,"['dummy variable', 'variable', 'function', 'coefficients']",,seg_603,"∞ =0 anxn is called the ordinary generating function (ogf) of {an}. here, x is a dummy variable, n is the indexvar, and a′ns are known constants. for different values of an, we get different ogfs. for example, if all an = 1, we get f (x) = (1 − x)−1, and if an = −1 for n odd and an = +1 for n even, we get (1 + x)−1. similarly, if even coefficients a2n = +1, and odd coefficients a2n+1 = 0, we get"
3272,1,['function'],,seg_603,(1 − x2)−1. the function g(x) = ∑n
3273,1,"['sample spaces', 'statistics', 'discrete', 'random', 'function', 'sample', 'exponential', 'functions', 'continuous random variables', 'continuous', 'random variables', 'variables']",,seg_603,"∞ =0 anxn∕n! is called the exponential generating function (egf), where the divisor of the nth term is n!. the generating functions used in statistics can be finite or infinite, because they are defined on (sample spaces of) random variables. the above is a discrete generating function as it is defined for a discrete sequence. they may also be defined on continuous random variables as shown below."
3274,0,[],,seg_603,example 9.1 kth derivative of egf
3275,1,"['process', 'summation', 'coefficients']",,seg_603,"solution 9.1 consider g (x) = a0 + a1x∕1! + a2x2∕2! + a3x3∕3! + · · · + akxk∕ k! + · · ·. take the derivative with respect to x of both sides. as a0 is a constant, its derivative is zero. use derivative of xn = n ∗ xn−1 for each term to get g′ (x) = a1 + a2x∕1! + a3x2∕2! + · · · + akxk−1∕ (k − 1)! + · · ·. differentiate again (this time a1 being a constant vanishes) to get g′′ (x) = a2 + a3x∕1! + a4x2∕2! + · · · + akxk−2∕ (k − 2)! + · · ·. repeat this process k times. all the terms whose coefficients are below ak will vanish. what remains is g(k) (x) = ak + ak+1x∕1! + ak+2x2∕2! + · · ·. this can be expressed using the summation notation introduced in chapter 1 as g(k) (x) = ∑n"
3276,0,['n'],,seg_603,∞ =k anxn−k∕ (n − k)!. using the change of indexvar introduced in section 1.5 this can be written as
3277,0,[],,seg_603,solution 9.2 by definition f (x) = a0 + a1x + a2x2 + · · · + anxn + · · ·. expand (1 − x)−1 as a power series 1 + x + x2 + .. and multiply with f(x) to get the rhs
3278,1,['summation'],,seg_603,change the order of summation to get
3279,0,[],,seg_603,this is the ogf of the given sequence.
3280,1,"['functions', 'factorial', 'moments', 'cumulant', 'even moments', 'function', 'factorial moments']", Generating Functions in Statistics,seg_605,"there are four popular generating functions used in statistics—namely (i) probability generating function (pgf), denoted by px (t), (ii) mgf, denoted by mx (t), (iii) cumulant generating function (cgf), denoted by kx (t), and (iv) characteristic function, denoted by x(t). in addition, there are still others to generate factorial moments (fmgf), inverse moments (imgf), inverse factorial moments (ifmgf), absolute moments, as well as for odd moments and even moments separately. these are called “canonical functions” in some fields."
3281,1,"['cumulants', 'characteristic function', 'probabilities', 'factorial', 'distribution', 'random variable', 'variable', 'random', 'function']", Generating Functions in Statistics,seg_605,"the pgf generates the probabilities of a random variable and is of type ogf. mgf (page 382) has further subdivisions as ordinary, and central mgf, factorial mgf, inverse mgf, inverse factorial mgf, cgf, and characteristic function (chf). all of these can also be defined for arbitrary origin. the cgf is defined in terms of the mgf as kx (t) = ln (mx (t)), which when expanded as a polynomial in t gives the cumulants. as every distribution does not possess an mgf, the concept is extended to the complex domain by defining the chf as  x (t) = e (eitx). note that the logarithm is to the base e (ln). if all of them exist for a distribution, then"
3282,1,['table'], Generating Functions in Statistics,seg_605,this can also be written in the alternate forms px (eit) = mx (it) = ekx(it)) =  x (−t) or as px (t) = mx (ln (t)) = ekx(ln(t))) =  x (i ln (t)) (see table 9.1).
3283,1,"['probabilities', 'random variable', 'variable', 'random']", PROBABILITY GENERATING FUNCTIONS PGF,seg_607,the pgf of a random variable is used to generate probabilities. it is defined as
3284,1,"['range', 'summation', 'distributions']", PROBABILITY GENERATING FUNCTIONS PGF,seg_607,"where the summation is over the range of x. this is a finite series for distributions with finite range. it may or may not possess a closed-form expression for other distributions. it converges for |t| 1, and appropriate derivatives exist. differentiating both sides of equation (9.4) k times with respect to t, we get ( k∕ tk)px (t) = k!p (k) +terms involving t. if we put t= 0, all higher-order terms that have “t” or higher powers vanish, giving k! p(k), from which p (k) is obtained as ( k∕ tk)px (0) ∕k!. if the px (t) involves powers or exponents, we take the log (with respect to e) of both sides and differentiate k times, and then use the following result on px (t = 1) to simplify the differentiation."
3285,1,"['distribution', 'discrete distribution', 'discrete']", PROBABILITY GENERATING FUNCTIONS PGF,seg_607,find px(t = 0) and px(t = 1) from the pgf of a discrete distribution.
3286,1,"['probabilities', 'probability']", PROBABILITY GENERATING FUNCTIONS PGF,seg_607,"solution 9.3 as ∑kp (k), being the sum of the probabilities, is one, it follows trivially by putting t = 1 in equation (9.4) that is px (t = 1) = 1. put t = 0 in equation (9.4) to get px (t = 0) = p(0), the first probability. similarly, put t = −1 to get the rhs as [p (0) + p (2) + · · · +] − [p (1) + p[3] + p[5] + · · · ]."
3287,1,"['poisson', 'distribution', 'poisson distribution']", PROBABILITY GENERATING FUNCTIONS PGF,seg_607,example 9.4 pgf of poisson distribution
3288,1,"['poisson', 'distribution', 'probabilities', 'poisson distribution']", PROBABILITY GENERATING FUNCTIONS PGF,seg_607,"find the pgf of a poisson distribution, and obtain the difference between the sum of even and odd probabilities."
3289,1,"['poisson', 'distribution', 'poisson distribution']", PROBABILITY GENERATING FUNCTIONS PGF,seg_607,solution 9.4 the pgf of a poisson distribution is ∞ ∞ ( t)x px (t) = e (tx) = ∑ txe−  x∕x! = e− ∑ = e− et  = e− [1−t]. (9.5) x! x=0 x=0
3290,1,"['distribution', 'geometric distribution', 'geometric']", PROBABILITY GENERATING FUNCTIONS PGF,seg_607,example 9.5 pgf of geometric distribution
3291,1,"['distribution', 'geometric distribution', 'geometric', 'probabilities']", PROBABILITY GENERATING FUNCTIONS PGF,seg_607,"find the pgf of a geometric distribution, and obtain the difference between the sum of even and odd probabilities."
3292,1,"['distribution', 'geometric distribution', 'geometric']", PROBABILITY GENERATING FUNCTIONS PGF,seg_607,"solution 9.5 as the geometric distribution takes x = 0, 1, 2, … ∞ values, we get the pgf as"
3293,1,"['continuous', 'discrete', 'discrete distributions', 'continuous distributions', 'distributions']", PROBABILITY GENERATING FUNCTIONS PGF,seg_607,closed-form expressions for px (t) are available for most of the common discrete distributions. they are seldom used for continuous distributions because ∫ txf (x) dx may not be convergent.
3294,0,[], PROBABILITY GENERATING FUNCTIONS PGF,seg_607,"example 9.6 pgf of bino(n, p)"
3295,1,['mean'], PROBABILITY GENERATING FUNCTIONS PGF,seg_607,"find the pgf of bino(n, p) and obtain the mean."
3296,0,[], PROBABILITY GENERATING FUNCTIONS PGF,seg_607,solution 9.6 by definition
3297,1,"['random variable', 'variable', 'mean', 'probability', 'random', 'coefficient']", PROBABILITY GENERATING FUNCTIONS PGF,seg_607,"the coefficient of tx gives the probability that the random variable takes the value x. to find the mean, we take the log of both sides. then log(px (t)) = n*log(q + pt). differentiate both sides with respect to t to get p′x (t) ∕px (t) = n ∗ p∕ (q + pt). now put t = 1 and use px (t = 1) = 1 to get the rhs as n*p/ (q + p) = np as q + p = 1."
3298,1,"['table', 'factorial', 'moments', 'factorial moments']", PROBABILITY GENERATING FUNCTIONS PGF,seg_607,lemma 1 the pgf (e(tx)) can be used to obtain the factorial moments using the relationship  (r) = p(xr) (1) (see table 9.1).
3299,1,"['moment', 'factorial', 'factorial moment']", Properties of PGF,seg_609,"by putting t = 1 in equation 9.8, the rhs becomes e[x (x − 1) · · · (x − r + 1)], which is the rth factorial moment. hence, some authors call this the factorial mgf (see section 9.8, p. 391)."
3300,1,['results'], Properties of PGF,seg_609,this result follows from the fact that v (x) = e[x2] − e[x]2 = e[x (x − 1)] + e[x] − e[x]2. now use the above results. 1 5. ∫tp (t) dt = e (x+1)
3301,1,"['random variables', 'moment', 'variables', 'random']", Properties of PGF,seg_609,"this is the first inverse moment, and holds for positive random variables."
3302,0,[], Properties of PGF,seg_609,this follows by writing tcx as (tc)x .
3303,1,"['transformation', 'change of origin and scale', 'change of origin']", Properties of PGF,seg_609,this is called the change of origin and scale transformation of pgf. this follows by combining (6) and (7).
3304,1,"['probabilities', 'random variable', 'variable', 'tail probabilities', 'random', 'tail']", GENERATING FUNCTIONS FOR CDF GFCDF,seg_611,"as the pgf of a random variable generates probabilities, it can be used to generate the sum of left tail probabilities (cdf) as follows. we have seen in example 9.2 that if f(x) is the ogf of the sequence a0, a1, … , an, … , finite or infinite, then f(x)/(1−x) is the ogf of the sequence a0, a0 + a1, a0 + a1 + a2, … by replacing ai’s by probabilities, we obtain a gf that generates the sum of probabilities as"
3305,1,"['discrete distributions', 'discrete', 'distributions']", GENERATING FUNCTIONS FOR CDF GFCDF,seg_611,this works only for discrete distributions.
3306,1,"['distribution', 'geometric distribution', 'geometric']", GENERATING FUNCTIONS FOR CDF GFCDF,seg_611,example 9.7 gf for cdf of geometric distribution
3307,1,"['distribution', 'geometric distribution', 'geometric']", GENERATING FUNCTIONS FOR CDF GFCDF,seg_611,obtain the gf for cdf of a geometric distribution.
3308,1,"['distribution', 'geometric', 'geometric distribution']", GENERATING FUNCTIONS FOR CDF GFCDF,seg_611,solution 9.7 we know that the pgf of geometric distribution is p∕ (1 − qt) from which the gfcdf is obtained as g (t) = p(1 − t)−1∕ (1 − qt). expand both (1 − t)−1 and (1 − qt)−1 as infinite series’ and combine like powers to get
3309,1,"['function', 'geometric']", GENERATING FUNCTIONS FOR CDF GFCDF,seg_611,write (1 + q + q2 + q3 + · · · + qk) as (1 − qk+1) ∕ (1 − q) and cancel (1 − q) = p with the numerator to get the generating function for geometric cdf as
3310,1,"['discrete', 'discrete distributions', 'deviations', 'mean', 'distributions']", GENERATING FUNCTIONS FOR MEAN DEVIATION GFMD,seg_613,the above result can be extended to obtain a gf for mean deviations of discrete distributions. we have seen in chapter 6 that the md of discrete distributions is given by
3311,1,"['distribution', 'arithmetic mean', 'mean', 'limit']", GENERATING FUNCTIONS FOR MEAN DEVIATION GFMD,seg_613,"where ll is the lower limit of the distribution,   is the arithmetic mean, and f(x) is the cdf. to obtain a gf for md, first rewrite equation (9.9) as"
3312,1,['probabilities'], GENERATING FUNCTIONS FOR MEAN DEVIATION GFMD,seg_613,where gk denotes the sum of probabilities. multiply both sides by (1 − t)−1 and denote the lhs (1 − t)−1g (t) by h(t) to get
3313,1,"['probabilities', 'tail probabilities', 'coefficient', 'tail', 'coefficients']", GENERATING FUNCTIONS FOR MEAN DEVIATION GFMD,seg_613,"the above step is equivalent to applying the result in example 9.2 in page 376 where pk = gk. as coefficients of h(t) accumulate the sum of the cdf (“sum of the sum” of left tail probabilities), the md is easily found as twice the coefficient of t −1 in h(t). this can be stated as the following theorem."
3314,1,"['discrete', 'distribution', 'mean', 'probability', 'discrete distribution', 'function', 'coefficient']", GENERATING FUNCTIONS FOR MEAN DEVIATION GFMD,seg_613,"theorem 9.1 the md of a discrete distribution is twice the coefficient of t −1 in the power series expansion of (1 − t)−2px (t), where   is the mean (or the nearest integer to it) and px (t) is the probability generating function."
3315,1,"['tail', 'tail probabilities', 'right tail probabilities', 'probabilities']", GENERATING FUNCTIONS FOR MEAN DEVIATION GFMD,seg_613,"in the above derivation, we have marked the probabilities as p1, p2, and so on. if they are denoted as p0, p1, p2, and so on, we need to consider the sums p0 + p1 and so on. a similar result could be obtained using right tail probabilities."
3316,1,"['distribution', 'geometric distribution', 'geometric']", GENERATING FUNCTIONS FOR MEAN DEVIATION GFMD,seg_613,example 9.8 md of geometric distribution
3317,1,"['distribution', 'geometric distribution', 'geometric']", GENERATING FUNCTIONS FOR MEAN DEVIATION GFMD,seg_613,find the md of geometric distribution using theorem 9.1.
3318,1,"['distribution', 'geometric distribution', 'geometric']", GENERATING FUNCTIONS FOR MEAN DEVIATION GFMD,seg_613,solution 9.8 we have seen in example 9.7 in page 381 that the gf for cdf of a geometric distribution is
3319,1,"['integer part', 'distribution', 'mean', 'coefficient', 'coefficients']", GENERATING FUNCTIONS FOR MEAN DEVIATION GFMD,seg_613,"denote (1 − qk+1) by gk (note that there is no (1 − q) term in equation (9.15) showing that the md is zero when q p or equivalently p 1/2) and obtain the gfmd with coefficients hk = ∑ gk = ∑(1 − qk+1). as the mean of a geometric distribution is q/p, we can simply fetch the coefficient of t −1 = t[q∕p−1] in [q∕p−1] h (t) and multiply by 2 to get the md as 2 ∑k=0 (1 − qk+1), where [q∕p − 1] denotes the integer part. see chapter 6, p. 217 for further simplifications."
3320,1,"['discrete random variable', 'discrete', 'random variable', 'variable', 'expectation', 'moments', 'random']", MOMENT GENERATING FUNCTIONS MGF,seg_615,"the mgf of a random variable is used to generate the moments algebraically. let x be a discrete random variable defined for all values of x. as etx has an infinite expansion in powers of x as etx = 1 + (tx) ∕1! + (tx)2∕2! + · · · + (tx)n∕n! + · · ·, we multiply both the sides by f(x), and take expectation on both the sides to get"
3321,1,['discrete'], MOMENT GENERATING FUNCTIONS MGF,seg_615,⎧⎪∑ etxp (x) if x is discrete; mx (t) = e (etx) = ⎨ x
3322,1,['continuous'], MOMENT GENERATING FUNCTIONS MGF,seg_615,∞ ∞ etxf x dx if x is continuous. ⎪ ( )
3323,1,"['case', 'discrete']", MOMENT GENERATING FUNCTIONS MGF,seg_615,"in the discrete case, this becomes mx (t) = ∑x"
3324,1,['distribution'], MOMENT GENERATING FUNCTIONS MGF,seg_615,"∞ =0 xkf (x) by  k to obtain the following series (which is theoretically defined for all values, but depends on the distribution)"
3325,1,"['random variables', 'variables', 'case', 'summation', 'random', 'continuous']", MOMENT GENERATING FUNCTIONS MGF,seg_615,"analogous result holds for the continuous case by replacing summation by integration. by choosing |t| 1, the above series can be made convergent for most random variables."
3326,1,"['continuous random variables', 'random variables', 'variables', 'random', 'continuous']", MOMENT GENERATING FUNCTIONS MGF,seg_615,proof: this follows trivially by replacing t by et in equation (9.4). note that it is also applicable to continuous random variables. put t = 0 and use e0 = 1 to get the second part.
3327,1,"['distribution', 'binomial distribution', 'binomial']", MOMENT GENERATING FUNCTIONS MGF,seg_615,example 9.9 mgf of binomial distribution from pgf
3328,1,['mean'], MOMENT GENERATING FUNCTIONS MGF,seg_615,"if the pgf of bino(n, p) is (q + pt)n, obtain the mgf and derive the mean."
3329,1,['mean'], MOMENT GENERATING FUNCTIONS MGF,seg_615,get the mean as np. take log again to get log (mx ′ (t)) − log (mx (t)) =
3330,0,[], MOMENT GENERATING FUNCTIONS MGF,seg_615,"log (np) + t − log (q + pet). differentiate again, and denote mx"
3331,1,"['distribution', 'exponential', 'exponential distribution']", MOMENT GENERATING FUNCTIONS MGF,seg_615,example 9.10 mgf of exponential distribution
3332,1,"['distribution', 'exponential', 'exponential distribution']", MOMENT GENERATING FUNCTIONS MGF,seg_615,obtain the mgf of an exponential distribution.
3333,1,['variable'], Properties of Moment Generating Functions,seg_617,1. mgf of an origin changed variate can be found from mgf of original variable
3334,1,['variable'], Properties of Moment Generating Functions,seg_617,2. mgf of a scale changed variate can be found from mgf of original variable as
3335,1,['variable'], Properties of Moment Generating Functions,seg_617,3. mgf of origin and scale changed variate can be found from mgf of original variable as
3336,1,['cases'], Properties of Moment Generating Functions,seg_617,this follows by combining both the cases above.
3337,1,"['random variables', 'independent', 'variables', 'independent random variables', 'random']", Properties of Moment Generating Functions,seg_617,theorem 9.3 the mgf of a sum of independent random variables is the product of their mgfs. symbolically mx+y (t) = mx (t) ∗ my (t).
3338,1,"['random variables', 'independent', 'variables', 'case', 'discrete', 'independent random variables', 'random', 'continuous', 'pairwise independent']", Properties of Moment Generating Functions,seg_617,"proof: we prove the result for the discrete case. mx+y (t) = e (et(x+y)) = e (etxety). if x and y are independent, we write the rhs as ∑xetxf (x) ∗ ∑yetyf (y) = mx (t) ∗ my (t). the proof for the continuous case follows similarly. this result can be extended to any number of pairwise independent random variables."
3339,1,['independent'], Properties of Moment Generating Functions,seg_617,"if x1,x2, … ,xn are independent, and y = ∑ixi then my (t) = ∏imxi (t)."
3340,1,['moments'], Properties of Moment Generating Functions,seg_617,example 9.11 moments from mx (t)
3341,0,[], Properties of Moment Generating Functions,seg_617,with rspect to t gives  
3342,1,['variable'], Properties of Moment Generating Functions,seg_617,"  t etx) = e (xetx) because x is considered as a constant (and t is our variable). putting t = 0 on the rhs  2 we get the result, as e0 = 1. differentiating a second time, we get  t2 mx (t) ="
3343,1,['moment'], Properties of Moment Generating Functions,seg_617,′′ (t = 0) = e (x2). repeated application of this operation allows us to find the kth moment as mx (k) (t = 0) = e (xk). this gives  2 = mx ′′ (t = 0) − [mx ′ (t = 0)]2.
3344,0,[], Properties of Moment Generating Functions,seg_617,"example 9.12 mgf of bino(n, p)"
3345,1,['moments'], Properties of Moment Generating Functions,seg_617,"find the mgf of bino(n, p), and obtain the first two moments."
3346,1,"['poisson', 'distribution', 'poisson distribution']", Properties of Moment Generating Functions,seg_617,example 9.13 mgf of a poisson distribution
3347,1,"['poisson', 'distribution', 'moments', 'poisson distribution']", Properties of Moment Generating Functions,seg_617,"find the mgf for central moments of a poisson distribution. hence, show that"
3348,0,['e'], Properties of Moment Generating Functions,seg_617,solution 9.13 first consider the ordinary mgf defined as mx (t) = e (etx) =
3349,1,"['poisson', 'distribution', 'poisson distribution']", Properties of Moment Generating Functions,seg_617,"mean of a poisson distribution is   =  , we use the property 1 to get"
3350,0,[], Properties of Moment Generating Functions,seg_617,expand et as an infinite series. the rhs becomes  ∑k
3351,0,[], Properties of Moment Generating Functions,seg_617,can be written as  ∑k
3352,1,['coefficients'], Properties of Moment Generating Functions,seg_617,∞ =0  jt j+k∕[j!k!]. equate coefficients of tr on both
3353,1,"['binomial coefficients', 'binomial', 'coefficients']", Properties of Moment Generating Functions,seg_617,"0 r)!]. cross-multiplying and identifying the binomial coefficients, this becomes"
3354,1,"['continuous', 'summation', 'case', 'discrete']", Properties of Moment Generating Functions,seg_617,"proof: we prove the result for the continuous case. the proof for discrete case follows easily by replacing integration by summation. as e[|x|k] exists, we have ∫x|x|kdf (x)   ∞. now consider an arbitrary j   k for which"
3355,0,[], Properties of Moment Generating Functions,seg_617,"the rhs of equation (9.24) is upper bounded by 1 + e[|x|k], and is  ∞. this proves that the lhs exists for each j."
3356,1,"['distribution', 'gamma distribution', 'gamma']", Properties of Moment Generating Functions,seg_617,example 9.14 mgf of a gamma distribution
3357,1,"['gamma', 'distribution', 'moments', 'gamma distribution']", Properties of Moment Generating Functions,seg_617,"find the mgf of a gamma distribution f (x) =  me− xxm−1∕γ (m), where x ≥ 0 and     0, and obtain the first two moments."
3358,1,['moment'], Properties of Moment Generating Functions,seg_617,"−m∕ (1 − t∕ ) (−1∕ ) from which by putting t = 0 we get the first moment as (m∕ ). taking the derivative again, we get [mx (t)m′′x (t) − (mx"
3359,1,['variance'], Properties of Moment Generating Functions,seg_617,"get m′′x (t = 0) = (m∕ )2 + (m∕ 2), from which the variance is obtained using  2 = mx"
3360,1,"['distribution', 'cases']", CHARACTERISTIC FUNCTIONS CHF,seg_619,the mgf of a distribution need not always exist. those cases can be dealt with in the
3361,1,['expected value'], CHARACTERISTIC FUNCTIONS CHF,seg_619,"complex domain by finding the expected value of eitx, where i = √"
3362,1,"['random variable', 'variable', 'random']", CHARACTERISTIC FUNCTIONS CHF,seg_619,"−1, which always exist. thus, the chf of a random variable is defined as"
3363,1,['discrete'], CHARACTERISTIC FUNCTIONS CHF,seg_619,eitxp if x is discrete; chf = e (eitx) = ∑∞
3364,1,['continuous'], CHARACTERISTIC FUNCTIONS CHF,seg_619,∫x=−∞ e f (x) dx if x is continuous.
3365,1,"['mcclaurin series', 'discrete', 'random variable', 'variable', 'moments', 'random', 'continuous']", CHARACTERISTIC FUNCTIONS CHF,seg_619,"we have seen above that the chf, if it exists, can generate the moments. irrespective of whether the random variable is discrete or continuous, we could expand the chf as a mcclaurin series as"
3366,1,"['distribution', 'case', 'continuous']", CHARACTERISTIC FUNCTIONS CHF,seg_619,which is convergent for an appropriate choice of t (which depends on the distribution). as x (t) in the continuous case can be represented as x (t) = ∫−
3367,0,['n'], CHARACTERISTIC FUNCTIONS CHF,seg_619,"∞ ∞ eitxdf (x), successive derivatives with t gives ∫ inxndf (x) = in n"
3368,1,['function'], CHARACTERISTIC FUNCTIONS CHF,seg_619,"′ . define  (n) (x) as the nth derivative of the delta function. then, the pdf can be written as an infinite sum as"
3369,0,[], CHARACTERISTIC FUNCTIONS CHF,seg_619,see references 134 and 287 for further details.
3370,1,"['characteristic function', 'cauchy', 'cauchy distribution', 'distribution', 'function']", CHARACTERISTIC FUNCTIONS CHF,seg_619,example 9.15 characteristic function of the cauchy distribution
3371,1,"['characteristic function', 'cauchy', 'cauchy distribution', 'distribution', 'function']", CHARACTERISTIC FUNCTIONS CHF,seg_619,find the characteristic function of the cauchy distribution.
3372,1,"['functions', 'continuous', 'continuous distributions', 'distributions']", Properties of Characteristic Functions,seg_621,"characteristic functions are laplace transforms of the corresponding pdf. as all laplace transforms have an inverse, we could invert it to get the pdf. hence, there is a one-to-one correspondence between the chf and pdf. this is especially useful for continuous distributions as shown below. there are many simple properties satisfied by the chf."
3373,0,[], Properties of Characteristic Functions,seg_621,"1. (t) = (−t) , (0) = 1, and | (±t) | ≤ 1. in words, this means that the complex conjugate of the chf is the same as that obtained by replacing t with -t in the chf. the assertion (0) = 1 follows easily because this makes eitx to be 1."
3374,0,[], Properties of Characteristic Functions,seg_621,2. ax+b (t) = eibt x (at). this result is trivial as it follows directly from the definition.
3375,1,['independent'], Properties of Characteristic Functions,seg_621,"3. if x and y are independent,  ax+by (t) =  x (at) . y (bt). putting a = b = 1, we get  x+y (t) =  x (t) . y (t) if x and y are independent."
3376,1,['continuous'], Properties of Characteristic Functions,seg_621,"4.   (t) is continuous in t, and convex for t   0. this means that if t1 and t2 are two values of t   0, then   ((t1 + t2) ∕2) ≤ 1 2 [ (t1) +  (t2)]."
3377,1,"['random variables', 'variables', 'symmetric', 'random']", Properties of Characteristic Functions,seg_621,example 9.16 symmetric random variables
3378,1,"['symmetric', 'random variable', 'variable', 'random']", Properties of Characteristic Functions,seg_621,prove that the random variable x is symmetric about the origin if the chf   (it) is real-valued for all t.
3379,1,"['function', 'symmetric']", Properties of Characteristic Functions,seg_621,"solution 9.16 assume that x is symmetric about the origin, so that f (−x) = f (x). then for a bounded and odd borel function g (x)we have ∫ g (x) df (x) = 0. as g (x) is odd, this is equivalent to ∫ sin (tx) df (x) = 0. hence,   (t) = e (eitx)"
3380,1,['table'], Properties of Characteristic Functions,seg_621,"= e[cos(tx)] is real. also, as  −x (t) =  x (−t) =  x (t) =  x (t),fx (x) and f−x (x) are the same (table 9.2)."
3381,1,"['characteristic function', 'distribution', 'function']", Properties of Characteristic Functions,seg_621,theorem 9.4 the characteristic function uniquely determines a distribution. the inversion theorem provides a means to find the pdf from the characteristic function as f (x) = 1∕ (2 ) ∫−
3382,1,"['random variables', 'variables', 'distribution', 'probability distribution', 'probability', 'random']", Properties of Characteristic Functions,seg_621,"9.6.1.1 uniqueness theorem let the random variables x and y have mgf mx (t) and my (t), respectively. if mx (t) = my (t) ∀t, then x and y have the same probability distribution."
3383,1,"['poisson', 'normal', 'exponential']", CUMULANT GENERATING FUNCTIONS CGF,seg_623,"the cgf is slightly easier to work with for exponential, normal, and poisson distributions. it is defined in terms of the mgf as kx (t) = ln (mx (t)) = ∑j"
3384,1,"['functions', 'cumulants', 'distribution', 'normal', 'moments', 'mean', 'cumulant', 'variance', 'normal distribution']", CUMULANT GENERATING FUNCTIONS CGF,seg_623,"∞ =1 kjtj∕j!, where kj is the jth cumulant. this relationship shows that cumulants are polynomial functions of moments (low-order cumulants can also be exactly equal to corresponding moments). for example, for the general univariate normal distribution with mean 1 = and variance 2 = 2, the first and second cumulants are, respectively, 1 = and 2 = 2."
3385,1,"['standardized', 'variable', 'standardized variable']", CUMULANT GENERATING FUNCTIONS CGF,seg_623,corollary 2 cgf of a standardized variable can be expressed as k(x− )∕  (t) = (− ∕ ) t + kx (t∕ ).
3386,0,[], CUMULANT GENERATING FUNCTIONS CGF,seg_623,proof: this follows from the above theorem by setting a = 1∕  and b = − ∕ .
3387,1,"['cumulants', 'moments']", CUMULANT GENERATING FUNCTIONS CGF,seg_623,"the cumulants can be obtained from moments and vice versa [225, 288]. this holds for cumulants about any origin (including zero) in terms of moments about the same origin."
3388,1,['moments'], Relations Among Moments and Cumulants,seg_625,"we have seen in chapter 8, equation 8.19 (p. 357) that the central and raw moments"
3389,0,[], Relations Among Moments and Cumulants,seg_625,′. as the cgfs of some distribu-
3390,1,"['cumulants', 'moment', 'moments']", Relations Among Moments and Cumulants,seg_625,"tions are easier to work with, we can find cumulants and use the relationship with moments to obtain the desired moment."
3391,1,['cumulant'], Relations Among Moments and Cumulants,seg_625,theorem 9.6 the rth cumulant can be obtained from the cgf as  r =  r
3392,0,[], Relations Among Moments and Cumulants,seg_625,proof: we have
3393,1,"['cumulant', 'case']", Relations Among Moments and Cumulants,seg_625,"as done in the case of mgf, differentiate (9.27) k times and put t = 0 to get the kth cumulant. see references 134 and 289 for details."
3394,1,"['cumulants', 'moments']", Relations Among Moments and Cumulants,seg_625,example 9.17 moments from cumulants
3395,0,[], Relations Among Moments and Cumulants,seg_625,substitute for kx (t) also to get
3396,0,['n'], Relations Among Moments and Cumulants,seg_625,differentiate n times and put t = 0 to get
3397,1,['cumulants'], Relations Among Moments and Cumulants,seg_625,"put n = 0,1, and so on to get the desired result. there is another way to get the result for low order cumulants. truncate mx (t) as 1 + t∕1! 1 + t2∕2! 2"
3398,1,['coefficients'], Relations Among Moments and Cumulants,seg_625,"compare the coefficients of tk∕k! to get  1 =  1,  2 =  2 −  1"
3399,0,[], Relations Among Moments and Cumulants,seg_625,"next, write mx (t) as 1 + [tx∕1! + (tx)2∕2! + · · ·], expand kx (t) = log (mx (t)) as an infinite series to get"
3400,1,['coefficients'], Relations Among Moments and Cumulants,seg_625,equate like coefficients of t to get
3401,1,"['distribution', 'normal', 'moments', 'normal distribution']", Relations Among Moments and Cumulants,seg_625,example 9.18 moments of normal distribution from cgf
3402,1,"['method', 'distribution', 'normal', 'moments', 'normal distribution']", Relations Among Moments and Cumulants,seg_625,obtain the first three moments of normal distribution using cgf method.
3403,1,['coefficients'], Relations Among Moments and Cumulants,seg_625,"taking natural log, we get kx (t) = t  + 1 2 t2 2. comparing the coefficients of t∕1! and t2∕2!, we get  1 =   and  2 =  2. as the t3 term is missing,  3 = 0."
3404,1,"['poisson', 'moment', 'factorial', 'moments', 'binomial', 'hypergeometric', 'factorial moment', 'stirling number', 'factorial moments', 'poisson distributions', 'falling factorial', 'distributions']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,"there are two types of factorial moments known as falling factorial and raising factorial moments. among these, the falling factorial moments are more popular. the kth (falling) factorial moment of x is defined as e[x (x − 1) (x − 2) · · · (x − k + 1)] = e[x!∕ (x − k)!], where k is an integer ≥ 1. it is easier to evaluate for those distributions that have an x! or γ (x + 1) in the denominator (e.g., binomial, negative binomial, hypergeometric, and poisson distributions). the factorial moments and ordinary moments are related through the stirling number of first kind as follows:–"
3405,1,"['factorial', 'moments', 'factorial moments']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,a reverse relationship exists between the ordinary and factorial moments using the
3406,1,['stirling number'], FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,"′ j), where s (r, j) is the stirling number of second kind [134]."
3407,1,"['factorial', 'moments', 'factorial moments']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,"there are two ways to get (falling) factorial moments. the simplest way is by differentiating the pgf (see section 9.2.1, p. 379). as px (t) = e (tx) = e (ex log(t)) = mx (log (t)), we could differentiate it k times as in equation (9.8), page 380, and put t = 1 to get factorial moments."
3408,1,"['binomial theorem', 'binomial']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,"we define it as e[(1 + t)x], because if we expand it using binomial theorem, we get"
3409,1,"['moment', 'factorial', 'moments', 'factorial moment', 'expectations', 'factorial moments']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,"by taking term-by-term expectations on the rhs, we get the factorial moments. the raising factorial moment is defined as e[x (x + 1) (x + 2) · · · (x + k − 1)] = e[(x + k − 1)!∕ (x − 1)!]. an analogous expression can also be obtained"
3410,0,[], FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,for raising factorials using the expansion (1 − t)−x = ∑k
3411,1,"['factorial', 'moments', 'expectations', 'factorial moments']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,"ing term-by-term expectations as e[(1 − t)−x] = e[1 + tx + t2x (x + 1) ∕2! + t3x (x + 1) (x + 2) ∕3! + · · ·], we get raising factorial moments. we could also get raising factorial moments from pgf px (t) = e (t−x). differentiating it once gives px′ (t) = e (−xt−x−1). from this, we get px′ (1) = e (−x). differentiating it r times,"
3412,1,"['results', 'continuous distributions', 'summation', 'continuous', 'distributions']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,replacing the summation by integration gives us the corresponding results for the continuous distributions.
3413,1,"['moment', 'factorial', 'moment ', 'mean', 'factorial moment', 'falling factorial']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,"to distinguish between the two, we will denote the falling factorial moment as e (x(k)) or  (k) and the raising factorial moment as e (x(k)) or  (k). unless otherwise specified, factorial moment will mean falling factorial moment  (k)."
3414,1,"['poisson', 'moment', 'factorial', 'distribution', 'factorial moment', 'poisson distribution']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,example 9.19 factorial moment of the poisson distribution
3415,1,"['poisson', 'moment', 'factorial', 'distribution', 'moments', 'factorial moment', 'poisson distribution']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,"find the kth factorial moment of the poisson distribution, and obtain the first two moments."
3416,0,[], FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,solution 9.19 by definition
3417,1,"['moment', 'factorial', 'moments', 'factorial moment']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,"alternatively, we could obtain the fmgf directly and get the desired moments. fmgfx (t) = e[(1 + t)x] = ∑∞ x=0 (1 + t)xe−  x∕x! = e− ∑∞ x=0 [  (1 + t)]x∕x! = e− e (1+t) = e t. the kth factorial moment is obtained by differentiating this expression k times and putting t = 0. we know that the kth derivative of e t is  ke t, from which the kth factorial moment is obtained as  k. putting k = 1 and 2 gives the desired moments."
3418,1,"['poisson', 'factorial', 'distribution', 'moments', 'poisson distribution', 'factorial moments']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,"corollary 3 the factorial moments of a poisson distribution are related as  (k) =  r (k−r). in particular,  (k) =   (k−1)."
3419,1,"['geometric distribution', 'factorial', 'distribution', 'geometric']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,example 9.20 factorial mgf of the geometric distribution
3420,1,"['geometric distribution', 'factorial', 'distribution', 'geometric']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,find the factorial mgf of the geometric distribution.
3421,0,[], FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,solution 9.20 by definition fmgfx (t) =
3422,1,"['moment', 'factorial', 'factorial moment']", FACTORIAL MOMENT GENERATING FUNCTIONS FMGF,seg_627,"as 1 − q = p, the denominator becomes [1 − q (1 + t)] = p − qt. hence, fmgfx (t) = p∕ (p − qt). the kth factorial moment is obtained by differentiating this expression k times and putting t = 0. we know that the kth derivative of 1∕ (ax + b) is arr!(−1)r∕(ax + b)r+1. hence, kth derivative of 1∕[1 − q (1 + t)] is r!qr∕pr+1, as q = 1 − p. this gives the kth factorial moment as pr!qr∕pr+1 = r!(q∕p)r."
3423,1,"['random variables', 'variables', 'random variable', 'variable', 'random']", CONDITIONAL MOMENT GENERATING FUNCTIONS CMGF,seg_629,consider an integer random variable that takes values ≥ 1. we define a sum of independent random variables as sn = ∑i
3424,1,['distribution'], CONDITIONAL MOMENT GENERATING FUNCTIONS CMGF,seg_629,"n =1 xi. for a fixed value of n = n, the distribution"
3425,1,"['independent', 'table', 'conditional', 'variates', 'distributions']", CONDITIONAL MOMENT GENERATING FUNCTIONS CMGF,seg_629,"of the finite sum sn can be obtained in closed form for many distributions when the variates are independent (see table 7.47, chapter 7). the conditional mgf can be expressed as"
3426,1,"['characteristic function', 'independent', 'mutually independent', 'conditional', 'variates', 'function']", CONDITIONAL MOMENT GENERATING FUNCTIONS CMGF,seg_629,"replacing t by “it” gives the corresponding conditional characteristic function. if the variates are mutually independent, this becomes ms|n (t|n) = [mx (t)]n ."
3427,1,"['functions', 'random variables', 'variables', 'random', 'distributions']", CONVERGENCE OF GENERATING FUNCTIONS,seg_631,properties of generating functions are useful in deriving the distributions to which a sequence of generating functions converge. let xn be a sequence of random variables with chf  n
3428,1,"['distribution', 'limit']", CONVERGENCE OF GENERATING FUNCTIONS,seg_631,"x (t) converges to a unique limit, say  x (t) for all points in n→∞ a neighborhood of t = 0, then that limit determines the unique cdf to which the distribution of xn converge. symbolically,"
3429,1,"['deviation', 'functions', 'statistics', 'discrete', 'discrete distributions', 'mean', 'probability', 'function', 'coefficient', 'distributions']", SUMMARY,seg_633,"this chapter introduced various generating functions encountered in statistics. these have wide applications in many other fields including astrophysics, fluid mechanics, spectroscopy, and various engineering fields. examples are included to illustrate the use of various generating functions. the classical probability generating function of discrete distributions is extended to get a new generating function for the cdf. this is then used to derive the mean deviation by extracting just one coefficient."
3430,1,['information'], SUMMARY,seg_633,"see references 134, 284, 287 and 290 for further information."
3431,1,"['distribution', 'discrete distribution', 'discrete']", SUMMARY,seg_633,a) pgf of discrete distribution can be obtained from its mgf
3432,1,"['continuous', 'distributions', 'continuous distributions']", SUMMARY,seg_633,b) pgfs of continuous distributions do not exist
3433,1,"['random variables', 'variables', 'random']", SUMMARY,seg_633,c) mgf is defined only for positive random variables
3434,1,['moments'], SUMMARY,seg_633,"d) moments of order k   n of a bino(n,p) are all zeros"
3435,1,"['functions', 'f distribution', 'distribution', 'moments']", SUMMARY,seg_633,e) all moments of f distribution are functions of numerator df
3436,1,"['cauchy', 'cauchy distribution', 'distribution', 'moments']", SUMMARY,seg_633,f) all odd moments of cauchy distribution are nonexistent
3437,1,"['functions', 'characteristic functions']", SUMMARY,seg_633,g) all characteristic functions are periodic with period 2 
3438,1,"['random variables', 'variables', 'random']", SUMMARY,seg_633,h) chf of a sum of random variables is always the product of the chf.
3439,0,[], SUMMARY,seg_633,9.2 find the cdf and mgf of the dis9.13 what are the conditions for a
3440,1,"['moment', 'moments', 'function']", SUMMARY,seg_633,"a tribution f (x; a, b) = xa−1e−(x∕b)a , function to be a moment generba ating function? are these funcand obtain the first two moments. tions true mgf? (a) ea(t−1)+b(t−1)2 , 9.3 prove that max (t) = mx (at) and (b) ea(t−1)∕(1−bt), (c) ea(t−1)+b("
3441,1,"['geometric distribution', 'distribution', 'moments', 'geometric']", SUMMARY,seg_633,"t2−1), max+b (t) = ebtmx (at). deduce (d) e|(t−1)∕(t2−1) | that m(x− )∕  (t) = e− t∕ mx (t∕ ). 9.14 find the mgf and first three 9.4 for geometric distribution with moments of the triangular distri-"
3442,1,"['moments', 'pareto']", SUMMARY,seg_633,"9.5 if x1,x2, … ,xn are iid and moments of the pareto distribu-"
3443,1,['independent'], SUMMARY,seg_633,9.6 find the mgf and derive the ky (t) if x and y are independent.
3444,1,['moments'], SUMMARY,seg_633,first two moments for the distri-
3445,1,"['distribution', 'exponential', 'exponential distribution']", SUMMARY,seg_633,"r, r + 1, … 9.18 prove that the mgf of a truncated 9.7 what do you get as the result of exponential distribution is  "
3446,1,"['random variable', 'variable', 'random', 'function']", SUMMARY,seg_633,probability generating function? 9.19 the mgf of a random variable
3447,1,['mean'], SUMMARY,seg_633,"9.8 if mx (t) = 1∕ (b2 − t2) find the is (e2t − e−2t) ∕4t. find the mean,"
3448,1,"['variance', 'probability']", SUMMARY,seg_633,mean and variance. and the probability that p[|x −
3449,1,"['moments', 'even moments']", SUMMARY,seg_633,a sequence of bounded numprove that the even moments are bers with egf∑n
3450,1,"['distribution', 'discrete distribution', 'discrete']", SUMMARY,seg_633,9.21 check whether the pgf exists for a xn∕n!. a discrete distribution with f (x) =
3451,1,"['variance', 'mean']", SUMMARY,seg_633,"9.11 find the pgf, mean, and variance c∕[x (x + 1)], for x = 1, 2, …"
3452,0,[], SUMMARY,seg_633,"(2y − (a + b)) ∕ (b − a), find the 9.12 for the logarithmic series distrimoments of x from those of y."
3453,1,"['moment', 'factorial', 'factorial moment']", SUMMARY,seg_633,"that the rth factorial moment 9.23 if x and y are iid, and z = cx +  (r) = c (r − 1)![  (1 −  )]r. (1 − c)y , find the mgf of z in"
3454,1,"['distribution', 'moments']", SUMMARY,seg_633,"terms of mgf of x and y. deduce 9.33 the maxwell distribution gives the first two moments. the velocity of a molecule at absolute temperature x as f (x) = 9.24 how are the pgf (px (t)), mgf 2 2"
3455,0,[], SUMMARY,seg_633,(kx (t)) related. boltzmann constant. find the pgf
3456,1,"['random variable', 'variable', 'random']", SUMMARY,seg_633,the random variable n − x. what temperature tr.
3457,1,"['continuous', 'random']", SUMMARY,seg_633,does px (t = −1) give? 9.34 if the pdf of a continuous random
3458,0,[], SUMMARY,seg_633,9.26 obtain the cdf generating funcvariable is f (x) =
3459,1,"['poisson', 'deviation', 'distribution', 'mean', 'poisson distribution']", SUMMARY,seg_633,"tion for the poisson distribution, and obtain the mean deviation. 2 (b + x) ∕[b (a + b)] ⎧ 9.27 obtain the cdf generating func⎪ if − b ≤ x 0"
3460,1,"['deviation', 'distribution', 'mean']", SUMMARY,seg_633,"tion for the logarithmic series ⎪⎪2 (a − x) ∕[a (a + b)] distribution, and obtain the mean ⎨ deviation. if 0 ≤ x   a ⎪⎪"
3461,1,"['function', 'mean']", SUMMARY,seg_633,"differentiable function of x with e[x]   ∞, then e[g (x) (x −  )] =  2e[g′(x)]. find the mgf and the mean."
3462,1,"['continuous', 'random']", SUMMARY,seg_633,9.35 if the pdf of a continuous random 9.29 if f (y|x) = exp (− (y − x)) for
3463,1,"['random variable', 'variable', 'random']", SUMMARY,seg_633,9.30 if x is a positive random variable ⎪(a − x) ∕a2 if 0 ≤ x   a
3464,1,"['random variables', 'variables', 'random']", SUMMARY,seg_633,"⎨ with pgf px (t) find the pgf of the ⎪0 elsewhere random variables x − c, and |x|. ⎩"
3465,1,['variable'], SUMMARY,seg_633,dom variable e[x2] = 2 ∫0
3466,1,['mean'], SUMMARY,seg_633,∞ x[1 − find the mgf and the mean.
3467,1,['bernoulli'], SUMMARY,seg_633,fx (x)]dx. 9.36 what does [kx (t) + kx (−t)]∕2 generate? 9.32 in a sequence of iid bernoulli
3468,1,"['success', 'probability of success', 'probability']", SUMMARY,seg_633,"trials with probability of success 9.37 what does [px (t) + px (−t)]∕2 p, suppose we count either the generate?"
3469,1,"['failures', 'successes', 'failure', 'moments']", SUMMARY,seg_633,"number of successes needed to 9.38 if all moments of x exist, prove get the first failure, or the number of failures needed to get the that (t) = ∑k"
3470,1,"['function', 'success', 'probability']", SUMMARY,seg_633,"first success. find the pdf and the 9.39 if f (x) = (c + 1) xc for 0   x   1, probability generating function. find the pgf and e[ln (x)]."
3471,1,"['monotonic', 'random variable', 'variable', 'monotonic function', 'random', 'function']", SUMMARY,seg_633,"9.40 if   (x) is a real-valued, monotonic function of a positive random variable"
3472,1,['function'], SUMMARY,seg_633,"1 eytta−1 γ(b) (1 − t)b−a−1dt where k = and 1f1 (a, b; y) is the confluent hypergeoγ(a)γ(b−a) metric function. hence, show that e (y) = a∕b."
3473,1,"['factorial', 'negative binomial', 'binomial', 'hypergeometric']", SUMMARY,seg_633,9.42 find the factorial mgf of hypergeometric and negative binomial distribu-
3474,1,"['factorial', 'moments', 'factorial moments']", SUMMARY,seg_633,"tions and show that the factorial moments are as given: (i) hypg(n,n,p):"
3475,1,"['geometric distributions', 'factorial', 'binomial', 'geometric', 'distributions']", SUMMARY,seg_633,9.43 find the factorial mgf of binomial and geometric distributions and show
3476,1,"['factorial', 'moments', 'factorial moments']", SUMMARY,seg_633,"that the factorial moments are as given: (i) bino(n, p): n(r)pr. (ii) geom(p): r!qr−1∕pr."
3477,0,[], FUNCTIONS OF RANDOM VARIABLES,seg_635,"after finishing the chapter, students will be able to"
3478,1,"['functions', 'random variables', 'variables', 'distribution', 'random']", FUNCTIONS OF RANDOM VARIABLES,seg_635,◾ understand distribution of functions of random variables
3479,1,"['linear', 'random variables', 'variables', 'transformations', 'random']", FUNCTIONS OF RANDOM VARIABLES,seg_635,◾ distinguish linear and other transformations of random variables
3480,1,"['random variables', 'variables', 'transformations', 'random']", FUNCTIONS OF RANDOM VARIABLES,seg_635,◾ comprehend trigonometric transformations of random variables
3481,1,"['arbitrary transformations', 'transformations']", FUNCTIONS OF RANDOM VARIABLES,seg_635,◾ describe arbitrary transformations
3482,1,['transformations'], FUNCTIONS OF RANDOM VARIABLES,seg_635,◾ apply various transformations to practical problems
3483,0,[],,seg_637,"after finishing the chapter, students will be able to"
3484,1,"['functions', 'random variables', 'variables', 'distribution', 'random']",,seg_637,◾ understand distribution of functions of random variables
3485,1,"['linear', 'random variables', 'variables', 'transformations', 'random']",,seg_637,◾ distinguish linear and other transformations of random variables
3486,1,"['random variables', 'variables', 'transformations', 'random']",,seg_637,◾ comprehend trigonometric transformations of random variables
3487,1,"['arbitrary transformations', 'transformations']",,seg_637,◾ describe arbitrary transformations
3488,1,['transformations'],,seg_637,◾ apply various transformations to practical problems
3489,1,"['functions', 'random variables', 'variables', 'random']",,seg_637,10.1 functions of random variables
3490,1,"['functions', 'random variables', 'random', 'independent', 'variables', 'standard normal', 'variates', 'distribution', 'random variable', 'variable', 'normal', 'standard', 'distributions']",,seg_637,"this chapter discusses the distribution of functions of a single random variable. there are many situations where simple functions of random variables have well-known distributions. one example is the relation between a standard normal and a chi-square distribution. as shown below, the square of a standard normal is chi-square distributed with one dof. if there are several independent normal variates, the sum of the squares is also chi-square distributed with n dof, where n is the number of variates."
3491,1,"['distribution function', 'change of origin', 'random', 'function', 'transformation', 'change of origin and scale', 'standard', 'transformed', 'standard normal', 'distribution', 'method', 'variable', 'normal']",,seg_637,"distribution of a function of random variable(s) has many applications in statistical inference. for instance, any general normal variate can be transformed into the standard normal (n(0, 1)) using a simple change of origin and scale transformation. the classical method known as the method of distribution function (modf) is useful when the cdf has closed form. the cdf of the transformed variable is easily"
3492,1,"['gumbel', 'logistic', 'method', 'continuous', 'table', 'case', 'pareto', 'function', 'transformation', 'distributions']",,seg_637,"obtained using modf when the transformation is strictly increasing or decreasing function, from which the pdf follows readily by differentiation (see table above) in the continuous case. this method can be used to find the pdf of the logistic, gumbel, pareto, and uniform distributions."
3493,1,"['functions', 'independent', 'variables', 'dependent', 'distribution', 'dependent and independent variables']",,seg_637,"we come across functions of a variate in engineering applications as well. these are sometimes governed by physical laws such as that between resistance and current in a circuit. these are usually represented as exact or approximate functional relationships among dependent and independent variables. if the distribution of one of them is known, the behavior of the other can easily be modeled. as an example, consider the"
3494,0,[],,seg_637,"electrical resistance of semiconductors, which depends on the temperature in ∘k as"
3495,1,"['parameters', 'distribution', 'exponential', 'variation']",,seg_637,"if the temperature variation is known, the resistance distribution can be derived for some values of the parameters. similarly, the potential energy of weakly interacting dielectric gas subjected to an external electric field is modeled using an exponential law as f (y) = exp(−y∕(kt)), where y = potential energy, k = boltzmann constant, and t = absolute temperature in ∘k."
3496,1,"['method', 'transformations', 'geometric', 'distributions']",,seg_637,"there are many ways to derive such related distributions (see references 137, 225, 291–293. the most popular among them are the (i) cdf method (ii) mgf (or chf) method (iii) trigonometric transformations (iv) geometric reasoning and (v) using jacobians."
3497,1,"['median', 'range', 'symmetric', 'results', 'change of origin', 'change of scale', 'location', 'distribution', 'transformations', 'mean', 'transformation', 'location measure']", DISTRIBUTION OF TRANSLATIONS,seg_639,"these are obtained by a change of origin transformation y = x ± c. as the variate values are shifted either to the right (c is positive) or to the left (c is negative), the pdf remains the same, but the range is modified accordingly. a special translation is called reflection as y = −x + c, where c is a location measure. if x is symmetric, y = −x + c results in the same distribution if c is the mean, median, or mode (all of which coincides for symmetric laws). this transformation is usually applied along with one of the following transformations such as change of scale or square transformation."
3498,1,['distribution'], DISTRIBUTION OF TRANSLATIONS,seg_639,"example 10.1 translations of cuni(a, b) distribution"
3499,1,['distribution'], DISTRIBUTION OF TRANSLATIONS,seg_639,"if x is cuni(a, b), find the distribution of (i) y = x − a (ii) y = (x − (a + b)∕2)."
3500,1,"['change of origin', 'case', 'transformations', 'distribution', 'rectangular distribution', 'mean', 'transformation', 'limit']", DISTRIBUTION OF TRANSLATIONS,seg_639,"solution 10.1 as both of these are change of origin transformations, the pdf remains the same as f (y) = 1∕(b − a). in case (i) the lower limit becomes 0 and upper limit becomes (b − a), so that f (y) = 1∕(b − a), 0   y   (b − a). in case (ii) the lower limit is a− (a + b)/2 = (a − b)/2, and the upper limit is b − (a + b)∕2 = (b − a)∕2 = f (y) = 1∕(b − a). this gives f (y) = 1∕(b − a),−(b − a)∕2   y   (b − a)∕2. as (a + b)∕2 is the mean of a rectangular distribution, this transformation is a reflection."
3501,1,"['poisson', 'discrete random variable', 'case', 'discrete', 'change of scale', 'distribution', 'variable', 'random variable', 'discrete distribution', 'random', 'poisson distribution', 'transformation']", DISTRIBUTION OF CONSTANT MULTIPLES,seg_641,"first, consider the case of a discrete random variable x. if c is an integer, then y = c ∗ x is a change of scale transformation that maps x values to positive or negative numbers. hence, depending on the sign of c, y could belong to the same family of distribution. as an example, if x is poisson ( ) with mgf exp( (et − 1)), y has mgf exp( (ect − 1)). this is the mgf of a poisson distribution. if c is a fraction, the distribution is still well defined if we assume that x takes fractional values (but still it is discrete distribution due to the discontinuity)."
3502,1,['distribution'], DISTRIBUTION OF CONSTANT MULTIPLES,seg_641,"example 10.2 constant multiple of cuni(a, b) distribution"
3503,1,['distribution'], DISTRIBUTION OF CONSTANT MULTIPLES,seg_641,"if x is cuni(a, b) find the distribution of y = (2x − (a + b))∕(b − a)."
3504,1,['range'], DISTRIBUTION OF CONSTANT MULTIPLES,seg_641,"solution 10.2 write y = (2x − (a + b))∕(b − a) as c ∗ x + d, where c = 2∕(b − a) and d = −(a + b)∕(b − a). solve for x to get x = (y − d)∕c. then use (10.3) to get g(y) = f ((y − d)∕c)∕c. as x is cuni(a, b), f (x) = 1∕(b − a), a   x   b. as this does not involve x, putting x = (y − d)∕c has no effect. the range is modified as −1 and +1. substitute the values of c and d to get g(y) = (1∕(b − a))∕(2∕(b − a)) = 1∕2,−1   y   1."
3505,1,"['method', 'continuous', 'range', 'transformations', 'distribution', 'function', 'transformation']", METHOD OF DISTRIBUTION FUNCTIONS MODF,seg_643,"the modf is a simple and powerful method to find the pdf of a variety of continuous transformations. consider the general transformation y = h(x). the modf works when (i) h(x) is either an increasing or a decreasing function of x without discontinuities, (ii) the first derivative of h(x) exists throughout the range of the variate, (iii) h(x) is invertible (so that x = h−1(y), is uniquely solvable), and (iv) f(x) is differentiable once. we illustrate the use of modf for various forms of h(x) in their respective sections. consider the transformation y = h(x) = c ∗ x + b, where x has a known distribution. it satisfies all the three conditions on h(x). the cdf of y is"
3506,0,[], METHOD OF DISTRIBUTION FUNCTIONS MODF,seg_643,"note that in equation (10.2), g(.) is the cdf of y, and f(.) is the cdf of x. as the cdf of y contains only y; and the constants (b,c), we have simply expressed y in terms of x. differentiate with respect to y to get"
3507,1,"['function', 'cases']", METHOD OF DISTRIBUTION FUNCTIONS MODF,seg_643,"here, we need to consider two cases as c   0 or c   0. the above result holds for c   0. when c   0 (y = h(x) is a decreasing function), we get"
3508,1,['cases'], METHOD OF DISTRIBUTION FUNCTIONS MODF,seg_643,differentiation gives us g(y) = (− ∕ y)f((y − b)∕c) = (−1∕c) ∗ f ((y − b)∕c). combine both cases to get
3509,1,['linear'], METHOD OF DISTRIBUTION FUNCTIONS MODF,seg_643,"where the vertical line means “absolute value,” which absorbs the ± sign. thus, the pdf of y is easily obtained from that of x. the only assumption we have made is that f(y) is differentiable, as the other conditions are satisfied by the linear"
3510,1,['range'], METHOD OF DISTRIBUTION FUNCTIONS MODF,seg_643,"transformation. the constant c can be any nonzero real number. depending on whether |c|   1 or   1, the new range is either expanded or contracted."
3511,1,['distribution'], Distribution of Absolute Value X Using MoDF,seg_645,"the modf can easily be adapted to find the distribution of y = |x|. this is meaningful only when x takes both positive and negative values (if x assumes only negative values, then |x| = −x). as above, let g(y) be the cdf of y. then,"
3512,1,"['symmetric', 'case']", Distribution of Absolute Value X Using MoDF,seg_645,"differentiate both sides to get g(y) = f (y) + f (−y). in the particular case when y is symmetric, f (y) = f (−y) so that g(y) = 2 f (y)."
3513,1,['absolute value'], Distribution of Absolute Value X Using MoDF,seg_645,"example 10.3 absolute value of cuni(− ∕2,  ∕2)"
3514,1,['distribution'], Distribution of Absolute Value X Using MoDF,seg_645,"if x is cuni(− ∕2,  ∕2) variate, find the distribution of y = |x|."
3515,1,['distribution'], Distribution of Absolute Value X Using MoDF,seg_645,"solution 10.3 as the cuni(− ∕2, ∕2) distribution exhibits special symmetry around zero point, f (−x) = f (x). we know f (x) = 1∕ for − ∕2 ≤ x ≤ ∕2. thus, using equation (10.8) the pdf of y = |x| is f (y) = 2 ∗ f (x) = 2∕ for 0 ≤ y ≤ ∕2."
3516,1,"['distribution', 'cauchy', 'absolute value']", Distribution of Absolute Value X Using MoDF,seg_645,example 10.4 distribution of absolute value of cauchy variate
3517,1,"['distribution', 'cauchy', 'standard']", Distribution of Absolute Value X Using MoDF,seg_645,"if x is a standard cauchy variate, find the distribution of y = |x|."
3518,1,"['distribution', 'symmetric']", Distribution of Absolute Value X Using MoDF,seg_645,"solution 10.4 we know that f (x) = 1∕[ (1 + x2)], which is symmetric in x. using equation (10.8), the distribution of y is 2 ∗ f (x) = 2∕[ (1 + x2)] for 0 ≤ x ≤ ∞."
3519,1,"['range', 'continuous random variable', 'distribution', 'random variable', 'variable', 'random', 'continuous']", Distribution of Fx and Fx Using MoDF,seg_647,"let f(x) be the cdf and f−1(x) be the inverse cdf of a continuous random variable. obviously, the minimum value that f(x) can take is 0, and the maximum value is 1. the following section derives the distribution of f(x), irrespective of the range."
3520,1,"['distribution', 'uniformly distributed', 'continuous']", Distribution of Fx and Fx Using MoDF,seg_647,"10.4.2.1 distribution of f(x) if x is a continuous variate, u = f(x) is uniformly distributed in [0, 1]. consider"
3521,1,"['distribution', 'rectangular distribution']", Distribution of Fx and Fx Using MoDF,seg_647,"we have seen in chapter 7 that the cdf of a rectangular distribution is (x − a)∕(b − a). put a = 0, b = 1 to get f(x) = x. equation (10.7) then shows that f(u) has a unit rectangular distribution."
3522,1,"['interval', 'case', 'distribution', 'cases', 'continuous']", Distribution of Fx and Fx Using MoDF,seg_647,"10.4.2.2 distribution of f−1(x) distribution of y = f−1(x) is well tractable in the continuous case when x is defined on unit interval. if we define f−1(x) as the minimum value of y satisfying f(y) ≥ x, we could use the modf to find the distribution in certain cases. obviously, f−1(x) is nondecreasing and satisfy (i) f−1(f(x)) ≤ x, for −∞   x   ∞ and (ii) f(f−1(y)) ≥ y for 0   y   1. let y = f−1(x). if there are no discontinuities,"
3523,1,"['interval', 'range', 'case', 'uniformly distributed']", Distribution of Fx and Fx Using MoDF,seg_647,"where we have used f(f−1(x)) = x (strictly). in the particular case, when y is cuni[0,1], we have f(y) = y. substitute in the last term (in square bracket) of equation (10.8) to get the cdf of f−1(x) as g(y) = f(f(y)) = f(y) = y showing that y is uniformly distributed with unit range. next, consider the general case. as the derivative of f(f(y)) is unambiguously determined when the argument (of outer f()) ranges over the unit interval, we could differentiate both sides of equation (10.8) to get"
3524,1,"['deviation', 'continuous', 'case', 'discrete', 'continuous distributions', 'mean', 'distributions']", Distribution of Fx and Fx Using MoDF,seg_647,"where we have used the function-of-a-function rule of differentiation because inner f(y), being the cdf, satisfies 0 ≤ f(y) ≤ 1. use f(y) − f(y − 1) = f (y) in the discrete case. an application of the above result is to find the mean deviation of continuous distributions given below."
3525,1,"['deviation', 'continuous distribution', 'continuous', 'range', 'distribution', 'mean']", Distribution of Fx and Fx Using MoDF,seg_647,"theorem 10.1 if f (f−1(x)) of a continuous distribution has closed form, and is integrable in the proper range, the mean deviation is given by"
3526,1,['functions'], Distribution of Fx and Fx Using MoDF,seg_647,proof: the proof follows easily by putting y = f−1(x) and using properties of distribution functions in the above result. this is illustrated below.
3527,1,"['deviation', 'exponential distribution', 'distribution', 'exponential', 'mean']", Distribution of Fx and Fx Using MoDF,seg_647,example 10.5 mean deviation of exponential distribution
3528,1,"['deviation', 'mean']", Distribution of Fx and Fx Using MoDF,seg_647,if x is exp( ) find the mean deviation.
3529,1,"['distribution', 'exponential', 'mean', 'exponential distribution']", Distribution of Fx and Fx Using MoDF,seg_647,"solution 10.5 consider the exponential distribution f (x) =  e− x with mean   = 1∕  and cdf 1 − e− x. put x = 1∕  in the cdf to get f( ) = 1 − e− (1∕ ) = 1 − e−1 = (e − 1)∕e. the inverse cdf is f−1(x) = −(log (1 − x))∕ , where log is to the base e (i.e., ln(1 − x)). put the values in equation (10.10) to get the simple integral (e−1)∕e md = 2 tdt∕ (1 − t). (10.11) ∫t=0"
3530,0,[], Distribution of Fx and Fx Using MoDF,seg_647,where we have used
3531,1,"['deviation', 'mean']", Distribution of Fx and Fx Using MoDF,seg_647,write t in the numerator as 1 − (1 − t) and split the integral into two. they evaluate to (1∕e − 1)∕ and 1∕ . adding them together gives the mean deviation as 1/(e ).
3532,1,"['distribution', 'function', 'distribution function']", Distribution of Fx and Fx Using MoDF,seg_647,"example 10.6 inverse of cuni(a, b) distribution function"
3533,1,['distribution'], Distribution of Fx and Fx Using MoDF,seg_647,"if x is cuni(a, b) find the distribution of f−1(x)."
3534,1,['range'], Distribution of Fx and Fx Using MoDF,seg_647,"solution 10.6 we know that the cdf of cuni(a, b) is f(x) = (x − a)∕(b − a). using equation (10.9), the pdf of y = f−1(x) is f ((y − a)∕(b − a)) ∗ f (y). as f (y) = 1∕(b − a) irrespective of the value of y, and f ((y − a)∕(b − a)) has unit range, we get the pdf of y as g(y) = 1∕(b − a), a ≤ y ≤ b."
3535,1,"['deviation', 'uniform distribution', 'distribution', 'mean']", Distribution of Fx and Fx Using MoDF,seg_647,example 10.7 mean deviation of uniform distribution
3536,1,"['deviation', 'mean']", Distribution of Fx and Fx Using MoDF,seg_647,"if x is cuni(a, b), find the mean deviation."
3537,1,"['range', 'symmetry', 'mean']", Distribution of Fx and Fx Using MoDF,seg_647,"solution 10.7 as the cdf of cuni(a, b) is f(x) = (x − a)∕(b − a),f( ) = f((a + b)∕2) = 1∕2 (this also follows trivially from the fact that cuni(a,b) has special symmetry so that the area up to the mean is 1/2). as the density is constant throughout the range, f (f−1(t)) = 1∕(b − a) always. substitute these values in equation (10.10) to get the md as"
3538,0,[], Distribution of Fx and Fx Using MoDF,seg_647,this tallies with the result given in page 264.
3539,1,"['functions', 'random variables', 'method', 'continuous', 'variables', 'change of variable', 'variable', 'random', 'transformation', 'average', 'distributions']", CHANGE OF VARIABLE TECHNIQUE,seg_649,the change of variable technique (cov-t) (also called transformation of variable technique (tov-t)) is a useful method to find distributions of simple continuous differentiable functions of real-valued random variables. it works on the principle that the average value of an integral favg(x) = 1∕(b − a) ∫a
3540,1,"['continuous', 'transformation']", CHANGE OF VARIABLE TECHNIQUE,seg_649,an arbitrary continuous transformation y = h(x) by the integral gavg(y) = 1∕(h(b) −
3541,1,"['jacobian', 'method', 'interval', 'function', 'transformation']", CHANGE OF VARIABLE TECHNIQUE,seg_649,"now consider limb→a[(h(b) − h(a))∕(b − a)], which is the limiting value of the derivative h(x) at x = a. if this derivative of h′ (x) exists for each point x in the interval (a, b), then the rhs will be finite. when b → a from above, the lhs integral approaches f (x) and rhs integral approaches g(y)∕h′ (y). this allows us to equate the width of an infinitesimal strip under the function f (x) as f (x)dx = g(y)dy for all points x in (a, b). because y = h(x) is invertible, we get g(y) = f (x)| h(y)∕ y| = |1∕j|f (h−1(y)), where j is called the jacobian of the transformation. the only conditions in this transformation are that the mapping is once differentiable (i.e., h′ (x) exists) and it is invertible (x can be expressed in terms of y). this can also be proved using the cdf method as follows."
3542,1,"['transformation', 'cases']", CHANGE OF VARIABLE TECHNIQUE,seg_649,theorem 10.2 the cdf of one-variable transformation is given by g(y) = f(u−1(y)) if u(x) is strictly increasing and 1 − f(u−1(y)) if u(x) is strictly decreasing. the pdf in both cases is g(y) = f (u−1(y))|  
3543,1,['function'], CHANGE OF VARIABLE TECHNIQUE,seg_649,"proof: if u(x) is a strictly increasing function and invertible,"
3544,1,"['transformation', 'results']", CHANGE OF VARIABLE TECHNIQUE,seg_649,"as the derivative is positive, the transformation results in g(y) y = f (x) x, so that g(y) = f (u−1(y))|  "
3545,1,['function'], CHANGE OF VARIABLE TECHNIQUE,seg_649,"x y |. if u(x) is a strictly decreasing function, the derivative is negative"
3546,1,"['transformation', 'results']", CHANGE OF VARIABLE TECHNIQUE,seg_649,so that the transformation results in g(y) = 1 − f(u−1(y)) and g(y) = −f (u−1(y))|  
3547,1,"['jacobian', 'results', 'absolute value']", CHANGE OF VARIABLE TECHNIQUE,seg_649,"x y |. this is the reason why we take the absolute value of the jacobian. these results can easily be generalized to n-dimensions, as shown in chapter 11."
3548,1,"['jacobian', 'deviation', 'standard normal distribution', 'standard normal', 'results', 'case', 'standardization', 'distribution', 'normal', 'mean', 'standard', 'transformation', 'standard deviation', 'normal distribution']", CHANGE OF VARIABLE TECHNIQUE,seg_649,"the standardization transformation z = (x −  )∕ , where   is the mean and   is the standard deviation, is the simplest and the most frequent covt. the jacobian in this case is | z∕ x| = 1∕ . when applied to an arbitrary normal distribution, this results in a standard normal distribution, which is extensively tabulated."
3549,1,"['transformed', 'random variable', 'variable', 'random']", Linear Transformations,seg_651,"any random variable x can be transformed linearly using y = cx + b, where c ≠ 0. as they are linearly related, we could directly invert it to get x = (y − b)∕c and |j| = 1∕|c|. let gy (y) denote the pdf of y, and fx(x) denote the pdf of x. then,"
3550,1,"['continuous', 'discrete']", Linear Transformations,seg_651,"which is the same result obtained by the modf technique. when the variate is discrete, we simply ignore the 1∕|c| multiplier. in general, if the transformation is y = h(x), the pdf of y is g(y) = f (h−1(y)) if x is discrete and g(y) = f (h−1(y))| h−1(y)∕ y| if x is continuous."
3551,1,"['functions', 'linear', 'distribution', 'binomial', 'binomial distribution']", Linear Transformations,seg_651,example 10.8 linear functions of binomial distribution
3552,1,['distributions'], Linear Transformations,seg_651,"if x ∼ bino(n, p) find distributions of y = n − x, and find e(y),v(y)."
3553,0,['n'], Linear Transformations,seg_651,"solution 10.8 as y = n − x takes integer values in reverse, it has the same dis-"
3554,0,[], Linear Transformations,seg_651,"y), this can also"
3555,0,[], Linear Transformations,seg_651,"y)qypn−y, y = 0, 1, .., n; which is the pmf of a bino(n, q)."
3556,1,"['confidence intervals', 'case', 'dependent', 'variates', 'sum of squares', 'random', 'multiple linear regression', 'significance', 'coefficients', 'sample', 'linear', 'intervals', 'cases', 'sample variance', 'confidence', 'statistical', 'distributions', 'functions', 'statistical inference', 'regression', 'distribution', 'linear regression', 'analysis of variance', 'variance', 'anova', 'f distribution', 'random variables', 'independent', 'variables', 'treatment', 'regression coefficients', 'normality', 'normal', 'variances']", DISTRIBUTION OF SQUARES,seg_653,"distribution of squares is especially important in statistical inference and analysis of variance. this is because we encounter sums of squares or functions thereof. for instance, the anova procedure is dependent on decomposing the total sum of squares as between treatment and within sums of squares. similarly, confidence intervals for variances are constructed using the distribution of sample variance, and testing of regression coefficients in multiple linear regression (mlr) uses the ratio of sums of squares. all of these require the distribution of appropriate sums of squares under normality assumption. in such cases we need to find the distribution of sums of independent normal random variates. although the distribution of squares of other random variables is seldom used in practice, they do have great theoretical significance. a special case of the above is the relation between student’s t and snedecor’s f distributions. if t ∼ tn, then the pdf of tn2 has an f distribution with 1"
3557,1,"['distribution', 'f distribution', 'independent']", DISTRIBUTION OF SQUARES,seg_653," 2n∕n so that tn2 = z2∕( 2n∕n). as the numerator and denominator are independent, both of them are chi-squared distributed so that their ratio has an f distribution."
3558,1,['distribution'], DISTRIBUTION OF SQUARES,seg_653,"there exist many methods to derive the distribution of squares. let y = x2 so that dy∕dx = 2x and dx∕dy = 1∕(2√y). if x takes positive values, the pdf of y is obtained as"
3559,1,"['distribution', 'functions']", DISTRIBUTION OF SQUARES,seg_653,the corresponding relationship between distribution functions for strictly increasing functions is
3560,1,['table'], DISTRIBUTION OF SQUARES,seg_653,"as (10.17) is valid for any x(−∞   x  ∞), we differentiate it with respect to y to get the pdf as g(y) = (f (√y) + f (−√y))∕2√y (see table 10.1)."
3561,1,['distribution'], DISTRIBUTION OF SQUARES,seg_653,example 10.9 distribution of the square of a t variate
3562,1,['distribution'], DISTRIBUTION OF SQUARES,seg_653,"if x is t(n), find the distribution of x2∕n."
3563,1,"['distribution', 't distribution']", DISTRIBUTION OF SQUARES,seg_653,solution 10.9 the pdf of student’s t distribution is given in section 7.18 (p. 7–85) as f (t) = k(1 + t2∕n)−(n+1)∕2. (10.18)
3564,0,[], DISTRIBUTION OF SQUARES,seg_653,differentiate (10.19) with respect to y to get the pdf of y as
3565,1,"['distribution', 'symmetric', 't distribution']", DISTRIBUTION OF SQUARES,seg_653,"as the t distribution is symmetric, f (√y) = f (−√y). substitute in"
3566,0,[], DISTRIBUTION OF SQUARES,seg_653,"equation (10.18), and cancel out √n to get the desired pdf as"
3567,1,['distribution'], DISTRIBUTION OF SQUARES,seg_653,write √y in the denominator as y1∕2−1 and take numerator expression to the denominator. then this is found to be a beta-ii distribution
3568,1,"['random variables', 'variables', 'random', 'continuous', 'transformation']", DISTRIBUTION OF SQUAREROOTS,seg_655,"as the square root of a negative number is imaginary, this transformation is defined only for random variables that take positive values. it makes sense for continuous"
3569,1,['discrete'], DISTRIBUTION OF SQUAREROOTS,seg_655,variates than discrete ones. let y = √
3570,0,[], DISTRIBUTION OF SQUAREROOTS,seg_655,"x, which gives x = y2 and dx∕dy = 2y. the straightforward way to find the pdf of y is to use"
3571,1,"['distribution', 'symmetric']", DISTRIBUTION OF SQUAREROOTS,seg_655,"if the resulting distribution of y is symmetric, we need to divide the final density by 2 to get the correct pdf, because both (−y)2 and (+y)2 map to x. symbolically, g(y) = yf (y2) if y is symmetric. this is summarized as"
3572,1,"['symmetric', 'asymmetric']", DISTRIBUTION OF SQUAREROOTS,seg_655,2yf (y2) if y is asymmetric; g(y) = f (x)| x∕ y| = { yf (y2) if y is symmetric.
3573,1,['distribution'], DISTRIBUTION OF SQUAREROOTS,seg_655,example 10.10 distribution of the square-root of a  2 variate
3574,1,['distribution'], DISTRIBUTION OF SQUAREROOTS,seg_655,"if x is  n2, find the distribution of √ x."
3575,1,"['distribution', 'rayleigh distribution', 'standard']", DISTRIBUTION OF SQUAREROOTS,seg_655,"as y ∗ (y2)n∕2−1 = yn−1 the pdf becomes f (y) = e−y2∕2yn−1∕[2n∕2−1γ(n∕2)]. this is the chi-distribution, or the standard form of rayleigh distribution (p. 7–107)."
3576,1,['distribution'], DISTRIBUTION OF SQUAREROOTS,seg_655,example 10.11 distribution of the square-root of an f variate
3577,1,['distribution'], DISTRIBUTION OF SQUAREROOTS,seg_655,"if x is f(1, n), find the distribution of √ x."
3578,1,"['distribution', 'f distribution']", DISTRIBUTION OF SQUAREROOTS,seg_655,solution 10.11 the pdf of f distribution was given in chapter 7 (p. 316) as
3579,1,['symmetric'], DISTRIBUTION OF SQUAREROOTS,seg_655,"the y cancels out with (y2)1∕2−1 = 1∕y. take n outside from the bracket in the denominator and cancel out with nn∕2 in the numerator to get a √n in the denominator. as the pdf now involves only powers of y2, it is symmetric. hence, we need to divide the resulting pdf by 2 to get the correct pdf as"
3580,1,['distribution'], DISTRIBUTION OF SQUAREROOTS,seg_655,which is the student’s t(n) distribution.
3581,1,"['distribution', 'random variable', 'variable', 'cases', 'random']", DISTRIBUTION OF RECIPROCALS,seg_657,"distribution of reciprocals is defined only in some particular cases. if the value of a random variable x at x = 0 is nonzero, the random variable y = 1∕x is well defined. it can also be used (along with section 10.5.1 discussed in p. 404) to find the distribution of (x − 1)∕x = 1 − 1∕x and (1 − x)∕x = 1∕x − 1. the straightforward way to find the pdf of y is to use"
3582,1,"['distribution', 'cauchy']", DISTRIBUTION OF RECIPROCALS,seg_657,example 10.12 distribution of the reciprocal of a cauchy variate
3583,1,"['distribution', 'cauchy', 'cauchy distributed']", DISTRIBUTION OF RECIPROCALS,seg_657,"if x is cauchy distributed, find the distribution of y = 1∕x."
3584,1,"['distribution', 'cauchy']", DISTRIBUTION OF RECIPROCALS,seg_657,"1 1 solution 10.12 we have seen in chapter 7 that f (x) = , −∞   x     1 + x2 ∞. as f (x = 0) is 1∕ , distribution of the reciprocal is well-defined. using 1 1 equation (10.29) the pdf becomes f (y) =   1 + (1∕y)2 ∕y2. the y2 cancels out 1 1 from the numerator and denominator, giving f (y) = , which is cauchy   1 + y2 distributed."
3585,0,[], DISTRIBUTION OF RECIPROCALS,seg_657,example 10.13 reciprocal of a unit rectangular variate
3586,1,['distribution'], DISTRIBUTION OF RECIPROCALS,seg_657,"if x is u(0, 1) distributed, find the distribution of y = 1∕x."
3587,1,"['sample', 'model', 'random sample', 'statistics', 'distribution', 'order statistics', 'random']", DISTRIBUTION OF MINIMUM AND MAXIMUM,seg_659,"the distribution of minimum and maximum (called extremes) finds applications in many fields. for example, distribution of maximum is used in life sciences to model the survival time of species, produce, machines, and various products. it is also used in reliability theory to model the life of equipments and parts, various devices, and consumer items (such as light bulbs and computer chips). the study of extremes is called order statistics. let x1,x2, … ,xn be a random sample from an arbitrary distribution with pdf f (y) and cdf f(y). let y1 = min(x1,x2, … ,xn). then,"
3588,1,['independence'], DISTRIBUTION OF MINIMUM AND MAXIMUM,seg_659,using independence. differentiate the above to get the pdf of y1 as
3589,1,['method'], DISTRIBUTION OF MINIMUM AND MAXIMUM,seg_659,"similarly, the pdf of yn using cdf method is f (yn) = nf (y)[f(y)]n−1."
3590,1,"['functions', 'normalized', 'random variables', 'random', 'variables', 'cauchy', 'cauchy distribution', 'correlation', 'distribution', 'correlation coefficient', 'standard', 'coefficient', 'geometric', 'distributions']", DISTRIBUTION OF TRIGONOMETRIC FUNCTIONS,seg_661,"trigonometric functions of some random variables are easy to work with. one example is the cauchy distribution. if x has a standard cauchy distribution, then cos(x) has the same distribution. trigonometric functions are also utilized to derive some distributions using geometric concepts. one example is the correlation coefficient. the cosine of the angle between two normalized vectors in n-dimensional euclidean space is called the correlation coefficient."
3591,1,['distribution'], DISTRIBUTION OF TRIGONOMETRIC FUNCTIONS,seg_661,example 10.14 distribution of u = tan(x)
3592,1,['distribution'], DISTRIBUTION OF TRIGONOMETRIC FUNCTIONS,seg_661,"if x has a u(0, 1) distribution, find the distribution of u = tan(x)."
3593,1,"['range', 'distribution', 'transformation']", DISTRIBUTION OF TRIGONOMETRIC FUNCTIONS,seg_661,"solution 10.14 the pdf of x is f (x) = 1, 0   x   1. the inverse transformation is x = tan−1(u). this gives | x∕ u| = 1∕(1 + u2). the range of u is modified as tan(0) = 0 to tan(1) =  ∕4. hence, the distribution of u is f (u) = 1∕(1 + u2) for 0   u    ∕4."
3594,1,['distribution'], DISTRIBUTION OF TRIGONOMETRIC FUNCTIONS,seg_661,example 10.15 distribution of u = sin(x)
3595,1,['distribution'], DISTRIBUTION OF TRIGONOMETRIC FUNCTIONS,seg_661,"if x has a cuni[− ∕2,  ∕2] distribution, find the distribution of u = sin(x)."
3596,1,['transformation'], DISTRIBUTION OF TRIGONOMETRIC FUNCTIONS,seg_661,solution 10.15 the inverse transformation is x = sin−1(u) so that | x∕ u| =
3597,1,"['functions', 'dependent variable', 'independent', 'variables', 'dependent', 'distribution', 'variable', 'function']", DISTRIBUTION OF TRANSCENDENTAL FUNCTIONS,seg_663,"distributions of transcendental functions are quite useful in engineering and related fields. several laws and principles in engineering and physical sciences are modeled as mathematical equations called functionals involving the unknown variables and known constants. in most applications, one variable (called dependent variable) is modeled as a function of two or more other variables (called independent variables), which can include time. if the number of variables involved is two, the covt technique is useful to derive the distribution of one, using the distribution of the other. for"
3598,1,['data'], DISTRIBUTION OF TRANSCENDENTAL FUNCTIONS,seg_663,"example, in data compression and telecommunications, the speech amplitude is mod-"
3599,1,"['lognormal', 'lognormal distribution', 'distribution', 'set']", DISTRIBUTION OF TRANSCENDENTAL FUNCTIONS,seg_663,"2) exp(−√ 2|x|∕ ), and compressed using the −law as a ∗ sign(x)[ln(1 + |x|)∕ ln(1 + )], where “a” is the peak input magnitude and is the compression constant (typically set to high, say 255). similarly, in wireless communication of fading channels, if x is n( , 2) distributed, y = ex has a lognormal distribution. this has applications in mining engineering, reliability, and spectroscopy among many other fields (chapter 7, p. 297)."
3600,1,"['distribution', 'transformation']", DISTRIBUTION OF TRANSCENDENTAL FUNCTIONS,seg_663,example 10.16 logarithmic transformation of cuni distribution
3601,1,['distribution'], DISTRIBUTION OF TRANSCENDENTAL FUNCTIONS,seg_663,"if x is cuni[a, b] distributed (chapter 7, p. 261), find the distribution of y = − log (x − a)∕(b − a)."
3602,1,"['distribution', 'transformation']", DISTRIBUTION OF TRANSCENDENTAL FUNCTIONS,seg_663,example 10.17 transformation of arc-sine distribution
3603,1,['distribution'], DISTRIBUTION OF TRANSCENDENTAL FUNCTIONS,seg_663,"if x is arc-sine distributed (chapter 7, p. 279), find the distribution of y = − log (x)."
3604,1,"['random variables', 'independent', 'variables', 'distribution', 'independent random variables', 'convolution', 'random', 'continuous']", Distribution of Sums,seg_665,"if x and y are independent random variables, we may need to find the distribution of u = x + y . this is easy to find using convolution of integrals when both x and y are continuous. let fu(u) be the cdf of u. then,"
3605,1,"['independence', 'adjusted']", Distribution of Sums,seg_665,"due to the assumption of independence. as x + y ≤ u represents the region below a straight line, the limits can be adjusted as ∫x"
3606,0,[], Distribution of Sums,seg_665,above then becomes ∫x
3607,1,['symmetry'], Distribution of Sums,seg_665,"∞ =−∞ gy (u − x)f (x)dx. owing to the symmetry of x and y,"
3608,0,[], Distribution of Sums,seg_665,we can rearrange the integrals to get a similar expression as ∫x
3609,1,"['random variables', 'independent', 'method', 'variables', 'distribution', 'independent random variables', 'convolution', 'random']", Distribution of Sums,seg_665,this is called the convolution of x and y. the mgf is related as mx+y (t) = mx(t) ∗ my (t) and the cgf is related as kx+y (t) = kx(t) + ky (t) if x and y are independent. this method can easily be extended to find the distribution of the difference of independent random variables.
3610,1,"['poisson', 'variates']", Distribution of Sums,seg_665,example 10.18 sum of poisson variates
3611,1,"['poisson', 'parameters']", Distribution of Sums,seg_665,"if x and y are poisson with parameters  1 and  2, find the pmf of x + y ."
3612,1,"['variable', 'joint', 'change of variable']", Distribution of Sums,seg_665,"solution 10.18 we could easily get the required pdf using the mgf technique. however, we proceed as follows to clarify the change of variable technique. the joint pmf of x and y is f (x, y) = e− 1 1"
3613,1,['range'], Distribution of Sums,seg_665," )!e− 2 2  ∕ !, where v = 0,1,2.. and u =  ,   + 1,... the pmf of u is obtained by summing over the entire range of v. as u has v as the lower bound, we need sum over v from 0 to u. hence fu(u) = e−( 1+ 2)∑ "
3614,1,['summation'], Distribution of Sums,seg_665,and divide by u! and take it outside the summation in the denominator. this gives fu(u) =
3615,1,"['exponential', 'variates']", Distribution of Sums,seg_665,example 10.19 sum of exponential variates
3616,1,"['independent', 'variates', 'distribution', 'shape parameter', 'exponential', 'parameter']", Distribution of Sums,seg_665,"if x and y are independent exponential variates with the same shape parameter, find the distribution of u = x + y ."
3617,1,['independent'], Distribution of Sums,seg_665,"solution 10.19 we could use equation (10.35) to get the pdf. it is much easier to use the mgf. we know that the mgf of x is (1 − t∕ )−1. as x and y are independent, mx+y (t) = mx(t) ∗ my (t) = (1 − t∕ )−2, which is the mgf of gamma(2, )."
3618,1,"['joint', 'table']", Distribution of Sums,seg_665,the joint pmf of x and y is given in table 10.2. find the pmf of u = x + y .
3619,1,"['range', 'results', 'tables']", Distribution of Sums,seg_665,"solution 10.20 clearly, x + y takes values in the range [2,5]. p[x + y = 2] = p[x = 1,y = 1] = 1∕12. p[x + y = 3] = p[x = 1,y = 2] + p[x = 2,y = 1] = 2∕12 + 3∕12 = 5∕12. similarly, p[x + y = 4] = p[x = 1,y = 3] + p[x = 2, y = 2] + p[x = 3,y = 1] = 2∕12 + 3∕12 = 5∕12, and so on. the results are given in tables 10.2 and 10.3."
3620,1,"['case', 'continuous', 'transformation']", Distribution of Arbitrary Functions,seg_667,"consider the transformation y = g(x), where g(x) is a one–one mapping that is invertible. this means that x can be expressed in terms of y as x = g−1(y). then, the pdf of y can be represented in the continuous case as follows. first express the cdf of y in terms of the cdf of x as"
3621,0,[], Distribution of Arbitrary Functions,seg_667,differentiate both sides to get the pdf as
3622,1,"['case', 'discrete']", Distribution of Arbitrary Functions,seg_667,and in the discrete case y = r(x) as
3623,1,"['distribution', 'integer part']", Distribution of Arbitrary Functions,seg_667,example 10.21 distribution of integer part
3624,1,"['distribution', 'cauchy', 'integer part', 'cauchy distribution']", Distribution of Arbitrary Functions,seg_667,"if x has cauchy distribution, find the distribution of the integer part y = ⌊x⌋."
3625,1,"['random variable', 'variable', 'random']", Distribution of Arbitrary Functions,seg_667,"solution 10.21 we have f (x) = 1∕[ (1 + x2)] for −∞   x   ∞. the random variable y takes integer values on the entire real line including y = ∓∞ (see discussion in section 7.2.1, p. 260). specifically,"
3626,1,['probabilities'], Distribution of Arbitrary Functions,seg_667,"a similar expression could be obtained for x 0 as pr[y = y] = pr[y − 1 ≤ x ≤ y]. as alternate terms in above equation (10.40) cancel out, this form is useful to compute the cdf, and to prove that the probabilities add up to 1. for example, sum from −∞ to ∞ to get tan(∞) − tan(−∞) = ∕2 − (− ∕2) = , which cancels with in the denominator. now use the identity tan−1(x) − tan−1(y) = tan−1 ((x − y)∕(1 + xy)) to get"
3627,0,[], Distribution of Arbitrary Functions,seg_667,"which is the desired form. as the terms do not cancel, this form is not useful to compute the cdf."
3628,1,"['functions', 'range', 'unimodal', 'discrete', 'discrete distributions', 'distribution', 'tails', 'distributions']", Distribution of Arbitrary Functions,seg_667,"this example has an enormous use. it shows another way to specify a pmf as the difference of two functions (say [tan−1(x + 1) − tan−1(x)] or [exp(− x) − exp(− (x + 1))]) that simply cancels out when summed over the proper range of x, leaving behind only two extreme terms whose difference appears as the normalizing constant in the denominator. this allows us to define a variety of new discrete distributions. as the pmf should be positive, we should form the difference separately for positive and negative values, to make it non-negative. the shape of the distribution (whether it is unimodal and tails off to the extremes, or it is u-shaped or j-shaped, etc.) must be known to form the pmf."
3629,1,['distribution'], Distribution of Arbitrary Functions,seg_667,example 10.22 distribution of fractional part
3630,1,"['distribution', 'exponential', 'exponential distribution']", Distribution of Arbitrary Functions,seg_667,"if x has an exponential distribution, find the distribution of the fractional part y = x − ⌊x⌋."
3631,1,['distribution'], Distribution of Arbitrary Functions,seg_667,"solution 10.22 it was shown in chapter 6 (p. 218) that if x has an exponential distribution, the distribution of y = ⌊x⌋ is geo(1 − exp(− )). the possible values of y = x − ⌊x⌋ are 0 ≤ y ≤ 1. if we assume that the integer and fractional"
3632,1,"['continuous distribution', 'random variables', 'independent', 'variables', 'geometric distribution', 'geometric random variables', 'discrete', 'distribution', 'exponential', 'random', 'continuous', 'geometric']", Distribution of Arbitrary Functions,seg_667,"parts are independent, y is the difference between an exponential and geometric random variables. this is of mixed type (as geometric distribution is discrete), where the continuous distribution dominates. this means that y has a continuous distribution. using the modf it is easy to show that y is distributed as f (y) = exp(− y)∕[1 − exp(− )], for 0 ≤ y ≤ 1."
3633,1,"['continuous random variables', 'random variables', 'method', 'continuous', 'variables', 'range', 'random variable', 'variable', 'random', 'function', 'transformation']", Distribution of Logarithms,seg_669,"the logarithmic transformation can be applied to any random variable that takes non-negative values. as log(0) = −∞, this transforms (0,∞) to the new range (−∞,∞). as the log() is a real function of its argument, this transformation is applied to continuous random variables. unless otherwise stated, the base of the logarithm is assumed to be e. a special transformation encountered in communication theory is y = log2(1 + x). using the method described above, the pdf of y is given by gy (y) = ln(2) f (2y − 1)2y."
3634,1,"['functions', 'variables', 'symmetric', 'skew']", Special Functions,seg_671,"there exist many symmetric and skew symmetric functions that possess interesting properties. these can be in single or multiple variables. examples are x∕(1 − x), (1 +"
3635,0,[], Special Functions,seg_671,1 + x2. square both sides to get y2 =
3636,1,"['cauchy', 'cauchy distributed', 'standard']", Special Functions,seg_671,"if x is standard cauchy distributed, then y = x∕√"
3637,1,['distribution'], Special Functions,seg_671,example 10.23 distribution of ratio of sums
3638,1,"['distribution', 'independent']", Special Functions,seg_671,"let xi’s be iid exp(  ) for i = 1, 2, ..,m. let yj’s be iid exp( ) for j = 1, 2, .., n. if xi’s and yj’s are pair-wise independent, find the distribution of the ratio w ="
3639,1,"['independent', 'gamma', 'variates', 'distribution', 'gamma distributions', 'joint', 'distributions']", Special Functions,seg_671,"solution 10.23 as xi’s are iid, the joint pdf is the product of individual pdfs. we first use the mgf technique to find the distribution of numerator and denominator. the mgf of exp( ) is mx(t) = 1∕[1 − t]. as the xi’s are iid, mu(t) = 1∕[1 − t]m. similarly, m (t) = 1∕[1 − t]n. these are the mgfs of gamma distributions. hence, w is the ratio of two independent gamma variates. see references 134, 138, 285 and 294 for further properties."
3640,1,"['normal', 'transformations']", TRANSFORMATIONS OF NORMAL VARIATES,seg_673,"most of the transformations discussed above are applicable to the normal variate. as this has many practical applications, this section briefly discusses some of them."
3641,1,"['independent', 'method', 'combination', 'variates', 'normally distributed', 'normal']", Linear Combination of Normal Variates,seg_675,"linear combination of any number of independent normal variates is normally distributed. this can be proved using induction. a simpler method is to use the mgf technique. we saw in chapter 9 that the chf of n( ,  2) is exp(it  − 1"
3642,1,"['random variables', 'independent', 'variables', 'normal', 'random']", Linear Combination of Normal Variates,seg_675,"2 t2 2). if x1,x2, … ,xn are independent normal random variables n( i,  i"
3643,1,"['normal', 'mean', 'variance']", Linear Combination of Normal Variates,seg_675,as this is the chf of a normal variate with mean ∑i i and variance ∑i i
3644,1,"['case', 'normally distributed']", Linear Combination of Normal Variates,seg_675,"2, it follows ′ that y is normally distributed. in the particular case, when each of the  i s and  i"
3645,1,"['normal', 'mean', 'variance']", Linear Combination of Normal Variates,seg_675,"this shows that ∑iciyi is normal with mean   = ∑ici i, and variance  2 = ∑ici"
3646,1,"['normal', 'mean', 'variance']", Linear Combination of Normal Variates,seg_675,′2 corollary 1 cx + b is normal with mean  ′ = c ∗   + b and variance   = c2 2.
3647,1,['normal'], Square of Normal Variates,seg_677,the square of a normal variate is  2
3648,1,"['independent', 'cauchy', 'cauchy distributed', 'variates', 'sum of squares', 'normal']", Square of Normal Variates,seg_677,"1 distributed. in general, the sum of squares of any number of independent normal variates is  2n distributed. similarly, the ratio of independent normal variates is cauchy distributed. this is proved in chapter 11."
3649,1,"['distribution', 'normal']", Square of Normal Variates,seg_677,example 10.24 distribution of the square of a normal variate
3650,1,['distribution'], Square of Normal Variates,seg_677,"if x is n(0, 1), find the distribution of x2."
3651,0,[], Square of Normal Variates,seg_677,differentiate with respect to y to get
3652,1,['leibnitz theorem'], Square of Normal Variates,seg_677,using leibnitz theorem this reduces to (1∕√ 2 )(e−y∕2 ∗ 1∕(2√y) − e−y∕2 ∗
3653,0,[], Square of Normal Variates,seg_677,(−1)∕(2√y)) = (1∕√ 2 )y1∕2−1e−y∕2. this is the pdf of a chi-square variate
3654,0,[], Square of Normal Variates,seg_677,"with 1 dof. alternatively, write (10.46) as  "
3655,1,"['probability', 'random', 'symmetric', 'results', 'independent random variables', 'mean', 'distributions', 'continuous random variables', 'transformations', 'distribution', 'continuous', 'deviation', 'random variables', 'independent', 'variables', 'treatment', 'probability distribution', 'continuous distributions']", SUMMARY,seg_679,"this chapter derives and explains the formulas for the probability distribution of a sum, difference, product, and ratio of two independent random variables. the distribution of squares, square-roots, of univariate and other transformations of two or more random variables are derived and illustrated. distribution of integer and fractional parts of some continuous random variables are discussed, as also the distribution of cdf (f(x)) and its inverse (f−1(x)). these known results are used to derive an expression for the mean deviation of some continuous distributions as twice the simple integral of t∕f (f−1(t)) from zero to f( ), where is the mean and f(x) is the cdf (f( ) = 1∕2 for symmetric laws). several examples are included to understand the need and usefulness of the transformations. advanced treatment can be found in references 134, 138, 225 and 295 and engineering applications in references 285 and 291."
3656,1,"['discrete', 'continuous distributions', 'continuous', 'distributions']", SUMMARY,seg_679,a) the covt is applicable to both discrete and continuous distributions.
3657,1,"['range', 'random variable', 'variable', 'random', 'transformation']", SUMMARY,seg_679,"b) if the range of a random variable x includes the origin, we cannot use the transformation y = 1∕x"
3658,1,"['discrete', 'variates']", SUMMARY,seg_679,c) a translation y = x ± c is not applicable to discrete variates
3659,1,"['distribution', 'normal']", SUMMARY,seg_679,d) the distribution of the square of a normal variate is student’s t
3660,1,"['cauchy', 'cauchy distributed']", SUMMARY,seg_679,e) the reciprocal of a cauchy variate is cauchy distributed.
3661,1,['distribution'], SUMMARY,seg_679,"tion of (i) y = c ∗ x + d, (ii) distribution of y = √x."
3662,0,[], SUMMARY,seg_679,"y = |x −  |. 10.5 if x is bino(n, p) find distri-"
3663,1,['exponentially'], SUMMARY,seg_679,"10.6 if x is geom(p), find the dis10.17 if x and y are exponentially dis"
3664,1,"['distribution', 'independent']", SUMMARY,seg_679,"n =1 xi, where tributed, find the distribution of each xi is independent. x + y and x − y ."
3665,1,['gamma'], SUMMARY,seg_679,10.7 if f and g are two cdf’s sym10.18 if x is distributed as gamma
3666,1,"['distribution', 'second moment', 'moment']", SUMMARY,seg_679,"metric around zero, with unit ( , 1), find the distribution of y = second moment, prove that h = log (x∕ )."
3667,1,['distribution'], SUMMARY,seg_679,"distributed. of y = − log (x). if a = −b, find 10.8 if x is duni(k) with f (x) = the distribution of y = x2."
3668,1,"['distribution', 'exponential', 'standard']", SUMMARY,seg_679,"− ln (x) is standard exponential. show that the variate y = what is the distribution when the ln (x∕(1 − x)) is generalized logarithm is not to the base e. logistic(p, q)."
3669,1,"['monotonic', 'distribution', 'monotonic function', 'function']", SUMMARY,seg_679,"10.11 if h(x) is a monotonic function, 10.22 find the distribution of abso-"
3670,1,"['normal', 'mean']", SUMMARY,seg_679,"prove that the cdf of y = h(x) lute value of a general normal can be expressed as fy (y) = n( ,  2), and its mean. fx(h−1(y)) if x is increasing,"
3671,0,[], SUMMARY,seg_679,"and fy (y) = 1 − fx(h−1(y)) 10.23 if x ∼ weibull(a, b), find the disotherwise. tribution of y = log (x∕b)."
3672,1,"['weibull', 'standard']", SUMMARY,seg_679,bution of u = c ∗ tan(1∕x) 10.25 if x ∼ standard weibull distribu-
3673,1,['distribution'], SUMMARY,seg_679,"tion with pdf f (x) = bxb−1e−xb 10.14 if x1,x2 are iid cuni(0, 1), find the distribution of y = x(b)."
3674,1,['distribution'], SUMMARY,seg_679,find the distribution of y1 =
3675,1,['transformation'], SUMMARY,seg_679,"√−2loge(x1) cos(2 x2) and y2 = 10.26 if u is cuni(0, 1) find the dis√−2loge(x1) sin(2 x2) (the intribution of (i) |u − 1 2 |, (ii) 1/ verse transformation being x1 = (1 + u)."
3676,1,"['distribution', 'variance', 'mean']", SUMMARY,seg_679,"1 10.27 if f (x) = (c − 1)∕(1 + x)c for 0 arctan(y2∕y1)). x ∞, find the distribution of y = 1∕(1 + x)c and find the mean 10.15 prove that the sum of indepenand variance."
3677,1,"['exponential', 'random']", SUMMARY,seg_679,dent exponential random vari-
3678,1,"['distribution', 'gamma distribution', 'gamma']", SUMMARY,seg_679,ables has a gamma distribution. 10.28 if y = g(x) is an arbitrary func-
3679,1,"['conditional', 'discrete', 'distribution', 'random', 'conditional distribution']", SUMMARY,seg_679,tion of a discrete random vari10.16 if conditional distribution of x
3680,1,"['variance', 'mean']", SUMMARY,seg_679,"able, prove that the pmf of y is is bino(n, p), where p is disf (y) = ∑x∈g−1(y)f (x). tributed as beta(p, q), find the mean and variance of x."
3681,0,[], JOINT DISTRIBUTIONS,seg_681,"after finishing the chapter, students will be able to"
3682,1,"['joint and conditional distributions', 'conditional', 'conditional distributions', 'joint', 'distributions']", JOINT DISTRIBUTIONS,seg_681,◾ distinguish joint and conditional distributions
3683,1,"['functions', 'distribution', 'random variable', 'variable', 'random']", JOINT DISTRIBUTIONS,seg_681,◾ find distribution of functions of a random variable
3684,1,"['linear', 'random variables', 'variables', 'transformations', 'linear transformations', 'random']", JOINT DISTRIBUTIONS,seg_681,◾ understand linear transformations of random variables
3685,1,"['jacobian', 'transformations']", JOINT DISTRIBUTIONS,seg_681,◾ comprehend jacobian of transformations
3686,1,"['arbitrary transformations', 'transformations']", JOINT DISTRIBUTIONS,seg_681,◾ describe arbitrary transformations
3687,1,"['polar transformations', 'transformations']", JOINT DISTRIBUTIONS,seg_681,◾ apply polar transformations
3688,0,[], JOINT DISTRIBUTIONS,seg_681,◾ utilize “do-little” technique to quickly find jacobians
3689,1,"['random variables', 'independent', 'variables', 'dependent', 'distribution', 'joint', 'random']", JOINT AND CONDITIONAL DISTRIBUTIONS,seg_683,definition 11.1 joint distribution is the distribution of two or more (dependent or independent) random variables.
3690,1,"['variables', 'discrete', 'continuous']", JOINT AND CONDITIONAL DISTRIBUTIONS,seg_683,"usually, the variables involved are either all discrete or all continuous. symbolically, it is represented as p(x, y) = p(x = x and y = y) (such that ∑(x,y)∈ap(x, y) = 1"
3691,1,"['continuous', 'case', 'discrete']", JOINT AND CONDITIONAL DISTRIBUTIONS,seg_683,"for discrete case and (∫ ∫ap(x, y)dxdy = 1 for the continuous case)."
3692,1,"['marginal', 'marginal distributions', 'variates', 'distributions']", Marginal Distributions,seg_685,definition 11.2 marginal distributions are distributions of individual variates.
3693,1,"['case', 'discrete', 'joint', 'summation', 'continuous']", Marginal Distributions,seg_685,marginal pdf’s can be obtained from joint pdf’s by summation (in discrete case) or integration (in continuous case) as follows:
3694,1,"['continuous', 'discrete']", Marginal Distributions,seg_685,"f (x) = ∑ f (x, y) and f (y) = ∑ f (x, y) (discrete), (11.1) y=−∞ x=−∞ ∞ ∞ f (x) = f (x, y)dy and f (y) = f (x, y)dx (continuous), (11.2) ∫y=−∞ ∫x=−∞"
3695,1,"['independent', 'variables', 'range', 'marginal', 'variates', 'joint', 'summation']", Marginal Distributions,seg_685,where the summation or integration is carried out only throughout the range of proper variate. extension to more than two variates is straightforward. joint pdf is the product of constituent marginal pdfs when the variables are independent.
3696,1,['estimators'], Marginal Distributions,seg_685,"this has important applications in obtaining likelihoods, finding estimators, and so on."
3697,1,"['distribution', 'marginal', 'marginal distribution']", Marginal Distributions,seg_685,example 11.1 find marginal distribution
3698,1,"['marginal', 'joint']", Marginal Distributions,seg_685,"if the joint pdf of x and y is given by f (x, y) = kx(1 + y), {x = 1, 2}, {y = 1, 2, 3} find the marginal pmf of x and y."
3699,1,"['total probability', 'discrete', 'probability']", Marginal Distributions,seg_685,"solution 11.1 both x and y are discrete in this example. as the total probability is unity, we have k[2 + 3 + 4 + 4 + 6 + 8] = 1, giving k = 1∕27."
3700,1,"['range', 'marginal', 'distribution', 'marginal distribution']", Marginal Distributions,seg_685,"to obtain the marginal distribution of x, we sum out y over its entire range. hence, f (x) = (x∕27)[9] = x∕3, {x = 1, 2}. similarly, f (y) = ((1 + y)∕27)[3] = (1 + y)∕9, {y = 1, 2, 3}."
3701,1,"['distribution', 'marginal', 'marginal distribution']", Marginal Distributions,seg_685,example 11.2 find marginal distribution
3702,1,"['poisson', 'interval', 'probability']", Marginal Distributions,seg_685,a radioactive source is emitting -particles intermittently in different directions. the number of particles emitted in a fixed time interval is poisson( ). a particle recorder is placed at a point in direct line-of-sight. it has probability p of recording any particle coming toward it. find the pmf of the number of particles recorded.
3703,0,[], Marginal Distributions,seg_685,"solution 11.2 let x be the number of particles emitted and y be the number of particles recorded. then, we are given that"
3704,1,"['distribution', 'marginal', 'independent', 'marginal distribution']", Marginal Distributions,seg_685,"as these two sources are independent, f (x, y) is the product of the individual pdfs. from this the marginal distribution of y is obtained using equation (11.1) as"
3705,1,"['probabilities', 'conditional probabilities', 'variables', 'conditional', 'joint', 'joint distributions', 'distributions']", Conditional Distributions,seg_687,conditional distributions are obtained from joint distributions by conditioning on one or more variables. conditional pdf’s can be expressed in terms of joint pdf’s using laws of conditional probabilities.
3706,1,"['levels', 'conditional', 'conditional distributions', 'distributions']", Conditional Distributions,seg_687,it is easy to see that multiple conditional distributions exist by conditioning y at different levels.
3707,1,"['poisson', 'gamma distributed', 'parameter', 'gamma']", Conditional Distributions,seg_687,example 11.3 gamma distributed poisson parameter
3708,1,"['poisson', 'gamma', 'distribution', 'negative binomial', 'binomial', 'parameter']", Conditional Distributions,seg_687,"assume that the number of accidents follows a poisson law with parameter  . if   itself is distributed according to the gamma law, prove that the unconditional distribution is negative binomial distributed."
3709,1,"['poisson', 'gamma', 'distribution', 'independence', 'joint', 'poisson distribution']", Conditional Distributions,seg_687,"solution 11.3 let f (x,  ) represent the pdf of poisson distribution and f ( ,m, p) denote the gamma pmf. owing to independence, the joint distribution is the product of the marginals and the unconditional distribution of x is obtained by integrating out   as"
3710,1,['independent'], Conditional Distributions,seg_687,take constants independent of   outside the integral to get
3711,1,['gamma'], Conditional Distributions,seg_687,"the integral in equation (11.9) is easily seen to be the gamma integral, so that it becomes"
3712,1,"['functions', 'gamma', 'negative binomial distribution', 'distribution', 'negative binomial', 'binomial', 'binomial distribution']", Conditional Distributions,seg_687,where the last expression is obtained by writing (1 + m)x+p = (1 + m)x ∗ (1 + m)p and expanding the gamma functions as γ(x + p) = (x + p − 1)! and γ(p) = (p − 1)!. this is a negative binomial distribution with p = 1∕(1 + m).
3713,1,"['trinomial', 'conditional', 'distribution', 'conditional distribution']", Conditional Distributions,seg_687,example 11.4 find conditional distribution from trinomial
3714,1,"['jointly', 'trinomial']", Conditional Distributions,seg_687,let x and y be jointly distributed as trinomial with pmf
3715,1,"['distribution', 'conditional distribution', 'conditional']", Conditional Distributions,seg_687,find the conditional distribution of (i) y|x = x and (ii) x|x + y = n. obtain e(y|x) and e(x|x + y = n).
3716,1,['marginal'], Conditional Distributions,seg_687,solution 11.4 we get the marginal pmf of y (resp x) by summing over x (resp y). multiply and divide the rhs by (n − y)! and sum over x to get
3717,1,"['summation', 'binomial']", Conditional Distributions,seg_687,"where p3 = 1 − p1 − p2. expression inside the summation is simply the successive terms of the binomial expansion of (p1 + p3)n−y. however, (p1 + p3) = p1 +"
3718,0,[], Conditional Distributions,seg_687,"which is bino(n, p2). similarly, x ∼ bino(n, p1). the pmf of y|x is"
3719,0,[], Conditional Distributions,seg_687,"where y = 0, 1, …, n − x. this is the pmf of bino(n − x, p2∕(1 − p1)). hence, e(y|x) = (n − x)p2∕(1 − p1). (ii) x + y is clearly distributed as a bino(n, p1 + p2), so that"
3720,0,[], Conditional Distributions,seg_687,"(p1 + p2)n as (p1 + p2)x(p1 + p2)n−x, the above reduces to a bino(n, p1∕(p1 + p2)). from this, we get e(x|x + y = n) = np1∕(p1 + p2)."
3721,1,"['jacobian', 'determinant', 'statistics', 'mean', 'transformation']", JACOBIAN OF TRANSFORMATIONS,seg_689,"the jacobian is a useful concept in various fields of applied sciences, including vector calculus, differential equations, atmospheric sciences, astronomy, and statistics, to name a few. the jacobian determinant measures the stretching effect of a mapping or transformation as explained later. carl gustav jacobi (1804–1851), whose work originated in mathematical physics, invented it in 1841. it could mean either the jacobian matrix or its determinant (if the matrix is square)."
3722,1,"['jacobian', 'levels', 'variables', 'sensitivity']", JACOBIAN OF TRANSFORMATIONS,seg_689,"the jacobian matrix could be rectangular when a mapping is induced from the euclidean space ℝn → ℝm, where m   n. this matrix contains the partial derivatives of the output variables with respect to the input variables in modeling problems that involve many input and output variables (which need not tally in number). in other words, the jacobian relates infinitesimal areas in the input space to infinitesimal areas in the output space of the same dimensionality (areas in 2d, volume elements in ≥ 3d). by analyzing the rows of the jacobian matrix, we can study the impact or sensitivity on output variables due to a selected subset of input variables (by keeping the other variables at fixed levels)."
3723,1,"['jacobian', 'functions', 'determinant', 'transformations']", JACOBIAN OF TRANSFORMATIONS,seg_689,"as the determinant of a square matrix exists only if the matrix is of full-rank, there are some regularity conditions to be satisfied by the transformations. we assume that there are m real-valued functions y1 = f1(x1, x2, …, xn), y2 = f2(x1, x2, …, xn), …, ym = fm(x1, x2, …, xn). then the jacobian matrix comprises of all first-order partial derivatives of mapping functions:"
3724,1,"['linear', 'determinant', 'transformed', 'geometric', 'variable', 'statistical', 'transformation']", JACOBIAN OF TRANSFORMATIONS,seg_689,"the (i,j)th entry of the above matrix affirms that a small change dxi in the original variate x should contract to ( yi∕ x)dxi in the transformed space. when m = n, the transformation is concisely expressed as the determinant of above matrix. the determinant of a square matrix |j| is the same as the determinant of its transpose matrix |j′|. this means that the variable order is unimportant in statistical applications. a geometric interpretation of jacobians is that it represents the best linear approximation to mapped domain at a general point using a tangent plane in the transformed"
3725,1,"['jacobian', 'transformed', 'factor', 'function']", JACOBIAN OF TRANSFORMATIONS,seg_689,"space. thus, we get equivalent density contractions of space in transformed domain by multiplying the original function by the jacobian (which acts as a magnification or contraction factor). for example,"
3726,1,"['cardinality', 'statistics', 'variates', 'set', 'probability', 'random', 'transformation', 'probability theory', 'functions', 'continuous random variables', 'distribution', 'continuous', 'jacobian', 'absolute value', 'determinant', 'random variables', 'independent', 'table', 'variables']", JACOBIAN OF TRANSFORMATIONS,seg_689,"jacobian determinant is used to obtain the distribution of one-to-one (bijective) invertible functions of continuous random variables in statistics (the one-to-one condition can be relaxed in certain situations). derivation is considerably simplified when the original variates are either statistically independent, or are identically distributed. all such transformation functions should be at least once differentiable. we denote the jacobian determinant simply as |j| (instead of ‖j‖), where the vertical line has double meaning—it denotes the absolute value of the determinant (the double vertical bar | has various meanings in different fields—it denotes the absolute value of the argument in algebra, determinant in matrices, norm of a vector or a matrix in geometry, cardinality of a set or a set expression (such as a∩b) in set-theory and probability theory, and so on. in some of the discussions below, the |j| denotes only the determinant without absolute value. see example 11.8 (p. 431), table 11.1 (p. 425), etc)."
3727,1,"['variates', 'set', 'random', 'function', 'functions', 'transformations', 'distribution', 'jacobian', 'independent', 'table', 'variables', 'variable', 'joint']", Functions of Several Variables,seg_691,"distribution of a function of random variable(s) has many applications in engineering and applied sciences. these are easily obtained when the variates are independent. it is fairly easy to obtain the joint distribution of identically distributed variables using a correct set of transformations. in majority of problems of this type, we have to employ one of the transformations summarized in table 11.1. for functions of two variables, we have to choose a convenient auxiliary function such that the jacobian is nonzero, and auxiliary variable is easy to integrate out."
3728,1,"['jacobian', 'average']", Functions of Several Variables,seg_691,"the 2d jacobian works on the principle that the average value of a double integral favg(x, y) = 1∕[(b − a) ∗ (d − c)] ∫a"
3729,1,"['jacobian', 'case', 'transformation', 'limit']", Functions of Several Variables,seg_691,"transformation u = h(x, y);   = g(x, y) as done in the univariate case. if the derivatives of h(x, y) and g(x, y) exist for each point in the rectangular region [a, b]x[c, d], then the above limit will be finite. this allows us to equate the width of an infinitesimal strip under the surface f (x, y) as f (x, y)dxdy =  (u,  )dud  for all points within the region. from this, we get  (u,  ) = f (x, y)|j| = |j|f (h−1(u,  ), g−1(u,  )), where j is the jacobian of the transformation. the only conditions in this transformation are that the mapping is once differentiable (i.e., h−1(u,  ), g−1(u,  )) exists), and it is invertible (x, y can be expressed in terms of u,  )."
3730,1,"['functions', 'bivariate', 'transformations', 'jointly', 'transformation']", Arbitrary Transformations,seg_693,"the above transformation can be applied to arbitrary continuously differentiable, and invertible functions in higher dimensions (as bivariate, trivariate, and multivariate transformations). let x, y be jointly distributed according to some pdf f (x, y). consider arbitrary continuously differentiable, and invertible functions of the form"
3731,1,"['variables', 'function', 'transformation']", Arbitrary Transformations,seg_693,"here, either u or v is the required transformation, and the other is called the auxiliary function. the choice of the auxiliary function is quite often arbitrary. it can be as simple as one of the original variables, provided that the inverse transformation is easy to find. it can be polar or trigonometric transformation when expressions such"
3732,1,"['linear', 'bivariate', 'case', 'linear transformation', 'transformation']", Arbitrary Transformations,seg_693,"2 or x2 ± y2 are present. bivariate linear transformation is a special case of the above in which the dependency is u = c1x + c2y and   = c3x + c4y, where c′"
3733,1,"['transformation', 'jacobian']", Arbitrary Transformations,seg_693,i s are arbitrary constants. the jacobian of the transformation considerably simplifies in
3734,1,['case'], Arbitrary Transformations,seg_693,"this case as  (x, y)"
3735,1,"['transformed', 'bivariate', 'range', 'case', 'transformations']", Arbitrary Transformations,seg_693,"a challenge in this type of transformations is the range mapping (see figure 11.4). we could visualize the transformed mapping easily in the bivariate case, but it is not easy in higher dimensions."
3736,1,"['jacobian', 'determinant', 'method', 'independent', 'variables', 'table', 'case', 'cases', 'variable', 'function', 'transformation']", Arbitrary Transformations,seg_693,"finding the determinant of a transformation involves much work in some cases. this can be reduced by the do-little method. consider the transformation u = g(x, y) and = h(x, y). if = y (or analogously u = x), we can reduce the work as follows. simply substitute for y = in u to get u = g(x, ). next, find the derivative u∕ x = g(x, )∕ x. substitute for any x and take the reciprocal to get the jacobian. alternatively, express x = g(u, ) and find j = x∕ u = g(u, )∕ u. as examples, consider the transformation u = x + y, = y. put y = to get u = x + , and 1∕j = u∕ x = 1; as v is constant. alternatively, solve for x to get x = u − y = u − . then, j = x∕ u = ( ∕ u)(u − ) = 1. similarly, if u = xy, = y, then x = u∕ , and j = x∕ u = 1∕ ; and for u = x∕y, = y we have x = uv and j = x∕ u = . as another example, if u = x∕(x + y) and = y, x = uv∕(1 − u), and j = x∕ u = ∕(1 − u)2. this works even for constant multiples. if u = kx∕y, = y; we have x = uv∕k and j = ∕k. this idea can be extended to those cases where one of the input variables is a function of just one output variable in the two variables case. consider x = u2 − 2 and y = 2 (independent of u) so that j = 2u∗2 = 4u (see table 11.1)."
3737,1,"['distribution', 'variates']", Arbitrary Transformations,seg_693,example 11.5 distribution of sum of rectangular variates
3738,1,"['distribution', 'independent']", Arbitrary Transformations,seg_693,"an electronic circuit consists of two independent identical transistors connected in parallel. let x and y be the lifetimes of them, distributed as cuni(0, b) with pdf f (x) = 1∕b, 0   x   b. find the distribution of (i) z = x + y (ii) u = xy ."
3739,1,"['transformation', 'jacobian', 'absolute value']", Arbitrary Transformations,seg_693,"solution 11.5 consider the transformation z = x + y ,w = y . the inverse transformation is y = w,x = z − w. the absolute value of the jacobian is"
3740,1,"['absolute value', 'determinant', 'condition', 'range', 'results', 'joint']", Arbitrary Transformations,seg_693,"(here the first vertical bar denotes determinant and second one denotes absolute value). as x and y are iid, the joint pdf is f (x, y) = 1∕b2. the joint pdf of w and z is f ( , z) = 1∕b2|j| = 1∕b2. the range for z is [0, + 2b], and for w is [0, b]. as 0   x   b, we need to impose the condition 0   z −    b. this in turn results in two regions of integration as shown in figure 11.1. for 0   z   b, w varies between 0 and z so that f (z) = ∫ f ( , z)dw = ∫ "
3741,1,"['condition', 'range', 'case', 'cases', 'joint']", Arbitrary Transformations,seg_693,"b =z−b dw∕b2 = (2b − z)∕b2. combining both the cases, we can write the pdf as f (z) = (b − |b − z|)∕b2 for 0   z   2b, because when z   b, |b − z| = b − z; and for z ≥ b, |b − z| = z − b. in the second case, we put u = xy and v = y so that x = u∕v , and j = 1∕ . the range for u is [0, b2], and for v is [0, b]. as 0   x   b, we need to impose the condition 0   u∕v   b, or equivalently u   bv. the region of integration is shown in figure 11.2. the joint pdf of u and v is f (u,  ) = 1∕( b2), 0   u   bv   b2. the pdf of u is obtained as f (u) = ∫u"
3742,1,['jacobian'], Image Jacobian Matrices,seg_695,"image jacobian matrices used in robotics, unmanned aerial vehicles (uavs), image and video compression, medical imaging, and so on are often rectangular. in image"
3743,1,"['jacobian', 'residual', 'case', 'residual distortions', 'loss']", Image Jacobian Matrices,seg_695,"compression and video processing applications, we look for an unknown displacement vector (or matrix) to minimize two successive time frames (or subframes of appropriate sizes) so as to align successive images with minimal loss of information. sparse residual distortions indicate almost still image frames. in this case, the jacobian matrix becomes"
3744,1,['determinant'], Image Jacobian Matrices,seg_695,"with the determinant sign (when the matrix is square, i.e., m = n) indicating volumetric expansion (|j| 1), shrinkage (|j| 1), or steadiness (|j| = 1)."
3745,1,"['range', 'statistics', 'variates', 'function', 'transformation', 'linear', 'statistical', 'functions', 'transformed', 'transformations', 'interactions', 'jacobian', 'absolute value', 'determinant', 'method', 'table', 'variables', 'linear transformations', 'variable', 'adjusted']", Image Jacobian Matrices,seg_695,"the jacobian determinant is a function of the variates (or a constant) when applied to variate transformations in statistics (table 11.1). this means that the determinant can be made nonzero almost always. as the first derivative of linear functions is a constant, |j| is a scalar constant for linear transformations (if y = ax, then |j| = |a| for multivariate transformation). the range of the transformed variates should be adjusted to account for this fact. as the jacobian determinant is used as a multiplier, we take the absolute value of jacobian in statistical applications (the sign of a determinant depends on the order of the columns (variables) in the corresponding matrix). we can do better without the jacobian method for simple transformation of variates such as translations (u = x + c, = y + d), and scaling (u = c ∗ x, = d ∗ y) (using the cdf or mgf methods). the power of the jacobian method becomes apparent when variable interactions are present. see references 137, 293 and 296 for further examples."
3746,1,"['functions', 'exponential distribution', 'distribution', 'exponential']", Image Jacobian Matrices,seg_695,example 11.6 functions of exponential distribution
3747,1,"['distribution', 'joint', 'variates']", Image Jacobian Matrices,seg_695,"1 let xi’s be iid exp( ) with pdf f (xi) =  e−xi∕ . define new variates yi’s as y1 = x1∕(x1 + x2 + · · · + xn),y2 = (x1 + x2)∕(x1 + x2 + · · · + xn), etc yk = (x1 + x2 + · · · + xk)∕(x1 + x2 + · · · + xn), and yn = (x1 + x2 + · · · + xn). prove that the joint distribution of (y1,y2, …,yn) depends on yn and yn−1 only."
3748,1,['joint'], Image Jacobian Matrices,seg_695,"solution 11.6 as xi’s are iid, the joint pdf is the product of individual pdfs. 1 thus f (x1, x2, …, xn) =  n e−∑i"
3749,1,['jacobian'], Image Jacobian Matrices,seg_695,"n =1 xi∕ . the inverse mapping is x1 = y1yn, x2 = yn(y2 − y1) , x3 = yn(y3 − y2), xk = yn(yk − yk−1) …, xn = yn(1 − yn−1). the jacobian is || (x1, x2, …, xn) || |j| = | | | (y1, y2, …, yn) |"
3750,1,"['determinant', 'transformations']", Image Jacobian Matrices,seg_695,"to evaluate this determinant, we apply the row transformations r′ 2 = r2 + r1, r′ 3 = r3 + r′"
3751,1,['determinant'], Image Jacobian Matrices,seg_695,"by expanding this determinant along the first column, we get |j| = ynn(1 − yn−1). 1 thus, f (y1, y2, …, yn) =  n e−yn∕ ynn(1 − yn−1)."
3752,1,"['functions', 'random variables', 'bivariate', 'variables', 'discrete random variables', 'discrete', 'joint', 'random', 'transformation']", Image Jacobian Matrices,seg_695,"this technique is applicable to discrete random variables as well. let u = g(x, y) and   = h(x, y) be the bivariate mapping as before. find the inverse transformation (express x and y as functions of u and v, say x = f1(u,  ) and y = f2(u,  ). then the joint pmf of u and   is puv (u,  ) = pxy (f1(u,  ), f2(u,  ))."
3753,1,"['jacobian', 'random variables', 'independent', 'method', 'variables', 'marginal', 'distribution', 'joint', 'independence', 'independent random variables', 'random', 'continuous', 'transformation']", Distribution of Products and Ratios,seg_697,"the distribution of products and ratios of independent random variables are of interest in some applications. these can be obtained by the jacobian technique when the variables involved are continuous and independent. make the transformation u = x∕y and v = xy . then, x = √u , and y = √ ∕u, so that the jacobian is 1∕(2u). the joint pdf is the product of the marginal pdfs (due to independence assumption). from this, the pdf of either of them can be obtained by integrating out the other. an alternate and simple method exists using the modf discussed in the last chapter."
3754,1,"['range', 'curve']", Distribution of Products and Ratios,seg_697,"as xy = c represents a parabolic curve, we split the range of integration of y from (−∞, z∕x] and from [z∕x,∞) to get"
3755,1,['transformation'], Distribution of Products and Ratios,seg_697,using the transformation u = xy this becomes
3756,0,[], Distribution of Products and Ratios,seg_697,"this, on rearrangement, becomes"
3757,0,[], Distribution of Products and Ratios,seg_697,differentiate with respect to z to get the pdf as
3758,1,"['cauchy', 'cauchy distribution', 'variates', 'distribution', 'normal']", Distribution of Products and Ratios,seg_697,"it is shown below that if x and y are iid normal variates, the ratio x/y has a cauchy distribution. analogously u = x∕y has pdf"
3759,1,['distributions'], Distribution of Products and Ratios,seg_697,example 11.7 ratio of uniform distributions
3760,1,['distribution'], Distribution of Products and Ratios,seg_697,"if x and y are cuni(0, b) distributed, find the distribution of u = x∕y ."
3761,1,"['plot', 'jacobian', 'curve', 'joint']", Distribution of Products and Ratios,seg_697,"solution 11.7 let u = x∕y ,v = y so that the inverse mapping is y = v ,x = uv . the jacobian is |j| =  . the joint pdf is f (x, y) = 1∕b2. hence, f (u,  ) =  ∕b2. the pdf of u is obtained by integrating out v. a plot of the mapping is shown in figure 11.3. the region of interest is a rectangle of sides 1 × b at the left, and a curve uv = b to its right. integrating out v, we obtain f (u) = ∫0"
3762,1,"['distribution', 'exponential', 'gamma', 'variates']", Distribution of Products and Ratios,seg_697,"see reference 294 for the distribution of ratios of exponential variates, and reference 297 for ratios of gamma variates."
3763,1,"['distribution', 'gamma distribution', 'gamma']", Distribution of Products and Ratios,seg_697,example 11.8 sum and ratio of gamma distribution
3764,1,['distribution'], Distribution of Products and Ratios,seg_697,"if x and y are iid gamm( ,  i), find the distribution of (i) x + y , (ii) x/y."
3765,1,"['transformation', 'jacobian']", Distribution of Products and Ratios,seg_697,"solution 11.8 let u = x + y and v = x∕y . solving for x and y in terms of u u  u and v, we get x = , and y = . the jacobian of the transformation is 1+  1+    u | | ||1 +   (1 +  )2 || −u |j| = | | = (1 +  )2 . 1 u | | − ||1 +   (1 +  )2 ||"
3766,1,"['jacobian', 'joint']", Distribution of Products and Ratios,seg_697,"  1+ 2 the joint pdf of x and y is f (x, y) = e− (x+y)x 1−1y 2−1. multiply by γ( 1)γ( 2) the jacobian, and substitute for x, y to get"
3767,0,[], Distribution of Products and Ratios,seg_697,the pdf of u is obtained by integrating out v as
3768,1,['gamma'], Distribution of Products and Ratios,seg_697,"which is gamma( 1,  2)."
3769,0,[], Distribution of Products and Ratios,seg_697,the pdf of v is found by integrating out u as
3770,1,"['distribution', 'pearson']", Distribution of Products and Ratios,seg_697,"1 + 2 , which is beta2( 1,  2) (also called γ( 1)γ( 2) pearson type vi distribution)."
3771,1,"['independent', 'pairwise independent', 'distributions']", Distribution of Products and Ratios,seg_697,example 11.9 ratio of pairwise independent distributions
3772,1,"['distribution', 'independent']", Distribution of Products and Ratios,seg_697,"let xi’s be iid exp(  ) for i = 1, 2, ..,m. let yj’s be iid exp( ) for j = 1, 2, .., n. if xi’s and yj’s are pair-wise independent, find the distribution of w = u∕v ="
3773,1,"['independent', 'gamma', 'variates', 'distribution', 'gamma distributions', 'joint', 'distributions']", Distribution of Products and Ratios,seg_697,"solution 11.9 as xi’s are iid, the joint pdf is the product of individual pdfs. we first use the mgf technique to find the distribution of numerator and denominator. the mgf of exp( ) is mx(t) = 1∕[1 − t]. as the xi’s are iid, mu(t) = 1∕[1 − t]m. similarly, m (t) = 1∕[1 − t]n. these are the mgfs of gamma distributions. hence, w is the ratio of two independent gamma variates, whose distribution is found in example 11.8."
3774,1,['gamma'], Distribution of Products and Ratios,seg_697,example 11.10 from gamma to beta
3775,1,"['distribution', 'joint']", Distribution of Products and Ratios,seg_697,"solution 11.10 we find the distribution of u = x + y and v = x∕(x + y). the joint pdf is   1+ 2 f (x, y) = x 1−1y 2−1e− (x+y). (11.27) γ( 1)γ( 2)]"
3776,1,"['jacobian', 'joint']", Distribution of Products and Ratios,seg_697,"the inverse mapping is x = uv, y = u(1 −  ), so that the jacobian is u. the joint pdf of u and v is"
3777,0,[], Distribution of Products and Ratios,seg_697,"0   u   1, 0       ∞. combining common terms this becomes"
3778,1,['distribution'], Distribution of Products and Ratios,seg_697,"integrating out u, it is easy to show that v has a beta1( 1,  2) distribution."
3779,1,"['normal distributions', 'independent', 'normal', 'distributions']", Distribution of Products and Ratios,seg_697,example 11.11 ratio of independent normal distributions
3780,1,['distribution'], Distribution of Products and Ratios,seg_697,"if x,y are iid n(0,  i 2), find the distribution of u = x∕y , and v ="
3781,1,['range'], Distribution of Products and Ratios,seg_697,"solution 11.11 here, both u and v have range −∞ to ∞. from u = x∕y , we"
3782,0,[], Distribution of Products and Ratios,seg_697,"within the square-root in the denominator outside, and canceling it out with the"
3783,1,['jacobian'], Distribution of Products and Ratios,seg_697,x =  √ u2 + 1. the jacobian is easily obtained as
3784,1,"['distribution', 'joint']", Distribution of Products and Ratios,seg_697,we first find the distribution of u. the joint pdf of x and y is
3785,0,[], Distribution of Products and Ratios,seg_697,"substituting the values of x and y, (x2∕ 1"
3786,1,['independent'], Distribution of Products and Ratios,seg_697,"write a = (1 + u2)( 1 + 1 ), which is independent of v. then, f (u,  ) =  12  22u2"
3787,1,['jacobian'], Distribution of Products and Ratios,seg_697,"1 e− 2 1 a 2 . multiply by the jacobian, and integrate out v, to get the pdf of u 2  1 2 as ∞ f (u) = 1 (1 + 1∕u2)  e− 1 2 a 2 dv. (11.31) 2  1 2 ∫−∞"
3788,0,[], Distribution of Products and Ratios,seg_697,substituting a = (1 + u2)( 1 + 1 ) this becomes  12  22u2
3789,0,[], Distribution of Products and Ratios,seg_697,"this, after simplification, becomes"
3790,1,"['independent', 'factor', 'cauchy', 'cauchy distributed', 'cauchy distribution', 'distribution', 'normal', 'standard', 'variance']", Distribution of Products and Ratios,seg_697,"this is the pdf of a scaled cauchy distribution with scaling factor  2∕ 1. when  1 =  2, equation (11.34) reduces to the standard cauchy distribution. thus, the ratio of two independent normals (with same variance) is cauchy distributed. this result can be used to characterize the normal law as follows:"
3791,1,"['random variables', 'independent', 'variables', 'cauchy', 'cauchy distributed', 'independent random variables', 'random', 'variance']", Distribution of Products and Ratios,seg_697,"remark 1 if x and y are independent random variables with the same variance, whose ratio is cauchy distributed, then x and y are n(0,  2) distributed."
3792,1,"['cauchy', 'cauchy distributed', 'dependent', 'variates', 'correlation', 'independence', 'normal']", Distribution of Products and Ratios,seg_697,"we have assumed the independence of the normal variates in the above derivation. if they are dependent with correlation  , the ratio is no longer cauchy distributed."
3793,0,[], Distribution of Products and Ratios,seg_697,"to find the pdf of v, integrate out u from −∞ to ∞. write the exponent as"
3794,1,['jacobian'], Distribution of Products and Ratios,seg_697,"multiply by the jacobian, and integrate out u, to get the pdf of v as"
3795,0,[], Distribution of Products and Ratios,seg_697,"to evaluate i1, we use the formula"
3796,0,['e'], Distribution of Products and Ratios,seg_697,2 √ 2 e  1 2 . substitute these values in equation (11.35) to get the pdf of v as
3797,1,['factors'], Distribution of Products and Ratios,seg_697,"canceling out common factors ( ,√"
3798,0,[], Distribution of Products and Ratios,seg_697,"2  from numerator and denominator) and taking the  1 2 in the denominator into the brackets, we simplify this to the form"
3799,1,"['distribution', 'normal', 'variance', 'normal distribution']", Distribution of Products and Ratios,seg_697,"2 + 1∕ 2 2), and note that (1∕ 1 2 + 1∕ 2 2 + ) = (1∕ 1 + 1∕ 2)2,  1 2 we find that this is the pdf of a normal distribution with variance ( 1 2∕[ 1 +  2])2."
3800,1,"['continuous random variables', 'random variables', 'independent', 'variables', 'random', 'continuous', 'variances']", Distribution of Products and Ratios,seg_697,remark 2 if x and y are independent continuous random variables with variances
3801,0,[], Distribution of Products and Ratios,seg_697,2) distributed.
3802,1,"['polar transformations', 'transformed', 'polar transformation', 'treatment', 'statistics', 'transformations', 'set', 'transformation']", POLAR TRANSFORMATIONS,seg_699,"polar transformation finds applications in integral calculus, differential equations, statistics, and image-based computing (log-polar transformations), among many other fields. it is so-called because the cartesian points that use horizontal and vertical coordinate axes are transformed into polar coordinates that use radius and angle with respect to a fixed set of coordinate axes. the most popular polar transformations are discussed in the following section. advanced treatment on this topic can be found in references 296, 299 and 300, and so on."
3803,0,[], Plane Polar Transformations PPT,seg_701,"the name comes from the fact that it is applied in 2d for cartesian to polar mapping. let (x, y) represent the cartesian coordinates and (r,  ) denote the corresponding polar coordinates. then, the mapping is defined by the relation x = r cos( ) and y ="
3804,1,"['transformation', 'jacobian']", Plane Polar Transformations PPT,seg_701,"2,   = tan−1(y∕x). the jacobian of the transformation is"
3805,1,"['functions', 'statistics', 'case', 'transformation']", Plane Polar Transformations PPT,seg_701,"as x2 + y2 = r2, this transformation is especially useful in statistics when the pdf contains functions of the form x2 or 1 ± x2 (in the univariate case) and ∑ixi"
3806,1,['case'], Plane Polar Transformations PPT,seg_701,2 (in the multivariate case).
3807,1,['distribution'], Plane Polar Transformations PPT,seg_701,example 11.12 find distribution of √ x2 + y2
3808,1,"['standard normal', 'normal', 'standard']", Plane Polar Transformations PPT,seg_701,"if x and y are iid standard normal n(0,  2), find the pdf of √ x2 + y2."
3809,1,"['joint', 'independent']", Plane Polar Transformations PPT,seg_701,"solution 11.12 as they are independent, the joint pdf is"
3810,1,"['transformation', 'joint', 'jacobian']", Plane Polar Transformations PPT,seg_701,"make the transformation x = r cos( ) and y = r sin( ), so that x2 + y2 = r2 and   = tan−1(y∕x). the jacobian is r. hence, the joint pdf of r and   is f (r,  ) = 1 −r2∕2 2 re . the density of r is found by integrating out   as 2  2"
3811,1,"['independent', 'distribution', 'joint', 'rayleigh distribution', 'uniformly distributed']", Plane Polar Transformations PPT,seg_701,"which is the rayleigh distribution. as the joint pdf of r and   is independent of  , this is an indication that   is uniformly distributed."
3812,1,"['distribution', 't distribution']", Plane Polar Transformations PPT,seg_701,example 11.13  2n to student’s t distribution
3813,1,"['transformation', 'jacobian', 'polar transformation']", Plane Polar Transformations PPT,seg_701,"solution 11.13 consider the polar transformation x = r cos2( ) and y = r sin2( ). then, x + y = r(sin2( ) + cos2( )) = r, x − y = r(cos2( ) − sin2( )) = r r2 r cos(2 ),√xy = r sin( ) cos( ) = sin(2 ), so that xy = sin2(2 ). the 2 4 jacobian of the transformation is"
3814,1,['joint'], Plane Polar Transformations PPT,seg_701,xy √n cot(2 ). the joint pdf of x and y is
3815,1,['jacobian'], Plane Polar Transformations PPT,seg_701,multiply by the jacobian and substitute the values of x and y to get
3816,0,[], Plane Polar Transformations PPT,seg_701,"this, after simplification, reduces to"
3817,0,[], Plane Polar Transformations PPT,seg_701,"distribution of   is obtained by integrating out r. thus,"
3818,1,"['transformation', 'jacobian']", Plane Polar Transformations PPT,seg_701,"   consider the transformation t = √n cot(2 ), so that = 1∕[2√n cosec2  t (2 )] = 1∕[2√n(1 + cot2(2 )]. writing sin(2 ) = 1∕√1 + cot2(2 ), and multiplying by the jacobian, we get"
3819,1,"['distribution', 'results', 't distribution', 'student t distribution']", Plane Polar Transformations PPT,seg_701,"multiply the numerator and denominator by γ((n + 1)∕2)γ(1∕2) and use the formula γ(n)γ(1∕2) = 2n−1γ(n∕2)γ( n+1 ) to get the constant multiplier in the form 2 1∕[√nb( 1 2 , n 2 )] (where γ(1∕2) = √ ). this is the student t distribution t(n). see cacoullos [254] for a cdf derivation of this and related results."
3820,1,"['functions', 'weibull', 'distributions']", Plane Polar Transformations PPT,seg_701,example 11.14 functions of weibull distributions
3821,1,['independent'], Plane Polar Transformations PPT,seg_701,"2 −x2∕b2 if x, y are iid weibull(2, b) with pdf f (x, b) = xe , for x ≥ 0, show that b2 z = xy∕(x2 + y2) has pdf f (z) = 2z∕√ 1 − 4z2, which is independent of b."
3822,1,"['transformation', 'joint', 'jacobian']", Plane Polar Transformations PPT,seg_701,"solution 11.14 put x = r cos( ) and y = r sin( ), so that x2 + y2 = r2 and   = tan−1(y∕x). the jacobian of the transformation is r. hence, the joint pdf of 4r3 2 2 r and   is f (r,  ) = sin( ) cos( )e−r ∕b . using 2 sin( ) cos( ) = sin(2 ) this b4 2r3 2 2 becomes f (r,  ) = sin(2 )e−r ∕b . the pdf of   is obtained by integrating out b4 r as ∞ f ( ) = 2 sin(2 ) r3e−r2∕b2 dr. (11.47) b"
3823,1,"['transformation', 'jacobian']", Cylindrical Polar Transformations CPT,seg_703,"this transformation is a simple extension of the above to 3d. the mapping is defined by the relations x = r cos( ), y = r sin( ), and z = z. the jacobian of this transformation also is r. this defines a cylinder of base r in the polar coordinates. inverse"
3824,0,[], Cylindrical Polar Transformations CPT,seg_703,mapping is easily obtained as r = √x2 + y
3825,1,"['transformation', 'jacobian']", Spherical Polar Transformations SPT,seg_705,"is defined as r = √x2 + y2 + z2,   = tan−1(y∕x), and   = sin−1(z∕r). the jacobian of this transformation is r2cos2( ). an equivalent mapping is defined by the relations x = r cos( ) sin( ), y = r sin( ) sin( ), and z = r cos( ). the inverse mapping is"
3826,1,"['jacobian', 'functions', 'transformation']", Spherical Polar Transformations SPT,seg_705,2∕z). the jacobian of this transformation is −r2 sin( ) (so that dxdydz = r2 sin( )drd d ). the name spherical transformation comes from the fact that its domain is of the form x2 + y2 + z2 or arithmetic functions of it.
3827,1,"['transformation', 'jacobian', 'helmert transformation']", Other Methods,seg_707,"the spt can be generalized to n-dimensions in multiple ways. one simple way is to use the helmert transformation x1 = r cos( 1), x2 = r sin( 1) cos( 2), x3 = r sin( 1) sin( 2) cos( 3), · · · , xn−1 = r sin( 1) sin( 2) … sin( n−2) cos( n−3), and xn = r sin( 1) sin( 2) … sin( n−1). the jacobian is given by |j| = rn−1sinn−2( 1)sinn−3 ( 2)sinn−4( 3) · · · sin( n−2). squaring and adding each term, we get r2 = x1"
3828,1,"['transformation', 'polar transformation', 'table']", Other Methods,seg_707,"toroidal polar transformation (tpt) is an extension of spt (see table 11.2), defined as"
3829,1,['jacobian'], Other Methods,seg_707,the jacobian is 1/j=
3830,1,"['jacobian', 'determinant']", Other Methods,seg_707,"where b = (r cos( ) + r). to evaluate this determinant, take out b from second row, r from third row, multiply new first column by cos( ), new second column by sin( ), and add new second column to the first column (c1 = c1 + c2). the (2, 2)th element also becomes zero. then, expand the determinant along second row to get the jacobian as b = (r cos( ) + r)."
3831,1,['transformation'], Other Methods,seg_707,"another general transformation is given by x = r cos( ) cos( ), y = r sin( )"
3832,1,"['jacobian', 'case']", Other Methods,seg_707,"1 − m2sin2( ), z = r sin( )√ 1 − n2sin2( ), where m2 + n2 = 1. squaring and adding gives us x2 + y2 + z2 = r2. the jacobian in this case is 1/j="
3833,1,"['factor', 'determinant']", Other Methods,seg_707,"to evaluate this determinant, take r as a common factor from second and third columns, multiply first column by sin( ) and second column by cos( ), and apply c1 = c1 + c2 (i.e., add new second column to new first column). the first element at (1,1) reduces to zero, so that the determinant becomes that of two 2×2 matrices. using the relationship m2 + n2 = 1, this is easily seen to be r2 (m2cos2( ) +"
3834,1,['table'], Other Methods,seg_707,2sin2( )] (see table 11.2).
3835,1,"['characteristic function', 'transformed', 'variables', 'variates', 'distribution', 'function', 'transformation']", Other Methods,seg_707,"there are many other ways to find the distribution of transformed variables. one possibility is to use the characteristic function (if it is easily invertible) of the original variates. let u = g(x1, x2, …, xn) be the transformation required. if  z(t) = e(eitg(x)) = ∫ ∫ eitg(x)f (x1, x2, …, xn)dx1..dxn is easy to evaluate, we could simply use inversion theorem to get the pdf of u."
3836,1,"['marginal', 'conditional', 'discrete', 'variates', 'probability', 'random', 'probability distributions', 'exponential', 'distributions', 'functions', 'continuous random variables', 'continuous distributions', 'distribution', 'continuous', 'jacobian', 'random variables', 'variables', 'probability distribution', 'normal', 'joint', 'conditional probability', 'joint probability']", SUMMARY,seg_709,"this chapter discusses the methodology to obtain the marginal, joint, and conditional probability distributions for both the discrete (i.e., count) distributions and continuous distributions. the concept and tools for the jacobian to derive the joint probability distribution of functions of continuous random variables are introduced and illustrated in this chapter. distribution of functions of two or more variates has received much attention in the literature. most of the research in this field uses the normal [218] and exponential [294] distributions"
3837,0,[], SUMMARY,seg_709,"see shepp [217], quine [218], baringhaus et al. [301], and jones [302] for alternative derivations of the result in example 11.0 (p. 433). bansal et al. [303] uses the"
3838,1,"['distribution', 'moments']", SUMMARY,seg_709,uniqueness of moments to prove that the distribution of 2xy/√
3839,1,"['jacobian', 'method', 'discrete', 'transformations', 'continuous']", SUMMARY,seg_709,a) the jacobian method is applicable to both discrete and continuous variate transformations.
3840,1,"['range', 'random variable', 'variable', 'random', 'transformation']", SUMMARY,seg_709,"b) if the range of a random variable x includes the origin, we cannot use the transformation y = 1∕x."
3841,1,"['marginal', 'marginal distributions', 'joint', 'joint distributions', 'distributions']", SUMMARY,seg_709,c) marginal distributions can be obtained from joint distributions.
3842,1,"['independent', 'marginal', 'marginal distributions', 'variates', 'joint', 'joint distributions', 'distributions']", SUMMARY,seg_709,d) marginal distributions determine joint distributions only when variates are independent.
3843,1,"['random variables', 'variables', 'joint', 'random']", SUMMARY,seg_709,e) joint pdf of random variables can be obtained uniquely from joint cdf.
3844,1,['joint'], SUMMARY,seg_709,"11.2 if f (x, y) = c(x + y) is the joint f (x, y, z) = k(x + 2y + 3z) for 0  "
3845,1,"['discrete', 'random']", SUMMARY,seg_709,"x   1, 0   y   2, 0   z   1. pdf of two discrete random vari-"
3846,1,['jacobian'], SUMMARY,seg_709,"ables (x = 1, 2, 3; y = 1, 2, 3, 4), 11.5 what is the jacobian of the trans-"
3847,0,[], SUMMARY,seg_709,"find the constant c and hence formation u = ax + by ,   = cx −"
3848,1,"['distribution', 'conditional distribution', 'conditional']", SUMMARY,seg_709,"obtain the conditional distribution dy . if x and y are triangular, find of y given x = k, and the distributhe distribution of u and v."
3849,1,['jacobian'], SUMMARY,seg_709,tion of x2. 11.6 find the jacobian of the rotation
3850,1,['jacobian'], SUMMARY,seg_709,11.3 what is the jacobian of the transtransformation u = x cos( ) − y
3851,1,"['distribution', 'independent']", SUMMARY,seg_709,"use it to find the distribution of u 11.7 if f (x, y) = k ∗ x3y2e−(x+(y∕2)) for and v when x and y are (i) cuni(0, x, y   0, find constant k. are x 1) and (ii) beta(0, 1). and y independent?"
3852,1,['jacobian'], SUMMARY,seg_709,11.4 find the unknown k in the fol11.8 what is the jacobian of the trans-
3853,1,['joint'], SUMMARY,seg_709,"lowing joint pdfs: (a) f (x, y) = formation x = r cosh( ) cosh( ), y"
3854,1,"['marginal', 'marginal distributions', 'distributions']", SUMMARY,seg_709,"1, 0   y   1 find constant k and (ii) gamm(mi, p), i = 1, 2, and obtain the marginal distributions. (iii)  2n."
3855,1,"['independent', 'variables', 'transformation', 'beta distributed', 'transform', 'distributions']", SUMMARY,seg_709,"jacobian for the transformation b − 1)]xa−1yb−1(1 − x − y)m−a−b−1, u = 2xy∕(x2 + y2), = x2 + y2 prove that the marginals and con+ z2, = (x2 + y2)∕z2 (hint: use ditional distributions of x|y, y|x spherical polar transform). are all beta distributed when the variables are independent. 11.11 find the inverse mapping and"
3856,1,"['jacobian', 'variates', 'transformation']", SUMMARY,seg_709,"jacobian for the transformation 11.14 find the jacobian of the rotau = x + y, = x2 − y2 if x and y tion transformation u = x cos( ) − are iid chi-square variates. y sin( ), and = x sin( ) + y cos( ). 11.12 find the jacobian of the shear"
3857,1,['distribution'], SUMMARY,seg_709,"transformation (parallel to the 11.15 if x is beta-i(p, q) and y is indey-axis) u = ax + y, = y. use it to pendent beta-i(p + q, r) find the find the distribution of u and v distribution of x∕(x + y)."
3858,1,['distribution'], SUMMARY,seg_709,"11.16 if x1,x2 are iid n(0,1), find the distribution of y1 = √−2log e(x1) cos(2 x2)"
3859,1,"['cauchy', 'independent']", SUMMARY,seg_709,11.17 if x and y are independent cauchy prove that e( |x) = (a + x)∕(a +
3860,1,"['jacobian', 'cauchy', 'cauchy distributed']", SUMMARY,seg_709,xy) are also cauchy distributed. and jacobian for the transfor-
3861,1,['independent'], SUMMARY,seg_709,"11.18 if x and y are independent mation x = r sin( ) sin( ), y ="
3862,1,"['function', 'density function', 'jacobian']", SUMMARY,seg_709,"gamma distributed, prove that r sin( ) cos( ), z = r cos( ) sin( ), z = x∕(x + y) is type i beta dis= r cos( ) cos( ). tributed. 11.22 suppose two fair dice are tossed. 11.19 find the jacobian of the find the density function of"
3863,0,[], SUMMARY,seg_709,transformation u1 = x
3864,1,"['distribution', 'joint', 'scores']", SUMMARY,seg_709,"1 ,u2 = (x1,x2) where x1 and x2 are the x2 , …,un−1 = xn−1 , and x1 2 + scores that show up. xn xn x2 2 + · · · + xn2 = 1. if xi ′s are iid 11.23 if x,y are iid cuni(a, b), find n(0,1), find the joint distribution"
3865,1,['distribution'], SUMMARY,seg_709,"a=−b, find the distribution of 11.20 suppose that x∼bino(n,  ), v = x2."
3866,1,['distribution'], SUMMARY,seg_709,"where   ∼ beta(a, b). find the −(ax+by) 11.24 if f (x, y) = ke , find k and unconditional distribution of x,"
3867,1,['distribution'], SUMMARY,seg_709,conditional distribution of  |x and obtain the pdf of x/y and x − y .
3868,1,['jacobian'], SUMMARY,seg_709,"11.26 if x ∼  m2 , and y ∼  n2 prove that zx , prove that the jacobian is a"
3869,1,['distribution'], SUMMARY,seg_709,"y x∕(x + y) ∼ beta(m∕2, n∕2), constant. find the distribution of u when x,y, z are (i) gamm(m, and x∕y ∼ beta2(m∕2, n∕2). p) and (ii) cuni(0, b). 11.27 if x ∼  2m, y∼  2n and z∼  2p find"
3870,0,[], SUMMARY,seg_709,"2m+2n and y ∼ as gamma(mn, p)."
3871,1,"['distribution', 'independent']", SUMMARY,seg_709,"beta1(p, q) be independent, find 11.33 express the cartesian coordinates distribution of xy and x(1 − y). in terms of cylindrical and spheri-"
3872,1,"['gamma distributed', 'gamma']", SUMMARY,seg_709,cal polar coordinates. 11.29 if x and y are gamma distributed
3873,1,"['parameters', 'distribution', 'exponentially']", SUMMARY,seg_709,"with parameters (p,m) and (q,m) 11.34 if x and y are exponentially disfind the distribution of x/y and tributed, find the distribution of"
3874,1,['joint'], SUMMARY,seg_709,"11.35 if x1,x2,x3 are iid gamma(mk, p) for k = 1,2,3 prove that the joint dis-"
3875,1,['bivariate'], SUMMARY,seg_709,"tribution of y1 = x1∕x3 and y2 = x2∕x3 is bivariate beta-ii(mk) with pdf m −1 m −1 1 2 f (y1, y2) = γ"
3876,1,"['transformation', 'jacobian']", SUMMARY,seg_709,11.36 find the jacobian of the transformation x1 = r sin( 1) sin( 2) · · · sin( n−2) sin
3877,1,['distribution'], SUMMARY,seg_709,"the distribution of (i) u + v , (ii) xn)∕√n(n − 1) for i = 3, 4, ..n."
3878,0,[], SUMMARY,seg_709,|u − v| 11.40 if y has a chi-distribution with
3879,1,"['joint', 'independent']", SUMMARY,seg_709,"m dof and z is beta-i((m − 11.38 if x and y have joint pdf f (x, y) = 1)∕2, (m − 1)∕2) is independent"
3880,1,['standard'], SUMMARY,seg_709,"exp(−x − y), x, y ≥ 0, find the disof y, then (2z − 1)y is standard tribution of x/y and x + y assumnormal."
3881,1,"['jacobian', 'independence']", SUMMARY,seg_709,ing independence. 11.41 find the inverse mapping and 11.39 find the jacobian of the jacobian for the transforma-
3882,0,[], SUMMARY,seg_709,transformation y1 = ∑i
